id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13373543,Integrate the Apache released HBase as back store of Ambari Metrics,"Since the non managed HDP tarballs are not accessible anymore publicly AMBARI-25599 is an attempt to replace those dependencies of Ambari Metrics with the open source versions.

However, the Apache version of HBase (  [https://archive.apache.org/dist/hbase/2.0.2/hbase-2.0.2-bin.tar.gz]  ) does not starting up and failing with:
{code:java}
[root@c7401 ambari-metrics-collector]# less hbase-ams-master-c7401.ambari.apache.org.log{code}
{code:java}
2021-02-10 08:19:01,953 ERROR [main] master.HMasterCommandLine: Master exiting
 java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
 at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<init>(DefaultMetricsSystem.java:38)
 at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<clinit>(DefaultMetricsSystem.java:36)
 at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:159)
 at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:140)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
 at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:149)
 at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2964)
 Caused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration{code}
The missing class '_org.apache.commons.configuration.Configuration_' is located in _'commons-configuration2'_ but strangely that is not present in '_/usr/lib/ams-hbase/lib/'_ if the Apache version of HBase is used for the build. 
 * *Investigate why HBase fails to start up ?*
 * *Why is commons-configuration2 lib missing ?*
 * *Integrate the Apache released HBase as back store of Ambari Metrics*",apache hbase metrics,['ambari-metrics'],AMBARI,Task,Major,2021-04-19 08:27:43,0
13368381,Components running on a Metrics Collector host should use the local Collector,"On a cluster where Ambari Metrics HA is set up, thus the cluster has multiple metrics collectors, the components running on the collector's host should always use the local collector if available.
{code:java}
2021-03-26 11:24:17,180 INFO [HBase-Metrics2-1] availability.MetricSinkWriteShardHostnameHashingStrategy: Calculated collector shard c7402.ambari.apache.org based on hostname: c7403.ambari.apache.org
{code}
In the above log the RegionServer running on c7403.ambari.apache.org is using the collector of c7402.ambari.apache.org  despite having a local collector on c7403.",HA metric-collector metrics shard,['ambari-metrics'],AMBARI,Bug,Minor,2021-03-29 14:07:33,0
13367557,FindBugs: Class defines equals() and uses Object.hashCode(),"FindBugs finding:
org.apache.ambari.server.state.alert.MetricSource$JmxInfo defines equals and uses Object.hashCode()
This class overrides {{equals(Object)}}, but does not override {{hashCode()}}, and inherits the implementation of {{hashCode()}} from {{java.lang.Object}} (which returns the identity hash code, an arbitrary value assigned to the object by the VM).  Therefore, the class is very likely to violate the invariant that equal objects must have equal hashcodes.",cleanup findbugs server,['ambari-server'],AMBARI,Bug,Major,2021-03-25 12:11:32,0
13365569,FindBugs: Comparison of String parameter using == or != ,Fix FindBugs issue: Comparison of String parameter using == or != in org.apache.ambari.server.state.host.HostImpl.setStatus(String),cleanup findbugs server,['ambari-server'],AMBARI,Bug,Minor,2021-03-16 15:18:33,0
13365487,Clear Cluster and METRIC_AGGREGATORS MBeans upon shutdown,"Following warnings appear in metrics-collector.log upon startup.
{code:java}
javax.management.InstanceAlreadyExistsException: ClusterStatus: cluster=ambari-metrics-cluster,instanceName=ctr-e153-1613480641811-87570-01-000057.hwx.site_12001
 at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
 at org.apache.helix.monitoring.mbeans.ClusterStatusMonitor.register(ClusterStatusMonitor.java:172)
 at org.apache.helix.monitoring.mbeans.ClusterStatusMonitor.registerInstances(ClusterStatusMonitor.java:498)
 at org.apache.helix.monitoring.mbeans.ClusterStatusMonitor.setClusterInstanceStatus(ClusterStatusMonitor.java:220)
 at org.apache.helix.controller.stages.ReadClusterDataStage.process(ReadClusterDataStage.java:91)
 at org.apache.helix.controller.pipeline.Pipeline.handle(Pipeline.java:48)
 at org.apache.helix.controller.GenericHelixController.handleEvent(GenericHelixController.java:295)
 at org.apache.helix.controller.GenericHelixController$ClusterEventProcessor.run(GenericHelixController.java:595)


{code}
and
{code:java}
2021-03-09 23:15:53,450 INFO org.apache.helix.monitoring.mbeans.ClusterStatusMonitor: Register MBean: ClusterStatus: cluster=ambari-metrics-cluster,instanceName=ctr-e153-1613480641811-87570-01-000057.hwx.site_12001,resourceName=METRIC_AGGREGATORS
2021-03-09 23:15:53,450 INFO org.apache.helix.monitoring.mbeans.ClusterStatusMonitor: Register MBean: ClusterStatus: cluster=ambari-metrics-cluster,instanceName=ctr-e153-1613480641811-87570-01-000057.hwx.site_12001,resourceName=METRIC_AGGREGATORS
2021-03-09 23:15:53,450 WARN org.apache.helix.monitoring.mbeans.ClusterStatusMonitor: Could not register MBean: ClusterStatus: cluster=ambari-metrics-cluster,instanceName=ctr-e153-1613480641811-87570-01-000057.hwx.site_12001,resourceName=METRIC_AGGREGATORS
javax.management.InstanceAlreadyExistsException: ClusterStatus: cluster=ambari-metrics-cluster,instanceName=ctr-e153-1613480641811-87570-01-000057.hwx.site_12001,resourceName=METRIC_AGGREGATORS
        at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
        at org.apache.helix.monitoring.mbeans.ClusterStatusMonitor.register(ClusterStatusMonitor.java:172)
        at org.apache.helix.monitoring.mbeans.ClusterStatusMonitor.registerPerInstanceResources(ClusterStatusMonitor.java:537)
        at org.apache.helix.monitoring.mbeans.ClusterStatusMonitor.setPerInstanceResourceStatus(ClusterStatusMonitor.java:307)
        at org.apache.helix.controller.stages.BestPossibleStateCalcStage.process(BestPossibleStateCalcStage.java:74)
        at org.apache.helix.controller.pipeline.Pipeline.handle(Pipeline.java:48)
        at org.apache.helix.controller.GenericHelixController.handleEvent(GenericHelixController.java:295)
        at org.apache.helix.controller.GenericHelixController$ClusterEventProcessor.run(GenericHelixController.java:595)
{code}",HA helix metric-collector,['ambari-metrics'],AMBARI,Bug,Major,2021-03-16 11:38:08,0
13365239,"Verify custom queries with ""IN"" clause for ORA-01795 issue","Review all custom queries with ""IN clause"" if they are vulnerable by ORA-01795 issue. Approximate the possible max size of passed list in-to the query and in case of need apply wrapper to batch process the query.",oracle server sql,['ambari-server'],AMBARI,Task,Major,2021-03-15 13:10:42,0
13363468,For kerberos service check IN clause must be split into batches,"javax.persistence.PersistenceException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000Error Code: 1795
Call: SELECT kkp_id, host_id, is_distributed, keytab_path, principal_name FROM kerberos_keytab_principal WHERE (principal_name IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?))
        bind => [1083 parameters bound]
Query: ReadAllQuery(referenceClass=KerberosKeytabPrincipalEntity sql=""SELECT kkp_id, host_id, is_distributed, keytab_path, principal_name FROM kerberos_keytab_principal WHERE (principal_nam
e IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?))"")
        at org.eclipse.persistence.internal.jpa.QueryImpl.getDetailedException(QueryImpl.java:382)
        at org.eclipse.persistence.internal.jpa.QueryImpl.executeReadQuery(QueryImpl.java:260)
        at org.eclipse.persistence.internal.jpa.QueryImpl.getResultList(QueryImpl.java:473)
        at org.apache.ambari.server.orm.dao.KerberosKeytabPrincipalDAO.findByFilter(KerberosKeytabPrincipalDAO.java:251)
        at org.apache.ambari.server.orm.AmbariLocalSessionInterceptor.invoke(AmbariLocalSessionInterceptor.java:54)
        at org.apache.ambari.server.orm.dao.KerberosKeytabPrincipalDAO.findByFilters(KerberosKeytabPrincipalDAO.java:262)
        at org.apache.ambari.server.serveraction.kerberos.stageutils.KerberosKeytabController.getFilteredKeytabs(KerberosKeytabController.java:135)
        at org.apache.ambari.server.serveraction.kerberos.stageutils.KerberosKeytabController.getFilteredKeytabs(KerberosKeytabController.java:149)
        at org.apache.ambari.server.events.publishers.AgentCommandsPublisher$KerberosCommandParameterProcessor.process(AgentCommandsPublisher.java:250)
        at org.apache.ambari.server.events.publishers.AgentCommandsPublisher.injectKeytab(AgentCommandsPublisher.java:184)
        at org.apache.ambari.server.events.publishers.AgentCommandsPublisher.populateExecutionCommandsClusters(AgentCommandsPublisher.java:132)
        at org.apache.ambari.server.events.publishers.AgentCommandsPublisher.sendAgentCommand(AgentCommandsPublisher.java:92)
        at org.apache.ambari.server.actionmanager.ActionScheduler.doWork(ActionScheduler.java:557)
        at org.apache.ambari.server.actionmanager.ActionScheduler.run(ActionScheduler.java:347)
        at java.lang.Thread.run(Thread.java:748)",oracle server sql,['ambari-server'],AMBARI,Task,Major,2021-03-10 08:50:12,0
13363177,ORA-01795 error when querying hostcomponentdesiredstate table on large cluster,"Ambari server is not able to login because the server is querying Oracle DB which has more than 1000 entries.
{noformat}
bind => [1173 parameters bound]
 Query: ReadAllQuery(name=""HostComponentDesiredStateEntity.findByHostsAndCluster"" referenceClass=HostComponentDesiredStateEntity sql=""SELECT id, admin_state, blueprint_provisio
 ning_state, cluster_id, component_name, desired_state, host_id, maintenance_state, restart_required, service_name FROM hostcomponentdesiredstate WHERE ((host_id IN ?) AND (clu
 ster_id = ?))"")
 at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
 at ....
 ... 27 more
 Caused by: java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000{noformat}",ambari-server oracle,['ambari-server'],AMBARI,Task,Critical,2021-03-09 06:34:31,0
13362028,druid.emitter.ambari-metrics.hostname set improperly,"When installing Druid the `druid.emitter.ambari-metrics.hostname` property is set to the first character of the hostname of the AMS collector host even though the property in Ambari is set to `{{metric_collector_host}}`. 

/etc/druid/conf/_common/common.runtime.properties file contains:
{code:java}
druid.emitter=ambari-metrics
druid.emitter.ambari-metrics.eventConverter={""type"":""whiteList""}
druid.emitter.ambari-metrics.hostname=c
druid.emitter.ambari-metrics.port=6188
druid.emitter.ambari-metrics.protocol=https
druid.emitter.ambari-metrics.trustStorePassword=clientTrustStorePassword
druid.emitter.ambari-metrics.trustStorePath=/etc/security/clientKeys/all.jks
druid.emitter.ambari-metrics.trustStoreType=jks{code}",ambari-metrics druid stack,['stacks'],AMBARI,Task,Major,2021-03-03 07:13:15,0
13342410,"org.postgresql.util.PSQLException: FATAL: password authentication failed for user ""mapred""","Ambari server is continuously logging password authentication failed for user mapred error and ward messages. However there is no user mapred in ambari. The ambari database used is embedded postgresql.  Will running the ambari-server setup again fix the issue?

 
{code:java}
2020-11-24 06:37:46,933 ERROR [ambari-client-thread-1425637] Driver:263 - Connection error:
org.postgresql.util.PSQLException: FATAL: password authentication failed for user ""mapred""{code}
 

Ambari database check gives below Warning in log
{code:java}
2020-11-18 13:51:01,416 WARN - You have config(s): falcon-startup.properties-TOPOLOGY_RESOLVED,spark-defaults-version1524543611951,spark-metrics-properties-version1485951378211,falcon-atlas-application.properties-version1524574152922,spark-env-version1485951378211,falcon-startup.properties-INITIAL,spark-defaults-version1501747039754,falcon-log4j-INITIAL,hcat-env-version1570571662097,mahout-env-TOPOLOGY_RESOLVED,flume-env-INITIAL,hive-exec-log4j-version1570571662921,slider-log4j-TOPOLOGY_RESOLVED,slider-client-INITIAL,falcon-client.properties-INITIAL,falcon-startup.properties-version1524543610919,falcon-atlas-application.properties-version1524543611402,falcon-runtime.properties-TOPOLOGY_RESOLVED,mahout-log4j-TOPOLOGY_RESOLVED,spark-defaults-version1524574152551,falcon-env-INITIAL,ranger-site-version1570571657763,falcon-runtime.properties-INITIAL,spark-defaults-version1485951378211,livy-conf-version1485951378211,webhcat-site-version1570571662592,falcon-atlas-application.properties-version1492596818127,mahout-log4j-INITIAL,falcon-startup.properties-version1496929735878,falcon-log4j-TOPOLOGY_RESOLVED,spark-env-version1493710636755,spark-thrift-sparkconf-version1492596818303,livy-log4j-properties-version1485951378211,livy-conf-version1524543611603,falcon-env-version1524574151887,falcon-atlas-application.properties-INITIAL,flume-conf-TOPOLOGY_RESOLVED,spark-thrift-sparkconf-version1524574152356,mahout-env-INITIAL,spark-defaults-version1492596818576,flume-conf-INITIAL,livy-env-version1493710636755,falcon-startup.properties-version1524574151504,webhcat-env-version1570571663253,flume-env-TOPOLOGY_RESOLVED,falcon-env-TOPOLOGY_RESOLVED,slider-client-TOPOLOGY_RESOLVED,livy-env-version1485951378211,usersync-properties-version1570571659178,hive-log4j-version1570571663758,webhcat-log4j-version1570571661148,spark-thrift-fairscheduler-version1485951378211,falcon-atlas-application.properties-TOPOLOGY_RESOLVED,spark-thrift-sparkconf-version1524543611628,slider-log4j-INITIAL,spark-hive-site-override-version1485951378211,spark-log4j-properties-version1485951378211,slider-env-TOPOLOGY_RESOLVED,livy-spark-blacklist-version1485951378211,spark-javaopts-properties-version1485951378211,livy-conf-version1524574153115,livy-conf-version1492596818283,falcon-startup.properties-version1492596817779,slider-env-INITIAL,falcon-client.properties-TOPOLOGY_RESOLVED,spark-thrift-sparkconf-version1485951378211 that is(are) not mapped (in serviceconfigmapping table) to any service!{code}
 ",ambari-server,['ambari-server'],AMBARI,Bug,Minor,2020-11-24 16:18:34,1
13339953,Change the way AMS Grafana datasource discovers the Kafka topics ,"Currently the available Kafka topics are discovered by extracting them from the kafka.log.Log.*
 metrics in the AMS Datasource to show them on the Kafka-Topics dashboard. This means that if whitelisting is used the kafka.log.Log.* metrics must be enabled and must not be excluded by 'external.kafka.metrics.exclude.prefix' Kafka property either.

However, in some cases the large amount of kafka.log.Log.* metrics can be a burden to AMS, so excluding them would be welcomed.

The possibility to discover the Kafka topics should be assessed! ",ambari-metrics grafana,[],AMBARI,Task,Major,2020-11-11 10:23:03,1
13339525,Prototype pollution issue in JQuery,"Ambari 2.7.5 is using JQuery 1.9.1 which is vulnerable by [https://snyk.io/vuln/SNYK-JS-JQUERY-174006]

 ",security vulnerability,['ambari-web'],AMBARI,Bug,Major,2020-11-09 13:18:32,2
13337049,Ambari Metrics save as JSON/CSV use metricName instead of  default name.,"The name of the metrics data exported in Ambari-web is always data.json/csv, and the exported file must be renamed to distinguish which metric the data belongs to.",pull-request-available widget,[],AMBARI,Improvement,Major,2020-10-24 17:37:47,1
13334999,Reassess Ambari Metrics data migration,"The data migration process of Ambari Metrics as described at [https://docs.cloudera.com/HDPDocuments/Ambari-2.7.5.0/bk_ambari-upgrade-major/content/upgrading_HDP_post_upgrade_tasks.html]

is causing issues, like not migrating data that would be expected by the user. (e.g. Yarn Queue metrics other than the root queue's.)

The data migration is usually called by the

 
{code:java}
/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start /etc/ambari-metrics-collector/conf/metrics_whitelist ""31556952000""
{code}
command where the whitelist is specified.

The migration code only looks for the metrics that are present in the whitelist file. This is true even in the case when the AMS Whitelisting is not enabled. The user will only have those metrics migrated that are present in the whitelist file, which is usually not all that are required.

 

I suggest the following change:
 - If whitelist file parameter *is provided* then
 ** migrate only the metrics that are in the whitelist file
 - if *--allmetrics* value is provided in place of whitelist file parameter then

 * 
 ** migrate all metrics regardless of other configuration settings

 - if whitelist file parameter is *not provided* ( and the time period for data migration is also not provided) then
 ** if whitelisting is *enabled* then
 *** discover the whitelist file configured in AMS and migrate only the metrics that are in the whitelist file
 ** if whitelisting is *disabled* then
 *** migrate *all the metrics* present in the database

*Examples:*
 * {{*Migrate the metrics present in the whitelist file that are not older than one year (365 days)*}}
/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start /etc/ambari-metrics-collector/conf/metrics_whitelist ""365""
 * {{*Migrate the metrics present in the whitelist file that are not older than the default one month (30 days)*}}
{{/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start /etc/ambari-metrics-collector/conf/metrics_whitelist}}
 * {{*Migrate all metrics that are not older than one year (365 days)*}}
 {{/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start --allmetrics ""365""}}
 * {{*Migrate all metrics*}} *that are not older than the default one month (30 days)*
{{/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start --allmetrics}}
 * *If whitelisting is enabled then migrate the metrics present in the whitelist file configured in Ambari that are not older than the default one month (30 days). If whitelisting is disabled M**igrate all metrics that are not older than the default one month.*
/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start

 

*1. Introduce an '--allmetrics' to enforce migration of all metrics regardless of other settings.*
 Due to the suboptimal argument handling, if one wants to define an argument that comes after the 'whitelist file'
 argument - like the 'starttime' - the 'whitelist file' argument must be defined.
 But when we don't want to use the whitelist data because we need to migrate all the metrics the '--allmetrics' argument can be provided instead of 'whitelist file'.

Example: migrate all the metrics from the last year
 {{/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade_start --allmetrics ""365""}}

*2. The start time handling should be fixed and changed*
 * The code is intended to migrate data from the ""last x milliseconds"" as the handling of the default data shows where the startTime is subtracted from the current timestamp.
 {{public static final long DEFAULT_START_TIME = System.currentTimeMillis() - ONE_MONTH_MILLIS; //Last month}}
 But when the user externally provided the {{startTime}} value it was not subtracted from the current timestamp, but was used as it is, which is indeed erroneous.
 * Also, I suggest using days instead of milliseconds to define the required migration time window, because it is a more realistic and convenient granularity. Like in the above example the command will migrate data from the last 365 days.

*3. Furthermore, the migration process frequently dies silently while saving the metadata.*

The log message ""Saving metadata to store..."" is present in the logs but the ""Metadata was saved."" is mostly never there, but there are no other error messages. I suggest revising the current solution where the saving of the metadata is triggered in a Shutdown hook.",metric-collector migration pull-request-available,['ambari-metrics'],AMBARI,Task,Major,2020-10-12 12:07:59,0
13333740,Storm dashboards are not showing metrics,All the Storm dashboards are failing to show metric data.,grafana storm,['ambari-metrics'],AMBARI,Bug,Major,2020-10-05 14:59:25,0
13325899,Update Grafana version to 6.7.4 to avoid CVE-2020-13379,"Uplift grafana vesion to 6.7.4

Grafana Vulnerability CVE-2020-13379

[https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13379]

[https://grafana.com/blog/2020/06/03/grafana-6.7.4-and-7.0.2-released-with-important-security-fix/]

 ",grafana,['ambari-metrics'],AMBARI,Bug,Critical,2020-09-03 10:50:21,0
13319683,Failed to execute goal on project ambari-metrics-timelineservice,"Attempting to build Ambari with this command:
{code:java}
mvn -B clean install rpm:rpm -DnewVersion=2.7.5.0.0 -DbuildNumber=5895e4ed6b30a2da8a90fee2403b6cab91d19972 -DskipTests -Dpython.ver=""python >= 2.7""
{code}
Throws DependencyResolutionException:
{code:java}
[ERROR] Failed to execute goal on project ambari-metrics-timelineservice: Could not resolve dependencies for project org.apache.ambari:ambari-metrics-timelineservice:jar:2.7.5.0.0: The following artifacts could not be resolved: org.apache.phoenix:phoenix-core:jar:5.0.0.3.1.4.0-315, org.apache.hadoop:hadoop-common:jar:3.1.1.3.1.4.0-315, org.apache.hadoop:hadoop-annotations:jar:3.1.1.3.1.4.0-315, org.apache.hadoop:hadoop-common:jar:tests:3.1.1.3.1.4.0-315, org.apache.hadoop:hadoop-yarn-common:jar:tests:3.1.1.3.1.4.0-315, org.apache.hadoop:hadoop-yarn-common:jar:3.1.1.3.1.4.0-315, org.apache.hadoop:hadoop-yarn-api:jar:3.1.1.3.1.4.0-315, org.apache.hadoop:hadoop-yarn-server-common:jar:3.1.1.3.1.4.0-315, org.apache.phoenix:phoenix-core:jar:tests:5.0.0.3.1.4.0-315, org.apache.hbase:hbase-testing-util:jar:2.0.2.3.1.4.0-315: Failure to find org.apache.phoenix:phoenix-core:jar:5.0.0.3.1.4.0-315 in https://repo.hortonworks.com/content/groups/public/ was cached in the local repository, resolution will not be reattempted until the update interval of apache-hadoop has elapsed or updates are forced -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
{code}",build,['ambari-metrics'],AMBARI,Bug,Blocker,2020-07-28 06:52:53,1
13297978,Hive Service Check Fails if the trustStorePassword has Special character $,"Problem statement : Hive Service Check Fails if the trustStorePassword has Special character $


the error logs say : 

{code:java}
2020-04-10 08:20:14,748 - Execute['! (beeline -u 'jdbc:hive2://c4229-node3.coelab.cloudera.com:10000/;transportMode=binary;ssl=true;sslTrustStore=/tmp/keystore1.jks;trustStorePassword=[PROTECTED]' -n hive -e ';' 2>&1 | awk '{print}' | grep -vz -i -e 'Connected to:' -e 'Transaction isolation:' -e 'inactive HS2 instance; use service discovery')'] {'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'], 'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'}
2020-04-10 08:20:19,509 - Connection to c4229-node3.coelab.cloudera.com on port 10000 failed
2020-04-10 08:20:24,513 - Execute['! (beeline -u 'jdbc:hive2://c4229-node3.coelab.cloudera.com:10000/;transportMode=binary;ssl=true;sslTrustStore=/tmp/keystore1.jks;trustStorePassword=[PROTECTED]' -n hive -e ';' 2>&1 | awk '{print}' | grep -vz -i -e 'Connected to:' -e 'Transaction isolation:' -e 'inactive HS2 instance; use service discovery')'] {'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'], 'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'}
2020-04-10 08:20:28,927 - Connection to c4229-node3.coelab.cloudera.com on port 10000 failed
{code}


Root cause : the trustStorePassword should idaelly be passed in single quotes to ignore the $ sign, the fix lies in : https://github.com/apache/ambari/blob/a4b2901a9a16e356c230fb647471e099b2d965ba/ambari-common/src/main/python/resource_management/libraries/functions/hive_check.py#L61

changing 

{code:java}
  beeline_url.extend(['ssl={ssl_str}', 'sslTrustStore={ssl_keystore}', 'trustStorePassword={ssl_password!p}'])
{code}

to : 


{code:java}
  beeline_url.extend(['ssl={ssl_str}', 'sslTrustStore={ssl_keystore}', ""trustStorePassword='{ssl_password!p}'""])
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2020-04-13 08:31:30,3
13291954,Extend the set of headers from server's response,Extend the set of headers from server's response.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2020-03-16 11:57:29,1
13289724,Ambari Sends Empty metrics_collector_hosts parameters after AMS service is deleted,"I am having an cluster where Ambari metrics collector was installed and removed later as they dont use it . Now there Nifi fails to start due to script error : 

{code:java}
----
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/NIFI/1.0.0/package/scripts/nifi.py"", line 304, in <module>
    Master().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/NIFI/1.0.0/package/scripts/nifi.py"", line 139, in start
    import params
  File ""/var/lib/ambari-agent/cache/common-services/NIFI/1.0.0/package/scripts/params.py"", line 302, in <module>
    metrics_collector_host = str(config['clusterHostInfo']['metrics_collector_hosts'][0])
IndexError: list index out of range
----
{code}

Issue is easy to reproduce

1) Deploy a fresh cluster with HDF
2) Remove AMS service 
3) try to start/stop or restart nifi service. it will fail with abouve error.


",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2020-03-05 08:06:40,3
13287721,Change authentication method from get to post,"Currently we are using GET method while login:
GET
api/v1/users/$

{username}

We need to change it to post.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2020-02-26 10:58:15,1
13286242,Badly formatted request may cause Ambari to crash,"Metrics request with only start timestamp can lead to throwing of out of memory exception:
curl -g -u admin:admin 'http://<hostName>:8080/api/v1/clusters/<clusterName>/?fields=metrics/load/1-min._avg[15]'",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2020-02-19 13:49:05,1
13283844,Upgrade Infra Solr Clients fails if it is installed on only one host,"Executed command:
{code:java}
[root@santal-atlas-dc-upgr ~]# /usr/lib/ambari-infra-solr-client/ambariSolrMigration.sh --ini-file $CONFIG_INI_LOCATION --mode backup | tee backup_output.txt Execute command: /usr/bin/python /usr/lib/ambari-infra-solr-client/migrationHelper.py --ini-file ambari_solr_migration.ini --action check-shards Dumping collections data to /usr/lib/ambari-infra-solr-client/migrate/data/check_collections.json ... DONE Checking available shards for 'fulltext_index' collection... ------------------------------ Number of shards: 1 OK: Found active leader replica for shard1 Index size per shard for fulltext_index: - shard1: 71 bytes Index size per host for fulltext_index (consider this for backup): - 172.22.111.200: 71 bytes ------------------------------ Checking available shards for 'edge_index' collection... ------------------------------ Number of shards: 1 OK: Found active leader replica for shard1 Index size per shard for edge_index: - shard1: 71 bytes Index size per host for edge_index (consider this for backup): - 172.22.111.200: 71 bytes ------------------------------ Checking available shards for 'vertex_index' collection... ------------------------------ Number of shards: 1 OK: Found active leader replica for shard1 Index size per shard for vertex_index: - shard1: 16.8 KB Index size per host for vertex_index (consider this for backup): - 172.22.111.200: 16.8 KB ------------------------------ Full index size per hosts: (consider this for backup) - 172.22.111.200: 16.94 KB All warnings: 0 Check shards - PASSED ------------------------------ Command elapsed time: 00:00:03 ------------------------------ Migration helper command FINISHED Execute command: /usr/bin/python /usr/lib/ambari-infra-solr-client/migrationHelper.py --ini-file ambari_solr_migration.ini --action upgrade-solr-clients Sending upgrade request: [Upgrade Solr Clients] Traceback (most recent call last): File ""/usr/lib/ambari-infra-solr-client/migrationHelper.py"", line 2002, in <module> upgrade_solr_clients(options, accessor, parser, config) File ""/usr/lib/ambari-infra-solr-client/migrationHelper.py"", line 517, in upgrade_solr_clients response = post_json(accessor, CLUSTERS_URL.format(cluster) + REQUESTS_API_URL, cmd_request) File ""/usr/lib/ambari-infra-solr-client/migrationHelper.py"", line 250, in post_json response = accessor(url, 'POST', json.dumps(request_body)) File ""/usr/lib/ambari-infra-solr-client/migrationHelper.py"", line 117, in do_request raise Exception('Problem with accessing api. Reason: {0}'.format(exc)) Exception: Problem with accessing api. Reason: HTTP Error 500: Internal Server Error Total Runtime: 00:00:03 Upgrade Solr Clients command FAILED. Stop migration commands ...

{code}
ambari-server.log:

 
{code:java}
Caused by: org.apache.ambari.server.ServiceComponentHostNotFoundException: ServiceComponentHost not found, clusterName=atlas_upgr_cl, serviceName=AMBARI_INFRA_SOLR, serviceComponentName=INFRA_SOLR_CLIENT, hostName=
        at org.apache.ambari.server.state.ServiceComponentImpl.getServiceComponentHost(ServiceComponentImpl.java:346)
        at org.apache.ambari.server.controller.MaintenanceStateHelper.isOperationAllowed(MaintenanceStateHelper.java:111)
        at org.apache.ambari.server.controller.MaintenanceStateHelper.isOperationAllowed(MaintenanceStateHelper.java:87)
        at org.apache.ambari.server.controller.AmbariCustomCommandExecutionHelper$1.shouldHostBeRemoved(AmbariCustomCommandExecutionHelper.java:287)
        at org.apache.ambari.server.controller.MaintenanceStateHelper.filterHostsInMaintenanceState(MaintenanceStateHelper.java:423)
        at org.apache.ambari.server.controller.AmbariCustomCommandExecutionHelper.addCustomCommandAction(AmbariCustomCommandExecutionHelper.java:278)
        at org.apache.ambari.server.controller.AmbariCustomCommandExecutionHelper.addExecutionCommandsToStage(AmbariCustomCommandExecutionHelper.java:1187)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createAction(AmbariManagementControllerImpl.java:4140)
        at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:283)
        at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:212)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:288)
{code}

The solr client update request must be terminated if no other host can be upgraded.
",pull-request-available,['ambari-infra'],AMBARI,Bug,Minor,2020-02-07 06:45:52,4
13282971,After node reboot autostart of components takes too much time.,"The reason for this is because ambari-agent does not know start order. And so
if INFRA_SOLR with timeout of 10 hours starts before ZOOKEEPER, which it
requires, it’s a big problem. As well as it blocks agent queue for any other
commands. The solution is to run the commands in parallel. Also for fast
autostart users might want to disable retry_gap: {noformat}/var/lib/ambari-
server/resources/scripts/configs.py -a set -l localhost -n cluster_name -u
admin -p admin -c cluster-env -k retry_gap_in_sec -v 0{noformat}

",pull-request-available,[],AMBARI,Bug,Major,2020-02-03 12:16:34,5
13282363,Bad UTF encoding on Alert listener receiver,"The problem observed on gce cluster during testing of Ambari Kerberos slowness
{code:java}
2020-01-29 09:42:37,550 ERROR [alert-event-bus-2] AmbariJpaLocalTxnInterceptor:188 - [DETAILED ERROR] Internal exception (1) :
org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding ""UTF8"": 0x00
        at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2433)
        at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2178)
        at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:306)
        at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
        at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
        at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:155)
        at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:132)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:892)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:964)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:633)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1845)
        at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4300)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5592)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
        at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:285)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
        at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:134)
        at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:153)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)
        at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)
        at org.apache.ambari.server.events.listeners.alerts.AlertReceivedListener$$EnhancerByGuice$$c6d5f173.saveEntities(<generated>)
        at org.apache.ambari.server.events.listeners.alerts.AlertReceivedListener.onAlertEvent(AlertReceivedListener.java:388)
        at org.apache.ambari.server.events.listeners.alerts.AlertReceivedListener$$EnhancerByGuice$$c6d5f173.CGLIB$onAlertEvent$0(<generated>)
        at org.apache.ambari.server.events.listeners.alerts.AlertReceivedListener$$EnhancerByGuice$$c6d5f173$$FastClassByGuice$$3f418344.invoke(<generated>)
        at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)
        at org.apache.ambari.server.orm.AmbariLocalSessionInterceptor.invoke(AmbariLocalSessionInterceptor.java:44)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)
        at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)
        at org.apache.ambari.server.events.listeners.alerts.AlertReceivedListener$$EnhancerByGuice$$c6d5f173.onAlertEvent(<generated>)
        at sun.reflect.GeneratedMethodAccessor718.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)
        at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2020-01-29 09:42:37,556 ERROR [alert-event-bus-2] default:232 - Exception thrown by subscriber method onAlertEvent(org.apache.ambari.server.events.AlertReceivedEvent) on subscriber org.apache.ambari.server.events.listeners.alerts.AlertReceivedListener$$EnhancerByGuice$$c6d5f173@29ba63f0 when dispatching event: AlertReceivedEvent{cluserId=0, alerts=[{clusterId=2, state=CRITICAL, name=hive_server_process, service=HIVE, component=HIVE_SERVER, host=<hostName>, instance=null, text='Connection failed on host <hostName>:10000 (Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/alerts/alert_hive_thrift_port.py"", line 213, in execute
    ldap_password=ldap_password, pam_username=pam_username, pam_password=pam_password)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/hive_check.py"", line 95, in check_thrift_port_sasl
    timeout_kill_strategy=TerminateStrategy.KILL_PROCESS_TREE,
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
ExecutionFailed: Execution of '! (beeline -u 'jdbc:hive2://<hostName>:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM' -n hive -e ';' 2>&1 | awk '{print}' | grep -vz -i -e 'Connected to:' -e 'Transaction isolation:' -e 'inactive HS2 instance; use service discovery')' returned 1. SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.5.0-152/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.5.0-152/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://<hostName>:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM
20/01/29 09:42:34 [main]: WARN jdbc.HiveConnection: Failed to connect to <hostName>:10000
Could not open connection to the HS2 server. Please check the server URI and if the URI is correct, then ask the administrator to check the server status.
Error: Could not open client transport with JDBC Uri: jdbc:hive2://<hostName>:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)

)'}]}
javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding ""UTF8"": 0x00
Error Code: 0{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2020-01-30 11:54:54,1
13282193,Postgresql service naming convention changed on SUSE 12 SP2,"Ambari server setup is failing to succeed when being initiated on Suse 12 SP2 operating system. 
The new installation of pgsql 9.6 creates ""/usr/lib/systemd/system/postgresql-9.6.service"" instead of ""/usr/lib/systemd/system/postgresql.service"". 
This was handled for Redhat previously but due to this [wiki|https://wiki.postgresql.org/wiki/Zypper_Installation] site the same fix is needed on SLES 12 as well.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2020-01-29 13:58:00,4
13281670,Components autostart does not work sometimes after ambari-agent restart,"If configurations where cached and didn't change during restart of agent (for some people they always change, they will not see the issue). Recovery didn't get enabled. Until some changes configs/topology changes where done.",pull-request-available,[],AMBARI,Bug,Major,2020-01-27 09:41:53,5
13280160,Ambari doesn't show versions page after invalid repo was added,"Steps to reproduce:
 # Deploy cluster.
 # Add repository version with ""****:****"" as credentials (set skip validation before saving).
 # Try to open versions page.

ambari-server log:
{code:java}
2020-01-18 00:45:14,915 ERROR [ambari-client-thread-388] ReadHandler:99 - Bad request:
 java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
 *****:*****
 ^
     at java.util.regex.Pattern.error(Pattern.java:1955)
     at java.util.regex.Pattern.sequence(Pattern.java:2123)
     at java.util.regex.Pattern.expr(Pattern.java:1996)
     at java.util.regex.Pattern.compile(Pattern.java:1696)
     at java.util.regex.Pattern.<init>(Pattern.java:1351)
     at java.util.regex.Pattern.compile(Pattern.java:1028)
     at java.lang.String.replaceFirst(String.java:2178)
     at org.apache.ambari.server.utils.URLCredentialsHider.hideCredentials(URLCredentialsHider.java:48)
     at org.apache.ambari.server.controller.internal.RepositoryResourceProvider.getResources(RepositoryResourceProvider.java:182)
     at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java: 965)
     at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:139)
     at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:529)
     at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:482)
     at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:503)
     at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:503)
     at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:454)
     at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:222)
     at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:77)
     at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
     at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:164)
     at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:128)
     at org.apache.ambari.server.api.services.ClusterStackVersionService.getClusterStackVersions(ClusterStackVersionService.java:68)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2020-01-17 14:57:20,6
13279623,Hive 3 Grafana dashboards showing outdated metrics ,"Some of the metrics' name has been changed in Hive 3. Due to this change many graphs shows no data on Hive Home, HiveServer2 and HiveMetastore dashboards.

Suggested changes: 

+HiveSever2+

The _default.General.api_get_all_databases_ and _default.General.api_get_partitions_by_names_ metrics are not provided by HiveServer2 anymore - only by HiveMetastore - so the ""API Times"" row with the two charts has been removed.

+Hive Metastore+
|*Original Name*| *New Name*|
|default.General.api_get_all_databases_75thpercentile|default.General.api_get_databases_75thpercentile|
|default.General.api_get_all_databases_999thpercentile|default.General.api_get_databases_999thpercentile|
|default.General.memory.heap.max|default.General.heap.max|
|default.General.memory.heap.used|default.General.heap.used|
|default.General.memory.heap.committed|default.General.heap.committed|
|default.General.memory.non-heap.max|default.General.non-heap.max|
|default.General.memory.non-heap.used|default.General.non-heap.used|
|default.General.memory.non-heap.committed|default.General.non-heap.committed|

 

+Hive Home+

 
| *Original Name*| *New Name*|
|default.General.init_total_count_db|default.General.total_count_dbs|
|default.General.init_total_count_tables|default.General.total_count_tables|
|default.General.init_total_count_partitions|default.General.total_count_partitions|
|default.General.api_create_table_count|api_create_table_req_count|
|default.General.memory.heap.max|default.General.heap.max|
|default.General.memory.heap.used|default.General.heap.used|
|default.General.memory.heap.committed|default.General.heap.committed|
|default.General.memory.non-heap.max|default.General.non-heap.max|",grafana hive metrics pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2020-01-15 16:06:56,0
13277302,API exception on trying to assign permission to user group with custom Ambari Views,"Issue appears when logic trying to fetch allowed statuses for the request scope. The problem is that views are pushed to the request scope and have no allowance to have Ambari admin permissions.  

Here are several ways to solve the problem:
- do not include views for such kind of the request to the scope
- instead of failing, give a warning to the log, and if there are no objects left in the scope, show to the user last warning in return

The less intrusive way is to done it in a second way as first way may change the logic of how it currently work",pull-request-available,"['ambari-server', 'ambari-views']",AMBARI,Bug,Major,2020-01-03 03:19:35,1
13274731,[ubuntu16] HDP install failed for upgrade from  HDP-3.0.1.0-187 to HDP-3.1.5.0-139,"Package Manager failed to install packages: No package found for
sqoop-$\\{stack_version}(expected name: sqoop-3-1-5-0-139) Traceback (most
recent call last): File ""/var/lib/ambari-
agent/cache/custom_actions/scripts/install_packages.py"", line 493, in
InstallPackages().execute() File ""/usr/lib/ambari-
agent/lib/resource_management/libraries/script/script.py"", line 352, in
execute method(env) File ""/var/lib/ambari-
agent/cache/custom_actions/scripts/install_packages.py"", line 156, in
actionexecute raise Fail(""Failed to distribute repositories/install packages"")
resource_management.core.exceptions.Fail: Failed to distribute
repositories/install packages Live Cluster :
[+http://172.27.120.193:8080/+|http://172.27.120.193:8080/] ambari-server
version : 2.7.5.0-67

",pull-request-available,[],AMBARI,Bug,Major,2019-12-16 10:49:10,5
13274644,Bulk DELETE API is broken,"When calling the bulk delete API, for example 

 
 {{curl -k -u admin:admin -H }}{{""X-Requested-By: ambari""}} {{-X DELETE}}
 {{""$HTTP_PROTOCOL://$CONSOLE_NODE:$PORT/api/v1/clusters/$CLUSTER/hosts/$hostName/host_components/""}}

{{the response we get is:}}

{ ""status"" : 400, ""message"" : ""Invalid Request: RequestStatusDetails is not of type DeleteStatusDetails"" }

{{The expected result is something like described at }}
 {{[https://cwiki.apache.org/confluence/display/AMBARI/Bulk+delete+components+on+multiple+hosts]}}",patch-available pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-12-15 17:14:34,1
13274257,Mask credentials during install step,This ticket is about masking out credentials (*****:*****) in the deploy phase,pull-request-available,['ambari-web'],AMBARI,Task,Blocker,2019-12-12 22:46:57,7
13273864,Credentials should not be shown on cleartext on Ambari UI,"Please see screenshot attached. Ambari UI - Stack and Versions page shows
username and password in cleartext . We should atleast hide the password Also
in Review page for UI install cc [~accountid:557058:af856db7-a0ad-
4e2b-b848-f11f481bf96f] / [~accountid:5dc59258b6e6b50c58af136b] /
[~accountid:557058:e797d548-8a74-4f63-a68d-616111201b1c]

",pull-request-available,[],AMBARI,Bug,Major,2019-12-11 12:16:50,5
13273682,VDF registration fails with SunCertPathBuilderException: unable to find valid certification path to requested target' on HTTPS cluster ,"Retrying deploy with latest build after BUG-122455 was fixed Here Stack
registration is failing with {code} An internal system exception occurred:
Could not load url from https://1055c7c3-1b7b-
43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-test.cloudera.com/p/HDP-
GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml.
sun.security.validator.ValidatorException: PKIX path building failed:
sun.security.provider.certpath.SunCertPathBuilderException: unable to find
valid certification path to requested target {code} 172.27.74.1 is a live node
where issue has occurred.

",pull-request-available,[],AMBARI,Bug,Major,2019-12-10 18:24:38,5
13273376,Deploy fails with 401:Unauthorized on HDP-GPL,"We are trying to deploy with paywalled repos of HDP and Ambari (ones supplied
in RELENG-7654) but deploy is failing at client install with {code}
RuntimeError: Failed to execute command '/usr/bin/yum -y install hdp-select',
exited with code '1', message: 'https://****:****@archive-test.cloudera.com/p
/HDP-GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml: [Errno 14] HTTPS
Error 401 - Unauthorized {code} This url is accessible with credentials RE has
supplied ie; https://1055c7c3-1b7b-
43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-test.cloudera.com/p/HDP-
GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml So not sure which
credential is Ambari picking as from the error it looks like the credentials
Ambari is using on this repo is incorrect. Could you please take a look Live
cluster : http://172.27.136.132:8080/ Repo urls used here for deploy: Ambari :
https://1055c7c3-1b7b-43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-
test.cloudera.com/p/ambari/2.7.5.0-64/centos7/ambaribn.repo HDP:
https://1055c7c3-1b7b-43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-
test.cloudera.com/p/HDP/centos7/3.x/BUILDS/3.1.5.0-139 cc
[~accountid:557058:af856db7-a0ad-
4e2b-b848-f11f481bf96f]/[~accountid:5dc59258b6e6b50c58af136b]

",pull-request-available,[],AMBARI,Bug,Major,2019-12-09 13:21:41,5
13273264,Create principal/keytab operation with multi threaded,"Ambari Principal and keytabs create operation runs in sequential manner, which taking a lot of time on a large cluster.

To cut the time, each step should be palatalized between cluster nodes.",pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2019-12-09 07:15:56,1
13271948,HDP-GPL field is not available in version registration page but present in edit page,New version registration page does not have HDP-GPL field. But this field is present when we are editing the version urls after registering.,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2019-12-03 13:21:23,7
13271936,Server sets blueprint_provisioning_state for component to finished before start command execution,"Server sends host level parameters update with blueprint_provisioning_state=""FINISHED"" right after start execution command generation. This may led to performing of autostart command on the agent side before execution of start command.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-12-03 12:40:00,6
13271922,XSS vulnerability for repo check hint,For now UI parses repo error hint as html. It is potential XSS vulnerability.,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2019-12-03 11:27:19,7
13271431,Unable to uncheck 'hidden' checkbox in ambari stackVersions page,If version is hidden - we need always allow to make it visible.,pull-request-available,['ambari-admin'],AMBARI,Task,Blocker,2019-11-29 14:49:48,7
13271267,No Validation error on UI for an 'Unauthorized' repo url,UI should show error for URLs if credentials are required but missed there.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-11-28 17:12:06,6
13271187,Ambari should add login and password to urls populated from VDF,"This was pointed out in https://cloudera.slack.com/archives/CQYUPAV7A chat and
we decided that it would be un-secure for RelEng to store login/password
inside url's in VDF's, hence ambari should add that to url's itself.

",pull-request-available,[],AMBARI,Bug,Major,2019-11-28 11:03:40,5
13271181,Ambari should add login and password to urls populated from VDF,"This was pointed out in https://cloudera.slack.com/archives/CQYUPAV7A chat and
we decided that it would be un-secure for RelEng to store login/password
inside url's in VDF's, hence ambari should add that to url's itself.

",pull-request-available,[],AMBARI,Bug,Major,2019-11-28 10:36:47,5
13270933,Error while Validating Coordinator xml in Workflow Manager View,"Description : 

there is an Javascript code when following operation is performed due to which there is an Error message which is unnecessary

*steps to reproduce*

*  Open WFM view
*  Create Coordinator
* Select xml file  & save
* Try Validate
* As soon as we open validate window it is giving error if we click on validate its validating though

The error Screen shot attached :  !Screenshot 2019-11-27 at 3.10.16 PM.png! 


*The error in javascript console : *

{code:java}
vendor-03ed9177dfebfe05111123a6e14f3a1c.js:13 Error: TypeError: Cannot read property '_name' of undefined
    at r.<anonymous> (oozie-designer-431d8215c58dd06e3b4e78162c4b0483.js:formatted:1422)
    at y (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:19)
    at b (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:19)
    at g (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:19)
    at h (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:19)
    at vendor-03ed9177dfebfe05111123a6e14f3a1c.js:13
    at invoke (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:5)
    at r.flush (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:5)
    at n.flush (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:5)
    at a.end (vendor-03ed9177dfebfe05111123a6e14f3a1c.js:6)
{code}
",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2019-11-27 09:42:28,3
13270786,Recommendations API error during cluster creation wizard,"Ambari Cluster wizard step #7 shows an error. The recommendations API returns with [400] BadRequest as no clusterId is provided in the http request body.
UI is sending wrong request.
Same issue exists on kerberos wizard ",pull-request-available,['ambari-web'],AMBARI,Task,Blocker,2019-11-26 18:45:49,7
13270682,Backport AMBARI-23256 to branch-2.6,Need to backport AMBARI-23256 fix in 2.6.2 release,pull-request-available,['ambari-agent'],AMBARI,Task,Major,2019-11-26 10:32:05,4
13269567,Cannot add or remove columns to logsearch log tables,It is not possible to add or remove columns to tables in either the service logs or audit logs.,logsearch pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2019-11-20 10:43:03,4
13269559,Add option to disable URL credential auto-update behaviour,"This is to enable customers using different auth credentials for different repos (mixed OS). The use-case currently unlikely, it is only to prevent later patches around this behaviour.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-11-20 10:23:26,7
13269313,Cannot select any configuration in logsearch configuration editor,"In the logsearch configuration editor it is not possible add or select another configuration. It is also not possible to navigate back the service log page.

The following error appears in the dev console.
{code:java}
ERROR Error: Uncaught (in promise): TypeError: e.find is not a function
TypeError: e.find is not a function
at main.bundle.js:1
at Array.forEach (<anonymous>)
at t.set as selection (main.bundle.js:1)
at t.writeValue (main.bundle.js:1)
at t._next (main.bundle.js:1)
at t.__tryOrSetError (vendor.bundle.js:999)
at t.next (vendor.bundle.js:999)
at t._next (vendor.bundle.js:999)
at t.next (vendor.bundle.js:999)
at t._emitFinal (vendor.bundle.js:999)
at l (polyfills.bundle.js:43)
at polyfills.bundle.js:43
at e.invokeTask (polyfills.bundle.js:36)
at Object.onInvokeTask (vendor.bundle.js:443)
at e.invokeTask (polyfills.bundle.js:36)
at t.runTask (polyfills.bundle.js:36)
at r (polyfills.bundle.js:36)
at o.invokeTask as invoke (polyfills.bundle.js:36)
at j (polyfills.bundle.js:8)
at XMLHttpRequest._ (polyfills.bundle.js:8){code}
Occasionally, after refreshing the page the error didn’t appear and everything worked fine.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Blocker,2019-11-19 13:09:07,4
13268570,AMS - metadata table has incorrect primary key,"The metrics metadata table has incorrect PK, which causes some metadata information to be overridden by false metrics that appear due to some other issue",pull-request-available,['ambari-metrics'],AMBARI,Task,Critical,2019-11-15 13:02:48,8
13268296,Configure heartbeat timeout,Make heartbeat timeout configurable to prevent it loosing on slow or overloaded instances.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-11-14 12:29:57,6
13268267,Ambari is changing the truststore permission from 444/644 to 640.,"When running ambari-server setup-security and choosing '[1]  Enable HTTPS for Ambari server.' we give the following information:
Do you want to disable HTTPS [y/n] (n)? n
SSL port [8080] ? 8080
Enter path to Certificate: <Certificate File>
Enter path to Private Key: <Key File>
Please enter password for Private Key: <empty>
Generating random password for HTTPS keystore...done.
Importing and saving Certificate...done.

Thereafter Unix permission of the systemwide Java truststore 
/var/lib/ca-certificates/java-cacerts are changed from mode 444 to 640.
In consequence Applications do not start anymore because the truststore is not world readable. It's creating impact on applications which is run by other users.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-11-14 10:01:30,4
13267689,Remove all tags from logger service methods in ambari-contrib,Supporting tags inject was removed in AMBARI-25384 due to XSS. We need to stop to using tags inside logger service,pull-request-available,['ambari-web'],AMBARI,Task,Blocker,2019-11-12 12:14:31,7
13267160,Cannot use HTTPS repourl and VDF url,"Using VDF https://<user>:<pass>@<pathToXML>
results in:
{noformat}
An internal system exception occurred: Could not load url from https://<user>:<pass>@<pathToXML>. Can't get secure connection to https://****:****@<pathToXML>. Truststore path or password is not set.{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-11-08 18:07:59,6
13265958,Add autocomplete for all repos url for login and password,"When user put his credentials for first or any links we need automatically to add them in every link field.
 If he will edit these credentials in any of existing fields - it should be updated everywhere
 We need to do this in ambari wizard and ambari-admin",pull-request-available,"['ambari-admin', 'ambari-web']",AMBARI,Task,Major,2019-11-03 15:13:56,7
13265790,Support basic auth for repositories,"User should be able to provide credentials for basic authorization inside repo base URL:

{code}

http://user:password@<host>:<port>/...

{code}

Also server should mask the credentials in URLs from API responses and logs.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2019-11-01 16:13:56,6
13265318,Upgrade Infra Solr to 7.7.2,upgrade Solr to v7.7.2 to bring in the fix for SOLR-13349.,pull-request-available,['ambari-infra'],AMBARI,Task,Major,2019-10-30 14:43:44,4
13264972,Ambari Management Pack: Ambari throws 500 error while downloading OneFS client configuration,"When attemping to download OneFS client configuration, Ambari throws 500 error with below exception:
{code:java}
org.apache.ambari.server.controller.spi.SystemException: 
Execution of ""ambari-python-wrap /var/lib/ambari-server/resources/stacks/HDP/3.0/services/ONEFS/package/scripts/onefs_client.py generate_configs 
/var/lib/ambari-server/data/tmp/ONEFS_CLIENT7933335097995561813-configuration.json /var/lib/ambari-server/resources/stacks/HDP/3.0/services/ONEFS/package 
/var/lib/ambari-server/data/tmp/structured-out.json INFO /var/lib/ambari-server/data/tmp"" returned 1. java.lang.Throwable: ambari_jinja2.exceptions.UndefinedError: 'java_version' is undefined
{code}
Reproduction steps:

1. Log into Amabri UI
 2. Click on ""Services"" --> ""Download All Client Configs"" 
 OR
 3. Click on ""Service"" --> ""OneFS"" --> ""Actions"" --> ""Download Client Configs""",pull-request-available,['contrib'],AMBARI,Bug,Major,2019-10-29 07:22:11,4
13264496,Please provide jvm metrics for kafka components in Ambari,Currently kakfa jvm metrics are not collected,pull-request-available,['ambari-metrics'],AMBARI,New Feature,Major,2019-10-25 15:20:56,4
13264124,Issue while determining live collector in case of HA,"If collector throws http://collectorhost:port/{color:#032f62}/ws/v1/timeline/metrics/livenodes{color} 500 error then sink is unable to determine live/healthy collector.

 

sink will try to connect to another collector only if it is not reachable/IOException to first collector.  if any other response code other than 200 then still it should consider as 1st Collector not reachable.

 

[https://github.com/apache/ambari/blob/release-2.7.4/ambari-metrics/ambari-metrics-common/src/main/java/org/apache/hadoop/metrics2/sink/timeline/AbstractTimelineMetricsSink.java#L629]

 

 Possible fix

 
{code:java}
      if (responseCode == 200) {
        try (InputStream in = connection.getInputStream()) {
          StringWriter writer = new StringWriter();
          IOUtils.copy(in, writer);
          try {
            collectors = gson.fromJson(writer.toString(), new TypeToken<List<String>>(){}.getType());
          } catch (JsonSyntaxException jse) {
            // Swallow this at the behest of still trying to POST
            LOG.debug(""Exception deserializing the json data on live "" +
              ""collector nodes."", jse);
          }
        }
      } else if (responseCode == 500){
        String warnMsg = ""Unable to connect to collector to find live nodes, Internal server error"";
        throw new MetricCollectorUnavailableException(warnMsg);
      }
{code}
 ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2019-10-23 21:48:11,2
13263986,Add hive PAM support for service check and alerts,Need to add PAM authentication support for hive service check and alerts.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-10-23 09:33:39,8
13263912,Textareas in configuration page can be resized beyond its container border limit,"The resizable textareas located in service configuration page (even in installization wizard) can be resized to a improper size, with pictures shown in the attachments. !Annotation 2019-10-22 102253.png!
",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Trivial,2019-10-23 01:18:29,1
13262846,Upgrading ambari-logsearch-logfeeder to 2.7.4 rpm gives warnings,"Customer is upgrading Ambari Logsearch to 2.7.4, while upgrading the rpm it gives below errors/warnings
{code:java}
ln: failed to create symbolic link '/usr/bin/logfeeder': File exists
rm: cannot remove '/etc/ambari-logsearch-logfeeder': Is a directory
warning: %postun(ambari-logsearch-logfeeder-2.7.4.*noarch) scriptlet failed, exit status 1
{code}
 - The issue can be reproduced in local environment
 - Also the symlink for file ""/usr/bin/logfeeder"" does not get created since Ambari 2.7.4
In Ambari 2.7.3 the symlink is created",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2019-10-17 12:16:58,4
13262628,cross-site scripting vulnerability on Ambari hosts,Fix xss vulnerability in ssh-key name and value in wizards.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-10-16 13:24:47,9
13262598,Update help text in Hive install provide a clearly formatted example,"Update the help text displayed in Ambari during the Hive service install so that there is a clearly labeled/formatted example of the download and/or an accurate JAR filename example. The text as displayed presents com.mysql.jdbc.driver as if it was a JAR file name, which may confuse some users. (screenshot attached)

Provide a monospaced example ambari-server-setup command, such as in the documentation here:

ambari-server setup --jdbc-db=mysql --jdbc-driver=/path/to/mysql/mysql-connector-java.jar",pull-request-available,[],AMBARI,Task,Major,2019-10-16 10:19:47,7
13261575,Ambari Metrics whitelisting is failing on * wildcard for HBase Tables,"On Ambari 2.6.x and 2.7.x if the Ambari Metrics Collector whitelisting is enabled, nothing appears in the Grafana HBase - Tables/Users dashboards.

The related metric-patterns are enabled in the whitelist file but it seems that the metrics matching with these patterns are filtered out and not shown on Grafana dashboard.

eg:
{code:java}
regionserver.Tables.*_metric_storeCount
regionserver.Tables.*_metric_storeFileCount
regionserver.Tables.*_metric_storeFileSize
regionserver.Tables.*_metric_tableSize
regionserver.Tables.*_metric_totalRequestCount
regionserver.Tables.*_metric_writeRequestCount
regionserver.Users.*_metric_append_num_ops
regionserver.Users.*_metric_delete_num_ops{code}
To fix this the ._p_ prefix should be ​​added to the above prefixes.",grafana hbase metrics pull-request-available,['ambari-metrics'],AMBARI,Task,Major,2019-10-10 13:47:05,4
13261516,Not able to register new HDP version in Ambari-2.7.4 without GPL with Local Repository,"We are not able to register the new HDP version in Ambari 2.7.4 without GPL with the LOCAL Repository. 

This issue is related to local repository validation with Ambari. We can skip the validation check and register the version.  Or else we can enable the GPL and successfully register the version without ""skip the validation check"".

If we have disabled the GPL then we will get this issue. This issue is not occurring with the public repository. I have tested the same in my cluster with the public repository.

I have attached a ScreenShots. ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-10-10 09:04:43,3
13261131,Backport: Performance Tune Hosts and service Configs Pages to 2.7.5,Backport for AMBARI-24842 and [AMBARI-24876|https://hortonworks.jira.com/issues/?jql=project+in+%2810320%2C+11620%2C+11320%2C+10520%29+AND+cf%5B11018%5D+%3D+AMBARI-24876] into ambari-2.7.5.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-10-08 14:40:22,6
13261100,Ambari logging Grafana Password in ActionQueue.py,"{code}
ActionQueue.py [ 6398 ] - root - INFO - Cmd log for taskId=456 and chunk 3/42 of log for command: #012s_user\', \'hdfs\')}}/hadoop-{{default(\'configurations/hadoop-
.
.
.
u'metrics_grafana_log_dir': u'/var/log/ambari-metrics-grafana',
u'metrics_grafana_username': u'admin',
 * u'metrics_grafana_password': u'mypassword123!',*
u'metrics_grafana_pid_dir': u'/var/run/ambari-metrics-grafana',
u'metrics_grafana_data_dir': u'/var/lib/ambari-metrics-grafana'
}, u'ranger-hive-policymgr-ssl': {
{code}",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2019-10-08 11:39:23,4
13260554,Ambari is indexing all subdirectories contents under /resources folder via API.,"GET /resources gives back the directory content of /var/lib/ambari-server/resources. The directory doesn't contain any sensitive information, only files which are already visible on github. But it might freak out security guys therefore it's best to disable the listing.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-10-04 11:03:28,5
13260530,Kafka brokers can not use PLAINTEXT and SASL_PLAINTEXT at the same time when Kerberos is enabled,"With Kerberos enabled, PLAINTEXT listeners get substituted with SASL_PLAINTEXT by ambari-agent.

This makes using both PLAINTEXT and SASL_PLAINTEXT at the same time impossible, as the listeners property will have SASL_PLAINTEXT listeners provided twice (on different ports).",ambari-agent kafka,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2019-10-04 08:34:25,10
13259866,Ambari-Web UI hosts Tab is vulnerable to XSS attack,"Problem Statement : Ambari-Web UI hosts Tab is vulnerable to XSS attack

Issue reproduction : 

1) Execute the Following CURL command to edit the Rack INFO . 


{code:java}
curl -u admin:admin -H ""X-Requested-By:ambari"" -i -X PUT http://ambari-server:8080/api/v1/clusters/asnaik/hosts -d '{""RequestInfo"":{""context"":""Set Rack"",""query"":""Hosts/host_name.in(abc.openstacklocal)""},""Body"":{""Hosts"":{""rack_info"":""/default-rack/<IMG SRC='x' onerror=javascript:alert(domain);>""}}}'
{code}

where abc.openstacklocal is the host i want to change the rack info
(please note <IMG SRC='x' onerror=javascript:alert(domain);> we cannot add in UI via edit rack_info as in UI we checks for special characters)


Login to Ambari-UI -> navigate to Hosts and go to host : abc.openstacklocal

XSS will be injected to DOM and following Alert will be displayed.

 !Screen Shot 2019-10-01 at 4.02.07 PM.png! 
",pull-request-available security,[],AMBARI,Bug,Major,2019-10-01 10:32:48,3
13259633,dfs_ha_initial_* properties should be removed during upgrade,"NameNode HA cluster was created via blueprint with specifying 'dfs_ha_initial_namenode_active' and 'dfs_ha_initial_namenode_standby' properties for hadoop-env with invalid values 'None'.
Afterwards ambari upgrade from 2.6.* to 2.7.5 was failed with following stacktrace:
{code}
ERROR [main] SchemaUpgradeHelper:240 - Upgrade failed. 
java.lang.IllegalArgumentException: NAMENODE HA hosts mapped incorrectly for properties 'dfs_ha_initial_namenode_active' and 'dfs_ha_initial_namenode_standby'. Expected hosts are: [<nnHAHost1>, <nnHAHost2>]
	at org.apache.ambari.server.topology.ClusterTopologyImpl.validateTopology(ClusterTopologyImpl.java:221)
	at org.apache.ambari.server.topology.ClusterTopologyImpl.<init>(ClusterTopologyImpl.java:79)
	at org.apache.ambari.server.topology.PersistedStateImpl.getAllRequests(PersistedStateImpl.java:217)
	at org.apache.ambari.server.topology.TopologyManager.ensureInitialized(TopologyManager.java:205)
	at org.apache.ambari.server.topology.TopologyManager.getPendingHostComponents(TopologyManager.java:819)
	at org.apache.ambari.server.utils.StageUtils.getClusterHostInfo(StageUtils.java:306)
	at org.apache.ambari.server.controller.KerberosHelperImpl.addAdditionalConfigurations(KerberosHelperImpl.java:2961)
	at org.apache.ambari.server.controller.KerberosHelperImpl.calculateConfigurations(KerberosHelperImpl.java:1723)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1630)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1060)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:238)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:458)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-09-30 10:17:12,6
13258779,Reduce cluster creation request processing time,"Identify where time is spent during processing of blueprint-based cluster creation request.
{noformat:title=HDInsight cluster with 7 nodes (14 seconds)}
19 Dec 2016 09:41:58,225  INFO [ambari-client-thread-45] AbstractResourceProvider:518 - Creating Cluster 'hdfs-toader-1219-dev' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 09:42:12,015  INFO [pool-18-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.
{noformat}
{noformat:title=OpenStack cluster with 8 nodes (9 seconds)}
19 Dec 2016 11:17:30,640  INFO [ambari-client-thread-23] AbstractResourceProvider:518 - Creating Cluster 'TEST' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 11:17:39,875  INFO [pool-17-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-09-25 11:40:56,6
13258544,Ambari Files View is Vulnerable to XSS attack,"Problem Statement :  Ambari Files view is vulnerable to XSS attack, if the Filename of the file uploaded in HDFS contains XSS scripts.

Reproduction : 

1) login to files view

2) create a file called in your local system and upload it to files view: <svg onload= alert(document.domain)>

3) try to delete the file or edit permission of the file. the malciious XSS script will be executed in the Browser. this is a security Issue.


Please see attached screenshot
",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2019-09-24 12:38:39,3
13258532,Ambari Metrics whitelisting is failing on * wildcard for Kafka Topics,"On Ambari 2.6.x and 2.7.x if the Ambari Metrics Collector whitelisting is enabled, the Kafka Topics are not discovered on the Kafka topics dashboard. (The Topics dropdown is empty.)

It can be remediated by adding '._p_kafka.log.Log.*' to the whitelist file and restarting the Metrics Collector.
 Adding the '._p_kafka.log.Log.*' to the whitelist file by default should be considered. Also it should be investigated why we need the ""._p_"" prefix.

Furthermore, it seems that the metrics enabled in the whitelist file as:
{code:java}
kafka.server.BrokerTopicMetrics.BytesInPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.BytesOutPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.MessagesInPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.TotalProduceRequestsPerSec.topic.*.count
{code}
are filtered out and not shown on Grafana dashboard.

The issue can be worked around by adding the '._p_' prefix to the corresponding metrics in the whitelist file, e.g.
._p_kafka.server.BrokerTopicMetrics.BytesInPerSec.topic.*.count .",grafana kafka metrics pull-request-available,['ambari-metrics'],AMBARI,Task,Major,2019-09-24 11:39:45,4
13257923,Issues with Views in ambari when  User Logs In from KNOX/LDAP and the username has spaces and Camel Case Letters,"Problem Statement : Ambari Views Doesnt work properly When User Logs In from KNOX and the username has spaces and Camel Case Letters

Affected Views :

Filesview  : login as Akhil S Naik, filesview will consider the user as akhil s naik only due to which user home directory wont be correct shown 

Yarn Capacity Scheduler : Even though user has permission , Yarn Capacity Scheduler will complain there is no permission


Issue reproduction :

configure Knox in ambari and LDAP authentication in KNOX

Create a user 'Akhil S Naik'

Login as 'Akhil S Naik' in knox, ambari will treat the username as akhil s naik (all lower case)

and due to this even if home directory exists in /user folder in HDFS the home directory icon wont be displayed ",pull-request-available,[],AMBARI,Bug,Major,2019-09-20 11:30:13,3
13257644,Save button is enabled without any config changes for Kerberos service,"problem Statement : Save button is enabled without any config changes for Kerberos service
Issue is reproducible in any cluster with kerberos enabled


Please see the attached image :  !Screen Shot 2019-09-19 at 5.02.46 PM.png|200*200!

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-09-19 11:33:15,3
13256939,UI does not reflect/update task logs ,"Steps to reproduce

--------------------
 # Install Ambari 2.7.x and install a cluster
 # start/install any service and navigate to any of the task before starting.
 # Wait till the task is finished ( showing completed icon on top) 
 # observe there is no task logs getting updated but it is completed refer: !Screen Shot 2019-09-16 at 11.01.39 AM.png!
 # now go back one step up and come back again.. now it shows task logs:
 # !Screen Shot 2019-09-16 at 11.01.49 AM.png!",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-09-16 18:08:43,6
13256822,Upgrade AMS Grafana version to 6.4.2,Upgrade AMS Grafana version to 6.4.2,metrics pull-request-available,['ambari-metrics'],AMBARI,Task,Major,2019-09-16 08:46:02,0
13255259,Use host names instead ip addresses while checking  hive interactive service status if new hive.server2.leadertest.use.ip is false,"Add logic:
* if hive.server2.webui.use.ssl is true we should use https
* add new property hive.server2.leadertest.use.ip and set it to true by default
* if hive.server2.leadertest.use.ip is false we sould use host names is url",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-09-06 13:01:50,7
13254737,Producer and Customer Request /s graphs are failing on Kafa Grafana dashboards,"From kafka 2.0.0 there has been addition of *version* tag in kafka.network.RequestMetrics.RequestsPerSec.request.* metrics.
 This is breaking the the default Grafana dashboard provided by Ambari. On the *Kafka - Home* and *Kafka - Hosts* dashboards the *Producer requests /s* and *Consumer requests /s* graphs are failing to show any data.

To get the total count for a specific request type, the tool needs to be updated to aggregate across different versions.

*Previous metric*: kafka.network:type=RequestMetrics,name=RequestsPerSec,request=

{Produce|FetchConsumer|FetchFollower|...}

*New metric*: kafka.network:type=RequestMetrics,name=RequestsPerSec,request=\{Produce|FetchConsumer|FetchFollower|...},version=INTEGER

 

Documentation of the Kafka change: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-272%3A+Add+API+version+tag+to+broker%27s+RequestsPerSec+metric]",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2019-09-04 15:30:39,0
13253919,SSTI in Ambari config,"Hello, i found SSTI to RCE vulnerability in Apache ambari, and i send three mails with technical details to  [private@ambari.apache.org, |mailto:private@ambari.apache.org] [security@apache.org and |mailto:private@ambari.apache.org] [root@apache.org|mailto:private@ambari.apache.org]

In dates: 4 jul 2019, 8 aug 2019, and 24 aug 2019, but you not responce, bug is critical, and i want cve approve, help me?

best regards",vulnerability,['ambari-admin'],AMBARI,Bug,Critical,2019-08-30 12:09:05,6
13253304,"CLONE - Ambari audit log shows ""null"" user when executing an API call as admin - Ambari 2.6.2","When running a simple REST API call from CLI, I could see two entries in ambari-audit.log file.

 

Following is my API call:

{{curl -k -i -u admin:<passwd> -H ""X-Requested-By: ambari"" -X GET [http://<ambari-host>:8080/api/v1/clusters|http://saurabh-ambari:8080/api/v1/clusters]}}

 

Following are the 2 entries in ambari-audit.log:
{quote}2019-04-08T10:19:04.991Z, User(null), RemoteIp(x.x.x.x), Operation(User login), Roles(
 ), Status(Failed), Reason(Authentication required), Consecutive failures(UNKNOWN USER)
 2019-04-08T10:19:04.999Z, User(admin), RemoteIp(x.x.x.x), Operation(User login), Roles(
     Ambari: Ambari Administrator
 ), Status(Success)
{quote}
 

The second line seems to be valid. However, the first line (with the null user) shouldn't be there.

Note: I'm not sure if it helps, but the cluster is Kerberized and Knox isn't involved.

 

Edit: This issue could be seen on both Ambari 2.5.2 and 2.7.3. Also, 2.5.2 version cluster is Kerberized, the 2.7.3 version is NOT Kerberized. ",newbie pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-08-27 17:08:08,6
13253268,backport of Heatmap Data Issues,"""Data Not Available"" in Dashboard / Heatmap for:
 * Host Disk Space Used %
 * Host Memory Used %

""Invalid Data"" in Dashboard / Heatmap for:
 * DataNode Process Disk I/O Utilization
 * DataNode Process Network I/O Utilization

These should all be reporting data.

STR:
 - Install ZooKeeper, HDFS, AMS, SmartSense
 - Wait a 10 minutes to ensure data is coming into AMS
 - Click on Dashboard / Heatmap and view these metrics",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-08-27 14:09:17,7
13251844,Daily Namenode Heap Usage alert does not work,"Daily NameNode Heap Usage alert has *Unknown* status with *There are not enough data points to calculate the standard deviation (0 sampled)* response.
It is caused by invalid case of *appId* alert property in alert definition.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-08-20 13:43:06,6
13250897,Hive View throws TimeoutException deadline passed for few queries randomly.,"Running some hive queries throws ""deadline passed"" error as following inside amber logs.

{code}
25 Jul 2019 13:07:53,134 ERROR [ambari-client-thread-591197] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] ServiceFormattedException:97 - deadline passed
25 Jul 2019 13:07:53,135 ERROR [ambari-client-thread-591197] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] ServiceFormattedException:98 - java.util.concurrent.TimeoutException: deadline passed

java.util.concurrent.TimeoutException: deadline passed
	at akka.actor.dsl.Inbox$InboxActor$$anonfun$receive$1.applyOrElse(Inbox.scala:117)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:189)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
	at akka.actor.dsl.Inbox$InboxActor.aroundReceive(Inbox.scala:62)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

Increasing the following timeouts inside ""/etc/ambari-server/conf/ambari.properties"" followed by ambari server restart does not help either.
{code}
views.ambari.hive.AUTO_HIVE20_INSTANCE.connection.inactivity.timeout=7200000
views.ambari.hive.AUTO_HIVE20_INSTANCE.result.fetch.timeout=7200000
views.ambari.request.connect.timeout.millis=7200000
views.ambari.request.read.timeout.millis=7200000
views.request.connect.timeout.millis=7200000
views.request.read.timeout.millis=7200000
{code}


Additionally setting the following property in HiveView20 and the executing the query still causes the same failure.
{code}
set hive.fetch.task.conversion=none  
{code}

""TimeoutException: deadline passed"" still occurs much prior to the timeout defined.",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2019-08-14 23:19:48,11
13250782,Disable Kerberos failed at hive with CNF exception,"ZKMigration tool doesn't bundled with zookeeper-jute, what caused the issue on de-Kerberization of the cluster

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 143, in <module>
    HiveServer().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 124, in disable_security
    zkmigrator.set_acls(self._base_node(params.hive_cluster_token_zkstore), 'world:anyone:crdwa')
  File ""/usr/lib/ambari-agent/lib/resource_management/core/resources/zkmigrator.py"", line 43, in set_acls
    tries=tries)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/lib/jvm/java-openjdk/bin/java -Djava.security.auth.login.config=/usr/hdp/current/hive-server2/conf/zkmigrator_jaas.conf -jar /var/lib/ambari-agent/tools/zkmigrator.jar -connection-string ctr-e141 -znode /hive -acl world:anyone:crdwa' returned 1. 
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/jute/Record
tat org.apache.ambari.tools.zk.ZkConnection.open(ZkConnection.java:43)
tat org.apache.ambari.tools.zk.ZkMigrator.setAcls(ZkMigrator.java:111)
tat org.apache.ambari.tools.zk.ZkMigrator.main(ZkMigrator.java:45)
Caused by: java.lang.ClassNotFoundException: org.apache.jute.Record
tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)
tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)
tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)
t... 3 more

{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2019-08-14 12:12:25,1
13250764,Hive Service Check fails during Rolling Upgrade from HDP-3.1.0.0 to HDP-3.1.4.0,"Hive Service Check fails during Rolling Upgrade from HDP-3.1.0.0 to
HDP-3.1.4.0-301 {code} 2019-08-11 17:24:29,680 - Using hadoop conf dir:
/usr/hdp/3.1.4.0-301/hadoop/conf 2019-08-11 17:24:29,698 - call['ambari-
python-wrap /usr/bin/hdp-select status hive-server2'] {'timeout': 20}
2019-08-11 17:24:29,732 - call returned (0, 'hive-server2 - 3.1.0.0-78')
2019-08-11 17:24:29,734 - Stack Feature Version Info: Cluster Stack=3.1,
Command Stack=None, Command Version=3.1.4.0-301, Upgrade Direction=upgrade ->
3.1.4.0-301 2019-08-11 17:24:29,775 - File['/var/lib/ambari-
agent/cred/lib/CredentialUtil.jar'] {'content':
DownloadSource('http://ctr-e141-1563959304486-21229-01-000007.hwx.site:8080/resources/CredentialUtil.jar'),
'mode': 0755} 2019-08-11 17:24:29,777 - Not downloading the file from
http://ctr-e141-1563959304486-21229-01-000007.hwx.site:8080/resources/CredentialUtil.jar,
because /var/lib/ambari-agent/tmp/CredentialUtil.jar already exists 2019-08-11
17:24:30,749 - Running Hive Server checks 2019-08-11 17:24:30,749 -
-------------------------- 2019-08-11 17:24:30,752 - Server Address List :
[u'ctr-e141-1563959304486-21229-01-000002.hwx.site',
u'ctr-e141-1563959304486-21229-01-000004.hwx.site'], Port : 10000, SSL
KeyStore : None 2019-08-11 17:24:30,752 - Waiting for the Hive Server to
start... 2019-08-11 17:24:30,753 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:30,863 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:36,712 - Successfully connected to
ctr-e141-1563959304486-21229-01-000002.hwx.site on port 10000 2019-08-11
17:24:36,713 - Successfully stayed connected to 'Hive Server' on host:
ctr-e141-1563959304486-21229-01-000007.hwx.site and port 10000 after
5.96049404144 seconds 2019-08-11 17:24:36,713 - Running Hive Server2 checks
2019-08-11 17:24:36,713 - -------------------------- 2019-08-11 17:24:36,719 -
Server Address List : [u'ctr-e141-1563959304486-21229-01-000002.hwx.site'],
Port : 10500, SSL KeyStore : None 2019-08-11 17:24:36,719 - Waiting for the
Hive Server2 to start... 2019-08-11 17:24:36,719 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:36,834 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:42,100 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:24:47,106 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:47,225 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:52,456 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:24:57,462 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:57,582 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:02,793 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:07,798 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:07,919 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:13,260 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:18,265 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:18,360 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:23,658 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:28,664 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:28,772 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:34,144 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:39,146 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:39,264 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:44,573 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:49,579 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:49,694 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:54,972 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:59,977 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:00,094 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:05,416 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:10,422 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:10,539 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:15,973 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:20,979 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:21,090 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:26,397 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:31,401 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:31,516 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:36,865 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:41,866 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:41,983 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:47,237 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:52,243 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:52,362 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:57,704 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:02,710 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:02,805 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:08,094 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:13,099 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:13,215 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:18,498 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:23,504 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:23,622 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:28,949 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:33,953 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:34,071 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:39,277 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:44,283 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:44,399 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:49,720 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:54,726 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:54,846 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:00,106 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:05,112 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:05,229 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:10,652 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:15,658 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:15,778 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:21,134 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:26,140 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:26,266 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:31,610 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:36,616 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:36,734 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:42,040 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:47,045 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:47,161 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:52,510 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:57,516 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:57,633 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:02,939 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:29:07,945 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:29:08,064 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:13,369 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:29:18,374 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:29:18,475 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:23,822 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed Command failed after 1 tries {code} Ambari was upgraded
from 2.7.3.0 to 2.7.4.0

",pull-request-available,[],AMBARI,Bug,Major,2019-08-14 11:04:52,5
13250211,Seeing an error stack when running an API call against Ambari server,"If an API call responds with stack traces that are not managed it could reveal information useful to attackers. This information could then be used in further attacks. Providing debugging information as a result of operations that generate errors is considered a bad practice due to multiple reasons. For example, it may contain information on internal workings of the application such as relative paths of the point where the application is installed or how objects are referenced internally

When a runtime error occurs during request processing, server will display debugging information to the requestor. Ideally, such debug information be withheld from the requestor.

More on the security threat:

[CWE-209: Information Exposure Through an Error Message|https://cwe.mitre.org/data/definitions/209.html]

[Improper Error Handling|https://www.owasp.org/index.php/Improper_Error_Handling]


 I see the following error stack when I run an Ambari API call:
{code:java}
# curl -X GET -u admin:admin ""http://<ambari-host>:8080/api/v1/security/userlist/q;%""

HTTP ERROR 500

Problem accessing /api/v1/security/userlist/q;%. Reason:

    Server Error

Caused by:
org.springframework.security.web.firewall.RequestRejectedException: The request was rejected because the URL contained a potentially malicious String "";""
	at org.springframework.security.web.firewall.StrictHttpFirewall.rejectedBlacklistedUrls(StrictHttpFirewall.java:265)
	at org.springframework.security.web.firewall.StrictHttpFirewall.getFirewalledRequest(StrictHttpFirewall.java:245)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:193)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:740)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:221)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:210)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:503)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
	at java.lang.Thread.run(Thread.java:745)

{code}",security,['ambari-server'],AMBARI,Bug,Major,2019-08-12 08:14:43,4
13249901,Upgrade fasterxml jackson dependency due to CVE,We should upgrade Jackson databind dependencies because of the CVE: [https://nvd.nist.gov/vuln/detail/CVE-2019-14379],pull-request-available,['security'],AMBARI,Bug,Critical,2019-08-09 13:57:44,12
13249062,Move Ambari metrics to guava 28.0-jre,"The new HDP stack have moved to the new and latest version of guava. Because Ambari Metrics depends on those components, it need to be moved to the same guava version ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2019-08-06 09:23:25,1
13249047,Change apache-hadoop repository from private nexus to public in AMS,"Change [https://github.com/apache/ambari/blob/branch-2.7/ambari-metrics/pom.xml#L77] to 
[https://repo.hortonworks.com/content/groups/public/]",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2019-08-06 08:11:59,8
13247375,Provide graceful start/stop option for RegionServer,RegionServer should have graceful start/stop options to move regions to other RegionServers before stopping and move regions back in after getting started. This avoids necessitating using HBase Balancer manually.,pull-request-available,['ambari-server'],AMBARI,New Feature,Major,2019-07-26 17:08:51,12
13247086,Storm Service Check fails during Rolling Upgrade due to Nimbus leader selection,"Storm Service Check fails during Rolling Upgrade

{code}
1099 [main] WARN  o.a.s.h.DefaultShader - Relocating backtype/storm/task/ShellBolt to org/apache/storm/task/ShellBolt in storm/starter/WordCountTopology$SplitSentence.class. please modify your code to use the new namespace
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-228/storm/lib/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-228/storm/contrib/storm-autocreds/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-228/storm/lib/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-228/storm/contrib/storm-autocreds/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Running: /usr/lib/jvm/java-openjdk/bin/java -Ddaemon.name= -Dstorm.options= -Dstorm.home=/usr/hdp/3.1.4.0-228/storm -Dstorm.log.dir=/grid/0/log/storm -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /usr/hdp/3.1.4.0-228/storm/*:/usr/hdp/3.1.4.0-228/storm/lib/*:/usr/hdp/3.1.4.0-228/storm/extlib/*:/usr/hdp/current/storm-supervisor/external/storm-autocreds/*:/tmp/ed6b6fbca91811e9a9970242ac1b7581.jar:/usr/hdp/current/storm-supervisor/conf:/usr/hdp/3.1.4.0-228/storm/bin -Dstorm.jar=/tmp/ed6b6fbca91811e9a9970242ac1b7581.jar -Dstorm.dependency.jars= -Dstorm.dependency.artifacts={} storm.starter.WordCountTopology WordCountid1bac8175_date001819
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-228/storm/lib/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-228/storm/contrib/storm-autocreds/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
745  [main] WARN  o.a.s.u.Utils - STORM-VERSION new 1.2.1.3.1.4.0-228 old null
768  [main] INFO  o.a.s.StormSubmitter - Generated ZooKeeper secret payload for MD5-digest: -5297560719882079603:-7456204693431648577
934  [main] INFO  o.a.s.m.n.Login - successfully logged in.
1006 [main] INFO  o.a.s.u.NimbusClient - Found leader nimbus : ctr-e139-1542663976389-174824-01-000005.hwx.site:6627
1012 [main] INFO  o.a.s.m.n.Login - successfully logged in.
1102 [main] INFO  o.a.s.s.a.AuthUtils - Got AutoCreds []
1107 [main] INFO  o.a.s.m.n.Login - successfully logged in.
1128 [main] INFO  o.a.s.u.NimbusClient - Found leader nimbus : ctr-e139-1542663976389-174824-01-000005.hwx.site:6627
1134 [main] INFO  o.a.s.m.n.Login - successfully logged in.
1188 [main] INFO  o.a.s.StormSubmitter - Uploading dependencies - jars...
1189 [main] INFO  o.a.s.StormSubmitter - Uploading dependencies - artifacts...
1189 [main] INFO  o.a.s.StormSubmitter - Dependency Blob keys - jars : [] / artifacts : []
1275 [main] INFO  o.a.s.StormSubmitter - Uploading topology jar /tmp/ed6b6fbca91811e9a9970242ac1b7581.jar to assigned location: /hadoop/storm/nimbus/inbox/stormjar-0728f59f-a74c-4edf-99a7-c77c3be6c7a3.jar
1314 [main] INFO  o.a.s.StormSubmitter - Successfully uploaded topology jar to assigned location: /hadoop/storm/nimbus/inbox/stormjar-0728f59f-a74c-4edf-99a7-c77c3be6c7a3.jar
1314 [main] INFO  o.a.s.StormSubmitter - Submitting topology WordCountid1bac8175_date001819 in distributed mode with conf {""storm.zookeeper.topology.auth.scheme"":""digest"",""storm.zookeeper.topology.auth.payload"":""-5297560719882079603:-7456204693431648577"",""topology.workers"":3,""topology.debug"":true}
1314 [main] WARN  o.a.s.u.Utils - STORM-VERSION new 1.2.1.3.1.4.0-228 old 1.2.1.3.1.4.0-228
Exception in thread ""main"" java.lang.RuntimeException: org.apache.storm.thrift.TApplicationException: Internal error processing submitTopology
	at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:273)
	at org.apache.storm.StormSubmitter.submitTopology(StormSubmitter.java:387)
	at org.apache.storm.StormSubmitter.submitTopology(StormSubmitter.java:159)
	at storm.starter.WordCountTopology.main(WordCountTopology.java:77)
Caused by: org.apache.storm.thrift.TApplicationException: Internal error processing submitTopology
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.storm.generated.Nimbus$Client.recv_submitTopology(Nimbus.java:279)
	at org.apache.storm.generated.Nimbus$Client.submitTopology(Nimbus.java:263)
	at org.apache.storm.StormSubmitter.submitTopologyInDistributeMode(StormSubmitter.java:326)
	at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:260)
	... 3 more

Command failed after 1 tries
{code}

STR: Deploy cluster with Ambari2.7.1.0 and HDP3.0.1.0
Upgrade ambari to 2.7.4.0-96 (5273932c0b8269aa46027356cfb61ef8a5347625)
Start RU to HDP-3.1.4.0-228

at nimbus logs (both nodes): there was some time (around 1 min) where both nodes were not a leader, and topology submission happened at that time.

Nimbus in node5 logged the first log message (Ranger plugin) at 2019-07-18 04:59:24.694, and nimbus in node6 logged same at 2019-07-18 04:59:10.717.

Both were started as “Not a leader” as it has to execute its own startup process. Since they’re not a leader, they added themselves to leader lock on zookeeper and waited for gaining leadership. (Node5: 2019-07-18 04:59:28.769, Node6: 2019-07-18 04:59:13.676)

And topology submission happened at the time where both hadn’t gain leadership. (Node5: 2019-07-18 05:00:25.020, Node6: 2019-07-18 05:00:02.609) Both would fail to process topology submission as they’re not a leader.

Node6 finally gain leadership at 2019-07-18 05:00:27.026. After this time Storm can properly receive topology submission.

Btw, the client side thrift error message seems to be misleading, as it doesn’t show actual cause in Nimbus log: 

{code}java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='host5', port=6627, isLeader=true}{code}

(though isLeader=true is incorrect too… anyway “not a leader” is the thing)

 We can’t ensure one nimbus must be gained leadership at any time. In H/A environment, client side also has to tolerate failure on “no leader”. We will add logic to retry",pull-request-available,[],AMBARI,Bug,Major,2019-07-25 15:01:35,13
13245999,SmartSense API call fails with Unsupported Media Type,"AMBARI-9016 is causing regression in SmartSense by inappropriately changing the Content-Type to ""text/plain"" for every request with ""application/json"" Content-Type. This is a proper workaround for the Ambari API but not for SmartSense.

Calling the SmartSense API failing with :
{code:java}
HTTP/1.1 415 Unsupported Media Type{code}
 
{code:java}
curl 'https://172.27.122.133:8443/api/v1/views/SMARTSENSE/versions/1.5.1.2.7.4.0-92/instances/SMARTSENSE_AUTO_INSTANCE/resources/hst/bundles' -H 'Cookie: AMBARISESSIONID=node0mmlb052gnyw31rznh33qguoem22.node0; SUPPORTSESSIONID=1nlk0dmxktunz1bvjr9259adjf' -H 'Origin: https://172.27.122.133:8443' -H 'Accept-Encoding: gzip, deflate, br' -H 'Accept-Language: en-US,en;q=0.9' -H 'X-Requested-By: ambari' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36' -H 'Content-Type: application/json; charset=UTF-8' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: https://172.27.122.133:8443/views/SMARTSENSE/1.5.1.2.7.4.0-92/SMARTSENSE_AUTO_INSTANCE/' -H 'X-Requested-With: XMLHttpRequest' -H 'Connection: keep-alive' -H 'withCredentials: true' --data-binary '{""caseNumber"":0}' --compressed --insecure -v -u *****

* Trying 172.27.122.133...
* TCP_NODELAY set
* Connected to 172.27.122.133 (172.27.122.133) port 8443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server did not agree to a protocol
* Server certificate:
* subject: C=UA; ST=Some-State; O=Internet Widgits Pty Ltd; CN=ctr-e139-1542663976389-172620-01-000002.hwx.site
* start date: Jul 16 08:42:47 2019 GMT
* expire date: Jul 15 08:42:47 2020 GMT
* issuer: C=UA; ST=Some-State; O=Internet Widgits Pty Ltd; CN=ctr-e139-1542663976389-172620-01-000002.hwx.site
* SSL certificate verify result: self signed certificate (18), continuing anyway.
* Server auth using Basic with user 'admin'
> POST /api/v1/views/SMARTSENSE/versions/1.5.1.2.7.4.0-92/instances/SMARTSENSE_AUTO_INSTANCE/resources/hst/bundles HTTP/1.1
> Host: 172.27.122.133:8443
> Authorization: Basic YWRtaW46YWRtaW4=
> Cookie: AMBARISESSIONID=node0mmlb052gnyw31rznh33qguoem22.node0; SUPPORTSESSIONID=1nlk0dmxktunz1bvjr9259adjf
> Origin: https://172.27.122.133:8443
> Accept-Encoding: gzip, deflate, br
> Accept-Language: en-US,en;q=0.9
> X-Requested-By: ambari
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36
> Content-Type: application/json; charset=UTF-8
> Accept: application/json, text/javascript, */*; q=0.01
> Referer: https://172.27.122.133:8443/views/SMARTSENSE/1.5.1.2.7.4.0-92/SMARTSENSE_AUTO_INSTANCE/
> X-Requested-With: XMLHttpRequest
> Connection: keep-alive
> withCredentials: true
> Content-Length: 16
>
* upload completely sent off: 16 out of 16 bytes
< HTTP/1.1 415 Unsupported Media Type
< Date: Tue, 16 Jul 2019 18:36:36 GMT
< Strict-Transport-Security: max-age=31536000
< X-Frame-Options: SAMEORIGIN
< X-XSS-Protection: 1; mode=block
< X-Content-Type-Options: nosniff
< Cache-Control: no-store
< Pragma: no-cache
< Content-Type: application/json;charset=utf-8
< User: admin
< X-Content-Type-Options: nosniff
< Content-Length: 0
<
* Connection #0 to host 172.27.122.133 left intact
{code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-07-19 13:34:51,0
13245805,KRB MissingKeytabs class method signature incorrect,class level method for MissingKeytabs is referring to self instead of cls and not calling static methods with the right context.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-07-18 15:01:13,12
13245774,Incorrect arguments for FatalException in setupMPacks,"setupMPacks doesn't have right argument for FataException in some places and hence, it is not working as expected",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-07-18 12:47:06,12
13244947,Update HDP dependencies of AMS for 2.7.4,"Update Hadoop, HBase and Phoenix dependencies of AMS and do dev-testing with the changes.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2019-07-15 12:09:08,8
13244672,Perf improvement with bulk operation over iterations,Some places in ambari-server can be updated to use bulk operations over iterations to improve the performance,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2019-07-12 18:30:36,12
13244312,Reduce cluster creation request processing time,"Identify where time is spent during processing of blueprint-based cluster creation request.
{noformat:title=HDInsight cluster with 7 nodes (14 seconds)}
19 Dec 2016 09:41:58,225  INFO [ambari-client-thread-45] AbstractResourceProvider:518 - Creating Cluster 'hdfs-toader-1219-dev' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 09:42:12,015  INFO [pool-18-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.
{noformat}
{noformat:title=OpenStack cluster with 8 nodes (9 seconds)}
19 Dec 2016 11:17:30,640  INFO [ambari-client-thread-23] AbstractResourceProvider:518 - Creating Cluster 'TEST' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 11:17:39,875  INFO [pool-17-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-07-11 13:02:31,6
13243733,Regenerate keytab generates empty keytab file if no file present in cache,"Steps to reproduce:

If Database table has a reference for a keytab file at /var/lib/ambari-server/data/cache/ and if file is not present then Ambari would copy empty keytab files to all the hosts.

1. Install and Kerberize the cluster.
 2. remove keytab file/s at /var/lib/ambari-server/data/cache/
 3. Now re-generate missing keytab files operation.
 4. You can find empty keytab file in cluster hosts.

Ambari should check keytab file exist or not while deciding what to re-generate.

 

This BUG can cause issue during the Upgrade as it runs re-generate missing keytab operation.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-07-08 17:57:18,14
13243238,Kerberos keytab regeneration  working slow,Regeneration keytab on big clusters during upgrade takes a lot of time. Cheking the logs show that Kerberos and hosts with the Ambari are in good shape and have no CPU or Memory spikes.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-07-04 15:45:39,1
13243232,[HA] RESOURCEMANAGER is not starting after adding and removing journal nodes,"Adding and removing the Journal nodes, Resource manager is not starting",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-07-04 15:19:25,15
13242494,Ambari breadcrumbs xss vulnerability,"Special characters should be encoded when displayed in Ambari Views.

If special characters are not encoded, then scripts ({{<script>alert(""xss!"")</script>}}) may be executed due to user input. For example, issues may occur by placing special character in the Display Name field of an Ambari View.",pull-request-available,['ambari-web'],AMBARI,Task,Blocker,2019-07-01 11:41:34,7
13242394,When spark_transport_mode is set to 'http' then STS server fails to start in Ambari 2.7,"When the ""hive.server2.transport.mode"" is set tp ""http"" (instead of ""binary"") inside the ""spark2-hive-site-override"" configuration then the ""Spark2 Thrift Server"" start operation fails via Ambari UI with the following error:

{code}
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
ExecutionFailed: Execution of '! /usr/hdp/current/spark2-thriftserver/bin/beeline -u 'jdbc:hive2://kerlatest4.example.com:10002/default;transportMode=http'  -e '' 2>&1| awk '{print}'|grep -i -e 'Connection refused' -e 'Invalid URL' -e 'Error: Could not open'' returned 1. Error: Could not open client transport with JDBC Uri: jdbc:hive2://kerlatest4.example.com:10002/default;transportMode=http: Could not create http connection to jdbc:hive2://kerlatest4.example.com:10002/default;transportMode=http. HTTP Response code: 405 (state=08S01,code=0)
Error: Could not open client transport with JDBC Uri: jdbc:hive2://kerlatest4.example.com:10002/default;transportMode=http: Could not create http connection to jdbc:hive2://kerlatest4.example.com:10002/default;transportMode=http. HTTP Response code: 405 (state=08S01,code=0)
{code}


This is because the ""spark_service.py"" script is not appending the ""httpPath=cliservice"" when the ""hive.server2.transport.mode"" is set to ""http"" in the ""beeline_url"" inside the ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/SPARK2/package/scripts/spark_service.py""  script  (On Ambari Agent ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/spark_service.py"").


*Manual Workaround:*
Update the ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/spark_service.py"" on agent and amabri server as following with additional If else block to check the spark_transport_mode == 'http':

{code}
146c146,149
<           beeline_url = [""jdbc:hive2://{fqdn}:{spark_thrift_port}/default;principal={hive_kerberos_principal}"",""transportMode={spark_transport_mode}""]
---
>           if params.spark_transport_mode == 'http':
>             beeline_url = [""jdbc:hive2://{fqdn}:{spark_thrift_port}/default;principal={hive_kerberos_principal}"",""transportMode={spark_transport_mode}"",""httpPath=cliservice""]
>           else:
>             beeline_url = [""jdbc:hive2://{fqdn}:{spark_thrift_port}/default;principal={hive_kerberos_principal}"",""transportMode={spark_transport_mode}""]
148c151,154
<           beeline_url = [""jdbc:hive2://{fqdn}:{spark_thrift_port}/default"",""transportMode={spark_transport_mode}""]
---
>           if params.spark_transport_mode == 'http':
>             beeline_url = [""jdbc:hive2://{fqdn}:{spark_thrift_port}/default"",""transportMode={spark_transport_mode}"",""httpPath=cliservice""]
>           else:
>             beeline_url = [""jdbc:hive2://{fqdn}:{spark_thrift_port}/default"",""transportMode={spark_transport_mode}""]
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-30 23:33:08,8
13241780,Prevent NPE for ControllerModule and ClusterResourceProvider,We should prevent possibilities of NPE with bindNotificationDispatchers() and  getServiceConfigVersionRequest().,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-26 19:23:50,12
13241084,RetryUpgradeActionService not able to update host_role_command in Transaction,RetryUpgradeActionService is not updating host_role_command table in Transaction since @Transactional is applied in same class non-public method which is not applied through AOP proxy,pull-request-available,[],AMBARI,Bug,Major,2019-06-23 10:13:34,12
13240955,Update commons-lang and commons-lang3 to latest versions,"For latest improvements and bugfixes, we should upgrade commons-lang and commons-lang3 to latest versions 2.6 and 3.9 respectively",pull-request-available,[],AMBARI,Improvement,Major,2019-06-21 20:38:21,12
13240655,Remove dependency on org.eclipse.jetty.*:9.3.19.v20170502 in Ambari Logsearch Logfeeder,"Remove dependency on org.eclipse.jetty.* 9.3.19.v20170502 in Ambari Logsearch Logfeeder due to security concerns. See 

https://nvd.nist.gov/vuln/detail/CVE-2018-12536

https://nvd.nist.gov/vuln/detail/CVE-2017-7658

https://nvd.nist.gov/vuln/detail/CVE-2017-7657

https://nvd.nist.gov/vuln/detail/CVE-2017-7656

{code}
± % mvn dependency:tree -Dincludes=org.eclipse.jetty
[INFO] Scanning for projects...
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-logsearch-logfeeder:jar:2.7.3.0.0
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-compiler-plugin @ org.apache.ambari:ambari-logsearch-logfeeder:[unknown-version], /Users/gboros/Documents/dev/ambari/ambari-logsearch/ambari-logsearch-logfeeder/pom.xml, line 311, column 15
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING]
[INFO]
[INFO] ------------< org.apache.ambari:ambari-logsearch-logfeeder >------------
[INFO] Building Ambari Logsearch Log Feeder 2.7.3.0.0
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-logsearch-logfeeder ---
[INFO] org.apache.ambari:ambari-logsearch-logfeeder:jar:2.7.3.0.0
[INFO] \- org.apache.hadoop:hadoop-common:jar:3.0.0:compile
[INFO]    +- org.eclipse.jetty:jetty-server:jar:9.3.19.v20170502:compile
[INFO]    |  +- org.eclipse.jetty:jetty-http:jar:9.3.19.v20170502:compile
[INFO]    |  \- org.eclipse.jetty:jetty-io:jar:9.3.19.v20170502:compile
[INFO]    +- org.eclipse.jetty:jetty-util:jar:9.3.19.v20170502:compile
[INFO]    +- org.eclipse.jetty:jetty-servlet:jar:9.3.19.v20170502:compile
[INFO]    |  \- org.eclipse.jetty:jetty-security:jar:9.3.19.v20170502:compile
[INFO]    \- org.eclipse.jetty:jetty-webapp:jar:9.3.19.v20170502:compile
[INFO]       \- org.eclipse.jetty:jetty-xml:jar:9.3.19.v20170502:compile
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1.939 s
[INFO] Finished at: 2019-06-17T14:36:58+02:00
[INFO] ------------------------------------------------------------------------
{code}
Recommendation is to remove the dependency or upgrade to version 9.3.24 or the latest version, if possible. ",pull-request-available,['logsearch'],AMBARI,Bug,Major,2019-06-20 14:08:03,16
13240590,Logsearch: Upgrade dependency on org.springframework.boot:spring-boot-starter-jetty:jar:2.0.6.RELEASE,"Remove dependency on org.mortbay.jasper:apache-el:jar:8.5.33 in Ambari Logsearch due to security concerns. See 

https://nvd.nist.gov/vuln/detail/CVE-2019-0199

{code}
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-logsearch-server ---
[INFO] org.apache.ambari:ambari-logsearch-server:jar:2.7.3.0.0
[INFO] \- org.springframework.boot:spring-boot-starter-jetty:jar:2.0.6.RELEASE:compile
[INFO]    \- org.mortbay.jasper:apache-el:jar:8.5.33:compile
[INFO]
[INFO] ------------< org.apache.ambari:ambari-logsearch-assembly >-------------
[INFO] Building Ambari Logsearch Assembly 2.7.3.0.0                     [13/14]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-logsearch-assembly ---
[INFO] org.apache.ambari:ambari-logsearch-assembly:jar:2.7.3.0.0
[INFO] \- org.apache.ambari:ambari-logsearch-server:jar:2.7.3.0.0:compile
[INFO]    \- org.springframework.boot:spring-boot-starter-jetty:jar:2.0.6.RELEASE:compile
[INFO]       \- org.mortbay.jasper:apache-el:jar:8.5.33:compile
[INFO]
[INFO] ---------------< org.apache.ambari:ambari-logsearch-it >----------------
[INFO] Building Ambari Logsearch Integration Test 2.7.3.0.0             [14/14]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-logsearch-it ---
[INFO] org.apache.ambari:ambari-logsearch-it:jar:2.7.3.0.0
[INFO] \- org.apache.ambari:ambari-logsearch-server:jar:2.7.3.0.0:compile
[INFO]    \- org.springframework.boot:spring-boot-starter-jetty:jar:2.0.6.RELEASE:compile
[INFO]       \- org.mortbay.jasper:apache-el:jar:8.5.33:compile
{code}

Recommendation is to remove the dependency or upgrade to version org.springframework.boot:spring-boot-starter-jetty:jar:2.0.9.RELEASE or the latest version, if possible.",pull-request-available,['logsearch'],AMBARI,Bug,Major,2019-06-20 07:07:17,16
13240491,Regenerate Key tabs action is missing from Service Action list,Action is missing even if not manual kerberos is enabled,pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-06-19 19:48:47,7
13240432,Incorrect css class is referred for popup,"For popups, incorrect css class is being referred which is not defined. We need to correct it.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-06-19 14:03:39,12
13240385,Upgrade dependency on org.springframework:spring-web:jar:4.3.18.RELEASE in Ambari Server,"Remove dependency on org.springframework.security:spring-security-web 4.3.18.RELEASE in Ambari Server due to security concerns. See 

https://nvd.nist.gov/vuln/detail/CVE-2018-15756

{code}
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------< org.apache.ambari:ambari-server >-------------------
[INFO] Building Ambari Server 2.7.3.0.0
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
...
[INFO] +- org.springframework:spring-web:jar:4.3.18.RELEASE:compile
{code}

Recommendation is to remove the dependency or upgrade to version 4.3.20.RELEASE or the latest version, if possible. ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-19 10:05:24,16
13240369,Spark2 thrift server alert doesn't work when SSL is not enabled,"With SSL disabled for Spark2, Ambari alerts fail with below exception:
{code:java}
Connection failed on host <host>:10016 (Traceback (most recent call last):
File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/alerts/alert_spark2_thrift_port.py"", line 173, in execute
timeout_kill_strategy=TerminateStrategy.KILL_PROCESS_TREE
File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
self.env.run()
File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
self.run_action(resource, action)
File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
provider_action()
File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
returns=self.resource.returns)
File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
result = function(command, **kwargs)
File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
result = _call(command, **kwargs_copy)
File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
raise ExecutionFailed(err_msg, code, out, err)
ExecutionFailed: Execution of '! beeline -u jdbc:hive2://<host>:10016/default;principal=hive/<host>@EXAMPLE.COM transportMode=binary -e '' 2>&1| awk '{print}'|grep -i -e 'Connection refused' -e 'Invalid URL'' returned 1. Connecting to jdbc:hive2://<host>:10016/default
19/06/19 08:13:44 [main]: WARN jdbc.HiveConnection: Failed to connect to <host>:10016
Error: Could not open client transport with JDBC Uri: jdbc:hive2://<host>/default: Peer indicated failure: Unsupported mechanism type PLAIN (state=08S01,code=0)
Beeline version 1.2.1000.2.6.4.0-91 by Apache Hive
0: jdbc:hive2://ctr-e139-1542663976389-141470 (closed)> 19/06/19 08:13:44 [main]: WARN jdbc.HiveConnection: Failed to connect to <host>:10016
Error: Could not open client transport with JDBC Uri: jdbc:hive2://<host>:10016/default: Peer indicated failure: Unsupported mechanism type PLAIN (state=08S01,code=0)
){code}
 

Looks like the logic on constructing JDBC URL is not correct. Given we have a chance to fix up the logic for JDBC URL, I'd address AMBARI-25211 here as well.",pull-request-available,['alerts'],AMBARI,Bug,Major,2019-06-19 08:25:37,17
13240179,Upgrade dependency on com.mchange:c3p0:jar:0.9.5.2 in Ambari Server,"Remove dependency on com.mchange:c3p0:jar:0.9.5. in Ambari Server due to security concerns. See 

https://nvd.nist.gov/vuln/detail/CVE-2018-20433

{code}
± % mvn dependency:tree -Dincludes=com.mchange:c3p0
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------< org.apache.ambari:ambari-server >-------------------
[INFO] Building Ambari Server 2.7.3.0.0
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
[INFO] org.apache.ambari:ambari-server:jar:2.7.3.0.0
[INFO] \- com.mchange:c3p0:jar:0.9.5.2:compile
{code}
Recommendation is to remove the dependency or upgrade to version 0.9.5.3 or the latest version, if possible.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-18 11:18:42,16
13240166,Ambari 2.7.4 is missing upgrade catalogs,Looks like we have no upgrade catalogs for Ambari 2.7.3 and 2.7.4. So new properties are not added on Ambari upgrade. ,pull-request-available,"['ambari-server', 'ambari-upgrade']",AMBARI,Bug,Major,2019-06-18 10:24:34,13
13240063,FinalizeKerberosServerAction timeout has to be configurable,"During the HDP Upgrade to 3.1 there is re-generate keytab operation which is timing out in some cases. here is the example log.
{noformat}
2019-06-11 22:26:14,854  INFO [Server Action Executor Worker 413349] KerberosServerAction:430 - Processing identities...
2019-06-11 22:31:17,327  WARN [Server Action Executor Worker 413349] ShellCommandUtil:213 - Can not perform chown yarn-ats /etc/security/keytabs/yarn-ats.hbase-master.service.keytab
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:396)
	at org.apache.ambari.server.utils.ShellCommandUtil.runCommand(ShellCommandUtil.java:495)
	at org.apache.ambari.server.utils.ShellCommandUtil.setFileOwner(ShellCommandUtil.java:210)
	at org.apache.ambari.server.serveraction.kerberos.FinalizeKerberosServerAction.processIdentity(FinalizeKerberosServerAction.java:110)
	at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:458)
	at org.apache.ambari.server.serveraction.kerberos.FinalizeKerberosServerAction.execute(FinalizeKerberosServerAction.java:180)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
	at java.lang.Thread.run(Thread.java:745)
2019-06-11 22:31:17,328 ERROR [Server Action Executor Worker 413349] FinalizeKerberosServerAction:119 - Failed to update the owner of the keytab file at /etc/security/keytabs/yarn-ats.hbase-master.service.keytab to yarn-ats: Cannot perform operation: null
2019-06-11 22:31:17,339  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/yarn-ats.hbase-master.service.keytab to hadoop
2019-06-11 22:31:17,351  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/yarn-ats.hbase-master.service.keytab to owner:'r' and group:''
2019-06-11 22:31:17,392  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/yarn-ats.hbase-client.headless.keytab to yarn-ats
2019-06-11 22:31:17,402  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/yarn-ats.hbase-client.headless.keytab to hadoop
2019-06-11 22:31:17,411  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/yarn-ats.hbase-client.headless.keytab to owner:'r' and group:''
2019-06-11 22:31:17,616  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/smokeuser.headless.keytab to ambari-qa
2019-06-11 22:31:17,627  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/smokeuser.headless.keytab to hadoop
2019-06-11 22:31:17,639  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/smokeuser.headless.keytab to owner:'r' and group:'r'
2019-06-11 22:31:17,681  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/activity-explorer.headless.keytab to null
2019-06-11 22:31:17,682  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/activity-explorer.headless.keytab to null
2019-06-11 22:31:17,694  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/activity-explorer.headless.keytab to owner:'r' and group:'null'
2019-06-11 22:31:17,779  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/spnego.service.keytab to root
2019-06-11 22:31:17,791  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/spnego.service.keytab to hadoop
2019-06-11 22:31:17,803  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/spnego.service.keytab to owner:'r' and group:'r'
2019-06-11 22:31:17,861  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/spark2.headless.keytab to spark
2019-06-11 22:31:17,874  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/spark2.headless.keytab to hadoop
2019-06-11 22:31:17,885  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/spark2.headless.keytab to owner:'r' and group:''
2019-06-11 22:31:17,952  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/hdfs.headless.keytab to hdfs
2019-06-11 22:31:17,961  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/hdfs.headless.keytab to hadoop
2019-06-11 22:31:17,970  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/hdfs.headless.keytab to owner:'r' and group:''
2019-06-11 22:31:18,024  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/spark.service.keytab to spark
2019-06-11 22:31:18,036  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/spark.service.keytab to hadoop
2019-06-11 22:31:18,045  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/spark.service.keytab to owner:'r' and group:''
2019-06-11 22:31:18,188  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/logfeeder.service.keytab to root
2019-06-11 22:31:18,198  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/logfeeder.service.keytab to hadoop
2019-06-11 22:31:18,207  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/logfeeder.service.keytab to owner:'r' and group:''
2019-06-11 22:31:18,282  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/yarn-ats.hbase-regionserver.service.keytab to yarn-ats
2019-06-11 22:31:18,293  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/yarn-ats.hbase-regionserver.service.keytab to hadoop
2019-06-11 22:31:18,303  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/yarn-ats.hbase-regionserver.service.keytab to owner:'r' and group:''
2019-06-11 22:31:18,376  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/hbase.headless.keytab to hbase
2019-06-11 22:31:18,386  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/hbase.headless.keytab to hadoop
2019-06-11 22:31:18,396  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/hbase.headless.keytab to owner:'r' and group:'r'
2019-06-11 22:31:18,442  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:114 - Updated the owner of the keytab file at /etc/security/keytabs/ams-monitor.keytab to ams
2019-06-11 22:31:18,453  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:128 - Updated the group of the keytab file at /etc/security/keytabs/ams-monitor.keytab to hadoop
2019-06-11 22:31:18,460  INFO [Server Action Executor Worker 413349] FinalizeKerberosServerAction:145 - Updated the access mode of the keytab file at /etc/security/keytabs/ams-monitor.keytab to owner:'r' and group:''{noformat}
By default it has 300seconds at

[https://github.com/apache/ambari/blob/release-2.7.3-rc0/ambari-server/src/main/java/org/apache/ambari/server/controller/KerberosHelperImpl.java#L3730]

this needs to be configurable.

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-17 21:22:58,14
13240003,Replace deprecated FileUtils methods references,We should remove deprecated API call: FileUtils.writeStringToFile() and readFileToString(),pull-request-available,[],AMBARI,Bug,Major,2019-06-17 16:59:20,12
13239638,Desired stack id is not updated for Rolling Upgrade,"After Rolling Upgrade is performed, Desired Stack id is not updated in clusters table. We should update it otherwise many subsequent operations present undesired behavior e.g. if desired stack id is not updated after successful upgrade from stack 2.2 to 2.3, any new service onboarding to stack 2.3 fails since Ambari tries to onboard new service to stack 2.2(old desired stack id, where new service code might not be present)",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-14 19:36:54,12
13239310,Ambari UI gets into corrupt state after installing a cluster,"Steps to reproduce:
 * Install a cluster via Ambari UI

 * In the ""Customize Services"" step open the ""All Configurations"" tab and view the main configuration page for a service. I checked it with HDFS and HIVE.

 * Continue installing the cluster.

 * When done, select the service of which you viewed the configs during installation and check the configs.

 * Configuration page will be corrupt. See screenshots below. It will go back to normal after reloading Ambari UI in the browser.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-06-13 15:14:40,9
13239192,Request configurations when needed during server-side actions rather than rely on configuration data from the execution command,"Request configurations when needed during server-side actions rather than rely on configuration data from the execution command.

Due to a recent change, which appeared to remove configuration data from the execution command JSON document, data needed for Kerberos-related service-side actions is missing. This data may be requested when needed from the cluster data at the time of execution rather than when setting up the stages.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-06-13 07:14:48,18
13239030,Cluster Information Page URL is broken,"Cluster Information Page URL is broken 
Before install HDP, login to ambari. The first page you will be navigated to is Cluster Information Page. The URL here is not what is expected

Expected: views/ADMIN_VIEW/2.7.4.0/INSTANCE/#/clusterInformation
Actual: views/ADMIN_VIEW/2.7.4.0/INSTANCE/#!/clusterInformation#%2F

",pull-request-available,['ambari-admin'],AMBARI,Bug,Blocker,2019-06-12 13:24:35,15
13239025,Migrate Ambari Metrics Data On Ambari Major Upgrade to ambari-2.7.x is Migrating on 1 months data,"I am upgrading from ambari-2.6.x to ambari-3.1.x (HDF3.1 to HDF - 3.3.1)

after upgrade of HDF and ambari, I follow the command to Migrate the Ambari metrics data: 


{code:java}
/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/
            upgrade_start /etc/ambari-metrics-collector/conf/metrics_whitelist
{code}

Now after migration i can see only last 30 days data , all the Metrics data prior to 30 days is lost.

*Analysis* : 
Upon checking the code : https://github.com/apache/ambari/blob/release-2.7.3/ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/MetricsDataMigrationLauncher.java#L285

I found the Class MetricsDataMigrationLauncher.java Actually needs `startTime` as an argument which is the StartTime from which the migration need to be done, but we are not giving that anywhere in the code : https://github.com/apache/ambari-metrics/blob/dfaf6a4fcb78a6c0516a769b2fbe9cf449400ee2/ambari-metrics-timelineservice/conf/unix/ambari-metrics-collector#L333

also we are having the Default value of start time as 1 month : 
https://github.com/apache/ambari/blob/release-2.7.3/ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/MetricsDataMigrationLauncher.java#L72


{code:java}
  public static final long DEFAULT_START_TIME = System.currentTimeMillis() - ONE_MONTH_MILLIS; //Last month
{code}

so inshort we are only migrating last one months data , but no where in docs we have mentioned that and it caused Data loss to Customer.

reference : https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.3.1/ambari-managed-hdf-upgrade/content/hdf-migrate-ambari-metrics-data.html
",migration,['ambari-metrics'],AMBARI,Bug,Major,2019-06-12 12:55:51,3
13238859,Ambari metrics HA does not work in 2.7.x for Hbase/any /services,"Steps to reproduce:
-------------------------
1. Install Ambari 2.7.3 and HDP3.1 cluster.
2. Add Ambari metrics service.
3. From host page, add another ambari metrics collector.
4. Hbase would not be able to connect to collector.

Root cause:
hadop-sink is not able to spit these below hosts and end of with bad URL.
{noformat}
hbase.sink.timeline.collector.hosts=c2111-node3.hdp.com,c2111-node2.hdp.com
{noformat}


{noformat}
2019-06-11 19:46:02,731 DEBUG [timeline] timeline.HadoopTimelineMetricsSink: Trying to find live collector host from : [c2111-node3.hdp.com,c2111-node2.hdp.com]
2019-06-11 19:46:02,731 DEBUG [timeline] timeline.HadoopTimelineMetricsSink: Requesting live collector nodes : http://c2111-node3.hdp.com,c2111-node2.hdp.com:6188/ws/v1/timeline/metrics/livenodes
2019-06-11 19:46:02,732 DEBUG [timeline] timeline.HadoopTimelineMetricsSink: Unable to connect to collector, http://c2111-node3.hdp.com,c2111-node2.hdp.com:6188/ws/v1/timeline/metrics/livenodes
{noformat}


In HadoopTimelineMetricsSink.java 

{code:java}
    collectorHosts = parseHostsStringArrayIntoCollection(conf.getStringArray(COLLECTOR_HOSTS_PROPERTY));
{code}

conf.getStringArray(COLLECTOR_HOSTS_PROPERTY) is not actually returning the String array.
",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2019-06-11 19:58:01,14
13238598,Improper Error Handling when user creates a duplicate alert,"Scenario: Verify no stack trace is displayed when we try to create a duplicate alert notification
Expected : User should not be allowed to create a duplicate Alert
Actual : An error message with complete stack trace is displayed in the response. The Ambari application does not handle error exceptions appropriately when a user is configuring duplicate alert notification. The stack trace discloses back-end database and application framework details.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-10 17:26:13,13
13238367,Improve hostnames with comma separated list conversion,"Instead of appending comma separated host names to StringBuilder and then doing toString, we can use String API for better readability and performance",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-06-08 17:59:46,12
13238156,alert_ldap_password for Hive service check does not allow single-quotes,"Hive's property `alert_ldap_password` is used to validate an ambari-qa for service checks against an HS2 using LDAP authentication. If that ldap password contains a single quote, the service check will fail with an error such as:
{code}
-bash: -c: line 1: syntax error: unexpected end of file{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2019-06-07 10:01:28,8
13237967,Ambari Server Error on submitting EU,"Ambari Server Error while submitting EU

STR: 
1) Deploy cluster with Ambari 2.6.0.0 HDP 2.6.3.0
2) Upgrade ambari to 2.7.4.0
3) Submit EU to HDP-3.1.4.0

from ambari server log
{code}
2019-05-31 03:17:00,750 ERROR [ambari-client-thread-539] UpgradeResourceProvider:360 - Error appears during upgrade task submitting
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)
	at org.apache.ambari.server.utils.ShellCommandUtil.runCommand(ShellCommandUtil.java:457)
	at org.apache.ambari.server.serveraction.kerberos.KerberosOperationHandler.executeCommand(KerberosOperationHandler.java:733)
	at org.apache.ambari.server.serveraction.kerberos.KDCKerberosOperationHandler.executeCommand(KDCKerberosOperationHandler.java:248)
	at org.apache.ambari.server.serveraction.kerberos.KDCKerberosOperationHandler.init(KDCKerberosOperationHandler.java:322)
	at org.apache.ambari.server.serveraction.kerberos.KDCKerberosOperationHandler.open(KDCKerberosOperationHandler.java:114)
	at org.apache.ambari.server.serveraction.kerberos.MITKerberosOperationHandler.open(MITKerberosOperationHandler.java:94)
	at org.apache.ambari.server.controller.KerberosHelperImpl.validateKDCCredentials(KerberosHelperImpl.java:2131)
	at org.apache.ambari.server.controller.KerberosHelperImpl.handle(KerberosHelperImpl.java:2248)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
	at org.apache.ambari.server.controller.KerberosHelperImpl.executeCustomOperations(KerberosHelperImpl.java:323)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createUpgrade(UpgradeResourceProvider.java:864)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:358)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:350)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:288)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createResources(UpgradeResourceProvider.java:350)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
	at org.apache.ambari.server.api.services.UpgradeService.createUpgrade(UpgradeService.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:294)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:215)
	at org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilterInternal(AmbariBasicAuthenticationFilter.java:139)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:123)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:66)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:690)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:221)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:210)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:503)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:411)
	at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:305)
	at org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:159)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
	at java.lang.Thread.run(Thread.java:745)

{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-06 13:39:26,13
13237800,"Druid requires HDFS_CLIENT to be co-hosted, doesn't recognize ONEFS_CLIENT","While deploying Druid with OneFS using the Ambari OneFS mpack, an error is emitted while assigning slaves and clients. If Druid Historical or Druid MiddleManager is assigned, the error reads ""Druid [component] requires HDFS_CLIENT to be co-hosted"". If either is NOT assigned, the error reads ""You have selected 0 Druid [component] components. Please consider that at least 1 Druid [component] components should be installed in cluster.""

 

See the attached images.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-05 19:42:19,18
13237468,Logsearch: logfeeder throws NPE when updating checkpoint,"Every time when logfeeder sends a log entry to solr it updates the corresponding checkpoint file. After a log file rolled over every subsequent checkpoint file update fails with NPE
{code}
2019-06-04 00:00:13,611 [OutputSolr,audit_logs,worker=0] ERROR org.apache.ambari.logfeeder.util.LogFeederUtil (LogFeederUtil.java:134) - Caught exception checkIn. , input=input:source=file, path=/var/log/hadoop/hdfs/hdfs-audit.log
java.lang.NullPointerException
        at org.apache.ambari.logfeeder.input.file.FileCheckInHelper.checkIn(FileCheckInHelper.java:45)
        at org.apache.ambari.logfeeder.input.InputFile.checkIn(InputFile.java:147)
        at org.apache.ambari.logfeeder.input.InputFile.checkIn(InputFile.java:49)
        at org.apache.ambari.logfeeder.output.OutputSolr$SolrWorkerThread.addToSolr(OutputSolr.java:472)
        at org.apache.ambari.logfeeder.output.OutputSolr$SolrWorkerThread.sendToSolr(OutputSolr.java:380)
        at org.apache.ambari.logfeeder.output.OutputSolr$SolrWorkerThread.run(OutputSolr.java:340)
{code}

There is no entry in the map returned by inputFile.getCheckPointFiles() with the new file key
https://github.com/apache/ambari/blob/fd3024cc2d7e8493446ccf42e81ed52497379cce/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/file/FileCheckInHelper.java#L43",pull-request-available,['logsearch'],AMBARI,Bug,Major,2019-06-04 12:04:53,16
13236283,"Custom timeout ignored, python script has been killed due to timeout after waiting 2700 secs during HDP upgrade","There is a bug with integer value overflow due to the limited capacity of data type used. If timeout value is too high, a default timeout value is used.

The proposed fix changes internal timeout representation to Integer. That should be enough for all cases.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-05-29 16:18:24,13
13235787,"[views][Filesview] ""Copy File"" Operation in Filesview is overwriting the file ","Problem Statement :  ""Copy File"" Operation in Filesview is overwriting the file  , but the actual behaviour when do the same via copy operation in command line is not overwrite the file but a message stating the file already exists.

I want ambari-filesview also to have the same behaviour.

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-05-27 11:06:27,3
13235737,Persistent Cross Site Scripting (XSS) in Ambari,"Below is the HTTP Request and Response issued when a user submits a note containing a JavaScript
after modifying some configuration in ""Tez"" service.
HTTP Request:
PUT /api/v1/clusters/<env> HTTP/1.1
Host: xyz601:8080
Content-Length: 199
Accept: application/json, text/javascript, /; q=0.01
Origin: http://xyz601:8080
X-Requested-With: XMLHttpRequest
X-Requested-By: X-Requested-By
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/70.0.3538.102 Safari/537.36
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
Referer: http://xyz:8080/
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9
Cookie: AMBARISESSIONID=vfiy4336mxwl1k5ehd6jrz43i
Connection: close
{""Clusters"":{""desired_service_config_versions"":

{""service_config_version"":4,""service_name"":""TEZ"",""service_config_version_note"":""Creat ed from service config version V4\n<img src=x onerror=alert(1)>""}
}}

Remediation Recommendations
Restrict all input passed to the application to valid, whitelisted content, and ensure that all
response/output sent by the server is HTML/URL/JavaScript encoded, depending on the context in
which the data is used by the application.
The remediation should not attempt to blacklist content and remove, filter, or sanitize it. There are
too many types of encoding it to get around filters for such content.
We strongly recommend a positive security policy that specifies what is allowed.
Negative or attack signature based policies are difficult to maintain and are likely to be incomplete.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-05-27 07:45:37,19
13235654,Optional used as createViewUrlResource arg,ViewUrlService is using Optional<String> for urlName to create resource. We should avoid conditional logic inside method.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-05-26 11:14:08,12
13235357,Ambari always copies and overrwrites mapreduce.tar.gz to hdfs when WebHDFS is not enabled while restarting HiveServer,"Problem Statement : 

When HiveServer2 is restarted, the startup python script will try to copy /usr/hdp/<version>/hadoop/mapreduce.tar.gz to /hdp/apps/<version>/mapreduce/mapreduce.tar.gz

Mapreduce jobs will fail with the error if the HiveServer2 restart happens and the YARN applications in ACCEPTED state go to RUNNING during the exact same time when the mapreduce.tar.gz file copy happens.

But when WebHDFS is enabled, this problem will never occur as the copying is skipped by Ambari and we can see the below line.


{code:java}
2019-05-23 10:11:18,371 - DFS file /hdp/apps/2.6.5.0-292/mapreduce/mapreduce.tar.gz is identical to /usr/hdp/2.6.5.0-292/hadoop/mapreduce.tar.gz, skipping the copying
{code}

When WebHDFS is disabled in the cluster, then the above line is not printed when starting HiveServer2.


But when WebHDFS is not started it will just overwrite the mapreduce.tar.gz without asking

analysis : 

Looks issue with this part of code : https://github.com/apache/ambari/blob/4eee0f56d2fbfdfb0caace955339bc0c46a85a3c/contrib/fast-hdfs-resource/src/main/java/org/apache/ambari/fast_hdfs_resource/Runner.java#L131

https://github.com/apache/ambari/blob/4eee0f56d2fbfdfb0caace955339bc0c46a85a3c/contrib/fast-hdfs-resource/src/main/java/org/apache/ambari/fast_hdfs_resource/Resource.java#L236

we are just creating the file and overwriting it if exists.
We should do basic check if the file already exists of not before this copy operation and skip if file is same. 

This will save the time of starting hive-server2 and also abnormal failure of mapreduce jobs.


",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2019-05-24 10:30:37,3
13234567,"Ambari UI evaluates Javascript embedded in user input when adding hosts, adding remote clusters, and renaming the cluster","Ambari's UI evaluates Javascript blocks embedded in user input when adding hosts, adding remote clusters, and renaming the cluster.

The script evaluation appears to occur before the data is submitted and saved to the Ambari database (if save at all).  Therefore, no XSS vulnerability needs to be reported since the scope of the threat is only to the interactive user at the instance the data is evaluated.

*Add remote cluster steps to reproduce:*
# Log into ambari and navigate to admin > Manage Ambari> Cluster Management>  Remote Cluster > Register Remote Cluster
# Enter malicious script in Ambari Cluster URL textbox and click on save. The output of XSS is reflected. 

*Add hosts steps to reproduce:*
# Log into ambari and navigate to Hosts> Actions>  Add New Hosts
# Enter malicious script in Target Hosts textbox and click on save. The output of XSS is reflected

*Edit cluster name steps to reproduce:*
# Log into ambari and navigate to admin > Manage Ambari> Cluster Management>  Cluster Information
# Enter malicious script in Cluster Name textbox. The output of XSS is reflected",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2019-05-21 10:36:37,15
13234561,hbase hbck failing with Auth error,"For Secured HBase cluster brought up with Ambari, hbase hbck is failing with Authentication Error from Zookeeper
{code:java}
zookeeper.ClientCnxn: SASL configuration failed: javax.security.auth.login.LoginException: Zookeeper client cannot authenticate using the 'Client' section of the supplied JAAS configuration


DEBUG [main] ipc.RpcClientImpl - Stopping rpc client
Exception in thread ""main"" java.io.IOException: Unexpected ZooKeeper exception
at org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.getTableNames(TableLockManager.java:396)
at org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.visitAllLocks(TableLockManager.java:380)
at org.apache.hadoop.hbase.util.hbck.TableLockChecker.checkTableLocks(TableLockChecker.java:78)
at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixTableLocks(HBaseFsck.java:3510)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:782)
at org.apache.hadoop.hbase.util.HBaseFsck.exec(HBaseFsck.java:5035)
at org.apache.hadoop.hbase.util.HBaseFsck$HBaseFsckTool.run(HBaseFsck.java:4829)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:4817)
Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hbase/table-lock
at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1532)
at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getChildren(RecoverableZooKeeper.java:292)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.listChildrenNoWatch(ZKUtil.java:515)
at org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.getTableNames(TableLockManager.java:393)
{code}
 ",pull-request-available,"['ambari-server', 'stacks']",AMBARI,Bug,Critical,2019-05-21 10:19:38,12
13234519,Hive View API response contains plain-text password,Getting response from http://c7401:8080/api/v1/views/HIVE/versions/2.0.0/instances/AUTO_HIVE20_INSTANCE contains plain text password (hive.ranger.password). This only effects auto created views and default passwords.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-05-21 07:47:53,18
13234502,Improper error handling when managing Ambari users,"The application does not handle the error properly and reveals internal class names in the error
message as shown in the below HTTP Request and Response. This happens when an admin user
tries to add an LDAP user that doesn't exist to a group.

HTTP Request:
{code}
PUT /api/v1/groups/csrf%20test/members HTTP/1.1
Host: xyz601:8080
Content-Length: 69
Accept: application/json, text/plain, */*
Origin: http://xyz601:8080
X-Requested-By: ambari
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/70.0.3538.102 Safari/537.36
Content-Type: plain/text
Referer: http://xyz601:8080/views/ADMIN_VIEW/2.6.2.2/INSTANCE/
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9
Cookie: AMBARISESSIONID=nd54akraeumr1cmnz0gazantv
Connection: close
[{""MemberInfo/user_name"":""test"",""MemberInfo/group_name"":""csrf test""}]
{code}
HTTP Response:
{code}
HTTP/1.1 500 Internal Server Error
X-Frame-Options: DENY
Severity: Low
Status: New
Ease of Exploit: Easy
Classification: Improper Output Handling
Hadoop refresh (Break Glass) - UMF Visa Restricted 32
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Cache-Control: no-store
Pragma: no-cache
User: hitepate
Content-Type: text/plain
Connection: close
{
""status"" : 500,
""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal
system exception occurred: User test doesn't exist""
}
{code}

*Remediation Recommendations*
When errors occur, the site should respond with a specifically designed result that is helpful to the
user without revealing unnecessary internal details.
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-05-21 06:39:05,16
13234203,Move Ambari from using jackson-databind-asl and jackson-core-asl (v1) to latest jackson v2,Move away from jackson v1 to the latest jackson v2 components,pull-request-available,['ambari-sever'],AMBARI,Bug,Critical,2019-05-20 05:41:48,1
13233042,Fix aggregate metric in Kafka's Grafana dashboard,"The following metrics display aggregate numbers over time on Grafana (they are ever increasing). They should show timely rate instead.

The following metrics are affected:

*Kafka Home:*
{code}
kafka.server.BrokerTopicMetrics.BytesInPerSec.count
kafka.server.BrokerTopicMetrics.BytesOutPerSec.count
kafka.server.BrokerTopicMetrics.MessagesInPerSec.count
kafka.network.RequestMetrics.RequestsPerSec.request.Produce.count
kafka.network.RequestMetrics.RequestsPerSec.request.FetchConsumer.count
kafka.controller.ControllerStats.LeaderElectionRateAndTimeMs.count
kafka.controller.ControllerStats.UncleanLeaderElectionsPerSec.count
kafka.server.ReplicaManager.IsrShrinksPerSec.count
kafka.server.ReplicaManager.IsrExpandsPerSec.count
{code}

*Kafka Hosts:*
{code}
kafka.server.BrokerTopicMetrics.BytesInPerSec.count
kafka.server.BrokerTopicMetrics.BytesOutPerSec.count
kafka.server.BrokerTopicMetrics.MessagesInPerSec.count
kafka.network.RequestMetrics.RequestsPerSec.request.Produce.count
kafka.network.RequestMetrics.RequestsPerSec.request.FetchConsumer.count
kafka.server.ReplicaManager.IsrShrinksPerSec.count
kafka.server.ReplicaManager.IsrExpandsPerSec.count
{code}

*Kafka Topics*
{code}
kafka.server.BrokerTopicMetrics.BytesInPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.BytesOutPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.BytesInPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.MessagesInPerSec.topic.*.count
kafka.server.BrokerTopicMetrics.TotalProduceRequestsPerSec.topic.*.count
{code}

They are typically needed to be converted from something like this:
{code}
  ""targets"": [
    {
      ""aggregator"": ""avg"",
      ""alias"": ""ISR Shrinks"",
      ""app"": ""kafka_broker"",
      ""downsampleAggregator"": ""avg"",
      ""errors"": {},
      ""metric"": ""kafka.server.ReplicaManager.IsrShrinksPerSec.count"",
      ""precision"": ""default"",
      ""refId"": ""A"",
      ""transform"": ""none"",
      ""transformData"": ""none"",
      ""seriesAggregator"": ""none""
    }
  ],
{code}

to this:
{code}
  ""targets"": [
    {
      ""aggregator"": ""avg"",
      ""alias"": ""ISR Shrinks"",
      ""app"": ""kafka_broker"",
      ""downsampleAggregator"": ""avg"",
      ""errors"": {},
      ""metric"": ""kafka.server.ReplicaManager.IsrShrinksPerSec.count"",
      ""precision"": ""default"",
      ""refId"": ""A"",
      ""transform"": ""rate"",
      ""transformData"": ""none"",
      ""seriesAggregator"": ""none""
    }
  ],
{code}

with topics metrics, _aggregator_ must be changed to _none_ in addition.

The Grafana dashboards are stored in the stack definitions.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-05-13 18:46:58,20
13232924,Changes in Yarn Capacity Scheduler is requesting for restart of Resource Manager,"On YARN Queue Manager view only deleting queues requires Yarn Resource Manager restart.

Disable the ""Save Only"" menu item when unsaved deleted queues are exists on the view and in this case only ""Save and Restart Resource Manager"" is active.",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2019-05-13 07:27:44,16
13231723,Ambari-server 2.7.3 uninstall removes ambari-python-wrap even when agent is still installed,"There is a erroneous equal sign in the following expression:

{code}
AMBARI_AGENT_ROOT_DIR==""${ROOT}/usr/lib/ambari-agent""
{code}

in /var/lib/ambari-server/install-helper.sh. The double equal sign causes the AMBARI_AGENT_ROOT_DIR to be evaluated to ""=/usr/lib/ambari-agent"" (note the leading =).",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-05-06 08:58:42,18
13231712,Ambari UI default Ajax Timeout is 3 minutes but some operations in server can take more than that,"Problem Statement :  Ambari UI code has set timeout for all the AJAX calls as 3 minutes ( 180000 ms) , but some operations like Deploy Cluster, Save Configs after validation can take upto 3 minutes , which willl cause UI to stall the request after 3 minutes and further blocking the code flow.

Code reference : https://github.com/apache/ambari/blob/0a271d4a6ed22120b3a841b5c2ca800fcd8952cc/ambari-web/app/config.js#L35



Proposed Solution : increase the Ajax timeout to 5 minutes

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-05-06 07:50:48,3
13231671,Prepare Rolling Upgrade unit test methods overriding,test_namenode has same method signatures for prepare rolling upgrade unit test methods - one of which is using secured.json config file whereas the other is using ha_secured.json config.,pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-05-05 19:09:25,12
13231484,"Admin View build fails due to unavailable npm package ""ecstatic""","{{ambari-admin}} build started failing with:

{noformat:title=https://builds.apache.org/job/Ambari-branch-2.7/494/consoleText}
[ERROR] npm ERR! notarget No compatible version found: ecstatic@'>=0.4.0 <0.5.0'
[ERROR] npm ERR! notarget Valid install targets:
[ERROR] npm ERR! notarget [""4.1.2""]
...
[ERROR] npm ERR! notarget It was specified as a dependency of 'http-server'
{noformat}

It seems old versions of [ecstatic|https://www.npmjs.com/package/ecstatic?activeTab=versions] have been [removed|https://github.com/jfhbrook/node-ecstatic/issues/255] due to a security issue.",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2019-05-03 16:58:04,21
13231418,Hive Server Interactive process alert triggered in HA setup,"With 2 HSI instances deployed in HA setup the _HiveServer2 Interactive Process_ alert is triggered for the inactive one:

{noformat}
Connecting to jdbc:hive2://...:10501/;transportMode=http;httpPath=cliservice
19/05/02 09:13:11 [main]: WARN jdbc.HiveConnection: Failed to connect to ...:10501
Error: Could not open client transport with JDBC Uri: jdbc:hive2://...:10501/;transportMode=http;httpPath=cliservice: Cannot open sessions on an inactive HS2 instance; use service discovery to connect (state=08S01,code=0)
Cannot run commands specified using -e. No current connection
{noformat}",pull-request-available,[],AMBARI,Bug,Major,2019-05-03 09:10:27,21
13230853,implement configurable password policy for Ambari users,"We need to add support for user configurable password policies. 

The following needs to be supported (ideally via regexp)

• Minimum length=15
• Must not contain account name or user name.
• Must contain at least three out of the following four character types
• numeric character
• lower case alphabetic character
• upper case alphabetic character
• punctuation/special symbol",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2019-04-30 09:15:07,13
13230242,fix test failures on branch-2.5,"Fix test failures on branch-2.5: 

build log at: [https://builds.apache.org/job/Ambari-branch-2.5/1856/consoleFull]
 
{code:java}
Failed tests: 
  AmbariManagementControllerTest.testServiceUpdateRecursive:3451 expected:<3> but was:<1>
  RoleCommandOrderTest.testInitializeAtGLUSTERFSCluster:104 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
  RoleCommandOrderTest.testInitializeAtHDFSCluster:151 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
  RoleCommandOrderTest.testInitializeAtHaHDFSCluster:195 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
  RoleCommandOrderTest.testInitializeAtHaRMCluster:241 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
  RoleCommandOrderTest.testInitializeDefault:387 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
  RoleCommandOrderTest.testMissingRestartDependenciesAdded:293 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
  RoleCommandOrderTest.testOverride:475 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getCurrentStackVersion(): expected: at least 1, actual: 0
  RoleCommandOrderTest.testTransitiveServices:432 
  Unexpected method call ClusterImpl.getDesiredStackVersion():
    ClusterImpl.getService(""HBASE""): expected: at least 1, actual: 0
    ClusterImpl.getServices(): expected: at least 1, actual: 0
    ClusterImpl.getCurrentStackVersion(): expected: 1, actual: 0
Tests in error: 
  RoleGraphTest.testGetOrderedHostRoleCommands:168 » NullPointer
  RoleGraphTest.testValidateOrder:81 » NullPointer
  TestStagePlanner.testDependencyOrderedStageCreate:205 » NullPointer
  TestStagePlanner.testManyStages:154 » NullPointer
  TestStagePlanner.testMultiStagePlan:100 » NullPointer
  TestStagePlanner.testRestartStagePlan:125 » NullPointer
  TestStagePlanner.testSingleStagePlan:81 » NullPointer{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-04-26 08:26:54,8
13229346,Service checks mentioned in HOU upgrade plan are run on the same node that was upgraded,"Now service checks are performed for any appropriate host during HOU. We should tune behaviour without API change to add some host selection restrictions.
{code:java}
[

{ ""hosts"": [""host1""], ""service_checks"": [""KAFKA"", ""ZOOKEEPER""] }
,

{ ""hosts"": [""host2""], ""service_checks"": [""ZOOKEEPER""] }
,

{ ""hosts"": [""host3"", ""host4""], ""service_checks"": [""CUSTOMSERVICE""] }
]
{code}
 - Each service check will be performed for all hosts from hosts section. e.g. CUSTOMSERVICE check will be run on both host3 and host4
 - In case there is no service component on proposed host the upgrade request will be failed. This means that user should change upgrade plan and post it again.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-04-22 09:24:58,6
13228860,The GPL repository field not appearing when adding a new repository,"The GPL repo entry is not present on the UI on the Versions page when you add a new OS.
This is an Ambari 2.6 with HDP-2.6.5.0. On Ambari 2.7 there is no such issue.
 * Install Ambari and *accept GPL license*.
 * Install a HDP version e.g. with [http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos6/2.x/BUILDS/2.6.5.0-292/HDP-2.6.5.0-292.xml]
 * Go to ""Stacks and Versions/Versions/Manage Versions/"".
 * On the ""Versions"" page select the link of the version (HDP-2.6.5.0)
 * On the ""Versions / HDP-2.6.5.0"" page Add an OS that is not in the VDF file
 * The input text box for ""HDP-2.6-GPL"" repo is missing.

The issue is also present on the ""Versions / Register Version"" page:
 * Go to ""Stacks and Versions/Versions/Manage Versions/Register Version"".
 * The latest stack version is HDP-2.6.5.1100.
 * Add ""redhat6"" OS. This OS is not described in the VDF file.
 * The input text box for ""HDP-2.6-GPL"" repo is missing.",ambari pull-request-available vdf,['ambari-admin'],AMBARI,Bug,Major,2019-04-18 13:07:43,7
13228838,upgrade moment.js to v2.22.2,The application is running a vulnerable version of Moment.js.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-04-18 11:19:32,15
13228348,Missed support for abfs protocol,"There are still present couple of places that require extending default protocol set with abfs protocol:
 - Pig and File views check *webhdfs.url* property, which contains protocol name, but only for non-local view's configuration. List of allowed protocols is loaded from ambari.properties.
 - ranger hive plugin property *ranger.plugin.hive.urlauth.filesystem.schemes*. We set default value during stack upgrade to HDP 2.6 stack.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-04-16 13:36:40,6
13228316,Disable directory Indexing at /resources,"GET /resources gives back the directory content of /var/lib/ambari-server/resources. The directory doesn't contain any sensitive information, only files which are already visible on github. But it might freak out security guys therefore it's best to disable the listing.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-04-16 10:49:11,18
13228253,Blueprint processor should support multiple ZooKeeper nodes for livy.server.recovery.state-store.url,"Livy can store its state in ZooKeeper for recovery ({{""livy.server.recovery.state-store"": ""zookeeper""}}).  In this case {{livy.server.recovery.state-store.url}} should point to the ZooKeeper quorum.

Setting {{""livy.server.recovery.state-store.url"": ""%HOSTGROUP::quorum%:2181""}} should be enough to specify ZK servers in a host group named {{quorum}}.  However, since it is not explicitly handled in {{BlueprintConfigurationProcessor}}, the placeholder is replaced only with a single address.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2019-04-16 06:19:53,21
13228139,Ambari UI quicklinks,"Ambari quicklinks for Hive (see the attached screenshots) shows a link to Grafana. Grafana is not installed, it is not part of the stack.

Ambari quicklinks for Jupyter has no links",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2019-04-15 15:32:58,9
13228123,Hive service check is failing after moving Hive Metastore from node to another using system tests.,hive-interactive-site/hive.metastore.uris is not updated after move of Hive Metastore via wizard,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2019-04-15 14:48:19,15
13228079,"CLONE - Ambari audit log shows ""null"" user when executing an API call as admin - Ambari 2.5.2","When running a simple REST API call from CLI, I could see two entries in ambari-audit.log file.

 

Following is my API call:

{{curl -k -i -u admin:<passwd> -H ""X-Requested-By: ambari"" -X GET [http://<ambari-host>:8080/api/v1/clusters|http://saurabh-ambari:8080/api/v1/clusters]}}

 

Following are the 2 entries in ambari-audit.log:
{quote}2019-04-08T10:19:04.991Z, User(null), RemoteIp(x.x.x.x), Operation(User login), Roles(
 ), Status(Failed), Reason(Authentication required), Consecutive failures(UNKNOWN USER)
 2019-04-08T10:19:04.999Z, User(admin), RemoteIp(x.x.x.x), Operation(User login), Roles(
     Ambari: Ambari Administrator
 ), Status(Success)
{quote}
 

The second line seems to be valid. However, the first line (with the null user) shouldn't be there.

Note: I'm not sure if it helps, but the cluster is Kerberized and Knox isn't involved.

 

Edit: This issue could be seen on both Ambari 2.5.2 and 2.7.3. Also, 2.5.2 version cluster is Kerberized, the 2.7.3 version is NOT Kerberized. ",newbie pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-04-15 11:45:03,16
13228045,Add module based dependency with custom version for the excluded transitiend modules,Add module based dependency with custom version for the excluded transitiend modules,pull-request-available,['ambari-sever'],AMBARI,Bug,Blocker,2019-04-15 09:10:26,1
13227719,Rolling Upgrade support for rack based hosts,"Ambari managed Rolling Upgrade has batch size concept with which multiple hosts are upgraded in parallel. e.g. slave/client components are restarted on multiple hosts in parallel.

However, we should provide support for parallel hosts upgrade that exist on single rack. If client provides location of rack to hosts mapping file location as part of cluster-env property, we should read the mapping and proceed with parallel Rolling Restart on multiple hosts on single rack.

This can significantly improve resiliency and availability as slave/client components that don't belong to single rack won't be restarted in parallel.",pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2019-04-12 14:30:10,12
13227694,Tez/MR service check fails with ClassNotFoundException LzoCodec during host ordered upgrade,"ClassPath used by on the client side is: /usr/hdp/<NEW.HDP.VERSION>/hadoop/lib/hadoop-lzo-0.6.0.${NEW.HDP.VERSION}.jar

LzoPackage is installed lazily therefore it's not available at the server side.",pull-request-available,[],AMBARI,Task,Major,2019-04-12 12:02:35,18
13227534,Dynamically update Rolling Upgrade batch size,"We have a default value 100 for Rolling Upgrade batch size. In order to modify that size, upgrade.xml should have <max-degree-of-parallelism> or stack should define it.

However, we can provide a way to update the batch size dynamically using some config that can be updated by client using Rest API and subsequent Rolling Upgrade can get the updated batch size from the config.

Before starting off with Rolling Upgrade, we can update ""cluster-env"" config with property key: ""max_degree_parallelism"". If the value of this key is integer, that value will be selected as Batch Size. If not, then the default behavior will continue.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-04-11 19:40:17,12
13226921,Host Ordered Upgrade: Pre Upgrade check of fs.defaultFS fails for ABFS,"# Install cluster with Ambari- 2.6.2.26 + HDP-2.6.5.3006
 # Set abfs protocol for mapreduce.application.framework.path/fs.defaultFS/tez.lib.uris properties.
 # Disable Service Auto Start
 # Register and Install stack HDP-2.6.5.3009
 # Run preupgrade checks. They fail with
{code:java}
MapReduce should reference Hadoop libraries from the distributed cache in HDFS
Reason: The mapred-site.xml property mapreduce.application.framework.path or the core-site.xml property fs.defaultFS should point to *dfs:/ url.
Failed on: MAPREDUCE2 Tez should reference Hadoop libraries from the distributed cache in HDFS
Reason: The tez-site.xml property tez.lib.uris or the core-site.xml property fs.defaultFS should point to *dfs:/ url.
Failed on: TEZ
{code}
For ABFS, fs.defaultFS is
{code:java}
abfs://<hostname>
{code}
Not seeing this issue for WASB where the value was
{code:java}
wasb://<hostname>
{code}
Also, we need to check if there are other places where we have this check, that will break too",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-04-09 11:30:16,6
13226920,Add a sysprep configurations to run conf-selects only a single time	,"Background:

On the cluster with 5-6 nodes conf-select cycles takes 3 seconds. Trying to create conf-dir for every single components. Which makes install stages like ~25-30seconds for hosts with a lot of clients.

Usually we run conf-select during every task. Since with every single install new packages might appear and new conf-select links will be required.

Solution:

If host_sysprep is true, run conf-select cycle only during first task and than create a file indicating this was done, so other tasks skip.

Since all packages are preinstalled this should work fine as during first single run it will be possible to create all links.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-04-09 11:20:53,6
13226666,"Ambari audit log shows ""null"" user when executing an API call as admin - Ambari 2.7.3","When running a simple REST API call from CLI, I could see two entries in ambari-audit.log file.

 

Following is my API call:

{{curl -k -i -u admin:<passwd> -H ""X-Requested-By: ambari"" -X GET [http://<ambari-host>:8080/api/v1/clusters|http://saurabh-ambari:8080/api/v1/clusters]}}

 

Following are the 2 entries in ambari-audit.log:
{quote}2019-04-08T10:19:04.991Z, User(null), RemoteIp(x.x.x.x), Operation(User login), Roles(
 ), Status(Failed), Reason(Authentication required), Consecutive failures(UNKNOWN USER)
 2019-04-08T10:19:04.999Z, User(admin), RemoteIp(x.x.x.x), Operation(User login), Roles(
     Ambari: Ambari Administrator
 ), Status(Success)
{quote}
 

The second line seems to be valid. However, the first line (with the null user) shouldn't be there.

Note: I'm not sure if it helps, but the cluster is Kerberized and Knox isn't involved.

 

Edit: This issue could be seen on both Ambari 2.5.2 and 2.7.3. Also, 2.5.2 version cluster is Kerberized, the 2.7.3 version is NOT Kerberized. ",newbie pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-04-08 10:21:50,16
13226534,DB consistency check to provide error cause in console output,"When Ambari Server starts, DB consistency checks are performed and for exception, we print DB_CHECK_ERROR. However, we should provide more info about what caused the failure.",pull-request-available,[],AMBARI,Bug,Major,2019-04-06 19:18:13,12
13226524,Decommission/Recommission workflow to refresh HDFS client,"During Decommission/Recommission workflow for DataNode, even though we change include/exclude files and apply refreshNodes command, sometimes NameNode UI still shows DataNodes in old status.

If we run configure action for NameNode hosts, NameNode correctly detects decommissioned/commissioned(live/dead) DataNodes.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-04-06 17:16:39,12
13226503,Update deprecated refreshNodes command with HDFS,"Hadoop refreshNodes command is under deprecation. We should start using new hdfs commands.

 

Upon executing hadoop command, we get the message:

*DEPRECATED: Use of this script to execute hdfs command is deprecated.*

*Instead use the hdfs command for it.*",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-04-06 12:50:58,12
13226322,Improve error handling in fast hdfs tool,"There are cases when the fast hdfs tool swallows the original exception an exits with an NPE hiding the original issue. The original exception is logged to the standard out but that may not be captured by calling tools.

Fixing exception handling would not affect normal tool execution but help finding the issue in error cases.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-04-05 08:40:29,20
13225723,add a second HiveServer2Interactive Button Missing in Ambari UI is there is config group for Hive Service,"*Problem Statement : * add a second HiveServer2Interactive Button Missing in Ambari UI is there is config group for Hive Service .

The UI console will show the following Error : 


{code:java}
app.js:78526 Uncaught TypeError: Cannot read property 'properties' of undefined
    at Class.onLoadHiveConfigs (app.js:78526)
    at Class.opt.success (app.js:193111)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-04-03 07:50:07,3
13225573,Unable to move Hive metastore from one node to another ,"While moving Hive Metastore from one node to another, it is failing at test DB connection step in configure Component page.Seeing following error in ambari-agent logs:

{noformat}19-03-22 11:15:38,527 - DB connection check started.
2019-03-22 11:15:38,527 - There was an unknown error while checking database connectivity: Configuration parameter 'db_name' was not found in configurations dictionary!
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py"", line 145, in actionexecute
    db_connection_check_structured_output = self.execute_db_connection_check(config, tmp_dir)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py"", line 281, in execute_db_connection_check
    if db_name == DB_MYSQL:
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'db_name' was not found in configurations dictionary!
2019-03-22 11:15:38,528 - Host checks completed.
2019-03-22 11:15:38,529 - Check db_connection_check was unsuccessful. Exit code: 1. Message: Configuration parameter 'db_name' was not found in configurations dictionary!

Command failed after 1 tries
{noformat}

Reproduce steps:
# Goto Hive Service.
# Click on Actions tab and select Move Hive Metastore.
# Fill appropriate values in Move Wizard.
# Before going to configure component page from Review page, please run following ambari-server command on ambari server host:
{{ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar}}
# Click next on review page, it will fail at test DB connection step in Configure component page.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2019-04-02 15:59:30,15
13225463,tproxy-enabled flag is not used by ambari-server setup-trusted-proxy,"When running

ambari-server setup-trusted-proxy --tproxy-enabled=true

ambari still asks the user interactively about enabling/disabling tproxy

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-04-02 10:40:21,18
13225461,Zeppelin Restart via Ambari Changes Permissions on log folder which causes Permission issue for impersonated users,"Problem Statement : I am Using User impersonation for some of interpreters in zeppelin , now that the interpreters write their own logs to the zeppelin log directory. This becomes impossible if the users do not have permissions to write to the zeppelin log directory. This is because while starting zeppelin the Ambari changes the permission of zeppelin log directory to 0755 which makes only Zeppelin user having having write permission in log directory.

expectation with user impersonation is, all the impersonating users will belong to hadoop group (which is same as zeppelin, zeppelin also belong to hadoop) . So Zeppelin Startup script should give 0775 Permission to /var/log/zeppelin folder

https://github.com/apache/ambari/blob/release-2.6.2/ambari-server/src/main/resources/common-services/ZEPPELIN/0.7.0/package/scripts/master.py#L104",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-04-02 10:09:36,3
13225015,Provide an option to print Hadoop Topology,"Ambari stores rack info for hosts in DB based on ClusterTopology data received in Blueprint request. However, it is not necessary that blueprint would contain correct data. After commission/decommission operation, topology should contain updated rack based topology.

Hence, we should provide an option in UI to print rack based datanodes as a topology.",pull-request-available,[],AMBARI,Task,Major,2019-03-30 19:49:22,12
13224537,Updating com.google.guava:guava version,"Due to security issues with currently used version of lib, used version should be updated.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2019-03-28 09:07:56,1
13224502,Updating commons-codec:commons-codec version,"Due to security issues with currently used version of lib, used version should be updated.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2019-03-28 07:12:02,1
13224501,Updating com.fasterxml.jackson.core:jackson-core version,"Due to security issues with currently used version of lib, used version should be updated.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2019-03-28 07:10:15,1
13224138,Moving resource manager does not update yarn.resourcemanager.address property,"When Resource Manager is move in a Yarn HA cluster (maybe without HA too), the *yarn.resourcemanager.address.<suffix>* property is not updated unlike others (e.g. *yarn.resourcemanager.hostname.<suffix>* or *yarn.resourcemanager.webapp.address.<suffix>*).

As a result, the moved resource manager stops after a while with an execption in the log:
{code}
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Invalid configuration! Can not find valid RM_HA_ID. None of yarn.resourcemanager.address.rm1 yarn.resourcemanager.address.rm2  are matching the local address OR yarn.resourcemanager.ha.id is not specified in HA Configuration
    at org.apache.hadoop.yarn.conf.HAUtil.throwBadConfigurationException(HAUtil.java:45)
    at org.apache.hadoop.yarn.conf.HAUtil.verifyAndSetCurrentRMHAId(HAUtil.java:151)
    at org.apache.hadoop.yarn.conf.HAUtil.verifyAndSetConfiguration(HAUtil.java:106)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:276)
    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1512)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-03-26 20:49:45,7
13223923,configs.py  output and Ambari API output of config properties are not in same sorted order.,"The output of the API call and the Config.py call should look identical. 

However ""Config.py"" is not returning the properties in the *Same Sorted Order* which Ambari APIs are actually returning. 
{code}
#  /var/lib/ambari-server/resources/scripts/configs.py --user=admin --password=admin --port=8080 --action=get --host=localhost --cluster=KerLatest --config-type=hive-site -f /tmp/hive-site-using-ConfigPy.json
{code}

However when the same is accessed via the Ambari API calls then the properties are in Sorted Order:
{code}
# curl -u admin:admin -H ""X-Requested-By: ambari"" -X GET ""http://kerlatest1.example.com:8080/api/v1/clusters/KerLatest/configurations?type=hive-site&tag=version1550835190608""  -o /tmp/hive-site-using-AmbariAPI.json
{code}


Ambari API returns the config properties inside JSON file in sorted order. But ""configs.py"" jumbles those properties. Ideally configs.py output should be in sync of API JSON response.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-26 01:57:23,22
13223693,ONEFS installation via blueprint fails,"error message comes from SingleHostTopologyUpdater

{code}
Logical Request: Provision Cluster 'hadoop5832' FAILED: Unable to update configuration property 'fs.defaultFS' with topology information. Component 'NAMENODE' is mapped to an invalid number of host groups '0'.
{code}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-03-25 08:19:59,18
13223512,Bulk Operation Refresh Configs for selected hosts,There are multiple requirements of performing Refresh Configs after some config update or as a recovery from failures. This operation should be provided as a bulk operation on selective hosts so that all client components can be refreshed on all selected hosts.,pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Improvement,Major,2019-03-23 15:33:23,12
13223506,Provide a way to setup HBase Replication,"Ambari should provide an API level implementation to enable replication between 2 HBase clusters.

In HBase, cluster replication refers to keeping one cluster state synchronized with that of another cluster, using the write-ahead log (WAL) of the source cluster to propagate the changes. Replication is enabled at column family granularity.

As part of this JIRA, we can provide functionality to add/remove peer cluster keys to existing HBase cluster.

 ",pull-request-available,['ambari-server'],AMBARI,New Feature,Critical,2019-03-23 14:51:45,12
13223274,The Ubuntu repository id for the cached apt package list is generated wrong in case if were used URL with https protocol,"When user would try to install files from the repository, which pointing to target location using https protocol, Ambari will fail to generate proper Repository Id for the cached package list of apt manager.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-03-22 08:02:39,1
13222999,HDFS NameSpace Widget Threshold Values  Are Not Updated after Change,"Assume that HDFS NameNode federation is enabled.
STR:
1. Open Dashboard ->METRICS and scroll to the NAMESPACE.
2. Edit any widget, change threshhold value and click apply buttton.
3. Open the widget again and find threshhold value is not updated.
Details see the attached pictures",pull-request-available,[],AMBARI,Improvement,Major,2019-03-21 05:43:37,23
13222987,hdfs_to_onefs_convert.py script missing HDP3.1 + upgrade logic,"hdfs to onefs convert script used to upgrade HDP2.6.5 to HDP3.0.1 stack is missing HDP3.1.* stack support. Please add HDP3.1.* support as well make it generic for all future stacks.

!hdfs to onefs issue.jpg!",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-21 02:57:35,18
13222943,Ambari returns stack trace in HTML doc when an error occurs retrieving details for an Ambari View resource that does not exist,"Ambari returns stack trace in HTML doc when an error occurs retrieving details for an *Ambari View* resource that does not exist.

{noformat}
curl -u admin:admin -X GET http://localhost:8080/api/v1/views/CAPACITY-SCHEDULER/versions/1.0.0/instances/AUTO_CS_INSTANCE/bad_resource
{noformat}

{noformat}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /api/v1/views/CAPACITY-SCHEDULER/versions/1.0.0/instances/AUTO_CS_INSTANCE/bad_resource. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>java.lang.IllegalArgumentException: A resource type bad_resource for view instance CAPACITY-SCHEDULER/AUTO_CS_INSTANCE can not be found.
	at org.apache.ambari.server.api.services.views.ViewInstanceService.getResourceHandler(ViewInstanceService.java:326)
	at sun.reflect.GeneratedMethodAccessor375.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
...
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-20 21:12:30,13
13222663,recoverHost operation fails if public hostname is different from actual hostname,"If public hostname is different from the actual hostname then operation would fail with below error.


{noformat}
2019-03-19 20:08:43,293 ERROR [ambari-client-thread-50] AmbariManagementControllerImpl:1284 - Host not found
org.apache.ambari.server.HostNotFoundException: Host not found, hostname=c3111-node2.hdp.com.pub
        at org.apache.ambari.server.state.cluster.ClustersImpl.getClustersForHost(ClustersImpl.java:443)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.getHostComponents(AmbariManagementControllerImpl.java:1277)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.getHostComponents(AmbariManagementControllerImpl.java:3812)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.getHostComponents(AmbariManagementControllerImpl.java:3801)
        at org.apache.ambari.server.controller.internal.HostComponentResourceProvider$2.invoke(HostComponentResourceProvider.java:260)
        at org.apache.ambari.server.controller.internal.HostComponentResourceProvider$2.invoke(HostComponentResourceProvider.java:257)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.getResources(AbstractResourceProvider.java:317)
        at org.apache.ambari.server.controller.internal.HostComponentResourceProvider.findResources(HostComponentResourceProvider.java:257)
        at org.apache.ambari.server.controller.internal.HostComponentResourceProvider.getResourcesForUpdate(HostComponentResourceProvider.java:245)
        at org.apache.ambari.server.controller.internal.HostComponentResourceProvider.doUpdateResources(HostComponentResourceProvider.java:820)
        at org.apache.ambari.server.controller.internal.HostComponentResourceProvider.updateResources(HostComponentResourceProvider.java:320)
        at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:317)
        at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.update(PersistenceManagerImpl.java:125)
        at org.apache.ambari.server.api.handlers.UpdateHandler.persist(UpdateHandler.java:46)
        at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
        at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
        at org.apache.ambari.server.api.services.HostComponentService.updateHostComponents(HostComponentService.java:291)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:291)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
{noformat}


Issue: UI is sending publichostname in the request instead of using the actual hostname. 


{noformat}
{""RequestSchedule"":{""batch"":[{""requests"":[{""order_id"":1,""type"":""PUT"",""uri"":""/clusters/c3111/hosts/c3111-node2.hdp.com.pub/host_components"",""RequestBodyInfo"":{""RequestInfo"":{""context"":""Init All Host Components"",""operation_level"":{""level"":""HOST"",""cluster_name"":""c3111"",""host_name"":""c3111-node2.hdp.com.pub""},""query"":""HostRoles/component_name.in(APP_TIMELINE_SERVER,DATANODE,HDFS_CLIENT,HISTORYSERVER,MAPREDUCE2_CLIENT,NAMENODE,NODEMANAGER,RESOURCEMANAGER,TIMELINE_READER,YARN_CLIENT,ZOOKEEPER_CLIENT,ZOOKEEPER_SERVER)""},""Body"":{""HostRoles"":{""state"":""INIT""}}}},{""order_id"":2,""type"":""PUT"",""uri"":""/clusters/c3111/hosts/c3111-node2.hdp.com.pub/host_components"",""RequestBodyInfo"":{""RequestInfo"":{""context"":""Install All Host Components"",""operation_level"":{""level"":""HOST"",""cluster_name"":""c3111"",""host_name"":""c3111-node2.hdp.com.pub""},""query"":""HostRoles/component_name.in(APP_TIMELINE_SERVER,DATANODE,HDFS_CLIENT,HISTORYSERVER,MAPREDUCE2_CLIENT,NAMENODE,NODEMANAGER,RESOURCEMANAGER,TIMELINE_READER,YARN_CLIENT,ZOOKEEPER_CLIENT,ZOOKEEPER_SERVER)""},""Body"":{""HostRoles"":{""state"":""INSTALLED""}}}},{""order_id"":3,""type"":""PUT"",""uri"":""/clusters/c3111"",""RequestBodyInfo"":{""RequestInfo"":{""context"":""Regenerate keytabs"",""query"":""regenerate_keytabs=all&regenerate_hosts=c3111-node2.hdp.com.pub&config_update_policy=none""},""Body"":{""Clusters"":{""security_type"":""KERBEROS""}}}}]},{""batch_settings"":{""batch_separation_in_seconds"":1,""task_failure_tolerance"":0}}]}}]
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-03-19 20:15:20,14
13222628,Updating a user's password does not validate the administrator's current password.,"When updating a user's password as an Ambari Administrator via the UI, the acting administrator user is prompted for their password...

 !image-2019-03-19-12-18-59-062.png! 

The password is never validated by the backend to end sure it is correct for the acting user.  Either the text box needs to be removed from the UI, or the backend should verify the administrator's password before changing the user's password. 

The current implementation does require that if the acting user is not an Ambari Administrator user, the acting user may only change their own password and must supply their current password as well as the new password:

Example: 
{noformat}
curl -H ""X-Requested-By:ambari""  -u user_a:hadoop -X PUT -d '{ ""Users"" : { ""old_password"" : ""hadoop"", ""password"" : ""hadoop_1234"" } }' http://localhost:8080/api/v1/users/user_a
{noformat}

",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2019-03-19 16:19:07,20
13222604,Ambari returns stack trace in HTML doc when an error occurs retrieving details for a user resource that does not exist,"
Ambari returns stack trace in HTML doc when an error occurs retrieving details for a user resource that does not exist.  The acting user must be an Ambari Administrator:

{noformat}
curl -u admin:admin -X GET http://localhost:8080/api/v1/users/doesnotexist/authorizations?fields=*
{noformat}
{noformat}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /api/v1/users/anyotheruser/authorizations. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>com.google.common.cache.CacheLoader$InvalidCacheLoadException: CacheLoader returned null for key anyotheruser.
at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2350)
at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2320)
at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)
at org.apache.ambari.server.controller.internal.UserPrivilegeResourceProvider.getResources(UserPrivilegeResourceProvider.java:282)
...
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-03-19 14:45:54,13
13222277,"Ambari does not update ""dfs.namenode.lifeline.rpc-address"" during Namenode move operation","While performing Namenode (NN) move from one host to another. And if ""dfs.namenode.lifeline.rpc-address"" property is set., during the NN move, we see that the value associated with the property ""dfs.namenode.lifeline.rpc-address"" that is being moved is not updated. Hence, NN would fail to start during restart stage of the NN Move operation.

This can be reproduced as below,

1. Have a HDFS HA cluster. 
2. Make sure ""dfs.namenode.lifeline.rpc-address"" is set for both the NNs.
3. Perform the NN move operation from any NN to new host.
4. Observe NN failure to startup during restart phase",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-03-18 10:41:21,18
13222274,MYSQL connector exception while upgrading ambari to ambari-2.7.3,"h2. Description
Problem statement : ambari-server upgrade command is failing with below exception in ambari-server.log If using mysqlconnector-8.0.1.x.jar
{code:java}
2018-12-28 13:59:07,062  INFO [main] DBAccessorImpl:869 - Executing query: CREATE TABLE ambari_configuration (category_name VARCHAR(100) NOT NULL, property_name VARCHAR(100) NOT NULL, property_value VARCHAR(2048)) ENGINE=INNODB
2018-12-28 13:59:07,087 ERROR [main] SchemaUpgradeHelper:209 - Upgrade failed.
java.sql.SQLSyntaxErrorException: Unknown table 'ambari_configuration' in information_schema
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
  at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
  at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1218)
  at com.mysql.cj.jdbc.DatabaseMetaData$7.forEach(DatabaseMetaData.java:2965)
  at com.mysql.cj.jdbc.DatabaseMetaData$7.forEach(DatabaseMetaData.java:2953)
  at com.mysql.cj.jdbc.IterateBlock.doForAll(IterateBlock.java:56)
  at com.mysql.cj.jdbc.DatabaseMetaData.getPrimaryKeys(DatabaseMetaData.java:3006)
  at org.apache.ambari.server.orm.DBAccessorImpl.tableHasPrimaryKey(DBAccessorImpl.java:1086)
  at org.apache.ambari.server.orm.DBAccessorImpl.addPKConstraint(DBAccessorImpl.java:577)
  at org.apache.ambari.server.orm.DBAccessorImpl.addPKConstraint(DBAccessorImpl.java:588)
  at org.apache.ambari.server.upgrade.UpgradeCatalog270.addAmbariConfigurationTable(UpgradeCatalog270.java:989)
  at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:319)
  at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:970)
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:207)
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
2018-12-28 13:59:07,093 ERROR [main] SchemaUpgradeHelper:475 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: Unknown table 'ambari_configuration' in information_schema
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:210)
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
Caused by: java.sql.SQLSyntaxErrorException: Unknown table 'ambari_configuration' in information_schema
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
  at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
{code}
Supportmatrix says ambari supports mysql-5.7 version. but its not mentioning anything about connector jar or connector.jar version.

Currently there is this exception in Ambari upgrade is mysqlConnector jar version is 8.0.15 .

If ambari doesnt support mysqlConnector.jar version 8.x this should be documented in hortonworks docs and also in supportmatrix website.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-18 10:34:18,6
13221975,NullPointerException if there is no proper HDP/HDF repo version folder,"Ambari server fails with ""NullPointerException"" if cluster is pointing to specific HDP/HDF/xxx
and if that folder does not exist in resources folders.
Should do NULL check and then print proper meaning full error message so that customer's can install/correct that.


{noformat}
15 Aug 2017 15:54:02,685  INFO [Stack Version Loading Thread] LatestRepoCallable:80 - Loading latest URL info for stack HDP-2.3 from http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json
15 Aug 2017 15:54:02,691 ERROR [main] AmbariServer:929 - Failed to run the Ambari Server
org.apache.ambari.server.AmbariException: An error occured during updating current repository versions with stack repositories.
	at org.apache.ambari.server.stack.UpdateActiveRepoVersionOnStartup.process(UpdateActiveRepoVersionOnStartup.java:108)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
	at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:921)
Caused by: java.lang.NullPointerException
	at org.apache.ambari.server.stack.UpdateActiveRepoVersionOnStartup.process(UpdateActiveRepoVersionOnStartup.java:94)
	... 2 more
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-15 18:00:46,14
13221909,HDFS Service Checking with an ClassNotFound Error in Ambari WFManager view,"Below is the error that is coming with the API Call

[/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFD/resources/proxy/hdfsCheck|https://ambviews-celws-igtc-143841.southeastasia.cloudapp.azure.com/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFD/resources/proxy/hdfsCheck]
{code:java}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFD/resources/proxy/hdfsCheck. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>javax.servlet.ServletException: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ajax/JSON$Convertor
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:420)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:291)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:215)
	at org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilterInternal(AmbariBasicAuthenticationFilter.java:139)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:123)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
{code}",pull-request-available,['ambari-views'],AMBARI,Bug,Blocker,2019-03-15 13:07:08,6
13221769,Increase the Agent cert validity to 3 years,"By Default Ambari generates Agent certificates with 1 year validity. with in 1 year of Ambari installation these certificates would expire and agent would stop heart beating. Freshly re-setting issues the certificates would be tedious task. 

so increasing it to 3 years would be ideal as these are internal certs.",pull-request-available,[],AMBARI,Improvement,Major,2019-03-14 22:58:08,14
13221085,Ambari server setup failed with postgres connectivity error when using postgres 9.3,"INFO 2019-03-04 17:45:49,694 serverSetup.py:1088 - Setup ambari-server.
ERROR 2019-03-04 17:45:50,498 ambari-server.py:901 - 'Fatal exception: Unable to start PostgreSQL server. Exiting, exit code 4'
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 884, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1145, in setup
    _setup_database(options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 958, in _setup_database
    dbmsAmbari.setup_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 150, in setup_database
    self._setup_local_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 474, in _setup_local_database
    raise FatalException(retcode, err)
FatalException: 'Fatal exception: Unable to start PostgreSQL server. Exiting, exit code 4'
INFO 2019-03-04 17:45:51,001 ambari-server.py:798 - loglevel=logging.INFO
INFO 2019-03-04 17:45:51,003 serverSetup.py:1208 - Reset ambari-server.
ERROR 2019-03-04 17:45:51,036 ambari-server.py:901 - 'Fatal exception: could not change directory to ""/root"": Permission denied\npsql: could not connect to server: No such file or directory\n\tIs the server running locally and accepting\n\tconnections on Unix domain socket ""/tmp/.s.PGSQL.5432""?\n, exit code 1'
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 884, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1236, in reset
    _reset_database(options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 999, in _reset_database
    dbmsAmbari.reset_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 157, in reset_database
    self._reset_local_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 513, in _reset_local_database
    raise FatalException(1, drop_errdata)
FatalException: 'Fatal exception: could not change directory to ""/root"": Permission denied\npsql: could not connect to server: No such file or directory\n\tIs the server running locally and accepting\n\tconnections on Unix domain socket ""/tmp/.s.PGSQL.5432""?\n, exit code 1'
INFO 2019-03-04 17:45:53,854 ambari-server.py:798 - loglevel=logging.INFO
INFO 2019-03-04 17:45:53,856 ambari-server.py:161 - Stopping ambari-server.
INFO 2019-03-04 17:45:53,857 ambari-server.py:190 - Ambari Server is not running",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-03-12 10:59:45,18
13220960,Enable livy.server.access-control.enabled by default,...,pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2019-03-11 23:05:35,24
13220851,Kerberos Client is unnecessarily installed via Blueprints when kerberos-env/kdc-type is none,"The Kerberos Client component is unnecessarily installed via Blueprints when \{{kerberos-env/kdc-type}} is ""none"". 

The Blueprint TopologyManager should only force the Kerberos client to be installed if Kerberos is enabled and {{kerberos_env/kdc_type}} is not ""none"".

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-11 13:47:18,20
13220810,Chrome and Firefox browsers are crashing while opening Ambari UI,Ambari UI is consuming almost 1.4 GB of browser memory.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-03-11 11:04:54,15
13220193,configs.py: cannot set an empty property value,"Try to set an empty value for a property using the configs.py script, using the following options:

   --action=set --key=""my.key"" --value="""" 

And get the following error:

""You should use option (-f) to set file where entire configurations are saved OR (-k) key and (-v) value for one property""",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-07 13:06:17,14
13220163,Need a stack feature constant for add zoneName field in collection used for Ranger plugin audit,"As a part to add {{zoneName}} field in Ranger Solr collection during upgrade, need a stack feature constant to support this feature.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-07 11:32:20,25
13219880,"Host registration through Ambari UI failed with an error ""Unsupported Media Type""","Host registration through Ambari UI failed with an error ""Unsupported Media Type""",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-03-06 11:08:14,18
13219594,[Knox TP] Config Group selection is empty while Adding Service in Ambari,"The following HTTP requests failed in the console:

GET https://ctr-e139-1542663976389-81666-01-000003.hwx.site:8443/gateway/default/ambari/api/v1/clusters/cl1/configurations/service_config_versions?is_current=true&group_id%3E0&fields=*&_=1551793258100

{""status"":400,""message"":""Unable to compile query predicate: Invalid Query Token: token='&', previous token type=PROPERTY_OPERAND""}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-03-05 16:03:35,18
13218836,"Start, Stop, Service Check and other request actions using PUT/POST from Ambari UI do not respond when tried via Knox TP","Getting the error message below when trying to start/stop a service using Ambari UI through Knox.

{code}
{
  ""status"" : 400,
  ""message"" : ""Invalid Request: Malformed Request Body.  An exception occurred parsing the request body: Unexpected character ('%' (code 37)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n at [Source: java.io.StringReader@5e9b8d28; line: 1, column: 3]""
}
{code}

",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2019-03-01 08:24:11,18
13218407,Regenerate keytab skips some keytabs on repaired host,"*STR*

After repairing a failed master node (single, not HA). Ambari has external db. Repair should also work if cluster is kerberized. Repair means following procedure (some steps are skipped for clarity):

1. terminate failed node, start a new one
2. start ambari on new node
3. *regenerate all keytabs*
4. reinstall components
5. start / restart all components

Ambari has external db, containing the successful installation before repair.


*The problem*

When calling regenerate keytabs some do get created without any keys:
- smokeuser.headless.keytab
- spnego.service.keytab

Quick analysis revealed that in case of some of the keytabs ambari stores to db a cache location on the hard drive. Since, however, that cache is note there on the new instance and so ambari for some reason fails to regenerate those specific keytabs.


*Version where this worked*
In ambari version 2.7.1.0 this has already worked.


",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-02-27 18:08:36,13
13218328,XSS - cross site scripting vulnerability,"I noticed there are some  web pages in Ambari Console vulnerable to XSS attack where attacker can perform a variety of actions: steal user's cookies, modify webpage contents, and perform operations with the site within user's session.

*Steps to reproduce !Screen Shot 2019-02-27 at 12.28.14.png!*

Step1: Login into the application.

Step2: Go to Services -> YARN (you can select any service here).

Step3: Select any existing widget in Metrics section and click on edit.

Step 4: Click on edit

Step 5: In the name field box, enter value “<img src=X onerror=alert(22)>”

Step6: Click on Next button and then save button.

Step 7: XSS popup will trigger once the summary page is refreshed.

*Note:* Create widget page is also vulnerable.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-02-27 12:29:22,7
13218285,Backport Knox Trusted Proxy changes into 2.7.4,Please backport all necessary changes from  Knox Trusted Proxy support into Ambari 2.7.4.,pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2019-02-27 08:22:00,18
13218184,"Dashboard ""Yarn Container"" widget always shows n/a in Ambari 2.7.3","- When opening the Ambari 2.7.3 Dashboard it always shows *""n/a""* for the *""Yarn Container""* widget.

See the attached screenshot [^Dashboard_Yarn_Containers.png]

*+Workaround:+*  [https://community.cloudera.com/t5/Support-Questions/Ambari-2-7-3-Dashboard-quot-Yarn-Container-quot-widget-shows/td-p/281836|https://community.cloudera.com/t5/Support-Questions/Ambari-2-7-3-Dashboard-quot-Yarn-Container-quot-widget-shows/td-p/281836]",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-02-26 22:31:10,22
13218043,Upgrade Apache Solr version to 7.7.0 or later in Ambari,"CVE-2017-3164 SSRF issue in Apache Solr

Severity: High

Vendor: The Apache Software Foundation

Versions Affected:
Apache Solr versions from 1.3 to 7.6.0

Description:
The ""shards"" parameter does not have a corresponding whitelist mechanism,
so it can request any URL.

Mitigation:
Upgrade to Apache Solr 7.7.0 or later.
Ensure your network settings are configured so that only trusted traffic is
allowed to ingress/egress your hosts running Solr.

Credit:
dk from Chaitin Tech

References:
https://issues.apache.org/jira/browse/SOLR-12770
https://wiki.apache.org/solr/SolrSecurity",pull-request-available,['ambari-infra'],AMBARI,Task,Major,2019-02-26 08:47:24,16
13217976,"HDFS Summary Dashboard does not show ""TOTAL FILES + DIRECTORIES"" properly In Ambari 2.7.","- In Ambari 2.7 when we access the HDFS Summary page then we notice that ""TOTAL FILES + DIRECTORIES"" . always shows ""n/a"".

Looks like this is because the [1] behaviour change in HDFS the  *""TotalFiles""* metric is removed from FSNameSystem we are supposed to use *""FilesTotal""* instead. However ambari web still uses ""TotalFiles"" attribute.

[1] https://hadoop.apache.org/docs/r3.0.3/hadoop-project-dist/hadoop-common/release/3.0.0-alpha1/RELEASENOTES.3.0.0-alpha1.html
{code}
HDFS-5165 | Minor | Remove the TotalFiles metrics
{code}

Please refer to the attached Screenshot.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-02-26 02:02:25,3
13217752,"Modifying the value of node memory in the YARN service will affect the maximum value of container memory, but the modification cannot be saved.","Modifying the value of node memory in the YARN service will affect the maximum value of container memory, but the modification cannot be saved.",pull-request-available,[],AMBARI,Bug,Major,2019-02-25 08:35:07,1
13217149,"Rack ""Config Refresh"" behaviour is different in Ambari 2.6 and 2.7.3","In the new ambari-2.7.3 version, there is no way (via web ui) to generally refresh configs on a host . There is a way to refresh HDFS_CLIENT config on the Namenode host, but refreshing HDFS_CLIENT config *does not* refresh the topology_mappings.data file. 


*In Ambari 2.6.2: (During the ""Refresh Configs"")*
When we refresh ""Refresh Configs"" then it refreshes all client configs which also includes HDFS and MapReduce2 config refresh. During the ""Restart MapReduce2 Client"" step it actually performs the ""/etc/hadoop/conf/topology_mappings.data"" file refresh with correct rack info which is updated via ambari ui.
Example : 
{code}
2019-02-09 21:31:05,642 - File['/usr/hdp/2.6.5.0-292/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2019-02-09 21:31:05,647 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
2019-02-09 21:31:05,655 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('P@.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
{code}


*In Ambari 2.7.3:  (During the ""Refresh Configs"")*
Even after refreshing HDFS Client (AND) MapReduce Clients individually using ""Refresh Configs"" iwe do not see above kind of message. Also no changes happens inside the ""/etc/hadoop/conf/topology_mappings.data"" is not updated until we restart daemon components like DataNode. So the behavior of client config refresh seems to be slightly changed from ambari side.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-02-21 09:57:27,19
13216706,http.strict-transport-security change does not take affect in 2.7.x,"Updating the below configurations does not take affect in Ambari 2.7.x version


{noformat}
http.strict-transport-security=max-age=0
views.http.strict-transport-security=max-age=0
{noformat}

After setting the above configurations still API response gives below max-age headers.


{noformat}
Strict-Transport-Security: max-age=31536000 ; includeSubDomains
{noformat}

I see AmbariServerSecurityHeaderFilter.java setting the correctly defined params but later somehow it is going to default value.

This works fine in 2.6.x versions.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-02-19 18:12:06,6
13216658,"Dashboard is unable to load . Common console error : ""SEVERE TypeError: widgetGroups is undefined""","I found a corner scenario where the dashboard never loads.
If we click on Dashboard link on landing page before dashboard completely loads upon login, dashboard never loads.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-02-19 14:30:58,19
13216625,Cover Overridden Row View,app/views/common/configs/overriddenPropertyRow_view.js,pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-19 13:19:17,7
13216621,ClientComponentHasNoStatus exception clutters Operating System's /var/log/messages,"As part of the status check, exception ClientComponentHasNoStatus throws ""detected unhandled Python exception"" in /var/log/messages every 20 seconds for each client installed.",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2019-02-19 12:41:38,5
13216325,Ambari 2.7.3 web hangs at first step installation wizard,"I installed 2.7.3 following the instructions from: [https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.7.3]

After a long process I managed to complete the installation and start both services successfully: ""ambari-server"" and ""ambari-agent""

Now I am stuck on the install wizard in the fisrt step without getting any error or any progress. That is, after hitting ""Next"" it never gets to step 2. (see picture) [screenshot showing the issue|https://i.stack.imgur.com/RDw4k.png] Things done so far to try to work this around:
 * restarting services (several times)
 * ambari-server upgrade (once more)
 * trying with chrome and IE
 * looking at the log files of ambari-server and ambari-agent

...but without success. Has anybody come across the same issue? Any clues?

Here's a picture of the console in Firefox (I get the same in chrome)

Thanks for your help!",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-02-18 08:45:03,7
13214689,Support SystemD integration of Ambari agent with SLES,"SLES 12.x uses Systemd v228 which imposes a 512 process/thread limit for processes started by ambari-agent if both ambari-agent and the process runs as root.

The solution should be a *systemd* service unit file that sets *TasksMax=infinity* for *ambari*-agent to overcome this limitation. _TasksMax_ is a parallel mechanism to process limits, so limits will still apply.

The ambari-agent rpm must be build with *-P suse12* in order to include the service file in the rpm.",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2019-02-08 18:18:26,20
13214585,Implement additional error reporting for ambari-web unit tests,"If ambari-web unit tests fail because of JS error thrown in test files outside the callbacks passed to {{it}}/{{before}}/{{beforeEach}}/{{after}}/{{afterEach}} methods, the execution is just stopped without reporting the cause. This might be confusing because failure isn't reported if none of the previous tests failed.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-08 11:17:33,15
13214352,Cover all not covered common views,"Cover these views:
views/common/log_search_ui_link_view.js
views/common/log_tail_view.js
views/common/not-scrollable-textarea.js
views/common/service_restart_view.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-07 11:57:20,7
13214345,Cover views of Widget Create wizard with unit tests,"Following files to cover:
* app/views/main/service/widgets/create/expression_view.js
* app/views/main/service/widgets/create/step2_view.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-07 10:58:10,19
13214159,[Log Search UI] Update the development readme,Add UI dev instruction to the development docs.,pull-request-available,['logsearch'],AMBARI,Task,Minor,2019-02-06 13:53:43,26
13213976,Cover controllers of High Availability wizard with unit tests,"Following files to cover:
* app/controllers/main/admin/highAvailability/hawq/removeStandby/step3_controller.js
* app/controllers/main/admin/highAvailability/hawq/removeStandby/wizard_controller.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-05 17:31:09,9
13213975,Cover config history controller with unit tests,"Following files to cover:
* app/controllers/main/dashboard/config_history_controller.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-05 17:30:45,9
13213890,Remove unused models from ambari-web,"Remove the following models:
- {{App.TargetCluster}}
- {{App.RootService}}
- {{App.RootServiceComponents}}
- {{App.Rack}}
- {{App.Authentication}}
- {{App.BackgroundOperation}}
- {{App.BackgroundOperationEvent}}
- {{App.Form}}
- {{App.FormField}}
- {{App.CreateUserForm}}
- {{App.ServiceAudit}}",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-05 12:33:05,15
13213810,Typo in ambari-server 'setup-security' help text,"When running the command 
{code:java}
ambari-server setup-security --help
{code}
The output for the option '--security-option' is missing the character 's' at the end of the sub option 'encrypt-passwords'

 ",Typo pull-request-available,['documentation'],AMBARI,Bug,Minor,2019-02-05 00:00:55,3
13213697,LDAP password in cleartext in ldap-password.dat file after encrypting passwords,"In 2.7.x we store LDAP password within its own file; however the content of that file is not encrypted even if password encryption is on. To approach this issue the following should be done:
 - in case password encryption is enabled we will encrypt the LDAP password in the credential store and write the corresponding CS alias in the LDAP password file (just like we do with other passwords in {{ambari.properties}})
 - in case the password encryption is disabled we will write the raw password in the LDAP password file

In both cases an additional level of security can be achieved by setting the appropriate user/group access on the file system to the LDAP password file.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-02-04 14:29:34,27
13213631,Cover widget mixin with unit tests,"Following files to cover:
* app/mixins/common/widgets/widget_mixin.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-04 07:39:02,19
13213622,Yarn Capacity Scheduler Authorization issues due to AuthToLocal Rules,"Yarn Capacity Scheduler is having issues with authorization if AuthToLocal rules are enabled.

Problem Statement : I am logging as LDAP User synced with ambari with my username contains spaces : For example : 'Akhil Naik' . the User is a Ambari Admin user.

In Core-site.xml the AuthToLocal rules are set :


{code:java}
RULE:[1:$1](. *.*)s/ /_/g
{code}


it will display :

*""Warning! You do not have permission to edit the Capacity Scheduler configuration. Contact your Cluster administrator.""*

and logs state :

{code:java}
The authenticated user is not authorized to perform the requested operation28 Jan 2019 17:56:03,488 ERROR [ambari-client-thread-277] [CAPACITY-SCHEDULER 1.0.0 AUTO_CS_INSTANCE] ConfigurationService:333 - Got Error response from url : /api/v1/users/chitrartha_sur?privileges/PrivilegeInfo/permission_name=AMBARI.ADMINISTRATOR|(privileges/PrivilegeInfo/permission_name.in(CLUSTER.ADMINISTRATOR,CLUSTER.OPERATOR)&privileges/PrivilegeInfo/cluster_name=v01eaedl). Response : {
  ""status"" : 403,
  ""message"" : ""The authenticated user is not authorized to perform the requested operation""
}
org.apache.ambari.view.AmbariHttpException: {
  ""status"" : 403,
  ""message"" : ""The authenticated user is not authorized to perform the requested operation""
}
        at org.apache.ambari.server.view.ViewAmbariStreamProvider.getInputStream(ViewAmbariStreamProvider.java:135)
        at org.apache.ambari.server.view.ViewAmbariStreamProvider.getInputStream(ViewAmbariStreamProvider.java:123)
        at org.apache.ambari.server.view.ViewAmbariStreamProvider.readFrom(ViewAmbariStreamProvider.java:85)
        at org.apache.ambari.view.utils.ambari.AmbariApi.readFromAmbari(AmbariApi.java:130)
        at org.apache.ambari.view.capacityscheduler.ConfigurationService.isOperator(ConfigurationService.java:322)
        at org.apache.ambari.view.capacityscheduler.ConfigurationService.getPrivilege(ConfigurationService.java:239)
{code}

Root cause: 
Currently After Fix of : https://issues.apache.org/jira/browse/AMBARI-14503 , I see Ambari Server is Converting AuthToLocal Changes for Usernames(Code : https://github.com/apache/ambari/blob/5460e8952729854f1c032a781c9a8de608ba4475/ambari-server/src/main/java/org/apache/ambari/server/view/ViewContextImpl.java#L233 )

and Yarn capacity Scheulder is calling this method (https://github.com/apache/ambari/blob/5460e8952729854f1c032a781c9a8de608ba4475/contrib/views/capacity-scheduler/src/main/java/org/apache/ambari/view/capacityscheduler/ConfigurationService.java#L319) , Ambari Server rejects the Request Stating No Permission.



*Ideally Yarn Capacity Scheduler should be calling : context. getLoggedinUser() instead of context. getUsername()*",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2019-02-04 04:48:33,3
13213376,Cover mappers files with unit tests,"Cover following files:
- {{ambari-web/app/mappers/alert_notification_mapper.js}}
- {{ambari-web/app/mappers/cluster_mapper.js}}
- {{ambari-web/app/mappers/stack_version_mapper.js}}
- {{ambari-web/app/mappers/widget_mapper.js}}
",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-01 18:23:45,15
13213024,Scale hosts ignores rack_info,"The following request is accepted by Ambari, but the new host is assigned to {{/default-rack}}:

{noformat}
$ curl -X POST -d @- ""http://${AMBARI_SERVER}:8080/api/v1/clusters/TEST/hosts"" <<EOF
[
    {
        ""blueprint"": ""blue"",
        ""host_group"": ""node"",
        ""host_name"": ""c7402.ambari.apache.org"",
        ""rack_info"": ""/rack/a""
    }
]
EOF
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-01-31 09:22:29,21
13212613,Cover service mixin with unit tests,"Following files to cover:
* app/mixins/main/service/groups_mapping.js
* app/mixins/main/service/themes_mapping.js

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-29 17:43:17,9
13212612,Cover controller of widget edit with unit tests,"Following files to cover:
* app/controllers/main/service/widgets/edit_controller.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-29 17:42:32,9
13212611,StackAdvisorAdapterTest result depends on method execution order,"{{StackAdvisorAdapterTest}} result depends on method execution order, {{getLayoutRecommendationInfo}} fails if executed after {{recommendConfigurations_alwaysApply}}.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-01-29 17:38:41,21
13212398,Alert notification properties are wiped after enabling/disabling the notification,"STR:
Create alert notification (tested on ALERT_SCRIPT notification_type)
Populate Script Dispatch Property and/or Script Filename
Disable the notification
Enable the notification
Script Dispatch Property and/or Script Filename properties are empty",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-01-29 05:34:07,28
13212390,Infra Manager: backport fixes from 2.8.0,"AMBARI-24987 - Infra Manager: parse logs by Logsearch
AMBARI-24980 - Infra Manager: java 11 support
AMBARI-24969 - Infra Manager: REST api cleanup
AMBARI-24952 - Infra Manager: basic authentication
AMBARI-24946 - Infra Manager: use HDFS client when writing to cloud 
AMBARI-24918 - Infra Manager: ssl support
AMBARI-24912 - Infra Manager: scheduled job fails with dateparse exception
AMBARI-24895 - Infra Manager: code clean up
AMBARI-24878 - Infra Manager: kerberos support
AMBARI-24838 - Infra Manager: zookeper connection string
AMBARI-24807 - ADDENDUM Infra Manager: unable to restart job
AMBARI-24832 - Infra Manager: set archive file permission on hdfs to 640
AMBARI-24831 - Infra Manager: upgrade spring to 5.1.1
AMBARI-24807 - Infra Manager: unable to restart job
AMBARI-24792 - Infra Manager: not all the documents archived
AMBARI-24777 - Infra Manager: Remove dependencies due to security concerns.
AMBARI-24761 - Infra Manager: hive support for archiving Infra Solr
AMBARI-24731 - Infra Manager: scheduled cleanup of old jobs",pull-request-available,['ambari-infra'],AMBARI,Task,Major,2019-01-29 03:55:45,16
13211812,Cover widget mixin with unit tests,"Following files to cover app/mixins/common/widgets/widget_section.js.

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-25 12:03:18,19
13211636,ambari-audit.log entries span multiple lines,"Many entries in ambari-audit.log span across multiple lines. This makes it impossible to search or parse them.

Example:
{code}
2019-01-24T14:06:01.427-0500, User(myuser), RemoteIp(172.31.1.10), Operation(User login), Roles(
    mycluster: Cluster User
    SmartSense View: View User
    Files View: View User
2019-01-24T14:07:01.924-0500, User(myuser), RemoteIp(172.31.1.10), Operation(User login), Roles(
    mycluster: Cluster User
    SmartSense View: View User
    Files View: View User
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-01-24 19:12:26,14
13211473,/var/lib/ambari-agent/cache not updating (Ambari 2.7),"As of Ambari 2.7, any changes to /var/lib/ambari-server/resources are not
pushed to ambari-agents cache (/var/lib/ambari-agent/cache/. This is not an
issue in Ambari 2.6.

Also, if you remove files or need to wipe ambari-agent cache(s), the files
will never repopulate.

Another example, is custom ambari host_scripts not being pushed to the ambari-
agents as should be done based on this article, and works in Ambari 2.6:
<https://community.hortonworks.com/articles/38149/how-to-create-and-register-
custom-ambari-alerts.html>

To reproduce:

  * 1\. deploy hosts with Ambari 2.7.3 and have them connected to an ambari-server
  * 2\. turn on DEBUG logging in ambari-agent
  * 3\. go to an ambari-agent. Notice that there is no /var/lib/ambari-agent/cache/host_scripts/.hash but there are .hash files for the stack directories.
  * 4\. on the ambari-server, add files into /var/lib/ambari-server/resources/host_scripts
  * 5\. restart ambari-agent and notice that their host_scripts directory is never updated and .hash is never generated
  * 6\. check ambari-agent.log and notice that other paths were checked (by FileCache.py) but host_scripts were not

",pull-request-available,[],AMBARI,Bug,Major,2019-01-24 07:09:58,5
13211337,UI unit tests are sometimes failing,"Error is thrown:
{noformat}
Ambari Web Unit tests ""after each"" hook:
Uncaught Error: assertion failed: You must pass at least an object and event name to Ember.addListener
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-01-23 17:31:11,15
13210959,getAsPost requests don't work when ambari is used behind knox proxy,"getting:

https://c7401.ambari.apache.org:8443/gateway/default/ambari/api/v1/clusters/cc/hosts?fields=Hosts/rack_info,Hosts/host_name,Hosts/maintenance_state,Hosts/public_host_name,Hosts/cpu_count,Hosts/ph_cpu_count,alerts_summary,Hosts/host_status,Hosts/host_state,Hosts/last_heartbeat_time,Hosts/ip,host_components/HostRoles/state,host_components/HostRoles/maintenance_state,host_components/HostRoles/stale_configs,host_components/HostRoles/service_name,host_components/HostRoles/display_name,host_components/HostRoles/desired_admin_state,host_components/metrics/dfs/namenode/ClusterId,host_components/metrics/dfs/FSNamesystem/HAState,metrics/disk,metrics/load/load_one,Hosts/total_mem,Hosts/os_arch,Hosts/os_type,metrics/cpu/cpu_system,metrics/cpu/cpu_user,metrics/memory/mem_total,metrics/memory/mem_free,stack_versions/HostStackVersions,stack_versions/repository_versions/RepositoryVersions/repository_version,stack_versions/repository_versions/RepositoryVersions/id,stack_versions/repository_versions/RepositoryVersions/display_name&minimal_response=true,host_components/logging&page_size=25&from=0&_=1546003790663


gives:

{""status"":400,""message"":""Invalid Request: Malformed Request Body.  An exception occurred parsing the request body: Unrecognized token 'from': was expecting \n at [Source: java.io.StringReader@9ba0d3a; line: 1, column: 6]""}

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-22 11:49:47,18
13210765,Bug at AbstractProviderModule class,"Here org/apache/ambari/server/controller/internal/AbstractProviderModule.java:974
there is a return statement within a loop without any condition. It is a definite bug
{code}
for (Resource res : configResources) {
return res.getPropertiesMap().get(PROPERTIES_CATEGORY);
}
return Collections.emptyMap();
{code}

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-01-21 13:20:08,13
13210513,Log Search: SSL props needs to be set if only ambari-server uses SSL,"Log Search SSL related java properties needs to passed to the application if ambari-server SSL is enabled, but not for Log Search",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Critical,2019-01-19 02:35:35,29
13210428,Cover errors utils with unit tests,"Cover the following files:
- {{utils/errors/assertions.js}}
- {{utils/errors/definitions.js}}",pull-request-available,[],AMBARI,Task,Major,2019-01-18 16:40:00,15
13210380,Intermittent ConcurrentModificationException exception during STOMP message emitting,"During 2k cluster deploying a lot of exceptions were found in the ambari-server log:
{code}
2019-01-15 16:25:33,567 ERROR [stomp-agent-bus-0] agent-update-bus:232 - Exception thrown by subscriber method onUpdateEvent(org.apache.ambari.server.events.STOMPEvent) on subscriber org.apache.ambari.server.events.listeners.requests.STOMPUpdateListener@5ee973ee when dispatching event: org.apache.ambari.server.events.TopologyAgentUpdateEvent@f4bf3c68
org.springframework.messaging.MessageDeliveryException: Failed to handle GenericMessage [payload=byte[577], headers={simpMessageType=MESSAGE, contentType=application/json;charset=UTF-8, simpDestination=/events/topologies}] to org.springframework.messaging.support.ExecutorSubscribableChannel$SendTask@13106eff in SimpleBrokerMessageHandler [DefaultSubscriptionRegistry[cache[5480 destination(s)], registry[2392 sessions]]]; nested exception is java.util.ConcurrentModificationException, failedMessage=GenericMessage [payload=byte[577], headers={simpMessageType=MESSAGE, contentType=application/json;charset=UTF-8, simpDestination=/events/topologies}]
        at org.springframework.messaging.support.ExecutorSubscribableChannel$SendTask.run(ExecutorSubscribableChannel.java:153)
        at org.springframework.messaging.support.ExecutorSubscribableChannel.sendInternal(ExecutorSubscribableChannel.java:100)
        at org.springframework.messaging.support.AbstractMessageChannel.send(AbstractMessageChannel.java:136)
        at org.springframework.messaging.support.AbstractMessageChannel.send(AbstractMessageChannel.java:122)
        at org.springframework.messaging.simp.SimpMessagingTemplate.sendInternal(SimpMessagingTemplate.java:187)
        at org.springframework.messaging.simp.SimpMessagingTemplate.doSend(SimpMessagingTemplate.java:162)
        at org.springframework.messaging.simp.SimpMessagingTemplate.doSend(SimpMessagingTemplate.java:48)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.convertAndSend(AbstractMessageSendingTemplate.java:151)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.convertAndSend(AbstractMessageSendingTemplate.java:129)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.convertAndSend(AbstractMessageSendingTemplate.java:122)
        at org.apache.ambari.server.events.MessageEmitter.emitMessageToAll(MessageEmitter.java:159)
        at org.apache.ambari.server.events.DefaultMessageEmitter.emitMessage(DefaultMessageEmitter.java:102)
        at org.apache.ambari.server.events.listeners.requests.STOMPUpdateListener.onUpdateEvent(STOMPUpdateListener.java:53)
        at sun.reflect.GeneratedMethodAccessor174.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)
        at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.ConcurrentModificationException
        at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)
        at java.util.LinkedHashMap$LinkedEntryIterator.next(LinkedHashMap.java:752)
        at java.util.LinkedHashMap$LinkedEntryIterator.next(LinkedHashMap.java:750)
        at java.util.Map.forEach(Map.java:620)
        at org.springframework.messaging.simp.broker.SimpleBrokerMessageHandler.sendMessageToSubscribers(SimpleBrokerMessageHandler.java:388)
        at org.springframework.messaging.simp.broker.SimpleBrokerMessageHandler.handleMessageInternal(SimpleBrokerMessageHandler.java:304)
        at org.springframework.messaging.simp.broker.AbstractBrokerMessageHandler.handleMessage(AbstractBrokerMessageHandler.java:256)
        at org.springframework.messaging.support.ExecutorSubscribableChannel$SendTask.run(ExecutorSubscribableChannel.java:144)
        ... 21 more
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-01-18 13:23:37,6
13210150,Cover views of the modals with unit tests,"Following files to cover app/views/common/modal_popups/*.

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-17 13:05:28,19
13209921,"Ambari is not respecting host component maintenance mode when performing ""Restart All Required"" at the cluster level","* Put HSI in maintenance mode.
* Changed auth to local mapping in core-site.
* A bunch of services got a restart indicator.
* Triggering ""Restart All Required"" at the cluster level schedules HSI to be restarted.  HSI restart fails so the entire operation fails.  This is cumbersome because now the user has to trigger ""restart affected"" for individual services.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-01-16 15:16:33,15
13209697,Cover views of the JournalNode wizard with unit tests,"Following files to cover:
* app/views/main/admin/highAvailability/journalNode/progress_view.js
* app/views/main/admin/highAvailability/journalNode/step3_view.js
* app/views/main/admin/highAvailability/journalNode/step5_view.js

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-15 17:22:34,9
13209455,Duplicate title on YARN summary page,'Components' sub-title is displayed twice,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-01-14 16:21:00,15
13209424,Dasboard metrics will not load for ambari user which has dot in their username.,"Ambari users which as dot in their username will not be able to see the dashboard metrics. 

It looks that js scripts are truncating the username to use only first part before dot and fails with below error.
{code:java}
{\""file\"":\""http://c116-node1.raghav.com:8080/javascripts/vendor.js\"",\""line\"":13510,\""col\"":12,\""error\"":\""Uncaught Error: Object in path user-pref-test1 could not be found or was destroyed.\"",\""stackTrace\"":\""Error: Object in path user-pref-test1 could not be found or was destroyed.\\n    at setPath (/vendor.js:13510:18)\\n    at Object.set (/vendor.js:13387:12)\\n    at Object.App.db.set (/app.js:200972:6)\\n    at Class.setDBProperty (/app.js:72577:12)\\n    at Class.saveWidgetsSettings (/app.js:238244:10)\\n    at Class.getUserPrefErrorCallback (/app.js:238258:10)\\n    at Class.newFunc [as getUserPrefErrorCallback] (/vendor.js:12954:16)\\n    at Class.opt.error (/app.js:193035:36)\\n    at fire (/vendor.js:1141:36)\\n    at Object.fireWith [as rejectWith] (/vendor.js:1252:15)\""},\""1541550308010\
{code}

Tried manually setting the persist data for the user which dint help. 

Steps to reproduce: (on Ambari 2.7.1)
-->Create user test.user1 in ambari: 
-->Assign privileges to see the dashboard
-->Login as test.user1, which should show loading of metric but never completes.

All other features work fine exception dashboard metrics

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-01-14 14:10:51,19
13208872,[Log Search UI] two timezones with the same ID,"In the time zone selection box the two CST time zones collide: ""Central Standard Time"" and ""China Standard Time"" are both abbreviated as CST, and due to this if the user clicks on the CST button, or moves the mouse over any of these regions, then both of them are highlighted.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Major,2019-01-10 22:06:29,26
13208789,Cover views of reassign wizard with unit tests,"Following files to cover:
* app/views/main/service/reassign/step1_view.js
* app/views/main/service/reassign/step3_view.js
* app/views/main/service/reassign/step5_view.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-10 13:48:13,19
13208776,Cluster Installation fails when installing the cluster via Redhat satellite repositories (no internet connectivity Cluster),"Problem Statement : 

I am installing HDP-3.1 using ambari-2.7.3. I am installing the cluster via Redhat Repositories and Host Check page show this warning to me.

But I have selected the Redhat7 as my operating System in the Selection version page.  This is a false warning and this should be fixed.

Attached the image of Warning shown in the UI  [Warning Message.jpg|https://issues.apache.org/jira/secure/attachment/12954420/Screen%20Shot%202019-01-10%20at%206.29.20%20PM.png]
Below Exception can be seen in the logs when click on deploy and Same is shown in UI .
The Cluster become unusable after this and we need to reset cluster via 'ambari-server reset'

{code:java}
2019-01-09 13:33:54,031 ERROR [ambari-client-thread-22] ContainerResponse:419 - The RuntimeException could not be mapped to a response, re-throwing to the HTTP container
java.lang.RuntimeException: org.apache.ambari.server.controller.spi.SystemException: Operating System matching redhat7 could not be found
	at org.apache.ambari.server.controller.AmbariManagementControllerImpl.retrieveHostRepositories(AmbariManagementControllerImpl.java:5921)
	at org.apache.ambari.server.agent.stomp.HostLevelParamsHolder.getCurrentDataExcludeCluster(HostLevelParamsHolder.java:79)
	at org.apache.ambari.server.agent.stomp.HostLevelParamsHolder.getCurrentData(HostLevelParamsHolder.java:68)
	at org.apache.ambari.server.controller.AmbariManagementControllerImpl.getAddedComponentsTopologyEvent(AmbariManagementControllerImpl.java:824) 

{code}
",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Critical,2019-01-10 13:01:45,3
13208700,deploy-gce-perf-cluster.py fails after upgrade on gce controller,"'gce fqdn' command was removed. While the script relied on it.

",pull-request-available,[],AMBARI,Bug,Major,2019-01-10 06:20:07,5
13208603,"Remove Flume Live widget from Ambari, alongside the Flume service during upgrade to HDP3. ","During the ambari-managed upgrade from HDP 2.6 -> HDP 3.0, the flume service is removed (as it is no longer supported in the new stack). 

The ""Flume Live"" widget is still displayed on the Ambari home page.  

Let's remove this widget as well, when the flume service is removed. ",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-09 18:35:06,15
13208567,Spark2 Thrift Server alert does not work with HTTPS/SSL,"With HTTPS enabled for Spark2 Thrift, the Ambari alerts fail with:
{code}
Connection failed on host hdpmasterp01.hwp.int.domain.com:10016 (Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/alerts/alert_spark2_thrift_port.py"", line 147, in execute
 ...
ExecutionFailed: Execution of '! /usr/hdp/current/spark2-client/bin/beeline -u 'jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http'  -e '' 2>&1| awk '{print}'|grep -i -e 'Connection refused' -e 'Invalid URL' -e 'Error: Could not open'' returned 1. Error: Could not open client transport with JDBC Uri: jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http: Could not create http connection to jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http. org.apache.http.client.ClientProtocolException (state=08S01,code=0)
Error: Could not open client transport with JDBC Uri: jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http: Could not create http connection to jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http. org.apache.http.client.ClientProtocolException (state=08S01,code=0)
{code}

When HTTPS is enabled, the alert scripts should add `;ssl=true;httpPath=cliservice` as I have below:
{code}
$ /usr/hdp/current/spark2-client/bin/beeline -u 'jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http;ssl=true;httpPath=cliservice'
Connecting to jdbc:hive2://hdpmasterp01.hwp.int.domain.com:10016/default;principal=spark/hdpmasterp01.hwp.int.domain.com@HWP.INT.DOMAIN.COM;transportMode=http;ssl=true;httpPath=cliservice
...
Connected to: Spark SQL (version 2.3.2.3.1.0.0-78)
Driver: Hive JDBC (version 1.21.2.3.1.0.0-78)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.21.2.3.1.0.0-78 by Apache Hive
{code}",pull-request-available,['alerts'],AMBARI,Bug,Major,2019-01-09 15:16:38,14
13208286,Cover flume metric graphs with unit tests,"Following files to cover:
* app/views/main/service/info/metrics/flume/flume_metric_graph.js
* app/views/main/service/info/metrics/flume/flume_metric_graphs.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-08 11:24:44,19
13208089,Logsearch should index keywords without the ending periods(.),"Logsearch should index keywords without the ending periods(.).

E.g : Log message : ""Caught exception checkIn."". Message filters were not able to filter the logs with partial text ""checkIn"" but only with ""checkIn.""
",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2019-01-07 17:56:20,16
13207537,Enable Kerberos fails when Ambari server is not on a registered host,"Enable Kerberos fails when Ambari server is not on a registered host.  

The following error is seen in /var/log/ambari-server.log

{noformat}
2019-01-03 15:28:34,238  WARN [Server Action Executor Worker 39] ServerActionExecutor:471 - Task #39 failed to complete execution due to thrown exception: org.apache.ambari.server.HostNotFoundException:Host not found, hostname=c7401.ambari.apache.org
org.apache.ambari.server.HostNotFoundException: Host not found, hostname=c7401.ambari.apache.org
        at org.apache.ambari.server.state.cluster.ClustersImpl.getHost(ClustersImpl.java:456)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:190)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:174)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.findConfigurationTagsWithOverrides(AmbariManagementControllerImpl.java:2431)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.google.inject.internal.DelegatingInvocationHandler.invoke(DelegatingInvocationHandler.java:50)
        at com.sun.proxy.$Proxy134.findConfigurationTagsWithOverrides(Unknown Source)
        at org.apache.ambari.server.state.ConfigHelper.calculateExistingConfigurations(ConfigHelper.java:2158)
        at org.apache.ambari.server.controller.KerberosHelperImpl.calculateConfigurations(KerberosHelperImpl.java:1722)
        at org.apache.ambari.server.controller.KerberosHelperImpl.getActiveIdentities(KerberosHelperImpl.java:1797)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.calculateServiceIdentities(KerberosServerAction.java:512)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:456)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.execute(CreatePrincipalsServerAction.java:92)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

This is caused when Ambari tried to find the host-specific configuration values when processing the Kerberos identities and the host is not registered for the relevant cluster. This can happen when the Ambari server Kerberos identity is being processed when the Ambari server host is not registered with the cluster. 

To solve this, host specific configuration values should not be obtained for the non-registered Ambari server host. 
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-01-03 19:49:40,30
13207528,Upgrading Using a Modified Default Version Repository Fails on Some Services,"When performing a stack upgrade, if the user chooses the ""Default Version Definition"" option and just modifies the default URLs, the backing VDF which is created and stored in the database contains which do not support upgrading. 

Before the upgrade, if any of these services are in maintenance mode (such as Ambari Infra), they will prevent the upgrade from starting.

During the actual upgrade, this will cause problems on finalization as those services/components have not participated, yet the VDF indicates that they should have.",pull-request-available,[],AMBARI,Task,Major,2019-01-03 18:37:50,31
13207480,Delete identities fails when removing service in reverse order,"(reverse order = host components first, then components, then service)

STR:

Install ZooKeeper + Kafka + Kerberos
Stop Kafka
curl -X DELETE http://c7401.ambari.apache.org:8080/api/v1/clusters/TEST/hosts/c7401.ambari.apache.org/host_components/KAFKA_BROKER
curl -X DELETE http://c7401.ambari.apache.org:8080/api/v1/clusters/TEST/services/KAFKA/components/KAFKA_BROKER
=> identities are removed
curl -X DELETE http://c7401.ambari.apache.org:8080/api/v1/clusters/TEST/services/KAFKA
=> identity removal fails with PersistenceException

{code}
Internal Exception: org.postgresql.util.PSQLException: ERROR: syntax error at or near "")""
  Position: 210
Error Code: 0
Call: SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ()) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))
        bind => [4 parameters bound]
Query: ReadAllQuery(name=""ClusterConfigEntity.findLatestConfigsByStackWithTypes"" referenceClass=ClusterConfigEntity sql=""SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ?) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))"")
FetchGroup(){serviceConfigEntities, stack, selectedTimestamp, clusterId, type, version, unmapped, configId, configGroupConfigMappingEntities, tag, selected, timestamp, clusterEntity}
javax.persistence.PersistenceException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: syntax error at or near "")""
  Position: 210
Error Code: 0
Call: SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ()) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))
        bind => [4 parameters bound]
Query: ReadAllQuery(name=""ClusterConfigEntity.findLatestConfigsByStackWithTypes"" referenceClass=ClusterConfigEntity sql=""SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ?) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))"")
FetchGroup(){serviceConfigEntities, stack, selectedTimestamp, clusterId, type, version, unmapped, configId, configGroupConfigMappingEntities, tag, selected, timestamp, clusterEntity}
        at org.eclipse.persistence.internal.jpa.QueryImpl.getDetailedException(QueryImpl.java:382)
        at org.eclipse.persistence.internal.jpa.QueryImpl.executeReadQuery(QueryImpl.java:260)
        at org.eclipse.persistence.internal.jpa.QueryImpl.getResultList(QueryImpl.java:473)
        at org.apache.ambari.server.orm.dao.DaoUtils.selectList(DaoUtils.java:53)
        at org.apache.ambari.server.orm.dao.ClusterDAO.getLatestConfigurationsWithTypes(ClusterDAO.java:226)
        at org.apache.ambari.server.orm.AmbariLocalSessionInterceptor.invoke(AmbariLocalSessionInterceptor.java:44)
        at org.apache.ambari.server.state.cluster.ClusterImpl.getLatestConfigsWithTypes(ClusterImpl.java:1185)
        at org.apache.ambari.server.controller.DeleteIdentityHandler$PrepareDeleteIdentityServerAction.extendWithDeletedConfigOfService(DeleteIdentityHandler.java:304)
        at org.apache.ambari.server.controller.DeleteIdentityHandler$PrepareDeleteIdentityServerAction.calculateConfig(DeleteIdentityHandler.java:296)
        at org.apache.ambari.server.controller.DeleteIdentityHandler$PrepareDeleteIdentityServerAction.execute(DeleteIdentityHandler.java:273)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:745)
{code}
",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-01-03 14:34:43,18
13207459,Cover HA wizard controller with unit tests,"Following files to cover:
* app/controllers/main/admin/highAvailability/hawq/addStandby/step3_controller.js
* app/controllers/main/admin/highAvailability/hawq/addStandby/wizard_controller.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-03 11:35:11,19
13207366,SPI Needs to Expose Gson,"When compiling consumers of the SPI which want to use {{Gson}}, the following is seen:

{code}

[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:compile (default-compile) on project hdp-common: Compilation failure: Compilation failure:
[ERROR] /Users/ncole/src/hwx/hdp_ambari_definitions/common/src/main/java/org/apache/ambari/hdp/upgrade/checks/RangerPasswordCheck.java:[49,23] package com.google.gson does not exist
[ERROR] /Users/ncole/src/hwx/hdp_ambari_definitions/common/src/main/java/org/apache/ambari/hdp/upgrade/checks/RangerPasswordCheck.java:[280,7] cannot find symbol
[ERROR]   symbol:   class Gson
[ERROR]   location: class org.apache.ambari.hdp.upgrade.checks.RangerPasswordCheck
[ERROR] /Users/ncole/src/hwx/hdp_ambari_definitions/common/src/main/java/org/apache/ambari/hdp/upgrade/checks/RangerPasswordCheck.java:[280,23] cannot find symbol
[ERROR]   symbol:   class Gson
[ERROR]   location: class org.apache.ambari.hdp.upgrade.checks.RangerPasswordCheck
[ERROR] /Users/ncole/src/hwx/hdp_ambari_definitions/common/src/main/java/org/apache/ambari/hdp/upgrade/checks/RangerPasswordCheck.java:[321,9] cannot find symbol
[ERROR]   symbol:   class Gson
[ERROR]   location: class org.apache.ambari.hdp.upgrade.checks.RangerPasswordCheck
[ERROR] /Users/ncole/src/hwx/hdp_ambari_definitions/common/src/main/java/org/apache/ambari/hdp/upgrade/checks/RangerPasswordCheck.java:[321,25] cannot find symbol
[ERROR]   symbol:   class Gson
[ERROR]   location: class org.apache.ambari.hdp.upgrade.checks.RangerPasswordCheck
[ERROR] -> [Help 1]
[ERROR]
{code}",pull-request-available,[],AMBARI,Task,Blocker,2019-01-02 21:42:21,31
13207246,Select Service page: Show Yarn and MR2 as separate services for selection instead of single selection (Yarn+MapReduce2),"This needs to be done so that user can select YARN without MR2 using UI installer wizard

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-02 09:29:48,19
13206462,Ambari writes Empty baseurl values written to Repo Files when using a local repository causing stack installation failure,"when using a local repository, installation fails due to empty baseurls being written to the Ambari repository (even though local repository baseurl values were provided).  The installation fails with below error : 
{code:java}
stderr: 
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 37, in <module>
    BeforeInstallHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 33, in hook
    install_packages()
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/shared_initialization.py"", line 37, in install_packages
    retry_count=params.agent_stack_retry_count)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 30, in action_install
    self._pkg_manager.install_package(package_name, self.__create_context())
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/yum_manager.py"", line 219, in install_package
    shell.repository_manager_executor(cmd, self.properties, context)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 753, in repository_manager_executor
    raise RuntimeError(message)
RuntimeError: Failed to execute command '/usr/bin/yum -y install hdp-select', exited with code '1', message: 'Repository InstallMedia is listed more than once in the configuration
 
 One of the configured repositories failed (Unknown),
 and yum doesn't have enough cached data to continue. At this point the only
 safe thing yum can do is fail. There are a few ways to work ""fix"" this:
 
     1. Contact the upstream for the repository and get them to fix the problem.
 
    2. Reconfigure the baseurl/etc. for the repository, to point to a working
 
       upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work).
 
     3. Run the command with the repository temporarily disabled
 
            yum --disablerepo=<repoid> ...
 
     4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage:
         yum-config-manager --disable <repoid>

        or 
            subscription-manager repos --disable=<repoid>

     5. Configure the failing repository to be skipped, if it is unavailable.

        Note that yum will try to contact the repo. when it runs most commands,so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice

        compromise:

            yum-config-manager --save --setopt=<repoid>.skip_if_unavailable=true
 
Cannot find a valid baseurl for repo: HDP-3.1-repo-1
'
Command aborted. Reason: 'Server considered task failed and automatically aborted it'
 stdout:
{code}

We can See that Ambari UI shows the empty baseURL in review step
 !Screen Shot 2018-12-26 at 10.12.29 AM.png|height=350,width=850! 

It also sends the empty Repo version while deploy stage
 !Screen Shot 2018-12-26 at 10.13.14 AM.png|height=350,width=850! ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-12-26 14:00:28,3
13205957,Update AngularJS version to 1.7.5 due to known vulnerabilities,"Update AngularJS version to 1.7.5 due to known cross site scripting vulnerabilities.

See https://snyk.io/vuln/npm:angular

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-21 12:34:28,19
13205841,Unable to add user in views 'grant permissions',"If number of users are more than 20 then it shows only 20 users list. for other user it shows as not present.

Steps to reproduce:
1. Create more than 20 users in Ambari

2. Go to File views and start typing the random username under 'Grant permission to these users' - for some it show red mark when try to save it.

Root cause:
UI is making REST API call to load only 20 users

http://172.25.33.6:8080/api/v1/users?Users/user_name.matches(.*.*)&from=0&page_size=20&_=1545350079963

so Ambari is returning 20 users and rest of them will not shown in views grant page.

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-21 00:03:06,3
13205779,Restarting Ambari Server Fails Due to Recursive Injection of STOMPUpdatePublisher,"In some cases during a restart of Ambari Server, the server fails to load due to:
{code:java}
Caused by: java.lang.IllegalStateException: Recursive load of: org.apache.ambari.server.events.publishers.STOMPUpdatePublisher.<init>()
	at com.google.common.base.Preconditions.checkState(Preconditions.java:197)
	at com.google.common.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2299)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2191)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)
	at com.google.inject.internal.FailableCache.get(FailableCache.java:48)
	at com.google.inject.internal.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
	at com.google.inject.internal.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:138)
	at com.google.inject.internal.InjectorImpl.initializeJitBinding(InjectorImpl.java:550)
	at com.google.inject.internal.InjectorImpl.createJustInTimeBinding(InjectorImpl.java:887)
	at com.google.inject.internal.InjectorImpl.createJustInTimeBindingRecursive(InjectorImpl.java:808)
	at com.google.inject.internal.InjectorImpl.getJustInTimeBinding(InjectorImpl.java:285)
	at com.google.inject.internal.InjectorImpl.getBindingOrThrow(InjectorImpl.java:217)
	at com.google.inject.internal.SingleFieldInjector.<init>(SingleFieldInjector.java:42)
	at com.google.inject.internal.MembersInjectorStore.getInjectors(MembersInjectorStore.java:131)
	at com.google.inject.internal.MembersInjectorStore.createWithListeners(MembersInjectorStore.java:98)
	at com.google.inject.internal.MembersInjectorStore.access$000(MembersInjectorStore.java:37)
	at com.google.inject.internal.MembersInjectorStore$1.create(MembersInjectorStore.java:45)
	at com.google.inject.internal.MembersInjectorStore$1.create(MembersInjectorStore.java:42)
	at com.google.inject.internal.FailableCache$1.load(FailableCache.java:37)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 378 more{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-12-20 16:33:10,6
13205774,Optionally execute the post user creation hook on existing users during LDAP sync,"Optionally execute the post user creation hook on existing users during LDAP sync. 

The post user creation hook is executed on users when created or imported into Ambari.  This hook is executed given the following criteria is met:
# The post user creation hook is enabled (ambari.properties - {{ambari.post.user.creation.hook.enabled = true}}, default: {{false}})
# The post user creation hook is set and available (ambari.properties - {{ambari.post.user.creation.hook = <path to script>}}, default: {{/var/lib/ambari-server/resources/scripts/post-user-creation-hook.sh}})
# HDFS is installed and running.

It is possible to have executed the LDAP sync process before all of the criteria has been met.  Therefore, it would be beneficial to trigger the post user creation hook to be executed on these users when the criteria has been met. 

To do this, an optional property should be set on the LDAP sync request - {{post_process_existing_users}}.  The {{post_process_existing_users}} property is part of a ""spec"" object and should be set to either ""true"" or ""false"", if set at all.  If set to ""true"", the post user creation hook will be executed on all user's that come back from the LDAP query that also exist in the Ambari database as LDAP users. 

Example REST API calls:
{noformat:title=Sync All Users and Groups}
POST /api/v1/ldap_sync_events
[
  {
    ""Event"": {
      ""specs"": [
        {
          ""principal_type"": ""users"",
          ""sync_type"": ""all"",
          ""post_process_existing_users"" : ""true""
        },
        {
          ""principal_type"": ""groups"",
          ""sync_type"": ""all"",
          ""post_process_existing_users"" : ""true""
        }
      ]
    }
  }
]
{noformat}

{noformat:title=Sync Specific Users}
POST /api/v1/ldap_sync_events
[
  {
    ""Event"": {
      ""specs"": [
        {
          ""principal_type"": ""users"",
          ""sync_type"": ""specific"",
          ""names"" : ""user1, user2, user3"",
          ""post_process_existing_users"" : ""true""
        }
      ]
    }
  }
]
{noformat}

{noformat:title=Sync Specific Groups}
POST /api/v1/ldap_sync_events
[
  {
    ""Event"": {
      ""specs"": [
        {
          ""principal_type"": ""groups"",
          ""sync_type"": ""specific"",
          ""names"" : ""hadoop_users, hadoop_admins"",
          ""post_process_existing_users"" : ""true""
        }
      ]
    }
  }
]
{noformat}

Using the Ambari sync-ldap CLI, an optional argument named ""--post-process-existing-users"" may be added to enable this feature.

Example CLI calls:
{noformat:title=Sync All Users and Groups}
ambari-server sync-ldap --all --post-process-existing-users
{noformat}

{noformat:title=Sync Specific Users}
ambari-server sync-ldap --users users.txt --post-process-existing-users
{noformat}

{noformat:title=Sync Specific Groups}
ambari-server sync-ldap --groups groups.txt --post-process-existing-users
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-20 16:16:42,30
13205748,Refactor AddServiceInfo to use a builder,"{{AddServiceInfo}} constructor could be improved by the Builder pattern.

https://github.com/apache/ambari/blob/1431ab44887c2ff7f9e94f8fcb42e24e3ce33800/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java#L45-L83",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-20 15:01:32,21
13205739,SPI Upgrade Improvements For Web Calls And Required Plugins,"This issue is to track two changes to the SPI and to Ambari:

- The SPI should provide a mechanism where {{UpgradeCheck}} classes can make web requests using a connection factory that is able to leverage Ambari's cookie store, truststore, and timeout values.

- Upgrade checks provided by a stack can be marked as {{required}} in the {{UpgradeCheckInfo}}. This prevents them from being explicitly defined in the upgrade pack XML. However, since Ambari loads 100's of 1000's of classes, a {{ClassLoader}} scan can take too long. Using the {{Reflections}} library, we're able to pass in the URLs of the JARs which comprise the plugin {{ClassLoader}} and are able to perform the scan very quickly.

- The {{pom.xml}} needed to be updated to include a version of {{guava}} which was compatible with the version of {{reflections}}.",pull-request-available,['ambari-server-spi'],AMBARI,Task,Critical,2018-12-20 14:18:53,31
13205714,Ambari Admin doesn't redirect user to login page if auth session becomes invalidated,"When auth session becomes invalid, nothing happens (instead of redirection to login page)",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2018-12-20 12:45:55,15
13205695,Script to delete an old HDP stack version - BUG,"As per the Jira https://issues.apache.org/jira/browse/AMBARI-18435 an API call to remove older versions was developed but its having issue.

 

ambari-server/src/main/resources/custom_actions/scripts/remove_previous_stacks.py [line: 67]

Execute(('rm', '-f', stack_root + version), sudo=True)

It will not delete the directory as there is no such directory, it should be:

Execute(('rm', '-f', stack_root *++'/'++* version), sudo=True)",patch pull-request-available,['ambari-sever'],AMBARI,Bug,Minor,2018-12-20 11:26:15,3
13205658,Fix typo in setup-ldap docs,"Fixing a typo in the doc: https://github.com/apache/ambari/blob/trunk/ambari-server/docs/security/ldap/index.md

where a non-interactive mode key is incorrectly set as {{ldap-manage-servic}} where as it should be {{ldap-manage-services}}",pull-request-available,['documentation'],AMBARI,Bug,Minor,2018-12-20 08:50:51,32
13205373,JS error after starting stack downgrade,"{code:java}
Uncaught TypeError: Cannot convert undefined or null to object
    at Function.keys (<anonymous>)
    at Object.<anonymous> (app.js:17261)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at Object.deferred.(:8080/anonymous function) [as resolve] (http://104.196.93.139:8080/javascripts/vendor.js:1341:40)
    at Class.complete (app.js:17012)
    at app.js:206411
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-12-19 13:36:37,19
13205329,"LogSearch - In configuration Editor page, under validator section, component Name suggestions are always over hiding with ""livy2_server"" component name.","n configuration editor we can specify for which component we are validating the log under validator section.

Steps to reproduce:
 # Login into LogSearch portal.
 2. Click on drop down present at top right side.
 3. Select Configuration editor.
 4. Select any service present at left side.
 5. Click on component Name text box present under validator.
 6. Now you can see livy2_server component name over hiding actual component names suggestion as drop down, check following screen shot for more info.
 # !bug116099.png!",pull-request-available,['logsearch'],AMBARI,Bug,Major,2018-12-19 10:27:55,16
13205322,Validate SQL schema creation scripts,Verify that SQL schema creation scripts are valid using different DBs in Docker container.,pull-request-available,['ambari-server'],AMBARI,New Feature,Major,2018-12-19 09:50:32,21
13205307,Unable to Add Services Due to toMapByProperty(),"STR:
- Install a simple cluster with ZK using a blueprint.
- Go to the web client and add HDFS as a service.
- You will not be able to progress past the master assignment page after clicking ""Next""

{code}
app.js:28589 Uncaught TypeError: App.HostComponent.find(...).toMapByProperty is not a function
    at Class.saveMasterComponentHosts (app.js:28589)
    at Class.newFunc [as saveMasterComponentHosts] (vendor.js:12954)
    at Class.next (app.js:94607)
    at Class.newFunc [as next] (vendor.js:12954)
    at Class.sendRecursively (vendor.js:27675)
    at Class.send (vendor.js:27660)
    at Class._goNextStepIfValid (app.js:42517)
    at Class.newFunc [as _goNextStepIfValid] (vendor.js:12954)
    at Class.showValidationIssuesAcceptBox (app.js:81684)
    at app.js:81665
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-12-19 08:35:25,19
13205221,Create Ambari Setting to Control VDF Filesystem Access,Create an entry in {{ambari.properties}} to control whether to allow reading filesystem VDF.  Default {{false}} for this access.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-12-18 20:46:13,33
13205121,Update momentum.js version to 2.22.2 due to known vulnerabilities,"Update momentum.js version to 2.22.2 due to known vulnerabilities.

See https://snyk.io/vuln/npm:Moment.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-12-18 12:27:13,19
13204902,UI logic for component dependencies needs to be modified to use the type field,"The component dependency now has the type field. If the value of that field is ""exclusive"" the dependent component should never be co-hosted.
{code}
{
  ""href"" : ""http://c7301:8080/api/v1/stacks/HDP/versions/3.0/services/HDFS/components/DATANODE/dependencies/OZONE_DATANODE"",
  ""Dependencies"" : {
    ""component_name"" : ""OZONE_DATANODE"",
    ""conditions"" : [ ],
    ""dependent_component_name"" : ""DATANODE"",
    ""dependent_service_name"" : ""HDFS"",
    ""scope"" : ""host"",
    ""service_name"" : ""OZONE"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""3.0"",
    ""type"" : ""exclusive""
  }
}{code}
Currently this dependency causes UI to force dependent component being add on the host
 !Screenshot from 2018-11-29 16-10-38.png|thumbnail! ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-17 14:24:09,19
13204252,"Sensitive Ambari configuration values should be encrypted in the Ambari server DB, if enabled","Sensitive Ambari configuration values should be encrypted in the Ambari server DB, if enabled.

Ambari configuration value types are defined in {{org.apache.ambari.server.configuration.AmbariServerConfigurationKey}}. Sensitive properties have property type of {{org.apache.ambari.server.configuration.ConfigurationPropertyType#PASSWORD}}.

Using this information, _if this feature is enabled_, the Ambari server should encrypt sensitive values before storing them in the {{ambari_configuration}} table in the Ambari DB.

The Ambari server should encrypt sensitive configuration values if the following has been met:
 * A master key has been setup using the ""ambari-server setup-security"" CLI (using option #2 - Encrypt passwords stored in ambari.properties file)
 * The Ambari server configuration property named ""{{security.server.encrypt_sensitive_data}}"" is set to ""true""

If encrypting sensitive data:
 * the value should be encrypted using a secure symmetric key encryption algorithm. For example AES - [https://aesencryption.net/].
 * the encryption key should be the previously set master key, or some reproducible encoding of it.
 * the encrypted bytes should be converted to a hex string
 * the value should be stored in the relevant field such that the value is declared as encrypted.
 ** for example:
{noformat}
""password"" : ""${enc=aes256_base64, value=5248...303d}""{noformat}

 ** this is needed in the event {{server.security.encrypt_sensitive_data}} is changed to false, but there are still encrypted values in the database.

Encrypted data needs to be decrypted before being used or returned via the REST API. The data may be re-encrypted depending on use.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-13 14:36:18,27
13204219,Cover host component view with unit tests,"Following files to cover:
app/views/main/host/details/host_component_view.js

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-12-13 12:11:00,19
13204100,Scale hosts does not install component if service is in maintenance mode,"STR:

# Create cluster via blueprint
# Turn on maintenance mode for some service
# Add host to cluster (scale up)

Result: components of the service in maintenance mode are not installed (install pending state) on the new host

Possible workaround: manually install and start such components once the host request is complete",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-12 21:11:36,21
13204025,Handle blueprint/VDF stack version mismatch,"If a repository version is explicitly specified in the cluster creation request (using {{repository_version_id}} or {{repository_version}}), then Ambari should reject the cluster creation request if the blueprint and the VDF stacks are different (eg. HDP 2.6 vs HDP 3.0).  If it allowed deployment, the cluster would run into various errors due to the mismatch (eg. UI glitches, attempting to install the wrong package, etc.)",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-12 15:25:59,21
13203982,Parse error should be reported as Bad Request,"Any JSON errors encountered while parsing Add Service request should be reported as HTTP 400 Bad Request.  Currently such errors result in HTTP 500 Server Error.

{noformat}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /api/v1/clusters/TEST/services. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>java.io.UncheckedIOException: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot deserialize instance of `java.util.HashSet` out of VALUE_STRING token
 at [Source: (String)&quot;{  &quot;operation_type&quot;: &quot;ADD_SERVICE&quot;,  &quot;components&quot;: &quot;asdf&quot;}&quot;; line: 1, column: 52] (through reference chain: org.apache.ambari.server.controller.AddServiceRequest[&quot;components&quot;])
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-12-12 12:25:53,21
13203829,Allow skipping parts of Add Service request validation,"Provide ability to disable parts of the validation for the Add Service request, eg. configuration validation and topology validation.  Some parts still need to be validated (eg. it probably makes no sense to add unknown services).",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-11 21:50:14,21
13203804,Update Stack Classloader Loading Pattern,Update the {{StackInfo}} to load an instance of a class with its own classloader,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-11 19:42:12,33
13203746,"Ambari is not updating core-site : ""ha.zookeeper.quorum"" onece a zookeeper server is added or removed","Ambari is not updating in core-site.xml of HDFS  if a zookeeper server is added or removed in cluster managed by ambari.

Ideally zookeeper has to update the server URL in :  ""ha.zookeeper.quorum"" 

this issue is same as : AMBARI-7217 and its now happening in latest ambari versions.
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-12-11 15:14:18,3
13203736,Layout recommendation adds unwanted components,"STR:

# Install ZooKeeper
# Try to add Metrics Collector and Metrics Monitor using Add Service request

{noformat}
{
  ""operation_type"": ""ADD_SERVICE"",
  ""components"": [
    { ""name"": ""METRICS_COLLECTOR"", ""hosts"": [ { ""fqdn"": ""c7401.ambari.apache.org"" } ] },
    { ""name"": ""METRICS_MONITOR"", ""hosts"": [ { ""fqdn"": ""c7401.ambari.apache.org"" } ] }
  ]
}
{noformat}

Result: Metrics Grafana is also installed.  It is being added by the layout recommendation, which should not be invoked if full layout is explicitly specified in the request (ie. no {{""services""}} part is present).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-11 14:26:40,21
13203693,Cover Service controllers with unit tests,"Following files to cover:
* app/controllers/main/service/add_controller.js
* app/controllers/main/service/item.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-12-11 12:06:59,19
13203657,Add better host-matching syntax to Add Service request,"Change syntax of the Add Service request:

# allow multiple hosts per component
# simplify {{component_name}} to {{name}} to be more consistent with blueprints

{noformat}
   ""components"" : [
      {
	  ""name"" : ""DATANODE"",
          “hosts”: [
             { ""fqdn"" : ""c7401.ambari.apache.org"" },
             { ""fqdn"" : ""c7402.ambari.apache.org"" },
             ...
          ]
      },
      {
         ""name"" : ""NODEMANAGER"",
         “hosts”: [
             { ""fqdn"" : ""c7401.ambari.apache.org"" },
             { ""fqdn"" : ""c7402.ambari.apache.org"" },
             ...
          ]
      }
  ],
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-12-11 09:28:19,21
13203429,[Log Search UI] Populate `Component Name` in validator,"Populate the `component name` field in validator when it is available from the configuration object. 

If more than component is available populate with the first one.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Sub-task,Minor,2018-12-10 13:29:21,26
13203400,[Log Search UI] Audit log list: missing event time value in the log list,The value of the event time is not displayed at all in the audit log list.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Improvement,Major,2018-12-10 10:37:40,26
13203295,Duplicate kerberos_descriptor name reported as HTTP 500,"Duplicate {{kerberos_descriptor}} name is reported as HTTP 500 Server Error.  It should result in HTTP 409 Conflict.

{noformat}
$ curl -X POST -d @metrics_descriptor.json ""http://${AMBARI_SERVER}:8080/api/v1/kerberos_descriptors/metrics_descriptor""
HTTP/1.1 201 Created
$ curl -X POST -d @metrics_descriptor.json ""http://${AMBARI_SERVER}:8080/api/v1/kerberos_descriptors/metrics_descriptor""
HTTP/1.1 500 Server Error
...
  Detail: Key (kerberos_descriptor_name)=(metrics_descriptor) already exists.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-12-09 18:25:22,21
13203152,Integrate add service API with StackAdvisor's Configuration recommendations,"This work involves integrating with the StackAdvisor's recommendations for configuration for the ""Add Service"" request.
",pull-request-available,[],AMBARI,Task,Critical,2018-12-07 21:53:30,20
13203149,Admin View cannot be Deployed Due to META-INF Ordering,"The new build process which was changed in AMBARI-24804 is causing JARs to be built slightly differently:

{code:title=unzip -l ambari-admin-2.7.3.0.0.jar | grep META-INF}
        0  12-07-2018 10:53   META-INF/
      133  12-07-2018 10:53   META-INF/MANIFEST.MF
        0  12-07-2018 10:53   META-INF/maven/
        0  12-07-2018 10:53   META-INF/maven/org.apache.ambari/
        0  12-07-2018 10:53   META-INF/maven/org.apache.ambari/ambari-admin/
    11110  12-05-2018 14:31   META-INF/maven/org.apache.ambari/ambari-admin/pom.xml
      118  12-07-2018 10:53   META-INF/maven/org.apache.ambari/ambari-admin/pom.properties
{code}

{code:title=unzip -l ambari-admin-3.0.0.0-SNAPSHOT.jar | grep META-INF}
    473  12-07-2018 11:37   META-INF/MANIFEST.MF
        0  12-07-2018 11:37   META-INF/
        0  12-07-2018 11:37   META-INF/maven/
        0  12-07-2018 11:37   META-INF/maven/org.apache.ambari/
        0  12-07-2018 11:37   META-INF/maven/org.apache.ambari/ambari-admin/
      106  12-07-2018 11:37   META-INF/maven/org.apache.ambari/ambari-admin/pom.properties
    11358  12-07-2018 11:37   META-INF/LICENSE
     5060  12-07-2018 11:37   META-INF/maven/org.apache.ambari/ambari-admin/pom.xml
      267  12-07-2018 11:37   META-INF/DEPENDENCIES
      165  12-07-2018 11:37   META-INF/NOTICE
{code}

Since the file {{META-INF/MANIFEST.MF}} appears before {{META-INF}}, it exposes a bug in the {{ViewExtractor}}",pull-request-available,['ambari-admin'],AMBARI,Bug,Blocker,2018-12-07 21:12:39,31
13203121,Allow unattended mpack install with purge,"The question that pops up if {{purge}} is specified prevents installing mpacks without user input.  The default answer is ""no"", which gets automatically selected in {{silent}} mode, causing the installation to be cancelled.

{noformat:title=ambari-server install-mpack --purge --silent --verbose --mpack=...}
...
CAUTION: You have specified the --purge option with --purge-list=['stack-definitions', 'mpacks']. This will replace all existing stack definitions, management packs currently installed.
Are you absolutely sure you want to perform the purge [yes/no]? (no)
ERROR: Exiting with exit code 1.
REASON: Management pack installation cancelled by user
{noformat}

The default answer should be ""no"" for regular mode, but ""yes"" for silent mode, to make unattended install possible.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-07 18:41:39,21
13203078,Update Ambari audit logger to handle proxied users,"
Update Ambari audit logger to handle proxied users.

All logging should be able to specify the authenticated proxy user as well as the proxied user.

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-07 16:13:45,13
13203054,setup-ldap can not be executed non-interactively when using SSL without custom TrustStore,"We should provide a way to our end user to execute {{ambari-server setup-ldap}} in a non-interactive way (i.e. all answers are provided by command line options).

This is not the case when we would like to setup a secure LDAP (SSL is set to true) but we do not want to use a custom trust store. In this case the following question(s) are being asked:
1. Do you want to provide custom TrustStore for Ambari?
2. Optionally: if custom trust store was set previously the tool displays the earlier configuration and asks the following: Do you want to remove these properties?

Sample run:
{code:java}
[root@c7401 ~]# ambari-server setup-ldap --ambari-admin-username=admin --ambari-admin-password=admin --ldap-url=ad-nano.qe.hortonworks.com:636 --ldap-secondary-url=: --ldap-user-class=user --ldap-user-
attr=sAMAccountName --ldap-group-class=group --ldap-group-attr=cn --ldap-member-attr=member --ldap-dn=distinguishedName --ldap-base-dn=CN=Users,DC=hwqe,DC=hortonworks,DC=com --ldap-bind-anonym=false --ldap-manager-dn=cn=manager,cn=Users,dc=hwqe,dc=hortonworks,dc=com --ldap-manager-password=TestUser123 --ldap-referral=follow --ldap-sync-username-collisions-behavior=skip --ldap-force-lowercase-usernames=false --ldap-pagination-enabled=false --ldap-ssl=true --ldap-sync-disable-endpoint-identification=true --ldap-force-setup --ldap-save-settings --ldap-enabled-ambari=true --ldap-manage-services=true --ldap-enabled-services=* --ldap-user-group-member-attr=myMemberOf
Using python  /usr/bin/python

Fetching LDAP configuration from DB.
Primary LDAP Host (ad-nano.qe.hortonworks.com): 
Primary LDAP Port (636): 
Secondary LDAP Host <Optional>: 
Secondary LDAP Port <Optional>: 
Use SSL [true/false] (true): 
Disable endpoint identification during SSL handshake [true/false] (true): 
Do you want to provide custom TrustStore for Ambari [y/n] (y)?n
The TrustStore is already configured: 
  ssl.trustStore.type = jks
  ssl.trustStore.path = /tmp/ambari-server-truststore
  ssl.trustStore.password = keystore
Do you want to remove these properties [y/n] (y)? y
User object class (user): 
User ID attribute (sAMAccountName): 
User group member attribute (myMemberOf): 
Group object class (group): 
Group name attribute (cn): 
Group member attribute (member): 
Distinguished name attribute (distinguishedName): 
Search Base (CN=Users,DC=hwqe,DC=hortonworks,DC=com): 
Referral method [follow/ignore] (follow): 
Bind anonymously [true/false] (false): 
Bind DN (cn=manager,cn=Users,dc=hwqe,dc=hortonworks,dc=com): 
Enter Bind DN Password: 
Confirm Bind DN Password: 
Handling behavior for username collisions [convert/skip] for LDAP sync (skip): 
Force lower-case user names [true/false] (false):
Results from LDAP are paginated when requested [true/false] (false):
====================
Review Settings
====================
Primary LDAP Host (ad-nano.qe.hortonworks.com):  ad-nano.qe.hortonworks.com
Primary LDAP Port (636):  636
Use SSL [true/false] (true):  true
User object class (user):  user
User ID attribute (sAMAccountName):  sAMAccountName
User group member attribute (myMemberOf):  myMemberOf
Group object class (group):  group
Group name attribute (cn):  cn
Group member attribute (member):  member
Distinguished name attribute (distinguishedName):  distinguishedName
Search Base (CN=Users,DC=hwqe,DC=hortonworks,DC=com):  CN=Users,DC=hwqe,DC=hortonworks,DC=com
Referral method [follow/ignore] (follow):  follow
Bind anonymously [true/false] (false):  false
Handling behavior for username collisions [convert/skip] for LDAP sync (skip):  skip
Force lower-case user names [true/false] (false): false
Results from LDAP are paginated when requested [true/false] (false): false
ambari.ldap.connectivity.bind_dn: cn=manager,cn=Users,dc=hwqe,dc=hortonworks,dc=com
ambari.ldap.connectivity.bind_password: *****
ambari.ldap.advanced.disable_endpoint_identification: true
ambari.ldap.manage_services: true
ambari.ldap.enabled_services: *
Saving LDAP properties...
Saving LDAP properties finished
Ambari Server 'setup-ldap' completed successfully.{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-07 14:22:19,27
13203005,[Log Search UI] There is console error about TimeZonePicker initialisation,"There is an error in console when the application is initialising:
{code:java}
ERROR TypeError: _co.initMap is not a function{code}",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Improvement,Trivial,2018-12-07 10:37:03,26
13202950,Ambari should optionally generate auth-to-local rules for the Kerberos identities of all components of installed services,"Ambari should optionally generate auth-to-local rules for the Kerberos identities of all components of installed services.  

Currently Ambari will generate auth-to-local rules for the installed components of installed services.  This is generally the accepted behavior. However, there may be cases where identities from remote clusters (using the same Kerberos realm) need to be translated to local names.  

A use case may be that some slave component for a service is installed on a remote cluster, but that component is not installed on the local cluster.  However a master component of that service is installed on the local cluster and the slave component from the remote cluster needs to communicate with it. 

The solution is to add a new property to {{kerberos-env}}, maybe named something like {{include_all_components_in_auth_to_local_rules}}, where the default value is {{false}}.  If set to {{true}}, when building the auth-to-local rules, Ambari should add the rules for all components of installed services, not just the installed components (which is what it does today).  

The relevant code to change is in {{org.apache.ambari.server.controller.KerberosHelperImpl#setAuthToLocalRules}}. 


",kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-07 02:34:56,27
13202944,Log Search UI: cannot save filters + select all selecting the ovirrides as well,"Clicking out a log level from debug to nothing, save button was still disabled.

Also clicking select all selected the overrides rows as well (as no hostname set there or anything, not sure what is expected there, hopefully we would not save that with filled empty hostname but with one debug log level)",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Improvement,Blocker,2018-12-07 01:28:24,26
13202801,Set unique configuration version tag,"Add Service does not work for add-delete-add scenario, because configuration tag is constant {{ADD_SERVICE}}.  It should include some changing part, eg. timestamp suffix.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-12-06 13:24:25,21
13202791,Process Kerberos descriptor for Add Service request,"The goal of this task is to process the Kerberos descriptor provided in the Add Service request, and merge it with the cluster's existing one.  Only settings for newly added services should be accepted.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-06 12:35:10,21
13202787,Support component-level provision_action in Add Service request,"AMBARI-24988 added support for request-level {{provision_action}} to the Add Service request.  The goal of this task is to add support at the host component-level, similar to blueprints.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-06 11:53:50,21
13202756,Ambari hides information about cred_store generation failures. Resulting in confusing errors at later stages,"Component was failing to install due to:

    
    
    Caught an exception while executing custom service command: <type 'exceptions.OSError'>: [Errno 2] No such file or directory: '/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks'; [Errno 2] No such file or directory: '/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks'
    
    Command failed after 1 tries
    

The reason was an empty password provided in blueprint. However it took lots
of time to debug this. Since ambari won't show any information regarding
failures during cred_store generation.

The goal is too fail earlier and show output of failed generation command.  
So with the patch it looks like this:

    
    
    Caught an exception while executing custom service command: <class 'resource_management.core.exceptions.ExecutionFailed'>: Execution of '/usr/lib/jvm/java-openjdk/bin/java -cp '/var/lib/ambari-agent/cred/lib/*' org.apache.hadoop.security.alias.CredentialShell create dpprofiler.spnego.signature.secret -value '[PROTECTED]' -provider jceks://file/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks' returned 1. SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
    Dec 06, 2018 11:05:45 AM org.apache.hadoop.util.NativeCodeLoader <clinit>
    WARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    java.lang.IllegalArgumentException: Empty key
    	at javax.crypto.spec.SecretKeySpec.<init>(SecretKeySpec.java:96)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.innerSetCredential(AbstractJavaKeyStoreProvider.java:304)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.createCredentialEntry(AbstractJavaKeyStoreProvider.java:269)
    	at org.apache.hadoop.security.alias.CredentialShell$CreateCommand.execute(CredentialShell.java:365)
    	at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:68)
    	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    	at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:442); Execution of '/usr/lib/jvm/java-openjdk/bin/java -cp '/var/lib/ambari-agent/cred/lib/*' org.apache.hadoop.security.alias.CredentialShell create dpprofiler.spnego.signature.secret -value '[PROTECTED]' -provider jceks://file/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks' returned 1. SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
    Dec 06, 2018 11:05:45 AM org.apache.hadoop.util.NativeCodeLoader <clinit>
    WARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    java.lang.IllegalArgumentException: Empty key
    	at javax.crypto.spec.SecretKeySpec.<init>(SecretKeySpec.java:96)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.innerSetCredential(AbstractJavaKeyStoreProvider.java:304)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.createCredentialEntry(AbstractJavaKeyStoreProvider.java:269)
    	at org.apache.hadoop.security.alias.CredentialShell$CreateCommand.execute(CredentialShell.java:365)
    	at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:68)
    	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    	at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:442)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-12-06 11:13:08,5
13202729,Directory/File creation hangs if relative path is supplied with cd_access,"
    Directory('hadoofs/fs1/ams-hbase-wal', owner='root', group='root', create_parents=True, recursive_ownership=True, cd_access='a')
    

Hangs as seen on s3a clusters when one of paths was set to relative by an
accident.

",pull-request-available,[],AMBARI,Bug,Major,2018-12-06 08:56:36,5
13202670,Starting JPA persistence service sometimes throws IllegalStateException,"Starting JPA persistence service sometimes throws IllegalStateException.  

For example:
{noformat}
Exception in thread ""main"" java.lang.IllegalStateException: Persistence service was already initialized.
	at com.google.common.base.Preconditions.checkState(Preconditions.java:173)
	at com.google.inject.persist.jpa.JpaPersistService.start(JpaPersistService.java:104)
	at com.google.inject.persist.jpa.AmbariJpaPersistService.start(AmbariJpaPersistService.java:27)
	at 
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-06 01:22:57,30
13202650,Allow Stack To Define Custom Rolling Orchestration Logic,Use the SPI to allow stacks to provide their own code-based logic classes.  The first iteration will allow a stack to compute how many slave components can run at the same time.,pull-request-available,"['ambari-server', 'ambari-server-spi']",AMBARI,Task,Critical,2018-12-05 22:07:49,33
13202506,START_ONLY provision action may be applied to the wrong component,"If component XY is provisioned with {{START_ONLY}}, and component X is a prefix of XY, then the provision action is applied to both X and XY.

{noformat:title=blueprint}
{
  ""host_groups"": [
    {
      ""components"": [
        { ""name"": ""HIVE_SERVER"" },
        { ""name"": ""HIVE_SERVER_INTERACTIVE"", ""provision_action"": ""START_ONLY"" },
...
{noformat}

{noformat:title=ambari-server.log}
AmbariManagementControllerImpl:3149 - Skipping create of INSTALL task for HIVE_SERVER on c7401.ambari.apache.org.
AmbariManagementControllerImpl:3149 - Skipping create of INSTALL task for HIVE_SERVER_INTERACTIVE on c7401.ambari.apache.org.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-05 11:29:54,21
13202505,Disallow changing Kerberos-related configs in Add Service request,"Ambari should not allow a user to set configurations in the {{kerberos-env}} and {{krb5-conf}} configuration types within the complex Add Service request.  Generally, setting these configurations on an existing Kerberized cluster could be problematic, since existing services already depend upon the existing configuration to interact with the KDC.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-05 11:29:41,21
13202494,Not supported actions(like Start/Stop) should not be shown for HDFS/YARN clients in WL cluster with externalized HDFS/YARN,"Start/Stop operations should not be shown/should be disabled for HDFS/YARN clients.
But as of Ambari 2.7, they are visible in Service actions. 

Also there are multiple other actions which are currently enabled and are not applicable for the clients(listed below). These should be masked in the UI and not be shown to the user. Please clarify , if there is any discrepancy in understanding.

*HDFS Client :*
Manage Journal Nodes
Add new HDFS Namespace
Rebalance HDFS
restart all

*YARN Client:*
Refresh YARN Capacity Scheduler.
Enable resourceManager HA
restart All",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-05 11:17:24,19
13202310,Hostname update command does not work if the new hostnames contain the old hostnames also,"
In AMBARI-24840, we made a fix so that the uppercase hostnames are taken care of.

Another issue related to hostnames change is about duplicate hostnames. If the hostnames file contains the pre-existing hostnames, the update process fails with exception.


",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-12-04 16:50:00,13
13202291,Updates to the SPI for Upgrade Action Operations,"During implementation of new {{UpgradeAction}}s, there were numerous places where the SPI needed to be updated:

- Some added methods for convenience
- Inclusion of {{UpgradeInformation}} instances when executing {{UpgradeAction}}s so that the actions have access to some information, such as repo versions and direction.
- Allowing creation of configuration types and removal of entire configuration types
",pull-request-available,['ambari-server-spi'],AMBARI,Task,Blocker,2018-12-04 15:20:06,31
13202259,Ambari Upgrade from 2.6.2.2 to 2.7.1.0 fails in Schema upgrade phase due to long certificate ,"I am trying to upgrade to ambari-2.7.0 using  command : *ambari-server upgrade*

the upgrade operation is failing on Schema upgrade phase

The exception is : 

{code:java}
Internal Exception: java.sql.BatchUpdateException: Batch entry 2 INSERT INTO ambari_configuration (property_name, category_name, property_value) VALUES ('ambari.sso.provider.certificate','sso-configuration',<LONG VERTIFICATE VALUE>') was aborted: ERROR: value too long for type character varying(2048)  Call getNextException to see other errors in the batch.
{code}


and i see as per https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/Ambari-DDL-MySQL-CREATE.sql#L125

ambari is having : 

{code:java}
CREATE TABLE ambari_configuration (
  category_name VARCHAR(100) NOT NULL,
  property_name VARCHAR(100) NOT NULL,
  property_value VARCHAR(2048),
  CONSTRAINT PK_ambari_configuration PRIMARY KEY (category_name, property_name));
{code}

property_value as 2048 but i am having certificate length 2050 and ambari is not accepting it.


this should be fixed.


",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-04 12:46:20,1
13202253,Commands timeout if stdout has non-unicode symbols.,"
    
    ERROR 2018-12-03 18:08:08,694 ActionQueue.py:198 - Exception while processing EXECUTION_COMMAND command
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 191, in process_command
        self.execute_command(command)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 379, in execute_command
        self.commandStatuses.put_command_status(command, role_result)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CommandStatusDict.py"", line 77, in put_command_status
        is_sent, correlation_id = self.force_update_to_server({command['clusterId']: [report]})
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CommandStatusDict.py"", line 95, in force_update_to_server
        correlation_id = self.initializer_module.connection.send(message={'clusters':reports_dict}, destination=Constants.COMMANDS_STATUS_REPORTS_ENDPOINT, log_message_function=CommandStatusDict.log_sending)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/security.py"", line 137, in send
        body = json.dumps(message)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/__init__.py"", line 230, in dumps
        return _default_encoder.encode(obj)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 202, in encode
        chunks = list(chunks)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 426, in _iterencode
        for chunk in iterencode_dict(o, current_indent_level):
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 400, in _iterencode_dict
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 400, in _iterencode_dict
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 323, in _iterencode_list
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 382, in _iterencode_dict
        yield _encoder(value)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 48, in py_encode_basestring_ascii
        s = s.decode('utf-8')
      File ""/usr/lib64/python2.7/encodings/utf_8.py"", line 16, in decode
        return codecs.utf_8_decode(input, errors, True)
    UnicodeDecodeError: 'utf8' codec can't decode byte 0xea in position 90211: invalid continuation byte
    

",pull-request-available,[],AMBARI,Bug,Major,2018-12-04 12:24:11,5
13202245,Knox: Reorder of query parameters breaks Ambari logic for retrieving configs,"Knox proxy sometimes modifies service config version request during transferring.
I extended logging on the ambari server to track actual requests. 
Direct API request:
{noformat}
https://172.27.16.232:8443/api/v1/clusters/cl1/configurations/service_config_versions?
service_name=HBASE&service_config_version.in(12)%7C
service_name.in(ATLAS,YARN,RANGER,HIVE,HDFS,MAPREDUCE2,SPARK2,TEZ,DRUID,ZOOKEEPER,AMBARI_METRICS,KAFKA,RANGER_KMS)&is_current=true&_=1542046206583
{noformat}
Via Knox:
{noformat}
https://ctr-e138-1518143905142-579999-01-000002.hwx.site/api/v1/clusters/cl1/configurations/service_config_versions?
service_name=HBASE&is_current=true&service_config_version.in(12)%7C
service_name.in(ATLAS,YARN,RANGER,HIVE,HDFS,MAPREDUCE2,SPARK2,TEZ,DRUID,ZOOKEEPER,AMBARI_METRICS,KAFKA,RANGER_KMS)&_=1542046206583
{noformat}

In both cases same request was used on client side:
{noformat}
/api/v1/clusters/cl1/configurations/service_config_versions?
service_name=HBASE&service_config_version.in(12)|
service_name.in(ATLAS,YARN,RANGER,HIVE,HDFS,MAPREDUCE2,SPARK2,TEZ,DRUID,ZOOKEEPER,AMBARI_METRICS,KAFKA,RANGER_KMS)&is_current=true&_=1542046206583
{noformat}

As we can see Knox proxy reordered request parameters (is_current=true was moved) and completely changed request logic.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-04 11:44:29,19
13202203,Support provision_action in complex Add Service request,"Support {{provision_action}} in complex Add Service request, similarly to blueprints.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-04 08:56:37,21
13201939,Use Ambari CLI to enable and disable trusted proxy support in Ambari,"Use Ambari CLI to enable and disable trusted proxy support in Ambari.

Information needed to be collected:
 * Enable/Disable trusted proxy support
 ** {{ambari.tproxy.authentication.enabled}} : ""true""|""false""
 * Trusted proxy user (the authenticated user allowed to declare a proxied user) details - One or more may be specified
 ** hosts from which the proxy user can connect
 *** {{ambari.tproxy.proxyuser.PROXY_USER.hosts}} : * or a comma-delimited list of hostname, ip address, CIDR Notation (intermixed is ok)
 ** users allowed to be specified as proxied users by the proxy user
 *** {{ambari.tproxy.proxyuser.PROXY_USER.users}} : *, or a comma-delimited list of usernames
 ** group for which users to be proxied are members
 *** {{ambari.tproxy.proxyuser.PROXY_USER.groups}} : *, or a comma-delimited list of group names

{noformat}
[root@c7402 ~]# ambari-server  setup-trusted-proxy
Using python  /usr/bin/python
Enter Ambari Admin login: admin
Enter Ambari Admin password:

Fetching Trusted Proxy configuration from DB.
Trusted Proxy support is currently disabled
Do you want to configure Trusted Proxy support [y/n] (y)?  y
The proxy user's (local) username? knox  
Allowed hosts for knox (*)? knox.ambari.apache.org
Allowed users for knox (*)? *
Allowed groups for knox (*)? users
Add another proxy user [y/n]?  y
The proxy user's (local) username? admin  
Allowed hosts for admin (*)? 192.168.74.0/24 
Allowed users for admin (*)? tom, sam, admin
Allowed groups for admin (*)? admin_users
Add another proxy user [y/n]?  n
Save settings [y/n] (y)? y
Saving Trusted Proxy configuration...
Saving Trusted Proxy configuration finished
Ambari Server ' setup-trusted-proxy' completed successfully.
{noformat}
The REST API calls to get and set the trusted proxy configurations are
{noformat:title=GET request}
GET /api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration
{noformat}
{noformat:title=GET example response}
{
  ""href"" : ""http://c7401.ambari.apache.org:8080/api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration"",
  ""Configuration"" : {
    ""category"" : ""tproxy-configuration"",
    ""component_name"" : ""AMBARI_SERVER"",
    ""service_name"" : ""AMBARI"",
    ""properties"" : {
      ""ambari.tproxy.authentication.enabled"" : ""true"",
      ""ambari.tproxy.proxyuser.admin.groups"" : ""admin_users"",
      ""ambari.tproxy.proxyuser.admin.hosts"" : ""192.168.74.0/24"",
      ""ambari.tproxy.proxyuser.admin.users"" : ""sam, tom, admin"",
      ""ambari.tproxy.proxyuser.knox.groups"" : ""users"",
      ""ambari.tproxy.proxyuser.knox.hosts"" : ""c7401.ambari.apache.org"",
      ""ambari.tproxy.proxyuser.knox.users"" : ""*""
    },
    ""property_types"" : {
      ""ambari.tproxy.authentication.enabled"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.admin.groups"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.admin.hosts"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.admin.users"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.knox.groups"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.knox.hosts"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.knox.users"" : ""PLAINTEXT""
    }
  }
}
{noformat}
{noformat:title=POST request}
POST /api/v1/services/AMBARI/components/AMBARI_SERVER/configurations
{
  ""Configuration"": {    
    ""category"" : ""tproxy-configuration"",
    ""properties"": {
      ""ambari.tproxy.authentication.enabled"" : ""true"",
      ""ambari.tproxy.proxyuser.knox.hosts"": ""c7401.ambari.apache.org"",
      ""ambari.tproxy.proxyuser.knox.users"": ""*"",
      ""ambari.tproxy.proxyuser.knox.groups"": ""users"",
      ""ambari.tproxy.proxyuser.admin.hosts"": ""192.168.74.0/24"",
      ""ambari.tproxy.proxyuser.admin.users"": ""sam, tom, admin"",
      ""ambari.tproxy.proxyuser.admin.groups"": ""admin_users""
    }
  }
}{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-03 09:27:41,27
13201894,Handle requests from a configured trusted proxy to identify a proxied user using Kerberos,"Handle requests from a configured trusted proxy to identify a proxied user using Kerberos.

Upon receiving a request where that caller is identified using Kerberos, check to see of the request was from a (trusted) proxy.  If so, validate the trusted proxy and set the authenticated user to the proxied user specified in the ""{{doAs}}"" query parameter. 

After receiving a request where the user is to be authenticated using Kerberos, perform the following steps:
# Determine if a proxied user is specified using a ""{{doAs}}"" query parameter.  
# Using the following Ambari configuration property, determine if a proxied user can be specified from the requesting host:
** {{ambari.tproxy.proxyuser.$username.hosts}}, where $username is the username of the authenticated user (not the user specified in the doAs query parameter)
# Obtain the proxied username from the {{doAs}} query parameter
# Using the following Ambari configuration property, determine if the proxied user can be specified based on the user's username:
** {{ambari.tproxy.proxyuser.$username.users}}, where $username is the username of the authenticated user 
# Using the following Ambari configuration property, determine if the proxied user can be specified based on the groups the proxied user belong to:
** {{ambari.tproxy.proxyuser.$username.groups}}, where $username is the username of the authenticated user t",pull-request-available tproxy,['ambari-server'],AMBARI,Task,Major,2018-12-02 21:29:38,30
13201811,APT/DPKG existence check broken for packages with long names,"AMBARI-24632 addressed package existence check for system packages.  However, it may still not work correctly for packages with longer names, depending on the environment:

{noformat}
output-2.txt:2018-12-01 19:33:54,576 - Installing package unzip ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install unzip')
output-2.txt:2018-12-01 19:34:02,873 - Installing package hdp-select ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install hdp-select')
output-2.txt:2018-12-01 19:34:05,443 - Installing package zookeeper-3-0-1-0-187 ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187')
output-2.txt:2018-12-01 19:34:08,832 - Installing package zookeeper-3-0-1-0-187-server ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187-server')
output-3.txt:2018-12-01 19:34:14,450 - Installing package zookeeper-3-0-1-0-187 ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187')
output-3.txt:2018-12-01 19:34:16,253 - Installing package zookeeper-3-0-1-0-187-server ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187-server')
{noformat}

This was reproduced with non-root agent user.",pull-request-available,[],AMBARI,Bug,Minor,2018-12-01 19:39:33,21
13201805,JournalNode may fail to start due to unreadable config file,"During startup, JournalNode reads config files that belong to YARN, not HDFS (eg. {{yarn-site.xml}}), if they are present in the Hadoop config dir.

Ambari's {{File()}} provider creates files at the final location, then sets metadata (ownership and permissions).  This makes the file visible to other processes with possibly the wrong metadata.

If agent-side parallel execution is enabled, JournalNode may be started concurrently with some YARN component.  JournalNode may encounter a config file that it cannot read, because the file's permissions and owner are still 640 and root, respectively, which causes it to shutdown:

{noformat}
ERROR conf.Configuration (Configuration.java:loadResource(2999)) - error parsing conf yarn-site.xml
java.io.FileNotFoundException: /etc/hadoop/conf/yarn-site.xml (Permission denied)
{noformat}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-12-01 16:53:00,21
13201661,Improve message order preserving,"We need to update spring to 5.x version and use message order preserving option.
[https://docs.spring.io/spring/docs/5.1.0.BUILD-SNAPSHOT/spring-framework-reference/web.html#websocket-stomp-ordered-messages]",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-30 16:04:56,6
13201652,Loading on Dashboard stuck,"Dashboard doesn't load, spinner keeps spinning infinitely due to incorrect work of App.get('isHaEnabled') property in a cluster which has only HDFS_CLIENT from HDFS service.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-30 15:23:18,19
13201601,Log Search UI: if there are multiple clusters - selecting one of it does not do anything,"When there is more then one clusters, if im selecting one, service log tabs / graphs are not changing at all. also im not seeing cluster field in the url hash

expectation: it should resend the queries",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Blocker,2018-11-30 11:57:15,26
13201378,Sometimes Task Log is not refreshed in UI after operation completes,"Task Log in Background Operations popup is sometimes not refreshed after the operation completes. The log may be completely empty or may just show a previous 10-line snippet. The breadcrumb at the top is updated with the status, though. Navigating back and forth in the popup fixes it.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-29 16:08:02,6
13201324,Improve Add Service request validation,Improve request validation for Add Service request (eg. reject request to add existing service).,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-11-29 11:51:37,21
13201296,HDFS datanode process fails with class not found exception when installed with OZONE ,"When HDFS DN is started with OZONE service installed DN process fails with ""Class org.apache.hadoop.ozone.HddsDatanodeService not found"".

This is caused because OZONE_HOME env variable is set only in /usr/hdp/3.0.100.0-75/hadoop-ozone/bin/ozone/usr/hdp/3.0.100.0-75/hadoop-ozone/bin/ozone :
{code:java}
export HADOOP_OZONE_HOME=${HADOOP_OZONE_HOME:-/usr/hdp/3.0.100.0-75/hadoop-ozone}{code}
Need to define the OZONE_HOME in the hadoop-env to add the jars into DN class path.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-11-29 09:55:57,28
13201259,Document enabling LDAP configuration management,"Document enabling LDAP configuration management in {{.../ambari-server/docs/security/ldap}}

The document should be similar to [https://github.com/apache/ambari/blob/trunk/ambari-server/docs/security/sso/index.md].",pull-request-available,['ambari-server'],AMBARI,Documentation,Major,2018-11-29 07:02:24,27
13201189,Start Namenode failing during Move master NN wizard on non-HA cluster with custom hdfs service user,"Start Namenode failing during Move master NN wizard.

From NN logs:

{code:java}
2018-11-21 19:43:57,126 WARN Encountered exception loading fsimage java.io.IOException: NameNode is not formatted. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:237) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1090) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2018-11-21 19:43:57,131 INFO Stopped o.e.j.w.WebAppContext@58359ebd{/,null,UNAVAILABLE}{/hdfs}
2018-11-21 19:43:57,135 INFO Stopped ServerConnector@75e91545{HTTP/1.1,[http/1.1]}{ctr-e139-1542663976389-4877-02-000004.hwx.site:50070}
2018-11-21 19:43:57,136 INFO Stopped o.e.j.s.ServletContextHandler@2f4205be{/static,file:///usr/hdp/3.1.0.0-13/hadoop-hdfs/webapps/static/,UNAVAILABLE}
2018-11-21 19:43:57,136 INFO Stopped o.e.j.s.ServletContextHandler@319bc845{/logs,file:///grid/0/log/hdfs/cstm-hdfs/,UNAVAILABLE}
2018-11-21 19:43:57,138 INFO Stopping NameNode metrics system...
2018-11-21 19:43:57,139 INFO timeline thread interrupted.
2018-11-21 19:43:57,140 INFO NameNode metrics system stopped.
2018-11-21 19:43:57,141 INFO NameNode metrics system shutdown complete.
2018-11-21 19:43:57,141 ERROR Failed to start namenode. java.io.IOException: NameNode is not formatted. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:237) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1090) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2018-11-21 19:43:57,143 INFO No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.
2018-11-21 19:43:57,143 INFO Exiting with status 1: java.io.IOException: NameNode is not formatted.
2018-11-21 19:43:57,145 INFO SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at ctr-e139-1542663976389-4877-02-000004.hwx.site/172.27.25.135 ************************************************************/
{code}

Ambari task logs

{code:java}
2018-11-22 03:31:30,588 - The NameNode is still in Safemode. Please be careful with commands that need Safemode OFF.
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 408, in <module>
    NameNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 138, in start
    upgrade_suspended=params.upgrade_suspended, env=env)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 264, in namenode
    create_hdfs_directories(name_service)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 336, in create_hdfs_directories
    nameservices=name_services
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 677, in action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 674, in action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 373, in action_delayed
    self.action_delayed_for_nameservice(None, action_name, main_resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 395, in action_delayed_for_nameservice
    self._assert_valid()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 334, in _assert_valid
    self.target_status = self._get_file_status(target)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 497, in _get_file_status
    list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 214, in run_command
    return self._run_command(*args, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 282, in _run_command
    _, out, err = get_user_call_output(cmd, user=self.run_user, logoutput=self.logoutput, quiet=False)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/get_user_call_output.py"", line 62, in get_user_call_output
    raise ExecutionFailed(err_msg, code, files_output[0], files_output[1])
resource_management.core.exceptions.ExecutionFailed: Execution of 'curl -sS -L -w '%{http_code}' -X GET -d '' -H 'Content-Length: 0' --negotiate -u : 'http://ctr-e139-1542663976389-4877-02-000004.hwx.site:50070/webhdfs/v1/tmp?op=GETFILESTATUS' 1>/tmp/tmpC3TR3n 2>/tmp/tmpTlrH_1' returned 7. curl: (7) Failed to connect to ctr-e139-1542663976389-4877-02-000004.hwx.site port 50070: Connection refused
000
{code}

Customized service users and ambari agent user is enabled in the test.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-11-28 20:41:28,19
13201178,Expose Upgrade Custom Server Actions via an Ambari Client Library,"During a stack upgrade, it is possible for upgrade packs to specify custom Java classes which will run in order to alter configurations or perform other tasks on the cluster. The following is an example of this type of action:

{code}
<task xsi:type=""server_action"" class=""org.apache.ambari.server.serveraction.upgrades.RangerKmsProxyConfig"">
  <summary>Adding Ranger proxy user properties under kms-site</summary>
</task>
{code}

Historically, these tasks have typically been used to alter configurations of the cluster using more complex logic than the XML markup in an upgrade pack could provide. 

With stacks and mpacks being moved out of Apache source control, Ambari needs a way for 3rd party developers to provide these actions with the delivery of their stack.

- The Ambari API/SPI created as part of AMBARI-24685 will need to include classes which provide retrieval and modification of configurations in addition to contracts for the server-side action
- Ambari will need to scan for matching classes and register them during the upgrade",pull-request-available,['ambari-server-spi'],AMBARI,Task,Blocker,2018-11-28 19:11:46,31
13201163,Integrate new Add Service Request with stack advisor layout recommendation,"In the new ""Add Service"" API, users of the API will be able to list out a set of services that need to be added to an existing cluster, without specifying any individual components or specifying where the components will be hosted. 

In the event that an ""Add Service"" request is submitted, with only service names specified, then the ""Add Service"" deployment code must implement the following: 

1. Based on the services included in the request, make a call to the StackAdvisor for component layout recommendations. 

The following code shows an example of creating this request: 

{code}org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper#createRecommendationCommand{code}

2. Take the output of the StackAdvisor, and use that to determine the list of components to install and start, as well as the hosts that will include these components.  

",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-28 17:59:00,20
13201082,Summary panel should not have any information if only client component is installed in a cluster,"# [^HDFS_client_installed.png] does not list number of HDFS clients installed
# Other data in summary panel fetched from ambari metrics api should be made hidden (like Disk usage etc)
# Same needs to be done for Yarn client service
",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-28 12:30:42,19
13201011,Ambari is unable to parse {{hostname}} in {hdfs-site/dfs.datanode.address} and is triggering alert for datanode process,"Ambari is unable to parse \\{\{hostname\}\} in {hdfs-site/dfs.datanode.address} and is triggering alert for datanode process

I changed the value of dfs.datanode.address in  Advanced hdfs configuration using ambari 

i set value to   \\{\{hostname\}\}:50010 ( default value is 0.0.0.0:50010 ) 

Now after restart datanode is working fine and but ambari is triggering alert on datanode process with error : 

""Connection failed: [Errno -2] Name or service not known to  \\{\{hostname\}\}:50010"" 

expected result : Ambari shouldnt trigger the alert and it should pass \\{\{hostname\}\} correctly to localhost
",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-11-28 05:00:03,3
13200888,Remove warning about requirement for IPA password policy without expiration in Ambari kerberos wizard,"The Ambari kerberos wizard for Existing FreeIPA displays a warning about setting up a password policy without expiration for the kerberos principals.

As these (user and service) principals are not created with a password, the password expiration policy does not apply to them. I verified this by maintaining a cluster by maintaining a kerberized cluster for 120+ days, where the password for my ldapbind (and other accounts that do have passwords) expired in 90 days per default policy, without any impact to my kerberos principals or cluster operations.

Unless we've seen contradictory information, let's please remove this warning from the wizard to avoid confusing users on what is needed here.",pull-request-available,['ambari-web'],AMBARI,Task,Minor,2018-11-27 15:56:38,27
13200830,Ambari YARN Queue Manager allows duplicate names when using 'rename' functionality,"Using the rename functionality in Ambari YARN Queue Manager, it is possible to set multiple queues of the same name.
 # On a new, empty cluster, create an ""example"" parent queue, and refresh YARN config
 # Create another new parent queue named ""MainExample""
 # Add two leaf queues into ""MainExample"", but note that Ambari will indicate that a leaf queue named ""example"" cannot be created as a queue with that name already exists. Therefore we now call the new leaf queues ""leaf1"" and ""leaf2"" instead.
 # The rename option will allow to rename the queue ""leaf1"" into ""example"" without any error (this is the bug, as it should give the same error as in step 3).
 # Saving and attempting to use the configuration after these steps will result in RM failing with a variety of errors.

We need logic in the Ambari YARN Queue Manager to verify new queues for duplicate queue names when using the rename function.",pull-request-available,['ambari-views'],AMBARI,Bug,Minor,2018-11-27 11:13:57,3
13200655,Create Tproxy configuration provider and supporting infrastructure,"Create Tproxy configuration provider and supporting infrastructure.

Also create a common configuration provider infrastructure for Ambari Server Configuration data. 
",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-26 19:15:58,30
13200605,Counter installedClients for AMBARI_INFRA_SOLR is null,"App.Service.find('AMBARI_INFRA_SOLR').get('installedClients') = null,
Should return number of installed clients.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-26 15:40:17,19
13200517,Use Ambari CLI to specify which services should be setup for LDAP integration,"Use Ambari CLI to specify which services should be setup for LDAP integration.
{noformat:title=Example}
[root@c7402 ~]# ambari-server setup-ldap
Using python  /usr/bin/python
Currently 'no auth method' is configured, do you wish to use LDAP instead [y/n] (y)? y
Enter Ambari Admin login: admin
Enter Ambari Admin password:

Fetching LDAP configuration from DB. No configuration.
Please select the type of LDAP you want to use [AD/IPA/Generic](Generic):
Primary LDAP Host (ldap.ambari.apache.org): c7401.ambari.apache.org
Primary LDAP Port (389):
Secondary LDAP Host <Optional>:
Secondary LDAP Port <Optional>:
Use SSL [true/false] (false):
User object class (posixUser):
User ID attribute (uid):
User group member attribute (memberOf): 
Group object class (posixGroup):
Group name attribute (cn):
Group member attribute (memberUid):
Distinguished name attribute (dn):
Search Base (dc=ambari,dc=apache,dc=org):
Referral method [follow/ignore] (follow):
Bind anonymously [true/false] (false):
Bind DN (uid=ldapbind,cn=users,dc=ambari,dc=apache,dc=org): uid=admin,cn=users,dc=ambari,dc=apache,dc=org
Enter Bind DN Password:
Confirm Bind DN Password:
Handling behavior for username collisions [convert/skip] for LDAP sync (skip):
Force lower-case user names [true/false]:true
Results from LDAP are paginated when requested [true/false]:true
Use LDAP authentication for Ambari [y/n] (n)?
Manage LDAP configurations for eligible services [y/n] (n)? y
 Manage LDAP for all services [y/n] (n)?
    Manage LDAP for HDFS [y/n] (y)? y
    Manage LDAP for YARN [y/n] (y)? y
    ...
Save settings [y/n] (y)? y
Saving LDAP properties...
Saving LDAP properties finished
Ambari Server 'setup-ldap' completed successfully.
{noformat}
NOTE: this will require obtaining an Ambari administrator username and password to GET, PUT, and POST to the Ambari REST API.

Note: ""User group member attribute (memberOf)"" is to be added to populate the existing {{ambari.ldap.attributes.user.group_member_attr}} Ambari configuration property (See {{org.apache.ambari.server.configuration.AmbariServerConfigurationKey#USER_GROUP_MEMBER_ATTRIBUTE}})",pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-11-26 10:02:26,27
13200492,Logsearch: use os timezone in Logfeeder,Logfeeder sets the default timezone to GMT at start up. Most service logs' logtime are in os timezone. If os timezone is set to other than UTC Logfeeder does not convert the logtime values to UTC when sending the log entry to Solr and Logsearch portal shows invalid log time values.,pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-11-26 07:09:45,16
13200285,Test ordering issue in ExecutionCommandWrapperTest,"Success/failure of test cases in {{ExecutionCommandWrapperTest}} depends on the execution order.  The two interdependent tests are {{testExecutionCommandNoRepositoryFile}} and {{testGetExecutionCommand}}, executing the former first makes the latter fail.

{noformat:title=order 1}
  @Test
  public void testExecutionCommandNoRepositoryFile_first() throws Exception {
    testExecutionCommandNoRepositoryFile();
    testGetExecutionCommand();
  }
{noformat}

{noformat:title=result 1}
[INFO] Running org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 11.002 s <<< FAILURE! - in org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
[ERROR] testExecutionCommandNoRepositoryFile_first(org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest)  Time elapsed: 0.031 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest.testGetExecutionCommand(ExecutionCommandWrapperTest.java:202)
	at org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest.testExecutionCommandNoRepositoryFile_first(ExecutionCommandWrapperTest.java:320)
{noformat}

{noformat:title=order 2}
  @Test
  public void testGetExecutionCommand_first() throws Exception {
    testGetExecutionCommand();
    testExecutionCommandNoRepositoryFile();
  }
{noformat}

{noformat:title=result 2}
[INFO] Running org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.304 s - in org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-23 16:18:51,21
13200186,Support for complex Add Service request in secure cluster,Support adding services using the complex Add Service request in clusters that have Kerberos enabled.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-23 06:28:49,21
13200038,Dir creation fails if webhdfs is enabled,"The previous clusters had webhdfs disabled as a workaround.

With webHDFS turned back on, dir creation continues to fail.

    
    
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 90, in <module>
        AmsCollector().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 345, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 48, in start
        self.configure(env, action = 'start') # for security
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 43, in configure
        hbase('master', action)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/hbase.py"", line 228, in hbase
        dfs_type=params.dfs_type
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 712, in action_create_on_execute
        self.action_delayed(""create"")
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 709, in action_delayed
        self.get_hdfs_resource_executor().action_delayed(action_name, self)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 385, in action_delayed
        self.action_delayed_for_nameservice(None, action_name, main_resource)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 407, in action_delayed_for_nameservice
        self._assert_valid()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 346, in _assert_valid
        self.target_status = self._get_file_status(target)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 509, in _get_file_status
        list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 226, in run_command
        return self._run_command(*args, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 294, in _run_command
        _, out, err = get_user_call_output(cmd, user=self.run_user, logoutput=self.logoutput, quiet=False)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/get_user_call_output.py"", line 62, in get_user_call_output
        raise ExecutionFailed(err_msg, code, files_output[0], files_output[1])
    resource_management.core.exceptions.ExecutionFailed: Execution of 'curl -sS -L -w '%{http_code}' -X GET -d '' -H 'Content-Length: 0' 'http://fakelocalhost:50070/webhdfs/v1s3a:/cloudhdp-dl-s3-2/c01/amshbase?op=GETFILESTATUS&user.name=hdfs' 1>/tmp/tmpsSkYRy 2>/tmp/tmpYQM8tv' returned 6. curl: (6) Could not resolve host: fakelocalhost; Unknown error
    000
    

",pull-request-available,[],AMBARI,Bug,Major,2018-11-22 10:38:38,5
13199927,Allow url overrides in quick link profiles,Quick link profiles provide a mechanism to control the visibility of quick links based on a set of filters. Filters based on link name should have the ability override the link url.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-21 20:59:22,20
13199854,Validate cluster name character and length requirements in the backend before creating or updating a cluster,"
Validate cluster name character and length requirements in the backend before creating or updating a cluster.

*Character Requirements*
* {{A}} through {{Z}}
* {{a}} through {{z}}
* {{0}} through {{9}}
* {{_}} (underscore)
* {{-}} (dash)

*Length Requirements*
* Minimum: 1 character
* Maximum: 100 characters

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-21 14:53:29,13
13199817,Accept legacy JSON configuration in Add Service request,"Cluster creation via blueprint accepts configuration in the following format, where the {{""properties""}} level is omitted:

{noformat}
  ""configurations"": [
    {
      ""cluster-env"": {
        ""custom-property"": ""whatever""
      }
    },
    {
      ""zoo.cfg"": {
        ""syncLimit"": ""7""
      }
    },
...
{noformat}

""Add Service"" request should accept the same format, too, but currently it results in:

{noformat}
<h3>Caused by:</h3><pre>java.lang.IllegalArgumentException: Invalid fields in cluster-env configuration: [custom-property]
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)
	at org.apache.ambari.server.topology.ConfigurableHelper.lambda$parseConfigs$1(ConfigurableHelper.java:102)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at org.apache.ambari.server.topology.ConfigurableHelper.parseConfigs(ConfigurableHelper.java:88)
	at org.apache.ambari.server.controller.AddServiceRequest.&lt;init&gt;(AddServiceRequest.java:88)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:124)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:283)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator.createFromObjectWith(ValueInstantiator.java:229)
	at com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:195)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:488)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:159)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4001)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2992)
	at org.apache.ambari.server.controller.AddServiceRequest.of(AddServiceRequest.java:115)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.createAddServiceRequest(ServiceResourceProvider.java:1242)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.processAddServiceRequest(ServiceResourceProvider.java:1232)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.createResourcesAuthorized(ServiceResourceProvider.java:257)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-11-21 12:07:55,21
13199678,Bug at MetricsCollectorHAManager class,"Here org/apache/ambari/server/controller/metrics/MetricsCollectorHAManager.java:80
the second return statement is inside a loop without any condition. That breaks an execution flow
{code:java}
    if (externalMetricCollectorsState.containsKey(clusterName)) {
      for (String externalCollectorHost : externalMetricCollectorsState.get(clusterName).keySet()) {
        if (externalMetricCollectorsState.get(clusterName).get(externalCollectorHost)) {
          return externalCollectorHost;
        }
        return refreshAndReturnRandomExternalCollectorHost(clusterName);
      }
    }{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-11-20 19:35:23,6
13199620,Recommendation configs request was failed with custom config group,"Recommendation configs request was failed with following issue:
{code:java}
Request body is not correct, error: org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse$ConfigGroup cannot be cast to java.lang.Comparable{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-11-20 15:05:19,6
13199568,Add LDAP integration support information to service information via Ambari's REST API,"Add LDAP integration support information to service information via Ambari's REST API. This information should be usable by Ambari's search predicate feature.

New _read-only_ properties for (stack) services should be:
 * *{{ldap_integration_supported}}* - Indicates whether the service supports LDAP integration or not
 ** Information is expected to be determined by service's meta info (see AMBARI-24907)

New _read-only_ properties for installed services should be:
 * *{{ldap_integration_supported}}* - Indicates whether the service supports LDAP integration or not
 ** Information is expected to be determined by service's meta info (see AMBARI-24907 )
 * *{{ldap_integration_enabled}}* - Indicates whether the service is configured for LDAP integration or not
 ** Information is expected to be determined by a value indicated in the service's meta info (see AMBARI-24907)
 * *{{ldap_integration_desired}}* - Indicates whether the service is chosen for LDAP integration or not (see AMBARI-24913)
 ** Information is expected to be in the Ambari configurations with the property name {{ldap_enabled_services}}

Examples:
{noformat:title=Get stack service details}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME"",
{
  ""href"" : "":URL"",
  ""StackServices"" : {
     ...
     ""ldap_integration_supported"": ""false"",
     ...
  },
  ...
{noformat}
{noformat:title=Get installed service information}
GET /api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME
{
  ""href"" : "":URL"",
  ""ServiceInfo"" : {
    ""cluster_name"" : "":CLUSTER_NAME"",
    ...
    ""ldap_integration_supported"": ""true"",
    ""ldap_integration_enabled"": ""false"",
    ""ldap_integration_desired"": ""false"",
     ...
    },
    ...
{noformat}
{noformat:title=List installed services that support LDAP integration}
GET /api/v1/clusters/:CLUSTER_NAME/services?ServiceInfo/ldap_integration_supported=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}
{noformat:title=List stack services that support LDAP integration}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services?StackServices/ldap_integration_supported=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-11-20 10:13:57,27
13199537,Apply user-defined configuration for Add Service request,"Continuing AMBARI-24917, apply any configuration specified in the request to override the stack defaults.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-20 07:46:05,21
13199424,Create tproxy-configuration category in Ambari Configurations data	,"Create tproxy-configuration category in Ambari Configurations data with the following properties:

* {{ambari.tproxy.authentication.enabled}}
** Determines whether to allow trusted proxy authentication when logging into Ambari
** {{true}} | {{false}}
* {{ambari.tproxy.proxyuser.$username.hosts}}
** List of hosts from which trusted-proxy user ‘$username’ can connect from
** {{\*}} | {{c7401.ambari.apache.org}} | {{10.42.80.64,10.42.80.65}} | {{10.222.0.0/16,10.113.221.221}}
* {{ambari.tproxy.proxyuser.$username.users}}
** List of users which the trusted-proxy user ‘$username’ can proxy for
** {{\*}} | {{user1,user2}}
* {{ambari.tproxy.proxyuser.$username.groups}}
** List of user-groups which trusted-proxy user ‘$username’ can proxy for
** {{\*}} | {{group1,group2}}

Note: {{$username}} is variable, declaring the values for a particular proxy user. For example ""knox"".",pull-request-available tproxy,['ambari-server'],AMBARI,Task,Major,2018-11-19 20:11:36,30
13199320,No need to create test jar if tests are skipped,"Some test-related tasks ({{create-sample-upgrade-check-jar}}, {{generate-test-oozie2-checks-dir}}, {{generate-test-oozie2-server-actions-dir}}) can be skipped during build if tests are skipped.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-11-19 12:22:32,21
13199299,"If service does not have client service component created then ""Run Service Check"" option should be made hidden","If service does not have client service component created then ""Run Service Check"" option should be made hidden",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-19 11:19:49,19
13199268,Allow Namenode HA setup with external namenodes in BlueprintConfigurationProcessor,"There is an increasing demand for clusters with some of the components being managed externally to Ambari.

This ticket addresses the problem where the cluster contains a HDFS_CLIENT but server HDFS server components are external and NAMENODE is HA.

In this cases, cluster template validation failed as there were 0 name nodes which is less than the expected minimum of two.

Going forward the following setup should pass validation:
- Namenode HA is enabled
- There are 0 namenodes
- all namenode dfs rpc addresses point to an external fqdn.

This will give sufficent protection against accidentally omitted namenodes, however will enable clusters with external namenodes.",pull-request-available,[],AMBARI,Task,Critical,2018-11-19 09:30:36,20
13199246,Implement complex Add Service request using default configs,"Continuing AMBARI-24901, create components and host components for the Add Service request.  Also create stack default configs, necessary for install and start tasks.

User-defined and advisor-recommended configs to be implemented separately.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-19 07:29:15,21
13199242,Ambari Server setup for non embedded DB miss out on default configuration,"If we setup Ambari Server with it's own embedded postgresql DB, we get all other configurations correctly e.g. setup command updates home.java, server.os_type, server.os_family etc in ambari.properties file that can be used during starting Ambari Server.

 

However, if we provide server setup command with arguments to use non-embedded standalone DB, server setup does not update ambari.properties and does not perform any further setup utility that can be leveraged during starting Ambari Server.

Example:

1) *ambari-server setup -j \{JDK_PATH} -s*

This command updates ambari.properties for home.java, server.os_type, server.os_family etc.

 

2) *ambari-server setup --java-home=\{JDK_PATH} --jdbc-db=postgres --jdbc-driver=\{JDBC_DRIVER_PATH} --databasehost=\{IP_ADDR} --databaseport=5432 --databasename=ambari --postgresschema=ambari --databaseusername=\{DB_USER} --databasepassword=\{DB_PASSWD} --database=postgres -s*

This command does not update ambari.properties and we need to update it manually to bring up Ambari Server. Hence, automation of server setup with server startup is blocked due to this bug.",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-11-19 06:59:11,21
13198921,Block Ozone datanode co-location with HDFS Datanode when added though the API,"* Do not allow user to call and API that add Ozone datanode to a host with HDFS datanode
 * Converse also holds
 * Make sure this is a flag at stack level and blueprints also follow the same validation rule",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-16 13:39:50,28
13198906,Update ldap-configuration category in Ambari Configurations data to contain properties to aid in automated LDAP configuration,"Update {{ldap-configuration}} category in Ambari Configurations data to contain properties to aid in automated LDAP configuration:
 * {{ambari.ldap.manage_services}}
 ** This property is used to indicate whether Ambari is to manage relevant services' LDAP configurations or not (""true"" | ""false"")
 * {{ambari.ldap.enabled_services}}
 ** This property is used to declare what services are expected to be configured for LDAP and is expected to be a comma-delimited list of services or ""{{*}}"" to indicate all services.

Examples:
{code:java|title=All services}
""ambari.ldap.manage_services"":""true"",
""ambari.ldap.enabled_services"":""*""
{code}
{code:java|title=Only Ranger}
""ambari.ldap.manage_services"":""true"",
""ambari.ldap.enabled_services"":""Ranger""
{code}
{code:java|title=Ranger, and Atlas}
""ambari.ldap.manage_services"":""true"",
""ambari.ldap.enabled_services"":""Ranger, Atlas""
{code}
{code:java|title=Do not manage services}
""ambari.ldap.manage_services"":""false"",
""ambari.ldap.enabled_services"":""""
{code}
Each service in the set of services should have indicated it supports LDAP (see BUG-114409) else it will silently be ignored.

This value should be set via Ambari's REST API or a Blueprint.

Upon setting this value via the Ambari REST API, it is expected that internal logic will be triggered to ensure the relevant services in the list are configured for LDAP or not configured for LDAP as the case may be.
 ",pull-request-available,"['ambari-server', 'security']",AMBARI,Task,Blocker,2018-11-16 12:55:11,27
13198753,Grafana: Fix for Storm Components and Storm Kafka Offset dashboards,These two dashboards error out if no data point is provided.,pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-11-15 22:06:37,34
13198703,Parallel Client Fixes for Server Side Tasks,The fix for parallel clients (AMBARI-24873) was broken for server-side action tasks,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-11-15 18:43:04,33
13198654,Update service metainfo to declare LDAP integration support,"Update service metainfo to declare LDAP integration support. The following tag may be optionally set in a service's metainfo.xml file:
{noformat}
<ldap>
 <supported>true</supported>
 <ldapEnabledTest>
    {
      ""equals"": [
        ""ranger-admin-site/ranger.authentication.method"",
        ""LDAP""
      ]
    }
 <ldapEnabledTest>
</ldap>{noformat}",pull-request-available,['ambari-sever'],AMBARI,Task,Blocker,2018-11-15 15:29:21,27
13198604,"Service display name on left navigation bar should be suffixed with ""Client"" if only client service component is present for a service","When a service only contains client service component then check service's displayname. If service's displayname does not and with ""Client"" then suffix service's displayname  with ""Client""

Current behavior:   !Dsplayname HDFS.png! 
Expected behavior:   !Displayname HDFS Client.png! ",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-15 11:59:33,19
13198530,"Handle complex ""Add Service"" request in ServiceResourceProvider","Related to AMBARI-24881, change {{ServiceResourceProvider}} to handle the complex ""Add Service"" request.  So far only stub logic is needed, details of the service creation will be filled later.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-15 06:33:02,21
13198483,[Log Search UI] Change capture countdown text,"The text should be: ""Please wait while the logs for your capture period are indexed...""",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Task,Trivial,2018-11-15 00:14:41,26
13198447,Add cluster drop down to Grafana aggregate dashboards.,"Currently, the HOST based dashboards (System-Servers, HDFS Namenodes, HBase-Regionservers etc) have a dropdown for the cluster level selection. This selection determines the set of hosts in the HOST dropdown, and the cluster is tagged in the API call made by every graph in the dashboard through the ""instanceId"" query parameter.

We need to add the cluster dropdown to aggregate dashboards as well. This cluster selection will also be tagged in the API call (instanceId) like in other dashboards.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-11-14 20:27:51,34
13198400,Reduce Visible Artifacts from the SPI,"The SPI exposes too many internal dependencies to its consumers. Some dependencies, such as {{commons-lang}} might be useful and should still be provided. Others, such as {{gson}} should be hidden.

- Remove non-essential dependencies from being seen by consumers of the SPI
- Prevent consumers of the SPI from double-copying dependencies which Ambari Server already has",pull-request-available,['ambari-server-spi'],AMBARI,Task,Critical,2018-11-14 17:07:59,31
13198364,[Log Search UI] Turn off features in the client when it is not available on backend,"Turn off features when it is not available on backend.

The list of features and their available status is under `/info/features`.

Turn off `Configuration Editor` when `metadata_patterns` is `false`.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Task,Major,2018-11-14 15:24:32,26
13198323,Infra Manager: code clean up,"* rename LOG -> logger
* remove slf4j use log4j2
* merge Job Properties and Job Parameter classes
",pull-request-available,['ambari-infra'],AMBARI,Task,Major,2018-11-14 12:59:50,16
13198258,"Sensitive service configuration values should be decrypted when processing the Ambari agent command script, if enabled","Sensitive service configuration values should be decrypted when processing the Ambari agent command script, if enabled.

During the processing of resource_management.libraries.script.script.Script#execute, the command data file is to be read in and the encrypted values in the JSON document are to be decrypted before executing the command.

Each encrypted value will be in the form of

${enc=<algorithm_encoding>, value=<value>}
For example:

${enc=aes265_hex, value=5248...303d}",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2018-11-14 08:34:29,18
13198114,Warn During Upgrade When Plugin ClassLoader is not Found,"If a stack's plugin classloader is not found, all plugin classes should be added to the set of failed plugins.",pull-request-available,[],AMBARI,Bug,Major,2018-11-13 18:06:22,31
13198079,[Log Serach UI] The Log Index Filter panel does not work,The Log Index Filter settings modal does not load the configuration.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Blocker,2018-11-13 15:25:32,26
13198055,Restart option should not be shown if service components are not created in a cluster,"In the attached screenshot ""Restart Nodemanager"" should not be shown because NodeManager service component is not present in the cluster",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-13 13:12:02,19
13198031,Fix SingleHostTopologyUpdater,"Newer Ambari installations often require clusters with partially installed services (some required components missing). These clusters typically connect to other clusters and use some of the services from the external cluster.

Ambari provides support for this by accepting blueprints with {{validate_topology=false}} flag, however the support is incomplete as usually exceptions are thrown later during the installation process with such blueprints.

Most of the exceptions are thrown in the {{SingleHostTopologyUpdater}} class which acts as a validator in addition to being a property updater.

The property update logic in {{SingleHostTopologyUpdater}} is the following:
* All {{SingleHostTopologyUpdater}} instances are associated with a component (e.g. NAMENODE)
* first, it does {{%HOSTGROUP%}} token replacement by calling the supertclas's method. *If this is successful, the replaced value is returned without any validation* (even with invalid topology)
* if there is only one matching hostgroup for the updater's associated component, *validation will be skipped too*. If the property's value contains {{localhost}}, it will be replaced with the FQDN of one of the host from the matching host groups.
* In all other cases, a validation process starts which tries to match a sequence of valid cases with the actual topology. If one of the cases match, the property will be returned unchanged. In none matches, an exception is thrown.

As it can be seen above, the validation logic is not completely consistent. There are cases where validation is bypassed. E.g. if the updater encounters a {{%HOSTROUP%}} token, it will do the replacement even if the topology is broken and the associated component is missing.

*Proposed solution*
Features:
* A general solution
* Still protects from accidentally broken topology
* Permits properties to point to external services
* Very easy to implement (just a few lines).

The proposed solution would simply leave the update logic in {{SingleHostTopologyUpdater}} as is and would restrict validation for the cases when the property contains {{localhost}}. This latter indicates that the property has the stack default value.

This way, validation would still be performed in most cases when a component is left out from the blueprint by mistake.

However, when a component is left out intentionally and the respective properties are intentionally set to point to an external FQDN or set to empty values, validation would be skipped.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-13 11:47:21,20
13197996,[Log Search UI] The component filter dropdown does not select any item,The user can not select any components from the component filter dropdown.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Blocker,2018-11-13 09:39:36,26
13197976,Add stack feature constant to support Ranger admin users password change,Current Ambari stack for Ranger supports feature to change Ranger admin users passwords through multiple calls. Ranger now supports ability to change all admin users passwords in one call. Need to add stack feature support for the same.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-13 06:49:20,35
13197974,Views Page redirecting to Home page when User doesn't have access to any group and just have access to view,"Ambari Files view not working when User doesnt have access to any group and just have access to Filesview


The Page redirects to View's Home Page each time when I click on the files view.

The issue is with javascript

attached video of the issue.  [^Views Issue.mov] 
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-13 05:56:27,3
13197953,AMS Grafana query editor panel does not work in upgraded version.,"Following issues are encountered
 - Dropdown list not appearing.
 - Metrics not automatically refreshed
 - Error on collapsing a metric.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-11-13 03:17:46,34
13197898,[Log Search UI] Change sticky filter panel solution from JS to clean css.,Instead of listening scroll event we can use `sticky` css `position`. It is not supported by all the browsers (IE does not support). This does not break functionality but improve performance.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Improvement,Major,2018-11-12 21:00:51,26
13197884,Implement JSON parsing code to handle Add Service request,"This JIRA tracks the work involved to implement the parsing/handling code for the JSON request and response elements of the ""Add Service"" API. 
",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-12 19:16:40,20
13197837,The kadmin service principal name should be configurable for MIT KDC interactions,"The kadmin service principal name should be configurable for MIT KDC interactions. The current process assumes the kadmin service principal is {{kadmin/FQDN_KADMIN_HOST}}, but this could be different on some installations. For example, {{kadmin/admin}}.

A new kerberos-env property should be added to allow a user to change the kadmin principal name - {{kerberos-env/kadmin_principal_name}}

The default value for the new property should be ""{{kadmin/$\{admin_server_host|stripPort()}}}"".  To be able to do this, we have to create a new variable replacement _function_. For example, {{stripPort}}.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-12 15:14:06,27
13197800,"Heatmap tab, metrics tab and QuickLinks section in summary tab should be hidden if service has only client service component","HDFS client and YARN client service component is created but other HDFS/YARN components are not created via blueprint deploy

*Expected behavior:*
Heatmap tab, metrics tab and QuickLinks section should be hidden if a service does not have any non-client servicecomponent created in the cluster

*Actual behavior:*
Heatmap and metrics tab are shown with empty metrics and no graphs",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-12 13:09:23,19
13197798,BE: Performance Tune service Configs Pages,"Need to be able to show configuration in less than 2 seconds. The heaviest part of configurations loading is recommendations request. Now for 2k cluster recommendations request is taking about 12 seconds:
{code:java}
/api/v1/stacks/HDP/versions/3.0/recommendations

{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-12 13:07:18,6
13197223,Rolling Upgrade Orchestration Changes For Client Performance,"Allow a new grouping for clients whereby:
* The first client is upgraded
* Run a service check on the first client to verify configs, etc.
* Run balance of clients in parallel",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-08 19:36:34,33
13197194,Create an Ambari-server class that is called as an entry point to encrypt service configuration sensitive data,"
Create a java class that is called as an entry point to encrypt service configuration sensitive data. This script will be called from a python script just like it is done for pre-checks, or for upgrade catalogs.


",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-08 17:50:25,13
13197187,Allow blueprint install with partial YARN: no HISTORYSERVER and APP_TIMELINE_SERVER,"There is a need to install clusters with some components external to the cluster. In this case the cluster has to have YARN but HISTORYSERVER and APP_TIMELINE_SERVER should not be installed.

Even with blueprint validation turned off, installation stalls with exceptions thrown in BlueprintConfigurationProcessor.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-11-08 17:29:36,20
13197144,Request configurations when needed during server-side actions rather than rely on configuration data from the execution command,"Request configurations when needed during server-side actions rather than rely on configuration data from the execution command.

Due to a recent change, which appeared to remove configuration data from the execution command JSON document, data needed for Kerberos-related service-side actions is missing. This data may be requested when needed from the cluster data at the time of execution rather than when setting up the stages.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-08 14:45:16,30
13197088,JS error when changing service auto-start toggle,"# Go to Admin / Service Auto Start page
# Toggle Auto Start Settings
# Click Save button

JS error appears:
{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
    at app.js:15024
    at Array.forEach (<anonymous>)
    at Class.syncStatus (app.js:15022)
    at Array.<anonymous> (app.js:15092)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at Class.<anonymous> (vendor.js:1379)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-08 10:58:14,19
13196814,The stack/service advisor should be able to return LDAP-related recommendations upon request,"The stack/service advisor should be able to return LDAP-related recommendations upon request.

A new \{StackAdvisorRequestType}} is to be created - 'ldap-configurations' - along with the infrastructure to call request LDAP-specific configuration recommendations.

It is expected that stack owners will implement the optional {{recommendConfigurationsForLDAP}} function to provide the service-specific data.

This stack advisor request should be invoked upon changing the LDAP configuration values and Ambari should automatically apply the recommended changes - potentially causing service configurations to be updated and the need to restart affected services. For example, see {{org.apache.ambari.server.controller.internal.AmbariServerSSOConfigurationHandler#updateComponentCategory}}.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-07 12:39:50,27
13196744,Build error at Findbugs with Maven 3.6,"{noformat:title=mvn -am -pl ambari-server clean verify}
...
[INFO] Ambari Server 3.0.0.0-SNAPSHOT ..................... FAILURE [01:16 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
...
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:findbugs-maven-plugin:3.0.3:findbugs (findbugs) on project ambari-server: Unable to parse configuration of mojo org.codehaus.mojo:findbugs-maven-plugin:3.0.3:findbugs for parameter pluginArtifacts: Cannot assign configuration entry 'pluginArtifacts' with value '${plugin.artifacts}' of type java.util.Collections.UnmodifiableRandomAccessList to property of type java.util.ArrayList -> [Help 1]
{noformat}

{noformat:title=mvn -version}
Apache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T20:41:47+02:00)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-07 06:05:52,21
13196630,Delete host confirm popup does not contain all master components,"# Navigate to host with one or more masters
# Click Delete Host from the Host Actions menu

Observed: All the masters are not mentioned in the popup. This is a frontend bug
 !Screen Shot 2018-11-05 at 2.47.04 PM.png! 
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-06 16:46:40,19
13196434,Provide wrapper class for LDAP-related data for use in service advisors,Provide wrapper class for LDAP-related data for use in service advisors like the wrapper class for SSO-related data - {{stacks.ambari_configuration.AmbariSSODetails}}.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-06 08:52:53,27
13195882,UI Changes for supporting Ozone deployment,"UI changes:
1. Allow multi file system for cases where only HDFS and Ozone are being deployed together. (Currently only single file system is allowed) 
2. If user selects HDFS and Ozone, allow only HDFS Datanodes to be deployed (Install wizard as well as Add Service Wizard)
3. If user select Ozone as only service, allow Ozone-Datanodes to be deployed (cardinality: 1+)
4. Do not allow HDFS installation via Add Service Wizard if Ozone is deployed.
5. If Hdfs is already installed don't allow ozone-datanodes during Add Host Wizard. 
6. Do not allow host component Ozone-Datanodes to be added if HDFS Datanodes are already added.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-02 12:09:50,34
13195664,VersionUtils JavaDoc Changes For Signing the SPI,"{{VersionUtils}} was brought into the SPI and has some invalid Javadoc:

{noformat}
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/server/utils/VersionUtils.java:206: warning: no description for @param
[ERROR]    * @param version
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/server/utils/VersionUtils.java:207: warning: no description for @return
[ERROR]    * @return
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/server/utils/VersionUtils.java:228: warning: no description for @return
[ERROR]    * @return
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/server/utils/VersionUtils.java:263: error: malformed HTML
[ERROR]    * @return 0 if both are equal, <0 if first one is lower, >0 otherwise
[ERROR]                                   ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/server/utils/VersionUtils.java:263: error: bad use of '>'
[ERROR]    * @return 0 if both are equal, <0 if first one is lower, >0 otherwise
{noformat}",pull-request-available,['ambari-server-spi'],AMBARI,Task,Critical,2018-11-01 13:55:21,31
13195645,Cluster user can't modify shared widgets,"If a user with role ""Cluster User"" tries to ""Edit Shared"" widgets, the server will return an error:
{noformat}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Only cluster operator can create widgets with cluster scope""
}
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-01 11:55:44,19
13195477,NPE in default host group replacement,"Cluster deployment fails if some property value is null:

{noformat}
2018-10-31 17:38:44,579  INFO [pool-3-thread-1] AsyncCallableService:100 - Task ConfigureClusterTask exception during execution
java.lang.NullPointerException
	at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
	at java.util.regex.Matcher.reset(Matcher.java:309)
	at java.util.regex.Matcher.<init>(Matcher.java:229)
	at java.util.regex.Pattern.matcher(Pattern.java:1093)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor$HostGroupUpdater.updateForClusterCreate(BlueprintConfigurationProcessor.java:1754)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.updateValue(BlueprintConfigurationProcessor.java:739)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.lambda$null$7(BlueprintConfigurationProcessor.java:705)
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1044)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.lambda$applyDefaultUpdater$8(BlueprintConfigurationProcessor.java:700)
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1044)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.applyDefaultUpdater(BlueprintConfigurationProcessor.java:697)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doGeneralPropertyUpdatesForClusterCreate(BlueprintConfigurationProcessor.java:664)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doUpdateForClusterCreate(BlueprintConfigurationProcessor.java:443)
	at org.apache.ambari.server.topology.ClusterConfigurationRequest.process(ClusterConfigurationRequest.java:152)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-31 20:20:20,21
13195303,Logsearch: debug mode using java 8 and 11,Logsearch server can not be started in debug mode if Logsearch is running in an Ambari managed cluster using java 8,pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-10-31 07:10:08,16
13195240,Use the proper stack root for Hive pre upgrade,"/usr/hdp was hard coded, need to use Script.get_stack_root() instead",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-30 21:39:57,36
13195199,Add Information to the SPI to Support Upgrade Checks,"While implementing some of the upgrade checks which were removed as part of AMBARI-24737, the SPI needed some extra information added.

- Some checks used {{VersionUtils}}, so this needed to be relocated to the SPI.
- Added service-level repository version information to {{ClusterInformation}}
- Simplified the {{UpgradeCheckDescription}} when there is only 1 failure message.
- Prevent NPE when {{CheckQualifications}} is {{null}} (even though Javadoc says it must return an empty iist.
",pull-request-available,['ambari-server-spi'],AMBARI,Task,Critical,2018-10-30 18:53:45,31
13195112,Logsearch: Cannot search for a term that includes spaces or dashes,"If I type the following ""Some terrible error has happened"" I got a huge amount irrelevant results from Logsearch, this is mainly because the search term is tokenized and finds every line that contains any of the words even if I am using "".
",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-10-30 13:00:02,16
13195108,Ambari-agent stop hangs if ambari-server is stopped.,"
    [root@gc7001 ~]# ambari-agent  stop
    Verifying Python version compatibility...
    Using python  /usr/bin/python
    Found ambari-agent PID: 7391
    Stopping ambari-agent
    ^C^C^X^Z
    [2]+  Stopped                 ambari-agent stop
    [root@gc7001 ~]# ^C
    [root@gc7001 ~]# kill -9 7391
    

",pull-request-available,[],AMBARI,Bug,Major,2018-10-30 12:43:08,5
13195094,Sometimes host status still in heartbeat lost after agent become heartbeating,Sometimes initial component states update does not fire host status recheck and host still in heartbeat lost.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-30 11:16:08,6
13194957,Improve copy of atlas.war,"Atlas Metadata Server's start script in Ambari includes a step to copy {{atlas.war}} from the default location to the {{expanded_webapp_dir}}.  This is performed using {{File(content=StaticFile)}}, which reads and writes the contents of the war file to/from memory.  It may take 1-3 seconds to perform, and is done even if the source and target files are the same, because there is no check on the paths.

This could be improved:

# Use {{cp}} to copy
# Skip the operation if source and target are the same",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-10-29 20:12:49,21
13194905,Make Ambaripreupload.py more configurable,"When testing {{Ambaripreupload.py}} locally, one needs to work around some hardcoded values (JDBC driver, DFS type). The goal of this change is to make the script more configurable.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-10-29 16:51:29,21
13194812,BE: Performance Tune Hosts Pages,"Hosts List - The initial host page and filter operations should all take less than 2 seconds. Today on the 5k node cluster all operations are taking 4 seconds.
Request URL for filtering:
{noformat}
/api/v1/clusters/c1/hosts?fields=Hosts/rack_info,Hosts/host_name,Hosts/maintenance_state,Hosts/public_host_name,Hosts/cpu_count,Hosts/ph_cpu_count,alerts_summary,Hosts/host_status,Hosts/host_state,Hosts/last_heartbeat_time,Hosts/ip,host_components/HostRoles/state,host_components/HostRoles/maintenance_state,host_components/HostRoles/stale_configs,host_components/HostRoles/service_name,host_components/HostRoles/display_name,host_components/HostRoles/desired_admin_state,host_components/metrics/dfs/namenode/ClusterId,host_components/metrics/dfs/FSNamesystem/HAState,Hosts/total_mem,stack_versions/HostStackVersions,stack_versions/repository_versions/RepositoryVersions/repository_version,stack_versions/repository_versions/RepositoryVersions/id,stack_versions/repository_versions/RepositoryVersions/display_name&minimal_response=true,host_components/logging&page_size=10&from=0&sortBy=Hosts/host_name.asc
{noformat}
with various parameters:
{noformat}
{""RequestInfo"":{""query"":""page_size:100
from:0
Hosts/host_status:HEALTHY""}}{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-29 11:48:23,6
13194800,Pause after first batch,Option to automatically pause after the first batch is completed,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-29 10:48:03,28
13194791,Change Hostname does not honor uppercase hosts,"During the [Ambari change hostname procedure|https://docs.hortonworks.com/HDPDocuments/Ambari-2.6.2.0/bk_ambari-administration/content/ch_changing_host_names.html], hostnames in configs when they were all uppercase are not being replaced.  I think the code is converting hosts to all lowercase to do replacements.  See:
https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/java/org/apache/ambari/server/update/HostUpdateHelper.java#L383-L393

Appears to affect up from 2.6.1 and 2.6.2
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-29 09:53:40,13
13194769,Ambari is trying to create hbase.rootdir using s3 url,"Please look at the related Jira for blueprint.

For shared services cluster, Hbase needs to use S3 Object store for rootdir.
Ambari is trying to create hbase.rootdir using s3 url, hence failing with
below error.

    
    
    2018-10-24 03:12:45,424 - get_user_call_output returned (0, u'<html>\n<head>\n<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>\n<title>Error 404 Not Found</title>\n</head>\n<body><h2>HTTP ERROR 404</h2>\n<p>Problem accessing /webhdfs/v1s3a:/san-s3-ohio/hbase. Reason:\n<pre>    Not Found</pre></p>\n</body>\n</html>\n404', u'')
    out: <html>
    <head>
    <meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
    <title>Error 404 Not Found</title>
    </head>
    <body><h2>HTTP ERROR 404</h2>
    <p>Problem accessing /webhdfs/v1s3a:/san-s3-ohio/hbase. Reason:
    <pre>    Not Found</pre></p>
    </body>
    </html>
    404​
    

",pull-request-available,[],AMBARI,Bug,Major,2018-10-29 08:32:35,5
13194759,Infra Manager: zookeper connection string,"Infra Manager can not connect to zookeeper if it is installed into a node where there is no zookeeper server instance, or the local instance is down.
* This is caused by that the infra manager stack code does not set the zookeeper quorum and the CloudSolrClient falls back to localhost:2181
* Also the builder which is used for construct an org.apache.solr.client.solrj.impl.CloudSolrClient instance is deprecated: it does not handle the zookeper connection string fromat which sould be set in the stack code.
{code}
zkhost0:port[,zkhost1:port...zkhostn:port][/solr_znode]
{code}
",pull-request-available,['ambari-infra'],AMBARI,Bug,Blocker,2018-10-29 07:39:36,16
13194526,Make Grafana connection attempts and retry delay configurable,"Metrics Grafana HTTP requests are attempted at most 15 times in case of failure, with 20 seconds delay each time.  The goal of this change is to make both the number of attempts and length of the delay configurable.  Shorter retry delay could be useful to reduce overall startup time.",pull-request-available,['ambari-metrics'],AMBARI,Improvement,Minor,2018-10-26 22:06:21,21
13194525,Service Auto start is enabled after page refresh,"Steps to Reproduce:
 # Goto Service Auto start page
 # Toggle Enable button to disable
 # Refresh the page OR Move to another page and come back to 'Service Auto start' page
Observed that button toggle back to Enabled state.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-26 22:02:59,34
13194475,Fix javadoc errors in ambari-utility,"Fix the following javadoc errors in {{ambari-utility}}:

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar (attach-javadocs) on project ambari-utility: MavenReportException: Error while generating Javadoc:
[ERROR] Exit code: 1 - ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:28: error: reference not found
[ERROR]  * {@link org.apache.ambari.swagger.NestedApiRecord} when {@link org.apache.ambari.swagger.AmbariSwaggerReader}
[ERROR]           ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:42: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:48: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:54: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:60: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerPreferredParent.java:37: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
{noformat}",pull-request-available,[],AMBARI,Bug,Minor,2018-10-26 17:47:20,21
13194253,Allow blueprint install for YARN without RESOURCEMANAGER and HDFS_CLIENT without HDFS server components,Make the BlueprintConfigurationProcessor process HDFS_CLIENT without HDFS server components and YARN timeline service components without the rest of YARN server components  without throwing Exceptions.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-25 21:33:18,20
13194231,Correct Javadoc Errors in the SPI,"During the preparation for distribution the SPI, the following Javadoc errors were produced by maven:

{code}
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/annotations/UpgradeCheckInfo.java:34: error: reference not found
[ERROR]  * to any {@link ClusterCheck} instance.
[ERROR]                  ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/annotations/UpgradeCheckInfo.java:35: error: self-closing element not allowed
[ERROR]  * <p/>
[ERROR]    ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/annotations/UpgradeCheckInfo.java:38: error: reference not found
[ERROR]  * {@link UpgradeCheckRegistry}.
[ERROR]           ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/annotations/UpgradeCheckInfo.java:53: error: self-closing element not allowed
[ERROR]    * <p/>
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/annotations/UpgradeCheckInfo.java:65: error: self-closing element not allowed
[ERROR]    * <p/>
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/CheckQualification.java:33: warning: no description for @param
[ERROR]    * @param request
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/CheckQualification.java:34: warning: no description for @return
[ERROR]    * @return
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/CheckQualification.java:35: warning: no description for @throws
[ERROR]    * @throws AmbariException
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckDescription.java:91: warning: no description for @return
[ERROR]    * @return
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckGroup.java:23: error: self-closing element not allowed
[ERROR]  * <p/>
[ERROR]    ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckRequest.java:44: error: @param name not found
[ERROR]    * @param clusterName
[ERROR]             ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckRequest.java:50: error: @param name not found
[ERROR]    * @param configurations
[ERROR]             ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckRequest.java:54: warning: no @param for clusterInformation
[ERROR]   public UpgradeCheckRequest(ClusterInformation clusterInformation, UpgradeType upgradeType,
[ERROR]          ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckRequest.java:54: warning: no @param for checkConfigurations
[ERROR]   public UpgradeCheckRequest(ClusterInformation clusterInformation, UpgradeType upgradeType,
[ERROR]          ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/upgrade/UpgradeCheckResult.java:59: warning: no description for @return
[ERROR]    * @return
[ERROR]      ^
[ERROR] /Users/jhurley/src/apache/ambari/ambari-server-spi/src/main/java/org/apache/ambari/spi/RepositoryVersion.java:52: warning: no description for @return
[ERROR]    * @return
[ERROR]      ^
{code}",pull-request-available,['ambari-server-spi'],AMBARI,Bug,Major,2018-10-25 19:38:05,31
13194220,LDAP users fail to authenticate using LDAPS due to 'No subject alternative DNS name' exception,"LDAP users fail to authenticate using LDAPS due to `No subject alternative DNS name` exception:

{noformat}
2018-10-26 14:49:45,716  WARN [ambari-client-thread-37] AmbariLdapAuthenticationProvider:126 - Failed to communicate with the LDAP server: simple bind failed: ad.example.com:636; nested exception is javax.naming.CommunicationException: simple bind failed: ad.example.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching ad.example.com found.]
{noformat}

This is the other half of the issue from AMBARI-24533 (which was related to the LDAP sync process).  

Note:  If LDAP sync is performed before a user attempts to log in, then the issue will not be seen since the system property, {{com.sun.jndi.ldap.object.disableEndpointIdentification}}, would have already been set to ""true"".   However, the logic path setting this value is not reached for an authentication attempt. 

Note: This occurs with OpenJDK 1.8.0.191 and maybe some earlier versions.
{noformat}
openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
{noformat}

This does not occur with Oracle JDK 1.8.0.112
{noformat}
java version ""1.8.0_112""
Java(TM) SE Runtime Environment (build 1.8.0_112-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-25 18:51:28,30
13194133,Log Feeder: Fix HDFS/S3 outputs,"Currently hdfs / s3 outputs are not working properly, they just open an empty file and nothing happens",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-10-25 12:55:17,29
13194123,Make cluster name inputs validation in Ambari UI consistent,"# User has ability to rename cluster on 'Cluster Information' page of Ambari Admin. Cluster name input isn't being validated there. E.g., it's possible to set the new name containing special characters like slash which would break API.
# It's possible to proceed by clicking 'Save' button in rename cluster scenario even if it's disabled.
# Cluster name validation is inconsistent in different possible scenarios of its setting. In scope of this task validation on step0 of Ambari installer should be changed.

Validation rules on Ui should be consistent with BE rules.",pull-request-available,"['ambari-admin', 'ambari-web']",AMBARI,Bug,Blocker,2018-10-25 11:14:53,9
13194086,Set path encoding for GCS,Add {{fs.gs.path.encoding}} to {{core-site}} with default value of {{uri-path}}.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-25 08:19:52,21
13193860,TrimmingStrategy implementations should be singletons,"{{BlueprintConfigurationProcessor}} trims properties during deployment using various implementations of {{TrimmingStrategy}}.  A new strategy object is created for each property.  This is unnecessary, since all current implementations are stateless.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-10-24 12:42:21,21
13193773,Set cloud storage tracking property for GCS,"Similarly to AMBARI-23329, set {{fs.gs.application.name.suffix}} for GCS.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-24 07:20:38,21
13193622,Knox topology files installed via Ambari are world readable,The Knox topologies should only be accessible to the knox user.,pull-request-available,[],AMBARI,Bug,Major,2018-10-23 16:45:30,37
13193502,Add column to show which service recommended changes during upgrade refer to,"During the stack upgrade from HDP 2.6 -> HDP 3.x, the list of changes Ambari wants to make to each service is very difficult to interpret. Below is an example of the config changes Ambari recommends when attempting to upgrade fresh-install HDP2.6.2 cluster. 

Here's an example of how these configs look like when the user presses the ""Open"" button:
open_recommended_config_changes.png 
Note the highlighting is not intentional - it is automatically done by the browser (chrome) due to the special characters in the configs.

A small step to improve the user experience would be to add a column that shows which service each config applies.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-10-23 09:22:50,19
13193492,Add a rolling_restart_supported flag at the stack service endpoint,"Similar to the one at stack service component level we need a flag for HDFS, YARN and Zookeeper
 at api/v1/stacks/HDP/versions/<version>/services/<serviceName>",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-23 08:47:23,28
13193415,Implement New Upgrade Check Which Warns About Missing Plugin Checks,"AMBARI-24737 removed stack upgrade checks from being shipped with Ambari and instead expects the stacks to deliver their own implementations. In the event that an upgrade check is defined in an upgrade pack but cannot be found or instantiated, there should be a warning displayed before allowing the upgrade to proceed.

For example:
{code}
  {
      ""href"" : ""http://localhost:8080/api/v1/clusters/c1/rolling_upgrades_check/PLUGIN_CHECK_LOAD_FAILURE"",
      ""UpgradeChecks"" : {
        ""check"" : ""Plugin Upgrade Checks"",
        ""check_type"" : ""CLUSTER"",
        ""cluster_name"" : ""c1"",
        ""failed_detail"" : [
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""ServicesYarnWorkPreservingCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""DruidHighAvailabilityCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""ServicesNamenodeHighAvailabilityCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""MapReduce2JobHistoryStatePreservingCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""YarnRMHighAvailabilityCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""ServicesMapReduceDistributedCacheCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""ServicesTezDistributedCacheCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""YarnTimelineServerStatePreservingCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""HiveMultipleMetastoreCheck""
          },
          {
            ""package_name"" : ""org.apache.ambari.server.checks"",
            ""class_name"" : ""SecondaryNamenodeDeletedCheck""
          }
        ],
        ""failed_on"" : [
          ""ServicesMapReduceDistributedCacheCheck"",
          ""ServicesTezDistributedCacheCheck"",
          ""SecondaryNamenodeDeletedCheck"",
          ""YarnTimelineServerStatePreservingCheck"",
          ""YarnRMHighAvailabilityCheck"",
          ""DruidHighAvailabilityCheck"",
          ""ServicesNamenodeHighAvailabilityCheck"",
          ""HiveMultipleMetastoreCheck"",
          ""MapReduce2JobHistoryStatePreservingCheck"",
          ""ServicesYarnWorkPreservingCheck""
        ],
        ""id"" : ""PLUGIN_CHECK_LOAD_FAILURE"",
        ""reason"" : ""The following upgrade checks could not be loaded and were not run. Although this will not stop your upgrade, it is advised that these checks be corrected to ensure a successful upgrade."",
        ""repository_version_id"" : 2,
        ""status"" : ""FAIL"",
        ""upgrade_type"" : ""NON_ROLLING""
      }
{code}",pull-request-available,[],AMBARI,Task,Major,2018-10-22 23:59:30,31
13193175,Logs are not showing in Hiveview2 for hive query.,Sometimes the logs in hive view does not show. This happens frequently for Hive interactive queries.,pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-10-22 06:47:38,38
13192841,Infra Manager: unable to restart job,"When a job is running, a stop signal is sent and just right after the infra manager process is killed the job remains in Stopping/Unknown state.
In this case the job won't be restarted when Infra Manager is restarted and also a new instance of the job can not be scheduled.",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-10-19 13:13:03,16
13192830,Swagger spec for Ambari REST API generation is failing,"Running the following command failed with the following error:

 
{noformat}
mvn -Del.log=WARN -Dcheckstyle.skip -Dfindbugs.skip -Drat.skip -DskipTests -DfailIfNoTests=false -am -pl ambari-server clean install
...
[ERROR] Failed to execute goal com.github.kongchen:swagger-maven-plugin:3.1.4:generate (default) on project ambari-server: Conflicting setter definitions for property ""requestBody"": org.apache.ambari.server.orm.entities.RequestScheduleBatchRequestEntity#setRequestBody(1 params) vs org.apache.ambari.server.orm.entities.RequestScheduleBatchRequestEntity#setRequestBody(1 params) -> [Help 1]
{noformat}
 

According to this thread t[his is a known issue|https://github.com/FasterXML/jackson-databind/issues/1251] in swagger

Upgrading the swagger plugin to 3.1.5 solves the issue.",pull-request-available,[],AMBARI,Bug,Blocker,2018-10-19 12:30:23,27
13192652,Unify Ambari Versioning Across Modules,"It looks like Ambari’s trunk pom.xml has not been updated with a proper version in at least 4 years. It currently still lists trunk as 2.0.0.0-SNAPSHOT:
{code}
  <groupId>org.apache.ambari</groupId>
  <artifactId>ambari</artifactId>
  <packaging>pom</packaging>
  <name>Ambari Main</name>
  <version>2.0.0.0-SNAPSHOT</version>
  <description>Ambari</description>
{code}
        
This poses several problems as we try to make our artifacts more 3rd-party friendly since it becomes ambiguous what other maven projects are actually depending on. We need to change our process to keep this version updated with every release of Ambari. Other Apache projects seem to do this (I took a look at Nifi, Hive, Storm, etc) and in each case, their trunk pom.xml was at least a minor version ahead of their most recent release branch.

- We should change the version specified for Ambari and its submodules to use the next major version after a release, such as {{3.0.0.0-SNAPSHOT}}. Once 3.0 has been released, this will be changed to {{4.0.0.0-SNAPSHOT}} even if there are more builds in the 3.x line off of {{trunk}}.

- Maven 3.5.0 allows the ability to specify a {{<version>${revision}</version>}} placeholder value in the parent {{pom.xml}} which submodules can inherit. This seems cleaner than having all of the submodules updated and checked in using the {{mvn versions:set -DnewVersion=3.0.0.0-SNAPSHOT}} command.

- If any submodule inherits from {{ambari}} or {{ambari-project}}, then they should broadcast their version as the same. This includes submodules such as {{ambari-utility}} which are hard coded at {{1.0.0.0-SNAPSHOT}} which makes understanding what actually exists in local repos very difficult.",pull-request-available,[],AMBARI,Task,Critical,2018-10-18 17:34:09,31
13192533,"Separating Restart All, Masters and Slaves",Three separate options for service restarts.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-18 12:55:29,34
13192393,service adviser changes for cluster specific configs,"Env: HDC Spark Data science (m4x4xlarge 16 CPU/64 GB)

Spark defaults aren't changed in ambari. It is loading with 1 GB spark.executor.memory.

 (Should this be 60-70% of yarn min container size. Need to consider spark.yarn.executor.memoryOverhead)

Add such logic for ""spark.shuffle.io.numConnectionsPerPeer"":
spark.shuffle.io.numConnectionsPerPeer should be configured dynamically based on cluster size.
Recommandation was to set it to 10 if number of nodes < 10 and remove (so that default value is used) for higher values.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-17 23:02:02,24
13192369,Fix jetty CVEs for Log Search (2.7.x),"See:
- CVE-2018-12536
- CVE-2017-9735",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-10-17 20:54:51,29
13192365,Support regex based metric inclusion in KafkaTimelineMetricsReporter,"The request is to to add an additional config property *external.kafka.metrics.include.regex* to include metrics based on regex patterns. This property can take multiple comma separated regex expressions.

Implementation Logic

* By default, a metric will be included.
* If the metric starts with any of the prefixes in the exclude.prefix property, it is a candidate for discarding.
* However, for an ""excluded"" metric, if the metric starts with any of the prefixes in include.prefix or matches any pattern in include.regex, it will NOT be discarded.",pull-request-available,['ambari-metrics'],AMBARI,Task,Major,2018-10-17 20:37:16,39
13192295,Intermittent CredentialStoreTest test failure,"Stacktrace
{code}
java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
{code}
Standard Output
{code}
2018-10-12 14:06:26,230 INFO  [main] encryption.MasterKeyServiceImpl (MasterKeyServiceImpl.java:initializeFromFile(277)) - Loading from persistent master: #1.0# Fri, Oct 12 2018 14:06:26.207
2018-10-12 14:06:28,371 INFO  [main] encryption.MasterKeyServiceImpl (MasterKeyServiceImpl.java:initializeFromFile(277)) - Loading from persistent master: #1.0# Fri, Oct 12 2018 14:06:28.370{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-17 16:20:25,6
13192257,Allow skipping Python unit tests,{{ambari-server}} Python tests can be skipped by passing {{-DskipPythonTests}} to Maven.  The goal of this task is to allow the same for {{ambari-agent}}.,pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-10-17 14:17:01,21
13192224,Adding Ranger Password configs in Admin Settings section under Advanced config,"This jira is fixing the below password configs to be shown in 'Admin Settings' section under Advanced Config section for Ranger service.
* rangerusersync_user_password
* rangertagsync_user_password
* keyadmin_user_password",pull-request-available,[],AMBARI,Bug,Major,2018-10-17 12:59:47,25
13192180,Infra Manager: not all the documents archived,Infra manager does not use the sort_column properties when reading the documents to archive. This leads to some of the documents are not exported.,pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-10-17 09:21:19,16
13192056,Node Managers fail to start after RM is moved to a different host as 'resource-tracker.address' config is not updated,"# Under Yarn --> Service Actions - choose to Move Resource Manager. Pick a new host for the active RM node and let the move operation complete
 # Observe the state of Node Managers in Ambari UI

*Result*
All Node Managers show as down.

Observed that one of the configs - 'yarn.resourcemanager.resource-tracker.address.rm2' still points to older RM node[!Screen Shot 2018-10-11 at 6.20.51 PM.png?default=false|thumbnail!|https://hortonworks.jira.com/secure/attachment/165347/165347_Screen+Shot+2018-10-11+at+6.20.51+PM.png] , which may be causing this issue",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-16 21:58:59,34
13191942,Install wizard config page blank on first load due to async bug,Bug in config loading code causes configuration page of install wizard to be blank initially.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-16 14:52:02,40
13191917,Eliminating duplicated sudo calls when copying files in Ambari with non-root configuration,"During NiFi start Ambari invokes ambari-common/src/main/python/resource_management/core/sudo.copy(src, dest) (line 306) in case of non-root configuration. Here the command we pass to the command wrapper within shell.py starts with 'sudo'. However the sudo flag is also set to True.

As a result we have a double 'sudo' invocation which may cause issues in ambari with non-root configuration.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-16 13:17:36,27
13191894,Ambari-agent cannot register sometimes,"
    ERROR 2018-10-11 13:45:57,401 HeartbeatThread.py:108 - Exception in HeartbeatThread. Re-running the registration
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 95, in run
        self.register()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 163, in register
        self.force_component_status_update()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 173, in force_component_status_update
        self.component_status_executor.force_send_component_statuses()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ComponentStatusExecutor.py"", line 206, in force_send_component_statuses
        service_name, component_name = service_and_component_name.split(""/"")
    ValueError: need more than 1 value to unpack
    

",pull-request-available,[],AMBARI,Bug,Major,2018-10-16 12:04:17,5
13191878,Cleanup dependencies in Capacity Scheduler,remove any dependencies with probable bugs in it.,pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-10-16 10:58:54,38
13191741,Ambari setup-ldap: change group member default for IPA,"The new cli options we introduced in Ambari 2.7.1 seem to be causing some problems. Here's a few observations from the past couple of days putting together the Ambari 2.7.1 - IPA security labs.
 - Even after encrypting passwords and persisting thekey, the ambari-server setup-ldap cli doesn't seem to store the previous settings in the database. 

 - The ldap-type option seemed to cause a lot of grief and confusion for the cli users. Could we please document its behavior in the cli help menu (and let's add it to the docs, after we get clarity)? 

 - The default options for IPA integration aren't quite working. Please see the IPA lab for the values we have to override to get a working group resolution (*User object class* and *Group member attribute*) [https://github.com/HortonworksUniversity/Security_Labs/blob/master/HDP-3.0-IPA.md#4-enable-ldap-for-ambari] ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-15 21:58:44,16
13191705,Add Hotfix to VDF Release Element,"VDF should allow for an optional element: {{hotfix}} that is separate from the build number.  In addition, the stack should be flexible enough to supply how version information is formatted.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-10-15 20:20:52,33
13191664,Move Namenode operation fails as it tries to install and start ZKFailoverController on non-HA cluster	,"Error at final screen during Service start for ZKFailoverController. This is unexpected as the cluster is non-HA and ZKFailoverController component should not be attempted for a start at all on the new NN host
{code:java}
2018-10-11 14:22:52,136 - Execute['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/3.0.3.0-74/hadoop/bin/hdfs --config /usr/hdp/3.0.3.0-74/hadoop/conf --daemon start zkfc''] {'environment': {'HADOOP_LIBEXEC_DIR': '/usr/hdp/3.0.3.0-74/hadoop/libexec'}, 'not_if': 'ambari-sudo.sh  -H -E test -f /var/run/hadoop/cstm-hdfs/hadoop-cstm-hdfs-zkfc.pid && ambari-sudo.sh  -H -E pgrep -F /var/run/hadoop/cstm-hdfs/hadoop-cstm-hdfs-zkfc.pid'}
2018-10-11 14:22:54,336 - Execute['find /grid/0/log/hdfs/cstm-hdfs -maxdepth 1 -type f -name '*' -exec echo '==> {} <==' \; -exec tail -n 40 {} \;'] {'logoutput': True, 'ignore_failures': True, 'user': 'cstm-hdfs'}
######## Hortonworks #############
This is MOTD message, added for testing in qe infra
==> /grid/0/log/hdfs/cstm-hdfs/hadoop-cstm-hdfs-zkfc-ctr-e138-1518143905142-517699-01-000003.hwx.site.out <==
Exception in thread ""main"" org.apache.hadoop.HadoopIllegalArgumentException: HA is not enabled for this namenode.
	at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.create(DFSZKFailoverController.java:134)
	at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:192)
core file size          (blocks, -c) unlimited
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-15 17:13:50,34
13191555,Fix CVE issues in ambari server,"The following 3rd party dependencies have to be eliminated/upgraded to a secure version based on the latest BlackDuck scan:
||Current version||Upgrade to||CVE issue(s)||
|org.springframework:spring-web:jar:4.3.17.RELEASE|org.springframework:spring-web:jar:4.3.18.RELEASE or the latest|CVE-2018-11039, CVE-2018-11040|
|jquery-1.8.3.min.js|1.9.0rc1 or the latest|CVE-2011-4969, CVE-2015-9251, CVE-2012-6708|
|org.eclipse.jetty:jetty-server:jar:9.4.11.v20180605|9.4.12.v20180830 or the latest|CVE-2017-9735, CVE-2018-12536|",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-15 11:06:31,27
13191524,Allow the agent's SSL certificate data to be accessible by heartbeat handlers,"If 2-way SSL is enabled, allow the agent's SSL certificate data to be accessible by heartbeat handers. The agent's SSL certificate contains the agent's public key, which may be used to encrypt data for that specific agent.

For example, the agent's SSL certificate data may be needed by org.apache.ambari.server.agent.rest.AgentResource#heartbeat and org.apache.ambari.server.agent.stomp.HeartbeatController#heartbeat, among other handlers used to communicate with the Ambari agent.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-15 08:56:09,18
13191509,Configuration warning: insert a space char between label value and unit-name,"On Ambari enhanced UI configuration and Configuration warning page inserting a space character between label value and unit-name when it is shown as a warning tooltip and text.
from
{code}
2days
100GB
{code}
to
{code}
2 days
100 GB
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-15 07:26:32,16
13191468,Wrong settings in exported blueprint,"Cluster blueprint export is broken wrt. the {{settings}} section.

# Only one component per service is included.
# Default setting value assumed to be {{false}}, ignoring component-level stack definition

STR:

# Disable ""Auto Start"" for Metrics Collector, for which it is enabled by default per stack definition
# Enable ""Auto Start"" for both DataNode and NameNode
# Export blueprint

Result: 

{noformat:title=http://$AMBARI_SERVER:8080/api/v1/clusters/TEST?format=blueprint}
  ""settings"" : [
    {
      ""recovery_settings"" : [
        {
          ""recovery_enabled"" : ""true""
        }
      ]
    },
    {
      ""service_settings"" : [
        {
          ""recovery_enabled"" : ""true"",
          ""name"" : ""HDFS""
        }
      ]
    },
    {
      ""component_settings"" : [
        {
          ""recovery_enabled"" : ""true"",
          ""name"" : ""NAMENODE""
        }
      ]
    }
{noformat}

Problem: creating a cluster using the exported blueprint results in different settings than the original cluster.  ""Auto Start"" would be disabled for DataNode and enabled for Metrics Collector per stack defaults.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-14 19:10:47,21
13191210,Load Upgrade Checks Dynamically From Stacks,"Stack-provided upgrade checks need to be loaded from a provided JAR when Ambari Server starts up. Currently, upgrade checks are loaded synchronously, scanning the entire classpath for classes matching {{UpgradeCheck}} in the package {{org.apache.ambari.server.checks}}. This can be extremely slow and cause Ambari Server to fail to start within a reasonable amount of time. The proposed changes for this issue are as follows: 

- Move to an asynchronous model for scanning the classpath for instances of {{UpgradeCheck}}
- Allow stacks to provide their own plugin directory & JAR for loading SPI implementations of {{UpgradeCheck}}",pull-request-available,[],AMBARI,Task,Critical,2018-10-12 14:21:13,31
13191169,Error while starting Timeline v2 Reader during Move operation,"# Deployed HDP-3.0.3 HA cluster with Ambari-2.7.3 (Active RM and Timeline v2 Reader are on the same host)
# Under Yarn --> Service Actions - choose to Move Timeline Service V2.0 Reader. Pick a new host and let the move operation complete

{code}
2018-10-11 13:04:33,887 ERROR reader.TimelineReaderServer (TimelineReaderServer.java:startTimelineReaderServer(236)) - Error starting TimelineReaderWebServer
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login from keytab
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.serviceInit(TimelineReaderServer.java:88)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.startTimelineReaderServer(TimelineReaderServer.java:233)
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.main(TimelineReaderServer.java:246)
Caused by: org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: yarn/ctr-e138-1518143905142-517945-01-000006.hwx.site@EXAMPLE.COM from keytab /etc/security/keytabs/yarn.service.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1847)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1215)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1008)
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:313)
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.serviceInit(TimelineReaderServer.java:85)
	... 3 more
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:897)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
	at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:1926)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1837)
	... 7 more
2018-10-11 13:04:33,890 INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status -1: Error starting TimelineReaderWebServer
2018-10-11 13:04:33,894 INFO  reader.TimelineReaderServer (LogAdapter.java:info(51)) - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down TimelineReaderServer at ctr-e138-1518143905142-517945-01-000002.hwx.site/172.27.74.4
************************************************************/
{code}",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-10-12 10:45:50,18
13191035,StackMerger utility tool for flattening stack definitions,Add StackMerger utility tool to flatten HDP-2.6 stack definition from Ambari-2.7 and export the flattened stack definition for use in Ambari 2.8,pull-request-available,"['ambari-server', 'stacks']",AMBARI,Task,Critical,2018-10-11 19:24:02,41
13191005,Infra Solr upgrade (2.7.x): ValueError: invalid literal for float():,"exception:
{code:java}
File ""/usr/lib/ambari-infra-solr-client/migrationHelper.py"", line 951, in parse_size 
return int(float(number)*units[unit]) 
ValueError: invalid literal for float(): 1,013.88 
{code}",pull-request-available,[],AMBARI,Bug,Blocker,2018-10-11 17:48:05,29
13190975,Ambari server continues to send request updates after all commands were completed.,After start/stop all request was completed server continues to send request updates. This causes UI shows invalid request/tasks progress statuses.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-11 16:18:27,6
13190903,Infra Manager: hive support for archiving Infra Solr,When exporting Solr documents from logsearch and ranger collections save it to a format which can be parsed by Hive.,pull-request-available,['infra'],AMBARI,Bug,Major,2018-10-11 13:20:08,16
13190872,Broken unit test in ambari-web/test/controllers/main/admin/highAvailability/journalNode/step3_controller_test.js,*Only for branch-feature-AMBARI-14714*.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-10-11 10:54:51,7
13190847,Ambari Server test failures with JDK11,"Ambari Server tests failures prevent build with JDK11:

{code}

± % JAVA_HOME=/tmp/jdk-11 mvn clean test -Drat.skip=true -am -pl ambari-server -DskipPythonTests=true

...
...
[ERROR]   ViewThrottleFilterTest.initializationError ? Objenesis java.lang.reflect.Invoc...
[ERROR]   DataStoreImplTest.initializationError ? Objenesis java.lang.reflect.Invocation...
[INFO]
[ERROR] Tests run: 4651, Failures: 0, Errors: 62, Skipped: 71
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Ambari Main ........................................ SUCCESS [  2.216 s]
[INFO] Apache Ambari Project POM .......................... SUCCESS [  0.008 s]
[INFO] Ambari Views ....................................... SUCCESS [ 57.359 s]
[INFO] ambari-utility ..................................... SUCCESS [  4.158 s]
[INFO] Ambari Server SPI .................................. SUCCESS [  1.341 s]
[INFO] Ambari Service Advisor ............................. SUCCESS [  0.355 s]
[INFO] Ambari Server ...................................... FAILURE [20:17 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 21:23 min
[INFO] Finished at: 2018-10-10T15:01:04+00:00
[INFO] Final Memory: 133M/494M
[INFO] ------------------------------------------------------------------------

{code}
",jdk11 pull-request-available,[],AMBARI,Bug,Major,2018-10-11 08:17:02,10
13190622,Ambari-agent takes up too many cpu on perf,"   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    14129 1426.122    0.101 1426.122    0.101 {time.sleep}
        1    0.337    0.337 1426.769 1426.769 main.py:358(run_threads)
      331    0.219    0.001    0.219    0.001 {method 'acquire' of 'thread.lock' objects}
       11    0.181    0.016    0.181    0.016 {built-in method poll}
        1    0.108    0.108    0.108    0.108 {method 'do_handshake' of '_ssl._SSLSocket' objects}
    14151    0.042    0.000    0.042    0.000 threading.py:571(isSet)
      125    0.028    0.000    0.028    0.000 {method 'flush' of 'file' objects}
     5078    0.027    0.000    0.052    0.000 decoder.py:65(py_scanstring)
       15    0.020    0.001    0.020    0.001 {posix.read}
     5093    0.020    0.000    0.024    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}
        1    0.019    0.019    0.019    0.019 {method 'connect' of '_socket.socket' objects}
    15365    0.018    0.000    0.018    0.000 {method 'match' of '_sre.SRE_Pattern' objects}
55424/13131    0.016    0.000    0.057    0.000 encoder.py:332(_iterencode_dict)
    38241    0.014    0.000    0.014    0.000 {isinstance}
       21    0.013    0.001    0.022    0.001 collections.py:282(namedtuple)
    473/5    0.012    0.000    0.073    0.015 decoder.py:148(JSONObject)
     5078    0.009    0.000    0.034    0.000 encoder.py:43(py_encode_basestring_ascii)
        5    0.006    0.001    0.070    0.014 __init__.py:122(dump)
    13251    0.005    0.000    0.005    0.000 {method 'write' of 'file' objects}
        7    0.004    0.001    0.004    0.001 {ambari_commons.libs.x86_64._posixsubprocess.fork_exec}
     5638    0.004    0.000    0.004    0.000 encoder.py:49(replace)
   3167/5    0.003    0.000    0.073    0.015 scanner.py:27(_scan_once)
13353/9909    0.003    0.000    0.030    0.000 encoder.py:279(_iterencode_list)
       75    0.003    0.000    0.003    0.000 {method 'read' of '_ssl._SSLSocket' objects}
   3177/8    0.003    0.000    0.008    0.001 Utils.py:124(make_immutable)
    13131    0.003    0.000    0.060    0.000 encoder.py:409(_iterencode)
    11128    0.003    0.000    0.003    0.000 {method 'groups' of '_sre.SRE_Match' objects}
  2202/45    0.003    0.000    0.004    0.000 Utils.py:135(get_mutable_copy)
   474/10    0.002    0.000    0.008    0.001 Utils.py:170(__init__)
      238    0.002    0.000    0.002    0.000 {time.localtime}
      119    0.002    0.000    0.004    0.000 __init__.py:242(__init__)
     3759    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
    14752    0.002    0.000    0.002    0.000 {method 'end' of '_sre.SRE_Match' objects}
    16854    0.002    0.000    0.002    0.000 {method 'append' of 'list' objects}
     5098    0.002    0.000    0.002    0.000 {method 'join' of 'unicode' objects}
      238    0.002    0.000    0.007    0.000 __init__.py:451(format)
       13    0.002    0.000    0.004    0.000 metric_alert.py:286(__init__)
        7    0.001    0.000    0.026    0.004 subprocess32.py:1153(_execute_child)
       41    0.001    0.000    0.001    0.000 {open}
      616    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      238    0.001    0.000    0.004    0.000 __init__.py:404(formatTime)
       18    0.001    0.000    0.001    0.000 {posix.listdir}
        5    0.001    0.000    0.072    0.014 ClusterCache.py:131(persist_cache)
     4032    0.001    0.000    0.003    0.000 collections.py:323(<genexpr>)
       18    0.001    0.000    0.001    0.000 {method 'sort' of 'list' objects}
       90    0.001    0.000    0.001    0.000 {built-in method now}
      238    0.001    0.000    0.001    0.000 {time.strftime}
      119    0.001    0.000    0.001    0.000 __init__.py:1215(findCaller)
     5706    0.001    0.000    0.001    0.000 {method 'group' of '_sre.SRE_Match' objects}
      281    0.001    0.000    0.001    0.000 threading.py:146(acquire)
      281    0.001    0.000    0.001    0.000 threading.py:186(release)
      119    0.001    0.000    0.001    0.000 {method 'seek' of 'file' objects}
        1    0.001    0.001    0.001    0.001 ClusterTopologyCache.py:58(on_cache_update)
      119    0.001    0.000    0.005    0.000 handlers.py:144(shouldRollover)
     50/5    0.001    0.000    0.035    0.007 decoder.py:223(JSONArray)
Major cpu cosumers:
1) Regexp operation: 
As we can see a lot of time is took for regexp operations. This happens because we use non-compiled regular expressions.
2) Json operations:
Another major cpu consumer is json module, because _speedups.so is not compiled for python2.7 currently. We have this situation. This is tackled by other issue
3) Main thread waking up/sleeping too often.
This seems to create quite a bit cpu usage.
The approach was implemented so agent can check for SIGTERM (ambari-agent stop). A proper solution should be a usage signal.pause() instead of sleep/wakeup.",pull-request-available,[],AMBARI,Bug,Major,2018-10-10 12:30:29,5
13190467,Restart All service on a host function not working,"Navigate to summary page for a single host.
Drop down Host Actions menu.
Click Restart All.

Server throws exception about missing stackId.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-09 19:31:41,40
13190352,upgrade ambari-utility and ambari-server-spi dependencies for JDK11,"{code}
± % java -version
java version ""11"" 2018-09-25
Java(TM) SE Runtime Environment 18.9 (build 11+28)
Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11+28, mixed mode)

± % mvn clean test -Dgenerate.swagger.resources=true -Drat.skip -pl ambari-utility
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building ambari-utility 1.0.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ ambari-utility ---
[INFO] Deleting /Users/gboros/Documents/dev/ambari/ambari-utility/target
[INFO] Deleting /Users/gboros/Documents/dev/ambari/ambari-utility (includes = [**/*.pyc], excludes = [])
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:regex-property (parse-package-version) @ ambari-utility ---
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:regex-property (parse-package-release) @ ambari-utility ---
[INFO]
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ambari-utility ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO]
[INFO] --- maven-compiler-plugin:3.2:compile (default-compile) @ ambari-utility ---
[INFO] Compiling 14 source files to /Users/gboros/Documents/dev/ambari/ambari-utility/target/classes
[INFO]
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ ambari-utility ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO]
[INFO] --- maven-compiler-plugin:3.2:testCompile (default-testCompile) @ ambari-utility ---
[INFO] Compiling 3 source files to /Users/gboros/Documents/dev/ambari/ambari-utility/target/test-classes
[INFO]
[INFO] --- maven-surefire-plugin:2.20:test (default-test) @ ambari-utility ---
[INFO]
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.ambari.swagger.AmbariSwaggerReaderTest
[INFO] Running org.apache.ambari.checkstyle.UndocumentedRestApiOperationCheckTest
[INFO] Running org.apache.ambari.checkstyle.AvoidTransactionalOnPrivateMethodsCheckTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.253 s - in org.apache.ambari.checkstyle.AvoidTransactionalOnPrivateMethodsCheckTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.287 s - in org.apache.ambari.checkstyle.UndocumentedRestApiOperationCheckTest
[ERROR] Tests run: 8, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 1.479 s <<< FAILURE! - in org.apache.ambari.swagger.AmbariSwaggerReaderTest
[ERROR] swaggerBasicCase(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 1.21 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerBasicCase(AmbariSwaggerReaderTest.java:71)
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.annotation.XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerBasicCase(AmbariSwaggerReaderTest.java:71)

[ERROR] swaggerConflictingNestedApis(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 0.006 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerConflictingNestedApis(AmbariSwaggerReaderTest.java:86)

[ERROR] swaggerConflictingNestedApisWithPreferredParent(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 0.004 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerConflictingNestedApisWithPreferredParent(AmbariSwaggerReaderTest.java:103)

[ERROR] swaggerConflictingNestedApisWithSamePreferredParent(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 0.003 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerConflictingNestedApisWithSamePreferredParent(AmbariSwaggerReaderTest.java:121)

[ERROR] swaggerConflictingNestedApisWithBadPreferredParent(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 0.003 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerConflictingNestedApisWithBadPreferredParent(AmbariSwaggerReaderTest.java:140)

[ERROR] swaggerNestedApisWithOverwrite(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 0.009 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerNestedApisWithOverwrite(AmbariSwaggerReaderTest.java:156)

[ERROR] swaggerApiThatIsBothTopLevelAndNestedIsCountedAsTopLevel(org.apache.ambari.swagger.AmbariSwaggerReaderTest)  Time elapsed: 0.003 s  <<< ERROR!
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlRootElement
	at org.apache.ambari.swagger.AmbariSwaggerReaderTest.swaggerApiThatIsBothTopLevelAndNestedIsCountedAsTopLevel(AmbariSwaggerReaderTest.java:171)

[INFO]
[INFO] Results:
[INFO]
[ERROR] Errors:
[ERROR]   AmbariSwaggerReaderTest.swaggerApiThatIsBothTopLevelAndNestedIsCountedAsTopLevel:171 » NoClassDefFound
[ERROR]   AmbariSwaggerReaderTest.swaggerBasicCase:71 » NoClassDefFound javax/xml/bind/a...
[ERROR]   AmbariSwaggerReaderTest.swaggerConflictingNestedApis:86 » NoClassDefFound java...
[ERROR]   AmbariSwaggerReaderTest.swaggerConflictingNestedApisWithBadPreferredParent:140 » NoClassDefFound
[ERROR]   AmbariSwaggerReaderTest.swaggerConflictingNestedApisWithPreferredParent:103 » NoClassDefFound
[ERROR]   AmbariSwaggerReaderTest.swaggerConflictingNestedApisWithSamePreferredParent:121 » NoClassDefFound
[ERROR]   AmbariSwaggerReaderTest.swaggerNestedApisWithOverwrite:156 » NoClassDefFound j...
[INFO]
[ERROR] Tests run: 10, Failures: 0, Errors: 7, Skipped: 0
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6.540 s
[INFO] Finished at: 2018-10-08T15:58:45+02:00
[INFO] Final Memory: 28M/104M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.20:test (default-test) on project ambari-utility: There are test failures.
[ERROR]
[ERROR] Please refer to /Users/gboros/Documents/dev/ambari/ambari-utility/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
{code}",jdk11 pull-request-available,[],AMBARI,Bug,Major,2018-10-09 12:03:30,10
13190183,Add Component to host functionality broken,"On the Host view, the Add Component functionality (clicking on the Add button in the Components section) is broken.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-10-08 18:45:07,40
13190059,Enable encryption of sensitive data in Ambari DB using Ambari CLI,"Enable encryption of sensitive data on Ambari DB using Ambari CLI. This is an update to the existing ""ambari-server setup-security"", option #2 - Encrypt passwords stored in ambari.properties file.

In addition to what the current script does, the following must also happen:

* Change the subtitle to ""Encrypt passwords managed by Ambari.""
* Authenticate an Ambari administrator user
* Ask the user if they want to encrypt sensitive service configuration data
* Add command line options for authenticating an Ambari administrator and whether sensitive service configuration data is to be encrypted
* Issue a request to Ambari to encrypt sensitive data (if the user wants this)",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-08 10:55:25,13
13189709,"Sensitive service configuration values should be encrypted in the Ambari server DB, if enabled","Sensitive service configuration values should be encrypted in the Ambari server DB, if enabled.

Sensitive service configuration values are defined by a service's configuration metadata. Properties are defined in XML files under the service's definition directory and contain attributes that Ambari may use to determine whether they should be encrypted or not.

Currently, Ambari uses the {{property-type}} attribute to determine the type of property. If the value of this attribute is ""PASSWORD"", than the value is considered sensitive and should be encrypted.
{code:java|title=Example: This password field is to be encrypted, implicitly}
  <property>
    <name>ssl.server.truststore.password</name>
    <value>bigdata</value>
    <property-type>PASSWORD</property-type>
    <description>Password to open the trust store file.</description>
    <value-attributes>
      <type>password</type>
    </value-attributes>
    <on-ambari-upgrade add=""false""/>
  </property>
{code}
Using this information, _if this feature is enabled_, the Ambari server should encrypt sensitive values before storing them in the database. Values should be encrypted within the container they are stored. For example, Ambari stores configurations as JSON documents. Before writing these JSON documents to the database, the Ambari server should process each name/value pair and encrypt only those that are deemed sensitive.

The Ambari server should encrypt sensitive configuration values if the following has been met:
 * A master key has been setup using the ""ambari-server setup-security"" CLI (using option #2 - Encrypt passwords stored in ambari.properties file)
 * The Ambari server configuration property named ""{{server.security.encrypt_sensitive_data}}"" is set to ""true""

If encrypting sensitive data:
 * the value should be encrypted using a secure symmetric key encryption algorithm. For example AES - [https://aesencryption.net/].
 * the encryption key should be the previously set master key, or some reproducible encoding of it.
 * the encrypted bytes should be converted to a hex string
 * the value should be stored in the relevant JSON document suck that that the value is declared as encrypted.
 ** for example:
{noformat}
""password"" : ""${enc=aes265_hex, value=5248...303d}""{noformat}

 ** this is needed in the event {{server.security.encrypt_sensitive_data}} is changed to false, but there are still encrypted values in the database.

Encrypted data needs to be decrypted before being used or returned via the REST API. The data may be re-encrypted depending on use. For example, when being sent to an Ambari agent.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-05 13:02:07,27
13189698,Update simplejson to newest with speedup bindings to python2.7 and PyUnicodeUCS4/PyUnicodeUCS2 variations,"Current version of simlejson budled with Ambari doesn't provide performance boost, as speedup lib is linked with libpython2.6, which caused slowness on cluster deployments and increasing the overall CPU resource usage.

Simplejson need to be updated to support python 2.7, specially speedup binding. As we supporting various distribs, we need to provide PyUnicodeUCS4/PyUnicodeUCS2 lib variations as well. In the future we need separate lib for ibm ppc

In addition, logging functionality should be added to agent, which will notify if the boost is used.

The way how logging should be done:

on Agent start with message like: ""Available boosts for: simplejson, subproccess32""",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Critical,2018-10-05 12:06:05,1
13189583,Update grafana datasource to get values from new object,In the new grafana version templateSrv doesn't have some fields. We need to get these values from a different fields,pull-request-available,['ambari-metrics'],AMBARI,Task,Major,2018-10-04 21:37:12,34
13189558,Expose Upgrade Check Classes via an Ambari Client Library,"The pre-upgrade check framework inside of Ambari needs to be migrated to the Ambari API/SPI being developed as part of AMBARI-24685 so that 3rd party stack developers can use this to compile and deliver their own custom upgrade checks for their stack.

The following items should be included:
- A way to obtain configuration information about the cluster's services
- Topology information, including services installed and the components/hosts of the cluster

The existing checks which are not stack-specific should remain in Apache and altered to use the new framework. This means that the {{ambari-server}} project will have a dependency on the new module created as part of AMBARI-24685.",pull-request-available,[],AMBARI,Task,Critical,2018-10-04 20:34:58,31
13189369,Infra Manager: scheduled cleanup of old jobs,"Infra manager should have a mechanism, which can be scheduled (like every day once), to check against old job history and get rid of them (if the job already finished about 7 days, it can be deleted -> all references in the db)",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-10-04 08:40:06,16
13189186,Update Ambari Server to new Java versioning scheme in Java 9,"Setting up Java 9 on Ambari Server prints the following error:
{code:java}
[root@gboros-jdk11-1 ~]# ambari-server setup -s -j /usr/java/jdk-9.0.4/ --stack-java-home /usr/java/jdk-9.0.4/
Using python  /usr/bin/python
Setup ambari-server
...
...
Checking JDK...
WARNING: JAVA_HOME /usr/java/jdk-9.0.4/ must be valid on ALL hosts
WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.
Setting JAVA_HOME for stack services...
WARNING: JAVA_HOME /usr/java/jdk-9.0.4/ (Stack) must be valid on ALL hosts
WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.
Check JDK version for Ambari Server...
JDK version found: 0
Minimum JDK version is 8 for Ambari. Setup JDK again only for Ambari Server.
{code}
This is a non-blocking error, the configuration is being written out properly:
{code:java}
[root@gboros-jdk11-1 ~]# cat /etc/ambari-server/conf/ambari.properties | grep 'java\.home'
java.home=/usr/java/jdk-9.0.4/
stack.java.home=/usr/java/jdk-9.0.4/
{code}
New versioning scheme is described at [Java 9 migration guide|https://docs.oracle.com/javase/9/migrate/toc.htm#JSMIG-GUID-3A71ECEF-5FC5-46FE-9BA9-88CBFCE828CB].

This becomes a blocker when using Java 10:

{code}
[root@gboros-jdk11-1 ambari]# ambari-server setup -s -j /usr/java/jdk-10.0.2/ --stack-java-home /usr/java/jdk-10.0.2/
Using python  /usr/bin/python
Setup ambari-server
...
...
Check JDK version for Ambari Server...
ERROR: Exiting with exit code 1.
REASON: Downloading or installing JDK failed: ""Fatal exception: Running java version check command failed: invalid literal for int() with base 10: '0 2018-07-17\\n'. Exiting., exit code 1"". Exiting.
{code}

 cc [~adoroszlai]

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-10-03 14:59:06,10
13189173,Ambari Server stops with Java 9 due to Guice error,"Ambari Server stopped a few seconds after start with the following error:

 
{code:title=/var/log/ambari-server/ambari-server.out}
Exception in thread ""main"" com.google.inject.CreationException: Unable to create injector, see the following errors:

1) No scope is bound to org.apache.ambari.server.AmbariService.
at org.apache.ambari.server.state.services.MetricsRetrievalService.class(MetricsRetrievalService.java:85)
while locating org.apache.ambari.server.state.services.MetricsRetrievalService
for field at org.apache.ambari.server.controller.jmx.JMXPropertyProvider.metricsRetrievalService(JMXPropertyProvider.java:88)
at org.apache.ambari.server.controller.metrics.MetricPropertyProviderFactory.createJMXPropertyProvider(MetricPropertyProviderFactory.java:1)
at com.google.inject.assistedinject.FactoryProvider2.initialize(FactoryProvider2.java:666)
at com.google.inject.assistedinject.FactoryModuleBuilder$1.configure(FactoryModuleBuilder.java:335) (via modules: org.apache.ambari.server.controller.ControllerModule -> com.google.inject.assistedinject.FactoryModuleBuilder$1)

1 error
at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)
at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:176)
at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)
at com.google.inject.Guice.createInjector(Guice.java:99)
at com.google.inject.Guice.createInjector(Guice.java:73)
at com.google.inject.Guice.createInjector(Guice.java:62)
at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:1079)
~
~
""/var/log/ambari-server/ambari-server.out"" 41L, 6558C{code}

{code}
cat /etc/ambari-server/conf/ambari.properties | grep 'java\.home'
java.home=/usr/java/jdk-9.0.4/
stack.java.home=/usr/java/jdk-9.0.4/
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-03 14:02:22,21
13189168,Orchestration Should Save Upgrade Pack for source or target,Stacks may now contain source stack information. Make sure that when orchestration happens that we indicate how to find the upgrade pack which was resolved and persist it.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-10-03 13:50:32,33
13189137,Infra Solr: Autoscaling based on metric alerts,Add simple Ambari Metrics client to ambari-commons which can be used from ambari alert python scripts.,pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-10-03 11:29:06,16
13189118,Remove outdated functions.list_ambari_managed_repos module,code cleanup ,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-03 10:13:15,1
13189093,Infra Solr: manage autoscaling properties in Ambari,"Add the feature upload autoscaling.json to the infra-solr znode to 
 AmbariSolrCloudCLI.",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-10-03 07:49:09,16
13189034,Remove ambari-metrics dependency on ambari-commons,The unnecessary dep was added to support OS defaults and test functions around windows supportability added a few years back. Since then the windows support has not been supported by any developers and is no longer of any use. We recently separated the metrics repo and this dep creates a circular build time dependency.,pull-request-available,['ambari-metrics'],AMBARI,Task,Critical,2018-10-03 00:37:25,42
13188795,Failed to force_non_member_install a stack version on hosts,"The ability to pre-install packages on hosts (before being added to the cluster) using the following API request is broken:

{noformat}
$ curl -X POST -d '\{ ""HostStackVersions"": { ""repository_version"": <repository_version>, ""stack"": ""HDP"", ""version"": ""2.6"", ""cluster_name"": ""TEST"", ""force_non_member_install"": true, ""components"": [ { ""name"" : ""ZOOKEEPER_SERVER"" }, \{ ""name"": ""ZOOKEEPER_CLIENT"" } ] } }' http://localhost:8080/api/v1/hosts/${hostname}/stack_versions
{noformat}

{noformat:title=ambari-agent.log}
ERROR 2018-06-19 08:54:08,222 CustomServiceOrchestrator.py:448 - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: 'Host_Level_Params for cluster_id=2 is missing. Check if server sent it.'; 'Host_Level_Params for cluster_id=2 is missing. Check if server sent it.'
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 322, in runCommand
    command = self.generate_command(command_header)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 487, in generate_command
    command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 38, in get_configuration
    host_level_params_cache = self.host_level_params_cache[cluster_id]
  File ""/usr/lib/ambari-agent/lib/ambari_agent/ClusterCache.py"", line 155, in __getitem__
    raise KeyError(""{0} for cluster_id={1} is missing. Check if server sent it."".format(self.get_cache_name().title(), key))
KeyError: 'Host_Level_Params for cluster_id=2 is missing. Check if server sent it.'
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-02 10:00:48,6
13188494,"STS fails after start, after stack upgrade from 3.0.1 to 3.0.3","See this exception in SHS log:
{code:java}
========================================
Warning: Master yarn-client is deprecated since 2.0. Please use master ""yarn"" with specified deploy mode instead.
Exception in thread ""main"" java.lang.IllegalArgumentException: requirement failed: Keytab file: none does not exist
 at scala.Predef$.require(Predef.scala:224)
 at org.apache.spark.deploy.SparkSubmit$.doPrepareSubmitEnvironment(SparkSubmit.scala:390)
 at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:250)
 at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:171)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
After i've removed spark.yarn.keytab/principal properties, it start work fine. By the way, this cluster is NOT kerberized. It's strange why SHS is trying to use these properties. In the same time properties spark.history.kerberos.keytab/principal also available but there is no issues. I expect question why spark.yarn.keytab/principal were added during stack upgrade if cluster is not kerberized, here is answer:
{code:java}
<transfer operation=""copy"" from-type=""spark2-defaults"" from-key=""spark.history.kerberos.keytab"" to-key=""spark.yarn.keytab"" default-value="""" if-type=""spark2-thrift-sparkconf"" if-key=""spark.yarn.keytab"" if-key-state=""absent""/>
 <transfer operation=""copy"" from-type=""spark2-defaults"" from-key=""spark.history.kerberos.principal"" to-key=""spark.yarn.principal"" default-value="""" if-type=""spark2-thrift-sparkconf"" if-key=""spark.yarn.principal"" if-key-state=""absent""/>
{code}
I thought if ""spark.history.kerberos.keyta/principal"" is available in non kerberized cluster then ""spark.yarn.keytab/principal"" could be added too. Also we have same logic for many other components in ambari. So the question should it be fixed on ambari side, i mean add spark.yarn.keytab/principal only if kerberos enabled or some condition should be modified/added on SPARK side, not to use it if kerberos disabled or value empty/none?

 ",pull-request-available,[],AMBARI,Bug,Blocker,2018-10-01 11:53:45,24
13188490,Ambari-agent does for save data hashes correctly,"This causes all the data to be re-send during registration. Which can be very
dramatic for perf clusters.

",pull-request-available,[],AMBARI,Bug,Major,2018-10-01 11:32:00,5
13188484,Restrict user permissions for Atlas configuration file,Need to restrict user permissions for Atlas configuration file.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-01 11:04:59,35
13188454,User aborted task reported as FAILED in Bgoperations,"User aborted task reported as FAILED in Bgoperations.

1. Install cluster with all services ( Including Druid)
2. Click on Stop all services
3. When BGoperations window is loaded, click on the abort button for the same task
4. The operation should be aborted and marked as Aborted with ""Aborted by user"" message in the sub tasks logs
5. But instead the operation is marked as failure.

Failed task is Druid Coordinator Stop
{code:java}
""href"": ""https://ctr-e138-1518143905142-466827-01-000002.hwx.site:8443/api/v1/clusters/cl1/requests/59/stages/0/tasks/756"",
""Tasks"": {
""attempt_cnt"": 1,
""cluster_name"": ""cl1"",
""command"": ""STOP"",
""command_detail"": ""DRUID_COORDINATOR STOP"",
""end_time"": 1536363316602,
""error_log"": ""/var/lib/ambari-agent/data/errors-756.txt"",
""exit_code"": 0,
""host_name"": ""ctr-e138-1518143905142-466827-01-000005.hwx.site"",
""id"": 756,
""ops_display_name"": null,
""output_log"": ""/var/lib/ambari-agent/data/output-756.txt"",
""request_id"": 59,
""role"": ""DRUID_COORDINATOR"",
""stage_id"": 0,
""start_time"": 1536363307548,
""status"": ""FAILED"",
""stderr"": ""\nCommand aborted. Reason: 'Aborted by user'"",
""stdout"": ""\nCommand aborted. Reason: 'Aborted by user'\n\nCommand failed after 1 tries\n"",
""structured_out"": {}
}{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-01 07:28:22,27
13188144,Implement support for Minimal Blueprint Export,"Implement a new mode for Blueprint exports, a ""minimal"" mode in which only modified configuration is included in the exported Blueprint. The original ""full"" Blueprint export should still be possible, in order to maintain backwards compatibility.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-09-28 12:34:35,21
13188138,Automation script for upgrade old style isilon cluster to the new mpack based structure,"A Python that can convert old style isilon onefs clusters to the new mpack based cluster. The script should replace all of the HDFS_CLIENTs to ONEFS_CLIENT and the HDFS_SERVICE to ONEFS_SERVICE. Config types like core-site, hdfs-site, hadoop-env should be preserved.",pull-request-available,['contrib'],AMBARI,Task,Major,2018-09-28 11:46:33,18
13188020,Build fails due to missing ambari-utility,"{noformat:title=mvn -Del.log=WARN -am -pl ambari-server -DskipTests clean test}
...
[ERROR] Failed to execute goal on project ambari-server: Could not resolve dependencies for project org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT: Could not find artifact org.apache.ambari:ambari-utility:jar:1.0.0.0-SNAPSHOT in oss.sonatype.org (https://oss.sonatype.org/content/groups/staging) -> [Help 1]
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-27 20:18:22,21
13187951,Problems with accounts page when sysprep_skip_create_users_and_groups is absent,When sysprep_skip_create_users_and_groups property is absent accounts page can not load.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-27 15:38:01,9
13187888,Slider widget: insert a space char between label value and unit-name,"On Ambari enhanced UI configuration change the slider widget by inserting a space character between label value and unit-name.
from
{code}
2days
{code}
to
{code}
2 days
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-27 12:14:08,16
13187747,UI: Options Page to choose Rolling or Express Restart,!Screen Shot 2018-09-26 at 3.21.27 PM.png!,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-09-26 22:04:09,34
13187698,Redo Add Host wizard,Add Host wizard needs major revamp to work in v3.0.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-26 18:34:12,40
13187634,Refactor Upgrade Classes to Proper Packages,"With SPI changes, key upgrade classes should move to the correct package to separate definition from orchestration",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-09-26 14:57:10,33
13187564,Ambari-server setup-ldap throws an error when the OU has spaces,"Ambari-server setup-ldap throws an error when the OU has spaces

{code}
ambari-server setup-ldap \
 --ldap-url=ldap.example.com:636 \
 --ldap-secondary-url=ldap.example.com:636 \
 --ldap-ssl=true \
 --ldap-user-class=person \
 --ldap-user-attr=sAMAccountName \
 --ldap-group-class=group \
 --ldap-group-attr=cn \
 --ldap-member-attr=member \
 --ldap-dn=distinguishName \
 --ldap-base-dn=""OU=Group Users,DC=example,DC=com"" \
 --ldap-referral=follow \
 --ldap-bind-anonym=false \
 --ldap-manager-dn=bind@example.com \
 --ldap-manager-password=**** \
 --ldap-sync-username-collisions-behavior=convert \
 --ldap-save-settings
{code}

error message:

{code}
Using python  /usr/bin/python
Usage: ambari-server.py action [options]

Options:
  -h, --help            show this help message and exit
...
  --truststore-reconfigure
                        Force to reconfigure TrustStore if exits
None
Usage: ambari-server.py action [options]

ambari-server.py: error: Invalid number of arguments. Entered: 2, required: 1
{code}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-09-26 10:13:31,18
13187435,Upgrade Pack Should Allow Source Information,Allow Upgrade Packs to specify {{source}} information in addition to {{target}} (for backward compatibility),pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-09-25 20:36:42,33
13187342,Create a Maven Consumable Ambari SPI Client Library,"With management packs and stacks being moved out of Ambari's source control, framework components such as upgrade checks and upgrade configuration actions will be to be delivered as part of the 3rd party stack and no long checked into Apache. 

However, Ambari does not expose any kind of API/SPI/library for consumers to use when creating their own custom classes. A new maven-compatible project should be created as a home for classes which we expect our 3rd party consumers to use when creating custom plugins to be delivered in their stacks. 

They can then compile their code against this API/SPI and deliver a JAR which can be discovered and loaded by Ambari.",pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-09-25 16:01:03,31
13187331,Wrong repo urls on UI for HDP-3.0.1.0 Default Version Definition,"Wrong repo urls on UI for HDP-3.0.1.0 Default Version Definition

Found this issue with public bits for Ambari-2.7.1
On ui install -> select version page, select HDP-3.0.1.0 (Default Version Definition) from the drop down

All the repository urls are still pointing to HDP-3.0.0.0",pull-request-available,['ambari-web'],AMBARI,Task,Blocker,2018-09-25 15:26:58,7
13187208,Rolling Restarts: Option to specify number of failures per batch,User may choose maximum number of failures or specify the number of failures per batch,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-25 08:24:36,28
13187182,Cannot deploy cluster without HDFS_CLIENT,"The attached blueprint cannot be deployed, because {{hadoop-env.sh}} is not saved by Ambari, because it requires {{HDFS_CLIENT}} component on at least one host:

https://github.com/apache/ambari/blob/8d145e0c04917866fd76690688826cf44065370e/ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/hook.py#L32-L33

Datanode start fails with:

{noformat}
ExecutionFailed: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/3.0.0.0-1634/hadoop/bin/hdfs --config /usr/hdp/3.0.0.0-1634/hadoop/conf --daemon start datanode'' returned 1. ERROR: JAVA_HOME is not set and could not be found.
{noformat}

This appears in output from {{after-INSTALL}} hook:

{noformat}
Parameter hadoop_conf_dir is missing or directory does not exist. This is expected if this host does not have any Hadoop components.
{noformat}

Additionally, host groups without {{HDFS_CLIENT}} cannot be scaled up.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-25 04:47:07,21
13187093,Fix race condition in agent during registration and topology updates.,"{code}
ambari_agent - CustomServiceOrchestrator.py - [4840] - root - ERROR - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: 7; 7#012Traceback (most recent call last):#012 File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 324, in runCommand#012 command = self.generate_command(command_header)#012 File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 504, in generate_command#012 command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)#012 File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 43, in get_configuration#012 'clusterHostInfo': self.topology_cache.get_cluster_host_info(cluster_id),#012 File ""/usr/lib/ambari-agent/lib/ambari_agent/ClusterTopologyCache.py"", line 108, in get_cluster_host_info#012 hostnames = [self.hosts_to_id[cluster_id]‌[host_id].hostName for host_id in component_dict.hostIds]#012KeyError: 7
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-09-24 17:40:17,39
13186916,Issues w.r.t Manage Journal Nodes (Particularly deletion) in HA enabled cluster.,"# we have a cluster of 6 hosts (postfixes are 5, 6, 8, 9, 10 and 14)
# this is a federated cluster with two namespaces: ns1 and ns2
# Namenode HA is enabled; JNs are located on 5, 6, and 14
# Added a new JN using the ""Manage Journalnodes"" wizard on host 8
# Removed an existing JN using the ""Manage Journalnodes"" wizard from host 6
# Relevant configurations after these steps:
{noformat}
dfs.namenode.shared.edits.dir=qjournal://ctr-e138-1518143905142-472105-01-000014.host.site:8485;ctr-e138-1518143905142-472105-01-000008.host.site:8485;ctr-e138-1518143905142-472105-01-000005.host.site:8485/ns1,ns2
dfs.namenode.shared.edits.dir.ns1=qjournal://ctr-e138-1518143905142-472105-01-000005.host.site:8485;ctr-e138-1518143905142-472105-01-000006.host.site:8485;ctr-e138-1518143905142-472105-01-000014.host.site:8485/ns1
dfs.namenode.shared.edits.dir.ns2=qjournal://ctr-e138-1518143905142-472105-01-000005.host.site:8485;ctr-e138-1518143905142-472105-01-000006.host.site:8485;ctr-e138-1518143905142-472105-01-000014.host.site:8485/ns2
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-23 23:52:05,9
13186644,Upgrade checkstyle version to 8.9,"Currently we use checkstyle 6.19 which is very old.

We should upgrade to 8.9 release.",pull-request-available,[],AMBARI,Task,Major,2018-09-21 15:53:05,21
13186607,The operation 'Install HDP-{$version} version' change name after ending and page reloading,"STR:
1) Deploy ambari 2.7.1 cluster
2) Make registration of new version HDP 

Actual result: The operation 'Install HDP-{$version} version' change name (from 'Install HDP-{$version} version' on 'Install Version') after ending of operation and page reloading. 
Note: take a look on screenshots

Expected result: The operation for installing of new HDP version must have the constant name.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-21 12:22:21,19
13186596, Regenerating keytabs for HDFS only does not re-create headless keytabs on all hosts where needed,"+*STR*+
 # install a cluster with HDFS and Tez (+other dependecies)
 # Kerberize the cluster
 # remove the HDFS client from the host where Tez client is installed
 # regenerate keytabs for HDFS

+*Actual results*+

  the headless keytab for HDFS has not been regenerated

+*Expected results*+

  the headless keytab is regenerated where it's necessary",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-09-21 11:51:37,27
13186544,Workaround for non-atomic directory creation,"{{before-*}} hooks create a few directories.  If parallel agent execution is enabled tasks may need to be retried, because directory creation is not atomic (see AMBARI-24670).  This causes delays during cluster deployment.  While the underlying problem is being fixed, the goal of this task is to provide a workaround by creating the directory during sysprep phase.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-09-21 05:29:29,21
13186542,Directory creation should be atomic,"If parallel execution is enabled on Ambari Agent, concurrent directory creation may fail with:

{noformat:title=errors-62.txt}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 31, in hook
    setup_hadoop_env()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py"", line 203, in setup_hadoop_env
    mode=01777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 191, in action_create
    sudo.makedir(path, self.resource.mode or 0755)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 121, in makedir
    os.mkdir(path)
OSError: [Errno 17] File exists: '/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'
{noformat}

or

{noformat:title=errors-63.txt}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 31, in hook
    setup_hadoop_env()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py"", line 203, in setup_hadoop_env
    mode=01777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 179, in action_create
    path = sudo.readlink(path)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 161, in readlink
    return os.readlink(path)
OSError: [Errno 22] Invalid argument: '/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'
{noformat}

The failed tasks need to be retried to succeed, causing delays.",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-09-21 05:21:16,5
13186362,Component status can stuck in starting/stopping status on heartbeat lost,"Now server sets components statuses to UNKNOWN and saves last actual state on heartbeat lost. After agent re-registering server restores saved state, but in case ""in progress"" state request is already aborted and agent status updates can not be applied. Agent should always send component statuses after registering, not only on status changes. Also last actual state logic should be removed. ",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-09-20 14:10:54,6
13186332,JS errors during adding hosts,"Steps:
# Deploy any cluster.
# Do not refresh the page.
# Go through add hosts wizard to deploy step.

Result: JS errors in console after deploy button click. Also some persist requests with errors were sent to the server. ""App.get('clusterId')"" returns null. For perf cluster deploy the number of errors increases to several hundred.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-20 12:48:01,19
13186309,Implement data visualization color palette,"Colors in data visualization should be used in the following order of priority:
# #41bfae
# #79e3d1
# #63c2e5
# #c4aeff
# #b991d9
# #ffb9bf
# #ffae65
# #f6d151
# #a7cf82
# #abdfd5
# #3aac9c
# #6dccbc
# #59aece
# #b09ce5
# #a682c3
# #e5a6ac
# #e59c5b
# #ddbc49
# #96ba75
# #9ac8bf
# #83d5ca
# #a8ede1
# #99d7ee
# #d9caff
# #d1b7e6
# #ffd1d5
# #ffca9b
# #f9e18e
# #c6e0ae
# #c8eae4",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-09-20 11:29:31,15
13186042,'ambari-server setup' does not use AMBARI_SECURITY_MASTER_KEY env var,"
*STR*
Installed ambari-server and configured password encryption, but chose not to persist master key
{code}
[root@h002 ~]# ambari-server setup-security
Using python  /usr/bin/python
Security setup options...
===========================================================================
Choose one of the following options:
[1] Enable HTTPS for Ambari server.
[2] Encrypt passwords stored in ambari.properties file.
[3] Setup Ambari kerberos JAAS configuration.
[4] Setup truststore.
[5] Import certificate to truststore.
===========================================================================
Enter choice, (1-5): 2
Password encryption is enabled.
Do you want to reset Master Key? [y/n] (n): y
Master Key not persisted.
Enter current Master Key:
Enter new Master Key:
Re-enter master key:
Do you want to persist master key. If you choose not to persist, you need to provide the Master Key while starting the ambari server as an env variable named AMBARI_SECURITY_MASTER_KEY or the start will prompt for the master key. Persist [y/n] (y)? n
Adjusting ambari-server permissions and ownership...
Ambari Server 'setup-security' completed successfully.
{code}

Then export environment variable
export AMBARI_SECURITY_MASTER_KEY=hadoop

ambari-server setup promts to enter current master key

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-09-19 11:51:16,13
13185820,Blueprint cluster installation with host group configuration throws exception,RegisterWithConfigGroupTask throws an exception during blueprint installation in case host group configs are defined.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-18 14:29:07,20
13185801,[Log Search UI] Handle the 401 and the 403 response status at login,Since the backend response at login/authorisation has been changed in the way that the 401 and the 403 status is handled differently the UI should reflect on this change.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Task,Major,2018-09-18 13:05:06,26
13185736,Tasks fail on ambari-agent intermittently under cpu load due to race condition in ambari-agent,"
    INFO 2018-09-13 18:30:15,817 ClusterCache.py:125 - Rewriting cache ClusterTopologyCache for cluster 2
    ERROR 2018-09-13 18:30:16,386 CustomServiceOrchestrator.py:456 - Caught an exception while executing custom service command: <type 'exceptions.TypeError'>: 'NoneType' object has no attribute '__getitem__'; 'NoneType' object has no attribute '__getitem__'
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 324, in runCommand
        command = self.generate_command(command_header)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 504, in generate_command
        command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 46, in get_configuration
        'agentLevelParams': {'hostname': self.topology_cache.get_current_host_info(cluster_id)['hostName']},
    TypeError: 'NoneType' object has no attribute '__getitem__'
    

This was found while perf testing.  
However can occur under any circumstances if ambari-agent node if slow enough.

",pull-request-available,[],AMBARI,Bug,Major,2018-09-18 08:01:07,5
13185644,Ambari metrics collector failed to show data for custom metrics,"*Steps to reproduce:
*
1. First generate metrics and post it to AMS.

curl --header ""Content-Type: application/json"" -X POST http://apappu3.hdp.com:6188/ws/v1/timeline/metrics --data @data1.json


{noformat}
{
  ""metrics"": [
    {
      ""metricname"": ""AMBARI_METRICS.SmokeTest.FakeMetric"",
      ""appid"": ""AMS-TEST-APPID"",
      ""hostname"": ""ambari20-5"",
      ""timestamp"": 1537211229000,
      ""starttime"": 1537211229000,
      ""metrics"": {
        ""1537211229000"": 0.963781711428,
        ""1537211230000"": 1432075898000
      }
    }
  ]
}
{noformat}

2. Now make REST api call to see if the data is present.

http://apappu3.hdp.com:3000/api/datasources/proxy/1/ws/v1/timeline/metrics?metricNames=AMBARI_METRICS.SmokeTest.FakeMetric._avg&hostname=%&appId=AMS-TEST-APPID

This does not return any data - reason for this is , while writing it takes the appid as case sensitive but while reading always looks for lower case appid.

Workaround: change the appid from ""AMS-TEST-APPID"" to ""ams-test-appid"" (all lower case)- then GET REST API works fine.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-17 21:30:51,14
13185599,Add cluster and stack settings properties to agent STOMP updates,Server should post cluster settings and stack settings to agent with STOMP updates.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-17 18:13:55,6
13185502,Log Search: add simple file based authorization ,"Make a simple way to authorize users. 

At fist just create a simple one where you can list to users (who can be admin or simple user) ",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-09-17 12:37:57,29
13185210,'ambari-server setup-ldap' fails with AttributeError when master_key is not persisted,"*STR*
Installed ambari-server and configured password encryption, but chose not to persist master key
{code}
[root@ctr ~]# ambari-server setup-security
Using python  /usr/bin/python
Security setup options...
===========================================================================
Choose one of the following options:
[1] Enable HTTPS for Ambari server.
[2] Encrypt passwords stored in ambari.properties file.
[3] Setup Ambari kerberos JAAS configuration.
[4] Setup truststore.
[5] Import certificate to truststore.
===========================================================================
Enter choice, (1-5): 2
Password encryption is enabled.
Do you want to reset Master Key? [y/n] (n): y
Master Key not persisted.
Enter current Master Key:
Enter new Master Key:
Re-enter master key:
Do you want to persist master key. If you choose not to persist, you need to provide the Master Key while starting the ambari server as an env variable named AMBARI_SECURITY_MASTER_KEY or the start will prompt for the master key. Persist [y/n] (y)? n
Adjusting ambari-server permissions and ownership...
Ambari Server 'setup-security' completed successfully.
{code}

Then export environment variable
export AMBARI_SECURITY_MASTER_KEY=hadoop

Thereafter ran the following:
*Issue #1* - Gave AttributeError after accepting the 'Save settings' prompt, instead of asking for master key
{code}
[root@ctr ~]# ambari-server setup-ldap -v
Using python  /usr/bin/python
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: about to run command: ps -p 5596
INFO:
process_pid=12677
Please select the type of LDAP you want to use (AD, IPA, Generic LDAP):Generic LDAP
Primary LDAP Host (ldap.ambari.apache.org): ctr
Primary LDAP Port (389):
Secondary LDAP Host <Optional>:
Secondary LDAP Port <Optional>:
Use SSL [true/false] (false):
User object class (posixUser):
User ID attribute (uid):
Group object class (posixGroup):
Group name attribute (cn):
Group member attribute (memberUid):
Distinguished name attribute (dn):
Search Base (dc=ambari,dc=apache,dc=org): dc=apache,dc=org
Referral method [follow/ignore] (follow):
Bind anonymously [true/false] (false):
Bind DN (uid=ldapbind,cn=users,dc=ambari,dc=apache,dc=org): uid=hdfs,ou=people,ou=dev,dc=apache,dc=org
Enter Bind DN Password:
Confirm Bind DN Password:
Handling behavior for username collisions [convert/skip] for LDAP sync (skip):
Force lower-case user names [true/false]:
Results from LDAP are paginated when requested [true/false]:
====================
Review Settings
====================
Primary LDAP Host (ldap.ambari.apache.org):  ctr
Primary LDAP Port (389):  389
Use SSL [true/false] (false):  false
User object class (posixUser):  posixUser
User ID attribute (uid):  uid
Group object class (posixGroup):  posixGroup
Group name attribute (cn):  cn
Group member attribute (memberUid):  memberUid
Distinguished name attribute (dn):  dn
Search Base (dc=ambari,dc=apache,dc=org):  dc=apache,dc=org
Referral method [follow/ignore] (follow):  follow
Bind anonymously [true/false] (false):  false
Handling behavior for username collisions [convert/skip] for LDAP sync (skip):  skip
ambari.ldap.connectivity.bind_dn: uid=hdfs,ou=people,ou=dev,dc=apache,dc=org
ambari.ldap.connectivity.bind_password: *****
Save settings [y/n] (y)? y
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Traceback (most recent call last):
File ""/usr/sbin/ambari-server.py"", line 1060, in <module>
mainBody()
File ""/usr/sbin/ambari-server.py"", line 1030, in mainBody
main(options, args, parser)
File ""/usr/sbin/ambari-server.py"", line 980, in main
action_obj.execute()
File ""/usr/sbin/ambari-server.py"", line 79, in execute
self.fn(*self.args, **self.kwargs)
File ""/usr/lib/ambari-server/lib/ambari_server/setupSecurity.py"", line 860, in setup_ldap
encrypted_passwd = encrypt_password(LDAP_MGR_PASSWORD_ALIAS, mgr_password, options)
File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 858, in encrypt_password
return get_encrypted_password(alias, password, properties, options)
File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 867, in get_encrypted_password
masterKey = get_original_master_key(properties, options)
File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 1022, in get_original_master_key
if options is not None and options.master_key is not None and options.master_key:
AttributeError: Values instance has no attribute 'master_key'
[root@ctr ~]#
{code}

*Issue #2* - Kept asking for Master key on the prompt, despite giving correct values
{code}
[root@ctr ~]# ambari-server setup
Using python  /usr/bin/python
Setup ambari-server
Checking SELinux...
WARNING: Could not run /usr/sbin/sestatus: OK
Customize user account for ambari-server daemon [y/n] (n)?
Adjusting ambari-server permissions and ownership...
Checking firewall status...
Checking JDK...
Do you want to change Oracle JDK [y/n] (n)?
Check JDK version for Ambari Server...
JDK version found: 8
Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.
Checking GPL software agreement...
Completing setup...
Configuring database...
Enter advanced database configuration [y/n] (n)?
Configuring database...
Enter current Master Key:
Default properties detected. Using built-in database.
Enter current Master Key:
Configuring ambari database...
Checking PostgreSQL...
Configuring local database...
Configuring PostgreSQL...
Backup for pg_hba found, reconfiguration not required
Creating schema and user...
done.
Creating tables...
done.
Enter current Master Key:
Enter current Master Key:
Enter current Master Key:
{code}


*Issue #3* - Gave an incorrect master key this time and the shell kept on printing ""ERROR: ERROR: Master key does not match."" and kept scrolling the page
{code}
[root@ctr ~]# ambari-server setup
Using python  /usr/bin/python
Setup ambari-server
Checking SELinux...
WARNING: Could not run /usr/sbin/sestatus: OK
Customize user account for ambari-server daemon [y/n] (n)?
Adjusting ambari-server permissions and ownership...
Checking firewall status...
Checking JDK...
Do you want to change Oracle JDK [y/n] (n)?
Check JDK version for Ambari Server...
JDK version found: 8
Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.
Checking GPL software agreement...
Completing setup...
Configuring database...
Enter advanced database configuration [y/n] (n)?
Configuring database...
Enter current Master Key:
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
ERROR: ERROR: Master key does not match.
^C
Aborting ... Keyboard Interrupt.
{code}

*Note/Workaround:* The issues are seen when master key is not persisted as part of the initial password encryption step

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-09-14 17:58:58,13
13185183,Issues with tooltip containing custom time range for charts,"- The tooltip content is not formatted
- Tooltip isn't displayed on some pages",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-14 16:10:18,15
13185127,hive service check failing related to LLAP in Ambari,"I have following configuration in ambari for my Hive

Default group: hive.server2.transport.mode=binary 
LLAP group:	hive.server2.transport.mode=http  (by adding this property in Custom hive-interactive-site )

The issue here is that when I'm doing a service check of Hive, this is what ambari is trying to do is trying to connect to the interactive server with transport mode 'binary' and port 10500 through it's in http mode.

{code:java}
2018-09-14 11:50:21,639 - Running Hive Server2 checks
2018-09-14 11:50:21,639 - --------------------------

2018-09-14 11:50:21,641 - Server Address List : ['hiveinteractive.openstacklocal'], Port : 10500, SSL KeyStore : None
2018-09-14 11:50:21,641 - Waiting for the Hive Server2 to start...
2018-09-14 11:50:21,641 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-kartik@godofwar.com; '] {'user': 'ambari-qa'}
2018-09-14 11:50:21,816 - Execute['! beeline -u 'jdbc:hive2://hiveinteractive.openstacklocal:10500/;transportMode=binary;principal=hive/_HOST@godofwar.com'  -e '' 2>&1| awk '{print}'|grep -i -e 'Connection refused' -e 'Invalid URL''] {'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'], 'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'}
{code}

ambari should connect through to LLAP through http and port 10501 as my LLAP server is in http mode",pull-request-available,[],AMBARI,Bug,Major,2018-09-14 12:16:34,3
13185115,"Stackadvisor error while trying to add atlas service. Error - ""if mountPoints[mountPoint] < reqiuredDiskSpace: KeyError: None""","Stack advisor error: 

{code}
Error details: None
2018-09-11 06:40:09,738  INFO [ambari-client-thread-24150] StackAdvisorRunner:167 -     Advisor script stderr: Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 184, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 138, in main
    result = stackAdvisor.validateConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1079, in validateConfigurations
 validationItems = self.getConfigurationsValidationItems(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1468, in getConfigurationsValidationItems
    items.extend(self.getConfigurationsValidationItemsForService(configurations, recommendedDefaults, service, services, hosts))
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1521, in getConfigurationsValidationItemsForService
    items.extend(serviceAdvisor.getServiceConfigurationsValidationItems(configurations, recommendedDefaults, services, hosts))
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/AMBARI_METRICS/service_advisor.py"", line 218, in getServiceConfigurationsValidationItems
    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1491, in validateListOfConfigUsingMethod
    validationItems = method(siteProperties, siteRecommendations, configurations, services, hosts)
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/AMBARI_METRICS/service_advisor.py"", line 556, in validateAmsHbaseSiteConfigurationsFromHDP206
    validationItems.extend([{""config-name"": 'hbase.rootdir', ""item"": self.validatorEnoughDiskSpace(properties, 'hbase.rootdir', host[""Hosts""], recommendedDiskSpace)}])
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 2998, in validatorEnoughDiskSpace
    if mountPoints[mountPoint] < reqiuredDiskSpace:
KeyError: None
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-09-14 11:23:34,18
13185113,Log Search: support to remove suffixes from field names,"new available properties to remove suffixes:
{{logsearch.web.labels.service_logs.field.fallback.suffixes=_l,_s,_b,_i}}
{{logsearch.web.labels.audit_logs.field.fallback.suffixes=_l,_s,_b,_i}}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Blocker,2018-09-14 10:58:02,29
13185090,Not able to turn off maintenance mode for a host.,"In the cluster , while trying to turn off maintenance mode on host , nothing happens and below error is seen in the console.

Initial analysis indicate that this could be because some host components are out of sync. But this may or may not be RC. Kindly check.

{code:java}
Uncaught TypeError: Cannot read property 'get' of undefined
    at Class.onOffPassiveModeForHost (app.js:26825)
    at Class.doAction (app.js:26800)
    at handler (vendor.js:31554)
    at HTMLAnchorElement.<anonymous> (vendor.js:23346)
    at HTMLDivElement.dispatch (vendor.js:3178)
    at HTMLDivElement.elemData.handle (vendor.js:2854)
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-14 08:52:55,19
13185077,Ambari-agent process memory leak,"There was one process which started using memory rapidly at certain point and grew up to ~27GB of RSS used until eventually we restarted it. Which happened after a month of running of 10 ambari-agent nodes.

    [root@andrew2-1n01 ~]# ps aux | grep ambari_agent
    root     39955  0.0  0.0  47580  6024 ?        S    Aug17   0:00 /usr/bin/python /usr/lib/ambari-agent/lib/ambari_agent/AmbariAgent.py start
    root     39959 20.4 10.2 31623096 27154348 ?   Sl   Aug17 7645:55 /usr/bin/python /usr/lib/ambari-agent/lib/ambari_agent/main.py start

Just before the growth in memory usage is seen. This exception pops out:

ERROR 2018-09-11 10:56:59,716 websocket.py:552 - Websocket connection was closed with an exception
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 549, in run
    if not self.once():
  File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 428, in once
    if not self.process(self.buf[:requested]):
  File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 483, in process
    self.reading_buffer_size = s.parser.send(bytes) or DEFAULT_READING_SIZE
ValueError: generator already executing

This exception is not seen on all other nodes or on this one at any other period (during 1 month). So I suggest it can be the root cause.
Basically this error means that generator is being used by multiple threads. So I will upload the fix to thread-lock this place.

This is just a guess solution which might work and might not. No way to test really. But definitely we should try this.
    
This is noticed in ambari-2.7.1.0-73 version as well.  

",pull-request-available,[],AMBARI,Bug,Major,2018-09-14 08:07:08,5
13185027,Ambari metrics service check failed during UI deploy,Race conditions in UUID computation helper caused issues in writes.,pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-09-13 23:04:14,39
13184971,Add Service wizard fails if a service without configs is installed,Fails during the Customize services page,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-13 18:12:23,34
13184930,Web Client Chooses Wrong Version When Reverting Configs,"STR:

- Setup a simple cluster with ZK, HDFS, YARN, Hive, Pig
- Create at least 5 different configuration versions for Pig
- Load the Pig configuration page and select v4
- Now select V3 and then choose ""Make Current""

At this point, the web client will populate a dialog that says {{Created from service config version V4}} and indeed it restores v4, not v3.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-13 14:51:58,9
13184847,Ambari strips leading space for valid YARN configs on the config compare screen,"2 optional properties in yarn-site,
{noformat}
yarn.timeline-service.read.allowed.users
<queue>.acl_application_max_priority
{noformat}
are of the form
{noformat}
<user1>,<user2>..<white_space><group1>,<group2>..
{noformat}
Here the users are optional. Hence only groups could be specified with a leading space followed by <group1>,<group2>..
But after setting such a configuration, Ambari strips the leading space and shows them as
{noformat}
<group1>,<group2>
{noformat}
instead of,
{noformat}
 <group1>,<group2>
{noformat}
when comparing 2 config versions.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-09-13 09:33:34,7
13184838,APT/DPKG existence check doesn't work for system packages,"Package existence check for APT/DPKG is a bit too strict:

{noformat:title=https://github.com/apache/ambari/blob/c17762bdd91fddc0cca87d3aa24e4121c7b5e675/ambari-common/src/main/python/ambari_commons/repo_manager/apt_manager.py#L70}
check_cmd = pkg_manager_bin + "" --get-selections | grep -v deinstall | awk '{print $1}' | grep ^%s$""
{noformat}

Also in previous versions: https://github.com/apache/ambari/blob/64cc76bfc7c83707f395d8487ea983dc8fddb267/ambari-common/src/main/python/resource_management/core/providers/package/apt.py#L51

System packages are postfixed by architecture, hence {{grep}} fails to find these due to {{$}}.  Examples where the existence check fails:

{noformat}
$ dpkg --get-selections | grep -v deinstall | awk '{print $1}' | grep -e '^liblzo2-2' -e '^libsnappy-dev'
liblzo2-2:amd64
libsnappy-dev:amd64
{noformat}

This results in {{Installing package ...}} being logged, which is a bit confusing.  Plus an unnecessary {{apt-get}} call, including wait for {{apt.lock}}, which the {{dpkg}} command was supposed to avoid.  Once {{apt-get}} gets to run, it returns quickly.",pull-request-available,[],AMBARI,Bug,Minor,2018-09-13 09:04:29,1
13184746,[Log Search UI] styles and layout fixes,"# Move number of events up on the page higher so it’s centered between the top of the gray area and the beginning of the Histogram frame
 # The message about triggering auto-refresh should be displayed in modal dialog
 # Filters panel should be a bit less transparent",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Major,2018-09-12 23:06:13,26
13184703,Redo Add Service wizard,Add Service wizard needs major revamp to work with v3.0 mpack model.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-12 19:28:15,40
13184664,"Fix possible ""Phishing by Navigating Browser Tabs"" vulnerability","According to details found at https://www.netsparker.com/web-vulnerability-scanner/vulnerabilities/phishing-by-navigating-browser-tabs/, it is possible to change the ""window.opener.location"" value in browser windows opened using normal anchor tags where the ""target"" attribute is specified as ""_blank"".

This gives an attacker the ability to change the parent location and thus potentially allow for a phishing attack to invoked.

To help this situation, it is suggested that the following attribute be set along with the ""target"" attribute:


{noformat}
rel=""noopener noreferrer""
{noformat}

For example:


{noformat}
<a href=""..."" target=""_blank"" rel=""noopener noreferrer"">...</a>
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-12 16:30:27,9
13184596,"""Host is in Maintenance mode"" text is not displayed in UI after maintenance mode is turned on","Behavior in ambari-2.7.0
1. login
2. navigate host details page.
3. click host action
4. click ""Turn On Maintenance Mode""
after the first confirm window. we can see the text which displays ""Host is in Maintenance mode""
 
But in ambari-2.7.1
The UI Tab which display ""Turn On Maintenance Mode"" 
It is not getting displayed.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-12 13:08:53,19
13184568,"Ambari self signed certificate generated with 1024 bits length, need to make this 2048","
Currently Ambari server/agent  are using self signed certificate and The certificate is generated with 1024 bits length. Generally the policy requirement is to have key length > 1048

We have this hard code in out code

{code}
GEN_AGENT_KEY=""openssl req -new -newkey rsa:1024 -nodes -keyout %(keysdir)s/%(hostname)s.key\
-subj /OU=%(hostname)s/\
-out %(keysdir)s/%(hostname)s.csr""
{code}

ref : https://github.com/hortonworks/ambari/blob/master/ambari-agent/src/main/python/ambari_agent/security.py



",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-09-12 11:01:39,13
13184553,Log Search: Field type 'key_lower_case' not found - in Solr,"in managed-schema of the service logs, key_lower_case was removed as a type, but there are references for that in the solrconfig.xml -> workaround, it can be changed to {{lowercase}} from {{key_lower_case}} in logsearch-service_logs-solrconfig content, and restart Log Search (after that: restart solr)",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Blocker,2018-09-12 09:15:41,29
13184542,hdfsResource fails to using nameservices for filesystems which does not support that,"Some filesystems does not support nameservices:

    
    
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hadoop --config /usr/hdp/3.0.1.0-179/hadoop/conf jar /var/lib/ambari-agent/lib/fast-hdfs-resource.jar /var/lib/ambari-agent/tmp/hdfs_resources_1536226305.38.json' returned 1. Exception occurred, Reason: abfs://mycluster has invalid authority.
    abfs://mycluster has invalid authority.
    	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:732)
    	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:129)
    	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:97)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3354)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
    	at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:83)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
    	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-09-12 08:22:44,5
13184530,Allow skipping package operations for LZO on sysprepped hosts,"LZO packages may be pre-installed in sysprepped environments, but Ambari still manages the repo and checks for existence of the packages, which takes time.  The goal of this change is to allow users who pre-install packages to skip package manager operations for LZO packages, too.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-09-12 05:45:37,21
13184400,Badge with count of empty or invalid properties are missed ar the services panel during cluster installation,"STR:
1)Initiate cluster installation
2)Navigate to customize all services page
3)Navigate to service tab
4)Clear some service required property

Expected
Count of errors near service name

Actual
Count of errors near service name is absent",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-11 16:25:48,9
13184361,Duplicate view of configurations in Add Service wizard,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-11 14:08:00,19
13184340,Horizontal scroll bar on assign slaves and clients page is not convenient for deploy with numerous hosts,It is not convenient to configure slaves and clients for deploy with numerous hosts because horizontal scroll bar is placed in the bottom of hosts list.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-11 12:55:00,15
13184272,Ambari generates wrong zookeeper connection string for KMS HA configuration,"* Zookeeper nodes are: node-1, node-2, node-3
 * KMS HA nodes: node-7, node-8.
Ambari generated value for the above property has node-7 and node-8 – see picture attached – but should have node-1, node-2 and node-3.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-09-11 09:08:55,7
13184100,Disable Kerberos from Ambari UI didn't clean up keytab directories,"Disable Kerberos from Ambari UI didn't clean up keytab directories,

stderr:

{code:java}
2018-09-08 05:27:19,276 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,298 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,325 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,348 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,465 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,491 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,515 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,539 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,671 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,696 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,723 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,744 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,959 - Failed to remove identity for nm/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,987 - Failed to remove identity for nm/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,049 - Failed to remove identity for nn/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,376 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,399 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,420 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,441 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,590 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,617 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,647 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,677 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,768 - Failed to remove identity for zookeeper/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,798 - Failed to remove identity for zookeeper/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type
{code}


{code:java}
[root@ctr-e138-1518143905142-467151-01-000002 ~]# ls -lrt /etc/security/keytabs/
total 56
-rw-r----- 1 ambari-qa  hadoop 318 Sep  7 23:00 kerberos.service_check.090718.keytab
-r-------- 1 slava      slava  353 Sep  7 23:05 ambari.server.keytab
-r--r----- 1 root       hadoop 538 Sep  7 23:05 spnego.service.keytab
-r-------- 1 cstm-ams   hadoop 548 Sep  7 23:05 ams-monitor.keytab
-r-------- 1 cstm-hdfs  hadoop 533 Sep  7 23:05 nfs.service.keytab
-r--r----- 1 cstm-hbase hadoop 338 Sep  7 23:05 hbase.headless.keytab
-r-------- 1 yarn-ats   hadoop 328 Sep  7 23:05 yarn-ats.hbase-client.headless.keytab
-r-------- 1 cstm-hdfs  hadoop 528 Sep  7 23:05 dn.service.keytab
-r-------- 1 yarn-ats   hadoop 588 Sep  7 23:05 yarn-ats.hbase-regionserver.service.keytab
-r--r----- 1 ambari-qa  hadoop 333 Sep  7 23:05 smokeuser.headless.keytab
-r-------- 1 cstm-hbase hadoop 543 Sep  7 23:05 hbase.service.keytab
-r-------- 1 cstm-hdfs  hadoop 528 Sep  7 23:05 nn.service.keytab
-r-------- 1 yarn-ats   hadoop 588 Sep  7 23:05 yarn-ats.hbase-master.service.keytab
-r-------- 1 cstm-hdfs  hadoop 333 Sep  7 23:05 hdfs.headless.keytab
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-10 17:08:19,30
13184016,[Log Search UI] Date picker preset ranges are not displayed properly,[Logsearch UI] Date picker preset ranges are not displayed properly,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-09-10 11:18:01,26
13184008,Host Level Maintenance mode is not working through Ambari UI,"Steps to reproduce:-
1. Login
2.Open host page.
3. select on host.
4. click on ""Host-Action"" Tab and select ""Turn On Maintenance mode ""

The message is showing 
{code}
Are you sure you want to Turn Off Maintenance Mode for ctr-e138-1518143905142-471677-01-000006.hwx.site?
{code}

I am trying to Turn On Maintenance Mode.

after that 
 !Screen Shot 2018-09-10 at 2.31.42 PM.png|thumbnail! 
{code}
Maintenance Mode has been turned off. It may take a few minutes for the alerts to be enabled.
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-09-10 11:05:34,19
13184001,Disabling an Alert Does Not Clear It in the Web UI,"When you disable an alert, the web client shows that the instance of the alert was removed immediately, however, the red badges indicating a critical alert do not disappear.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-10 10:28:23,6
13183813,Ability to install common ambari python libraries to maven repository (local / remote),"New cli script to create a maven tar.gz artifact:
 - `--clean` option to just delete dist directories
 - by default it uses `mvn install` on the generated tar.gz, that install the artifact into local maven repository
 - `--deploy` option to use `mvn deploy` for uploading generated tar.gz artifact to remote maven repository
 - `--version` option to provide the artifact and generated python dist version
 - update setup.py script to read the version from pom.xml during dist building, otherwise it uses PKG-INFO to get the right version, therefore if you are overriding the version during the build, it will always use the right version with pip or setup.py

How it works:
 with setup.py an installable distribution created, with pip the packages are installed locally, then the install-ambari-python script creates a tar.gz from the installd packages and upload it to a maven repo (tar.gz content is the site-packages, the python version before that is not included)

after the artifacts are installed to maven repos, these could be provided in external projects (to use to extend PYTHONPATH variable, in order to run tests against any stack code, like in MPacks);
 Maven example:
{code:xml}
 <dependency>
 <groupId>org.apache.ambari</groupId>
 <artifactId>ambari-python</artifactId>
 <version>2.0.0.0-SNAPSHOT</version>
 <scope>test</scop>
 </dependency>
{code}
 Gradle example:
{code:groovy}
 testCompile ""org.apache.ambari:ambari-python:2.0.0.0-SNAPSHOT@tar.gz""
{code}

So in an external project, the ambari python files can be extracted during build time, and add them to the PYTHONPATH

Usage examples:
 Install to maven local repo
{code:bash}
 ./install-ambari-python.sh
{code}

Deploy to maven remote repo
{code:bash}
 ./install-ambari-python.sh --deploy -i <repoId> -r <repoUrl>
{code}",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2018-09-08 12:20:20,29
13183691,Package install should handle multiple deployed mpacks,Installation of packages should cross mpacks. This changes from previous due to installation out of several repo sets. Previous ambari was one single repo set per install.,pull-request-available,['ambari-admin'],AMBARI,Task,Critical,2018-09-07 17:41:07,33
13183651,ambari-server exception if any user with cluster operator role is trying to edit the widget.,"*STR*
1. login using widgetuser(username),widgetpass(password) the role is cluster operator
2. go to hdfs-> metrics tab
3. EDIT shared the ""Under Replicated Blocks"" widgets. 
4. the error can be reproduced

*Actual Result:*

!errorWhileEditingWidgetsAsClusterOperator.png!

*Expected Result:*

According to our Role Chart a user with Cluster Operator role should be able to create/edit widgets

!RoleChart.png!

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-07 14:39:28,27
13183643,"Stack advisor error popup with ""500 status code"" thrown during customize service page of install wizard","ams-hbase hbase.rootdir property being set to ""file://"" leads to 500 server error",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-07 14:04:24,28
13183469,Hard to scroll components list in Host details -> Logs page when there are > 15 components in host,"When there are lot of components installed in a host, it is hard to scroll list of all components in Host Details -> Logs Page.

Fix: to add a scroller.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-06 18:01:22,32
13183371,Post user creation script sets invalid owner to HDFS dirs with secure cluster,During LDAP user sync the post-user-creation-hook.sh is called which creates directory for the given user in hdfs://user/username. The owner of this directory is set to the username (shortname) which does not work for some HDFS implementations. To fix this the script now sets the full username (including the realm) as the owner of the dir. ,pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-09-06 10:49:46,10
13183370,Broken markup for alert on Installer Step7,Install Wizard -> Step 7 shows alert popup on the top of the page when user creates config group and switches to it. Looks like it has broken markup or styles.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-06 10:45:30,9
13183360,Popup shown when Ambari started on a different port other than 8080,"Change the Ambari server port by setting the following 
{code:title=ambari.properties}
client.api.port=8000
{code}

Ab error popup is observed.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-06 10:04:45,9
13183275,Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'onefs' ,"Below error message pop-up when make and save any configuration changes to the service running on HDP301 cluster.

 

Error

500 status code received on POST method for API: /api/v1/stacks/HDP/versions/3.0/validations 
 

*Error message:* Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'onefs'

StdOut file: /var/run/ambari-server/stack-recommendations/20/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/20/stackadvisor.err

 

!image-2018-09-06-09-12-32-080.png!

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-06 01:23:16,18
13183182,PrincipalKeyCredential.equals fails for subtypes,"
Fix org.apache.ambari.server.security.credential.PrincipalKeyCredential#equals to allow for subtypes.

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-05 16:30:44,13
13183089,Download client config fails if user running Ambari server has UID>2097151	,"When ambari is running with a user that has UID/GID >2097151 download client configs fails with the following error:

{code}
{ 
""status"" : 500, 
""message"" : ""org.apache.ambari.server.controller.spi.SystemException: group id '19600006' is too big ( > 2097151 )"" 
} 
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-05 10:34:01,18
13182811,"Blueprint installation stuck (cannot resolve cluster-env), continues after ambari-server restart",The issue occurs with exported blueprints. These blueprints contain the stack defaults. When a new cluster is installed from such blueprint the *BlueprintConfigurationProcessor* falsely thinks it updates _cluster-env_ when setting the stack defaults.,pull-request-available,[],AMBARI,Bug,Critical,2018-09-04 13:28:57,20
13182693,"UI Hangs on Deploy wizard without any visible error, if stack metadata have error and no available stacks returned","Ambari should provide to user information, that some stacks are not okay and if no available stacks present - disallow possibility to deploy cluster at all.",pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-09-03 22:14:12,7
13182692,HDI Livy2 fails to restart,"Livy2 restart fails from Ambari due to Ambari could not fetch some Hadoop
configs?

StdErr:
{code}    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 148, in <module>
        LivyServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 351, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 62, in start
        self.configure(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 52, in configure
        setup_livy(env, 'server', upgrade_type=upgrade_type, action = 'config')
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/setup_livy2.py"", line 53, in setup_livy
        params.HdfsResource(None, action=""execute"")
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 681, in action_execute
        self.get_hdfs_resource_executor().action_execute(self)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 164, in action_execute
        logoutput=logoutput,
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
        returns=self.resource.returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hadoop --config /usr/hdp/3.0.1.0-175/hadoop/conf jar /var/lib/ambari-agent/lib/fast-hdfs-resource.jar /var/lib/ambari-agent/tmp/hdfs_resources_1535895647.58.json' returned 1. Initializing filesystem uri: hdfs://mycluster
    Creating: Resource [source=null, target=wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, type=directory, action=create, owner=livy, group=null, mode=700, recursiveChown=false, recursiveChmod=false, changePermissionforParents=false, manageIfExists=true] in hdfs://mycluster
    Exception occurred, Reason: Wrong FS: wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, expected: hdfs://mycluster
    java.lang.IllegalArgumentException: Wrong FS: wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, expected: hdfs://mycluster
    	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:781)
    	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:240)
    	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1583)
    	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1580)
    	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1595)
    	at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1768)
    	at org.apache.ambari.fast_hdfs_resource.Resource.checkResourceParameters(Resource.java:193)
    	at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:112)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
    	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
    
{code}

Please note that the system test sets the

property | from | to  
---|---|---  
`livy.server.recovery.state-store` | `zookeeper` | `filesystem`  
`livy.server.recovery.state-store.url` |
`zk1-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181,zk2-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181,zk5-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181`
| `wasb://spark2l-at30wu-
livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-
69e01f25-3b7b-4af4-a787-37664ab45f0c`  
  
and then the restart fails.

",pull-request-available,[],AMBARI,Bug,Major,2018-09-03 21:49:19,5
13182674,User get stuck between steps if API returns empty version_definitions array,In Install Wizard when user goes to step1 API request to load version definitions is sent. Sometimes this request returns empty array and UI is not showing any error and just shows empty page.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-03 16:47:29,9
13182670,'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' is not equal to the value of yarn_hierarchy in UI Deploy,"The property 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' does not show up in the UI install wizard even when {{yarn_cgroups_enabled}} and {{gpu_module_enabled}} are enabled in YARN configs. But this property is available after deployment of the cluster.  

 !Screen Shot 2018-09-02 at 10.20.43 PM.png|thumbnail! 
 !Screen Shot 2018-09-02 at 10.22.33 PM.png|thumbnail! 
When trying to set a value for the config 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy', Stack advisor displays the following:
{code}
yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value
yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value
Name of the Cgroups hierarchy under which all YARN jobs will be launched
{code}
 !Screen Shot 2018-09-02 at 10.25.08 PM.png|thumbnail! 
From the stack advisor description, looks like 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' property needs to be same as yarn_hierarchy. As part of UI Deploy, the property {{yarn_hierarchy}} is set to {{hadoop-yarn-tmp-ctr-e138-1518143905142-461959-01-000002.hwx.site}}. But after deploy, the {{yarn.nodemanager.linux-container-executor.cgroups.hierarchy}} is set to {{/yarn}} and not the property set while UI Deploy. This is causing Deploy failures due to Nodemanagers not getting started. When this property is set correctly, Nodemanagers start successfully. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-09-03 16:19:01,19
13182400,Ambari Alert - HiveServer2 Process - False negative in Certain Scenarios,"The Hive Alert & Hive Service Check both share the same logic from {{hive_check}} which attempts to execute something similar to:

{code}
beeline -u '%s' %s -e ';' 2>&1| awk '{print}' | grep -i -e 'Connection refused' -e 'Invalid URL'""
{code}

This type of negative condition checking misses a lot of failures, such as permission problems and SSL/JKS problems. Instead, we should change this to a positive check.

- Add the {{-n}} option to specify the hive user since the hive warehouse inode is protected. This option is ignored for Kerberos
- Change the grep to look for positive conditions
-- {code}
Connecting to jdbc:hive2://c7403.ambari.apache.org:10000/;transportMode=binary
Connected to: Apache Hive (version 3.0.0.3.0.0.0-1553)
Driver: Hive JDBC (version 3.0.0.3.0.0.0-1553)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.0.0.3.0.0.0-1553 by Apache Hive
Closing: 0: jdbc:hive2://c7403.ambari.apache.org:10000/;transportMode=binary
{code}",pull-request-available,[],AMBARI,Bug,Critical,2018-08-31 17:50:49,31
13182360,[Log Search UI] Change the fix width and the height for the modal to flexible layout,The modals have fixed with and height and it can make the app difficult to use. So the modals should be flexible enough to be visible everytime.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Major,2018-08-31 13:31:20,26
13182337,Quicklinks URL overflow outside the UI box,"Quicklinks in HDI environment are overflowing in UI. See attached.

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-31 10:59:36,19
13182252,"Services should display the ""Stop"" button if any of their components are started","In Ambari 2.7, the ""Stop"" button for each service is only available if all its components are started. This means that if any component is not started (or down), the rest of the components must be stopped individually from every host.

[!5516EB4E-F2A5-44DA-8A4A-453C40FDA0E4.png?default=false|thumbnail!|https://hortonworks.jira.com/secure/attachment/162070/162070_5516EB4E-F2A5-44DA-8A4A-453C40FDA0E4.png] 
For services where one component cannot be started (for example, due to memory limitations), stopping the service can be a very painful exercise. Please enable the ""stop"" button to be triggered, if any component of a service can be stopped.

Similarly for the Start button",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-30 22:21:35,34
13182161,Ambari Metrics should handle a customized Zookeeper service principal name,"AMS should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be zookeeper/_HOST as opposed to something like zookeeper-mycluster/_HOST.",pull-request-available,['stacks'],AMBARI,Bug,Major,2018-08-30 15:43:00,18
13182156,User get stuck between steps if API returns empty version_definitions array,In Install Wizard when user goes to step1 API request to load version definitions is sent. Sometimes this request returns empty array and UI is not showing any error and just shows empty page.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-30 15:27:32,9
13182145,Clarify the warning message during ambari server setup,"Change
WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql
so that it's clear that the dll script should be run from the DB console.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-30 14:37:24,28
13182137,Cluster name is not aligned with background ops ,"When the cluster name is long, the cluster name overflows to the next line, even though there is a lot of gap to accomodate the cluster name.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-30 14:10:47,32
13182116,Atlas should handle a customized Zookeeper service principal name,"Atlas should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be zookeeper/_HOST as opposed to something like zookeeper-mycluster/_HOST.
",pull-request-available,['stacks'],AMBARI,Bug,Major,2018-08-30 12:53:08,18
13182104,Cannot deploy Hive Metastore with Kerberos without HDFS,"In order to enable Kerberos for Hive MetaStore we need a property in {{core-site}}, which is ignored by Ambari if HDFS is not present.  Hive Metastore start fails due to empty {{core-site}} with:

{noformat}
KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL for /hive/cluster/delegationMETASTORE/keys
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-30 11:37:25,21
13182021,Commenting call to checkAndAddExternalCollectorHosts() function to avoid POST HOST API call failure.,"While installing cluster, POST HOST API calls fails as follows:

 

HOST POST API CALL that fails while deploying cluster:

{code:title=POST http://IP:8080/api/v1/clusters/c1/hosts}
Body :
{""RequestInfo"":\{""query"":""Hosts/host_name.in(host-1.openstacklocal)""},""Body"":\{""host_components"":[{""HostRoles"":{""component_name"":""ZOOKEEPER_SERVER"",""service_name"":""ZOOKEEPER"",""service_group_name"":""HDPCORE""}}]}}:
{code}

 
{code:java}
java.lang.NullPointerException
 at org.apache.ambari.server.controller.internal.ConfigurationResourceProvider.getResourcesAuthorized(ConfigurationResourceProvider.java:258)
 at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.getResources(AbstractAuthorizedResourceProvider.java:277)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.getConfigProperties(AbstractProviderModule.java:974)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.getDesiredConfigMap(AbstractProviderModule.java:999)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.getDesiredConfigMap(AbstractProviderModule.java:990)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.checkAndAddExternalCollectorHosts(AbstractProviderModule.java:386)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.initProviderMaps(AbstractProviderModule.java:919)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.checkInit(AbstractProviderModule.java:857)
 at org.apache.ambari.server.controller.internal.AbstractProviderModule.getMetricsServiceType(AbstractProviderModule.java:348)
 at org.apache.ambari.server.controller.metrics.MetricsPropertyProviderProxy.checkPropertyIds(MetricsPropertyProviderProxy.java:97)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.providesRequestProperties(ClusterControllerImpl.java:554)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.populateResources(ClusterControllerImpl.java:152)
 at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:424)
 at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:222)
 at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:77)
 at org.apache.ambari.server.api.handlers.QueryCreateHandler.handleRequest(QueryCreateHandler.java:58)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:162)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
 at org.apache.ambari.server.api.services.HostService.createHosts(HostService.java:172)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:291)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:94)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:66)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
 at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:674)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:221)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:210)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:531)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
 at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)
 at java.lang.Thread.run(Thread.java:745)

{code}
 

 

*Reason:* 

checkAndAddExternalCollectorHosts() fn. looks for cluster-env. cluster-env is supposed to retire and clusterSettings should be used.
 * Thus, this code needs transitioning in branch *branch-feature-AMBARI-14714* to refer clusterSettings now. 
 * AMBARI-24568 tracks the actual fix.

 

*Fix in this JIRA:* Commented the call to checkAndAddExternalCollectorHosts()

 ",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-08-30 01:08:36,43
13182017,Ambari-server cannot start,StackManager.init() throw NPE,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-29 23:59:10,44
13181978,HDFS should handle a customized Zookeeper service principal name,"
HDFS should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
* Add {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} to {{HADOOP_OPTS}}

",pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-08-29 19:36:47,13
13181974,Zookeeper should handle a customized Zookeeper service principal name,"
Zookeeper should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

Note: The Zookeeper service check fails.

",pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-08-29 19:22:35,13
13181969,global name 'VERIFY_DEPENDENCY_CMD' is not defined',"STR:-
1. deploy the cluster with HDP-3.0.1.0-164 + ambari_version : 2.7.1.0-147
2. added a patch targetVDF-29-08-2018-00-53-55.xml 
3. install version

{code}
2018-08-29 16:47:27,225 - Package Manager failed to install packages: Failed to execute command '/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install -o Dir::Etc::SourceList=/dev/null -o Dir::Etc::SourceParts= hdp-select', exited with code '100', message: 'E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. 
'
2018-08-29 16:47:27,250 - Could not install packages. Error: global name 'VERIFY_DEPENDENCY_CMD' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 136, in actionexecute
    ret_code = self.install_packages(package_list)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 447, in install_packages
    if not self.repo_mgr.verify_dependencies():
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/apt_manager.py"", line 218, in verify_dependencies
    err_msg = Logger.filter_text(""Failed to verify package dependencies. Execution of '%s' returned %s. %s"" % (VERIFY_DEPENDENCY_CMD, code, out))
NameError: global name 'VERIFY_DEPENDENCY_CMD' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 136, in actionexecute
    ret_code = self.install_packages(package_list)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 447, in install_packages
    if not self.repo_mgr.verify_dependencies():
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/apt_manager.py"", line 218, in verify_dependencies
    err_msg = Logger.filter_text(""Failed to verify package dependencies. Execution of '%s' returned %s. %s"" % (VERIFY_DEPENDENCY_CMD, code, out))
NameError: global name 'VERIFY_DEPENDENCY_CMD' is not defined

The above exception was the cause of the following exception:

Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 486, in <module>
    InstallPackages().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 351, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 149, in actionexecute
    raise Fail(""Failed to distribute repositories/install packages"")
resource_management.core.exceptions.Fail: Failed to distribute repositories/install packages
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-08-29 18:29:45,18
13181955,Protect the ClusterConfig resource so that only authorized users may have read-only access the data,"Protect the ClientConfig resource so that only authorized users may have read-only access the data.

Users with the following permission should have read-only access:
* {{CLUSTER.VIEW_CONFIGS}}
* {{SERVICE.VIEW_CONFIGS}}
* {{HOST.VIEW_CONFIGS}}

These permissions should be allow for the following roles:
* {{AMBARI.ADMINISTRATOR}}
* {{CLUSTER.ADMINISTRATOR}}
* {{CLUSTER.OPERATOR}}
* {{SERVICE.ADMINISTRATOR}}
* {{SERVICE.OPERATOR}}
* {{CLUSTER.USER}}

Users with no role related to the cluster may not view the data.

Example REST API entry point:
{noformat}
GET /api/v1/clusters/cl1/services/HDFS/components/HDFS_CLIENT?format=client_config_tar
{noformat}
",pull-request-available rbac,['ambari-server'],AMBARI,Bug,Major,2018-08-29 17:27:30,30
13181905,Update styles for pre-upgrade modals,"- Move 'Preparing the Upgrade' text from the corresponding modal to its header
- Upgrade Options popup: add some vertical padding between peragraphs
- Upgrade Options popup: center the contents of upgrade type blocks and add more padding to them
- Make pre-upgrade checks popup wider
- Pre-upgrade checks popup: display warnings as regular text instead of well blocks",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-08-29 12:31:05,15
13181875,Better debugging for agent start failure due to stuck fuser call,"While debugging with we encountered an issue where call to `fuser 8670/tcp` it got blocked for minutes
- in fact we did not ever see the agent start and had to comment the call. Two
improvements:

  * Need timeout for such calls (lets also research and see why the call may get stuck). We checked that the port was free.
  * Need more debug logging in the initial start up path to help debug what command is getting stuck - in main.py, and PingPortListener.py lets see if more debug logging can be added


",pull-request-available,[],AMBARI,Bug,Major,2018-08-29 10:38:52,5
13181872,"Diff in Downloaded client config: Host file has Stack info where as downloaded file has 'None' in ""user.agent.prefix"" properties","  * Download client config for HDFS from UI
  * Compare the contents of core-site.xml between downloaded config and the one in /etc/hadoop/conf/core-site.xml

Following properties have a difference:

  * fs.azure.user.agent.prefix
  * fs.s3a.user.agent.prefix

Both of them have value **User-Agent: APN/1.0 Hortonworks/1.0 HDP/None** in
downloaded file where as in host config file it is **User-Agent: APN/1.0
Hortonworks/1.0 HDP/3.0.0.0-1453**

Could you please check

",pull-request-available,[],AMBARI,Bug,Major,2018-08-29 10:32:44,5
13181626,Nifi Registry install fails,"Facing issue installing Nifi Registry on HDP_HDF cluster. The create keytab step in Ambari is failing during installation. Below exception is seen in ambari logs.
 
{code}
2018-08-21 13:11:03,401 ERROR [Server Action Executor Worker 1305] CreatePrincipalsServerAction:309 - Failed to create principal,  - Failed to create new principal - no principal specified
org.apache.ambari.server.serveraction.kerberos.KerberosOperationException: Failed to create new principal - no principal specified
        at org.apache.ambari.server.serveraction.kerberos.MITKerberosOperationHandler.createPrincipal(MITKerberosOperationHandler.java:159)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.createPrincipal(CreatePrincipalsServerAction.java:268)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.processIdentity(CreatePrincipalsServerAction.java:157)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:460)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.execute(CreatePrincipalsServerAction.java:92)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:748)
2018-08-21 13:11:03,401  INFO [Server Action Executor Worker 1305] KerberosServerAction:481 - Processing identities completed.
2018-08-21 13:11:04,191 ERROR [ambari-action-scheduler] ActionScheduler:482 - Operation completely failed, aborting request id: 117
 {code}

The Ambari UI should not display any properties from Kerberos identity blocks that indicate they are referencing another Kerberos identity. There are 2 ways we know this:

- The new/preferred way: the identity block has a non-empty/non-null ""reference"" attribute
- The old (backwards compatible way): the identity block has a ""name"" attribute the starts with a '/'.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-28 14:44:35,15
13181623,UX issues with yarn containers widget ,"# The n/a content in yarn container widget seems to be bold as compared to other widgets where it is faded out.
# no padding among three n/a makes it look a little unintuitive  
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-28 14:34:15,9
13181605,Cannot start Hive Metastore without HDFS,"Starting Hive Metastore fails if HDFS is not present in the cluster with the error: {{JAVA_HOME is not set and could not be found.}}

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 211, in <module>
    HiveMetastore().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 61, in start
    create_metastore_schema() # execute without config lock
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 374, in create_metastore_schema
    user = params.hive_user
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-server2-hive2/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED] -verbose' returned 1. Error: JAVA_HOME is not set and could not be found.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-28 13:31:40,21
13181593,Storm service-check fails due to missung StringUtils class definition,"{code:java}
2018-08-21 21:46:35.810 o.a.s.util Thread-24-__metricsorg.apache.hadoop.metrics2.sink.storm.StormTimelineMetricsSink-executor[4 4] [ERROR] Async loop died!
java.lang.NoClassDefFoundError: org/apache/commons/lang/StringUtils
        at org.apache.hadoop.metrics2.sink.storm.StormTimelineMetricsSink.prepare(StormTimelineMetricsSink.java:133) ~[ambari-metrics-storm-sink-with-common-2.6.1.0.144.jar:?]
        at org.apache.storm.metric.MetricsConsumerBolt.prepare(MetricsConsumerBolt.java:75) ~[storm-core-1.1.0.2.6.6.0-26.jar:1.1.0.2.6.6.0-26]
        at org.apache.storm.daemon.executor$fn__10252$fn__10265.invoke(executor.clj:800) ~[storm-core-1.1.0.2.6.6.0-26.jar:1.1.0.2.6.6.0-26]
        at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:482) [storm-core-1.1.0.2.6.6.0-26.jar:1.1.0.2.6.6.0-26]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.ClassNotFoundException: org.apache.commons.lang.StringUtils
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_112]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_112]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_112]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_112]
        ... 6 more{code}
 ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-08-28 12:57:03,28
13181591,[Log Search UI] get rid of redundant requests after undoing or redoing several history steps,"After undoing or redoing more than one history items several redundant API requests are sent. This occurs because changes for several filter controls are applied step-by-step, and each control change generates new request.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-08-28 12:33:48,26
13181577,Yarn Timeline Service V2 Reader goes down after Ambari Upgrade from 2.7.0.0 to 2.7.1.0,"STR:

1) Install cluster with Ambari2.7.0.0 + HDP-3.0.0.0
2) Upgrade Ambari to 2.7.1.0

Yarn Timeline Service V2 Reader goes down after some time.

Reason: the placeholders in yarn.timeline-service.reader.webapp.address and yarn.timeline-service.reader.webapp.https.address are no longer replaced by the stack code so these values become empty. In this case the timeline reader will use the default ports which may conflict with   other ports used by other components.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-28 10:46:13,18
13181568,Move blueprint provisioning state property to host component level,Move blueprint provisioning state property from cluster level to host component level.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-28 09:46:08,6
13181544,Allow skipping Hive Metastore schema creation for sysprepped cluster,"Similar to AMBARI-24540, Hive Metastore DB schema may be manually pre-created to save time during initial service start. However, {{schematool}} could still take quite some time to confirm that the schema exists. The goal of this change is to allow users who pre-create Hive Metastore DB schema to make Ambari skip managing the DB (create or check existence).",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-08-28 08:32:17,21
13181498,A foreign key constraint fails when deleting a cluster from ambari,"when deleting a cluster which has been called some schedule requests, ambari server will reponse 500 , ambari-server.log has a exception like this:
{code:java}
org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Cannot delete or update a parent row: a foreign ke
y constraint fails (`aquila`.`request`, CONSTRAINT `FK_request_schedule_id` FOREIGN KEY (`request_schedule_id`) REFERENCES `requestschedule` (`schedule_id`))
Error Code: 1451
Call: DELETE FROM requestschedule WHERE (schedule_id = ?)
bind => [1 parameter bound]
{code}",ambari-server pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-28 03:03:14,1
13181385,Protect the Request resource so that only authorized users may have read-only access the data,"Protect the Request resource so that only authorized users may have read-only access the data.

Users with the following roles should have read-only access:
* {{AMBARI.ADMINISTRATOR}}
* {{CLUSTER.ADMINISTRATOR}}
* {{CLUSTER.OPERATOR}}
* {{SERVICE.ADMINISTRATOR}}
* {{SERVICE.OPERATOR}}
* {{CLUSTER.USER}}

Users with no role related to the cluster may not view the data.",pull-request-available rbac,['ambari-server'],AMBARI,Bug,Major,2018-08-27 14:36:04,27
13181286,Implement Notifications/Coasters from Fluid Design,See screenshot,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-27 10:37:50,19
13181244,Rename LDAP configuration ambari.ldap.advance.collision_behavior,In Ambari 2.7 we moved LDAP configuration into the Ambari DB and introduced common naming pattern. However a typo has been made in _'ambari.ldap.*advance*.collision_behavior_'. This should be renamed to _'ambari.ldap.*advanced*.collision_behavior_',pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-27 06:23:40,27
13181152,Allow skipping Oozie DB schema creation for sysprepped cluster,"Oozie DB schema may be manually pre-created to save time during initial service start.  However, {{ooziedb.sh}} could still take quite some time to confirm that the schema exists.  The goal of this change is to allow users who pre-create Oozie DB schema to make Ambari skip managing the DB (create or check existence).",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-08-25 20:09:31,21
13181092,OneFS mpack should not include webhdfs enable setting,"webhdfs.dfs.enabled is included in the configurations for OneFS management pack. That is not needed because OneFS 8.1.2.0 (used with Ambari 2.7 and the mpack) does not require the disablement of webhdfs in Ambari to support Ambari Views.

webhdfs.dfs.enabled property should be removed entirely from configurations/hdfs-site.xml ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-25 00:27:07,18
13180966,Use Mpack Instance Manager To Switch Component Instance Version on Upgrade,"Mpacks will not continue use the {{stack-select}} tools which have been used by various stacks over the past few years. Previously, each stack shipped a tool which would manipulate the symlinks on the file system in order to both report versions for components and to change versions during an upgrade.

The following pieces were required for this old framework:
- A file, such as {{stack_packages.json}} which provided a mapping between Ambari's component name and the symlink name on the file system.
- Each component's Python file would need code which flips the version before restarting during an upgrade:
{code}
  def pre_upgrade_restart(self, env, upgrade_type=None):
    if params.version and check_stack_feature(StackFeature.ROLLING_UPGRADE, format_stack_version(params.version)):
      stack_select.select_packages(params.version)
{code}

Mpacks v2 will instead use the mpack instance manager to change the pointer on the file system:
{noformat}
/usr/hwx/mpacks
└── hdpcore
    ├── 1.0.0-b575
    │   ├── zookeeper_client -> /usr/hwx/modules/zookeeper_clients/3.4.0.0-b42
    │   └── zookeeper_server -> /usr/hwx/modules/zookeeper/3.4.0.0-b42
    └── 1.0.0-b580
        ├── zookeeper_client -> /usr/hwx/modules/zookeeper_clients/3.4.0.0-b43
        └── zookeeper_server -> /usr/hwx/modules/zookeeper_clients/3.4.0.0-b43
{noformat}

{noformat}
/usr/hwx/instances
└── hdpcore
    └── HDPCORE
        └── default
            ├── zookeeper
            │   └── zookeeper_server
            │       └── ZK-1
            │           ├── current -> /usr/hwx/mpacks/hdpcore/1.0.0-b575/zookeeper_server
            └── zookeeper_client
                ├── current -> /usr/hwx/mpacks/hdpcore/1.0.0-b575/zookeeper_client
{noformat}

For each instance of a component, there is a {{current}} pointer which references the mpack (and indirectly) the module version. However, Ambari knows all of this information up front since it's all based on metadata contained with the {{mpack.json}}. Therefore, it is possible for Ambari to do all of this version linking inside of its own framework during the upgrade. The individual Python scripts will no longer have to perform these actions.",pull-request-available,[],AMBARI,Task,Blocker,2018-08-24 12:18:17,31
13180962,Ambari SPNEGO breaks SSO redirect,"When SPNEGO is enabled (`ambari-server setup-kerberos`), the SSO (`ambari-server setup-sso`) redirect no longer works.

How to reproduce:
# Enable SSO `ambari-server setup-sso`
# `ambari-server restart`
# Visit Ambari and notice that you are redirected to the SSO system (i.e. Knox)
# Enable SPNEGO `ambari-server setup-kerberos`
# `ambari-server restart`
# Visit Ambari and notice that you are *NOT redirected* to the SSO system (i.e. Knox)",kerberos pull-request-available security spnego sso,"['ambari-server', 'security']",AMBARI,Bug,Major,2018-08-24 12:03:46,30
13180959,File View not accessible in Ambari 2.7 after enabling 3 namenodes in HDP 3.0,"*Ambari-2.7.0* supports HDFS Federation architecture 

But if we configure 3 namenode the Files view never works.

It will fail with below exception


{code:java}
Service 'hdfs' check failed: HDFS080 webhdfs.ha.namenodes.list namenodes count is not exactly 2

Service 'hdfs' check failed: 
org.apache.ambari.view.utils.hdfs.HdfsApiException: HDFS080 webhdfs.ha.namenodes.list namenodes count is not exactly 2 
at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.copyHAProperties(ConfigurationBuilder.java:247) 
at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.parseProperties(ConfigurationBuilder.java:110) 
at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.buildConfig(ConfigurationBuilder.java:327) 
at org.apache.ambari.view.utils.hdfs.HdfsApi.<init>(HdfsApi.java:75) 
at org.apache.ambari.view.utils.hdfs.HdfsUtil.getHdfsApi(HdfsUtil.java:157) 
at org.apache.ambari.view.utils.hdfs.HdfsUtil.connectToHDFSApi(HdfsUtil.java:145) 
at org.apache.ambari.view.commons.hdfs.HdfsService.hdfsSmokeTest(HdfsService.java:145) 
at org.apache.ambari.view.filebrowser.HelpService.hdfsStatus(HelpService.java:95) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:498) 
at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) 
{code}


By inspecting the code , it looks like this is not supposed to work in filesview and it wil only allow two namenodes 

code : 
https://github.com/apache/ambari/blob/trunk/contrib/views/utils/src/main/java/org/apache/ambari/view/utils/hdfs/ConfigurationBuilder.java#L247",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-08-24 11:57:32,38
13180886,Ambari Server Ldap Sync Failed upon subject alternative DNS name check,"STR:
 1. Install Ambari
 2. Get certificate for secure LDAP (LDAPS) connection to your AD server.
 3. Generate ambari truststore with LDAPS certificate.
 4. Setup Ambari to use LDAPS with providing truststore.
{code:java}
2018-08-20 18:38:04,763 DEBUG com.hw.commonuifrm.impl.commands.CommandExecutorImpl.executeCommand(): Sending command [(echo ""admin"" ; echo ""admin"") | ambari-server sync-ldap --users /tmp/users.txt --groups /tmp/groups.txt]


2018-08-20 18:38:05,666 DEBUG com.hw.commonuifrm.impl.commands.ProcessDataImpl.buildOutputAndErrorStreamData(): /usr/lib64/python2.7/getpass.py:83: GetPassWarning: Can not control echo on the terminal.
  passwd = fallback_getpass(prompt, stream)
Warning: Password input may be echoed.
Enter Ambari Admin password:


2018-08-20 18:38:07,169 INFO com.hw.ambari.ui.util.cluster_managers.LDAPClusterManager.ambariServerSyncLDAPWithAD(): Result: Using python  /usr/bin/python
Syncing with LDAP...
Enter Ambari Admin login: 
Fetching LDAP configuration from DB.
Syncing specified users and groups...ERROR: Exiting with exit code 1. 
REASON: Caught exception running LDAP sync. ***.com:636; nested exception is javax.naming.CommunicationException: ***.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching ***.com found.]

2018-08-20 18:38:07,170 INFO com.hw.ambari.ui.tests.console.ldap.TestLDAPSOnAD.test010_AmbariSynchronizeWithADThroughLDAPS(): AMBARI LDAPS synchronization result: Using python  /usr/bin/python
Syncing with LDAP...
Enter Ambari Admin login: 
Fetching LDAP configuration from DB.
Syncing specified users and groups...ERROR: Exiting with exit code 1. 
REASON: Caught exception running LDAP sync. ***.com:636; nested exception is javax.naming.CommunicationException: ***.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching ***.com found.]{code}
The issue is that the AD server's certificate contains a section:
{noformat}
X509v3 Subject Alternative Name: othername:<unsupported>, DNS:***-2.COM{noformat}
As you can see this is not the same that we use to connect to the AD server (***.com:636). Even if this is a certificate issue the connection could be open and we should be able to sync LDAP users/groups.

*Important note*: it's reproducible only with OpenJDK (I used openjdk-1.8.0.181-3.b13.el7_5.x86_64); working properly with Oracle's JDK.

+*Recommended solution*+

We can disable endpoint identification when the client is negotiating with the server during SSL handshake by setting _com.sun.jndi.ldap.object.disableEndpointIdentification_ to _true_ (see [https://github.com/ojdkbuild/lookaside_java-1.8.0-openjdk/blob/master/jdk/src/share/classes/com/sun/jndi/ldap/Connection.java#L386]). By default this should not be the case but end users may set this up when configuring LDAP if they face this issue.

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-24 07:10:15,27
13180705,"Kerberos ""Additional Realms"" should not require keytab re-generation and cluster restart","""Admin -> Kerberos -> Additonal Realms""
* Currently requires keytab re-generation which in turn requires restarting the cluster. *But it is completely unrelated to keytabs*.

Fix:
* Move ""Additional Realms"" to the ""Kerberos"" service configs where it belongs, along with the ""auth_to_local"" setting which is what it is used for.
* When it is changed:
   ** No keytab re-generation is then required.
   ** Instead of silently altering ""auth_to_local"" rules, they should come up as ""Recommendations"".",auth_to_local kerberos,"['ambari-admin', 'security']",AMBARI,Bug,Major,2018-08-23 12:41:46,30
13180484,Accumulo does not startup in Federated Cluster,"In a manually setup federated cluster (not through deployNG) --

Accumulo was installed and when trying to start, below error thrown --

{noformat}
2018-08-16 07:33:31,748 [start.Main] ERROR: Thread 'org.apache.accumulo.master.state.SetGoalState' died.
java.lang.IllegalArgumentException: Expected fully qualified URI for instance.volumes got ns2/apps/accumulo/data
	at org.apache.accumulo.core.volume.VolumeConfiguration.getVolumeUris(VolumeConfiguration.java:107)
	at org.apache.accumulo.server.fs.VolumeManagerImpl.get(VolumeManagerImpl.java:334)
{noformat}

Caused by incorrect config value:
{{instance.volumes = hdfs://ns1,ns2/apps/accumulo/data}}
where ns1 and ns2 are namespaces

Expected is --
{{instance.volumes = hdfs://ns1/apps/accumulo/data,hdfs://ns2/apps/accumulo/data}}

according to --
https://accumulo.apache.org/docs/2.0/administration/multivolume
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-22 14:25:09,15
13180444,"Unable to validate password complexity for properties rangertagsync_user_password, rangerusersync_user_password","STR

- perform UI deploy with selecting Ranger & Ranger KMS

Used password: {{78C%4KwS5J$&}}   or any other, created by password generator with defined complexity 


Additionally, check on glitches picture 1 & 2. Additionally, I believe it is best to put special characters to grayed/dashed box to make user attention to them.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-22 10:46:06,19
13180300,Hive and Oozie JDBC url reset after set manually, JDBC urls of Hive and Oozie are reset even after being set manually when toggling RANGER plugins.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-21 20:42:45,34
13180268,Cannot connect to MIT KDC admin server when port is specified in kerberos-env/admin_server_host,"Cannot connect to MIT KDC admin server when port is specified in {{kerberos-env/admin_server_host}}.  The following error is seen when validating the KDC admin credentials:

{noformat}
kinit: Server not found in Kerberos database while getting initial credentials
{noformat}

The reason for this is due to how the credentials are created for accessing the MIT KDC administration server. 

{noformat}
kinit -c <path> -S kadmin/<kerberos-env/admin_server_host>  <principal>
{noformat}

If a port was added to the {{kerberos-env/admin_server_host}} value then the server principal will be generated like {{kadmin/kdc.example.com:4749}} rather than {{kadmin/kdc.example.com}}. Therefore the server principal is not found.
",kerberos pull-request-available regression,['ambari-server'],AMBARI,Bug,Critical,2018-08-21 18:14:21,30
13180258,CLONE - Requests STOMP topic sent updates for host check request,"Server should post only cluster related request updates to ""/events/requests"" STOMP topic.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-21 17:39:56,6
13180249,Fix set KDC admin credentials section in enable Kerberos doc,"Fix set KDC admin credentials section in enable Kerberos doc

The current documentation at https://github.com/apache/ambari/blob/branch-2.7/ambari-server/docs/security/kerberos/enabling_kerberos.md#set-the-kdc-administrator-credentials indicates an incorrect URL for the API call. 

It should read:

{noformat}
http://AMBARI_SERVER:8080/api/v1/clusters/CLUSTER_NAME/credentials/kdc.admin.credential
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-08-21 16:49:52,30
13180217,Requests STOMP topic sent updates for host check request,"Server should post only cluster related request updates to ""/events/requests"" STOMP topic.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-21 14:00:11,6
13180164,Default value for LDAP type,"1. Set default value for ldap type 'Generic'

2. command line option for disable asking ldap-type. Use Generic defaults for if set and properties are not exists and not given.  

3. Ask for ldap type only if any of the properties which default value is depending from ldap type is missing.

4. Ask for the user credentials and queries Ambari for the existing values first. Than offer these values as defaults.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-21 09:03:56,16
13180160,Remove dependency on JQuery 1.8.0 for Ambari Server UI,"Remove dependency on JQuery 1.8.0 for Ambari Server UI due to security concerns. See 
* CVE-2012-6708 - https://nvd.nist.gov/vuln/detail/CVE-2012-6708
* CVE-2011-4969 - https://nvd.nist.gov/vuln/detail/CVE-2011-4969
* CVE-2015-9251 - https://nvd.nist.gov/vuln/detail/CVE-2015-9251

It is recommended that JQuery is updated to 1.8.3+1

Path to offending file:
{noformat}
ambari
|- ambari-server-2.7.1.0-119.x86_64.rpm
|  |- usr
|  |  |- lib
|  |  |  |- ambari-server
|  |  |  |  |- web
|  |  |  |  |  |- api-docs
|  |  |  |  |  |  |- lib
|  |  |  |  |  |  |  |- jquery-1.8.0.min.js
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-21 08:54:57,19
13180142,NPE when migrating users table during upgrade to Ambari 2.7.0 with Oracle DB,"NPE when migrating users table during upgrade to Ambari 2.7.0 with Oracle DB:
{noformat}
2018-08-20 11:36:46,395 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed. 
java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.convertUserCreationTimeToLong(UpgradeCatalog270.java:595)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:342)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:318)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:970)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
2018-08-20 11:36:46,395 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.convertUserCreationTimeToLong(UpgradeCatalog270.java:595)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:342)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:318)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:970)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
	... 1 more
{noformat}
*Cause*
This is caused by one or more records with a {{NULL}} value in the {{create_time}} field.

For example:
||user_id||user_name||user_type||create_time||
|1|admin|LOCAL|NULL|

*Workaround* 
Update the relevant records to not have a {{NULL}} in the {{create_time}} column.

For example:
{noformat}
UPDATE users SET create_time=systimestamp WHERE create_time IS NULL;
{noformat}
*Solution*
During upgrade, protect against a {{null}} value for {{currentUserCreateTime.getValue()}} at:
{code:java|title=org/apache/ambari/server/upgrade/UpgradeCatalog270.java:595}
          dbAccessor.updateTable(USERS_TABLE, temporaryColumnName, currentUserCreateTime.getValue().getTime(),
              ""WHERE "" + USERS_USER_ID_COLUMN + ""="" + currentUserCreateTime.getKey());
{code}
If {{currentUserCreateTime.getValue()}} is null, the current timestamp should be used.

*Note:* This may be a reoccurring issue since there is no provision to ensure that {{create_time}} is not {{NULL}} when initializing the Ambari database:
{code:java}
insert into users(user_id, principal_id, user_name, user_password)
select 1,1,'admin','538916f8943ec225d97a9a86a2c6ec0818c1cd400e09e03b660fdaaec4af29ddbb6f2b1033b81b00' from dual;{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-21 07:38:53,27
13180121,Security vulnerabilities with Hive view (XSS),"It is possible for an attacker to steal information or access from users by executing malicious javascript. This is possible due to hive directly taking data/information from events and directly populating messages, this includes directly inserting data that contains html or javascript code. Leveraging this one user could create a malicious message to steal access or information of another user. Upon viewing the malicious message the vicitim would be comprimised by directly scraping any information on the page, modify its appearence, or having their session information stolen.

Bug reproduce steps:
1. go to Hive view from Ambari
2. click on 'Tables' and click on '+' to create a new table
3. In the table name input: '""<img src=x onerror=alert(document.domain)>""' and add a column with name <img src=x onerror=alert(document.domain)> and datatype TINYINT and click on create
4. There is a javascript popup showing the document name and domain name

",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-08-21 04:49:51,38
13180069,Remove dependency on org.bouncycastle bcprov-jdk15on before version 1.6.0 for Ambari Server,"Remove dependency on org.bouncycastle bcprov-jdk15on before version 1.6.0 for Ambari Server security concerns. See 
* CVE-2018-1000180 - https://nvd.nist.gov/vuln/detail/CVE-2018-1000180

This dependency is compiled into the apacheds-all.jar from 

{code}
    <dependency>
      <groupId>org.apache.directory.server</groupId>
      <artifactId>apacheds-all</artifactId>
      <version>2.0.0-M24</version>
    </dependency>
{code}

The relevant parts of this need to be broken out and the offending bouncy castle JAR needs to be excluded as needed. 
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-20 21:19:23,30
13180045,Upgrade: Infra Solr service is not renamed in Upgrade History table,"*STR*
# Deployed cluster with Ambari version: 2.5.2.0-298 and HDP version: 2.6.2.14-5
# Upgrade Ambari to 2.6.2.0, HDP to 2.6.5.0
# Upgrade Ambari to 2.7.0, HDP to 3.0.1.0
# Go to Upgrade history page

Upgrade history page shows empty (see attached for js error)

Reason: upgrade_history record is not upgarade during ambari 2.7.0 upgrade

*Workaround* (before 2.7.1):
{code:java}
UPDATE upgrade_history
SET service_name = 'AMBARI_INFRA_SOLR'
WHERE service_name = 'AMBARI_INFRA'
{code}
",pull-request-available,"['ambari-infra', 'ambari-server']",AMBARI,Bug,Major,2018-08-20 19:26:56,29
13179943,host component out of sync error message on version screen,"Below is the screenshot on versions screen of ambari where below error message is displayed.

""Host component out of sync HDP-2.2.9.0-3393
One of more host components did not report the version that Ambari expected. Please re-install the failed host component, or remove it.""

Please note these hosts have gone through an CentOS6 to CentOS7 OS upgrade and then ambari-server is upgraded to 2.7..1#73.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-20 11:36:09,19
13179772,Solr API reports backup as complete while copy is still in progress,"Below command completed successfully however the solr continues to copy the file async.

# /usr/lib/ambari-infra-solr-client/ambariSolrMigration.sh --ini-file $CONFIG_INI_LOCATION --mode backup | tee backup_output.txt

This can be an issue if we issue the followup delete command

{code}
root@hcube1-2n02 snapshot.ranger_audits_shard1_replica1]# du -h .
28G	.
[root@hcube1-2n02 snapshot.ranger_audits_shard1_replica1]# du -h .
34G	.
{code}

*backup_status.json*
{code}
{""responseHeader"":{""status"":0,""QTime"":1},""details"":{""indexSize"":""262.86 GB"",""indexPath"":""/grid/1/infra-solr/data/ranger_audits_shard1_replica1/data/index/"",""commits"":[[""indexVersion"",1515831777053,""generation"",961228,""filelist"",[""_10o64.cfe"",""_10o64.cfs"",""_10o64.si"",""_11tn0.cfe"",""_11tn0.cfs"",""_11tn0.si"",""_14pbq.cfe"",""_14pbq.cfs"",""_14pbq.si"",""_15jff.cfe"",""_15jff.cfs"",""_15jff.si"",""_174b1.cfe"",""_174b1.cfs"",""_174b1.si"",""_1934r.cfe"",""_1934r.cfs"",""_1934r.si"",""_19e6h.cfe"",""_19e6h.cfs"",""_19e6h.si"",""_1aytr.cfe"",""_1aytr.cfs"",""_1aytr.si"",""_1cz9h.cfe"",""_1cz9h.cfs"",""_1cz9h.si"",""_1dsmp.cfe"",""_1dsmp.cfs"",""_1dsmp.si"",""_1eknb.cfe"",""_1eknb.cfs"",""_1eknb.si"",""_1gqfs.cfe"",""_1gqfs.cfs"",""_1gqfs.si"",""_1jg42.cfe"",""_1jg42.cfs"",""_1jg42.si"",""_1jqqz.cfe"",""_1jqqz.cfs"",""_1jqqz.si"",""_1k5u8.cfe"",""_1k5u8.cfs"",""_1k5u8.si"",""_1k5ua.cfe"",""_1k5ua.cfs"",""_1k5ua.si"",""_1k5ug.cfe"",""_1k5ug.cfs"",""_1k5ug.si"",""_1laci.cfe"",""_1laci.cfs"",""_1laci.si"",""_1no19.cfe"",""_1no19.cfs"",""_1no19.si"",""_1osp3.cfe"",""_1osp3.cfs"",""_1osp3.si"",""_1osp3_6.liv"",""_1qbbd.cfe"",""_1qbbd.cfs"",""_1qbbd.si"",""_1qbbd_1.liv"",""_1s5jc.cfe"",""_1s5jc.cfs"",""_1s5jc.si"",""_1u9mv.cfe"",""_1u9mv.cfs"",""_1u9mv.si"",""_1u9mv_1.liv"",""_1ws6d.cfe"",""_1ws6d.cfs"",""_1ws6d.si"",""_1ws6d_3.liv"",""_1ys74.cfe"",""_1ys74.cfs"",""_1ys74.si"",""_20tad.cfe"",""_20tad.cfs"",""_20tad.si"",""_232t3.cfe"",""_232t3.cfs"",""_232t3.si"",""_24c9c.cfe"",""_24c9c.cfs"",""_24c9c.si"",""_26ro9.cfe"",""_26ro9.cfs"",""_26ro9.si"",""_28oz3.cfe"",""_28oz3.cfs"",""_28oz3.si"",""_2apem.cfe"",""_2apem.cfs"",""_2apem.si"",""_2d9sm.cfe"",""_2d9sm.cfs"",""_2d9sm.si"",""_2flec.cfe"",""_2flec.cfs"",""_2flec.si"",""_2i8xe.cfe"",""_2i8xe.cfs"",""_2i8xe.si"",""_2jl5e.cfe"",""_2jl5e.cfs"",""_2jl5e.si"",""_2m6gz.cfe"",""_2m6gz.cfs"",""_2m6gz.si"",""_2oj2h.cfe"",""_2oj2h.cfs"",""_2oj2h.si"",""_2s5lj.cfe"",""_2s5lj.cfs"",""_2s5lj.si"",""_2t4my.cfe"",""_2t4my.cfs"",""_2t4my.si"",""_2wghp.cfe"",""_2wghp.cfs"",""_2wghp.si"",""_2xlnj.cfe"",""_2xlnj.cfs"",""_2xlnj.si"",""_2zywz.cfe"",""_2zywz.cfs"",""_2zywz.si"",""_31s0c.cfe"",""_31s0c.cfs"",""_31s0c.si"",""_34a7e.cfe"",""_34a7e.cfs"",""_34a7e.si"",""_35o1p.cfe"",""_35o1p.cfs"",""_35o1p.si"",""_37x28.cfe"",""_37x28.cfs"",""_37x28.si"",""_39917.cfe"",""_39917.cfs"",""_39917.si"",""_3bavd.cfe"",""_3bavd.cfs"",""_3bavd.si"",""_3d4ac.cfe"",""_3d4ac.cfs"",""_3d4ac.si"",""_3esnp.cfe"",""_3esnp.cfs"",""_3esnp.si"",""_3gr4b.cfe"",""_3gr4b.cfs"",""_3gr4b.si"",""_3jecp.cfe"",""_3jecp.cfs"",""_3jecp.si"",""_3kh9c.cfe"",""_3kh9c.cfs"",""_3kh9c.si"",""_3mima.cfe"",""_3mima.cfs"",""_3mima.si"",""_3mjkp.cfe"",""_3mjkp.cfs"",""_3mjkp.si"",""_3o876.cfe"",""_3o876.cfs"",""_3o876.si"",""_3oh2x.cfe"",""_3oh2x.cfs"",""_3oh2x.si"",""_3q3ky.cfe"",""_3q3ky.cfs"",""_3q3ky.si"",""_3qjum.cfe"",""_3qjum.cfs"",""_3qjum.si"",""_3qpc2.cfe"",""_3qpc2.cfs"",""_3qpc2.si"",""_3r3qd.cfe"",""_3r3qd.cfs"",""_3r3qd.si"",""_3ray8.cfe"",""_3ray8.cfs"",""_3ray8.si"",""_3rcrk.cfe"",""_3rcrk.cfs"",""_3rcrk.si"",""_3rdbs.cfe"",""_3rdbs.cfs"",""_3rdbs.si"",""_3rg6k.cfe"",""_3rg6k.cfs"",""_3rg6k.si"",""_3riin.cfe"",""_3riin.cfs"",""_3riin.si"",""_3rjm5.cfe"",""_3rjm5.cfs"",""_3rjm5.si"",""_3rjv1.cfe"",""_3rjv1.cfs"",""_3rjv1.si"",""_3rkh7.cfe"",""_3rkh7.cfs"",""_3rkh7.si"",""_3rkpt.cfe"",""_3rkpt.cfs"",""_3rkpt.si"",""_3rkt3.cfe"",""_3rkt3.cfs"",""_3rkt3.si"",""_3rkwq.cfe"",""_3rkwq.cfs"",""_3rkwq.si"",""_3rl0u.cfe"",""_3rl0u.cfs"",""_3rl0u.si"",""_3rl57.cfe"",""_3rl57.cfs"",""_3rl57.si"",""_3rl6p.cfe"",""_3rl6p.cfs"",""_3rl6p.si"",""_3rl6z.cfe"",""_3rl6z.cfs"",""_3rl6z.si"",""_3rl75.cfe"",""_3rl75.cfs"",""_3rl75.si"",""_3rl7a.cfe"",""_3rl7a.cfs"",""_3rl7a.si"",""_obvr.cfe"",""_obvr.cfs"",""_obvr.si"",""_obvr_1.liv"",""_ptzx.cfe"",""_ptzx.cfs"",""_ptzx.si"",""_qtqk.cfe"",""_qtqk.cfs"",""_qtqk.si"",""_txqa.cfe"",""_txqa.cfs"",""_txqa.si"",""_txqa_2.liv"",""_v3zg.cfe"",""_v3zg.cfs"",""_v3zg.si"",""_xrjk.cfe"",""_xrjk.cfs"",""_xrjk.si"",""segments_klos""]]],""isMaster"":""true"",""isSlave"":""false"",""indexVersion"":1515831777053,""generation"":961228,""master"":{""replicateAfter"":[""commit""],""replicationEnabled"":""true""},""backup"":[""startTime"",""Sat Aug 18 12:33:17 UTC 2018"",""fileCount"",259,""status"",""success"",""snapshotCompletedAt"",""Sat Aug 18 16:04:40 UTC 2018"",""snapshotName"",""ranger_audits_shard1_replica1""]}}
{code}",pull-request-available,"['ambari-infra', 'ambari-server']",AMBARI,Bug,Major,2018-08-18 20:05:49,29
13179728,LLAP application on Yarn fails with CNF exception on sysprepped cluster.,"LLAP Yarn application failed to start due to missing azure class.

{code}
18/08/15 20:08:28 INFO service.ServiceMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 4 cluster_timestamp: 1534200616336 } attemptId: 3 } keyId: 1688030095)
18/08/15 20:08:28 INFO service.AbstractService: Service Service Master failed in state INITED
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2596)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:226)
	at org.apache.hadoop.yarn.service.utils.CoreFileSystem.<init>(CoreFileSystem.java:73)
	at org.apache.hadoop.yarn.service.utils.SliderFileSystem.<init>(SliderFileSystem.java:41)
	at org.apache.hadoop.yarn.service.ServiceMaster.serviceInit(ServiceMaster.java:100)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.service.ServiceMaster.main(ServiceMaster.java:338)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2500)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2594)
... 12 more

 18/08/15 20:08:28 INFO service.ServiceMaster: Stopping app master
18/08/15 20:08:28 ERROR service.ServiceMaster: Error starting service master
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem not found
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2596)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3320)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3352)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:226)
at org.apache.hadoop.yarn.service.utils.CoreFileSystem.<init>(CoreFileSystem.java:73)
at org.apache.hadoop.yarn.service.utils.SliderFileSystem.<init>(SliderFileSystem.java:41)
at org.apache.hadoop.yarn.service.ServiceMaster.serviceInit(ServiceMaster.java:100)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
at org.apache.hadoop.yarn.service.ServiceMaster.main(ServiceMaster.java:338)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem not found
at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2500)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2594)
... 12 more
{code}
",pull-request-available,[],AMBARI,Bug,Major,2018-08-18 05:39:37,39
13179647,Update UpgradeSummary to Include Mpack Information,"The {{upgradeSummary}} object being sent down during an upgrade needs to be updated to include the following fields:

- Mpack To/From Information
- Service Group To/From/Type Information
- Component Version Information

The proposed structure is as follows:
{code}
      ""upgradeSummary"": {
        ""serviceGroups"":{
          ""SG1"":{
            ""type"":""express_upgrade"",
            ""serviceGroupId"": 1,
            ""serviceGroupName"": ""SG1"",
            ""sourceMpackId"": 50,
            ""targetMpackId"": 100,
            ""sourceStack"": ""HDPCORE-1.0"",
            ""targetStack"": ""HDPCORE-1.5"",
            ""sourceMpackVersion"": ""1.0.0.0-b1"",
            ""targetMpackVersion"": ""1.5.0.0-b1"",
            ""services"":{
              ""HDFS"":{
                ""serviceName"": ""HDFS"",
                ""sourceVersion"":""3.0.0.0-b1"",
                ""targetVersion"":""3.1.0.0-b1"",
                ""components"": {
                  ""componentName"": ""NAMENODE"",
                  ""sourceVersion"": ""3.0.0.0-b1"",
                  ""targetVersion"":""3.1.0.0-b1"",
                }
              }
            }
          }
        }
{code}",pull-request-available,[],AMBARI,Task,Critical,2018-08-17 17:57:11,31
13179642,Stack and Versions page style fixes,"- Change text in 'Manage versions' modal
- Fix alignment issues on Versions tab
- Wrap services column on Versions tab into white block
- Remove redundant horizontal space in services column
- Make shadow of version column softer",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-17 17:36:26,15
13179641,Ambari Blueprint deployments fail with multiple TIMELINE_READER instances,"The BlueprintConfigurationProcessor does not current handle the case of multiple Yarn TIMELINE_READER instances properly.  

Generally, only a singleton instance of the TIMELINE_READER instance is deployed, but there might be scenarios in which customers will choose to deploy more than one instance, with just one instance running at a given time.  

Currently, deploying a Blueprint with multiple TIMELINE_READER instances fails with the following exception: 
{code:java}
java.lang.IllegalArgumentException: Unable to update configuration property 'yarn.timeline-service.reader.webapp.address' with topology information. Component 'TIMELINE_READER' is mapped to an invalid number of hosts '2'.
at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor$SingleHostTopologyUpdater.updateForClusterCreate(BlueprintConfigurationProcessor.java:1945)
at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.updateValue(BlueprintConfigurationProcessor.java:739)
at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doGeneralPropertyUpdatesForClusterCreate(BlueprintConfigurationProcessor.java:641)
at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doUpdateForClusterCreate(BlueprintConfigurationProcessor.java:443)
at org.apache.ambari.server.topology.ClusterConfigurationRequest.process(ClusterConfigurationRequest.java:152)
at org.apache.ambari.server.topology.tasks.ConfigureClusterTask.call(ConfigureClusterTask.java:80)
at org.apache.ambari.server.security.authorization.internal.InternalAuthenticationInterceptor.invoke(InternalAuthenticationInterceptor.java:45)
at org.apache.ambari.server.topology.tasks.ConfigureClusterTask.call(ConfigureClusterTask.java:45)
at org.apache.ambari.server.security.authorization.internal.InternalAuthenticationInterceptor.invoke(InternalAuthenticationInterceptor.java:45)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745){code}

The BlueprintConfiguration processor should be updated to allow for more than one TIMELINE_READER instance to be deployed.  This assumes that the Blueprint developer will set any Yarn properties required that point towards the TIMELINE_READER instance, including the usage of HOSTGROUP properties.  ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-17 17:32:19,20
13179544,Ambari updates lates_url info for HDF stacks with HDP link,Ambari updates the repoinfo.xml files in each stack and changes the latest value to the json.url which is found in the repo file (for example /etc/yum.d/ambari.repo). In a non-HDP stack this causes the wrong url to be added to repoinfo.xml,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-17 09:19:20,18
13179508,ambari-server setup fails with postgresql >= 9.3 ,"ambari-server setup with default postgres has been failing with

    
    
    OSError: [Errno 2] No such file or directory: '/usr/bin/postgresql-setup'
    

Looks like postgres packages/dependencies are not installed along with
installing ambari server install.  
below is output from ambari-server install from ambari-2.7.1 cluster

    
    
    2018-08-07 11:17:56,106|executor.py.167|INFO|14260|MainThread|172.27.25.203|executing the command='yum -y install ambari-server'
    2018-08-07 11:17:56,432|executor.py.167|INFO|14260|Thread-191|stdout: Loaded plugins: ovl, priorities
    2018-08-07 11:17:57,971|executor.py.167|INFO|14260|Thread-191|stdout: 12711 packages excluded due to repository priority protections
    2018-08-07 11:17:58,754|executor.py.167|INFO|14260|Thread-191|stdout: Resolving Dependencies
    2018-08-07 11:17:58,754|executor.py.167|INFO|14260|Thread-191|stdout: --> Running transaction check
    2018-08-07 11:17:58,755|executor.py.167|INFO|14260|Thread-191|stdout: ---> Package ambari-server.x86_64 0:2.7.1.0-63 will be installed
    2018-08-07 11:17:58,974|executor.py.167|INFO|14260|Thread-191|stdout: --> Finished Dependency Resolution
    2018-08-07 11:17:59,146|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,146|executor.py.167|INFO|14260|Thread-191|stdout: Dependencies Resolved
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout:  Package            Arch        Version            Repository              Size
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: Installing:
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout:  ambari-server      x86_64      2.7.1.0-63         ambari-2.7.1.0-63      366 M
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: Transaction Summary
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: Install  1 Package
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: Total download size: 366 M
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: Installed size: 434 M
    2018-08-07 11:17:59,152|executor.py.167|INFO|14260|Thread-191|stdout: Downloading packages:
    2018-08-07 11:18:03,735|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction check
    2018-08-07 11:18:03,736|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction test
    2018-08-07 11:18:04,048|executor.py.167|INFO|14260|Thread-191|stdout: Transaction test succeeded
    2018-08-07 11:18:04,049|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction
    2018-08-07 11:18:21,843|executor.py.167|INFO|14260|Thread-191|stdout:   Installing : ambari-server-2.7.1.0-63.x86_64                              1/1
    2018-08-07 11:18:22,568|executor.py.167|INFO|14260|Thread-191|stdout:  
    2018-08-07 11:18:22,569|executor.py.167|INFO|14260|Thread-191|stdout:   Verifying  : ambari-server-2.7.1.0-63.x86_64                              1/1
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout:  
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout: Installed:
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout:   ambari-server.x86_64 0:2.7.1.0-63                                             
    2018-08-07 11:18:22,677|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:18:22,677|executor.py.167|INFO|14260|Thread-191|stdout: Complete!
    

where as on an ambari-2.7.0 cluster we do have postgres server packages and
lib and is being installed with ambari-server install

    
    
    [root@c7401 ~]# yum install -y ambari-server
    Loaded plugins: fastestmirror
    ambari-2.7.0.0                                                                                                                                             | 2.9 kB  00:00:00     
    ambari-2.7.0.0/primary_db                                                                                                                                  |  25 kB  00:00:00     
    Loading mirror speeds from cached hostfile
     * base: repos.lax.quadranet.com
     * extras: linux.mirrors.es.net
     * updates: mirror.keystealth.org
    Resolving Dependencies
    --> Running transaction check
    ---> Package ambari-server.x86_64 0:2.7.0.0-897 will be installed
    --> Processing Dependency: postgresql-server >= 8.1 for package: ambari-server-2.7.0.0-897.x86_64
    --> Running transaction check
    ---> Package postgresql-server.x86_64 0:9.2.23-3.el7_4 will be installed
    --> Processing Dependency: postgresql-libs(x86-64) = 9.2.23-3.el7_4 for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Processing Dependency: postgresql(x86-64) = 9.2.23-3.el7_4 for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Processing Dependency: libpq.so.5()(64bit) for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Running transaction check
    ---> Package postgresql.x86_64 0:9.2.23-3.el7_4 will be installed
    ---> Package postgresql-libs.x86_64 0:9.2.23-3.el7_4 will be installed
    --> Finished Dependency Resolution
    
    Dependencies Resolved
    
    ==================================================================================================================================================================================
     Package                                        Arch                                Version                                     Repository                                   Size
    ==================================================================================================================================================================================
    Installing:
     ambari-server                                  x86_64                              2.7.0.0-897                                 ambari-2.7.0.0                              367 M
    Installing for dependencies:
     postgresql                                     x86_64                              9.2.23-3.el7_4                              base                                        3.0 M
     postgresql-libs                                x86_64                              9.2.23-3.el7_4                              base                                        234 k
     postgresql-server                              x86_64                              9.2.23-3.el7_4                              base                                        3.8 M
    
    Transaction Summary
    ==================================================================================================================================================================================
    Install  1 Package (+3 Dependent packages)
    
    Total download size: 374 M
    Installed size: 468 M
    Downloading packages:
    (1/4): postgresql-libs-9.2.23-3.el7_4.x86_64.rpm                                                                                                           | 234 kB  00:00:00     
    (2/4): postgresql-server-9.2.23-3.el7_4.x86_64.rpm                                                                                                         | 3.8 MB  00:00:00     
    (3/4): postgresql-9.2.23-3.el7_4.x86_64.rpm                                                                                                                | 3.0 MB  00:00:04     
    warning: /var/cache/yum/x86_64/7/ambari-2.7.0.0/packages/ambari-server-2.7.0.0-897.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 07513cad: NOKEY17 MB/s | 371 MB  00:00:00 ETA 
    Public key for ambari-server-2.7.0.0-897.x86_64.rpm is not installed
    (4/4): ambari-server-2.7.0.0-897.x86_64.rpm                                                                                                                | 367 MB  00:00:23     
    ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    Total                                                                                                                                              16 MB/s | 374 MB  00:00:23     
    Retrieving key from http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.0.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    Importing GPG key 0x07513CAD:
     Userid     : ""Jenkins (HDP Builds) <jenkin@hortonworks.com>""
     Fingerprint: df52 ed4f 7a3a 5882 c099 4c66 b973 3a7a 0751 3cad
     From       : http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.0.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    Running transaction check
    Running transaction test
    Transaction test succeeded
    Running transaction
      Installing : postgresql-libs-9.2.23-3.el7_4.x86_64                                                                                                                          1/4 
      Installing : postgresql-9.2.23-3.el7_4.x86_64                                                                                                                               2/4 
      Installing : postgresql-server-9.2.23-3.el7_4.x86_64                                                                                                                        3/4 
      Installing : ambari-server-2.7.0.0-897.x86_64                                                                                                                               4/4 
      Verifying  : postgresql-9.2.23-3.el7_4.x86_64                                                                                                                               1/4 
      Verifying  : ambari-server-2.7.0.0-897.x86_64                                                                                                                               2/4 
      Verifying  : postgresql-libs-9.2.23-3.el7_4.x86_64                                                                                                                          3/4 
      Verifying  : postgresql-server-9.2.23-3.el7_4.x86_64                                                                                                                        4/4 
    
    Installed:
      ambari-server.x86_64 0:2.7.0.0-897                                                                                                                                              
    
    Dependency Installed:
      postgresql.x86_64 0:9.2.23-3.el7_4                     postgresql-libs.x86_64 0:9.2.23-3.el7_4                     postgresql-server.x86_64 0:9.2.23-3.el7_4                    
    
    Complete!
    


",pull-request-available,[],AMBARI,Bug,Major,2018-08-17 05:55:04,5
13179372,Refactor select mpacks view,"The select mpacks view needs to be refactored for two reasons:
 # Needs to be able to display previously installed mpacks/services.
 # Needs to be aligned with the multi-instance model behind the scenes, even though we won't expose service groups/service instances in the UI for now.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-16 14:39:52,40
13179332,Workflow Manager should add ${nameNone}/ string to the beginning of app path,"When I use Workflow Manager to create a workflow for Oozie and add a spark action and provide *Application* path using the *Browse* button, Ambari WFM only inserts the app path in this form: */path/to/app*
In this case the result is a FileNotFoundException in Oozie.

*Workaround*:
When I modify the app path to *${nameNode}/path/to/app* then the job will run successfully.
Please see <jar> tag of the attached generated good and bad workflow xmls.",pull-request-available,['ambari-views'],AMBARI,Bug,Blocker,2018-08-16 12:01:01,45
13179324,[Log Search UI] Time Histogram Chart keep invert grey selection area when no selection happened,"When the use clicks on the histogram, but he/she does not create selection the grey invert-selection area stays there.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Trivial,2018-08-16 11:32:28,26
13179283,repoVersion entity should show resolved full stack version when available,We should get full stack version from repository file and sent it in the command.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-16 07:55:49,1
13179272,Yarn Timeline Service V2 Reader is found down after EU (Atlantic to AtlanticM05) With error - Address already in use,{{{{timeline_reader_address_http}}}} and {{{timeline_reader_address_https}}}} properties are not replaced in yarn-site by the FixTimelineReaderAddress upgrade task.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-16 07:17:29,18
13179169,HDP-3.0 Hive Interactive server fails to start on a sys-prepped cluster,"On sysprepped clusters, Ambari needs to create the /user/hive/.yarn/package/LLAP in the sysprep step.",pull-request-available,[],AMBARI,Bug,Critical,2018-08-15 20:53:15,39
13179113,HDP-GPL repo's shouldn't be pushed to hosts when GPL license was not applied,Only push the HDP-GPL repo to hosts when user applied GPL license during ambari-server setup.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-15 15:49:27,6
13179067,Timeline v2 HBase Does Start After Upgrade Due to Missing System Queue,"*During Upgrade*
# If cluster is big i.e >50GB and each machines has minimum 10GB resources
#* If *yarn-system* queue doesn't exist then we shall create a queue with *zero capacity*. 
#* All the necessary setting for queue such as *queue-priority*, *queue-acls*, *disabling pre-emption* for this queue are set.
#* ats-hbase app queue name also set to *yarn-system*
#* But *do NOT* enable system service mode by default. Let upgrade happens in embedded mode successfully. 
#* Post upgrade, admin can change flag to enable system_service mode if required by providing sufficient capacity to *yarn-system* queue. This step *need to be documented.*
# If cluster is small
#* Continue upgrade in embedded mode without creating a yarn-system queue. ",pull-request-available,[],AMBARI,Bug,Blocker,2018-08-15 13:01:35,31
13179065,Overlapping text in Recommendations in Configurations page while UI installer,Overlapping text in Centralized Configurations page: [^config-validation-overlap.png],pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-15 12:45:31,15
13178936,Orchestration: Remove singular upgrade pack from UpgradeContext; resolve ServiceGroup names,"This JIRA addresses the following issues in orchestration:
* Remove service group name stub.
* Remove UpgradeContext upgrade pack since each service group may have its own upgrade pack.
* Allow subsequent calls to attempt to create an upgrade",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-08-14 20:07:35,33
13178898,Log Search: use better default labels for logsearch components,logsearch stack code should set {{logsearch.web.service_logs.component.labels}} property by default,pull-request-available,"['ambari-logsearch', 'ambari-server']",AMBARI,Bug,Major,2018-08-14 16:47:38,29
13178883,ambair-agent floods data directory with files created for status commands,"
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-001de461-9c2f-417a-b323-6233ed86eb37.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00209852-ad33-4026-90a0-4da2aac71403.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00333009-87b2-4649-80d1-c97b639a31f0.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0042da27-dae6-4e9b-9ffb-336cd17787c5.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-005b6b80-baf7-43a5-aa7f-2e4695b3f478.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0066dcd4-9406-49c7-a2b3-effd9a3a4f2b.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-006a1999-d33d-4d48-a74f-8a7830b449ec.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0094b368-c5e1-496e-ba5b-b5080767dbc0.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-00b2a497-0088-464c-a4e4-855534d082ed.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00bbd469-370f-4eca-aee9-109db61edd75.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00bbe2f2-aeb2-4eb7-9bfb-6ce5fe9950b0.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00c341b7-5c4c-4dc5-a5ba-a550b0cabf67.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00d80541-e6a5-4bc4-9020-c018a928c6b9.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00da8bca-ac68-4d47-b492-2a978f368a1c.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00ed37ff-a9b8-4fe6-a46d-7d792586ac62.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00f49f50-cd1e-498f-ac97-5cc9724adf56.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00f74fcb-8cc1-46f8-b74a-6ea3089e19d9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-010174eb-dee9-41a7-9afb-25c5b16e185e.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01024c7d-beb5-40b2-914c-aece83011cea.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0106b69f-9343-40d1-a225-65112470f884.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0109e024-4cbf-49a8-8496-9d05787ed58d.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-011e9c15-5531-43c9-aabf-a9efa5123d2f.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-012f2630-46ad-4608-b7ac-971d3ee35f59.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0130258e-5c15-4147-a348-c86fa0ae5a1c.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01357110-1577-4d4f-908d-7956232ecd73.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-013a04e5-d2e8-4d9e-8cb6-3fe3ac6f75ca.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01456ca3-f135-4fd8-8b58-83ea513f7c06.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01473538-7a02-4df9-92c4-728d158d3e0f.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-014bdd5a-899d-43f6-a375-d65e30e0a2c3.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-014e0a58-fdc5-4d0a-8b0c-ac62129ba0ce.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-015c87f3-cdaa-4cf5-91ee-bf9af09e4262.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-015c972a-9210-4854-a49b-a1d591f799e6.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-016d1956-8076-4722-829d-e10748420432.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-017e22f1-561a-41ca-a59e-1473f3ec3065.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-017fe18b-5dc7-4c97-891a-737810ec5416.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01d63c54-9033-4932-974f-f6c756a2d1d4.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01ff5f2e-4572-4f52-b77c-12e3aea1c6fa.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02216bb7-9bac-4c5b-a074-f3e2b33c7a86.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02239432-9ba1-4899-923f-2f94c5facb89.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0225d7ba-22c5-412e-bea1-931732bf98f2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-023d005d-d253-4445-951b-22b903e9ff85.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-023e4124-bd5d-4e0d-8f1e-b146b5f9bb03.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-024c5c89-56c0-4470-8d64-2ab9e48707fa.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-025bb933-f6a5-4495-8df4-342e0c695845.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-026dcffa-015b-4ac0-be4e-70c2092a091a.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0272568c-5492-46be-8c6d-26edebb06f92.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0283f8dd-0d90-4978-9b44-c982a0ee78e9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0291a3b2-9b11-4e60-9f36-d63e11d48803.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02a4a54a-fc9c-4839-94b3-fd29cdbaeda6.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02a781a3-4db4-468b-a180-964d4fcdf176.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02bdfe4c-a2ce-4e38-990b-12c889a06a32.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02c873ea-60ce-444a-8b67-05c648d4c8d5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02d706ec-4b9d-44ad-baf6-67ec62dbdcdd.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02de1f1d-d3e8-4c74-b5dc-550540f4277e.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-02e34a4b-8f2d-489b-acd3-3c77b07e4de9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-02e42e6d-8756-45f6-8097-16430aebb4e3.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02e48587-00ff-48e4-81d4-a8535b3d6cd7.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02eebc26-43ed-4b36-99df-091e66ed6317.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0304d392-7471-422c-9229-2222ca7be62a.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-031091e8-b6cd-47d8-804c-29947ad34c87.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03140883-e4d2-4cc3-9731-7406762199da.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03203cad-edfc-4293-90db-8edd305f470b.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-032c1a82-386b-43de-9e17-d6228ce4ca40.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-03351bb9-9cc3-4d80-9bbb-4edf49b93b9c.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0348d610-18bf-42cc-8f3b-e16671f0f7b8.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-034d698d-7946-4503-b64b-20c9592c363b.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03640ef9-54f3-45c9-b9de-a0a211524093.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-036ad39f-9ef1-4967-b023-316e3429af34.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0377c628-708b-41f5-b3cf-1dda26686fda.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-037b5ae4-966c-4815-82fb-d10d7e7947c5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-037ce4fe-4c96-4614-bca1-ddb4d702ce03.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-039472ba-ea55-4a68-922b-dd0d799c1c07.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03990281-da3a-4d79-8998-5ae573d62fae.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03f2239b-f8d9-4bb4-a043-a675b91dd2e2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03f70e9b-549f-4019-9bd6-19a511ce67e1.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0403a633-8967-4408-bd94-e8a5d1e1b4b2.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0416dae8-52da-48c6-9451-e3bd10f8a0c7.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-041eebf1-8908-4484-aa5b-67c22101942c.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0452e168-012a-4b77-b73e-89f900fd9ce5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-045c0c2a-4a93-4ccb-8b02-e31f32f297c4.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-046d50dd-f5de-4ef9-a0eb-36b6946a3ce9.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0490547f-88dd-446c-aa64-1897521e2eba.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-049a2eca-90a1-4ca4-9216-6065f90bf855.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-049f27b7-f073-41a3-a415-1d8975b2d608.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04a989b1-2d78-4f35-bef9-9be4ab6c13e0.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04b2d2ee-f5d4-4c98-8011-db57ec5c28da.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-04b3c7f8-5c80-418c-a4f6-c6148e0c8a92.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04be6118-ae18-4d2d-80ca-5fc79dacf86d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04bedda7-53ab-40b4-8c7d-33242b660f06.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04bfc4e2-aa8f-4a24-a79f-b483bb80d210.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04c7e1dc-3846-453e-be10-75955445088c.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04dd82ce-25da-4dea-8b20-cda14df347fd.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04f65567-6f8c-4da8-8505-80664a4ee439.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-050f7e3c-6460-4045-9bdb-1eb3b602136d.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-051e0ccb-d6b8-4158-a1a9-9d7d0d52224d.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-052e32da-f71b-4f09-b62b-b3f30b25ec32.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-056e9da7-5a96-4646-bae6-2d080d0b2a82.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-056f3673-aa47-4efc-b0ad-e0ab96812654.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05708b0e-0df8-4b16-8bd9-874a72607186.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0577ca36-f1f2-49ce-852a-43b56a6656ca.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-059f1d92-1419-42c4-a7fd-d0f85a85f240.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05af50fb-e48f-40a9-9877-962d1718bbff.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05b2dcef-6aea-400d-992d-f0443d4c56d2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05b706f4-cd06-49b1-8d9c-c356f38c99d3.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05f829b6-8340-4673-97a2-94eaf6290e23.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05ff36a6-c39a-412e-b6c4-58f0f50cbab0.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0609de51-1254-4385-98f8-0bc027510908.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06123cae-b89d-496b-8a26-13c3e208b087.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0625192e-495d-4a2f-8226-54befd87daea.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-062ab0e0-9854-46ff-91a0-47e6d920e055.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-062bef42-9ef9-4808-896d-4b77a124458d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06510548-2629-480b-99e0-9769a4a3e254.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-066265b2-56e9-4629-8c13-309fd5b9bedb.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-066f2f1e-337e-4dca-a7aa-51b4efcb2b4d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-067d3e69-333a-424f-a846-c6411d8e8e87.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068226fb-f9d8-46bb-8bd7-3a13056bb983.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0688025a-d92d-4a8d-b14d-f7486a3f0d59.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068c77f3-ad4f-4986-836f-f4b9f44b53bf.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068f3c75-a176-49c2-96ba-48a4a6cc76ed.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06adba4c-9a60-432a-80f4-780602f476a7.json
    ...
    

",pull-request-available,[],AMBARI,Bug,Major,2018-08-14 15:36:16,5
13178857,Disable autostart during blueprint deploy,Ambari should disable components' autostart during blueprint deploy.,pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-08-14 13:40:39,6
13178837,Data in Install Wizard is restored even after logout,If user in Install Wizard goes up to Review step then all his changes are persisted to the server. So after every page refresh old data will be loaded even if later user goes back and makes another changes.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-14 12:24:07,9
13178831,Ambari upgrade fails due to NPE when processing Ambari Infra kerberos descriptor changes,"Ambari upgrade fails due to NPE when processing Ambari Infra changes

{noformat}
2018-07-17 01:57:48,282 ERROR [main] SchemaUpgradeHelper:238 - Upgrade failed.
java.lang.NullPointerException
                at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateInfraKerberosDescriptor(UpgradeCatalog270.java:1282)
                at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosDescriptorArtifact(UpgradeCatalog270.java:1202)
                at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.updateKerberosDescriptorArtifacts(AbstractUpgradeCatalog.java:797)
                at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1052)
                at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
                at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:236)
                at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:456)
2018-07-17 01:57:48,282 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException
                at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:239)
                at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:456)
Caused by: java.lang.NullPointerException
                at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateInfraKerberosDescriptor(UpgradeCatalog270.java:1282)
                at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosDescriptorArtifact(UpgradeCatalog270.java:1202)
                at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.updateKerberosDescriptorArtifacts(AbstractUpgradeCatalog.java:797)
                at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1052)
                at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
                at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:236)
                ... 1 more
{noformat}

This appears to be due to a failure to handle the case where no component-level Kerberos descriptors are supplied for a service in the user-supplied Kerberos descriptor.  Since the user-supplied Kerberos descriptor can be sparse, missing normally expected pieces of the Kerberos descriptor is acceptable.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-14 11:56:38,29
13178706,URL rewrite rules in gateway cannot be applied when the response content from ambari server is compressed,"We we try to access the cluster through gateway using the cluster public endpoint, we receive 500 internal server error with following error message.

Outbound rewrite rules cannot be applied when the content of the HTTP response is encoded (""gzip"").

We need a way to disable the response content compression on server side.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-13 21:25:22,39
13178704,HDP 3.0 Livy2-Server fails to start,"{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 148, in <module>
    LivyServer().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 62, in start
    self.configure(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 52, in configure
    setup_livy(env, 'server', upgrade_type=upgrade_type, action = 'config')
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/setup_livy2.py"", line 52, in setup_livy
    params.HdfsResource(None, action=""execute"")
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 663, in action_execute
    self.get_hdfs_resource_executor().action_execute(self)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 159, in action_execute
    logoutput=logoutput,
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'hadoop --config /usr/hdp/3.0.1.0-65/hadoop/conf jar /var/lib/ambari-agent/lib/fast-hdfs-resource.jar /var/lib/ambari-agent/tmp/hdfs_resources_1533930112.17.json' returned 1. Initializing filesystem uri: hdfs://mycluster
Creating: Resource [source=null, target=zk0-host:2181,zk1-host:2181,zk2-host:2181, type=directory, action=create, owner=livy, group=null, mode=700, recursiveChown=false, recursiveChmod=false, changePermissionforParents=false, manageIfExists=true] in hdfs://mycluster
Exception occurred, Reason: java.net.URISyntaxException: Relative path in absolute URI: zk0-host:2181,zk1-host:2181,zk2-host:2181
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: zk0-host:2181,zk1-host:2181,zk2-host:2181
	at org.apache.hadoop.fs.Path.initialize(Path.java:259)
	at org.apache.hadoop.fs.Path.<init>(Path.java:217)
	at org.apache.ambari.fast_hdfs_resource.Resource.checkResourceParameters(Resource.java:193)
	at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: zk0-host:2181,zk1-host:2181,zk2-host:2181
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:256)
	... 9 more
Traceback (most recent call last):
{noformat}",pull-request-available,[],AMBARI,Bug,Critical,2018-08-13 21:17:13,39
13178698,Add ClusterSettings and StackSettings module in execution_command library,These two modules should help retrieve clusterSettings and stackSettings configuration from command.json,pull-request-available,['ambari-server'],AMBARI,New Feature,Major,2018-08-13 20:48:01,44
13178666,Report Component Instance On Agent Registration,"AMBARI-24416 attempted to fix a problem where component versions were no longer reported  on startup since the agents were switched to a push-style framework using STOMP endpoints for information.

It added a new endpoint a some logic to push version information to Ambari Server when agents were registered. This uses the old style version information, not the new style for mpacks. 

- The endpoint needs to accept version structured output
- Agents must use the correct information about mpack name/version to obtain the right component versions",pull-request-available,[],AMBARI,Task,Critical,2018-08-13 18:29:40,31
13178663,Add README.md for Ambari project,Add README.md for Ambari project,pull-request-available,[],AMBARI,Improvement,Major,2018-08-13 18:21:45,46
13178614,"Warnings during ambari-agent, ambari-server upgrade","On such yum/apt commands - custom installation scripts causing extensive abusing log output by package manager: 

1) yum install/upgrade/remove
2) apt-get install/uninstall

Sample:
{code}
ambari-agent-2.7.0.0-876.x86_64.rpm                                                                                                                        |  36 MB  00:00:05
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Updating   : ambari-agent-2.7.0.0-876.x86_64                                                                                                                                1/2
Upgrading configs in /etc/ambari-agent/conf/ambari-agent.ini
Values will be updated from /etc/ambari-agent/conf/ambari-agent.ini.old except the following list: [('heartbeat', 'dirs'), ('heartbeat', 'state_interval')], ['stack', 'puppet', 'command', 'python']
  Cleanup    : ambari-agent-2.6.2.0-155.x86_64                                                                                                                                2/2
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.5/services/KAFKA/kerberos.json: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.5/services/KAFKA/configuration/ranger-kafka-audit.xml: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.2/services/LOGSEARCH/role_command_order.json: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.1/services/SMARTSENSE/package/files/view/smartsense-ambari-view-1.4.5.2.6.2.0-155.jar: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/widgets.json: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/AMBARI_INFRA/role_command_order.json: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/AMBARI_INFRA/metainfo.xml: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/AMBARI_INFRA: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/kerberos.json: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/templates/topology_mappings.data.j2: remove failed: No such file or directory
warning: file /var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/templates/include_hosts_list.j2: remove failed: No such file or directory
{code}",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-08-13 15:31:01,1
13178599,Integrate Blueprints with the new MPackAdvisor API for Configuration Recommendations,"This integration effort can be considered part of the work to fully integrate Blueprints with the new MPack-based approach.

Previous versions of Ambari Blueprints relied on the Ambari StackAdvisor API to provide configuration recommendations for a cluster deployment.

As part of the new MPack development effort, a new mpack-aware Advisor has been implemented to support recommendations for a cluster based on multiple MPacks.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-13 14:03:00,20
13178561,Heatmaps:Specific calculation methods for Host Memory Used and Host CPU Wait IO are not correct,"1.Host Memory Used calculation method : ((mem_total - mem_free - mem_cached)/mem_total)*100 
The value of mem_free was collected by psutil.virtual_memory()  method in host_info.py file and it correspond to 'available'  in psutil.virtual_memory() results.Actually,the available = free + buffers + cached.So,the Host Memory Used calculation method should be ((mem_total - mem_free)/mem_total)*100

2.Host CPU Wait IO was collected by  psutil.cpu_times_percent() method in host_info.py.It has been converted to percentage.So,we should use ${cpu_wio} instead of ${cpu_wio*100} as its value in widgets.json.

",pull-request-available,[],AMBARI,Bug,Major,2018-08-13 11:00:39,47
13178549,ambari-server upgrade stuck with NPE,"====================================
Ambari 2.6.2
HDP 2.6.5.0-292
Cluster Details : https://github.com/hortonworks/HCube#stanley-hotel-hdp-26
Please login with okta credential to these cluster machines
https://hcube1-1n01.eng.hortonworks.com:8443/
====================================

This is cluster is being upgraded to Ambari 2.7.1 and the schema upgrade step is hung.
{code}
root@hcube1-1n01 ~]# ambari-server upgrade
Using python  /usr/bin/python
Upgrading ambari-server
INFO: Upgrade Ambari Server
INFO: Updating Ambari Server properties in ambari.properties ...
INFO: Updating Ambari Server properties in ambari-env.sh ...
INFO: Original file ambari-env.sh kept
WARNING: Original file krb5JAASLogin.conf kept
INFO: File krb5JAASLogin.conf updated.
INFO: Fixing database objects owner
Ambari Server configured for Postgres. Confirm you have made a backup of the Ambari Server database [y/n] (n)? y
INFO: Upgrading database schema



{code}


Below exception is noticed in ambari-server.log

{code}
2018-08-10 06:57:50,795 ERROR [main] AbstractUpgradeCatalog:375 - Error in transaction
java.lang.NullPointerException
        at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1617)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:184)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:94)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270$3.run(UpgradeCatalog270.java:1637)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.executeInTransaction(AbstractUpgradeCatalog.java:367)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1633)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1060)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:457)
2018-08-10 06:57:50,799 ERROR [main] SchemaUpgradeHelper:239 - Upgrade failed.
org.apache.ambari.server.AmbariException: Failed to upgrade kerberos tables
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1644)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1060)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:457)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.executeInTransaction(AbstractUpgradeCatalog.java:379)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1633)
        ... 4 more
Caused by: java.lang.NullPointerException
        at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1617)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:184)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:94)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270$3.run(UpgradeCatalog270.java:1637)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.executeInTransaction(AbstractUpgradeCatalog.java:367)
        ... 5 more
{code}

Attached is the complete ambari-server logs


Below is the jstack dump of schemaupgradehelper

{code}
[root@hcube1-1n01 ambari-server]# /opt/java/jdk1.8.0_152/bin/jstack -l 8239
2018-08-10 07:10:00
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.152-b16 mixed mode):

""Attach Listener"" #40 daemon prio=9 os_prio=0 tid=0x00007f9a6c001000 nid=0x3afa waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""DestroyJavaVM"" #39 prio=5 os_prio=0 tid=0x00007f9b6c00d000 nid=0x2030 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""FileWatchdog:ambari.properties"" #38 daemon prio=5 os_prio=0 tid=0x00007f9b6d684000 nid=0x2c19 waiting on condition [0x00007f9b2e9d4000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.log4j.helpers.FileWatchdog.run(FileWatchdog.java:104)

   Locked ownable synchronizers:
	- None

""pool-6-thread-1"" #37 prio=5 os_prio=0 tid=0x00007f9a10006800 nid=0x210f waiting on condition [0x00007f9b2eed9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080dc3270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""pool-5-thread-1"" #36 prio=5 os_prio=0 tid=0x00007f9b6d497800 nid=0x2108 waiting on condition [0x00007f9b2edd8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080dc2fe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""pool-3-thread-1"" #35 prio=5 os_prio=0 tid=0x00007f9b6d0cb000 nid=0x2107 waiting on condition [0x00007f9b2f2dd000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080dc2b38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""server-action-executor-cache-timer"" #23 daemon prio=5 os_prio=0 tid=0x00007f9b6da29000 nid=0x20d8 in Object.wait() [0x00007f9b34120000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000080f0b6e0> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x0000000080f0b6e0> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

   Locked ownable synchronizers:
	- None

""InMemoryCredentialStore active cleanup timer"" #22 daemon prio=5 os_prio=0 tid=0x00007f9b6d541800 nid=0x20cd waiting on condition [0x00007f9b35021000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080f2c750> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""Service Thread"" #20 daemon prio=9 os_prio=0 tid=0x00007f9b6c107800 nid=0x205c runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread14"" #19 daemon prio=9 os_prio=0 tid=0x00007f9b6c104800 nid=0x205b waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread13"" #18 daemon prio=9 os_prio=0 tid=0x00007f9b6c102800 nid=0x205a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread12"" #17 daemon prio=9 os_prio=0 tid=0x00007f9b6c100000 nid=0x2059 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007f9b6c0fe000 nid=0x2058 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007f9b6c0fb800 nid=0x2057 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f9800 nid=0x2056 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f7800 nid=0x2054 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread7"" #12 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f5000 nid=0x2053 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread6"" #11 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f3000 nid=0x2052 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread5"" #10 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f0800 nid=0x2051 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread4"" #9 daemon prio=9 os_prio=0 tid=0x00007f9b6c0ee800 nid=0x2050 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007f9b6c0ec000 nid=0x204f waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f9b6c0ea000 nid=0x204e waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f9b6c0e8000 nid=0x204d waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f9b6c0e5000 nid=0x204c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f9b6c0e3800 nid=0x204b runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f9b6c0af800 nid=0x204a in Object.wait() [0x00007f9b3d0e2000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008020b148> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x000000008020b148> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

   Locked ownable synchronizers:
	- None

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f9b6c0ab000 nid=0x2049 in Object.wait() [0x00007f9b3d1e3000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008020c698> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x000000008020c698> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

""VM Thread"" os_prio=0 tid=0x00007f9b6c0a3800 nid=0x2048 runnable 

""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c022800 nid=0x2031 runnable 

""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c024000 nid=0x2032 runnable 

""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c026000 nid=0x2033 runnable 

""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c028000 nid=0x2034 runnable 

""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c029800 nid=0x2035 runnable 

""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c02b800 nid=0x2036 runnable 

""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c02d000 nid=0x2037 runnable 

""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c02f000 nid=0x2038 runnable 

""GC task thread#8 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c031000 nid=0x2039 runnable 

""GC task thread#9 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c032800 nid=0x203a runnable 

""GC task thread#10 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c034800 nid=0x203b runnable 

""GC task thread#11 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c036800 nid=0x203c runnable 

""GC task thread#12 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c038000 nid=0x203d runnable 

""GC task thread#13 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03a000 nid=0x203e runnable 

""GC task thread#14 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03c000 nid=0x203f runnable 

""GC task thread#15 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03d800 nid=0x2040 runnable 

""GC task thread#16 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03f800 nid=0x2041 runnable 

""GC task thread#17 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c041800 nid=0x2042 runnable 

""GC task thread#18 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c043000 nid=0x2043 runnable 

""GC task thread#19 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c045000 nid=0x2044 runnable 

""GC task thread#20 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c046800 nid=0x2045 runnable 

""GC task thread#21 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c048800 nid=0x2046 runnable 

""GC task thread#22 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c04a800 nid=0x2047 runnable 

""VM Periodic Task Thread"" os_prio=0 tid=0x00007f9b6c10c800 nid=0x205d waiting on condition 

JNI global references: 328
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-13 09:53:14,18
13178379,Merge trunk into branch-feature-AMBARI-14714,"After AMBARI-24446, we should merge trunk into the feature branch to pickup the removal of stacks.",pull-request-available,[],AMBARI,Task,Major,2018-08-10 21:32:24,31
13178330,Start operation is disabled for HDFS after stopping components for one namespace,"Start operation is disabled for HDFS after stopping components for one namespace

 

STR:

1) On hdfs service page, stop components for one namespace (ns1)
2) Try to start them. Start button is disabled",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-10 17:18:44,28
13178263,Hiveserver2 can't connect to metastore when using OneFS,"{code}
2018-07-12T07:56:27,148 ERROR [pool-6-thread-3]: metastore.HiveMetaStore (HiveMetaStore.java:get_current_notificationEventId(7617)) - Not authorized to make the get_current_notificationEventId call. You can try to disable metastore.metastore.event.db.notification.api.auth
org.apache.hadoop.hive.metastore.api.MetaException: User hive is not allowed to perform this API call
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7655) ~[hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7615) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at com.sun.proxy.$Proxy34.get_current_notificationEventId(Unknown Source) [?:?]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_112]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_112]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) [hadoop-common-3.1.0.3.0.0.0-1628.jar:?]
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-12T07:56:27,153 ERROR [pool-6-thread-3]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(201)) - org.apache.thrift.TException: MetaException(message:User hive is not allowed to perform this API call)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7619)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy34.get_current_notificationEventId(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:User hive is not allowed to perform this API call)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7655)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7615)
        ... 20 more
{code}

hadoop.proxyuser.hive.hosts and hadoop.proxyuser.hive.groups is not set when OneFS is used in place of HDFS",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-10 12:21:47,18
13178213,Ambari server can not start on latest Amazon Linux 2,"On latest Amazon Linux 2 ambari-server cannot start with error message:

    
    
    [root@ip-10-0-222-155 cloudbreak]# ambari-server start
    Using python  /usr/bin/python
    Starting ambari-server
    Traceback (most recent call last):
      File ""/usr/sbin/ambari-server.py"", line 37, in <module>
        from ambari_server.dbConfiguration import DATABASE_NAMES, LINUX_DBMS_KEYS_LIST
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 30, in <module>
        from ambari_server.serverConfiguration import decrypt_password_for_alias, get_ambari_properties, get_is_secure, \
      File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 46, in <module>
        OS_VERSION = OSCheck().get_os_major_version()
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 322, in get_os_major_version
        return OSCheck.get_os_version().split('.')[0]
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 301, in get_os_version
        return OSCheck.get_alias(OSCheck._get_os_type(), OSCheck._get_os_version())[1]
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 313, in _get_os_version
        raise Exception(""Cannot detect os version. Exiting..."")
    Exception: Cannot detect os version. Exiting...
    

Additional infos from the machine:

    
    
    [root@ip-10-0-222-155 cloudbreak]# cat /etc/system-release
    Amazon Linux 2
    [root@ip-10-0-222-155 cloudbreak]# cat /etc/*release
    NAME=""Amazon Linux""
    VERSION=""2""
    ID=""amzn""
    ID_LIKE=""centos rhel fedora""
    VERSION_ID=""2""
    PRETTY_NAME=""Amazon Linux 2""
    ANSI_COLOR=""0;33""
    CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2""
    HOME_URL=""https://amazonlinux.com/""
    Amazon Linux 2
    [root@ip-10-0-222-155 cloudbreak]#
    

",pull-request-available,[],AMBARI,Bug,Major,2018-08-10 08:17:46,5
13178140,[UI Deploy] LLAP queue is not created/set in YARN configs while enabling HSI,"In a cluster installed via UI deploy:

HSI was enabled as part of UI install wizard, but in the YARN configs, the 'llap' queue is not added leading to deploy failures. (Hive Server Interactive did not start)
{code:java}
Failed: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1532935384747_0002 to YARN : Application application_1532935384747_0002 submitted by user hive to unknown queue: llap
java.lang.RuntimeException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1532935384747_0002 to YARN : Application application_1532935384747_0002 submitted by user hive to unknown queue: llap
	at org.apache.hadoop.hive.llap.cli.LlapSliderUtils.startCluster(LlapSliderUtils.java:154)
	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.run(LlapServiceDriver.java:602)
	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.main(LlapServiceDriver.java:120)
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1532935384747_0002 to YARN : Application application_1532935384747_0002 submitted by user hive to unknown queue: llap
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:304)
	at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:837)
	at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:365)
	at org.apache.hadoop.yarn.service.client.ServiceClient.actionLaunch(ServiceClient.java:351)
	at org.apache.hadoop.hive.llap.cli.LlapSliderUtils.startCluster(LlapSliderUtils.java:149)
	... 2 more

{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-09 22:09:28,34
13178076,"ambari.ldap.advanced.group_mapping_rules does not work, LDAP sync does not add admin roles for configured group(s)","If I configure ambari.ldap.advanced.group_mapping_rules, it does not add ambari admin roles to the configured role. I did some debug and the AmbariLdapConfiguration is uninitialized in Users.java class, so the adminGroupMappings value is the default ""Ambari Administrators"" not the one I configured before. 

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-09 17:40:42,30
13178074,Remove dependencies with potential security vulnerabilities from fast-hdfs-resource,"
Remove dependencies with potential security vulnerabilities from fast-hdfs-resource.  Most issues appear to be coming from dependencies of org.apache.hadoop:hadoop-core:1.2.1.

* Apache Tomcat 5.5.12 - recommendation, exclude or update to 6.0.20.0 or above.
{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Building fast-hdfs-resource 0.0.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ fast-hdfs-resource ---
[INFO] org.apache.ambari:fast-hdfs-resource:jar:0.0.1-SNAPSHOT
[INFO] \- org.apache.hadoop:hadoop-core:jar:1.2.1:compile
[INFO]    +- tomcat:jasper-runtime:jar:5.5.12:compile
[INFO]    \- tomcat:jasper-compiler:jar:5.5.12:compile
[INFO] ------------------------------------------------------------------------
{noformat}

* org.mortbay.jetty:jetty:6.1.26 - exclude or update to 6.1.26-hwx
* org.mortbay.jetty:jsp-2.1:6.1H.14.1 - exclude or update to 6.1.0.0-fuse
* org.mortbay.jetty:servlet-api-2.5:6.1.12rc1 - exclude or update to 6.1.0.1-fuse
* org.mortbay.jetty:jetty-test:6.1.26 - exclude or update to 6.1.26.hwx
{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Building fast-hdfs-resource 0.0.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ fast-hdfs-resource ---
[INFO] org.apache.ambari:fast-hdfs-resource:jar:0.0.1-SNAPSHOT
[INFO] \- org.apache.hadoop:hadoop-core:jar:1.2.1:compile
[INFO]    +- org.mortbay.jetty:jetty:jar:6.1.26:compile
[INFO]    |  \- org.mortbay.jetty:servlet-api:jar:2.5-20081211:compile
[INFO]    +- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO]    +- org.mortbay.jetty:jsp-api-2.1:jar:6.1.14:compile
[INFO]    |  \- org.mortbay.jetty:servlet-api-2.5:jar:6.1.14:compile
[INFO]    \- org.mortbay.jetty:jsp-2.1:jar:6.1.14:compile
[INFO] ------------------------------------------------------------------------
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-09 17:33:02,13
13178050,No subject alternative DNS name exception encountered when Enabling Kerberos against an Active Directory even when SSL verification is off,"No subject alternative DNS name exception encountered when Enabling Kerberos against an Active Directory even when SSL verification is off.

{noformat}
2018-08-09 14:48:28,275  WARN [ambari-client-thread-35] ADKerberosOperationHandler:471 - Failed to communicate with the Active Directory at ldaps://adserver.example.com:636: adserver.example.com:636
javax.naming.CommunicationException: adserver.example.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching adserver.example.com found.]
        at com.sun.jndi.ldap.Connection.<init>(Connection.java:238)
        at com.sun.jndi.ldap.LdapClient.<init>(LdapClient.java:137)
...
Caused by: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching adserver.example.com found.
        at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
        at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1964)
...
Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching adserver.example.com found.
        at sun.security.util.HostnameChecker.matchDNS(HostnameChecker.java:214)
        at sun.security.util.HostnameChecker.match(HostnameChecker.java:96)
        at sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:459)
        at sun.security.ssl.AbstractTrustManagerWrapper.checkAdditionalTrust(SSLContextImpl.java:1026)
        at sun.security.ssl.AbstractTrustManagerWrapper.checkServerTrusted(SSLContextImpl.java:993)
        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1596)
{noformat}

Note: This occurs when the hostname embedded in the SSL certificate does not match the hostname of the Active Directory host and Open JDK 1.8.181-b13 is used.  This is not seen when Oracle JDK is used. 

{noformat:title=Observed with this version of JDK}
openjdk version ""1.8.0_181""
OpenJDK Runtime Environment (build 1.8.0_181-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
{noformat}


{noformat:title=Not observed with this version of JDK}
java version ""1.8.0_112""
Java(TM) SE Runtime Environment (build 1.8.0_112-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)
{noformat}

*Solution*
The {{org.apache.ambari.server.security.InternalSSLSocketFactory.LenientTrustManager}} class needs to extend {{javax.net.ssl.X509ExtendedTrustManager}} and do nothing in the additional overridden methods. 


",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-09 16:31:17,30
13178049,Removal of Unsupported Stacks,"Ambari 3.0 will no longer support the stacks which currently exist in the Apache repository. The following stacks should be removed:

- BIGTOP
- HDP
- HDPWIN
- PERF
- PHD

With Ambari 3.0, all stacks will be delivered via management packs. This furthers the goal of making Ambari a framework where providers can plugin their own stack-based logic.",pull-request-available,[],AMBARI,Task,Blocker,2018-08-09 16:21:44,31
13177997,[Log Search UI] exclude mock data from production build,"Currently, mock data is included to production build code, which is redundant.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-08-09 13:34:29,26
13177947,Slider type widget displays erroneously before first touch,"Under some circumstances slider type widget minimum and maximum values are shown both on the left side one on the other, and the actual value pointer is a bit off the point when the slider is.

STR:

1. Go to the Hive/Configs
2. Enable Interactive Query
3. Click on Select on a the pop-up window
4. Roll down to ""Memory per Daemon""
5. Touch the widget, it gets fixed immediately",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-09 11:02:09,9
13177946,HIVESERVER2 JDBC URL doesn't fit inside block ,On Hive Service Summary page text doesn't fit inside the appropriate block if HIVESERVER2 JDBC URL has much hosts in its value.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-09 11:01:26,9
13177931,[Log Search UI] The graphs are not always resized on window resize event.,"When the user resizes the browser window the graphs/charts are do not resized fully.

Expectation: every time when the browser window resized the graph width should be recalculated.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Major,2018-08-09 09:51:09,26
13177924,[Log Search UI] Default logs list length per page should be 100 instead of 10,Default logs list length per page should be 100 instead of 10,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Trivial,2018-08-09 09:36:54,26
13177915,[Log Search UI] center the log tabs so they're aligned with the buttons,Center the log screen tabs so they're aligned with the buttons.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Trivial,2018-08-09 09:22:00,26
13177910,[Log Search UI] App Loader Page - add initial progress state,"Add some initial progress state to the app loader progress bar:
 * initial x% value
 * indeterminate progress bar",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-08-09 09:09:16,26
13177905,[Log Search UI] App Loader Page - add more space above the loader animations,Add space above the loader animations to be more separated from the top.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Trivial,2018-08-09 09:02:44,26
13177903,[Log Search UI] App Loader Page - Align the logo left,Align left the logo on the application loader page.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Trivial,2018-08-09 08:49:09,26
13177901,[Log Search UI] Remove underline text decoration from dropdowns on hover,"The dropdown buttons have underline text decoration on mouse hover, it should be removed.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-08-09 08:42:03,26
13177899,Cannot deploy HBase without HDFS,"HBase can store data on filesystems other than HDFS, but Ambari cannot deploy it without HDFS.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-09 08:31:48,21
13177897,[Log Search UI] Reset to default selection in the dropdowns,Table list column selection: create an action button to reset the selection to the default.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-08-09 08:20:43,26
13177760,Orchestration: Display Names are not being resolved,This is a result of new methods that passed in {{null}} in order to compile.,pull-request-available,[],AMBARI,Task,Critical,2018-08-08 18:12:50,33
13177749,Remove dependencies with CVE issues from Ambari Agent,"Remove dependencies with CVE issues from Ambari Agent

* org.apache.commons:commons-collections4:jar before version 4.0-alpha1-RC1
** CVE-2015-6420 - https://nvd.nist.gov/vuln/detail/CVE-2015-6420

{noformat}
[INFO] org.apache.ambari.contrib.views:ambari-views-utils:jar:2.0.0.0-SNAPSHOT
[INFO] \- org.apache.commons:commons-collections4:jar:4.0:compile
{noformat}
",cleanup pull-request-available,['ambari-agent'],AMBARI,Task,Critical,2018-08-08 17:51:17,30
13177727,Remove Accidental Check-in of HDP Stack File,"The following file was incorrectly added to Apache's repository:

https://github.com/apache/ambari/commit/5fa81ce899319710e99d4f33309201641f20f6b2#diff-7f82152a9a73b67036a3fd2db8dec1ff",pull-request-available,[],AMBARI,Bug,Major,2018-08-08 17:01:17,31
13177680,[Log Search UI] Show user friendly component names in Log Index Filter screen,"Show the user friendly component names on the screen of the Log Index Filter, where the filter level can be set up.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Major,2018-08-08 14:11:45,26
13177677,[Log Search UI] Create visible time histogram selection area,The selection area or the invert selection area is missing when the user is selecting a time range.,pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Major,2018-08-08 14:03:50,26
13177670,[Log Search UI] Align the time histogram chart axis,"On the service logs screen there is a gap between the two axis of the chart.
Expected behaviour: the two axis start from the same point.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-08-08 13:37:23,26
13177531,HDFS Service page Datanode does not show count,In the components section data just shows 'Started' without the actual count.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-07 21:41:25,34
13177496,Component Versions Are Not Reported On Initial Status Commands Anymore,"In Ambari 2.6, some status commands were able to supply the version of the
component they were running for. This was needed especially during upgrades to
correct situations where components would fail to start but the processes were
actually running and reporting.

  * During heartbeat registration, Ambari asked the agent to get the version in the next status command:  
<https://github.com/apache/ambari/blob/branch-2.6/ambari-
server/src/main/java/org/apache/ambari/server/agent/HeartBeatHandler.java#L416-L419>

  * The status command would place the version information from the structured out into the ""extra"" mapping, which would then be handled here via an event:  
<https://github.com/apache/ambari/blob/trunk/ambari-
server/src/main/java/org/apache/ambari/server/agent/HeartbeatProcessor.java#L599-L603>

We need to replace this functionality so that component versions are reported
on registration of agents.

",pull-request-available,[],AMBARI,Bug,Major,2018-08-07 19:27:21,5
13177485,Remove dependencies with CVE issues from Ambari Server,"Remove dependencies with CVE issues from Ambari Server

* org.springframework:spring-beans:jar before 4.3.17.RELEASE 
** CVE-2018-1270 - https://nvd.nist.gov/vuln/detail/CVE-2018-1270
** CVE-2018-1275 - https://nvd.nist.gov/vuln/detail/CVE-2018-1275
** CVE-2018-1199 - https://nvd.nist.gov/vuln/detail/CVE-2018-1199
** CVE-2018-1271 - https://nvd.nist.gov/vuln/detail/CVE-2018-1271
** CVE-2018-1257 - https://nvd.nist.gov/vuln/detail/CVE-2018-1257
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.springframework.security:spring-security-core:jar:4.2.4.RELEASE:compile
[INFO]    \- org.springframework:spring-beans:jar:4.3.12.RELEASE:compile
{noformat}

* org.kohsuke:libpam4j:jar before version 1.9
** CVE-2017-12197 - https://nvd.nist.gov/vuln/detail/CVE-2017-12197
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.kohsuke:libpam4j:jar:1.8:compile
{noformat}

* org.springframework:spring-context before version 4.3.17.RELEASE
** CVE-2018-1257 - https://nvd.nist.gov/vuln/detail/CVE-2018-1257
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.springframework:spring-context:jar:4.3.16.RELEASE:compile
{noformat}

* org.springframework.security:spring-security-ldap:jar before version 4.1.5.RELEASE 
** CVE-2018-1199 - https://nvd.nist.gov/vuln/detail/CVE-2018-1199
** CVE-2016-9879 - https://nvd.nist.gov/vuln/detail/CVE-2016-9879
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.springframework.security:spring-security-ldap:jar:4.1.1.RELEASE:compile
{noformat}

* com.jcraft:jsch:jar before version 1.54 
** CVE-2016-5725 - https://nvd.nist.gov/vuln/detail/CVE-2016-5725
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- com.jcraft:jsch:jar:0.1.45:compile
{noformat}
",cleanup pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-08-07 18:38:23,30
13177482,Service config versions call fails at deploy step,"Deploy step fails due to a change to the service_config_versions API. Error message:
{code:none}
The properties [ServiceConfigVersion/service_config_version_note, ServiceConfigVersion/stack_id] specified in the request or predicate are not supported for the resource type ServiceConfigVersion.
{code}
Need to remove the properties that are no longer supported.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-07 18:35:24,40
13177416,Unable to Revert a Patch When There Are Multiple Patches,"When multiple patch upgrades are performed, reverting the latest patch might throw the following exception:

{code}
018-08-07 11:26:57,703 ERROR [ambari-client-thread-6474] CreateHandler:74 - Caught a system exception while attempting to create a resource: An internal system exception occurred: Upgrade from 3.0.1.1-10 could not be reverted as there is no single  repository across services.
org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Upgrade from 3.0.1.1-10 could not be reverted as there is no single  repository across services.
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:297)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createResources(UpgradeResourceProvider.java:350)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
	at org.apache.ambari.server.api.services.UpgradeService.createUpgrade(UpgradeService.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:291)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:94)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)
{code}

STR:
- Install ZK and Storm on 3.0.0.0-1
- Patch Upgrade ZK to 3.0.0.0-2
- Patch Upgrade ZK and Storm to 3.0.0.0-3

You are now not able to revert the latest patch b/c Storm would go to 3.0.0.0-1 while ZK goes to 3.0.0.0-2.  This should be allowed.",pull-request-available,[],AMBARI,Task,Blocker,2018-08-07 14:15:10,31
13177212,Remove conf-select Tool From Ambari Framework,"Mpack do not provide a replacement for {{conf-select}}, a utility which was previously used to provide parallel configurations so that 2 components (such as NameNode and DataNode) could use different configurations while in an upgrade. It ensured that breaking configuration changes stay isolated in their respective directories:

{noformat}
/usr/hdp/2.5.0.0/zookeeper/conf -> /etc/zookeeper/2.5.0.0/0
/usr/hdp/2.6.0.0/zookeeper/conf -> /etc/zookeeper/2.6.0.0/0
/usr/hdp/current/zookeeper-server -> /usr/hdp/2.5.0.0/zookeeper
{noformat}
 
When {{hdp-select}} was used to change the {{current}} symlink, the {{conf}} directories would “automatically” switch over. This allowed a complete separation of configurations and the ability to have parallel configurations on disk. 
 
If Ambari is managing your configurations, then we know what to write and when to write it
Even in cases where breaking configuration changes were made, since Ambari kept both old and new configurations in our database, a downgrade would always write out the correct configurations for the version being downgraded to
 
Let’s fast-forward to Ambari 3.0 and mpacks … The current structure does not allow for multiple configurations for a given service inside of a service group:
 
{noformat}
instances
└── hdpcore
    ├── default -> /usr/hwx/instances/hdpcore/HDPCORE
    └── HDPCORE
        └── default
            ├── zookeeper
            │   └── zookeeper_server
            │       └── ZOOKEEPER
            │           ├── conf
            │           │   ├── configuration.xsl
            │           │   ├── log4j.properties
            │           │   ├── zoo.cfg
            │           │   ├── zookeeper-env.sh
            │           │   └── zoo_sample.cfg
{noformat} 
 
Instead of symlinks scoped by a version, each service instance has a regular conf directory. A few points here:
- Since configurations are now at the component instance level, DataNode and NameNode won't share configs during an upgrade.
- For Ambari managed configurations, this should be fine since we keep track of old and new versions. So, even on a downgrade, we’d know to write the correct values in here
- If Ambari is not managing a configuration file, say foo-site.xml, then:
-- We don’t have to worry about copying it from one versioned directory to another. This seeding process is necessary in Ambari 2.x, but wouldn’t be here
-- If there was a change to the structure of a file which Ambari does not manage, then we have a problem on downgrade as Ambari wouldn’t know to replace anything. I suppose it’s the same issue on upgrade too since Ambari wouldn’t know to change the file either.

",pull-request-available,[],AMBARI,Task,Major,2018-08-06 20:29:18,31
13177152,Infra Solr migration: Restore collection fails after EU on Custom Users + WE cluster. Error - Permission denied: u'/tmp/ranger/restore_core_pairs.json',"{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/infra_solr.py"", line 171, in <module>
    InfraSolr().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/infra_solr.py"", line 146, in restore
    restore_collection(env)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/collection.py"", line 103, in restore_collection
    core_pairs = command_commons.create_core_pairs(original_core_host_pairs, new_core_host_pairs)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/command_commons.py"", line 289, in create_core_pairs
    with open(format(""{index_location}/restore_core_pairs.json""), 'w') as outfile:
IOError: [Errno 13] Permission denied: u'/tmp/ranger/restore_core_pairs.json
{code}",pull-request-available,"['ambari-infra', 'ambari-server']",AMBARI,Bug,Major,2018-08-06 16:14:45,29
13177146,Update org.eclipse.jetty version to 9.4.11.v20180605 to avoid CVE issues,"Update org.eclipse.jetty version to 9.4.11.v20180605 to avoid CVE issues.

See https://dev.eclipse.org/mhonarc/lists/jetty-announce/msg00123.html for reported issues. ",cleanup pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-06 15:53:03,30
13176727,Fix validate single sign-in support information to look for ssoEnabledTest,"Fix validate single sign-in support information to look for {{ssoEnabledTest}}.

The current test looks for the old {{enabledConfiguration}} value only. However {{ssoEnabledTest}} is the new way, leaving {{enabledConfiguration}} for backward compatibility. ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-03 14:47:18,30
13176441,Components in hosts page should be sorted by display name,"Components in hosts page seem to be sorted by component type (master, slave, client etc) and within them they are not sorted by ""display names"", ""Timeline"" comes before ""History""

More importantly people when looking for a component don't look by their type but instead look by their name. So all the components should be alphabetically sorted by their display name irrespective of their type. Else it becomes painful to look for a component of interest. (and finally have to rely on browser's search capability)",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-02 15:00:17,9
13176436,Button label appears incorrect during service deletion,"Try to delete Ranger service from a cluster, upon deletion stack advisor recommends changes to configs.

This is fine, however the button at the bottom of the dialog says 'DELETE'
It should instead say something like 'Proceed' and once user clicks Proceed, the delete pop-up should appear.

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-02 14:40:13,9
13176434,[Ambari metrics][grafana] grafana is not showing any datapoints if the queue name contains special characters,"grafana is not showing any data points if the queue name contains special characters

I have a queue named 'A&B' , in Yarn queues dashboard if I select this queue name I am getting empty data showing no data points.

The issue is reproducible

Analysis : 

In grafana UI we are sending the request to fetch the data by encoding the URI.
for which the server is interpreting it as separate parameters 

http://mygrafanaserver.com:3000/api/datasources/proxy/1/ws/v1/timeline/metrics?metricNames=yarn.QueueMetrics.Queue=root.{color:red}A&B{color}.AppsRunning._max&appId=resourcemanager&startTime=1533181180&endTime=1533202780

Which should ideally be :

http://mygrafanaserver.com:3000/api/datasources/proxy/1/ws/v1/timeline/metrics?metricNames=yarn.QueueMetrics.Queue=root.{color:red}A%26B{color}.AppsRunning._max&appId=resourcemanager



",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-08-02 14:31:58,3
13176252,Components start failing with 'Holder DFSClient_NONMAPREDUCE does not have any open files' while adding Namespace ,"STR: 
Add a namespace from UI. In the last step restart required services, hiveserver2 restart fails. Although on retrying it comes back up

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.status(env)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 79, in status
 check_process_status(status_params.hive_pid)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/check_process_status.py"", line 43, in check_process_status
 raise ComponentIsNotRunning()
ComponentIsNotRunning

The above exception was the cause of the following exception:

Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 137, in <module>
 HiveServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 993, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 50, in start
 self.configure(env) # FOR SECURITY
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 45, in configure
 hive(name='hiveserver2')
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive.py"", line 119, in hive
 setup_hiveserver2()
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive.py"", line 167, in setup_hiveserver2
 skip=params.sysprep_skip_copy_tarballs_hdfs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 516, in copy_to_hdfs
 replace_existing_files=replace_existing_files,
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 654, in action_create_on_execute
 self.action_delayed(""create"")
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 651, in action_delayed
 self.get_hdfs_resource_executor().action_delayed(action_name, self)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 354, in action_delayed
 self.action_delayed_for_nameservice(nameservice, action_name, main_resource)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 380, in action_delayed_for_nameservice
 self._create_resource()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 396, in _create_resource
 self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 511, in _create_file
 self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 199, in run_command
 return self._run_command(*args, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 272, in _run_command
 raise WebHDFSCallException(err_msg, result_dict)
resource_management.libraries.providers.hdfs_resource.WebHDFSCallException: Execution of 'curl -sS -L -w '%\{http_code}' -X PUT --data-binary @/usr/hdp/3.0.1.0-30/hive/hive.tar.gz -H 'Content-Type: application/octet-stream' --negotiate -u : -k 'https://<HOST-FQDN>:50470/webhdfs/v1/hdp/apps/3.0.1.0-30/hive/hive.tar.gz?op=CREATE&overwrite=True&permission=444'' returned status_code=404. 
{
 ""RemoteException"": {
 ""exception"": ""FileNotFoundException"", 
 ""javaClassName"": ""java.io.FileNotFoundException"", 
 ""message"": ""File does not exist: /hdp/apps/3.0.1.0-30/hive/hive.tar.gz (inode 16450) Holder DFSClient_NONMAPREDUCE_-1764810327_120 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2800)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:597)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)\n""
 }
}
{code}

 ",pull-request-available,[],AMBARI,Bug,Major,2018-08-01 22:03:23,5
13176170,Allow PATCH VDFs to Specify Services Which Are Not Installed in the Cluster,"AMBARI-21832 limited the flexibility of a PATCH VDF by requiring that the list of {{available-services}} match what is installed in the cluster. For example, if a cluster contained ZooKeeper and Storm, a patch VDF which specified Storm and Accumulo could not be registered.

Ambari should allow registration of a VDF without restricting it to the services which are currently installed in the cluster. In the above mentioned case, one concern would be what would happen if Accumulo was added after the patch was applied. In this case, Ambari should add Accumulo from the parent {{STANDARD}} repo. 

When a patch is reverted, Ambari must now check to ensure that a service included in that patch wasn't added after the patch was applied. Consider this scenario:
- Install a ZK only cluster
- Register and patch using a VDF with ZK, STORM
- Add Storm
- Revert the patch
- Re-apply the patch

When the patch is re-applied, the hosts will not have the new storm packages installed since the patch repository was distributed before Storm was a part of the cluster.",pull-request-available,[],AMBARI,Task,Critical,2018-08-01 15:37:52,31
13175935,Service theme call failing,"pon clicking service summary page themes call is failing. ../api/v1/stacks/ODS/versions/1.0.0-b483/services/ZOOKEEPER/themes?ThemeInfo/default=true

for ODS mpack zookeeper is being checked for theme info which is incorrect",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-31 19:32:59,34
13175858,Align the entries in drop downs of Ambari UI left hand side pane,"See   !Cluster-Admin.png!  - it would be good to right align the entries in the drop down

The Service pane looks better   !Services.png!  ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-31 14:46:50,9
13175857,Properties filter in Customize Services Page brings up LOGDIRS and PIDDIRS even if they dont qualify,- At customize services page - Directories tab - use properties filter to show 'Property issues' and 'Final properties'. In both cases LOGDIR and PIDDIR shows up as show in screenshot. There is no property in that panel but they do show up,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-31 14:45:06,9
13175854,Filter services eligible for Ambari Single Sign-on Configuration if Kerberos is required but not enabled,"Filter services from Ambari CLI when setting up SSO if not eligible when Kerberos is not enabled.  

In Ambari 2.7, services that are eligible for Ambari to manage their SSO configurations specify this in their metainfo file using like:

{code}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>application-properties/atlas.sso.knox.enabled</enabledConfiguration>
      </sso>
{code}

See AMBARI-23253
See [Ambari Single Sign-on Configuration|https://github.com/apache/ambari/blob/branch-2.7/ambari-server/docs/security/sso/index.md] documentation

However some services require Kerberos to be enabled for SSO to work.  For example, HDFS, Yarn, and Oozie.  For this case, the metadata is enhanced allowing for the metadata to indicate whether Kerberos is required (AMBARI-24335) and whether Kerberos is enabled (AMBARI-24384) for that service.

This information can be found in the service resource data

{code:title=GET /api/v1/clusters/CLUSTERNAME/services/OOZIE}
{
  ""href"" : ""http://ambari_host:8080/api/v1/clusters/CLUSTERNAME/services/OOZIE"",
  ""ServiceInfo"" : {
    ...
    ""kerberos_enabled"" : true,
    ...
   ""sso_integration_desired"": false,
   ""sso_integration_enabled"": false,
   ""sso_integration_requires_kerberos"": true,
   ""sso_integration_supported"": true,
   ...
   },
   ...
}
{code}

Using this information, services may be included in or excluded from the list of services a user can choose for enabling SSO integration. 

For example
||sso_integration_supported||sso_integration_requires_kerberos||kerberos_enabled||Can Enable SSO||
|true|true|true|yes
|true|true|false|no
|true|false|true|yes
|true|false|false|yes
|false|true|true|no
|false|true|false|no
|false|false|true|no
|false|false|false|no

  ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-31 14:35:43,18
13175846,Update Ambari Single Sign-on Configuration documentation to include Kerberos options,"Update Ambari Single Sign-on Configuration documentation to include Kerberos options.

See https://github.com/apache/ambari/blob/branch-2.7/ambari-server/docs/security/sso/index.md.",documentation pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-31 14:08:21,30
13175804,Install HSI using Add HiveServer2 Interactive button failing,Hide 'Add hiveserver interactive' option if 'Enable Interactive Query ' is disabled,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-31 10:59:30,7
13175802,Support YARN Application timeout feature in Ambari Capacity Scheduler View,"The CapacityScheduler supports the following parameters to lifetime of an application:

At high level, we need a mechanism to set timeout values for each leaf queue from Capacity Scheduler view. The sample configuration that should reflected in backend is


{code:java}
yarn.scheduler.capacity.root.default.maximum-application-lifetime=<values in seconds>
yarn.scheduler.capacity.root.default.default-application-lifetime=<values in seconds>
{code}


Capacity Scheduler view should support following tags.

description of this tags can be found in : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
",pull-request-available,['ambari-views'],AMBARI,New Feature,Major,2018-07-31 10:56:32,3
13175748,[Log Search] Input config validator unable to parse any log entry,"1. Login to Logsearch portal
2. Select ""Configuration Editor"" from the right upper corner menu
3. Select an input config from ""All Configuration"" (ambari)
4. Enter a ""Component Name"" on the ""Validator"" section (ambari_agent)
5. Enter a log entry to the ""Sample Data"" text area
{code}
INFO 2018-07-31 06:14:06,550 security.py:67 - SSL connection established. Two-way SSL authentication is turned off on the server.
{code}
6. Click ""Test""
7. The result is the configuration is not valid

Validator throws java.lang.NullPointerException
{code}
/var/log/ambari-logsearch-portal/logsearch.log
{code}

{code}
2018-07-31 06:27:54,674 [qtp1933799970-15] ERROR apache.ambari.logfeeder.plugin.input.Input (Input.java:193) - Error during filter apply: {}
java.lang.NullPointerException
        at org.apache.ambari.logfeeder.output.OutputManagerImpl.getDefaultLogLevels(OutputManagerImpl.java:165)
        at org.apache.ambari.logfeeder.output.OutputManagerImpl.write(OutputManagerImpl.java:147)
        at org.apache.ambari.logfeeder.plugin.filter.Filter.apply(Filter.java:121)
        at org.apache.ambari.logfeeder.filter.FilterGrok.applyMessage(FilterGrok.java:278)
        at org.apache.ambari.logfeeder.filter.FilterGrok.apply(FilterGrok.java:206)
        at org.apache.ambari.logfeeder.plugin.input.Input.outputLine(Input.java:191)
        at org.apache.ambari.logfeeder.common.LogEntryParseTester.parse(LogEntryParseTester.java:136)
        at org.apache.ambari.logsearch.manager.ShipperConfigManager.testShipperConfig(ShipperConfigManager.java:113)
        at org.apache.ambari.logsearch.rest.ShipperConfigResource.testShipperConfig(ShipperConfigResource.java:105)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161)
        at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:160)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
        at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)
        at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305)
        at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154)
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:473)
        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
        at org.eclipse.jetty.websocket.server.WebSocketUpgradeFilter.doFilter(WebSocketUpgradeFilter.java:215)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
        at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
        at org.apache.ambari.logsearch.web.filters.LogSearchLogLevelFilterManagerFilter.doFilter(LogSearchLogLevelFilterManagerFilter.java:79)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
        at org.apache.ambari.logsearch.web.filters.LogSearchConfigStateFilter.doFilter(LogSearchConfigStateFilter.java:83)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
        at org.apache.ambari.logsearch.web.filters.AbstractLogsearchGlobalStateFilter.doFilter(AbstractLogsearchGlobalStateFilter.java:77)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)
...
{code}",pull-request-available,['logsearch'],AMBARI,Bug,Blocker,2018-07-31 06:42:45,16
13175691,Unit test error in ambari-metrics-timelineservice: metric_blacklist.dat (No such file or directory),"Many branch-2.7 test runs from Github are failing with 

{noformat}
2018-07-30 21:33:23,838 ERROR [main] timeline.TimelineMetricsFilter (TimelineMetricsFilter.java:readMetricWhitelistFromFile(132)) - Unable to parse metric file
java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder%402/ambari-metrics/ambari-metrics-timelineservice/target/test-classes/test_data/metric_blacklist.dat (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilter.readMetricWhitelistFromFile(TimelineMetricsFilter.java:117)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilter.initializeMetricFilter(TimelineMetricsFilter.java:85)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testMetricBlacklisting(TimelineMetricsFilterTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:344)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:269)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:240)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:184)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:286)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:240)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2018-07-30 21:33:23,842 INFO  [main] timeline.TimelineMetricsFilter (TimelineMetricsFilter.java:initializeMetricFilter(86)) - Blacklisting 0 metrics
{noformat}
",unit-test,['ambari-metrics'],AMBARI,Bug,Critical,2018-07-30 22:22:09,21
13175604,Logic and declaration used to determine if Kerberos is enabled for a service,"Add logic and declaration used to determine if Kerberos is enabled for a service.

To support a robust method to determine whether Kerberos is enabled or not, a new attribute should be added - {{kerberosEnabledTest}}.  

The {{kerberosEnabledTest}} attribute is to contain a JSON document that can be _compiled_ into a {{org.apache.commons.collections.Predicate}} (ideally using {{org.apache.ambari.server.collections.PredicateUtils#fromJSON}}).  For example

{code}
<sso>
  <supported>true</supported>
  <kerberosRequired>true</kerberosRequired>
  ...
</sso>
<kerberosEnabledTest>
    {
      ""equals"": [
        ""service-properties/kerberos.enabled"",
        ""true""
      ]
    }
</kerberosEnabledTest>

{code}

{code}
<sso>
  <supported>true</supported>
  <kerberosRequired>true</kerberosRequired>
  ...
</sso>
<kerberosEnabledTest>
    {
      ""or"": [
        {
          ""equals"": [
            ""oozie-site/oozie.authentication.type"",
            ""kerberos""
          ]
        },
        {
          ""equals"": [
            ""oozie-site/oozie.authentication.type"",
            ""org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler""
          ]
        }
      ]
    }
  </kerberosEnabledTest>
{code}
The result of the test, is to be available via the services REST API:

{code:title=GET /api/v1/clusters/CLUSTERNAME/services/OOZIE}
{
  ""href"" : ""http://ambari_host:8080/api/v1/clusters/CLUSTERNAME/services/OOZIE"",
  ""ServiceInfo"" : {
    ...
    ""kerberos_enabled"" : true,
    ...
   },
   ...
}
{code}


",pull-request-available,[],AMBARI,Bug,Major,2018-07-30 16:25:41,30
13175588,A NPE occurs if a Kerberos identity descriptor is missing a Keytab specification,"A NPE occurs if a Kerberos identity descriptor is missing a Keytab specification.  For example, if the Kerberos identity descriptor is as follows:

{code}
{
  ""name"": ""druid"",
  ""principal"": {
    ""value"": ""druid_124@${realm}"",
    ""type"": ""user""
  }
}
{code}

{noformat:title=Log Message}
2018-07-30 12:58:33,332  WARN [Server Action Executor Worker 265] ServerActionExecutor:471 - Task #265 failed to complete execution due to thrown exception: java.lang.NullPointerException:null
java.lang.NullPointerException
	at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1603)
	at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:168)
	at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:94)
	at org.apache.ambari.server.serveraction.kerberos.PrepareKerberosIdentitiesServerAction.execute(PrepareKerberosIdentitiesServerAction.java:129)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

*Note*: This is technically an unexpected Kerberos identity specification since the identity will be unusable. However Ambari should not encounter a NPE because of it. 
",kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-30 15:24:41,13
13175493,Updating LDAPSettings section for Ranger in Advanced config section,"Adding {{ranger.ldap.group.searchbase}}, {{ranger.ldap.group.searchfilter}} configs in LDAP category. 

 

 ",pull-request-available,['ambari-sever'],AMBARI,Bug,Major,2018-07-30 08:32:53,25
13175355,Adding services when Kerberos is enabled incorrectly changes unrelated service configurations,"Adding services when Kerberos is enabled incorrectly changes unrelated service configurations.  For example, {{kerberos-env/service_check_principal_name}} is changed from ""{{$\{cluster_name|toLower()\}-$\{short_date\}}}"" to a concrete value like ""{{c1-072818}}"".

This is a regression created with the resolution of [AMBARI-23292|https://issues.apache.org/jira/browse/AMBARI-23292].

",kerberos pull-request-available regresion,['ambari-server'],AMBARI,Bug,Critical,2018-07-28 18:43:05,30
13175233,Check Keytabs Kerberos Client step failing while re-dding a service via UI,"*STR*

install ambari-2.7.0.1-11

deploy a kerberized cluster (i.e. with ZK and Atlas)

remove one of the services (I removed Atlas)

add that service again in via UI

*Result*

The following error occurred at the Check Keytabs Kerberos Client task:
{code:java}
stderr: 
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KERBEROS/package/scripts/kerberos_client.py"", line 91, in 
    KerberosClient().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KERBEROS/package/scripts/kerberos_client.py"", line 87, in check_keytabs
    find_missing_keytabs(params, output_hook)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/kerberos/kerberos_common.py"", line 166, in find_missing_keytabs
    missing_keytabs = MissingKeytabs.from_kerberos_records(params.kerberos_command_params, params.hostname)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/kerberos/kerberos_common.py"", line 55, in from_kerberos_records
    with_missing_keytab = (each for each in kerberos_record \
TypeError: 'NoneType' object is not iterable
 stdout:


Command failed after 1 tries{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2018-07-27 17:22:29,27
13175179,How alert count is presented for config errors/warnings/suggestions confusing and misleading,See screenshots,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-07-27 14:41:45,19
13175021,Fix integration test regressions in AMS collector due to scale changes.,"* Extend HBaseTestingUtility to spin up standalone HBase cluster for integraton testing instead of distributed mode instance with HDFS.
* Fix integration test regressions from 2.7.0.
",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-07-26 23:19:10,39
13175020,Honor zeppelin.livy.url setting as an interpreter setting if specified,The resolved value for this settings from Ambari auto update is not good enough in certain scenarios. It just uses the first host where Livy server is running. We want to have the flexibility to honor an explicit configuration for this setting.,pull-request-available,['stacks'],AMBARI,Improvement,Major,2018-07-26 23:13:36,24
13175014,Add livy-client.conf setting to Ambari spark stack,"We have run into a need to add livy-client.conf to Ambari's capability. For example we were trying to configure ""livy.rsc.launcher.address"" in this setting file.",pull-request-available,['stacks'],AMBARI,Improvement,Major,2018-07-26 22:35:13,24
13175004,Mpack Instance Manager Not Listing Versions Correctly,"Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDPCORE/1.0.0-b450/services/ZOOKEEPER/package/scripts/zookeeper_server.py"", line 124, in <module>
    ZookeeperServer().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDPCORE/1.0.0-b450/services/ZOOKEEPER/package/scripts/zookeeper_server.py"", line 106, in status    import status_params
  File ""/var/lib/ambari-agent/cache/stacks/HDPCORE/1.0.0-b450/services/ZOOKEEPER/package/scripts/status_params.py"", line 43, in <module>
    component_instance_name=component_instance_name)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/mpack_manager_helper.py"", line 45, in get_component_conf_path    return conf_json[COMPONENTS_PLURAL_KEY_NAME][components_instance_type.lower()][COMPONENT_INSTANCES_PLURAL_KEY_NAME][
KeyError: 'components'",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-26 21:22:20,44
13174918,Fixes for modal with config validations and dependent properties,"Dependent configs table:
- Checkboxes should be located on the left side
- Properties names hyperlinks are broken; remove the links and add the tooltip with property description on hover instead
- Make modal wider
- Move modal closer to the top

Config validations results table:
- Limit the width and wrap the contents of 'Current Value' column
- Make modal wider",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-26 16:39:48,15
13174876,Make upgrade progress counter more readable,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-26 14:51:22,19
13174809,Back and forth on All Configurations Page has multiple tabs marked as active,"In ambari-2.7.1.0-11
# Navigating to All configurations page in installer, 
# selecting multiple services
# Navigate to Directories section
# Navigate back to All configs page.

Multiple services are marked as active in All Configuration page leading to confusion
 !Screen Shot 2018-07-25 at 11.19.37 PM.png|thumbnail! ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-26 11:42:27,19
13174338,[Log Search UI] wrong filter label on open log tab,"Steps to reproduce:
 - Set date (Like Today) on filter tab
 - go to the logs table and click on open log tab
 - on the new tab, the date filer shows a NaN value",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-07-25 10:57:18,26
13174321,Log Feeder: generate solr id from specific fields,"usage (in output.json configs)
{code:json}
{
  outputs: [
  {
  ""id_fields"" : [
  ""log_message"", ""logtime"", ""type"", ""path"" 
  ]
  }
]
{code}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-07-25 09:54:09,29
13174298,Updating Ranger and Ranger KMS configs during Ambari upgrade,"*For Ranger*

Updating Ranger Admin: {{ranger.logs.base.dir/ranger-admin-site.xml}} and Ranger Usersync: {{ranger.usersync.logdir/ranger-ugsync-site.xml}} log directory configs in stack.



*For Ranger KMS*
 Adding new config {{ranger_kms_privelege_user_jdbc_url/kms-env.xml}} to have Test Connection button for verify communication to selected database using database root credentials.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-25 08:43:39,25
13174280,Logic and declaration used to determine if SSO is enabled for a service needs to be able to handle more than a boolean property,"Logic and declaration used to determine if SSO is enabled for a service needs to be able to handle more than a boolean property.

The current way Ambari determines whether SSO is enabled for a service or not is by getting the value of the property indicated in {{sso/enabledConfiguration}} property in the service's metadata:
{code:java|title=Example}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>service-properties/sso.knox.enabled</enabledConfiguration>
      </sso>
{code}
Using the above example, the {{service-properties/sso.knox.enabled}} is checked to see if its value is ""true"" or ""false"".

This method works for a few services, but other services require more elaborate checks. For example Oozie relies on the value of {{oozie-site/oozie.authentication.type}}. If the value is ""org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler"", then SSO is enabled; otherwise it is not. This is different that just a Boolean value.

*Solution*

To support a more robust method to determine whether SSO is enabled or not, a new attribute should be added - {{ssoEnabledTest}}. The existing attribute, {{enabledConfiguration}}, should be available for backward compatibility - but converted on the backend.

The {{ssoEnabledTest}} attribute is to contain a JSON document that can be _compiled_ into a {{org.apache.commons.collections.Predicate}} (ideally using {{org.apache.ambari.server.collections.PredicateUtils#fromJSON}}). For example
{code:java}
      <sso>
        <supported>true</supported>
        <ssoEnabledTest>
          {
            ""equals"": [
              ""service-properties/sso.knox.enabled"",
              ""true""
            ]
          }
      </sso>
{code}
{code:java}
      <sso>
        <supported>true</supported>
        <ssoEnabledTest>
          {
            ""equals"": [
              ""oozie-site/oozie.authentication.type"",
              ""org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler""
            ]
          }
        </ssoEnabledTest>
      </sso>{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-25 06:32:49,27
13174176,UI service configs page hangs.,"UI code fails :
{code:java}
app.js:1898 Uncaught TypeError: Cannot read property 'always' of undefined
    at Class.<anonymous> (app.js:1898)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}
at *line 6* while processing the response:
{code:java}
1  loadConfigProperties: function loadConfigProperties() {
2    var self = this;

3    App.config.loadConfigsFromStack(App.Service.find().mapProperty('serviceName')).always(function () {
4      App.config.loadClusterConfigsFromStack().always(function () {
5        App.router.get('configurationController').updateConfigTags().always(function () {
6          App.router.get('updateController').updateClusterEnv().always(function () {
7            self.set('isConfigsPropertiesLoaded', true);
8          });
        });
      });
    });
  },
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-24 18:45:16,34
13174129,Rescheduled and canceled tasks stay in progress forever,"1\. Ambari-server reschedules task (timeout #1)  
2\. Task did yet got rescheduled (due to something else being in queue)  
3\. but, ambari-server cancels it (timeout #2)  
4\. The tasks keeps reprorting that's it's in progess forever, however being
canceled

",pull-request-available,[],AMBARI,Bug,Major,2018-07-24 15:32:25,5
13174123,Update description of Save Admin Credentials,Change tooltip description to 'Ambari must be configured to encrypt the passwords stored in Ambari before you can save admin credentials',pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-24 15:10:44,7
13174103,Host Details page: alignment issues,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-24 13:27:12,19
13174028,[LogSearch] Wrong (error) response format,"When I call the 
{noformat}
/shipper/input/\{clusterName}/services/\{serviceName}
{noformat}
 api endpoint with wrong json schema the backend responses with plain text body instead of json or empty body.
For example:


{code:json}
{
 ""input"": [
 {
 ""type"": ""zookeeper"",
 ""rowtype"": ""service"",
 ""path"": ""/var/log/zookeeper/zookeeper*.log""
 }
 ],
 ""filter"": [
 {
 ""filter"": ""grok"",
 ""conditions"": {
 ""fieldss"": {
 ""type"": [
 ""zookeeper""
 ]
 }
 },
 ""skipOnError"": false,
 ""deepExtract"": false,
 ""post_map_values"": {
 ""logtime"": [
 {
 ""map_date"": {
 ""target_date_pattern"": ""yyyy-MM-dd HH:mm:ss,SSS""
 }
 }
 ]
 },
 ""log4j_format"": ""%d\{ISO8601} - %-5p [%t:%C\{1}@%L] - %m%n"",
 ""multiline_pattern"": ""^(%\{TIMESTAMP_ISO8601:logtime})"",
 ""message_pattern"": ""(?m)^%\{TIMESTAMP_ISO8601:logtime}%\{SPACE}-%\{SPACE}%\{LOGLEVEL:level}%\{SPACE}\\[%\{DATA:thread_name}\\@%\{INT:line_number}\\]%\{SPACE}-%\{SPACE}%\{GREEDYDATA:log_message}""
 }
 ]
}
{code}


Where the `fieldss` is not a valid JSON key.

The response plain text body:
{code:javascript}
Unrecognized field ""fieldss"" (class org.apache.ambari.logsearch.model.common.LSServerConditions), not marked as ignorable (one known property: ""fields""])
 at [Source: UNKNOWN; line: -1, column: -1] (through reference chain: org.apache.ambari.logsearch.model.common.LSServerInputConfig[""filter""]->org.apache.ambari.logsearch.model.common.LSServerFilterGrok[""conditions""]->org.apache.ambari.logsearch.model.common.LSServerConditions[""fieldss""])
{code}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-07-24 08:02:34,16
13173945,"PostTrunkMerge : UI issue at Step 7 of deploy, while getting configs for service.","*ISSUE:*

Post Trunk merge, deploy fails at Step 7.

*Console O/P:*
{code:java}
app.js:67915 Uncaught TypeError: Cannot read property 'get' of undefined
    at Class.updateAttributesFromTheme (http://172.22.107.63:8080/javascripts/app.js:67915:90)
    at http://172.22.107.63:8080/javascripts/app.js:44914:21
    at Array.forEach (native)
    at Class.updateConfigAttributesFromThemes (http://172.22.107.63:8080/javascripts/app.js:44913:41)
    at Class.applyServicesConfigs (http://172.22.107.63:8080/javascripts/app.js:43405:10)
    at Class.loadStep (http://172.22.107.63:8080/javascripts/app.js:43345:12)
    at Class.willInsertElement (http://172.22.107.63:8080/javascripts/app.js:249561:30)
    at Class.newFunc [as willInsertElement] (http://172.22.107.63:8080/javascripts/vendor.js:12954:16)
    at Class.trigger (http://172.22.107.63:8080/javascripts/vendor.js:25526:21)
    at Class.newFunc [as trigger] (http://172.22.107.63:8080/javascripts/vendor.js:12954:16)
{code}
Code line:
{code:java}
  updateAttributesFromTheme: function updateAttributesFromTheme(serviceName) {
    this.prepareSectionsConfigProperties(serviceName);
    var serviceConfigs = this.get('stepConfigs').findProperty('serviceName', serviceName).get('configs'),  <-- Issue LINE
      
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-23 22:46:18,34
13173871,AMS Migration tools should auto-detect whitelist file,"In a default Ambari 2.7 cluster that has just completed HDP 2.6 to HDP 3.0 upgrade, running the following command with no arguments fails:

{noformat}
# /usr/sbin/ambari-metrics-collector upgrade_start
{noformat}

This is the error in the logs:
{noformat}
2018-07-12 13:47:05,373 INFO org.apache.ambari.metrics.core.timeline.discovery.TimelineMetricMetadataManager: Retrieved 9504, metadata objects from store.
2018-07-12 13:47:05,517 INFO org.apache.ambari.metrics.core.timeline.discovery.TimelineMetricMetadataManager: Retrieved 5 host objects from store.
2018-07-12 13:47:05,528 INFO org.apache.ambari.metrics.core.timeline.upgrade.core.MetricsDataMigrationLauncher: Looking for whitelisted metric names...
2018-07-12 13:47:05,529 ERROR org.apache.ambari.metrics.core.timeline.upgrade.core.MetricsDataMigrationLauncher: No whitelisted metrics specified. Exiting...
2018-07-12 13:47:05,530 ERROR org.apache.ambari.metrics.core.timeline.upgrade.core.MetricsDataMigrationLauncher: Exception during system setup, exiting...
java.lang.Exception: List of whitelisted metrics must be provided
        at org.apache.ambari.metrics.core.timeline.upgrade.core.MetricsDataMigrationLauncher.<init>(MetricsDataMigrationLauncher.java:113)
        at org.apache.ambari.metrics.core.timeline.upgrade.core.MetricsDataMigrationLauncher.main(MetricsDataMigrationLauncher.java:303)
{noformat}",pull-request-available,[],AMBARI,Bug,Critical,2018-07-23 17:34:19,39
13173827,Background Operations: minor UX changes,"See screenshots

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-23 14:40:55,19
13173777,Update service metainfo to declare Kerberos is required for SSO integration support,"Update service metainfo to declare Kerberos is required for SSO integration support.

The metainfo file allow for a service to indicate that is supports SSO integration.
{code:java|title=Example}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>service-site/knox.sso.enabled</enabledConfiguration>
      </sso>
{code}
However, some services required that Kerberos is enabled to support SSO. This needs to be indicated in the metainfo so that Ambari knows how to behave properly.
{code:java|title=Example}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>service-site/knox.sso.enabled</enabledConfiguration>
        <kerberosRequired>true</kerberosRequired>
      </sso>
{code}
Along with this change, the following API request needs to be updated to supply the relevant data:
{noformat:title=Get stack service details}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME"",
{
  ""href"" : "":URL"",
  ""StackServices"" : {
     ...
     ""sso_integration_supported"": ""true"",
     ""sso_integration_requires_kerberos"": ""true"",
     ...
  },
  ...
{noformat}
{noformat:title=Get installed service information}
GET /api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME
{
  ""href"" : "":URL"",
  ""ServiceInfo"" : {
    ""cluster_name"" : "":CLUSTER_NAME"",
    ...
    ""sso_integration_supported"": ""true"",
    ""sso_integration_requires_kerberos"": ""true"",
    ""sso_integration_enabled"": ""false"",
    ""sso_integration_desired"": ""false"",
     ...
    },
    ...
{noformat}
{noformat:title=List installed services that support SSO integration only when Kerberos to be enabled}
GET /api/v1/clusters/:CLUSTER_NAME/services?ServiceInfo/sso_integration_supported=true&ServiceInfo/sso_integration_requires_kerberos=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}
{noformat:title=List stack services that support SSO integration only when Kerberos is enabed}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services?StackServices/sso_integration_supported=true&StackServices/sso_integration_requires_kerberos=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-23 11:23:46,27
13173774,User is able to set the same short URLs for different view instances,"*STR*
 # Let's say the user has CAPACITY-SCHEDULER view instance with {{/main/view/CAPACITY-SCHEDULER/auto_cs_instance}} short URL set
 # Clone this instance or create another CAPACITY-SCHEDULER instance manually
 # Set the same short URL as for the first instance

*Result*
 - If 'Name' property is the same, request for adding new URL fails
request:
{noformat}
POST api/v1/view/urls/auto_cs_instance

{
    ""ViewUrlInfo"": {
        ""url_name"": ""auto_cs_instance"",
        ""url_suffix"": ""auto_cs_instance"",
        ""view_instance_version"": ""1.0.0"",
        ""view_instance_name"": ""AUTO_CS_INSTANCE_Copy"",
        ""view_instance_common_name"": ""CAPACITY-SCHEDULER""
    }
}
{noformat}
response:
{noformat}
{
  ""status"" : 500,
  ""message"" : ""An internal system exception occurred: This view URL name exists, URL names should be unique""
}
{noformat}

 - If name is different, the REST call succeeds even if URL itself is duplicate:
{noformat}
POST api/v1/view/urls/auto_cs_instance1

{
    ""ViewUrlInfo"": {
        ""url_name"": ""auto_cs_instance1"",
        ""url_suffix"": ""auto_cs_instance"",
        ""view_instance_version"": ""1.0.0"",
        ""view_instance_name"": ""AUTO_CS_INSTANCE_Copy"",
        ""view_instance_common_name"": ""CAPACITY-SCHEDULER""
    }
}
{noformat}
response is 201 in this case; URL is created

 - As result there are two view instances with the same URL, and the first one is no longer accessible by this URL
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-23 11:14:24,27
13173655,Log Feeder define default log levels per component,"Right now we can define a global default log level, make this work per component by defining default log levels in input configs.

example:
{code:json}
{""inputs"" [
  {
  ""type"": ""logsearch_server"",
  ""default_log_levels"" : [
     ""WARN"", ""DEBUG""
   ],
  ....
  }
]
{code}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-07-22 11:09:25,29
13173544,PostTrunkMerge : UI Host page API call is broken.,"*ISSUE:*

Post Trunk merge, Host API call breaks.

API call:

http://172.27.25.196:8080/api/v1/clusters/cl1/hosts?fields=Hosts/rack_info,Hosts/host_name,Hosts/maintenance_state,Hosts/public_host_name,Hosts/cpu_count,Hosts/ph_cpu_count,alerts_summary,Hosts/host_status,Hosts/host_state,Hosts/last_heartbeat_time,Hosts/ip,host_components/HostRoles/state,host_components/HostRoles/maintenance_state,host_components/HostRoles/stale_configs,host_components/HostRoles/service_name,host_components/HostRoles/display_name,host_components/HostRoles/desired_admin_state,host_components/metrics/dfs/namenode/ClusterId,host_components/metrics/dfs/FSNamesystem/HAState,Hosts/total_mem,stack_versions/HostStackVersions,stack_versions/repository_versions/RepositoryVersions/repository_version,stack_versions/repository_versions/RepositoryVersions/id,stack_versions/repository_versions/RepositoryVersions/display_name&minimal_response=true,host_components/logging&page_size=10&from=0&sortBy=Hosts/host_name.asc&_=1532118655459

Response:

{code}
{
  ""status"" : 400,
  ""message"" : ""The properties [stack_versions/repository_versions/RepositoryVersions/repository_version, stack_versions/HostStackVersions, stack_versions/repository_versions/RepositoryVersions/display_name, stack_versions/repository_versions/RepositoryVersions/id] specified in the request or predicate are not supported for the resource type Host.""
}
{code}
 !Screen Shot 2018-07-20 at 1.32.48 PM.png! ",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-07-20 20:20:26,43
13173515,Zeppelin server is shown as started even if it fails during start up,"Steps to reproduce:
1) Set incorrect values in shiro.ini. For example set the value of ldapRealm.hadoopSecurityCredentialPath to incorrect path
2) Restart zeppelin.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-20 18:25:44,24
13173483,"Upgrade creates flume kerberos principal, after removing flume service","
HDP Upgrade appears to create an additional flume principal, after ambari removes the service (as flume is no longer supported in hdp 3.0.0).  This requirement can be problematic for clusters integrated with kerberos using manual management.

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-20 15:45:55,13
13173453,[Log Search UI] Unaccessible query filter type ahead element in small window,"* Click into the query filter bar input field
* A type ahead dropdown will appear
* When the user's screen is small the bottom of the dropdown is not visible

Expected behaviour: All the dropdown element should be available for selection.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Minor,2018-07-20 12:55:31,26
13173437,install tasks sometimes fail,"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ZOOKEEPER/package/scripts/zookeeper_client.py"", line 81, in <module>
        ZookeeperClient().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ZOOKEEPER/package/scripts/zookeeper_client.py"", line 61, in install
        self.install_packages(env)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 836, in install_packages
        retry_count=agent_stack_retry_count)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 30, in action_install
        self._pkg_manager.install_package(package_name, self.__create_context())
      File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/yum_manager.py"", line 219, in install_package
        shell.repository_manager_executor(cmd, self.properties, context)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 742, in repository_manager_executor
        call_result = subprocess_executor(cmd, timeout=-1, env=env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 446, in subprocess_executor
        lines = [line for line in output]
      File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
        self.gen.next()
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 528, in process_executor
        kill_timer.join()
      File ""/usr/lib64/python2.7/threading.py"", line 940, in join
        raise RuntimeError(""cannot join thread before it is started"")
    RuntimeError: cannot join thread before it is started
    

",pull-request-available,[],AMBARI,Bug,Major,2018-07-20 12:02:03,5
13173416,Log Search / Ambari upgrade: db config consistency check has warnings (*-logearch-conf configs),"{code:java}
2018-07-11 15:09:54,135  WARN - You have config(s): ams-logsearch-conf-version1,zookeeper-logsearch-conf-version1,hbase-logsearch-conf-version1,infra-logsearch-conf-version1,hdfs-logsearch-conf-version1,mapred-logsearch-conf-version1530850353441,logfeeder-custom-logsearch-conf-version1,atlas-logsearch-conf-version1,kafka-logsearch-conf-version1,yarn-logsearch-conf-version1530850353441 that is(are) not mapped (in serviceconfigmapping table) to any service!
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-20 10:16:22,29
13173412,Fix background colors on pages,"The title should have a white background on Stack and Versions -> Stack page and Service Accounts page.

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-20 09:30:49,19
13173398,SET_KEYTABS is preformed twice per host during Kerberos operations,"*STR*
* Enable Kerberos and check the DB after the 'Test Kerberos' step, or
* Regenerate keytabs on a secure cluster with the option 'Only regenerate keytabs for missing hosts and components' enabled (this is important, otherwise the issue is not reproducible)

*Result:*

{noformat}
SELECT hrc.task_id, hrc.request_id, hrc.host_id, hrc.stage_id, st.request_context, hrc.status, hrc.role, hrc.role_command, hrc.custom_command_name FROM host_role_command hrc, stage st WHERE hrc.stage_id = st.stage_id AND hrc.request_id = st.request_id AND hrc.custom_command_name = 'SET_KEYTAB' ORDER BY request_id, host_id, stage_id;{noformat}

||task_id||request_id||host_id||stage_id||request_context||status||role||role_command||custom_command_name||
|63|17|1|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|68|17|1|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|64|17|2|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|69|17|2|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|65|17|3|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|70|17|3|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|83|19|1|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|88|19|1|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|84|19|2|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|89|19|2|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|85|19|3|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|90|19|3|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|

*Expected result:*
only one SET_KEYTAB command should be recorded",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-20 08:28:40,27
13173394,"Multiple alerts after HDFS service only regenerate keytabs, as keytabs are out of sync","*STR*
 # Install Ambari 2.7.0
 # Deploy a cluster with some services
 # Execute to HDFS / Actions / Regenerate Keytabs
 # Restart all required components

*Result*

Multiple alerts have been shown saying they receive 403 when trying to connect to different web UIs

*Expected result*

No alerts; everything should work as expected",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-20 08:22:07,30
13173242,Report Mpack Installation State When Installing Packages For Upgrade,"When reporting mpack installation success in preparation for an upgrade, a normal {{install}} command is not used. Instead, a custom action is used. This action needs to work along with {{Script}} in order to report the mpack installation information.

- The success/failure of the installation should be based off of the command's status, not some weird property like {{package_installation_result}}
- Some version reporting code that is no longer used can also be removed",pull-request-available,[],AMBARI,Task,Blocker,2018-07-19 16:35:47,31
13173222,Lowercase g characters in the left-hand nav are getting slightly clipped bottom-up making them look like lowercase q's,We need to make sure the service names have enough vertical padding or ensure that they don't get clipped.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-19 15:33:22,9
13173220,Inconsistent Ambari warnings,"STR:

#  Go to Hive/Configs
#  Enable Interactive Query, select the host, Save
#  6 Warnings are coming up, though the REST API call returned only 2, the other 4 are produced by Ambari Web
#  Click on cancel, discard the changes.
#  Go to the Summary, tab, then back to the Configs tab, and do steps #1-#3 again
#  This time only the two warnings are displayed, Ambari Web doesn't generates those extra 4 (desired outcome)
#  Log out and log in again, and the same starts again, for the first modification you'll receive 4 extra warnings.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 15:28:19,19
13173216,Clicking on password warnings doesn't update the service tab navigation,"When choosing an insufficient password like 'admin' for services, we're seeing the critical warning for password strength which is what we want, but when you click on the property to navigate to it, the service tab navigation is not getting updated so it ""looks"" like you're always in the HDFS service.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 15:09:05,9
13173204,UI Elements in Incorrect layout at Customize Services Page,"- Please see screenshots attached
- After a config group is created, message 'You are changing not default config group,' is towards the right side with no formatting
- Switch to Misc tab, dfs.permissions.superusergroup has a longer textbox
- Undo icon in Misc tab is below the textbox for some elements where as it is to the right for others 
- closing bracket for Services should be shrinked if there are less number of services

Screenshot attached for all 4",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 14:15:49,19
13173194,"Service disappear from the UI when Going back to ""Customize Services"" page or doing page refresh","Here is the STR:
- install isilon mpack
- go through the cluster creation wizard and select onefs service
- on the configuration page check that there are onefs related properties (for example smart connect zone name)
- either hit f5 or go to the next page then go back
- on the configuration page there will be no onefs related properties any more
- if you continue the deployment this way it will fail at the end
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-19 13:49:49,19
13173191,[LogSearch UI] Audit screen sub tab changes clear the Summary charts.,"* Select 'Audit logs' tab
 * Change from Summary sub tab to Logs sub tab
 * Change back to Summary sub tab
 * The charts disappear

The charts should be visible when the user selects the Summary tab any time.",pull-request-available,"['ambari-logsearch', 'logsearch']",AMBARI,Bug,Critical,2018-07-19 13:41:52,26
13173189,Host and Alerts page style fixes,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-19 13:33:27,19
13173183,UI sends host check request for custom jdk with invalid hosts,"UI sends ""java_home_check"" host check with all populated hosts instead hosts with passed registration only. This can cause 500 server exception due HostNotFoundException.
{code}
2018-07-05 14:10:30,930 ERROR [ambari-client-thread-573] AbstractResourceProvider:295 - Caught AmbariException when creating a resource
org.apache.ambari.server.HostNotFoundException: Host not found, hostname=test-0000
        at org.apache.ambari.server.state.cluster.ClustersImpl.getHost(ClustersImpl.java:456)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:189)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:173)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.findConfigurationTagsWithOverrides(AmbariManagementControllerImpl.java:2353)
{code}
{code}
POST http://host:8080/api/v1/requests 500 (Internal Server Error)
send @ vendor.js:8630
jQuery.extend.ajax @ vendor.js:8082
send @ app.js:192735
doCheckJDK @ app.js:41241
(anonymous function) @ app.js:41318
fire @ vendor.js:1141
self.fireWith @ vendor.js:1252
done @ vendor.js:8178
callback @ vendor.js:8702
{code}
Body of request:
{code}
{""RequestInfo"":{""context"":""Check hosts"",""action"":""check_host"",""parameters"":{""threshold"":""60"",""java_home"":""/tmp/jdk1.8.0_171/"",""jdk_location"":""http://host:8080/resources"",""check_execute_list"":""java_home_check""}},""Requests/resource_filters"":[{""hosts"":""test-0000,test-0001,test-0002,...
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 12:51:39,15
13173170,'Changed properties' filter for configs is displayed even in non-compare mode,"Filtering configs by changed properties should be available in versions comparison mode only, but as of now it's displayed always, not filtering out anything in regular mode.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 12:00:07,15
13173147,convert hive table upgrade step fails with hive db priv issue,"convert hive table step fails due to improper classpath, hadoop-hdfs should be added, and conf/conf.server should be used",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-19 09:41:46,36
13173022,-Dhdp.version shows blank value in process output for Datanodes,"# When we check the output of {{ps -ef | grep SecureDataNodeStarter}} it shows multiple instances of {{-Dhdp.version}} being blank/empty and some having the right value as shown below:

{quote}hdfs     40829 40798  1 14:11 ?        00:00:18 jsvc.exec -Dproc_datanode -outfile /hdplogs/hadoop/hdfs/jsvc.out -errfile /hdplogs/hadoop/hdfs/jsvc.err -pidfile /var/run/hadoop/hdfs/hadoop_secure_dn.pid -nodetach -user hdfs -cp /usr/hdp/current/hadoop-client/conf:/usr/hdp/2.6.3.0-235/hadoop/lib/*:/usr/hdp/2.6.3.0-235/hadoop/.//*:/usr/hdp/2.6.3.0-235/hadoop-hdfs/./:/usr/hdp/2.6.3.0-235/hadoop-hdfs/lib/*:/usr/hdp/2.6.3.0-235/hadoop-hdfs/.//*:/usr/hdp/2.6.3.0-235/hadoop-yarn/lib/*:/usr/hdp/2.6.3.0-235/hadoop-yarn/.//*:/usr/hdp/2.6.3.0-235/hadoop-mapreduce/lib/*:/usr/hdp/2.6.3.0-235/hadoop-mapreduce/.//* -Xmx1024m {color:#14892c}*-Dhdp.version=2.6.3.0-235*{color} -Djava.net.preferIPv4Stack=true {color:#d04437}-Dhdp.version= {color}-Djava.net.preferIPv4Stack=true {color:#d04437}-Dhdp.version= {color}-Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/hdplogs/hadoop/ -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.6.3.0-235/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/2.6.3.0-235/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.3.0-235/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true {color:#14892c}*-Dhdp.version=2.6.3.0-235*{color} -Dhadoop.log.dir=/hdplogs/hadoop/ -Dhadoop.log.file=hadoop-hdfs-datanode-xxxxx.log -Dhadoop.home.dir=/usr/hdp/2.6.3.0-235/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,RFA -Djava.library.path=:/usr/hdp/2.6.3.0-235/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.3.0-235/hadoop/lib/native:/usr/hdp/2.6.3.0-235/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.3.0-235/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/hdplogs/hadoop/hdfs -Dhadoop.id.str=hdfs -jvm server -server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:ErrorFile=/hdplogs/hadoop/hdfs/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/hdplogs/hadoop/hdfs/gc.log-201807051411 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms1024m -Xmx1024m -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:ErrorFile=/hdplogs/hadoop/hdfs/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/hdplogs/hadoop/hdfs/gc.log-201807051411 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms1024m -Xmx1024m -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:ErrorFile=/hdplogs/hadoop/hdfs/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/hdplogs/hadoop/hdfs/gc.log-201807051411 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms1024m -Xmx1024m -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter{quote}

A few params have been repeated multiple times as seen above, example {{-Dhdp.version}} and {{-Djava.net.preferIPv4Stack}}

It will be great to fix this so it appears consistent.


As a temporary workaround, I was able to bypass this issue by updating hadoop-env template from Ambari > HDFS > Configs > Advanced > Advanced hadoop-env
Old Value: {{export HADOOP_OPTS=""-Dhdp.version=$HDP_VERSION $HADOOP_OPTS""}}
Updated Value: {{export HADOOP_OPTS=""-Dhdp.version=`hdp-select --version` $HADOOP_OPTS""}}

P.S. I also checked this on a HDP-2.6.2 cluster and this problem was not seen there.

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-18 20:04:19,13
13172861,[Yarn Queue Manager] Yarn Queue manager View is resetting value some properties that are not defined UI,"Whene I add this properly in yarn scheduler 
>>> yarn.scheduler.capacity.root.default.maximum-allocation-mb=2048
>>> yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4

then restarting the required services and refreshing the queue scheduler at this time properties are present.
But when I want to do some changes in YARN queue manager like adding a Child Node, At this time it'll ask for saving and refresh, after that, I cannot see this 2 properties.

So Ambari-Yarn-queue-manager view is resetting this configs.

",pull-request-available,[],AMBARI,Bug,Major,2018-07-18 08:34:02,3
13172749,Ambari preupload script error zeppelin dependecies,"File ""/var/lib/ambari-server/resources/scripts/Ambaripreupload.py"", line 439, in <module>

File ""/var/lib/ambari-server/resources/scripts/Ambaripreupload.py"", line 284, in copy_zeppelin_dependencies_to_hdfs

copy_tarballs_to_hdfs(spark_deps_full_path[0], hdfs_path_prefix+'/apps/zeppelin/', 'hadoop-mapreduce-historyserver', params.hdfs_user, 'zeppelin', 'zeppelin')

*TypeError: copy_tarballs_to_hdfs() takes exactly 5 arguments (6 given)*",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-17 19:50:11,10
13172640,Show full JDBC URL's for Hive,"Instead of ellipsis, we should use forced breaks. Also, we should align elements better.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-17 12:37:11,7
13172629,[Yarn Queue Manager] Yarn Queue manager is allowing to Create a Child Queue for queue-mappings enabled Queue and Save it,"Yarn Queue manager is allowing to Create a Child Queue for queue-mappings enabled Queue and Save the changes
But the Operating is Failing as Yarn is having a check that  Queue-mappings should only be allowed for Leaf queue 

Code which validates whether queue-mappings are enabled only for leaf-queue ( YARN CODE)
{code:java}
private static QueueMapping validateAndGetQueueMapping(
      CapacitySchedulerQueueManager queueManager, CSQueue queue,
      QueueMapping mapping, QueuePath queuePath) throws IOException {
    if (!(queue instanceof LeafQueue)) {
      throw new IOException(
          ""mapping contains invalid or non-leaf queue : "" + mapping.getQueue());
    }
{code}

The buggy code in [Ambari Yarn Queue Manager which needs rework]: 

https://github.com/apache/ambari/blob/79cca1c7184f1661236971dac70d85a83fab6c11/contrib/views/capacity-scheduler/src/main/resources/ui/app/controllers/queues.js#L410

{code:java}

 hasInvalidMapping = queues.filter(function(queue){
            return !queue.get(""queues""); //get all leaf queues
          }).map(function(queue){
            return queue.get(""name"");
          }).indexOf(mapping[2]) == -1;
{code}
",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-07-17 12:00:46,3
13172365,Start All Services on 100-nodes cluster timed out after 1 hour,"ambari-server with 16 core CPU, 32GB RAM, fake agents on 2 hosts (16GB RAM, 2 core CPU) - 50 agents on each host, Mysql db on last host
 # Deploy a PERF cluster with 100 fake agents
 # Initiate Stop All components
 # Initiate Start All components

Observations:
 # As part of initial deploy and start services, the Start All Services call took about 6 minutes.
 # The first call to Stop All Services took about 23 minutes.
 # The subsequent call to Start All services timed out after 1 hour 1 minute.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-16 10:51:45,6
13172063,Remove org.apache.directory.api:api-ldap-model from Ambari server's dependencies due to security concerns,"Remove {{org.apache.directory.api:api-ldap-model}} from Ambari server's dependencies due to security concerns regarding the following CVE:

* CVE-2018-1337: Plaintext Password Disclosure in Secured Channel

See https://cve.mitre.org/cgi-bin/cvename.cgi?name=2018-1337

Though Ambari server includes {{api-ldap-model-1.0.0.jar}} in {{/usr/lib/ambari-server}}, the library is not used.  Therefore, the vulnerability is not exposed and the library may be excluded from Ambari's package. 
",cleanup pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-13 15:19:20,30
13171941,Enabling Hive Server Interactive doesn't work with ONEFS,"The following popup prevents enabling HSI:

HDFS service should be added to the cluster to enable interactive query (requires yarn pre-emption)

The check is located in assign_master_controller.js/showPopup.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-13 08:35:24,18
13171747,[Log Search UI] Make the app header aligned with the design,Fix the font styles and the background colors in the application header.,pull-request-available,['logsearch'],AMBARI,Bug,Minor,2018-07-12 14:52:18,26
13171522,DB consistency warning due to config group without service name,"STR:

1. Deploy ZooKeeper with Ambari 2.5.2
2. Create config group on UI
3. Upgrade to Ambari 2.6.x

{noformat}
 group_id | group_name |    tag    | service_name
----------+------------+-----------+--------------
        3 | asdf       | ZOOKEEPER |
{noformat}

Result: DB consistency check during startup after the upgrade produces false positive warning about the config group

{noformat}
WARN - You have config groups present in the database with no corresponding service found, [(ConfigGroup, Service) => ( asdf, null )]. Run --auto-fix-database to fix this automatically.
{noformat}

Following the instructions ({{--auto-fix-database}}) leads to deleting the config group.

This check was added for the case where the service the config group belongs to was deleted from the cluster.",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-07-11 19:48:30,16
13171518,Report MPack Version on Start/Status,"When components are installed or are started/restarted, they must report their versions back to Ambari server so that we can accurately track whether or not they are running the correct binaries. With the switch to mpacks, this means that {{stack-select}} is deprecated and must be replaced:

- Add mpack version tracking to the SQL and java classes
- Introduce a {{deprecated}} annotation and begin marking Python code dealing with stack tools as deprecated
- Use the mpack instance manager to report versions
- Change the structured output of the agent to report multiple possible results in a single payload
- Handle incoming version changes and update the appropriate service component hosts.
- Update the version alert to check for mpack versions",pull-request-available,[],AMBARI,Task,Critical,2018-07-11 19:31:18,31
13171492,Infra Solr migration: migrationConfigGenerator script failed with custom service user,"if custom service user is used for ambari infra solr (config: {{infra-solr-env/infra_solr_user}}) the following error could happen

{code}
[root@ctr-e138-1518143905142-409212-01-000007 ~]# export CONFIG_INI_LOCATION=/hwqe/hadoopqe/artifacts/ambarieu-e2e-fnm30toatl-we-ha/config_ini/ambari_solr_migration.ini
[root@ctr-e138-1518143905142-409212-01-000007 ~]#
[root@ctr-e138-1518143905142-409212-01-000007 ~]# python /usr/lib/ambari-infra-solr-client/migrationConfigGenerator.py --ini-file $CONFIG_INI_LOCATION --host ctr-e138-1518143905142-409212-01-000006.hwx.site --port 8443 -s --cluster cl1 --username admin --password admin --backup-base-path=/tmp/ --java-home /usr/lib/jvm/java-openjdk
Start generating config file: /hwqe/hadoopqe/artifacts/ambarieu-e2e-fnm30toatl-we-ha/config_ini/ambari_solr_migration.ini ...
Get Ambari cluster details ...
Set JAVA_HOME: /usr/lib/jvm/java-openjdk
Service detected: ZOOKEEPER
Zookeeper connection string: ctr-e138-1518143905142-409212-01-000002.hwx.site:2181,ctr-e138-1518143905142-409212-01-000003.hwx.site:2181,ctr-e138-1518143905142-409212-01-000005.hwx.site:2181
Service detected: AMBARI_INFRA_SOLR
Infra Solr znode: /infra-solr
2018-07-11 18:06:16,822 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:16,824 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 1)
2018-07-11 18:06:21,850 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:21,851 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 2)
2018-07-11 18:06:26,875 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:26,876 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 3)
2018-07-11 18:06:31,901 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:31,902 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 4)
2018-07-11 18:06:36,931 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:36,932 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 5)
2018-07-11 18:06:41,959 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:41,959 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 6)
2018-07-11 18:06:46,996 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:46,997 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 7)
2018-07-11 18:06:52,040 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:52,040 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 8)
2018-07-11 18:06:57,075 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:06:57,076 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 9)
2018-07-11 18:07:02,098 - kinit: Keytab contains no suitable keys for cstm-infra-solr/ctr-e138-1518143905142-409212-01-000007.hwx.site@EXAMPLE.COM while getting initial credentials

2018-07-11 18:07:02,098 - Get clusterstate.json: waiting for 5 seconds before retyring again (retry count: 10)
Traceback (most recent call last):
  File ""/usr/lib/ambari-infra-solr-client/migrationConfigGenerator.py"", line 507, in <module>
    generate_ambari_solr_migration_ini_file(options, accessor, protocol)
  File ""/usr/lib/ambari-infra-solr-client/migrationConfigGenerator.py"", line 343, in generate_ambari_solr_migration_ini_file
    coll_shard_map=get_shard_numbers_per_collections(state_json_map)
  File ""/usr/lib/ambari-infra-solr-client/migrationConfigGenerator.py"", line 124, in get_shard_numbers_per_collections
    for key,val in state_json_data.iteritems():
AttributeError: 'NoneType' object has no attribute 'iteritems'

Config file generation failed

{code}",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-07-11 18:28:07,29
13171459,Fix ambari-admin UI unit tests,"As of now UTs are failing:
{noformat}
#Editablelist directive Editing Updates permissions after save FAILED

#Editablelist directive Editing Show dialog window if user trying to leave page without save FAILED

#Editablelist directive Editing Saves current user in editing window if user click ""save"" FAILED

Executed 61 of 61 (3 FAILED) (0.387 secs / 0.378 secs)
{noformat}",pull-request-available,['ambari-admin'],AMBARI,Bug,Critical,2018-07-11 16:29:52,15
13171259,"Upgrade failed at ConvertTable, container mode, with data deployed","Upgrade failed for cluster where pre-upgrade data deploy was done, container mode.
{noformat}
Exception in thread ""main"" java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.AutoloadedDriver40
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.sql.DriverManager.isDriverAllowed(DriverManager.java:556)
	at java.sql.DriverManager.getConnection(DriverManager.java:661)
	at java.sql.DriverManager.getConnection(DriverManager.java:247)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:351)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:1935)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:1928)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.setConf(TxnHandler.java:270)
	at org.apache.hadoop.hive.metastore.txn.TxnUtils.getTxnStore(TxnUtils.java:115)
	at org.apache.hadoop.hive.upgrade.acid.PreUpgradeTool.prepareAcidUpgradeInternal(PreUpgradeTool.java:229)
	at org.apache.hadoop.hive.upgrade.acid.PreUpgradeTool.main(PreUpgradeTool.java:148)

Command failed after 1 tries{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-10 20:42:08,36
13171248,Unable to Restart Hive When Using An Ambari-Managed MySQL Server,"During restart of Hive, the following is observed:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/mysql_server.py"", line 63, in <module>
    MysqlServer().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 958, in restart
    self.stop(env, upgrade_type=upgrade_type)
TypeError: stop() got an unexpected keyword argument 'upgrade_type'
{code}",pull-request-available,[],AMBARI,Bug,Major,2018-07-10 19:36:46,31
13171123,hadoop-env is not regenerated when OneFS is used as a FileSystem,"The before-ANY/shared_initialization.py only regenerates hadop_env if there is a namenode or dfs_type is set to HCFS

{code}
  def hook(self, env):
    import params
    env.set_params(params)

    setup_users()
    if params.has_namenode or params.dfs_type == 'HCFS':
      setup_hadoop_env()
    setup_java()
{code}

This is no longer true because in the latest ambari-server we set dfs_type as follows:

{code}
    Map<String, ServiceInfo> serviceInfos = ambariMetaInfo.getServices(stackId.getStackName(), stackId.getStackVersion());
    for (ServiceInfo serviceInfoInstance : serviceInfos.values()) {
      if (serviceInfoInstance.getServiceType() != null) {
        LOG.debug(""Adding {} to command parameters for {}"", serviceInfoInstance.getServiceType(),
            serviceInfoInstance.getName());

        clusterLevelParams.put(DFS_TYPE, serviceInfoInstance.getServiceType());
        break;
      }
    }
{code}

This iterates over all of the stack service which will find HDFS first, so that the dfs_type will be HDFS instead of HCFS.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-10 12:04:47,18
13171090,OneFS service check fails with 'dfs_type' is not set.,"Following error comes when running a OneFS service check.

{code}
resource_management.core.exceptions.Fail: Resource parameter 'dfs_type' is not set.


stderr:
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ONEFS/package/scripts/service_check.py"", line 59, in <module>
    HdfsServiceCheck().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ONEFS/package/scripts/service_check.py"", line 43, in service_check
    mode=0777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 119, in run_action
    provider = provider_class(resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 594, in __init__
    self.assert_parameter_is_set('dfs_type')
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 673, in assert_parameter_is_set
    raise Fail(""Resource parameter '{0}' is not set."".format(parameter_name))
resource_management.core.exceptions.Fail: Resource parameter 'dfs_type' is not set.
stdout:
2018-07-09 17:29:05,706 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1485/hadoop/conf
2018-07-09 17:29:05,715 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1485/hadoop/conf
2018-07-09 17:29:05,717 - checked_call['hostid'] {}
2018-07-09 17:29:05,721 - checked_call returned (0, 'f20a5572')
2018-07-09 17:29:05,722 - HdfsResource['/tmp'] {'security_enabled': False, 'hadoop_bin_dir': '/usr/hdp/3.0.0.0-1485/hadoop/bin', 'keytab': [EMPTY], 'dfs_type': '', 'default_fs': 'hdfs://ah-onefs-hdp3.west.isilon.com:8020', 'hdfs_resource_ignore_file': '/var/lib/ambari-agent/data/.hdfs_resource_ignore', 'hdfs_site': ..., 'kinit_path_local': 'kinit', 'principal_name': None, 'user': 'hdfs', 'action': ['create_on_execute'], 'hadoop_conf_dir': '/usr/hdp/3.0.0.0-1485/hadoop/conf', 'type': 'directory', 'immutable_paths': [u'/apps/hive/warehouse', u'/mr-history/done', u'/app-logs', u'/tmp'], 'mode': 0777}

Command failed after 1 tries
{code}


In params_linux.py, dfs_type should come from clusterLevelParams instead of commandParams.
 
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-10 08:29:40,18
13170948,Spark thrift server is not starting on Upgraded cluster,"Steps to reproduce :
1) Deploy cluster with Ambari-2.6.2.0 + HDP-2.6.5.0
2) Upgrade to Ambari-2.7.0.0-876
3) Perform Express Upgrade to HDP-3.0.0.0-1621

After upgrade STS is failing with below error :

{code}
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.YarnException): org.apache.hadoop.security.AccessControlException: User spark does not have permission to submit application_1531132300904_0038 to queue default
 at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)
 at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:435)
 at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:320)
 at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:645)
 at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:277)
 at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:563)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-09 19:14:36,24
13170872,All input fields are disabled after validation - cancel - filter for something on advanced tab,"STR: (see attached video)

1. try to do some property modifications, and save it
2. when some warnings appear, click cancel
3. filter for a variable that is not on the Advanced tab
4. everything is disabled on all tabs, even after discarding the modifications",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-09 14:28:13,19
13170847,yarn params not found in Ambaripreupload.py,"Running Ambari preupload script is throwing errors because of invalid yarn parameter references: 

 
{noformat}
[root@c7301 ~]# /var/lib/ambari-server/resources/scripts/Ambaripreupload.py
2018-07-09 12:28:04,121 - call['/usr/bin/hdp-select status hadoop-mapreduce-historyserver > /tmp/tmpimy0Rf'] {}
2018-07-09 12:28:04,149 - call returned (0, '')
Returning fs.defaultFS -> hdfs://c7301.ambari.apache.org:8020
2018-07-09 12:28:04,154 - Creating /usr/hdp/3.0.0.0-1621/hadoop-yarn/lib/service-dep.tar.gz
Traceback (most recent call last):
File ""/var/lib/ambari-server/resources/scripts/Ambaripreupload.py"", line 360, in <module>
create_yarn_service_tarball()
File ""/var/lib/ambari-server/resources/scripts/Ambaripreupload.py"", line 317, in create_yarn_service_tarball
folders = [yarn_home_dir, yarn_lib_dir, hdfs_home_dir, hdfs_lib_dir, hadoop_home_dir, hadoop_lib_dir]
NameError: global name 'yarn_home_dir' is not defined{noformat}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-09 12:35:10,10
13170827,Ambari Alerts are not triggered,"Even when Namenodes, Datanodes/ entire services are down, there are no alerts in the Ambari UI.

Most of the summary information in HDFS, YARN show as n/a. There are no errors in the JS console too.
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-09 10:52:42,18
13170730,Suppress log messages from the credential_store_helper,Suppress log messages from the credential_store_helper since they are not necessary.,pull-request-available,[],AMBARI,Bug,Major,2018-07-08 16:04:18,30
13170706,"Restart services just before stack upgrade fails due to AMS package incompatibility errors, causing EU not to be started","*STR*
 # Deploy HDP-2.6.5 cluster with Ambari-2.6.2 (Atlas is installed on the cluster)
 # Upgrade Ambari server to 2.7
 # Upgrade AMS, Smartsense
 # Register HDP-3.0 version and install the packages
 # Try to start Express Upgrade. One of the pre-check fails: ""The property atlas.migration.data.filename is not found in application-properties, need to add the property before upgrade.""
 # Add the atlas property and save the config
 # Now Atlas and Hive are marked as stale config services and need restart
 # Restart Hive, it fails with compatibility error
 # Try to start Upgrade - if fails with below error:

Reason: The following Service Components should be in a started state.  Please invoke a service Stop and full Start and try again. HIVE: HIVE_SERVER (in INSTALLED on host dk-upgradetest-2.openstacklocal), HIVE: HIVE_METASTORE (in INSTALLED on host dk-upgradetest-2.openstacklocal)
Failed on: HIVE

As a result, Upgrade cannot be started",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-08 13:23:50,6
13170642,Hive Client Restart Fails When Using A Credential Store,"*STR*
# Deployed HDP-2.6.5 cluster with Ambari-2.6.2
# Upgrade Ambari to 2.7
# Upgrade AMS, Smartsense
# Update Atlas config to satisfy stack upgrade [pre-check|https://github.com/hortonworks/ambari/blob/AMBARI-2.7.0.0/ambari-server/src/main/java/org/apache/ambari/server/checks/AtlasMigrationPropertyCheck.java#L62] - atlas.migration.data.filename=/tmp/atlas
# Restart stale config services

*Result*
Hive client restart failed with below error:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_client.py"", line 63, in 
    HiveClient().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 953, in restart
    self.install(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_client.py"", line 35, in install
    self.configure(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_client.py"", line 43, in configure
    hive(name='client')
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 75, in hive
    params.user_group
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/security_commons.py"", line 55, in update_credential_provider_path
    content = StaticFile(src_provider_path)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 120, in action_create
    raise Fail(""Applying %s failed, parent directory %s doesn't exist"" % (self.resource, dirname))
resource_management.core.exceptions.Fail: Applying File['/usr/hdp/current/hive-client/conf/conf.server/hive-site.jceks'] failed, parent directory /usr/hdp/current/hive-client/conf/conf.server doesn't exist
{code}",pull-request-available,[],AMBARI,Bug,Blocker,2018-07-07 13:41:30,31
13170631,Logfeeder fail to start due to missing conf file post logsearch upgrade,"Upgraded above cluster from Ambari 2.6.2 to Ambari 2.7.0 and logsearch also
from 2.6.2 to 2.7.0.0

In ambari 2.6.2 we had created file ""kafka-output.json"" under ""/usr/lib
/ambari-logsearch-logfeeder/conf"" so that logsearch data is pushed to kafka
topic.

Looks like Ambari upgrade renames the conf dir to ""conf-old"" as below and due
to which kafka-output.json file in missing from the new config dir. This is
causing logfeeder start failure.
    
    
    [root@ctr-e138-1518143905142-369208-01-000002 ambari-logsearch-logfeeder]# ll
    total 24
    drwxr-xr-x 2 root root  4096 Jul  5 16:59 bin
    drwxr-xr-x 3 root root  4096 Jul  6 08:11 conf
    drwxr-xr-x 3 root root  4096 Jul  5 16:59 conf-old
    drwxr-xr-x 2 root root 12288 Jul  5 16:59 libs
    

",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-07-07 10:48:36,29
13170579,Ambari shows success when HBase Decommission/Recommission operations fail,"If HBase Decommission/Recommission operation fails with non zero exit code, that component still gets decommissioned/recommissioned. We need to handle this failure and not transition the state to Decommissioned/Recommissioned when the respective operation fails",pull-request-available,[],AMBARI,Bug,Major,2018-07-06 20:46:34,15
13170571,ambari-server sync-ldap with users and groups fails with NoSuchAlgorithmException,"Setup secure ldap went through fine but later on sync-ldap with users and groups has failed with below error
{code:java}
[root@ctr-e138-1518143905142-391100-01-000006 ~]# ambari-server sync-ldap --users /tmp/users.txt --groups /tmp/groups.txt
Using python  /usr/bin/python
Syncing with LDAP...
Enter Ambari Admin login: admin
Enter Ambari Admin password: 

Fetching LDAP configuration from DB.
Syncing specified users and groups..ERROR: Exiting with exit code 1. 
REASON: Caught exception running LDAP sync. ad-nano.qe.hortonworks.com:636; nested exception is javax.naming.CommunicationException: ad-nano.qe.hortonworks.com:636 [Root exception is java.net.SocketException: java.security.NoSuchAlgorithmException: Error constructing implementation (algorithm: Default, provider: SunJSSE, class: sun.security.ssl.SSLContextImpl$DefaultSSLContext)]
[root@ctr-e138-1518143905142-391100-01-000006 ~]# cat /tmp/users.txt {code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-06 20:30:15,27
13170502,Tasks fail from time to time due error in file download,"
    ERROR 2018-06-27 11:29:49,955 CustomServiceOrchestrator.py:448 - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: u'/var/lib/ambari-agent/cache/stack-hooks'; u'/var/lib/ambari-agent/cache/stack-hooks'
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 351, in runCommand
        hook_dir = self.file_cache.get_hook_base_dir(command, server_url_prefix)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/FileCache.py"", line 100, in get_hook_base_dir
        server_url_prefix)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/FileCache.py"", line 163, in provide_directory
        wait_for_another_execution_event = self.currently_providing[full_path]
    KeyError: u'/var/lib/ambari-agent/cache/stack-hooks'
    

",pull-request-available,[],AMBARI,Bug,Major,2018-07-06 14:41:10,5
13170460,"Can't register HDP 3.0.0.0 version and ""Use Local Repository""","When trying to register a VDF for HDP-3.0.0 and click on ""Use Local Repository"" the ""Save"" button is inactive and you can't register the version.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-07-06 10:26:30,19
13170304,Ambari Schema Upgrade is failing after creating views,"

Created below Views On Cluster with ambari 2.6.0 :
1. Workflow Manager (Create a workflow and save it)
2. Hive
3. Pig
4. Files

Upgrading the Ambari Server to 2.7.0, below is the stacktrace:

{code}
SchemaUpgradeHelper:238 - Upgrade failed.
org.postgresql.util.PSQLException: ERROR: update or delete on table ""servicecomponentdesiredstate"" violates foreign key constraint ""fk_hostcomponentdesiredstate_component_name"" on table ""hostcomponentdesiredstate""
Detail: Key (component_name, service_name, cluster_id)=(INFRA_SOLR_CLIENT, AMBARI_INFRA, 2) is still referenced from table ""hostcomponentdesiredstate"".
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2433)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2178)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:306)
at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307)
at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293)
at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270)
at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:244)
at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:822)
at org.apache.ambari.server.upgrade.UpgradeCatalog270.renameAmbariInfra(UpgradeCatalog270.java:1056)
at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1033)
at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:236)
at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:456)
2018-06-29 14:12:03,840 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: ERROR: update or delete on table ""servicecomponentdesiredstate"" violates foreign key constraint ""fk_hostcomponentdesiredstate_component_name"" on table ""hostcomponentdesiredstate""
Detail: Key (component_name, service_name, cluster_id)=(INFRA_SOLR_CLIENT, AMBARI_INFRA, 2) is still referenced from table ""hostcomponentdesiredstate"".
at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:239)
at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:456)
Caused by: org.postgresql.util.PSQLException: ERROR: update or delete on table ""servicecomponentdesiredstate"" violates foreign key constraint ""fk_hostcomponentdesiredstate_component_name"" on table ""hostcomponentdesiredstate""
Detail: Key (component_name, service_name, cluster_id)=(INFRA_SOLR_CLIENT, AMBARI_INFRA, 2) is still referenced from table ""hostcomponentdesiredstate"".
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2433)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2178)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:306)
at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307)
at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293)
at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270)
at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:244)
at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:822)
at org.apache.ambari.server.upgrade.UpgradeCatalog270.renameAmbariInfra(UpgradeCatalog270.java:1056)
at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1033)
at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:236)
... 1 more
{code}

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-05 16:08:55,13
13170294,Create Checkpoint page stuck while Enabling HA on Namenode,"Ambari UI stuck at Manual Steps Required: Create Checkpoint on NameNode page while Enabling HA after performing the manual steps.

{code}
[root@ctr-e138-1518143905142-378399-01-000004 ~]# sudo su cstm-hdfs -l -c 'hdfs dfsadmin -safemode enter'
######## Hortonworks #############
This is MOTD message, added for testing in qe infra
Safe mode is ON
[root@ctr-e138-1518143905142-378399-01-000004 ~]# sudo su cstm-hdfs -l -c 'hdfs dfsadmin -saveNamespace'
######## Hortonworks #############
This is MOTD message, added for testing in qe infra
Save namespace successful
{code}


The stat result of file /etc/security/keytabs/ambari.server.keytab, looks like the final change in the file occured at 2018-07-04 05:53:52.

{code:java}
[root@ctr-e138-1518143905142-395357-01-000002 ~]# stat /etc/security/keytabs/ambari.server.keytab
  File: ‘/etc/security/keytabs/ambari.server.keytab’
  Size: 333       	Blocks: 8          IO Block: 4096   regular file
Device: fd12h/64786d	Inode: 16390172    Links: 1
Access: (0400/-r--------)  Uid: ( 1803/agentslava)   Gid: ( 1803/agentslava)
Access: 2018-07-04 03:01:39.282093801 +0000
Modify: 2018-07-04 03:01:39.282093801 +0000
Change: 2018-07-04 05:53:52.333709142 +0000
 Birth: -
{code}

From the ambari server logs at the same time stamp:

{code:java}
2018-07-04 05:53:52,054  INFO [Server Action Executor Worker 586] KerberosServerAction:411 - Processing identities...
2018-07-04 05:53:52,322  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:111 - Updated the owner of the keytab file at /etc/security/keytabs/ambari.server.keytab to null
2018-07-04 05:53:52,323  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:125 - Updated the group of the keytab file at /etc/security/keytabs/ambari.server.keytab to null
2018-07-04 05:53:52,337  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:142 - Updated the access mode of the keytab file at /etc/security/keytabs/ambari.server.keytab to owner:'r' and group:'null'
2018-07-04 05:53:52,352  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:111 - Updated the owner of the keytab file at /etc/security/keytabs/ams-monitor.keytab to cstm-ams
2018-07-04 05:53:52,371  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:125 - Updated the group of the keytab file at /etc/security/keytabs/ams-monitor.keytab to hadoop
2018-07-04 05:53:52,387  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:142 - Updated the access mode of the keytab file at /etc/security/keytabs/ams-monitor.keytab to owner:'r' and group:''
{code}

",pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2018-07-05 15:15:32,30
13170247,Kafka failed to stop,"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka_broker.py"", line 145, in <module>
        KafkaBroker().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka_broker.py"", line 99, in stop
        ensure_base_directories()
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka.py"", line 266, in ensure_base_directories
        recursive_ownership = True,
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 125, in __new__
        cls(names_list.pop(0), env, provider, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 199, in action_create
        recursion_follow_links=self.resource.recursion_follow_links, safemode_folders=self.resource.safemode_folders)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 73, in _ensure_metadata
        sudo.chown_recursive(path, _user_entity, _group_entity, recursion_follow_links)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 55, in chown_recursive
        os.lchown(os.path.join(root, name), uid, gid)
    OSError: [Errno 2] No such file or directory: '/grid/0/log/kafka/controller.log'
    

",pull-request-available,[],AMBARI,Bug,Major,2018-07-05 11:35:08,5
13170160,Fix FindBugs warnings,"{noformat}
[INFO] --- findbugs-maven-plugin:3.0.3:check (default) @ ambari-server ---
...
[INFO] Total bugs: 1700
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-07-05 04:24:15,21
13170049,data-qa attribute present not for all properties,"data-qa attribute present not for all properties

Example with the attribute:
{code}
<input id=""ember38283"" class=""ember-view ember-text-field form-control col-md-9 long-input"" data-qa=""manage.include.files"" type=""text"" value=""false"" data-original-title="""" title="""">
{code}

Example without attribute:
{code}
<input id=""ember39059"" class=""ember-view ember-text-field form-control"" placeholder=""Type password"" type=""password"" value="""">
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-04 11:08:32,19
13170029,Host details page: remove duplicate title,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-04 09:28:23,19
13170025,Admin View: Dashboard link leads to login page,"Steps to reproduce:

* Go to Admin View
* Click Dashboard link

Actual result:
Ambari web firstly show login page and then redirects to the Dashboard page.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-04 09:22:00,19
13169609,UI Install: Custom Yarn Capacity Scheduler property set is missing after deploy,"STR:
1) During UI install,add a custom property for \{code}capacity-scheduler\{code}

property: \{code}test.capacity.scheduler=test-value\{code}

After deployment this property goes missing",pull-request-available,[],AMBARI,Bug,Critical,2018-07-02 19:39:05,48
13169577,"Ambari UI ""NEXT"" button is disabled for add service step of Spectrum Scale","Hi ,
     I was trying to install/add service the Spectrum Scale service to Ambari UI on IBM Power machine , in order to achieve that I followed below steps :

1. Stop all services 
2. Run Spectrum Scale installer from command line at Ambari host
2.At Ambari UI:  Click on Services --> Add Service
3. Selected the service ""Spectrum Scale"", then clicked on ""NEXT"" button
4. At next page , there will be drop down box for selecting the host for ""GPFS Master"" role, 

The issue comes at step 4 after selecting host for ""GPFS Master"" , ""NEXT"" button is not enabled and doesn't gets enabled even after variating the options too from drop down box.

Relevant screen shots are attached along with this bug.


Details for version are as below :

Ambari - 2.7.0.0-765",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-07-02 17:47:20,19
13169572,Alerts Are Running For Components Which Are Not Installed,"I recently noticed on a cluster that there were alerts being received and discarded for JournalNode on a cluster which had no JournalNodes installed. I did a quick grep of my own logs, and found a massive amount of alerts that are running for components which are:

* Not installed on those hosts
* Not installed anywhere in the cluster

{code}
2018-06-26 19:03:31,054  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:03:31,054  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:03:31,997  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:04:25,730  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:04:27,011  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:04:30,731  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:04:31,069  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:05:30,765  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:05:30,767  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:05:31,090  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:05:31,091  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:05:32,036  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:06:25,775  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:06:27,047  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:06:30,779  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:06:31,104  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:07:30,815  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:30,816  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_rpc_latency for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:30,821  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:30,821  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_cpu for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:31,136  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:07:31,136  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:07:32,064  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:08:25,835  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:08:27,092  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:08:30,835  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:08:31,150  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:09:25,862  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:09:25,863  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:09:31,174  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:09:31,174  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:09:32,097  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:10:25,883  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:10:25,883  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:10:27,115  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:10:31,199  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:11:25,897  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:11:25,897  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:11:27,136  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:11:31,229  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:11:31,229  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:16:27,291  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:16:30,970  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:30,971  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_rpc_latency for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:30,974  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:30,992  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_cpu for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:31,310  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:16:31,310  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:17:25,985  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:30,987  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_rpc_latency for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:30,990  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:30,990  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_cpu for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:31,324  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:17:31,324  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:17:32,274  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:18:26,002  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:18:26,004  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:18:27,294  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:18:31,332  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:19:26,020  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:19:26,021  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:19:27,327  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:19:31,354  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:19:31,354  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:20:26,051  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:20:26,052  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:20:27,354  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:20:31,370  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
{code}

STR:
* Install a simple cluster with ZK, HDFS, and YARN from HDP 2.6
* Monitor the logs and observe entries for components which are not installed",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-02 17:19:26,18
13169519,Alert definitions have not updated after stack upgrade,"After Stack upgrade , an alert is seen for Hive Metastore with following error:

{code:java}
Metastore on ctr-e138-1518143905142-380941-01-000006.hwx.site failed (Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/alerts/alert_hive_metastore.py"", line 203, in execute
    timeout_kill_strategy=TerminateStrategy.KILL_PROCESS_TREE,
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
ExecutionFailed: Execution of 'export HIVE_CONF_DIR='/usr/hdp/current/hive-metastore/conf' ; hive --hiveconf hive.metastore.uris=thrift://ctr-e138-1518143905142-380941-01-000006.hwx.site:9083                 --hiveconf hive.metastore.client.connect.retry.delay=1                 --hiveconf hive.metastore.failure.retries=1                 --hiveconf hive.metastore.connect.retries=1                 --hiveconf hive.metastore.client.socket.timeout=14                 --hiveconf hive.execution.engine=mr -e 'show databases;'' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/grid/0/hdp/3.0.0.0-1554/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/grid/0/hdp/3.0.0.0-1554/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Connecting to jdbc:hive2://ctr-e138-1518143905142-380941-01-000003.hwx.site:2181,ctr-e138-1518143905142-380941-01-000002.hwx.site:2181,ctr-e138-1518143905142-380941-01-000007.hwx.site:2181,ctr-e138-1518143905142-380941-01-000004.hwx.site:2181/default;principal=hive/_HOST@EXAMPLE.COM;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
18/06/27 10:37:10 [main]: INFO jdbc.HiveConnection: Connected to ctr-e138-1518143905142-380941-01-000006.hwx.site:10000
18/06/27 10:37:10 [main]: WARN jdbc.HiveConnection: Failed to connect to ctr-e138-1518143905142-380941-01-000006.hwx.site:10000
18/06/27 10:37:10 [main]: ERROR jdbc.Utils: Unable to read HiveServer2 configs from ZooKeeper
Error: Could not open client transport for any of the Server URI's in ZooKeeper: Failed to open new session: java.lang.IllegalArgumentException: Cannot modify hive.metastore.client.connect.retry.delay at runtime. It is not in list of params that are allowed to be modified at runtime (state=08S01,code=0)
Cannot run commands specified using -e. No current connection
)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-02 11:46:23,1
13169301,Prevent Configuration Changes During Keytab Regeneration in an Upgrade,"Certain configuration changes should be avoided when regenerating keytab files during different scenarios.  

For example, existing non-Kerberos configurations should not be changed during the regenerate keytabs operation performed during an upgrade. However it is necessary for Kerberos identity-related configurations (such as keytab file paths and principal names) to be added and updated; as well as allow for new Kerberos-related configurations to be added. 

To allow for this, a new _update configuration policy_ value has been added to the set of directives (*_config_update_policy_*) allowed when issuing a call to regenerate keytab files. This directive replaces the less flexible *_ignore_config_updates_* directive which only allows a user to enable or disable the ability for the operation to change configurations. The values allowed for *_config_update_policy_* are as follows:
* {{none}} - No configurations will be updated
* {{identities_only}} - New and updated configurations related to Kerberos identity information - principal, keytab file, and auth-to-local rule properties
* {{new_and_identities}} - Only new configurations declared by the Kerberos descriptor and stack advisor as well as the identity-related changes
* {{all}} - All configuration changes

During an upgrade, the _update configuration policy_ is set to {{new_and_identities}}.
",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-30 01:01:23,30
13169247,Agent-side command-*.json files should optionally be deleted when no longer needed by the command,"Agent-side _command JSON_ files ({{command-*.json}}, {{status_command.json}}) should optionally be deleted when no longer needed by the command.  One reason for this is to reduce the risk of leaking sensitive data stored at plaintext in the _command JSON_ files. 

Currently the _command JSON_ files are stored on disk in /var/lib/ambari-agent/data.  These files may be cleared out over time, but there is a need to have them removed as soon as they are no longer needed.

To do this, a retention policy may be defined so that the Ambari agent behaves accordingly:

* {{keep}}
** No automatic removal is performed
**  This is the default behavior  
* {{remove}}
** The _command JSON_ file are removed as soon as the command completes
* {{remove_on_success}} 
** The _command JSON_ files are removed as soon as the command *successfully* completes
** The _command JSON_ files are not removed on failure conditions

This value is to be set in the {{ambari-agent.ini}} file, typically found at {{/etc/ambari-agent/conf/ambari-agent.ini}} using the *{{command_file_retention_policy}}* property.  After setting this property, the agent needs to be restarted. ",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-06-29 20:31:23,30
13169223,Ambari ldap integration has inconsistent behavior in Group Object Class definition after upgrade,"The behavior of the ambari ldap integration seems inconsistent across 2.6.2 -> 2.7.0 upgrade.

In Ambari 2.6.2, the Group Object Class property is exemplified as taking what is commonly known as the group object class attribute.

Group object class* (posixGroup):
In Ambari 2.7.0, the Group Object Class property is exemplified as taking what is commonly known as the group search base.

Group object class* (ou=groups,dc=ambari,dc=apache,dc=org):

The first one is the correct.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 18:12:46,18
13169205,Error in persisting web client state at ambari-server in editing a widget of HDFS ,"STR
 # Login as admin to Ambari
 # Install a cluster with HDFS, ZK and AMS
 # Go to Services / HDFS / Metrics and edit one of the widgets (for instance change the name of the widget)
 # Create a new user with 'Cluster Operator' role
 # Logout and login with the newly created user
 # Try to edit the previously edit widget

The action will fail due to an authorization issue (403 is thrown).",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-29 16:53:52,27
13169196,Component with recovery Enabled are not coming up when autostart is enabled ,"STR:-  
1\. test enable autostart for all component  
2\. test restart all the host at once.  
3\. test expect within 45 min. each and every component should be up.  
4\. test failed with the following exception

    
    
     ||Host:- nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal|| 
    
    
     |ComponentName : RANGER_TAGSYNC Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : DRUID_ROUTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
     ComponentName : HIVE_SERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : SPARK2_THRIFTSERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : HISTORYSERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : DRUID_BROKER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
      |ComponentName : HBASE_MASTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : HIVE_METASTORE Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : ZEPPELIN_MASTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : ACTIVITY_ANALYZER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : DRUID_OVERLORD Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : TIMELINE_READER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED
      |ComponentName : RANGER_ADMIN Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : OOZIE_SERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
      ComponentName : METRICS_COLLECTOR Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : METRICS_GRAFANA Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
    

That means these components are not up untill the waiting time completes.

I checked the agent logs:-  
attching the ambari-agent logs generated from  
cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py  
[agent-
autostart.log![](/images/icons/link_attachment_7.gif)](/secure/attachment/159734
/159734_agent-autostart.log ""agent-autostart.log attached to BUG-106407"")  
the relevent part if

    
    
    cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py
    
    INFO 2018-06-28 11:14:31,282 RecoveryManager.py:454 - RecoverConfig = {u'components': ()}
    INFO 2018-06-28 11:14:34,208 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'METRICS_COLLECTOR',
    INFO 2018-06-28 11:14:34,208 RecoveryManager.py:178 - New status, desired status is set to INIT for METRICS_COLLECTOR
    INFO 2018-06-28 11:14:50,695 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HBASE_REGIONSERVER
    INFO 2018-06-28 11:14:51,404 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SUPERVISOR
    INFO 2018-06-28 11:14:51,954 RecoveryManager.py:157 - New status, current status is set to INSTALLED for TEZ_CLIENT
    INFO 2018-06-28 11:14:52,672 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_CLIENT
    INFO 2018-06-28 11:14:53,467 RecoveryManager.py:157 - New status, current status is set to INSTALLED for PIG
    INFO 2018-06-28 11:14:54,241 RecoveryManager.py:157 - New status, current status is set to INSTALLED for KERBEROS_CLIENT
    INFO 2018-06-28 11:14:54,683 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HST_AGENT
    INFO 2018-06-28 11:14:55,367 RecoveryManager.py:157 - New status, current status is set to INSTALLED for METRICS_MONITOR
    INFO 2018-06-28 11:14:55,920 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ZOOKEEPER_CLIENT
    INFO 2018-06-28 11:14:56,458 RecoveryManager.py:157 - New status, current status is set to INSTALLED for LOGSEARCH_LOGFEEDER
    INFO 2018-06-28 11:14:57,017 RecoveryManager.py:157 - New status, current status is set to INSTALLED for INFRA_SOLR_CLIENT
    INFO 2018-06-28 11:14:57,823 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DATANODE
    INFO 2018-06-28 11:14:58,443 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HST_SERVER
    INFO 2018-06-28 11:14:59,205 RecoveryManager.py:157 - New status, current status is set to INSTALLED for TIMELINE_READER
    INFO 2018-06-28 11:15:00,015 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_ROUTER
    INFO 2018-06-28 11:15:00,617 RecoveryManager.py:157 - New status, current status is set to INSTALLED for NAMENODE
    INFO 2018-06-28 11:15:01,151 RecoveryManager.py:157 - New status, current status is set to INSTALLED for YARN_REGISTRY_DNS
    INFO 2018-06-28 11:15:01,745 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRPC_SERVER
    INFO 2018-06-28 11:15:02,250 RecoveryManager.py:157 - New status, current status is set to INSTALLED for STORM_UI_SERVER
    INFO 2018-06-28 11:15:02,812 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HBASE_MASTER
    INFO 2018-06-28 11:15:03,353 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_USERSYNC
    INFO 2018-06-28 11:15:03,825 RecoveryManager.py:157 - New status, current status is set to INSTALLED for NIMBUS
    INFO 2018-06-28 11:15:04,413 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SUPERSET
    INFO 2018-06-28 11:15:04,996 RecoveryManager.py:157 - New status, current status is set to INSTALLED for APP_TIMELINE_SERVER
    INFO 2018-06-28 11:15:05,552 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ACTIVITY_EXPLORER
    INFO 2018-06-28 11:15:06,202 RecoveryManager.py:157 - New status, current status is set to INSTALLED for LOGSEARCH_SERVER
    INFO 2018-06-28 11:15:07,436 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SPARK2_JOBHISTORYSERVER
    INFO 2018-06-28 11:15:07,962 RecoveryManager.py:157 - New status, current status is set to INSTALLED for INFRA_SOLR
    INFO 2018-06-28 11:15:08,494 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_METASTORE
    INFO 2018-06-28 11:15:09,508 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RESOURCEMANAGER
    INFO 2018-06-28 11:15:10,197 RecoveryManager.py:157 - New status, current status is set to INSTALLED for OOZIE_SERVER
    INFO 2018-06-28 11:15:10,840 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 11:15:11,517 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SPARK2_THRIFTSERVER
    INFO 2018-06-28 11:15:12,028 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_OVERLORD
    INFO 2018-06-28 11:15:12,998 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_BROKER
    INFO 2018-06-28 11:15:13,650 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_SERVER
    INFO 2018-06-28 11:15:14,321 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_TAGSYNC
    INFO 2018-06-28 11:15:14,852 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ACTIVITY_ANALYZER
    INFO 2018-06-28 11:15:14,956 RecoveryManager.py:163 - current status is set to INSTALLED for METRICS_COLLECTOR
    INFO 2018-06-28 11:15:15,655 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ZEPPELIN_MASTER
    INFO 2018-06-28 11:15:16,199 RecoveryManager.py:157 - New status, current status is set to INSTALLED for METRICS_GRAFANA
    INFO 2018-06-28 11:15:16,859 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HISTORYSERVER
    INFO 2018-06-28 11:17:36,038 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'METRICS_COLLECTOR',
    INFO 2018-06-28 11:17:36,042 RecoveryManager.py:183 - desired status is set to STARTED for METRICS_COLLECTOR
    INFO 2018-06-28 11:27:33,451 RecoveryManager.py:183 - desired status is set to INSTALLED for METRICS_COLLECTOR
    INFO 2018-06-28 11:27:33,451 RecoveryManager.py:620 - Received EXECUTION_COMMAND (STOP/INSTALL), desired state of METRICS_COLLECTOR to INSTALLED
    INFO 2018-06-28 11:29:10,328 RecoveryManager.py:588 - After EXECUTION_COMMAND (STOP/INSTALL), with taskId=63, current state of METRICS_COLLECTOR to INSTALLED
    INFO 2018-06-28 11:45:03,661 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 11:45:12,168 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 11:45:20,469 RecoveryManager.py:163 - current status is set to STARTED for HST_SERVER
    INFO 2018-06-28 11:45:26,682 RecoveryManager.py:163 - current status is set to STARTED for RANGER_TAGSYNC
    INFO 2018-06-28 11:45:46,474 RecoveryManager.py:163 - current status is set to STARTED for YARN_REGISTRY_DNS
    INFO 2018-06-28 11:45:54,622 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    INFO 2018-06-28 11:46:27,459 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_SERVER
    INFO 2018-06-28 11:48:53,946 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 14:34:17,142 RecoveryManager.py:583 - After EXECUTION_COMMAND (START), with taskId=1530194603, current state of ACTIVITY_EXPLORER to STARTED
    INFO 2018-06-28 14:34:18,039 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,105 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,170 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,259 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,303 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,409 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,478 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,570 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,616 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,644 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,674 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,730 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,764 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,780 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,831 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,875 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,888 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,960 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,982 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,015 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,051 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,090 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,108 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,125 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,220 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,290 RecoveryManager.py:454 - RecoverConfig = {u'components': ()}
    INFO 2018-06-28 14:35:36,027 RecoveryManager.py:163 - current status is set to INSTALLED for SUPERVISOR
    INFO 2018-06-28 14:36:05,838 RecoveryManager.py:163 - current status is set to STARTED for SUPERSET
    INFO 2018-06-28 14:36:45,585 RecoveryManager.py:163 - current status is set to STARTED for YARN_REGISTRY_DNS
    INFO 2018-06-28 14:37:18,681 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 14:37:55,603 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 14:38:14,872 RecoveryManager.py:163 - current status is set to STARTED for RANGER_TAGSYNC
    INFO 2018-06-28 14:38:29,806 RecoveryManager.py:163 - current status is set to STARTED for DATANODE
    INFO 2018-06-28 14:38:43,789 RecoveryManager.py:163 - current status is set to STARTED for RANGER_USERSYNC
    INFO 2018-06-28 14:38:54,727 RecoveryManager.py:163 - current status is set to INSTALLED for METRICS_MONITOR
    INFO 2018-06-28 14:39:40,572 RecoveryManager.py:163 - current status is set to STARTED for STORM_UI_SERVER
    INFO 2018-06-28 14:39:42,898 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 14:40:49,742 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 14:41:13,508 RecoveryManager.py:163 - current status is set to STARTED for HBASE_REGIONSERVER
    INFO 2018-06-28 14:44:02,873 RecoveryManager.py:163 - current status is set to STARTED for NIMBUS
    INFO 2018-06-28 14:45:17,040 RecoveryManager.py:163 - current status is set to STARTED for METRICS_COLLECTOR
    INFO 2018-06-28 14:45:20,928 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_LOGFEEDER
    INFO 2018-06-28 14:46:24,130 RecoveryManager.py:163 - current status is set to INSTALLED for INFRA_SOLR
    INFO 2018-06-28 14:46:24,598 RecoveryManager.py:163 - current status is set to STARTED for ACTIVITY_ANALYZER
    INFO 2018-06-28 14:47:14,171 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 14:49:22,384 RecoveryManager.py:163 - current status is set to STARTED for NAMENODE
    INFO 2018-06-28 14:49:31,552 RecoveryManager.py:163 - current status is set to STARTED for DRPC_SERVER
    INFO 2018-06-28 15:10:55,994 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_SERVER
    INFO 2018-06-28 15:11:01,316 RecoveryManager.py:163 - current status is set to STARTED for SPARK2_JOBHISTORYSERVER
    INFO 2018-06-28 15:11:11,251 RecoveryManager.py:163 - current status is set to STARTED for METRICS_GRAFANA
    INFO 2018-06-28 15:11:37,007 RecoveryManager.py:163 - current status is set to STARTED for HST_SERVER
    INFO 2018-06-28 15:11:57,521 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    INFO 2018-06-28 15:12:03,321 RecoveryManager.py:163 - current status is set to STARTED for OOZIE_SERVER
    INFO 2018-06-28 15:13:24,798 RecoveryManager.py:163 - current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 15:14:13,744 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 15:15:03,889 RecoveryManager.py:163 - current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 15:15:50,119 RecoveryManager.py:163 - current status is set to INSTALLED for OOZIE_SERVER
    INFO 2018-06-28 15:15:50,451 RecoveryManager.py:163 - current status is set to INSTALLED for NAMENODE
    INFO 2018-06-28 15:15:51,459 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 15:15:57,678 RecoveryManager.py:163 - current status is set to STARTED for NAMENODE
    INFO 2018-06-28 15:16:22,782 RecoveryManager.py:163 - current status is set to STARTED for HBASE_MASTER
    INFO 2018-06-28 15:16:40,512 RecoveryManager.py:163 - current status is set to STARTED for APP_TIMELINE_SERVER
    INFO 2018-06-28 15:16:43,582 RecoveryManager.py:163 - current status is set to STARTED for OOZIE_SERVER
    INFO 2018-06-28 15:17:45,952 RecoveryManager.py:163 - current status is set to STARTED for HISTORYSERVER
    INFO 2018-06-28 15:37:57,495 RecoveryManager.py:163 - current status is set to INSTALLED for HST_AGENT
    INFO 2018-06-28 15:38:47,195 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    

I checked is this because of memory issue. but

    
    
    [root@nat-yc-r7-irrs-ambari-autostart-1-re-re-3 data]# df -h
    Filesystem             Size  Used Avail Use% Mounted on
    /dev/mapper/rhel-root   17G   11G  6.0G  65% /
    devtmpfs               7.8G     0  7.8G   0% /dev
    tmpfs                  7.8G     0  7.8G   0% /dev/shm
    tmpfs                  7.8G  361M  7.5G   5% /run
    tmpfs                  7.8G     0  7.8G   0% /sys/fs/cgroup
    /dev/vda1             1014M  172M  843M  17% /boot
    /dev/vdb               246G   19G  216G   8% /grid/0
    tmpfs                  1.6G     0  1.6G   0% /run/user/1037
    tmpfs                  1.6G     0  1.6G   0% /run/user/0
    

I checked some of auto_errors

    
    
    cat auto_errors-1530190498.txt
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 132, in <module>
        HistoryServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 100, in start
        skip=params.sysprep_skip_copy_tarballs_hdfs) or resource_created
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 497, in copy_to_hdfs
        source_file = prepare_function()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 97, in _prepare_tez_tarball
        hadoop_lib_native_lzo_dir = os.path.join(stack_root, service_version, ""hadoop"", ""lib"", ""native"")
      File ""/usr/lib64/python2.7/posixpath.py"", line 75, in join
        if b.startswith('/'):
    AttributeError: 'NoneType' object has no attribute 'startswith'
    

attching the ambari-agent logs generated from  
cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py  
[agent-
autostart.log![](/images/icons/link_attachment_7.gif)](/secure/attachment/159734
/159734_agent-autostart.log ""agent-autostart.log attached to BUG-106407"")

Repro cluster:- <http://linux-jenkins.qe.hortonworks.com:8080/job/Nightly-
Start-EC2-Run-HDP/985907/>  
lifetime extended to 72 hours <http://linux-
jenkins.qe.hortonworks.com:8080/job/update-openstack-lifetime/12085/>

",pull-request-available,[],AMBARI,Bug,Major,2018-06-29 16:07:59,5
13169165,"Ambari setup-ldap fails with ""internal server error"" after upgrade","Steps to reproduce:

# integrate ambari 2.6.0 with ldap
# upgrade to ambari 2.7.0
# integrate ambari 2.7.0 with same ldap

Result:

{code}
ERROR: Unexpected HTTPError: HTTP Error 500: Internal Server Error
For more info run ambari-server with -v or --verbose option
{code}

Cause:

{code}
2018-06-29T09:43:32.624Z, User(admin), RemoteIp(127.0.0.1), RequestType(PUT), url(http://127.0.0.1:8080/api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/ldap-configuration), ResultStatus(500 Internal Server Error), Reason(org.apache.ambari.server.controller.spi.SystemException: 
Invalid Ambari server configuration key: ldap-configuration:ssl.trustStore.path)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 13:49:03,18
13169155,Add Service Wizard: Next Button is not enabled while adding Ranger after fixing an erroneous property,"Following are the steps performed
- On an already deployed cluster (without Ranger) setup ldap
- navigate to Add Service wizard to add ranger
- Use Authentication method as LDAP
- At customize services property populate all required fields
- Set one of the password to not meet the requirement (say set it as rangeradmin)
- Click Next. There will be a warning popup which says properties did not meet the requirements and user has to change it before proceeding
- Close the popup and find the property which has to be changed (It would be better if user is navigated to the property upon clicking on the warning popup itself)
- Fix this property, notice that there are no other properties which needs attention
- Still Next button is not enabled",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-29 13:10:02,15
13169154,Blueprint installation throws an exception if the same mpack is defined in both the blueprint and the cluster template,"An exception is thrown during blueprint installation if the same mpack is defined in both the blueprint and the cluster template. See below.
{code}
06 Jun 2018 10:16:34,164 ERROR [ambari-client-thread-26] CreateHandler:74 - Caught a system exception while attempting to create a resource: An exception occurred during cluster provisioning: Duplicate key HDPCORE-1.0.0-b368
org.apache.ambari.server.controller.spi.SystemException: An exception occurred during cluster provisioning: Duplicate key HDPCORE-1.0.0-b368
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider.processBlueprintCreate(ClusterResourceProvider.java:547)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider.createResourcesAuthorized(ClusterResourceProvider.java:236)
        at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.createResources(AbstractAuthorizedResourceProvider.java:231)
        at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
        at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
        at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
        at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
        at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:162)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
        at org.apache.ambari.server.api.services.ClusterService.createCluster(ClusterService.java:187)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:215)
        at org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilterInternal(AmbariBasicAuthenticationFilter.java:115)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:123)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
        at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
        at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
        at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
        at org.eclipse.jetty.server.Server.handle(Server.java:564)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
        at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: Duplicate key HDPCORE-1.0.0-b368
        at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133)
        at java.util.HashMap.merge(HashMap.java:1253)
        at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320)
        at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
        at java.util.Iterator.forEachRemaining(Iterator.java:116)
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
        at org.apache.ambari.server.topology.StackComponentResolver.getMpackStackIds(StackComponentResolver.java:129)
        at org.apache.ambari.server.topology.StackComponentResolver.resolveComponents(StackComponentResolver.java:49)
        at org.apache.ambari.server.topology.TopologyManager.provisionCluster(TopologyManager.java:291)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider.processBlueprintCreate(ClusterResourceProvider.java:541)
        ... 106 more
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 13:02:11,20
13169117,Add UTs for Mpack Advisor.,"Added UTs for AMBARI-23870, AMBARI-23923, AMBARI-23638.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-06-29 09:12:31,43
13169091,Blueprint deployment with custom service name,"Blueprint deployment with custom service name fails, because some parts of Ambari mix and match service name (eg. zk1) and service type (eg. ZOOKEEPER).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 07:31:51,21
13169075,Updating admin user password field value for Ranger and Atlas,"Updating below admin user password configs in stack.
 Ranger: {{admin_password/ranger-env}}
 Atlas: {{atlas.admin.password/atlas-env}}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 06:10:10,25
13169022,"Log Search: config api validator in Configuration Editor throws ""IllegalArgumentException""","1. Login to logsearch portal
2. Navigate to Configuration Editor
3. Select a valid component in the Validator drop down, and Type in some sample test data in the Sample Data text area, click Test. This should validate the sample data and show valid response to the user, but throws a illegalArgumentException.


{code:java}
java.lang.IllegalArgumentException: Error validating shipper config:
[ConstraintViolationImpl{interpolatedMessage='may not be null', propertyPath=filter[3].multilinePattern, rootBeanClass=class org.apache.ambari.logsearch.model.common.LSServerInputConfig, messageTemplate='{javax.validation.constraints.NotNull.message}'}]
{code}",pull-request-available quickfix,['ambari-logsearch'],AMBARI,Bug,Major,2018-06-29 00:22:12,29
13169019,Enabling RM HA should not be allowed if YARN is stopped,"Enabling RM HA should not be allowed if YARN is stopped. There should be a popup message saying RM should be up before enabling HA, similar to NN HA wizard warning",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-28 23:58:44,9
13168965,Provide Button to Download CSV of Keytabs From Kerberos Section in the UI,"When Ambari is configured to use Kerberos, but the administrator has opted to use manage their keytabs manually, it would be good to provide a way for them to download the Keytab CSV.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-28 20:06:11,34
13168836,Python unit test failure on 2.7.6,"Python unit tests fail on Python 2.7.6, because [SSLContext|https://docs.python.org/2/library/ssl.html#ssl-contexts] was only added in 2.7.9.

{noformat:title=https://builds.apache.org/job/Ambari-trunk-Commit/9543/consoleText}
ERROR: test_ldap_sync_ssl (TestAmbariServer.TestAmbariServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/TestAmbariServer.py"", line 7804, in test_ldap_sync_ssl
    sync_ldap(options)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/main/python/ambari_server/setupSecurity.py"", line 405, in sync_ldap
    raise FatalException(1, err)
FatalException: ""Fatal exception: Sync event creation failed. Error details: 'module' object has no attribute 'SSLContext', exit code 1""

ERROR: test_get_ssl_context (TestServerUtils.TestServerUtils)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/TestServerUtils.py"", line 124, in test_get_ssl_context
    context = get_ssl_context(properties)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/main/python/ambari_server/serverUtils.py"", line 274, in get_ssl_context
    context = ssl.SSLContext(protocol)
AttributeError: 'module' object has no attribute 'SSLContext'
{noformat}

CC [~rlevas]",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-28 11:03:12,30
13168749,Improve Kafka service check to check for under-replicated partitions,"During Ambari Rolling Upgrade, we invoke Kafka Service check to ensure that Kafka is healthy. The current implementation only does basic topic creation/deletion. We need to extend the implementation to check for number of under-replicated partitions.

We have this support in kafka-topics.sh script which is already used in the kafka service check script. We need to extend above script to call below command
{noformat}
sh kafka-topics.sh --describe --zookeeper localhost:2181 --under-replicated-partitions
{noformat}
If the command output is empty, then there are no under replicated partotions.
If the output contains ""Topic:"" string, then there are some under replicated partitions, we can fail the service check.
{noformat}
sh kafka-topics.sh --describe --zookeeper localhost:2181 --under-replicated-partitions
 Topic: TEST Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1
 Topic: TEST Partition: 1 Leader: 1 Replicas: 2,1 Isr: 1{noformat}",pull-request-available,['stacks'],AMBARI,Bug,Major,2018-06-28 01:40:59,49
13168672,Allow Conditional Upgrade Elements Based on KDC Type,"Ambari can only conditionally skip upgrade items based on whether the cluster's security type is {{KERBEROS}} or not. However, there may be a need to fine tune this even further by applying conditional elements to the type of Kerberos in effect, such as only when {{NONE}} is the {{KDCType}}. This would indicate that the cluster's Kerberos configuration is not managed by Ambari.",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-27 16:51:15,31
13168610,Unable to filter on the alerts in dashboard page,"WIth the new Alerts shown in the Ambari dashboard, unable to filter the various alerts seen. When filter is selected, the dropdown immediately disappears and doesn't allow the user to select the alert level to filter.
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-27 12:13:17,19
13168526,Fix Atlas HA support in Ambari via blueprint deployment,"Currently deploying Atlas in HA mode via blueprint is not possible due to property [{{atlas.server.bind.address}}|https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java#L3156] being set by Blueprint Processor, need to remove the property from being processed in Blueprint Processor and let the default value to be effective.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-27 06:16:12,20
13168513,Ranger KMS Database Connection Test hangs with JS error,"During Install Wizard > Customize Services > Databases > Ranger KMS:
 * Chose MYSQL
 * Filled out appropriate details for the JDBC connection
 * ""Setup database and database user"" off
 * Clicked TEST CONNENCTION
 * Got an infinite spinner due to a JS error (see attached.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-27 02:16:46,34
13168470,Change Orchestrator for Lifecycle groupings,Change code to add lifecycles when orchestrating.  This is the just the first step of changes required to support this mode.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-06-26 21:15:10,33
13168461,Mpack Instance Manager Produces Bad JSON and Doesn't List Versions,"The Mpack Instance manage is producing JSON output which cannot be parsed by non-Python parsers:
- Includes unicode markers for strings (like \{{u'ZOOKEEPER'}})
- Uses non-standard single quotes
{code}
[root@c7403 mpack-instance-manager]$ python mpack-instance-manager.py list-instances
{'mpacks': \{'hdpcore': {'mpack-instances': {'HDPCORE': {'name': 'HDPCORE', 'subgroups': {'default': {'modules': {'zookeeper': {'category': u'SERVER', 'name': 'zookeeper', 'components': {'zookeeper_server': {'component-instances': {'default': {'path': '/usr/hwx/mpacks/hdpcore/1.0.0-b450/zookeeper_server', 'name': 'default'}}}}}, 'zookeeper_clients': \{'category': u'CLIENT', 'name': 'zookeeper_clients', 'components': {'zookeeper_client': {'component-instances': {'default': {'path': '/usr/hwx/mpacks/hdpcore/1.0.0-b450/zookeeper_client', 'name': 'default'}}}}}}}}}}}}}
{code}

It also fails to provide information about the mpack version and component version. The output from the command \{{list-instances}} needs to provide both the mpack version and the module component version. Otherwise, this data cannot be reported back.

If the \{{list-instances}} is not the correct command for this, then that's fine, but we still need a command we can invoke to get this information back in a structure manner.",pull-request-available,['ambari-server'],AMBARI,New Feature,Major,2018-06-26 20:39:31,44
13168421,Ranger Storm plugin toggle behavior changes under different scenarios,"Here is a related issue which only occurs for fresh HDP-3.0 installs where the cluster is setup manually and is a bit strange.

There are two scenarios:
 1. When a cluster is kerberos enabled with Ranger installed, Now using Add service wizard Add Storm service and on the customize configuration page, we see that Ranger Storm plugin is enabled and [service-advisor|https://github.com/hortonworks/hdp_ambari_definitions/blob/AMBARI-2.7.0.0/src/main/resources/stacks/HDP/3.0/services/STORM/service_advisor.py#L216-L222] provides appropriate recommendations by showing:
{noformat}
* ranger-storm-plugin-enabled with recommended value Yes.
* nimbus.authorizer with recommended value org.apache.ranger.authorization.storm.authorizer.RangerStormAuthorizer.
{noformat}
2. When a cluster is kerberos enabled with Ranger and Storm installed and we then enable Ranger Storm plugin the service-advisor recommendations differ this time by showing:
{noformat}
* ranger-storm-plugin-enabled with recommended value Yes.
* nimbus.authorizer with recommended value Property removed.
{noformat}
Looks like in the first case the {{security_enabled}} is {{True}} whereas in the second case the flag is {{False}} while the cluster is actually kerberized.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-26 17:56:02,27
13168366,Upgrade Packs Do Not Allow Downgrade by Default,Noticed on a recent upgrade that should have allowed a downgrade that the option was not present in the UI. Looks like the {{UpgradePack}} class needs to default the boolean to {{true}} if it's not specified.,pull-request-available,[],AMBARI,Bug,Blocker,2018-06-26 14:13:48,31
13168344,WebSockets traffic does not work between Ambari Web UI and Ambari Server when it is accessed via Knox Proxy (UI side changes),When connected via Knox proxy ambari server web socket URL should be changed from <ambari-server protocol>://<ambari-server host>:<ambari-server port>/api/stomp/v1/websocket to <knox protocol>://<knox host>:<knox port>/gateway/default/ambari/websocket,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-26 13:07:22,19
13168322,Typo in div class name at CustomizeServicespage - Accounts tab,Fix class name _use-ambari-chekboxes_ to be _use-ambari-checkboxes_,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-26 10:58:53,9
13168277,Remove reference to JDK 1.7 in ambari-server setup,"We need to remove the reference to JDK 1.7, as it's not supported with HDP 3.0. We still see the following in ambari-server
setup:
    
    
    [2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-26 07:30:04,5
13168270,The button 'NEXT ->' should contain a special html tag 'data-qa',To add data-qa attribute for kerberos wizards,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-26 06:21:23,32
13168255,Ambari Server Setup LDAP Label Updates,"Ask users to select ldap type ( AD, IPA, generic ldap) and provide inteligent defaults based on their choice:

Active Directory 
{code:java}
# ambari-server setup-ldap
Using python  /usr/bin/python
Currently 'no auth method' is configured, do you wish to use LDAP instead [y/n] (y)?
Primary LDAP Host:
Primary LDAP Port:
Secondary LDAP Host <Optional>:
Secondary LDAP Port <Optional>:
Use SSL [true/false] (false):
User object class (user): 
User ID attribute (sAMAccountName): 
Group object class (group):
Group name attribute (cn): 
Group member attribute (member): 
Distinguished name attribute (distinguishedName): 
Search Base (dc=ambari,dc=apache,dc=org): 
User Search Base (ou=users,dc=ambari,dc=apache,dc=org): 
Group Search Base (ou=groups,dc=ambari,dc=apache,dc=org): 
Referral method [follow/ignore] (follow): 
Bind anonymously [true/false] (false): 
Bind DN (cn=ldapbind,dc=ambari,dc=apache,dc=org):
Enter Bind DN Password: 
Confirm Bind DN Password: 
Handling behavior for username collisions [convert/skip] for LDAP sync (skip): 
Force lower-case user names [true/false] (true):
Results from LDAP are paginated when requested [true/false] (false):
{code}

IPA (very similar to generic ldap, but needs explicit search customization into cn=accounts (or cn=compat) else it returns 2 search results which tends to cause problems)
{code:java}

{code:java}
# ambari-server setup-ldap
Using python  /usr/bin/python
Currently 'no auth method' is configured, do you wish to use LDAP instead [y/n] (y)?
Primary LDAP Host (ipa.ambari.apache.org):
Primary LDAP Port (636):
Secondary LDAP Host <Optional>:
Secondary LDAP Port <Optional>:
Use SSL [true/false] (true):
Do you want to provide custom TrustStore for Ambari [y/n] (y)?
TrustStore type [jks/jceks/pkcs12] (jks):
Path to TrustStore file (/etc/pki/java/cacerts):
Password for TrustStore (changeit):
User object class (posixUser): 
User ID attribute (uid): 
Group object class (posixGroup):
Group name attribute (cn): 
Group member attribute (memberUid): 
Distinguished name attribute (dn): 
Search Base (dc=ambari,dc=apache,dc=org): 
User Search Base (cn=users,cn=accounts,dc=ambari,dc=apache,dc=org): 
Group Search Base (cn=groups,cn=accounts,dc=ambari,dc=apache,dc=org): 
Referral method [follow/ignore] (follow): 
Bind anonymously [true/false] (false): 
Bind DN ( uid=ldapbind,cn=users,cn=accounts,dc=ambari,dc=apache,dc=org):
Enter Bind DN Password: 
Confirm Bind DN Password: 
Handling behavior for username collisions [convert/skip] for LDAP sync (skip): 
{code}

Generic LDAP (defaults here need validation, i'm not as familiar with this one)
{code:java}
# ambari-server setup-ldap
Using python  /usr/bin/python
Currently 'no auth method' is configured, do you wish to use LDAP instead [y/n] (y)?
Primary LDAP Host (ldap.ambari.apache.org):
Primary LDAP Port (389):
Secondary LDAP Host <Optional>:
Secondary LDAP Port <Optional>:
Use SSL [true/false] (false):
Do you want to provide custom TrustStore for Ambari [y/n] (n)?
User object class (posixUser): 
User ID attribute (uid): 
Group object class (posixGroup):
Group name attribute (cn): 
Group member attribute (memberUid): 
Distinguished name attribute (dn): 
Search Base (dc=ambari,dc=apache,dc=org): 
User Search Base (cn=users,dc=ambari,dc=apache,dc=org): 
Group Search Base (cn=groups,dc=ambari,dc=apache,dc=org): 
Referral method [follow/ignore] (follow): 
Bind anonymously [true/false] (false): 
Bind DN ( uid=ldapbind,cn=users,dc=ambari,dc=apache,dc=org):
Enter Bind DN Password: 
Confirm Bind DN Password: 
Handling behavior for username collisions [convert/skip] for LDAP sync (skip): 
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-26 04:54:30,16
13168227,Add alert for HBase_Regionserver of Metrics Collector,"Since distributed mode is used for AMS,we should add alert for ams_hbase_regionserver process.",pull-request-available,['ambari-metrics'],AMBARI,Improvement,Major,2018-06-26 02:01:24,47
13168203,Log Feeder: read and ship docker container logs,"- implement a container registry which can update / gather docker metadata for logfeeder (like actual container log path etc.)

- new field in input configuration: ""docker"", e.g.:
{code:java}
""inputs"" : [
{
  ...
 ""docker"" : ""true""
}
{code}

If this value is set to `true`, then for every line a json pattern should be applied first (before go with anything else like grok filters)",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-06-25 23:11:26,29
13168145,Assign Slaves and Clients Page missing warning message when no slave/client is selected,"
If no hosts are selected in assign slaves and clients page Ambari used to show warning in earlier versions, which is missing in Ambari-2.7.0

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-25 20:26:07,13
13168141,Build failure due to bower version deprecation,"The build is unable to resolve {{bower}} dependency, with HTTP 502 BAD GATEWAY.",pull-request-available,['contrib'],AMBARI,Bug,Major,2018-06-25 20:07:19,34
13168058,Database tab disabled if Setup Database and Database User,"Although 'Database' tab doesn't allow to go to the next step if the required properties are empty it is not allowing even if there are no required properties left on the screen.

STR
1. Go to Customize Services --> Databases --> Ranger.
2. Enter the Ranger DB host.
3. Set/Toggle 'Setup Database and Database Use' to No.

Although no required properties are present but the 'Next' button is disabled.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-25 14:03:24,19
13168012,Ambari gets stuck at host checks (after successful registration) when installing more number of nodes via UI,"While installing 22 node cluster via ambari, it gets stuck at checking hosts for potential warnings after successful registration.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-25 11:06:04,6
13168010,Deleting a service fails to remove keytabs,"while deleting Atlas service, faced this issue:

{code}
stderr: 
Caught an exception while executing custom service command: : Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969; Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969
 stdout:
Caught an exception while executing custom service command: : Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969; Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969

Command failed after 1 tries
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-25 10:48:18,18
13167998,Error processing agent reports due to wrong stack usage,"{noformat}
22 May 2018 15:33:05,830 ERROR [agent-report-processor-0] AgentReportsProcessor:90 - Error processing agent reports
org.apache.ambari.server.StackAccessException: Stack data, stackName=HDPCORE, stackVersion=1.0.0-b368, stackServiceName=HBASE
        at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:604)
        at org.apache.ambari.server.api.services.AmbariMetaInfo.getComponent(AmbariMetaInfo.java:357)
        at org.apache.ambari.server.state.host.HostImpl.calculateHostStatus(HostImpl.java:1247)
        at org.apache.ambari.server.agent.HeartbeatProcessor.processHostStatus(HeartbeatProcessor.java:300)
        at org.apache.ambari.server.agent.HeartBeatHandler.handleCommandReportStatus(HeartBeatHandler.java:275)
        at org.apache.ambari.server.agent.AgentReportsProcessor$AgentReportProcessingTask.run(AgentReportsProcessor.java:83)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-25 09:40:36,21
13167806,Agent failed to process execution command,"Some execution commands were failed during blueprint deploy:

    
    
    
    ERROR 2018-06-19 22:29:28,058 ActionQueue.py:221 - Exception while processing EXECUTION_COMMAND command
     Traceback (most recent call last):
       File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 214, in process_command
         self.execute_command(command)
       File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 352, in execute_command
         commandresult['stdout'] += '\n\nCommand completed successfully!\n' if status == self.COMPLETED_STATUS else '\n\nCommand failed after ' + str(numAttempts) + ' tries\n'
     UnboundLocalError: local variable 'commandresult' referenced before assignment
     INFO 2018-06-19 22:29:28,100 ActionQueue.py:238 - Executing command with id = 4-0, taskId = 5 for role = MAPREDUCE2_CLIENT of cluster_id 2.
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-23 09:17:58,5
13167701,Fix issues in AMS aggregation and writes.,"* AMS aggregation does not work when metrics have instanceId defined
* Fix bug in distributed 5-min host aggregator.
* Fix issues where AMS discards whole set of metrics even when 1 of them has NaN values.
* Fix issues in precision calculation whereby the table TTL is also taken into account.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-06-22 17:39:45,39
13167664,Test Connection button not working in Database Tab,"Before running below command, 'Test Connection' button was showing the excepted output to run below command:
{noformat}
ambari-server setup --jdbc-db=mysql --jdbc-driver=/root/mysql-connector-java.jar
{noformat}

After running the command, 'Test Connection'  keeps on spinning the loader and browser console shows below error
{noformat}
Uncaught TypeError: Cannot read property 'filter' of undefined
    at Class.getConnectionProperty (app.js:216783)
    at Class.<anonymous> (app.js:216763)
    at ComputedPropertyPrototype.get (vendor.js:14954)
    at get (vendor.js:13360)
    at Class.get (vendor.js:19791)
    at Class.getTaskInfoSuccess (app.js:216816)
    at Class.opt.success (app.js:192713)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
{noformat}

This issue is not seen on 'All Configurations' tab under services smart config tab.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-22 13:54:13,19
13167637,Express Upgrade Blocked on Missing OS in repo_version,"We should add a prereq check that for EU, the SOURCE version has an entry for each OS type in the cluster, and the TARGET version also has an entry (this better be true, or distribution would fail).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-22 11:58:09,18
13167597,Files View Is Not Properly Displayed,"Files View is not properly displayed, please find the attached screenshot.",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-06-22 08:56:19,50
13167576,Metric Collector goes down after HDFS restart post EU,"
**STR**

  1. Deployed cluster with Ambari version: 2.6.1.5-3 and HDP version: 2.6.1.0-129
  2. Upgrade Ambari to Target Version: 2.7.0.0-709
  3. Upgrade AMS and Smartsense (keeping them stopped)
  4. Perform EU to HDP-3.0 and let it complete
  5. Restart HDFS
  6. Observe state of Metrics Collectors (AMS is configured in distributed mode)

**Result**  
Both metrics collectors are down (auto start is enabled for Metrics Collector)

From logs:

    
    
    
    2018-06-13 16:45:05,620 ERROR org.apache.ambari.metrics.core.timeline.discovery.TimelineMetricMetadataManager: TimelineMetricMetadataKey is null for : [-8, 31, -72, 32, 88, -8, -51, -88, -104, 12, -123, 99, 55, -90, 45, -12, 115, 0, -6, 13]
    2018-06-13 16:45:05,622 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
    java.lang.NullPointerException
            at org.apache.ambari.metrics.core.timeline.aggregators.TimelineMetricReadHelper.getTimelineMetricCommonsFromResultSet(TimelineMetricReadHelper.java:116)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getLastTimelineMetricFromResultSet(PhoenixHBaseAccessor.java:446)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getLatestMetricRecords(PhoenixHBaseAccessor.java:1134)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getMetricRecords(PhoenixHBaseAccessor.java:953)
            at org.apache.ambari.metrics.core.timeline.HBaseTimelineMetricsService.getTimelineMetrics(HBaseTimelineMetricsService.java:288)
            at org.apache.ambari.metrics.webapp.TimelineWebServices.getTimelineMetrics(TimelineWebServices.java:261)
            at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
    
    2018-06-13 16:45:07,887 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=ctr-e138-1518143905142-361872-01-000005.hwx.site:2181,ctr-e138-1518143905142-361872-01-000006.hwx.site:2181,ctr-e138-1518143905142-361872-01-000003.hwx.site:2181 sessionTimeout=120000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$13/572967831@60474c94
    2018-06-13 16:45:07,889 INFO org.apache.zookeeper.client.ZooKeeperSaslClient: Client will use GSSAPI as SASL mechanism.
    2018-06-13 16:45:07,891 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
    2018-06-13 16:45:07,891 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181, initiating session
    2018-06-13 16:45:07,894 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181, sessionid = 0x363f94c8d6d0059, negotiated timeout = 90000
    2018-06-13 16:45:11,938 INFO org.apache.hadoop.hbase.client.RpcRetryingCallerImpl: Call exception, tries=6, retries=6, started=4153 ms ago, cancelled=false, msg=Call to ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320, details=row 'SYSTEM.CATALOG' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=ctr-e138-1518143905142-361872-01-000007.hwx.site,61320,1528896330963, seqNum=-1
    2018-06-13 16:45:15,954 INFO org.apache.hadoop.hbase.client.RpcRetryingCallerImpl: Call exception, tries=7, retries=7, started=8169 ms ago, cancelled=false, msg=Call to ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320, details=row 'SYSTEM.CATALOG' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=ctr-e138-1518143905142-361872-01-000007.hwx.site,61320,1528896330963, seqNum=-1
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-22 07:19:21,5
13167569,hive view : remove conflicting set statements while uploading the table,"The following 2 set statements can give error at run time since they are supposed to be only set through hive-site.xml file.

https://github.com/nitirajrathore/ambari/blob/4595bd2a239cbe4838c8eeb5410b161a3d8a7ee6/contrib/views/hive20/src/main/java/org/apache/ambari/view/hive20/internal/query/generators/InsertFromQueryGenerator.java#L51


    insertQuery.append(""SET hive.support.concurrency=true;"").append(""\n"");
    insertQuery.append(""SET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;"").append(""\n"");
",pull-request-available,[],AMBARI,Bug,Major,2018-06-22 06:40:48,38
13167449,Support for cluster-settings (remove cluster-env),"Per the configuration refactoring occuring for Management Pack support, all configuration types will be scoped at the service level, rather than at the cluster level.  

The Blueprint processor will need to be updated, in order to treat all configuration at the service level.  The Blueprint processor will need to take the configuration specified in a Blueprint, and convert this (where possible) to a service-level configuration in the internal representation of the configuration.  

The Blueprint processor will need to implement support for ""cluster-settings"", which replaces the ""cluster-env"" configuration type.  

We should accept older Blueprints with ""cluster-env"", and make the necessary conversions on the server side during deployment.  

",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-06-21 17:44:42,20
13167435,Cannot distinguish components on Host Details page due to shortened display name (Ambari should show full component name on mouse over),"Yarn has 2 version of timeline service .
1) TIMELINE SERVICE V1.5
2) TIMELINE SERVICE V2.0 READER
When both of these services are installed on one host, go to Component page . Component Page only shows first few chars ""Timeline Service... "".
Ambari UI shows ""Timeline Service.."" for both Timeline service 1.5 and 2.0 . Thus, user can not identify which is Timeline service 1.5 or 2.0
ambari should show the full component name when user brings mouse pointer on the component name.
",pull-request-available,[],AMBARI,Bug,Major,2018-06-21 16:03:54,15
13167373,Inaccurate error message showed during service removal in some circumstances  ,"When stop action for removed service failed and after this were posted DELETE request through API, error message will display desired status of the component instead of the current",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-21 11:28:58,1
13167364,Provide a way to disable topology validation in cluster creation request,"The topology validation that takes place during blueprint creation request can be disabled by passing {{validate_topology=false}} as query param.  Validation also has a side-effect of adding any auto-deployable components/dependencies to the topology.

For mpack-based deployment most of the validation is moved to the cluster creation request, since mpacks may not be available prior to that.  We need to provide a way to disable validation in this request, too.  Using the same {{validate_topology=false}} flag may be the best for this purpose.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-06-21 10:33:15,21
13167344,Backup External Metastore Database Task should communicate properly with the user,"In case of external Hive database the backup step should not fail if the db is not found, but there should be an additional step telling the user that they should check the previous step's log if the backup was successful. If it wasn't, then they should follow the instructions to do the backup manually.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-21 08:34:14,36
13167256,Remove RepositoryVersionEntity and Begin Switching Checks To Use UpgradePlan,"This commit includes the following changes:

- Final removal of all {{RepositoryVersionEntity}} references
- Switching pre-upgrade checks over to using an Upgrade Plan
-- Mostly just giving them a {{null}} plan
-- In some cases, iterating over Service Groups to get the source/target mpacks
- Cleaning up a lot of much in the pre-checks to make them cleaner and easier to extend by mpacks
- Removal of reliance on a single Upgrade Pack in the pre-checks",pull-request-available,[],AMBARI,Task,Critical,2018-06-20 20:48:53,31
13167191,Ranger and KMS tab still present in Customize ServicesPage even after going back and deselecting them from Choose ServicesPage,"- Navigate to Customize ServicePage of UI install wizard by selecting all services from ChooseServicesPage. 
- Populate credentials tab
- Switch to Databases tab, Noticed that Ranger and Ranger KMS have multiple properties to be filled in
- Now go back to Choose Services page and deselect Ranger and Ranger KMS
- Proceed through wizard to reach Customize ServicesPage
- Ranger and KMS sub tabs are still present with errors at Databases tab. 
(None of the ranger/KMS credential properties are present in Credential tabs)

- Now even if we go back and choose Ranger and KMS - update all necessary properties for Ranger - Test Connection is hung with error",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-20 15:27:56,15
13167164,DB Connectivity Warnings for oozie & hive show up even after tests are passed,"During initial installation, if both hive and oozie are configured to use external databases (and therefore both test connection), the test results seem to get lost. This causes a warning to appear, even though both tests have passed",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-20 13:18:27,7
13167147,Customize Service step issues,"There are some issues in Step 7 of the Installer Wizard
1. Even if there are some *required* changes in the 'Database' tab it allows to go to the next step. Next should only be enabled if the required property has been provided with an input
2. If there are some CRITICAL errors, the top notification bell should be 'Red' (similar to what we see if there is an empty required property) 
3. If I click on any of the above critical error properties it takes me to a wrong page. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-20 12:06:15,19
13167118,Ambari Workflow Manager (wfmanager) sends plaintext content over API. JSON is expected.,"The Ambari API is expected to respond with JSON content.

However the wfmanager's /resources/proxy/getCurrentUserName returns plaintext content.

This is breaking Knox proxying of the UI and is likely to break other proxies/clients who expect the API to respond with JSON.

https://github.com/apache/ambari/blob/trunk/contrib/views/wfmanager/src/main/java/org/apache/oozie/ambari/view/OozieProxyImpersonator.java#L154-L158

Full API url would be:
/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/instance_name/resources/proxy/getCurrentUserName

How to reproduce:
# GET http://hostname:8080/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/instance_name/resources/proxy/getCurrentUserName
# Notice that it responds with a username only which is plaintext not JSON.",pull-request-available,"['ambari-views', 'contrib']",AMBARI,Bug,Major,2018-06-20 09:38:47,45
13167069,Agent fails to auto-start HDFS when using LZO,"Service auto-start for HDFS fails with the following error when using LZO libraries:
{noformat:title=auto_errors-...txt}
...
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs.py"", line 137, in hdfs
    install_lzo_if_needed()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/lzo_utils.py"", line 87, in install_lzo_if_needed
    Script.repository_util.create_repo_files()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/repository_util.py"", line 53, in create_repo_files
    if self.command_repository.version_id is None:
AttributeError: RepositoryUtil instance has no attribute 'command_repository'
{noformat}
{{repositoryFile}} entry is missing from auto-start commands.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-20 05:51:06,5
13166972,Make STOMP updates immutable,STOMP event update object can be corrupted before emitting to subscribers by other update handling.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-19 16:20:18,6
13166963,Metrics migrated during AMS upgrade are not saved into metadata table,"the data migration itself works, but the UUIDs calculated for those metrics are not inserted into METADATA_UUID table

 ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-06-19 15:23:31,28
13166914,Ambari Master selection page breaks when you have large number of hosts,"I'm trying to create a a cluster via Ambari UI with 16 nodes. In the Master selection page, if i select the the service on the bottom page and open up the dropdown for host, the page breasks as the dropdown is larger than the bottom div. My host maping image on the right dissapers and my navigation buttons dissaper. I have to refresh the page to get around it..

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-19 12:09:10,9
13166842,Show/Hide config-section not working for Ranger and Ranger KMS theme,"Ranger service smart config tabs have Ranger Admin Tab, in that if 'Setup Database and Database User' toggle is set to 'Yes', show the Ranger Root Database related section. 

Currently the toggle is not working as expected.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-19 04:41:32,34
13166835,Rolling restarts not working when Kerberos Auto sign in is enabled for Ambari,"Rolling restarts not working when Kerberos Auto sign in is enabled for Ambari.  This is due to a missing user ID value in the {{requestschedule}} table, resulting in the Rolling Restart operation aborting with no deals in the ambari-server.log file.

||schedule_id||cluster_id||authenticated_user_id||create_user||update_user||last_execution_status||
|2|2|-1|rlevas|rlevas|IN_PROGRESS|

This occurs because the {{org.springframework.security.core.Authentication}} class stored in Ambari's SecurityContext for the authenticated user does not contain the authenticated user's ID data. Adjustments need to be made to the infrastructure to capture this data upon login and then retrieve it later, when needed. 

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-19 03:00:24,30
13166725,"Use name, version, and registry ID when registering mpacks","When registering mpacks from a registry, the UI should use the form of the registration request that passes the mpack name, mpack version, and registry ID instead of the form that passes the mpack.json URL. Using the verbose form allows the server to associate the mpack with the registry from which is was registered.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-18 15:01:24,40
13166709,Failed to force_non_member_install a stack version on hosts,"The ability to pre-install packages on hosts using the following API request is broken since 2.6.0:

{noformat}
$ curl -X POST -d '{ ""HostStackVersions"": { ""repository_version"": ""2.6.1.0-129"", ""stack"": ""HDP"", ""version"": ""2.6"", ""cluster_name"": ""TEST"", ""force_non_member_install"": true, ""components"": [ { ""name"" : ""ZOOKEEPER_SERVER"" }, { ""name"": ""ZOOKEEPER_CLIENT"" } ] } }' http://localhost:8080/api/v1/hosts/${hostname}/stack_versions
{noformat}

The request is accepted, but:

* on 2.6.0: package installation (1st task) fails due to missing {{stack_name}} and {{stack_version}}
* on 2.6.1: ""set all"" (2nd task) fails for the same reason",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-18 13:51:17,21
13166705,Customer cannot back from step 4 to step 3 during adding a new host to cluster,"STR:
1) Deploy cluster
2) try to add a new host (only with client) to the cluster
3) On step 4 try to get back on step 3 (using button back)

Actual result: Customer cannot back from step 4 to step 3 during adding a new host to cluster.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-18 13:39:22,19
13166703,Mpack display for a service showing NULL,"On the summary view for a service, the Mpack display is showing ""NULL (NULL)"" where it is expected to show the correct Mpack name and (version) for the service.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-18 13:20:09,40
13166702,"Components Filter in ""Manage ConfigGroup at Customize Services page"" doesn't list hosts correctly","- At Assign Slaves and Client page chose to have clients on all nodes
- At Customize Services Page create a config group by navigating to ManageConfigGroups
- create a new group
- While adding hosts use filter COMPONENTS to find hosts with particular component
- It doesn't list any host for HDFS client. It should actually list host based on what user selected in Assign slaves page",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-18 13:19:21,15
13166682,Stop of Knox Gateway fails after deleting Hive from cluster post upgrade.,"
*STR*
1) Deployed cluster with Ambari version: 2.6.2.0-155 and HDP version: 2.6.2.0-205
2) Ambari upgrade - 2.7.0.0-709
3) Stack upgrade to 3.0.0.0-1478
4) Delete Hive from the cluster
5) Stop All Services
Stop of Knox Gateway fails with below error:


{code:java}
Traceback (most recent call last):
File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KNOX/package/scripts/knox_gateway.py"", line 215, in <module>
KnoxGateway().execute()
File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
method(env)
File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KNOX/package/scripts/knox_gateway.py"", line 152, in stop
import params
File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KNOX/package/scripts/params.py"", line 27, in <module>
from params_linux import *
File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KNOX/package/scripts/params_linux.py"", line 229, in <module>
hive_server_host = hive_server_hosts[0]
IndexError: list index out of range
{code}


",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-18 10:55:13,13
13166677,Failure of component install leave ambari ui with inactive Customize Services,"Tried installing hive on a cluster where hive client install failed with following error

{noformat}
resource_management.core.exceptions.ExecutionFailed: Execution of 'cp --remove-destination /var/lib/ambari-agent/tmp/mysql-connector-java.jar /usr/hdp/current/hive-client/lib/mysql-connector-java.jar' returned 1. cp: cannot create regular file '/usr/hdp/current/hive-client/lib/mysql-connector-java.jar': No such file or directory
{noformat}

Post that it is unable to customize any configs since all links except Assign Masters was inactive",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-18 10:20:11,19
13166667,Deleting and adding the same host fails in Ambari,"# Delete an existing host
# Add the same host back via the Add Host wizard
# Navigate to the Review step and click Deploy button

The same step works when the page is refreshed.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-18 09:39:43,9
13166664,Manage Config groups option is missing in add service wizard	,Manage Config groups option is missing in add service wizard on step 4,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-18 09:16:41,7
13166625,Unable to Perform Rolling Restarts Because of Outdated Postgres Driver,"When attempting to perform any rolling restart where Ambari's database is Postgres 10+, the following error is seen in the logs:

{code}
2018-06-15 20:31:24,606  WARN [C3P0PooledConnectionPoolManager[identityToken->1br4e2s9wodqogrilkm7|7536ef0f]-HelperThread-#1] BasicResourcePool:223 - com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@48d8907f -- Acquisition Attempt Failed!!! Clearing pending acquires. While trying to acquire a needed new resource, we failed to succeed more than the maximum number of allowed acquisition attempts (30). Last acquisition attempt exception: 
org.postgresql.util.PSQLException: This ResultSet is closed.
	at org.postgresql.jdbc2.AbstractJdbc2ResultSet.checkClosed(AbstractJdbc2ResultSet.java:2852)
	at org.postgresql.jdbc2.AbstractJdbc2ResultSet.setFetchSize(AbstractJdbc2ResultSet.java:1875)
	at org.postgresql.jdbc4.Jdbc4Statement.createResultSet(Jdbc4Statement.java:37)
	at org.postgresql.jdbc2.AbstractJdbc2Statement$StatementResultHandler.handleResultRows(AbstractJdbc2Statement.java:219)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1816)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403)
	at org.postgresql.jdbc2.AbstractJdbc2Connection.execSQLUpdate(AbstractJdbc2Connection.java:376)
	at org.postgresql.jdbc2.AbstractJdbc2Connection.getTransactionIsolation(AbstractJdbc2Connection.java:898)
	at com.mchange.v2.c3p0.impl.NewPooledConnection.<init>(NewPooledConnection.java:120)
	at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:240)
	at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:206)
	at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:203)
	at com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1138)
	at com.mchange.v2.resourcepool.BasicResourcePool.doAcquireAndDecrementPendingAcquiresWithinLockOnSuccess(BasicResourcePool.java:1125)
	at com.mchange.v2.resourcepool.BasicResourcePool.access$700(BasicResourcePool.java:44)
	at com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask.run(BasicResourcePool.java:1870)
	at com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:696)

{code}

Ambari uses Quartz to schedule the rolling requests and the current Postgres driver (9.3) doesn't work against a Postgres 10 database.",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-18 00:32:41,31
13166576,Ranger/Atlas Default Install Changes,"There are more changes that we need to make to streamline the default Atlas/Ranger installation
 # Change Limited Functionality Warning message for Ranger.
 # Remove Ranger Requirements modal
 # Remove DBA credentials for Ranger and Ranger KMS.
 # Add jdbc warning similar to Hive for Ranger and Ranger KMS",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-17 00:35:36,34
13166540,Add notification to the Alert Groups not working,"Add notification to the Alert Groups not working.

STR:
1. Navigate to alerts page
2. Open Manage Alert Groups
3. Select an Alert Group
4. Add Notification by clicking Add and selecting a 'pre created' Alert Notification
5. Click Save and close the popup, the alert group should be saved with selected notification type, but only an empty String is being saved.

E.g API being invoked:

{code:java}
PUT https://<host>:<port>/api/v1/clusters/<cluster>/alert_groups/4

{""AlertGroup"":{""name"":""AMBARI_INFRA_SOLR"",""definitions"":[14],""targets"":[4]}}: 
{code}

Notes:
1. Observed that the auto fill for the notification is case sensitive, which is not ideal.
2. Also observed trying to add another notification after performing the steps in STR, a 500 server error is thrown.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-16 14:10:39,15
13166531,Username case changes from Cluster info page to install wizard,Cluster info page shows user name as 'admin' where as in rest of the pages its switched to uppercase and even after deployment.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-16 11:16:14,9
13166527,"Ambari setup-ldap does not prompt for ldaps cert path, even when use-ssl is set to true","Running  ambari-server setup-ldap with use SSL option gives the following error:

{code}
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 1056, in <module>
    mainBody()
  File ""/usr/sbin/ambari-server.py"", line 1026, in mainBody
    main(options, args, parser)
  File ""/usr/sbin/ambari-server.py"", line 976, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/setupSecurity.py"", line 799, in setup_ldap
    if get_YN_input(""Do you want to remove these properties [y/n] (y)? "", True, options.trust_store_reconfigure):
AttributeError: Values instance has no attribute 'trust_store_reconfigure'
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-16 10:17:40,18
13166452,"In collapsed navigation view, the list of services is not scrollable","With the new UI navigation, when a large number of services is installed, the list of services shown is not scrollable in collapsed navigation.
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-15 19:28:34,32
13166435,Duration for a failed BG Operation is incorrect,"There are several failed install components and they report the incorrect duration.

Example: Install Nodemanager failed and was executed only for a few seconds, whereas in the Duration column, it shows 16 hours.
 !Screen Shot 2018-06-15 at 12.12.38 PM.png! 

Primarily because the output from the server has the end time as -1:
{noformat}
{
  ""itemTotal"" : ""10"",
  ""items"" : [
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 31,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install NFSGateway"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1528983750924,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 32,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install NodeManager"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1528983763421,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 33,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install YARN Client"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1528983776062,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1528985439608,
        ""id"" : 34,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Reinstall Failed Components"",
        ""request_status"" : ""COMPLETED"",
        ""start_time"" : 1528985380177,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1528985625102,
        ""id"" : 35,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Reinstall Failed Components"",
        ""request_status"" : ""COMPLETED"",
        ""start_time"" : 1528985594981,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1528986079631,
        ""id"" : 36,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""put services into STARTED"",
        ""request_status"" : ""COMPLETED"",
        ""start_time"" : 1528986013070,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1529043756536,
        ""id"" : 37,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Start Knox Gateway"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529043756451,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 38,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install MapReduce2 Client"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529044241508,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 39,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install Hive Client"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529044679827,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 40,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install Metrics Collector"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529045027390,
        ""user_name"" : ""admin""
      }
    }
  ]
}
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-15 17:47:03,27
13166430,cluster version is in invalid state,"Steps to reproduce:

1. Make sure you have stable cluster

2. add a new host - and make sure it fails with the installation at the end step.

3. Now go to ""stack and versions"" screen - you will see the cluster in kind of messed up state.

Usually customers may not see the 3rd step right after adding the host and if they observe after few days then it gives very wrong impression that cluster is it messed up state though it is not.

Output for host_version


{noformat}
select * from host_version where host_id in ( 301, 551) and repo_version_id = 102

'901','102','301','CURRENT'
'1052','102','551','OUT_OF_SYNC'

2nd row is problematic one.

{noformat}

Workaround:

Remove the host and add it again and make sure installation is not failing
or
change the status of host_version to 'CURRENT'",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-15 17:04:09,19
13166427,TaskWrapper should only wrap a single task,"{{TaskWrapper}} should be holding one and only task - the hosts are the multiplicity of the wrapper, not the task.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-15 16:58:22,33
13166384,"Ambari ""assign masters"" should display available hosts in sorted order ",The service layout often needs manual customization. We can make this process a little easier for users by listing the available hosts in some sort of sorted order (preferably alphabetized).,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-15 14:44:34,9
13166380,Web Client Pulls Back Too Much Information in Upgrade Wizard,"During an upgrade, the upgrade wizard is very inefficient at the data that it pulls back. On larger upgrades, expanding groups in progress is practically unusable and can actually cause the browser to crash.

I recently observed this on a small upgrade but quickly opening and closing the active group (see screenshot). The expansion of groups/tasks just sat there spinning.

I think that the upgrade wizard needs to pull back a lot less data and possibly cache the majority of what it has in order to make it usable during a larger upgrade.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-15 14:33:40,19
13166377,Start all services with a single request,"During wizard installation, start all services using a single request rather than one request per service group. This allows the services to be started in the correct order. However, we currently need to use the deprecated endpoint to do this since the new one is not available yet.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-15 14:24:44,40
13166361,Ranger server password checks are not performed during Cluster Install Wizard,"While installing a new cluster, if the Ranger-specific passwords do not meet the (non-obvious) requirements, failures will occur during the install Ranger task. Once this happens, there is no way for the user to go back and fix the issue. So the issue needs to be caught sooner.

STR
# Create new Ambari 2.7.0 cluster
# Include Ranger
# Set simple passwords when prompted - for example: hadoop
# Proceed to install
# See failure",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-15 13:47:14,15
13166356,Actions for components adding are absent at the Druid service page	,"Actions for components adding are absent at the Druid service page

Expected components adding actions:
{code:java}
ADD_DRUID_BROKER, ADD_DRUID_ROUTER, ADD_DRUID_OVERLORD, ADD_DRUID_COORDINATOR{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-15 13:40:11,7
13166353,Requests failed after Ambari upgrade with exception while executing custom service command,"*STR*
 # Deployed cluster with Ambari version: 2.6.2
 # Upgrade Ambari to Target Version: 2.7.0
 # Restart all services with stale configs

*Result*
Some of the requests failed with below error
{code:java}
stderr: 
Caught an exception while executing custom service command: : 'Metadata for cluster_id=2 is missing. Check if server sent it.'; 'Metadata for cluster_id=2 is missing. Check if server sent it.'
 stdout:
Caught an exception while executing custom service command: : 'Metadata for cluster_id=2 is missing. Check if server sent it.'; 'Metadata for cluster_id=2 is missing. Check if server sent it.'

Command failed after 1 tries{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-15 13:39:36,6
13166349,Configs not defined in stack are not getting added,"Due to a property (""configTypes"") not being populated at a certain point in the code, any config that is provided by mpack advisor but that was NOT part of the configs defined in the stack xml files is not getting added to the final set of configurations to be used when deploying the cluster.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-15 13:25:09,40
13166328,Add Ranger Kms Server doesn't not trigger Install request while enabling HA,"Steps to reproduce:
- Install Cluster with Ambari-2.7.0.0
- Add Ranger service +  Ranger KMS service
- Go to Action option of Ranger KMS service > Click on Add Ranger KMS server > Confirmation Pop up shows up, after clicking Confirm ADD option Install request is not triggered.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-15 11:19:07,19
13166320,Adding a New Component to a Host Doesn't Get Reflected Without a Hard Refresh	,"STR:
 - Install a simple cluster with ZK and HDFS (without NFS Gateway)
 - Pick a host and install NFS Gateway on that host

After installation and browsing around for a bit, the dashboard for HDFS still shows 0/0 NFS Gateway. You have to do a hard refresh to get it to show the new component.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-15 10:58:48,7
13166186,Ranger & Atlas initial install require additional properties to be explicitly configured in the UI,"Great to see Ambari now allows deployment of Ranger and Altas services at initial cluster install time. However, doing so complicates the install process, as there are some additional configurations the user is required to explicitly fill. Could we pre-populate this information, as to avoid interrupting the install flow? ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-14 19:33:24,34
13166184,Upgrade History Model Changes,"Upgrade History must be tied to MPacks/Service Groups/Service IDs. The following changes (in bulk) need to be performed:

- Removing {{RepositoryVersionEntity}} from upgrade history
- Associating history with a specific service group
-- Associating that service group with to/from mpack versions
- Exposing upgrade summary information for
-- Service Groups
-- Modules
-- Module Components
",pull-request-available,[],AMBARI,Task,Critical,2018-06-14 19:28:56,31
13166099,topology_script.py return incorrect data,"STR: 
1) Change the rack of master HOST on '/\{YOUR_NEW_RACK_NAME}' 
2) Restart all required services 
3) Check that execute of bash 'topology_script.py HOST' returned \{YOUR_NEW_RACK_NAME}

Actual result: topology_script.py return incorrect data, when the rack of host was changed on UI.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-14 13:23:45,28
13166086,Update the Manage JN wizard step for multiple nameservice scenario,Update the Save namespace commands based on available nameservices.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-14 12:19:36,15
13166080,Blueprint MPack Auto-Download needs more detailed error reporting,"The current error message does indicate which MPack is causing the issue, and the ambari-server logs do show the exceptions causing the problem, but it might be a good idea to make this error more obvious in the client error message as well.

This JIRA tracks the work to investigate this problem, to determine if more error-related information can be added to the client response when failures of this type are detected.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-14 11:54:23,20
13166077,Websocket connection. Port is hardcoded,"If the URL does not contain port then the protocol defines the port in case of http it is 80, in case of https it is 443, if it contains it we should use it.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-14 11:40:45,7
13166074,"Service check failure is not skipped during upgrade even though ""Skip All Service Check Failures"" options is selected before Upgrade Start","
Even after selecting ""Skip all service check failures"" option before starting upgrade , the upgrade is not ignoring service check failures and upgrade gets paused at the failure step.
The expected behaviour is: it should skip the failed service check and have a message like ""There are failures that were automatically skipped"".

This behaviour is working fine for Slave Component failure skip.

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-14 11:29:59,13
13166066,Filter have incorrect name on host page,"Actual result: some filter for components has an incorrect name ""Install Pending"" but should be ""Install Pending...""",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-14 10:35:39,9
13166050,Canceling task during blueprint install results in agent not responding to any other tasks,"If failing operation which is in retry cycle is canceled during blueprint
install. The agent will still continue to execute the canceled task. Resulting
in ActionQueue being stuck and no other task being able to start execution

",pull-request-available,[],AMBARI,Bug,Major,2018-06-14 09:33:24,5
13165938,Add Upgrade Pack to Upgrade Plan,Add the upgrade pack to the detail for an UpgradePlan.  This value will be used to test the API and will eventually be calculated when creating a plan that doesn't include this value.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-06-13 20:53:04,33
13165912,Update usage of StackService data model due to change in ID format,"The StackService model data model need to be changed so that the IDs are no longer just the service name, since we can have the same service in multiple service groups and we need to be able to retrieve the service metadata specific to the mpack applied to each service group. Therefore, the ID format has been changed to <serviceName>\-<mpackName>\-<mpackVersion>. All usages of the StackService model must be updated to account for this change.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-13 19:24:26,40
13165871,Delete Button not enabled on Druid Router component,"facing issue while trying to delete Druid Router, where the delete button is disabled. 

The Test performs the following:
* Select component from list (after clicking on add button)
* Start component
* Restart all component with stale config
* Run Service check against service
* Stop component
* Delete component
* Restart all component with stale config
* Run Service check
* Repeat the above steps for all possible components.

Noticed that the delete button was not enabled for Druid Router only. When I accessed the cluster later, the delete button was enabled. This looks like an intermittent issue. Only once I could reproduce this on my local after doing various actions related to add/delete component.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-13 15:23:39,19
13165868,Need xpaths for content/textarea to be unique,"Please see screenshot attached. 
xpath //div[@data-qa='property-form-group']//textarea[@data-qa='content'] 4 textarea nodes. 
There is no other attribute that can be used to distinguish between them.
for eg: title is also empty on all. It would be great if you could take a look and make each of them unique.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-13 15:18:44,34
13165859,Blueprint should add service dependencies from specific mpack,"Currently client components need to be specified in the blueprint once per mpack, and {{mpack_instance}} needs to be explicitly declared, which is a bit cumbersome.

{{DependencyAndCardinalityValidator}} should be mpack-aware, and add client components from each mpack as required.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-06-13 14:41:15,21
13165846,Resuming a Paused Upgrade Attempts To Retry Tasks Which Were Already Passed,"STR:
- Perform a stack upgrade where during the upgrade, there's a slave failure due to a timed out task
- Ignore and Proceed past this task and get to the Component Version Check. It should fail (since you had a timed out task on a slave)
- Pause the upgrade and then fix the version by restarting the component
- Resume the upgrade

The upgrade attempts to place the timed out task back into a PENDING state. 

I think I have an idea about what's going on here. Consider the following upgrade tasks:

- 1: COMPLETED
- 2: HOLDING_TIMEDOUT
- 3: PENDIND
- 4: PENDING
- 5: PENDING

When you ""Ignore and Proceed"", it sets the {{HOLDING_TIMEDOUT}} to {{TIMEDOUT}} which is technically a completed state:

- 1: COMPLETED
- 2: TIMEDOUT
- 3: COMPLETED
- 4: HOLDING_FAILED
- 5: PENDING

Now, you go to pause the upgrade and we set every ""scheduled"" state to {{ABORTED}}, so this preserves existing states:

- 1: COMPLETED
- 2: TIMEDOUT
- 3: COMPLETED
- 4: ABORTED
- 5: ABORTED

When you go to resume the upgrade, it searches for all {{ABORTED}} _AND_ {{TIMEDOUT}} to reset to {{PENDING}}

- 1: COMPLETED
- 2: PENDING
- 3: COMPLETED
- 4: PENDING
- 5: PENDING

So, because an earlier task is now set to {{PENDING}}, this causes the scheduler to barf.
",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-13 13:57:40,31
13165817,Cannot add component to 25+ hosts,"STR:
1. On 26+ nodes cluster go to Hosts -> All Hosts
2. Add any component to all hosts:
3. Obverse this error from UI:
{noformat}
app.js:23508 Uncaught TypeError: Cannot read property 'get' of undefined
    at app.js:23508
    at Array.forEach (<anonymous>)
    at Class._getComponentsFromServerForHostComponentsAddCallback (app.js:23507)
    at Object.callback (app.js:23498)
    at Object.getComponentsFromServerSuccessCallback (app.js:181958)
    at Class.opt.success (app.js:181325)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{noformat}

As per discussion with UI guys. This is because model is only available for first 25 hosts, and so for others it's not possible to check if heartbeat is not lost.

Possibly the solution could be as simple as UI asking heartbeat state in scope of the request along with other info.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-13 11:41:53,19
13165762,"The alert ""DataNode Unmounted Data Dir"" did not appear","str:


1) execute commands on hosts:

dd if=/dev/zero of=/grid/0/fs bs=1 count=0 seek=10G
mkfs.ext3 -F /grid/0/fs
mkdir /grid/1/
mount -o loop,rw /grid/0/fs /grid/1/
chown cstm-hdfs:hadoop /grid/1/
2) set DN dir property (dfs.datanode.data.dir) to ""/grid/0/hadoop/hdfs/data,/newdndir,/grid/1/hadoop/hdfs"" and restart needed services 
3) check that alert is not present ""DataNode Unmounted Data Dir"" (after 2 min)
4) Stop DN(s)
5) umount /grid/1/
6) Start DN(s)
7) check that alert is present",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-06-13 07:59:01,18
13165684,setup-sso in Ambari fails when SSL is enabled,"When SSL is enabled and the python version is 2.7.14, accessing the Ambari server via the {{ambari-server}} CLI fails with CERTIFICATE_VERIFY_FAILED.

 
{noformat:title=Example}
-bash-4.2# ambari-server setup-sso -v
Using python /usr/bin/python
Setting up SSO authentication properties...
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Setup SSO.
INFO: about to run command: ps -p 33705
INFO:
process_pid=107113
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Enter Ambari Admin login: admin
Enter Ambari Admin password:
INFO: Fetching SSO configuration from DB
INFO: Fetching information from Ambari's REST API
Traceback (most recent call last):
 File ""/usr/sbin/ambari-server.py"", line 1056, in <module>
 mainBody()
 File ""/usr/sbin/ambari-server.py"", line 1026, in mainBody
 main(options, args, parser)
 File ""/usr/sbin/ambari-server.py"", line 976, in main
 action_obj.execute()
 File ""/usr/sbin/ambari-server.py"", line 90, in execute
 self.need_restart = self.fn(*self.args, **self.kwargs)
 File ""/usr/lib/ambari-server/lib/ambari_server/setupSso.py"", line 266, in setup_sso
 properties = get_sso_properties(ambari_properties, admin_login, admin_password)
 File ""/usr/lib/ambari-server/lib/ambari_server/setupSso.py"", line 221, in get_sso_properties
 response_code, json_data = get_json_via_rest_api(properties, admin_login, admin_password, SSO_CONFIG_API_ENTRYPOINT)
 File ""/usr/lib/ambari-server/lib/ambari_server/serverUtils.py"", line 206, in get_json_via_rest_api
 with closing(urllib2.urlopen(request)) as response:
 File ""/usr/lib64/python2.7/urllib2.py"", line 154, in urlopen
 return opener.open(url, data, timeout)
 File ""/usr/lib64/python2.7/urllib2.py"", line 429, in open
 response = self._open(req, data)
 File ""/usr/lib64/python2.7/urllib2.py"", line 447, in _open
 '_open', req)
 File ""/usr/lib64/python2.7/urllib2.py"", line 407, in _call_chain
 result = func(*args)
 File ""/usr/lib64/python2.7/urllib2.py"", line 1243, in https_open
 context=self._context)
 File ""/usr/lib64/python2.7/urllib2.py"", line 1200, in do_open
 raise URLError(err)
urllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)>
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-06-12 22:07:01,30
13165627,The livy2 Package in stack_packages is Wrong for conf-select,"There are 2 {{livy2}} instances in {{stack_packages}}

{code}
      ""livy2"": [
        {
          ""conf_dir"": ""/etc/livy2/conf"",
          ""current_dir"": ""{0}/current/livy2-client/conf"",
          ""component"": ""livy2-client""
        }
      ],
      ""livy2"": [
        {
          ""conf_dir"": ""/etc/livy2/conf"",
          ""current_dir"": ""{0}/current/livy2-client/conf""
        }
{code}",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-12 17:16:56,31
13165598,Allow clients from multiple mpacks to be installed,"Due to service names being used as unique IDs, the same service from multiple mpacks were not being installed, as configured in the installer wizard.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-12 15:28:46,40
13165592,Reordering of dashboard widgets doesn't work after enabling NN federation,"*STR*
# Enable NameNode federation
# Go to dashboard
# Try to reorder some widgets (common or namespace-specific ones)

*Expected result*
New widgets order is persisted

*Actual result*
- JS error is thrown: {{app.js:237946 Uncaught TypeError: Cannot read property 'getAttribute' of undefined}}
- New order isn't persisted",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-12 15:05:12,15
13165544,Ambari UI isn't loading in IE,"After submitting login credentials, UI is stuck on loading data.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-12 12:10:53,15
13165541,[UI] Ambari label do not navigate to Dashboard page,Ambari label(not img) do not navigate to Dashboard page.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-12 11:33:52,9
13165425,Components Added During Upgrade Are Not Scheduled for Restart,"During an upgrade, if a component is automatically added that is already a part of the cluster, it's not scheduled for restart. 

 

STR:
 * Install a cluster with ZK, HDFS, Hive, Spark
 * Make sure not to put Hive Client on every machine
 * During an upgrade, Hive Client is automatically added, but the new components are not scheduled for restart.",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-11 21:14:33,31
13165398,Upgrade wizard shows incorrect host count if there is failure on some host during any upgrade step.,"If there is a failure on any host during one of the upgrade steps , paused upgrade wizard at ""finalize upgrade pre checks"" shows incorrect hosts count (0) in the message.

Please see attached screenshot. datanode upgrade failed on one of the hosts. But still upgrade wizard says .
""Upgrade did not succeed on *0* hosts""
And on clicking 0 Hosts, following error is seen in console.
Expected behaviour is - message should have shown 1 hosts and on clicking , failed hosts should have been displayed.
{code}
 {code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-11 19:29:27,34
13165369,Guarantee execution command retrying task completion,Server should guarantee execution command retry task will not stuck.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-11 16:46:20,6
13165351,Deploy button doesn't show in Review Page of Move Master Wizard,"Noticed in the Review Page of Move Master Wizard for Hive Metastore is stuck and doesn't enable the Deploy button to proceed further.

Steps:
# Add Hive Metastore to a host
# Select Move action in the Host Details page for Hive Metastore
# Navigate the wizard till the Review Page

Javascript errors: 
{code}
app.js:31203 Uncaught TypeError: Cannot read property 'tag' of undefined
    at app.js:31203
    at Array.forEach (<anonymous>)
    at Class.getConfigUrlParams (app.js:31202)
    at Class.onLoadConfigsTags (app.js:31187)
    at Class.opt.success (app.js:183168)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-11 15:18:35,9
13165324,Guarantee STOMP update contains not null hash/timestamp if exists,Server should not populate STOMP update with null hash/timestamp.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-11 13:18:58,6
13165320,Adding alerts to alert groups not working,"STR:
1. Navigate to Alerts> Manage Alert groups >  Create alert group e.g Test_group
2. Add alerts to the alert groups and click Save button
3. Navigate to Alerts> Manage Alert groups > Select Test_group and the alerts selected should be listed in the alert group, but only empty entry is available.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-11 12:52:43,15
13165306, NN cannot start due do not have permission for creation of the folde,"STR:  
1) Install ambari cluster with custom user configuration via BP  
Cluster: <http://172.27.14.154:8080>

Actual result: NN cannot start due do not have permission for creation of the
folder ""/var/run/hadoop/cstm-hdfs""  
Looks like some script changed permission for

    
    
    
    [root@ctr-e138-1518143905142-357962-01-000006 ~]# ls -la /var/run/ | grep ""hadoop""
    drwxr-xr-x  2 cstm-ams       hadoop   4096 Jun 11 01:56 ambari-metrics-monitor
    drwxr-xr-x  6 root           root     4096 Jun 11 01:56 hadoop
    

NN Logs:

    
    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 414, in 
        NameNode().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 138, in start
        upgrade_suspended=params.upgrade_suspended, env=env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 115, in namenode
        format_namenode()
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 369, in format_namenode
        logoutput=True
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
        returns=self.resource.returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hdfs --config /usr/hdp/3.0.0.0-1469/hadoop/conf namenode -format -nonInteractive' returned 1. ######## Hortonworks #############
    This is MOTD message, added for testing in qe infra
    WARNING: /var/run/hadoop/cstm-hdfs does not exist. Creating.
    mkdir: cannot create directory ‘/var/run/hadoop/cstm-hdfs’: Permission denied
    ERROR: Unable to create /var/run/hadoop/cstm-hdfs. Aborting.
     stdout:
    2018-06-11 10:00:42,196 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:42,308 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:43,323 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:43,353 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:43,358 - Group['cstm-users'] {}
    2018-06-11 10:00:43,364 - Group['cstm-ranger'] {}
    2018-06-11 10:00:43,364 - Group['cstm-zeppelin'] {}
    2018-06-11 10:00:43,365 - Group['hdfs'] {}
    2018-06-11 10:00:43,365 - Group['cstm-livy'] {}
    2018-06-11 10:00:43,365 - Group['hadoop'] {}
    2018-06-11 10:00:43,366 - Group['cstm-knox'] {}
    2018-06-11 10:00:43,366 - Group['cstm-spark'] {}
    2018-06-11 10:00:43,368 - User['yarn-ats'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,371 - User['cstm-ranger'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-ranger', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,373 - User['cstm-hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,376 - User['cstm-sqoop'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,378 - User['cstm-ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,381 - User['cstm-yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,384 - User['cstm-tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,386 - User['cstm-atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,389 - User['cstm-storm'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,391 - User['cstm-knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'cstm-knox'], 'uid': None}
    2018-06-11 10:00:43,394 - User['cstm-kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,397 - User['cstm-logsearch'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,399 - User['cstm-infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,402 - User['cstm-hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,404 - User['cstm-hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,407 - User['cstm-mr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,409 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,412 - User['cstm-zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-zeppelin', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,414 - User['cstm-zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,417 - User['cstm-livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-livy', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,419 - User['cstm-oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,422 - User['cstm-spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'cstm-spark'], 'uid': None}
    2018-06-11 10:00:43,424 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,549 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-06-11 10:00:43,558 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-06-11 10:00:43,558 - Directory['/tmp/hbase-hbase'] {'owner': 'cstm-hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:43,696 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,819 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,942 - call['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase'] {}
    2018-06-11 10:00:43,953 - call returned (0, '1817')
    2018-06-11 10:00:43,954 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase /home/cstm-hbase,/tmp/cstm-hbase,/usr/bin/cstm-hbase,/var/log/cstm-hbase,/tmp/hbase-hbase 1817'] {'not_if': '(test $(id -u cstm-hbase) -gt 1000) || (false)'}
    2018-06-11 10:00:43,961 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase /home/cstm-hbase,/tmp/cstm-hbase,/usr/bin/cstm-hbase,/var/log/cstm-hbase,/tmp/hbase-hbase 1817'] due to not_if
    2018-06-11 10:00:43,962 - Group['hdfs'] {}
    2018-06-11 10:00:43,963 - User['cstm-hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-06-11 10:00:43,964 - FS Type: HDFS
    2018-06-11 10:00:43,964 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-06-11 10:00:44,026 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'root', 'group': 'hadoop'}
    2018-06-11 10:00:44,116 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-06-11 10:00:44,225 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}
    2018-06-11 10:00:44,235 - Skipping Execute[('setenforce', '0')] due to not_if
    2018-06-11 10:00:44,236 - Directory['/grid/0/log/hdfs'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:44,436 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}
    2018-06-11 10:00:44,595 - Directory['/tmp/hadoop-cstm-hdfs'] {'owner': 'cstm-hdfs', 'create_parents': True, 'cd_access': 'a'}
    2018-06-11 10:00:44,719 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'root'}
    2018-06-11 10:00:44,807 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'root'}
    2018-06-11 10:00:44,897 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:45,020 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-metrics2.properties'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:45,112 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
    2018-06-11 10:00:45,233 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/configuration.xsl'] {'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:45,306 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'cstm-hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:45,409 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
    2018-06-11 10:00:45,545 - Skipping unlimited key JCE policy check and setup since the Java VM is not managed by Ambari
    2018-06-11 10:00:46,488 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:46,490 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:46,614 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:46,658 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}
    2018-06-11 10:00:46,721 - File['/etc/security/limits.d/hdfs.conf'] {'content': Template('hdfs.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}
    2018-06-11 10:00:46,836 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs_dn_jaas.conf'] {'content': Template('hdfs_dn_jaas.conf.j2'), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:46,925 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs_nn_jaas.conf'] {'content': Template('hdfs_nn_jaas.conf.j2'), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:47,011 - XmlConfig['hadoop-policy.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,026 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-policy.xml
    2018-06-11 10:00:47,026 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-policy.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,118 - XmlConfig['ssl-client.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,132 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-client.xml
    2018-06-11 10:00:47,133 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-client.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,234 - Directory['/usr/hdp/3.0.0.0-1469/hadoop/conf/secure'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'cd_access': 'a'}
    2018-06-11 10:00:47,489 - XmlConfig['ssl-client.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf/secure', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,503 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/secure/ssl-client.xml
    2018-06-11 10:00:47,504 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/secure/ssl-client.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,602 - XmlConfig['ssl-server.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,615 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-server.xml
    2018-06-11 10:00:47,615 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-server.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,712 - XmlConfig['hdfs-site.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {u'final': {u'dfs.datanode.failed.volumes.tolerated': u'true', u'dfs.datanode.data.dir': u'true', u'dfs.namenode.http-address': u'true', u'dfs.namenode.name.dir': u'true', u'dfs.webhdfs.enabled': u'true'}}, 'configurations': ...}
    2018-06-11 10:00:47,725 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs-site.xml
    2018-06-11 10:00:47,725 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs-site.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,889 - XmlConfig['core-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'xml_include_file': None, 'mode': 0644, 'configuration_attributes': {u'final': {u'fs.defaultFS': u'true'}}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:47,906 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml
    2018-06-11 10:00:47,906 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}
    2018-06-11 10:00:48,038 - Writing File['/usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml'] because contents don't match
    2018-06-11 10:00:48,093 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/slaves'] {'content': Template('slaves.j2'), 'owner': 'root'}
    2018-06-11 10:00:48,181 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,214 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,287 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,304 - Repository['HDP-3.0-GPL-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469', 'action': ['create'], 'components': [u'HDP-GPL', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,329 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-1]\nname=HDP-3.0-GPL-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,397 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,417 - Repository['HDP-UTILS-1.1.0.22-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,446 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-1]\nname=HDP-3.0-GPL-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-1]\nname=HDP-UTILS-1.1.0.22-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,518 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,540 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:48,551 - Package['lzo'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,196 - Skipping installation of existing package lzo
    2018-06-11 10:00:49,196 - Package['hadooplzo_3_0_0_0_1469'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,402 - Skipping installation of existing package hadooplzo_3_0_0_0_1469
    2018-06-11 10:00:49,402 - Package['hadooplzo_3_0_0_0_1469-native'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,857 - Skipping installation of existing package hadooplzo_3_0_0_0_1469-native
    2018-06-11 10:00:49,861 - Directory['/grid/0/hadoop/hdfs/namenode'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
    2018-06-11 10:00:50,128 - Directory['/usr/lib/ambari-logsearch-logfeeder/conf'] {'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
    2018-06-11 10:00:50,315 - Generate Log Feeder config file: /usr/lib/ambari-logsearch-logfeeder/conf/input.config-hdfs.json
    2018-06-11 10:00:50,315 - File['/usr/lib/ambari-logsearch-logfeeder/conf/input.config-hdfs.json'] {'content': Template('input.config-hdfs.json.j2'), 'mode': 0644}
    2018-06-11 10:00:50,410 - Skipping setting up secure ZNode ACL for HFDS as it's supported only for NameNode HA mode.
    2018-06-11 10:00:50,415 - Called service start with upgrade_type: None
    2018-06-11 10:00:50,415 - HDFS: Setup ranger: command retry not enabled thus skipping if ranger admin is down !
    2018-06-11 10:00:50,417 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-client'] {'timeout': 20}
    2018-06-11 10:00:50,450 - call returned (0, 'hadoop-client - 3.0.0.0-1469')
    2018-06-11 10:00:50,451 - RangeradminV2: Skip ranger admin if it's down !
    2018-06-11 10:00:50,493 - checked_call['/usr/bin/kinit -c /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147 -kt /etc/security/keytabs/nn.service.keytab nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM > /dev/null'] {'user': 'cstm-hdfs'}
    2018-06-11 10:00:50,611 - checked_call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,612 - call['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'curl --location-trusted -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/b6b261de-4ab4-4c87-a271-bbaa9fc306f4 -c /var/lib/ambari-agent/tmp/cookies/b6b261de-4ab4-4c87-a271-bbaa9fc306f4 -w '""'""'%{http_code}'""'""' http://ctr-e138-1518143905142-357962-01-000006.hwx.site:6080/login.jsp --connect-timeout 10 --max-time 12 -o /dev/null 1>/tmp/tmpzBm4Ow 2>/tmp/tmprJvRDV''] {'quiet': False, 'env': {'KRB5CCNAME': '/var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'}}
    2018-06-11 10:00:50,729 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,729 - get_user_call_output returned (0, u'200', u'  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3630  100  3630    0     0  1954k      0 --:--:-- --:--:-- --:--:-- 3544k')
    2018-06-11 10:00:50,731 - call['/usr/bin/klist -s /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'] {'user': 'cstm-hdfs'}
    2018-06-11 10:00:50,843 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,844 - call['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'curl --location-trusted -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/49ac670c-d187-4b4c-8c13-32bc9f8ac060 -c /var/lib/ambari-agent/tmp/cookies/49ac670c-d187-4b4c-8c13-32bc9f8ac060 '""'""'http://ctr-e138-1518143905142-357962-01-000006.hwx.site:6080/service/public/v2/api/service?serviceName=cl1_hadoop&serviceType=hdfs&isEnabled=true'""'""' --connect-timeout 10 --max-time 12 -X GET 1>/tmp/tmpskeV7D 2>/tmp/tmpIUYUU7''] {'quiet': False, 'env': {'KRB5CCNAME': '/var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'}}
    2018-06-11 10:00:50,984 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,985 - get_user_call_output returned (0, u'[{""id"":2,""guid"":""92620a51-bd3f-44f1-aed6-29a7c80809ec"",""isEnabled"":true,""createdBy"":""cstm-hdfs"",""updatedBy"":""cstm-hdfs"",""createTime"":1528682426000,""updateTime"":1528682426000,""version"":1,""type"":""hdfs"",""name"":""cl1_hadoop"",""description"":""hdfs repo"",""configs"":{""commonNameForCertificate"":""-"",""dfs.secondary.namenode.kerberos.principal"":""nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""hadoop.security.authentication"":""kerberos"",""hadoop.security.auth_to_local"":""RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/\\nRULE:[1:$1@$0](cstm-hbase@EXAMPLE.COM)s/.*/cstm-hbase/\\nRULE:[1:$1@$0](cstm-hdfs@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[1:$1@$0](cstm-spark@EXAMPLE.COM)s/.*/cstm-spark/\\nRULE:[1:$1@$0](cstm-zeppelin@EXAMPLE.COM)s/.*/cstm-zeppelin/\\nRULE:[1:$1@$0](yarn-ats@EXAMPLE.COM)s/.*/yarn-ats/\\nRULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//\\nRULE:[2:$1@$0](activity_analyzer@EXAMPLE.COM)s/.*/activity_analyzer/\\nRULE:[2:$1@$0](activity_explorer@EXAMPLE.COM)s/.*/activity_explorer/\\nRULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](amsmon@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](atlas@EXAMPLE.COM)s/.*/cstm-atlas/\\nRULE:[2:$1@$0](ats-hbase@EXAMPLE.COM)s/.*/yarn-ats/\\nRULE:[2:$1@$0](cstm-knox@EXAMPLE.COM)s/.*/cstm-knox/\\nRULE:[2:$1@$0](cstm-livy@EXAMPLE.COM)s/.*/cstm-livy/\\nRULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/cstm-hbase/\\nRULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/cstm-hive/\\nRULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/cstm-mr/\\nRULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/cstm-yarn/\\nRULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/cstm-oozie/\\nRULE:[2:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/cstm-ranger/\\nRULE:[2:$1@$0](rangertagsync@EXAMPLE.COM)s/.*/rangertagsync/\\nRULE:[2:$1@$0](rangerusersync@EXAMPLE.COM)s/.*/rangerusersync/\\nRULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/cstm-yarn/\\nRULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/cstm-yarn/\\nDEFAULT"",""dfs.datanode.kerberos.principal"":""dn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""tag.download.auth.users"":""cstm-hdfs"",""password"":""*****"",""policy.download.auth.users"":""cstm-hdfs"",""hadoop.rpc.protection"":""authentication"",""dfs.namenode.kerberos.principal"":""nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""fs.default.name"":""hdfs://ctr-e138-1518143905142-357962-01-000006.hwx.site:8020"",""hadoop.security.authorization"":""true"",""username"":""hadoop""},""policyVersion"":3,""policyUpdateTime"":1528682427000,""tagVersion"":1,""tagUpdateTime"":1528682426000}]', u'  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r  0     0    0  2603    0     0  92268      0 --:--:-- --:--:-- --:--:-- 92268')
    2018-06-11 10:00:50,986 - Hdfs Repository cl1_hadoop exist
    2018-06-11 10:00:50,989 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-security.xml'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:51,064 - Writing File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-security.xml'] because contents don't match
    2018-06-11 10:00:51,124 - Directory['/etc/ranger/cl1_hadoop'] {'owner': 'cstm-hdfs', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:51,306 - Directory['/etc/ranger/cl1_hadoop/policycache'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:51,515 - File['/etc/ranger/cl1_hadoop/policycache/hdfs_cl1_hadoop.json'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:51,605 - XmlConfig['ranger-hdfs-audit.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,617 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-audit.xml
    2018-06-11 10:00:51,618 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-audit.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:51,750 - XmlConfig['ranger-hdfs-security.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,767 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-security.xml
    2018-06-11 10:00:51,768 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-security.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:51,892 - XmlConfig['ranger-policymgr-ssl.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,905 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-policymgr-ssl.xml
    2018-06-11 10:00:51,905 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-policymgr-ssl.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:52,023 - Execute[(u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/ranger_credential_helper.py', '-l', u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/install/lib/*', '-f', '/etc/ranger/cl1_hadoop/cred.jceks', '-k', 'sslKeyStore', '-v', [PROTECTED], '-c', '1')] {'logoutput': True, 'environment': {'JAVA_HOME': u'/usr/lib/jvm/java-openjdk'}, 'sudo': True}
    Using Java:/usr/lib/jvm/java-openjdk/bin/java
    Alias sslKeyStore created successfully!
    2018-06-11 10:00:53,259 - Execute[(u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/ranger_credential_helper.py', '-l', u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/install/lib/*', '-f', '/etc/ranger/cl1_hadoop/cred.jceks', '-k', 'sslTrustStore', '-v', [PROTECTED], '-c', '1')] {'logoutput': True, 'environment': {'JAVA_HOME': u'/usr/lib/jvm/java-openjdk'}, 'sudo': True}
    Using Java:/usr/lib/jvm/java-openjdk/bin/java
    Alias sslTrustStore created successfully!
    2018-06-11 10:00:54,461 - File['/etc/ranger/cl1_hadoop/cred.jceks'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0640}
    2018-06-11 10:00:54,556 - File['/etc/ranger/cl1_hadoop/.cred.jceks.crc'] {'owner': 'cstm-hdfs', 'only_if': 'test -e /etc/ranger/cl1_hadoop/.cred.jceks.crc', 'group': 'hadoop', 'mode': 0640}
    2018-06-11 10:00:54,652 - File['/etc/hadoop/conf/dfs.exclude'] {'owner': 'cstm-hdfs', 'content': Template('exclude_hosts_list.j2'), 'group': 'hadoop'}
    2018-06-11 10:00:54,749 - call[('ls', u'/grid/0/hadoop/hdfs/namenode')] {}
    2018-06-11 10:00:54,756 - call returned (0, '')
    2018-06-11 10:00:54,757 - Execute['ls /grid/0/hadoop/hdfs/namenode | wc -l  | grep -q ^0$'] {}
    2018-06-11 10:00:54,765 - Execute['hdfs --config /usr/hdp/3.0.0.0-1469/hadoop/conf namenode -format -nonInteractive'] {'logoutput': True, 'path': ['/usr/hdp/3.0.0.0-1469/hadoop/bin'], 'user': 'cstm-hdfs'}
    ######## Hortonworks #############
    This is MOTD message, added for testing in qe infra
    WARNING: /var/run/hadoop/cstm-hdfs does not exist. Creating.
    mkdir: cannot create directory ‘/var/run/hadoop/cstm-hdfs’: Permission denied
    ERROR: Unable to create /var/run/hadoop/cstm-hdfs. Aborting.
    
    Command failed after 1 tries
    

Artifacts:

    
    
    
    http://testqelog.s3.amazonaws.com/qelogs/nat/107592/ambari-blueprints/split-6/nat-yc-r7-gfgs-ambari-blueprints-6/log_tree/index.html
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-11 11:45:30,5
13165295,Final flag (blue icon) is not set for Final properties in Settings tab,"Please see screenshot attached
- We have final property set in Settings tab as well as Advanced tab.
- The one in Settings tab doesn't have blue icon where as it should be
- While setting a property to final we should be seeing blue icon which is not happening in Settings tab. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-11 10:54:43,19
13165267,SSL Connection logs missing as a side effect of AMBARI-23222,"Logging missed due to https://github.com/apache/ambari/commit/54ba89133f61b8ad013d96f4f8d881844229abd9#diff-639c7b5dc2005f486841e0317a228e9aL63

https://github.com/apache/ambari/commit/54ba89133f61b8ad013d96f4f8d881844229abd9#diff-639c7b5dc2005f486841e0317a228e9aL80
",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-06-11 08:28:44,32
13165138,Log Search: indexed string field could be too large for audit_logs,"{code:java}
2018-05-29 19:33:18,434 [qtp225493257-718281] ERROR [c:audit_logs s:shard4 r:core_node6 x:audit_logs_shard4_replica1] org.apache.solr.common.SolrException (SolrException.java:148) - org.apache.solr.common.SolrException: Exception writing document id 8c5deae9-c1b7-4ebf-9981-1ac92f1d2f84 to the index; possible analysis error: Document contains at least one immense term in field=""action"" (whose UTF8 encoding is longer than the max length 32766), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '[105, 110, 115, 116, 97, 108, 108, 101, 100, 58, 32, 97, 108, 108, 32, 115, 101, 114, 118, 105, 99, 101, 115, 32, 111, 110, 32, 97, 108, 108]...', original message: bytes can be at most 32766 in length; got 106331. Perhaps the document has an indexed string field (solr.StrField) which is too large
{code}",pull-request-available,"['ambari-infra', 'ambari-logsearch']",AMBARI,Bug,Critical,2018-06-09 16:39:27,29
13165032,Metrics Collector issues modify HTable descriptor calls every time it restarts.,"* Fix issues in AMS HBase tables policies initialization.
* Cleanup downsampling code and config.
* Add ability to blacklist metrics through a file.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-06-08 21:46:40,39
13164983,Missing LdapFacade in HostUpdateHelper,"The following error is encountered when executing executing
{noformat}
ambari-server update-host-names host_names_changes.json
{noformat}

{noformat}
2018-06-07 18:04:12,933 ERROR [main] HostUpdateHelper:573 - Unexpected error, host names update failed
com.google.inject.CreationException: Unable to create injector, see the following errors:

1) No implementation for org.apache.ambari.server.ldap.service.LdapFacade was bound.
  while locating org.apache.ambari.server.ldap.service.LdapFacade
    for the 1st parameter of org.apache.ambari.server.controller.internal.AmbariServerLDAPConfigurationHandler.<init>(AmbariServerLDAPConfigurationHandler.java:53)
  while locating org.apache.ambari.server.controller.internal.AmbariServerLDAPConfigurationHandler
    for field at org.apache.ambari.server.controller.internal.RootServiceComponentConfigurationHandlerFactory.ldapConfigurationHandler(RootServiceComponentConfigurationHandlerFactory.java:33)
  while locating org.apache.ambari.server.controller.internal.RootServiceComponentConfigurationHandlerFactory
    for field at org.apache.ambari.server.controller.internal.RootServiceComponentConfigurationResourceProvider.rootServiceComponentConfigurationHandlerFactory(RootServiceComponentConfigurationResourceProvider.java:48)
  at org.apache.ambari.server.controller.ResourceProviderFactory.getRootServiceHostComponentConfigurationResourceProvider(ResourceProviderFactory.java:1)
  at com.google.inject.assistedinject.FactoryProvider2.initialize(FactoryProvider2.java:666)
  at com.google.inject.assistedinject.FactoryModuleBuilder$1.configure(FactoryModuleBuilder.java:335) (via modules: org.apache.ambari.server.update.HostUpdateHelper$UpdateHelperModule -> com.google.inject.assistedinject.FactoryModuleBuilder$1)

1 error
        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)
        at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:176)
        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)
        at com.google.inject.Guice.createInjector(Guice.java:99)
        at com.google.inject.Guice.createInjector(Guice.java:73)
        at com.google.inject.Guice.createInjector(Guice.java:62)
        at org.apache.ambari.server.update.HostUpdateHelper.main(HostUpdateHelper.java:544)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-08 17:39:09,13
13164958,Command retry on server when agent-server connection drops,Server should try to re-send execution command message when agent does not confirmed successful receiving.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-08 15:55:20,6
13164949,Clicking on 'Details' button in Upgrade wizard shows no data,"When clicked on 'show details' button while Upgrade is in progress, it shows a spinning icon and no data underneath.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-08 14:50:31,9
13164939,Sys-prepped Hosts do not Install and Finalize correctly,"* Installation of packages using a host that is sys-prepped is choosing package names in a non-deterministic way on ubuntu14/16
* Finalization is failing when there are no packages installed on-system.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-06-08 14:15:49,33
13164924,Ambari Views auto create instance should support matching to all known stacks,"The Ambari Views framework currently defines a set of configuration tags that can be used to configure the auto-instance deployment of a view.  Currently, this configuration allows for a matching stack to be explicitly configured, or a wildcard ('*"") can be used to allow more flexible matching against a set of stack version numbers. 

The current configuration already supports this type of matching: 

 
{code:java}
<auto-instance>
<name>AUTO_INSTANCE</name>
<label>View Name</label>
<stack-id>HDP-*</stack-id>
:
</auto-instance>{code}
This configuration needs to be more flexible in order to allow a given View implementation to specify that it can match to any stack, including those defined outside of the ""HDP"" stack name. 

The Ambari Views Registry should be updated to allow for specifying a wilcard character in order to instruct the Views framework to match against any stack, regardless of whether the stack begins with the ""HDP"" stack name. 
{code:java}
<auto-instance>
<name>AUTO_INSTANCE</name>
<label>View Name</label>
<stack-id>*</stack-id>
:
</auto-instance>{code}
This change should allow for greater flexibility for service and view developers, in cases where a given service's view can be used by more than one stack.  ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-06-08 13:17:11,51
13164883,Autostart is not working for TIMELINE_READER. ,"Steps to reproduce:-  
1\. Enable Autostart for all components.  
2.restart one host which contain TIMELINE_READER component.  
3\. wait for 5-10 minute for autostart to take an effect

verify from the logs  
tail -f /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py

    
    
    
    INFO 2018-06-07 08:32:32,487 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:34,488 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:36,489 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:38,490 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:40,491 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:42,496 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:44,501 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:46,505 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:48,518 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:50,519 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:52,521 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:54,533 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:56,534 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:58,535 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:33:00,537 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:33:02,538 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    

repro cluster:-<http://172.22.107.230:8080/>  
repro job:- <http://linux-jenkins.qe.hortonworks.com:8080/job/Nightly-Start-
EC2-Run-HDP/965535/>

",pull-request-available,[],AMBARI,Bug,Major,2018-06-08 09:15:52,5
13164752,Test Kerberos Client fail after reenter of right realm value,"Test Kerberos Client fail after reenter of right realm value during kerberos enabling

STR:
1)Start of kerberos enabling
2)Select existing MIT
3)Go to Configure Kerberos page
4)Set wrong ""Realm name"" value. As an example: ""example.com"" (expected EXAMPLE.COM)
5)Go to Install and Test Kerberos Client step
6)Wait of Test Kerberos Client fail
7)Back to Configure Kerberos step
8)Enter right ""Realm name"" value 
9)Go to Install and Test Kerberos Client step

Expected:
Test Kerberos Client pass successfully

Actual:
Test Kerberos Client fail

The supposed main reason is using old realm value for principals creation:
{code:java}
2018-05-30 11:13:38,895 - Failed to create principal, cl1-053018@example.com - Failed to create service principal for cl1-053018@example.com
STDOUT: Authenticating as principal admin/admin@EXAMPLE.COM with existing credentials.

STDERR: WARNING: no policy specified for cl1-053018@example.com; defaulting to no policy
add_principal: No such entry in the database while creating ""cl1-053018@example.com"".

Administration credentials NOT DESTROYED.
{code}
This is happening only using MySQL.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-07 18:07:10,27
13164749,Do not move service-specific configs to Misc section,"Previously, we would consolidate certain service-specific configs, specifically user settings, onto the Misc tab in the UI. This made send when there was only one instance of each service and we wanted to have all user settings in one place. However, in the future we will have multiple instances of services and user settings will need to be specific to each instance. Therefore, we need to keep these user settings with their respective service (instances).",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-07 17:45:23,40
13164723,Add Kerberos-related configuration recommendations to the stack advisor,"Add Kerberos-related configuration recommendations to the stack advisor.
* Add a new action, {{recommend-configurations-for-kerberos}}, to query services for only Kerberos-related configuration changes
",pull-request-available stackadvisor,['ambari-server'],AMBARI,Bug,Major,2018-06-07 15:50:52,30
13164668,Save button is disabled after adding the custom property,"Save button is disabled after adding the custom property

STR:
# Navigate to Ranger page
# Go to advanced tab of config page
# Add custom property

Expected:
Save button is enabled

Actual:
Save button is disabled",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-07 11:40:19,15
13164658,"Oozie service check failed during EU on unsecure cluster due to presence of ""ELService"" configs","*STR*
# Deployed an unsecure cluster with Ambari version: 2.6.2.0-155 and HDP version: 2.6.5.0-292 (cluster has Oozie, Falcon installed)
# Upgrade Ambari to Target Version: 2.7.0.0-652
# Start an Express Upgrade to HDP-3.0.0.0-1440, Falcon got deleted at one of the steps and later Oozie got restarted

*Result*
Afterwards observed Oozie service check failed during EU
Further checked that Oozie logs say the following:
{code}
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property 'log4j2.debug' to show Log4j2 internal initialization logging.
log4j:WARN No appenders could be found for logger (org.apache.oozie.service.Services).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

ERROR: Oozie could not be started

REASON: org.apache.oozie.service.ServiceException: E0113: class not found [org.apache.oozie.extensions.OozieELExtensions]

Stacktrace:
-----------------------------------------------------------------
org.apache.oozie.service.ServiceException: E0113: class not found [org.apache.oozie.extensions.OozieELExtensions]
        at org.apache.oozie.service.ELService.findMethod(ELService.java:226)
        at org.apache.oozie.service.ELService.extractFunctions(ELService.java:104)
        at org.apache.oozie.service.ELService.init(ELService.java:135)
        at org.apache.oozie.service.Services.setServiceInternal(Services.java:386)
        at org.apache.oozie.service.Services.setService(Services.java:372)
        at org.apache.oozie.service.Services.loadServices(Services.java:305)
        at org.apache.oozie.service.Services.init(Services.java:213)
        at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)
        at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4276)
        at org.apache.catalina.core.StandardContext.start(StandardContext.java:4779)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)
{code}

Need to remove ELService properties during upgrade",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-07 10:42:07,28
13164525,"Fix the incorrect string from ""config_name"" to ""config-name"" in mpack_advisor.py","*Issue:*
Call to function toConfigurationValidationProblems() breaks for following :

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/mpack_advisor_wrapper.py"", line 148, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/mpack_advisor_wrapper.py"", line 126, in main
    result = mpackAdvisor.validateConfigurations()
  File ""/var/lib/ambari-server/resources/scripts/../stacks/mpack_advisor.py"", line 1149, in validateConfigurations
    validationItems = self.getConfigurationsValidationItems()
  File ""/var/lib/ambari-server/resources/scripts/../stacks/mpack_advisor.py"", line 1588, in getConfigurationsValidationItems
    self.validateMinMax(items, itemHead, recommendedDefaults, configurations)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/mpack_advisor.py"", line 1563, in validateMinMax
    validatedItems = self.toConfigurationValidationProblems(validationItems, configName)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/mpack_advisor.py"", line 1518, in toConfigurationValidationProblems
    ""config_type"": siteName, ""config_name"": validationProblem[""config-name""] }
KeyError: 'config-name'
{code}

*Reason :* In *validateMinMax()* fn, when adding to validationItems, the string should be ""config-name"" instead of ""config_name"".

*Fix* : Updated ""config_name"" to ""config-name"".",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-06 18:27:36,43
13164491,Widget Browser add/hide functionality is not working correctly,Widget browser add/hide widget functionality is not working correctly if widget is included in several widget_layouts as in case of several NameNode Namespaces.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-06 15:46:06,9
13164448,Increase outbound message buffer size,Make websocket outbound message buffer size configurable to prevent limit exceeding for large messages (agent configurations stomp updates).,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-06 12:48:24,6
13164434,Kafka Service Check failed on https enabled/WE cluster after Ambari Upgrade. Error : Failed to check that topic exists,"When delete.topic.enable is set to false, ambari service check is not able to delete the topic and fails.",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-06-06 11:38:48,18
13164301,AMS truncates metric response quietly,"In a cluster with around 120 topics available, trying to fetch all the topic metrics from AMS using the wild card query returns truncated response.

Query with Wildcard

Time interval around 7 days since the beginning of the cluster
http://<>:6188/ws/v1/timeline/metrics?appId=kafka_broker&metricNames=kafka.server.BrokerTopicMetrics.MessagesInPerSec.topic.%.count._sum&precision=MINUTES&startTime=1527587600000&endTime=1528192400000 (returns truncated response)

Latest metrics
http://<>:6188/ws/v1/timeline/metrics?appId=kafka_broker&metricNames=kafka.server.BrokerTopicMetrics.MessagesInPerSec.topic.%.count._sum (returns valid response)

Metadata Query returns all 120 topic metrics.
",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-06-05 22:31:40,39
13164249,Quicklinks for HBASE are not displayed,"Notice that the HBASE quicklinks is present in the stack definition. However quicklinks for only HDFS is being displayed, Quicklinks for HBASE are not being displayed.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-06-05 18:26:42,34
13164246,HDFS Metrics shows all blocks as 'Corrupt Replicas',"*STR*
# Deployed a cluster with HDP-3.0 and Ambari-2.7 (or upgrade from HDP-2.6 to HDP-3.0)
# Go to HDFS Summary page
# Observe the count of blocks under 'Corrupt Replica'

*Result*
All blocks show as corrupt",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-05 18:21:16,15
13164230,Alerts label isn't clickable at the service page after turning on mm for service,"Alerts label isn't clickable at the service page after turning on mm for service

STR:
# Go to the service page
# Enable mm for service
# Try to click alerts(bell) icon for the current service

Expected:
Will open alerts window

Actual
Nothing happens",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-05 17:29:31,15
13164153,Config Group Names are converted to uppercase once selected on UI,Once selected config group names are converted to uppercase on UI.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-05 11:37:40,9
13164118,Autostart is not obeying Maintenance Mode,"Steps to reproduce:-
1. enable autostart for all the components.
2. enable Maintenance Mode for a host
3. reboot the host
4. wait for the autostart to take an effect

If the maintenance mode is enabled then autostart should not take an effect.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-05 08:33:17,16
13163988,Fixes the following : 1. Reading the Node Manager Host (nmHost) correctly.   2. Comments the incorrectly implemented fn : isServiceDeployed().,"Fixes the following: 

1. Reading the Node Manager Host (nmHost) correctly. 

2. Comments the incorrectly implemented fn : isServiceDeployed(). When called, it breaks with: 'se' and 'serviceList' variables not defined.
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-04 21:05:45,43
13163940,Missing name for the request related to EU pre-check for Hadoop sink version,"With the fix of  AMBARI-23973, a new pre-check was added for Upgrades. This makes an entry in Background Ops for which the title says 'Request name not specified'.

If would be good to add an appropriate name for the request so as to avoid confusion on the purpose of this request",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-06-04 18:12:43,39
13163923,Service deletion requests get queued up in Background Ops during Upgrade,"On a cluster having Flume and Slider with HDP-2.6, started an Express Upgrade to HDP-3.0

The above services were deleted as part of removal task !Screen Shot 2018-06-03 at 12.18.49 PM.png|thumbnail! during EU


However the above operation created a corresponding request, which shows up in Background Operations window that is always in-progress state !Screen Shot 2018-06-03 at 12.18.23 PM.png|thumbnail!

*Expected Result*
We need to mark the request complete, else any failure at a later step would cause the user to see the above entries as in-progress which is confusing and undesirable
See the number 4 showing up here, towards top right in the background !Screen Shot 2018-06-03 at 12.23.29 PM.png|thumbnail!


Note: If the user pauses the Upgrade, that is when the above requests get marked as completed",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-04 17:28:39,31
13163873,Display Mpack Info on Service Summary Page,"At present service summary page displays ""Components Status Summary"" and ""Service Metrics"". We should also display Mpack Info in the Summary Page as follows

{code}
Summary:
    Mpack:   HDPCORE (1.0.0-b123)
    Components:    
                              Started           Started
                              NAMENODE          SNAMENODE
    Service Metrics:
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-04 14:48:48,19
13163819,Make `/hdp/apps/3.0.0.0-X/spark2/spark2-hdp-hive-archive.tar.gz`,"To optimize YARN cluster application, Ambari makes an additional file `spark2-hdp-yarn-archive.tar.gz` and upload it to HDFS. We need to do the same thing for `spark2-hdp-hive-archive.tar.gz` like the following.",pull-request-available,['ambari-sever'],AMBARI,Bug,Blocker,2018-06-04 12:25:52,24
13163814,AutoStart Is not working for some of the components in the cluster,"Autostart is not working for the following components:-

TIMELINE_READER
 HIVE_SERVER
 SPARK2_THRIFTSERVER
 HISTORYSERVER
 HBASE_MASTER
 ZEPPELIN_MASTER
 OOZIE_SERVER
 APP_TIMELINE_SERVER
 SPARK2_JOBHISTORYSERVER

 {code}

[root@re-nat-yc-r7-nubs-ambari-autostart-1-1 ~]# tail -f /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py
 INFO 2018-06-04 11:31:51,233 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,236 RecoveryManager.py:213 - SPARK2_THRIFTSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,238 RecoveryManager.py:213 - ZEPPELIN_MASTER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,250 RecoveryManager.py:213 - HISTORYSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,251 RecoveryManager.py:213 - HIVE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,254 RecoveryManager.py:213 - HBASE_MASTER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,255 RecoveryManager.py:213 - APP_TIMELINE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,257 RecoveryManager.py:213 - HIVE_METASTORE needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,258 RecoveryManager.py:213 - SPARK2_JOBHISTORYSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,261 RecoveryManager.py:213 - OOZIE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.

 {code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-04 12:11:57,5
13163811,Atlas fails to start in an unsecure cluster after Ambari Upgrade . Error - Unable to run the custom hook script,"1) Deployed cluster with Ambari version: 2.6.2.0-155 and HDP version: 2.6.5.0-292 in an unsecure cluster.
2) Upgrade Ambari to 2.7.0.0-630

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py"", line 29, in hook
    setup_users()
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/shared_initialization.py"", line 50, in setup_users
    groups = params.user_to_groups_dict[user],
KeyError: u'livy'
Error: Error: Unable to run the custom hook script ['/usr/bin/python', '/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py', 'ANY', '/var/lib/ambari-agent/data/command-232.json', '/var/lib/ambari-agent/cache/stack-hooks/before-ANY', '/var/lib/ambari-agent/data/structured-out-232.json', 'INFO', '/var/lib/ambari-agent/tmp', 'PROTOCOL_TLSv1_2', '']
{code}",pull-request-available,[],AMBARI,Improvement,Major,2018-06-04 11:57:04,18
13163806,Remove HDFS Disk Usage widget from host summary page,"There are NameNode widgets on host summary page which are specific for the namespace containing this host. Since HDFS DIsk Usage metrics is an aggregate from all the namespaces, the corresponding widget on host page is confusing and should be removed from there.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-04 11:36:50,15
13163803,Upgrade Task details is not loading in Upgrade History Details Window.,"On Clicking Upgrade on upgrade history tab under Stacks and Versions , details (logs etc) are not loading for upgrade tasks. JS error is seen in the console. Attaching screenshot .",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-04 11:20:42,19
13163782,Log Search component log url does not work on Ambari web UI,"on ambari-web, if you clinck on the Hosts -> select one host -> Logs, you can see the component list log, and if you click on Open In Log Search, it navigates you to logsearch UI with proper hash additions.

But if you click on the log itself, there is an another Open in LogSearch link there, that one uses the old query parameters.",pull-request-available,"['ambari-logsearch', 'ambari-web']",AMBARI,Bug,Blocker,2018-06-04 09:24:54,26
13163669,Log Search: java.lang.RuntimeException: Could not create directory /etc/ambari-logsearch-portal/conf/keys (if SSL enabled),"{code:java}
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.context.embedded.EmbeddedServletContainerFactory]: Factory method 'containerFactory' threw exception; nested exception is java.lang.RuntimeException: java.lang.RuntimeException: Could not create directory /etc/ambari-logsearch-portal/conf/keys
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:189)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:588)
	... 18 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not create directory /etc/ambari-logsearch-portal/conf/keys
	at org.apache.ambari.logsearch.configurer.SslConfigurer.loadKeystore(SslConfigurer.java:359)
	at org.apache.ambari.logsearch.conf.LogSearchServletConfig.containerFactory(LogSearchServletConfig.java:83)
	at org.apache.ambari.logsearch.conf.LogSearchServletConfig$$EnhancerBySpringCGLIB$$36529215.CGLIB$containerFactory$0(<generated>)
	at org.apache.ambari.logsearch.conf.LogSearchServletConfig$$EnhancerBySpringCGLIB$$36529215$$FastClassBySpringCGLIB$$68f33629.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:358)
	at org.apache.ambari.logsearch.conf.LogSearchServletConfig$$EnhancerBySpringCGLIB$$36529215.containerFactory(<generated>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:162)
	... 19 more
Caused by: java.lang.RuntimeException: Could not create directory /etc/ambari-logsearch-portal/conf/keys
	at org.apache.ambari.logsearch.util.FileUtil.createDirectory(FileUtil.java:57)
	at org.apache.ambari.logsearch.configurer.SslConfigurer.loadKeystore(SslConfigurer.java:335)
	... 30 more
{code}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Blocker,2018-06-03 12:01:10,29
13163527,Eliminate LogSearch / LogFeeder zk session/connection timeout warning,"WARN: session timeout [15000] is less than connection timeout [30000]

quick fix: increase the session timeout",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-06-01 20:47:16,29
13163518,Infra Solr start failed - Auth Failed on zk root (/),"Output of --create-znode command:
{code:java}
01 Jun 2018 03:30:06,343  WARN [zkConnectionManagerCallback-2-thread-1] ConnectionManager:106 - Watcher org.apache.solr.common.cloud.ConnectionManager@28e6348d name: ZooKeeperConnection Watcher:ctr-e138-1518143905142-340498-01-000006.hwx.site:2181,ctr-e138-1518143905142-340498-01-000007.hwx.site:2181,ctr-e138-1518143905142-340498-01-000008.hwx.site:2181 got event WatchedEvent state:AuthFailed type:None path:null path: null type: None
01 Jun 2018 03:30:06,343  WARN [zkConnectionManagerCallback-2-thread-1] ConnectionManager:187 - zkClient received AuthFailed
01 Jun 2018 03:30:06,346  WARN [main-SendThread(ctr-e138-1518143905142-340498-01-000007.hwx.site:2181)] ClientCnxn:1102 - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
01 Jun 2018 03:30:06,450  WARN [main-SendThread(ctr-e138-1518143905142-340498-01-000008.hwx.site:2181)] ClientCnxn:957 - SASL configuration failed: javax.security.auth.login.LoginException: Client cannot SASL-authenticate because the specified JAAS configuration section 'Client' could not be found. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.
{code}",pull-request-available,"['ambari-logsearch', 'ambari-server']",AMBARI,Bug,Major,2018-06-01 20:20:08,29
13163505,Add workaround to hide client modules in the dashboard,"Add workaround to hide client modules in the dashboard until we figure out how to represent client modules across mpacks. 

 

cc: [~ishanbha] [~jgolieb]",pull-request-available,[],AMBARI,Task,Blocker,2018-06-01 19:07:01,41
13163325,Enhance ERROR returned in case of incorrect  Mpack Version passed during Mpack Advisor call. ,"For instance during Mpack Advisor's Recommendation call, in case wrong Mpack Version is passed, the error returned as part of API response is following:

{code:title=POST http://[AmbariServer]:8080/api/v1/mpacks/recommendations}
Error parsing services.json file content: null
{code}

One possible reason is if wrong/non-existing Mpack version is passed-in in the REQUEST body.

{code}
{
	""hosts"": [
		""host1.openstacklocal"",
		""host2.openstacklocal""
	],
	""recommend"": ""configurations"",
	""recommendations"": {
		""blueprint"": {
                              ""mpack_instances"": [{
				""name"": ""HDPCORE"",
				""type"": ""HDPCORE"",
				""version"": ""1.0.0-b121"",   ---- > if WRONG_VERSION_NUMBER
                                 .........
{code}


*Fix:* Return information on passed-in Mpack Name and Version, and suggested checking for the Name, version and its existence.

Error retuned after fix:

{code}
Error parsing created services object for Mpack Advisor Python Code. Attempt to read services information for passed-in Mpack : ""HDPCORE"" and version : ""1.0.0-b4121"" is NULL. Check passed-in Mpack Name, Version and corresponding stack's existence in cluster.
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-01 01:24:01,43
13163323,"[Logsearch UI] ""Open log"" context menu navigates user to default 'last 1 hour' filtered logs","""Open log"" context menu navigates user to default 'last 1 hour' filtered logs

STR:
1. Login to logsearch portal
2. Select a valid time range with logs available for a specific component (e.g ambari-server for last 3 hours)
3. Pick one specific log row from the log table and click context menu and click open log, and the user should be navigated to the newly opened logsearch tab with the selected log highlighted and with the same filtered time window chosen by the user.
4. But the user is navigated to a new tab with custom filter of last one 1 hour and the user may not find the log he/she selected in the previous tab unless the log belongs in the last 1-hour window.

Note:
Also observed that when the user closes the newly opened tab with the custom filter, the previously searched 'Service Logs' tab also has the filter set as the custom tab (i.e 'Last 1 hour' and the component selected.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-06-01 01:10:07,26
13163319,[Logsearch UI] Add hosts link in log index filter popup is not working,"Add hosts link in log index filter popup is not working.

STR
1. Login to Logsearch portal
2. Click to filter log index from menu, select a valid cluster from the drop down
3. Try to add custom filters for host, user should be able to add custom filter index for additional hosts by clicking add hosts link.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-06-01 00:46:51,26
13163243,Duplicate data is showing up when enabling NN Federation,"We need to make a few changes to the HDFS Summary screen when NN Federation is enabled.  We're seeing the HDFS fs usage data duplicated in each namespace, when it should be separated out in a new section.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-31 18:42:40,15
13163209,Set All Versions only on Upgrades which cannot be Downgraded,"The legacy {{ru_set_all.py}} was deprecated a long time ago. It no longer makes sense now that clusters can have services and components on different versions. 

However, it still might be nice on a full upgrade (which cannot be downgraded) to allow it to run in order to reset all symlink pointers.
 * Renamed {{ru_set_all}} to {{stack_select_set_all}}
 * Removing {{stack_select_set_all}} from most upgrade packs
 * Scoped {{stack_select_set_all}} to only work on upgrades with are non-downgradeable",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-31 17:20:57,31
13163196,Recommission action doesn't appear for decommissioned component,"Recomission action doesn't appear for decommissioned component

STR:
1)Go to the host details page with Datanode
2)Decommission Datanode
3)Open component action

Expected:
Decommission is absent
Recommission is present

Actual:
Recommission is absent
Decommission is present",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-31 16:29:10,9
13163193,Fix ambari-web pom.xml (version not updated correctly),"ambari-web/pom.xml is hardcoding the version to 2.99.99.

This needs to be fixed so that it can be overwritten correctly when mvn versions:set is run from the top-level directory.  Current pom.xml leaves the Ambari Web version to 2.99.99 even after the command is run.",pull-request-available,[],AMBARI,Bug,Critical,2018-05-31 16:20:40,52
13163125,Server metadata cache does not have valid hash after updates with no any registered agent,Server metadata cache should always have actual hash.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-31 12:02:54,6
13163100,Ambari server installation failing on debian cluster with mysql db,"Ambari server install is failing at db create phase
{noformat}
2018-05-28 09:49:40,565|executor.py.165|DEBUG|13917|MainThread|172.27.17.22|executing the command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql'
2018-05-28 09:49:40,822|executor.py.165|DEBUG|13917|Thread-165|stdout: mysql: [Warning] Using a password on the command line interface can be insecure.
2018-05-28 09:49:40,822|executor.py.165|DEBUG|13917|Thread-165|stdout: ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
2018-05-28 09:49:40,822|executor.py.128|ERROR|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution failed with exitcode=1
2018-05-28 09:49:40,823|executor.py.116|INFO|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution re-try #1
2018-05-28 09:49:41,250|executor.py.165|DEBUG|13917|Thread-166|stdout: mysql: [Warning] Using a password on the command line interface can be insecure.
2018-05-28 09:49:41,250|executor.py.165|DEBUG|13917|Thread-166|stdout: ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
2018-05-28 09:49:41,251|executor.py.128|ERROR|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution failed with exitcode=1
2018-05-28 09:49:41,252|executor.py.116|INFO|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution re-try #2
2018-05-28 09:49:41,561|executor.py.165|DEBUG|13917|Thread-167|stdout: mysql: [Warning] Using a password on the command line interface can be insecure.
2018-05-28 09:49:41,561|executor.py.165|DEBUG|13917|Thread-167|stdout: ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
2018-05-28 09:49:41,561|executor.py.128|ERROR|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution failed with exitcode=1
{noformat}
Ambari server version - 2.7.0.0-568
Mysql version - 5.7.21

The root cause is (per documentation):
{noformat}
NOW() (or its synonyms) can be used as the default value for TIMESTAMP columns as well as,
since MariaDB 10.0.1, DATETIME columns. Before MariaDB 10.0.1, it was only possible for a single TIMESTAMP 
column per table to contain the CURRENT_TIMESTAMP as its default.
{noformat}
In _user_authentication_ there is another TIMESTAMP column with default value of CURRENT_TIMESTAMP called '_create_time_' and this causes the issue in older versions (< v10).

For the sake of consistency the following columns' type should be changed to BIGINT (defaulting to 0):
 * users.create_time
 * user_authentication.create_time
 * user_authentication.update_time

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-31 08:50:43,27
13163098,stack advisor error : UnboundLocalError: local variable 'host' referenced before assignment,"running hive stack advisor when tez + druid is installed gives

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 178, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 132, in main
    result = stackAdvisor.validateConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1059, in validateConfigurations
    validationItems = self.getConfigurationsValidationItems(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1446, in getConfigurationsValidationItems
    recommendations = self.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1608, in recommendConfigurations
    servicesList)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1549, in recommendConfigGroupsConfigurations
    serviceAdvisor.getServiceConfigurationRecommendations(configurations, cgClusterSummary, cgServices, cgHosts)
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/HIVE/service_advisor.py"", line 127, in getServiceConfigurationRecommendations
    recommender.recommendHiveConfigurationsFromHDP30(configurations, clusterData, services, hosts)
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/HIVE/service_advisor.py"", line 196, in recommendHiveConfigurationsFromHDP30
    druid_coordinator_host_port = str(host['Hosts']['host_name']) + "":"" + str(
UnboundLocalError: local variable 'host' referenced before assignment
{code}",pull-request-available,['stacks'],AMBARI,Improvement,Major,2018-05-31 08:45:40,18
13163074,"""Ambari persisted credential store"" pre check appearing in Patch Upgrade.",We should add logic to the pre req to see if the regen keytab task is present in the upgrade pack,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-05-31 07:01:56,18
13163040,Log Feeder: NullPointer during file check in.,"During file check in, we are referencing base64_file_key instead of file key, that can cause a NullPointer exception",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-05-31 00:31:01,29
13163009,credential_store_helper.get_password_from_credential_store does not return the correct password string,"credential_store_helper.get_password_from_credential_store does not return the correct password string. 

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-30 21:35:14,30
13163001,Provide Patch or Maint Flag to Command JSON,Provide information for either Upgrade or Downgrade that indicates when the bits only are switched.  This means some commands may be omitted if they won't affect the state of the cluster.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-05-30 20:58:59,33
13162993,Set 'skip.service.checks' = true during deploy.,"We need review the Service Checks for the Services before turning them on.
Thus, making the global *'skip.service.checks' = true* in ambari.properties.
",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-05-30 20:30:09,43
13162971,Host components API call doesn't return all host components,"Some host components are missing from the list response, but can be queried individually:

{noformat}
$ curl ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components"" | jq -r '.items[].href'
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/1
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/2
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/3
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/4
$ curl --head ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/5""
HTTP/1.1 200 OK
$ curl --head ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/6""
HTTP/1.1 200 OK
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-30 18:49:17,21
13162931,'Start All' services call fails post EU as the state of Timeline Reader is INIT,"*STR*
# Upgraded a cluster from HDP-2.6 to 3.0 (after Ambari upgrade to 2.7)
# During the upgrade, Yarn Timeline Reader v2 and Registry DNS got added. However their state is marked as INIT instead of STARTED (see attached)
# Stop any service post EU and then try 'Start All'

{code:title=Result}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Invalid transition for servicecomponent, clusterName=cl1, clusterId=2, serviceName=YARN, componentName=TIMELINE_READER, currentDesiredState=INIT, newDesiredState=STARTED""
}
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-30 15:55:27,31
13162928,HDFS Summary widgets disappear after changing widgets order,"STR:
1. Add Namespace.
2. Open HDFS service metrics page and change order of NS-specific widgets.
3. Refresh the page and check general widgets.

It should not be empty.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-30 15:23:43,9
13162898,Agent can send too large websocket message with command output.,Agent should properly limit report size instead of limiting line count.,pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2018-05-30 13:32:41,6
13162862,Updating ownership and permission of .crc file for Ranger service and Ranger plugin supported services,Setting up appropriate permission and ownership on .crc file.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-30 10:21:32,25
13162861,Express Upgrade: Clicking on upgrade item shows no tasks on large clusters,"There are two issues over here.
*1st issue:*
This seems to be a functional bug. As it seems from the attached screenshot that even after API call was completed content was not rendered.

*2nd issue:*
There seems to be a performance issue. UI asks for all tasks fields instead of only ones that are needed making api call really expensive. Expected behavior is that only status should be asked for and when user clicks on a task then  log for that task should be requested. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-30 10:17:33,19
13162733,Pre Upgrade check for AMS hadoop sink in HDP 2.6 to 3.0 EU.,"Add a pre upgrade check to verify that the version of hadoop-sink package on ALL hosts is 2.7.0.0.

* Create a custom command which checks the version of hadoop-sink.
* Create a server side request in the pre-upgrade check which invokes the command in all hosts and collects the results.
* All hosts MUST have upgraded versions of hadoop 2.7.0.0.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-29 21:41:13,39
13162674,UI should load stack services from multiple mpacks,Currently Ambari Web loads stack services from a single stack (eg: HDP). But in a multi-mpack environment this behavior needs to change to get stack services from all mpacks which are being used for installing services.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-29 18:13:32,34
13162614,"Dashboard,component information for few componets is not able to load after Ambari Upgrade","1) Upgrade Ambari from 2.6.1.5 to 2.7.0.0
2) Upgrade Ambari Infra and Logsearch 

Post this, Ambari Dashboard is unable to load. Also for many components , the component information is missing. Actions Menu keeps loading on clicking. Quick links also do not load.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-29 14:54:03,9
13162593,Upgrade Spark/Zeppelin/Livy from HDP 2.6 to HDP 3.0,"Upgrade Spark/Zeppelin/Livy from HDP 2.6 to HDP 3.0

Configuration changes/ any data migration changes needed to upgrade using express from HDP 2.6 to HDP 3.0.",pull-request-available,['ambari-sever'],AMBARI,Bug,Blocker,2018-05-29 13:48:24,24
13162208,Spark2 Thrift Server fails to start if LLAP and parallel execution are enabled,"STR:

# Enable parallel execution in Ambari Agent
# Deploy cluster via blueprint, placing Spark2 Thrift Server on the same node as Hive Server Interactive

Result: Spark2 Thrift Server runs into the following error during startup:

{noformat}
ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper
...
KeeperErrorCode = NoNode for /hiveserver2-hive2
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-26 06:01:20,21
13162157,Fix broken unit tests,The unit test script as a whole was broken due to not removing references to test files that were removed. This fixes that problem and the subsequent unit test failures revealed by the fix.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-25 20:37:30,40
13162140,"Implement 3.0 Mpack Advisor's Configuration ""Validations"" and updates code for Component Layout ""Validations""","*Background on Mpack Advisor: *

AMBARI-23870 
   - Implemented 3.0 Mpack Advisor's Host Component Layout Recommendation and Validations API Server code.  
   - Pull Request: https://github.com/apache/ambari/pull/1296 

AMBARI-23923
   - Implement 3.0 Mpack Advisor's Configuration Recommendation API Server code for ""Cluster Create"".
   - https://github.com/apache/ambari/pull/1348
-----------------------------------------------------------------

*Current Task:*

Implements :
  -  3.0 Mpack Advisor's Configuration ""Validations"", and 
  - updates the code for Component Layout ""Validations"" checked in AMBARI-23870.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-05-25 18:40:31,43
13162111,YARN start failed during EU with IllegalArgumentException,"*STR*
# Deployed cluster with Ambari version: 2.6.0.0-267 and HDP version: 2.6.0.3-8 (UI based install and cluster was kerberized)
# Upgrade Ambari to Target Version: 2.7.0.0-588
# Start EU to HDP-3.0.0.0-1390

*Result*
Observed following error at Timeline Reader v2 start:
{code}
2018-05-25 11:47:44,940 INFO  timeline.RollingLevelDBTimelineStore (RollingLevelDBTimelineStore.java:run(404)) - Deletion thread received interrupt, exiting
2018-05-25 11:47:44,941 ERROR applicationhistoryservice.ApplicationHistoryServer (ApplicationHistoryServer.java:launchAppHistoryServer(180)) - Error starting ApplicationHistoryServer
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: AHSWebApp failed to start.
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.startWebApp(ApplicationHistoryServer.java:322)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceStart(ApplicationHistoryServer.java:121)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:178)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:187)
Caused by: java.io.IOException: Problem starting http server
        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1165)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.startWebApp(ApplicationHistoryServer.java:313)
        ... 4 more
Caused by: java.lang.IllegalArgumentException: Could not parse [${clusterHostInfo/rm_host}]
        at org.apache.commons.net.util.SubnetUtils.calculate(SubnetUtils.java:275)
        at org.apache.commons.net.util.SubnetUtils.<init>(SubnetUtils.java:51)
        at org.apache.hadoop.util.MachineList.<init>(MachineList.java:108)
        at org.apache.hadoop.util.MachineList.<init>(MachineList.java:82)
        at org.apache.hadoop.util.MachineList.<init>(MachineList.java:74)
        at org.apache.hadoop.security.authorize.DefaultImpersonationProvider.init(DefaultImpersonationProvider.java:98)
        at org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration(ProxyUsers.java:75)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:202)
        at org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter.init(TimelineAuthenticationFilter.java:47)
        at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)
        at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873)
{code}
",kerberos pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Major,2018-05-25 15:36:47,30
13162064,Blueprint export from Amin / Cluster Information is broken,"The ""Download"" button on the Admin / Cluster Information screen successfully gives you a file, but the files contents are incomplete (in firefox).

STR:
 # Install Ambari 2.7
 # Create a cluster
 # Go to Admin > Manage Ambari > Cluster Information
 # Click Download
 # Download the file
 # Open the file and find that it's incomplete",pull-request-available,['ambari-admin'],AMBARI,Bug,Blocker,2018-05-25 12:17:17,7
13162061,Selection information disappears when a filter is applied,"As part of AMBARI-23911, selection information was added back to Ambari. But when a filter is applied, the selected hosts section is removed and refreshed. In previous versions, the selection information persisted even when a filter was applied.
Another issue faced while automating is that there is no unique xpath to the selection information. It would be great if a class/data-qa attribute is added to the anchor tag.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-25 12:12:42,15
13162053,Next button enabled when invalid values entered,"STR:
Enter invalid value on Configs page for any property.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-25 11:44:03,15
13161937,Accessing Swagger API when Ambari Server is configured for HTTPS,"The api-docs end-point is available but the Try button doesn't work when the Ambari Server is configured for HTTPS.

STR:
# Install Ambari Server
# Use /api-docs Try button on any API and notice that it works
# Configure Ambari Server to use SSL on port 8443
# Try to use /api-docs Try button now and notice it picks up the right port (8443) but the protocol is (http://) instead of (https://)",pull-request-available,"['ambari-sever', 'ambari-web']",AMBARI,Bug,Critical,2018-05-25 00:23:42,48
13161911,Proxy user settings are missing for livy in unsecured clusters,"below hadoop proxy user settings are missing for livy in HDFS core-site.xml:
hadoop.proxyuser.livy.groups=*
hadoop.proxyuser.livy.hosts=*",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-24 22:06:41,24
13161873,Resume installer after re-authenticating without browser refresh,"Clear cluster data so that it is reloaded every time the user logs in. This ensures that the wizard is resumed if the user gets logged out after a partial deployment and then logs back in, but did not refresh their browser.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-24 20:04:22,40
13161812,Stack upgrade from 2.6.5 to 3.0.0 fails because WebHCat component doesn't exist,Target and Source stacks are not represented correctly during an express upgrade,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-24 15:55:29,33
13161746,[Logsearch] Components filter does not have effect on graph data in service logs screen,Changing the components filters does not have effect on graph data.,pull-request-available,['logsearch'],AMBARI,Bug,Major,2018-05-24 11:49:47,29
13161698,Hiveserver2 fails to start on viewFS enabled cluster: {hive_server2_zookeeper_namespace} is not ready yet,"  * Tried to deploy a cluster with viewFS enabled via blueprint
  * HiveServer2 is failing to start with below error 
    
        
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 137, in 
        HiveServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 53, in start
        hive_service('hiveserver2', action = 'start', upgrade_type=upgrade_type)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_service.py"", line 102, in hive_service
        wait_for_znode()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 62, in wrapper
        return function(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_service.py"", line 191, in wait_for_znode
        raise Fail(format(""ZooKeeper node /{hive_server2_zookeeper_namespace} is not ready yet""))
    resource_management.core.exceptions.Fail: ZooKeeper node /hiveserver2 is not ready yet
    

out log has a reference to hdfs:// Filesystem

    
        
    2018-05-17 23:54:17,398 - Skipping fs root check as fs_root does not start with hdfs://
    2018-05-17 23:54:17,399 - Execute['/var/lib/ambari-agent/tmp/start_hiveserver2_script /grid/0/log/hive/hive-server2.out /grid/0/log/hive/hive-server2.err /var/run/hive/hive-server.pid /usr/hdp/current/hive-server2/conf/ /grid/0/log/hive'] {'environment': {'HIVE_BIN': 'hive', 'JAVA_HOME': u'/usr/lib/jvm/java-openjdk', 'HADOOP_HOME': u'/usr/hdp/current/hadoop-client'}, 'not_if': 'ls /var/run/hive/hive-server.pid >/dev/null 2>&1 && ps -p  >/dev/null 2>&1', 'user': 'hive', 'path': [u'/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/var/lib/ambari-agent:/usr/hdp/current/hive-server2/bin:/usr/hdp/3.0.0.0-1345/hadoop/bin']}
    2018-05-17 23:54:17,507 - Execute['/usr/lib/jvm/java-openjdk/bin/java -cp /usr/lib/ambari-agent/DBConnectionVerification.jar:/usr/hdp/current/hive-server2/lib/mysql-connector-java.jar org.apache.ambari.server.DBConnectionVerification 'jdbc:mysql://ctr-e138-1518143905142-317960-01-000006.hwx.site/hivedb?createDatabaseIfNotExist=true' hiveuser [PROTECTED] com.mysql.jdbc.Driver'] {'path': ['/usr/sbin:/sbin:/usr/local/bin:/bin:/usr/bin'], 'tries': 5, 'try_sleep': 10}
    2018-05-17 23:54:18,156 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server ctr-e138-1518143905142-317960-01-000005.hwx.site:2181,ctr-e138-1518143905142-317960-01-000006.hwx.site:2181,ctr-e138-1518143905142-317960-01-000007.hwx.site:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
    2018-05-17 23:54:19,032 - call returned (1, 'Node does not exist: /hiveserver2')
    2018-05-17 23:54:19,033 - Will retry 29 time(s), caught exception: ZooKeeper node /hiveserver2 is not ready yet. Sleeping for 5 sec(s)
    2018-05-17 23:54:24,038 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server ctr-e138-1518143905142-317960-01-000005.hwx.site:2181,ctr-e138-1518143905142-317960-01-000006.hwx.site:2181,ctr-e138-1518143905142-317960-01-000007.hwx.site:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
    2018-05-17 23:54:24,842 - call returned (1, 'Node does not exist: /hiveserver2')
    


",pull-request-available,[],AMBARI,Bug,Major,2018-05-24 08:51:16,5
13161648,Invalid repo url displayed on Ambari UI during service installation	,In case there are multiple repo versions for a stack UI picks up the first one and uses it in the Add Service Wizard.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-24 02:36:19,34
13161607,Failure during re-installation of services,"Steps to reproduce :

1) Create a healthy cluster
2) Stop a service and delete it(say Kafka or Zookeeper)
3) Try to reinstall the same service and the action fails.

Seeing below error in ambari-agent.log
{code:java}
INFO 2018-05-21 10:17:56,852 ActionQueue.py:276 - Command execution metadata - taskId = 300, retry enabled = False, max retry duration (sec) = 0, log_output = True
ERROR 2018-05-21 10:17:56,852 CustomServiceOrchestrator.py:448 - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: u'KAFKA'; u'KAFKA'
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 322, in runCommand
    command = self.generate_command(command_header)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 487, in generate_command
    command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 50, in get_configuration
    command_dict['serviceLevelParams'] = metadata_cache.serviceLevelParams[service_name]
KeyError: u'KAFKA'
INFO 2018-05-21 10:17:56,853 ActionQueue.py:324 - Quit retrying for command with taskId = 300. Status: FAILED, retryAble: False, retryDuration (sec): -1, last delay (sec): 1
INFO 2018-05-21 10:17:56,853 ActionQueue.py:339 - Command with taskId = 300 failed after 1 tries
{code}
Attaching the screenshot.

I have tried the same action for Zookeeper and it fails with similar exception.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-23 22:30:40,27
13161588,Data missing from configuration recommendation API call,Some parts of the recommendation call for configurations are missing from the request and must be restored.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-23 20:51:16,40
13161523,Make config request work with multiple mpacks,"The logic around the request for config properties in the installer needs to be updated to work with multiple mpacks. It needs to request configs for each mpack (stack) version being installed and display them all on the config screen.

NOTE: For now, we do not have the same service appearing in more than one mpack. We will need to deal with this case in the future.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-23 17:12:47,40
13161521,Reassign Master Wizard issues,"- On step 1, namespace ids are displayed next to hostname selects. When user selects hostname from other namespace, the displayed id isn't changed
- After completing the last step, page URL remains {{#/main/service/reassign/step6}}, with JS error thrown: {{app.js:212117 Uncaught TypeError: Cannot read property 'modal' of undefined}}. After page refresh user is broutght to the final step of wizard again, containing no relevant data",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-23 17:09:02,15
13161380,ambari ui fails to load due to js error,"Ambari UI is failing to load completely due to below js error

{code}
app.js:16513 Uncaught TypeError: Cannot read property 'get' of undefined
    at app.js:16513
    at Array.forEach (<anonymous>)
    at Class.getServiceVersionFromRepo (app.js:16494)
    at invokeAction (vendor.js:4833)
    at iterateSet (vendor.js:4815)
    at Object.sendEvent (vendor.js:4932)
    at notifyObservers (vendor.js:3524)
    at Object.Ember.notifyObservers (vendor.js:3639)
    at Object.propertyDidChange (vendor.js:4272)
    at ChainNodePrototype.chainDidChange (vendor.js:3987)
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-23 11:03:58,19
13161131,"Ambari Metrics reports incorrect values in aggregated host metric data when requesting ""avg""","Regression introduced by AMBARI-23008.

We changed the way SUM aggregate was stored in higher order Host aggregate tables. The corresponding change to read path was incomplete. 
 ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-22 16:44:20,39
13161064,Provide a Framework For Regenerating Keytabs During Upgrade,"There have been cases in the past where a manual step required after performing a stack upgrade was to regenerate keytabs. This was necessary for a variety of reasons, but it wasn't problematic enough to warrant Ambari doing this as part of an upgrade.

With the stack upgrade from HDP 2.6 to 3.0, 2 new components are added: Registry DNS and ATR. If the cluster is kerberized, these new components won't start until keytabs have been generated for them.

The follow will be able to be added to an upgrade pack in order to instruct the upgrade to regenerate missing keytabs for the new components:

{code}
    <group xsi:type=""cluster"" name=""REGENERATE_KEYTABS"" title=""Regenerate Missing Keytabs"">
      <condition xsi:type=""security"" type=""kerberos""/>
      <direction>UPGRADE</direction>
      <execute-stage title=""Regenerate Missing Keytabs"">
        <task xsi:type=""regenerate_keytabs""/>
      </execute-stage>
    </group>
{code}

A credential store will need to be setup before hand so that the kerberos credentials are available when this step in the upgrade runs.",pull-request-available,[],AMBARI,Bug,Blocker,2018-05-22 13:28:01,31
13161062,Stack changes for AMS to help with HDP 2.6 to 3.0 EU.,"* Add RESTART AMS tasks in HDP 2.6 to 3.0 EU after stack upgrade and before Smartsense.
 * Add check in HDFS, HBase, YARN, Hive start scripts in HDP 2.6 stack to make sure AMS hadoop sinks are not 2.7.0.0 version.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-22 13:22:12,28
13161057,Database script issue when running MYSQL DDL scripts,"When running Ambari-DDL-MySQL-CREATE.sql file on Mysql with Ubuntu16, getting the following error: 
{code}
root@ctr-e138-1518143905142-326354-01-000006:~# mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-22 13:12:06,32
13161037,"Disabled alert status isn't ""NONE""","Disabled alert status isn't ""NONE""

STR:
1)Stop Hbase
2)Navigate to alerts page
3)Move state of first CRITICALalert to ""Disabled""

Expected:
Alert status is NONE

Actual:
Alert status is CRITICAL
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-22 11:59:52,19
13161029,Phantom null request shows up on the bg ops for Stop All SCH,Server sends STOMP request updates with null host_name for tasks.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-22 11:05:05,6
13161008,Zeppelin is not respecting the absolute hdfs path for notebooks,"Zeppelin is not respecting the absolute hdfs path and picking up the incorrect path for notebooks.

Even when user is setting 'zeppelin.notebook.dir' as 'hdfs://ns2/zeppelin/tmp/tmp1/notebook', ambari is picking up the path as /user/zeppelin/hdfs://ns2/zeppelin/tmp/tmp1/notebook 
and due to this zeppelin service start is failing.",pull-request-available,['ambari-sever'],AMBARI,Bug,Critical,2018-05-22 09:38:36,53
13160983,"Implement 3.0 Mpack Advisor's Configuration Recommendation API Server code for ""Cluster Create"". ",https://github.com/apache/ambari/pull/1296 has the implementation details.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-05-22 08:08:38,43
13160969,PERF 1.0 package not installed in the cluster,"the PERF stack version is not installed on the agents
I attempted to reinstall the packages and got the following errors:

{code}
stderr: 
Traceback (most recent call last):
 File ""/home/perf-0001/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 493, in 
 InstallPackages().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/home/perf-0001/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 66, in actionexecute
 command = 'ambari-python-wrap \{0} install \{1}'.format(stack_selector_path, repository_version)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
 raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'repository_version' was not found in configurations dictionary!
 stdout:


Command failed after 1 tries
{code}

This effectively blocks EU on the PERF 1.0 cluster",pull-request-available,[],AMBARI,Bug,Blocker,2018-05-22 07:16:59,16
13160936,"If Kerberos is enabled, then stack upgrade prerequisite check should ensure the KDC admin credential is persisted","If Kerberos is enabled and Ambari is managing Kerberos identities, then the stack upgrade prerequisite check should ensure that the KDC admin credential is stored in the Ambari persisted credential store.

# The Ambari credential store must be set up (ambari-server setup-security, option #2)
# The KDC administrator credential is stored in the Ambari credential store (persisted)",kerberos pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-05-22 02:10:30,30
13160850,Ambari 2way SSL does not work if CA signed certs are used,"Enable 2 way SSL between Ambari server and agent using CA Signed certificates.  Communication fails with below error/Exception

{noformat}
ERROR 2018-05-21 15:57:35,357 Controller.py:226 - Unable to connect to: https://apappu4.hdp.com:8441/agent/v1/register/apappu4.hdp.com
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ambari_agent/Controller.py"", line 175, in registerWithServer
    ret = self.sendRequest(self.registerUrl, data)
  File ""/usr/lib/python2.6/site-packages/ambari_agent/Controller.py"", line 549, in sendRequest
    raise IOError('Request to {0} failed due to {1}'.format(url, str(exception)))
IOError: Request to https://apappu4.hdp.com:8441/agent/v1/register/apappu4.hdp.com failed due to [Errno 1] _ssl.c:492: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
ERROR 2018-05-21 15:57:35,357 Controller.py:227 - Error:Request to https://apappu4.hdp.com:8441/agent/v1/register/apappu4.hdp.com failed due to [Errno 1] _ssl.c:492: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
{noformat}


Root cause: As part of the setup - CA Root and CA Cert chains are imported to PKCS file. but Ambari server is not pushing these root/chain to Ambari agents and Agents are unable to trust the server certs.

*+Workaround:+*

Combine certs, Chains, root and then copy to agent hosts.

{noformat}

cat certchain.pem  servercert.pem root.pem  > caroot.pem
{noformat}

then copy this file to


{noformat}
cp caroot.pem /var/lib/ambari-agent/keys/ca.crt
{noformat}

Restarting agent should resolve the issue.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-21 23:06:04,30
13160782,Stack upgrade from 2.6.5 to 3.0.0 fails because WebHCat component doesn't exist,"When upgrading from one stack to another where the target stack is missing services or components, we hit the following NPE:

{code}
2018-05-21 14:27:59,197  WARN [ambari-client-thread-153] HttpChannel:507 - /api/v1/clusters/fff/upgrades
java.lang.NullPointerException
  at org.apache.ambari.server.state.UpgradeHelper.setDisplayNames(UpgradeHelper.java:838)
  at org.apache.ambari.server.state.UpgradeHelper.createSequence(UpgradeHelper.java:467)
  at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createUpgrade(UpgradeResourceProvider.java:721)
  at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:348)
  at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:340)
  at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
  at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:288)
  at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createResources(UpgradeResourceProvider.java:340)
  at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
  at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
  at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
  at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
  at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
  at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
  at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
  at org.apache.ambari.server.api.services.UpgradeService.createUpgrade(UpgradeService.java:59)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-21 18:36:14,31
13160713,[Logsearch UI] 'Component' option missing in the autofill in include filter ,"'Component' option missing in the autofill in include filter

STR:
1. Login to Logsearch portal
2. Try to type word 'Component' in the include filter text box
3. User should be displayed with Component autofill option in drop down and the available component list should be displayed if the user hits enter with Component keyword
4. No option 'Component' is displayed and 'Message:Component' filter is enabled when the user hits enter.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-05-21 14:41:57,26
13160706,[Logsearch UI] Rows per page drop down not working as expected in the log table,"Logs per page drop down not working as expected in the log table.

STR:
1. Login to Logsearch portal
2. Select valid time range with more than 10 log entries
3. From the log table, click on Rows per page and select 100
4. 100 or the maximum number (If lesser than 100) of logs should be displayed in the logs table, but still displays only 10 entries which is the default setting.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Blocker,2018-05-21 14:14:26,26
13160689,Upgrade History page shows non-stack services as also being upgraded to new stack version,"Upgrade history page shows non-stack services version as also HDP-3.0.0.0-x (see attached)

We need to add a filter here or show the exact version (same as Ambari-server version) for non-stack services like AMS, Smartsense, Infra, Logsearch",system_test,['ambari-web'],AMBARI,Bug,Critical,2018-05-21 12:35:28,13
13160687,Upgrade wizard text during patch revert indicates wrong version,"See attachments:
Tried a patch Upgrade: from HDP-3.0.0.0-1356 to 3.0.0.4-11 (patch-storm-s1, patch-storm-s2)

Thereafter tried to revert the patch - patch-dowgrade.png
The text at the top of wizard says: ""Express Downgrade from 3.0.0.0-1356""

It should say either ""Express Downgrade to 3.0.0.0-1356"" OR ""Express Downgrade from 3.0.0.4-11 to 3.0.0.0-1356"" OR ""Express Downgrade from 3.0.0.4-11""",pull-request-available system_test,['ambari-web'],AMBARI,Bug,Critical,2018-05-21 12:32:43,7
13160682,"New UI Makes it hard to see that a host is in a ""Decommissioning"" state","When decommissioning a host in Ambari 2.6 - the button indicating the component state clearly shows ""Decommissioning"". With the change to the ... menu in Ambari 2.7, it's hard to see that the component is in a decommissioning state because the tooltip for the status indicator only shows ""Started"".

We need to change the tooltip from ""Started"" to ""Decommissioning"" when the host is in a decommissioning state.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-21 12:20:31,19
13160671,Selection information not present in Hosts page filtering,"In the new Ambari web UI, when a host is selected by selecting the checkbox, the number of items selected is not shown. This was shown in previous releases.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-21 11:19:12,15
13160433,Change config tab layout for service tabs in installer,Initial Install Wizard with Ranger service shows Ranger Tagsync and Advanced Tab title shifted down. !screenshot-UI-CSS-ISSUE (1).png!,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-18 21:09:55,34
13160426,HDFS Prepare should not be called if there is no path to Downgrade,Add a boolean check for whether a downgrade is allowed before calling prepare during an EU,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-18 20:27:49,33
13160416,Topology Event Should Be Sent When Adding Host Components During Upgrade,"With AMBARI-23852, it's possible that new components can be added during upgrade. However, Ambari currently has a problem where topology update events are separate from {{Cluster.addServiceComponentHost()}} method calls.

This dis-association is being addressed as a whole in another Jira. However, for now, we can easily fix this by called {{ServiceComponentHost.setVersion()}} after creating it since this method actually sends the update we want.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-18 19:22:34,31
13160399,ZKFC fails to start while moving Namenode on a cluster with multiple namespaces,"STR: 
- Deploy a cluster with 2 namespaces using blueprint.
- Use the UI wizard to move Active Namenode for namespace NS2
- Perform manual operations when prompted in the wizard (FormatZkfc on other 3 hosts, perform bootstrapStandby on the new NN)
- In the final step, start all services, ZKFC fails to start on the host where we moved the NN 

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py"", line 192, in <module>
    ZkfcSlave().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py"", line 71, in start
    ZkfcSlaveDefault.start_static(env, upgrade_type)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py"", line 96, in start_static
    create_log_dir=True
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/utils.py"", line 258, in service
    Execute(daemon_cmd, not_if=process_id_exists_command, environment=hadoop_env_exports)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 308, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/3.0.0.0-1316/hadoop/bin/hdfs --config /usr/hdp/3.0.0.0-1316/hadoop/conf --daemon start zkfc'' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
WARNING: HADOOP_ZKFC_OPTS has been replaced by HDFS_ZKFC_OPTS. Using value of HADOOP_ZKFC_OPTS.
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 17:54:00,15
13160361,Heatmap Data Issues,"""Data Not Available"" in Dashboard / Heatmap for:
* Host Disk Space Used %
* Host Memory Used %

""Invalid Data"" in Dashboard / Heatmap for:
* DataNode Process Disk I/O Utilization   
* DataNode Process Network I/O Utilization

These should all be reporting data.

STR:
- Install ZooKeeper, HDFS, AMS, SmartSense
- Wait a 10 minutes to ensure data is coming into AMS
- Click on Dashboard / Heatmap and view these metrics",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 15:07:53,19
13160355,Server posts numerous redundant metadata updates to agent,Server should send metadata updates only when metadata was really changed.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-18 14:55:39,6
13160334,Log Search UI: login with invalid password – no error message is displayed,There is no error message when the user uses invalid auth credentials.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-05-18 12:14:52,26
13160322,Issues with pagination in Upgrade History page,"The number of items in a page for this should be the number of upgrade history records, not the number of services under each history record.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 11:31:19,7
13160315,Configuration Validation is throwing errors on correct configuration,"During installation if you change configuration there are a lot of configuration validation errors that appear to be using > and not >=.  See screenshot.  This results in 20+ things for the user to review, but most of them are correct because the value is 19 and we're asking them to set it to 19.

Steps to Reproduce:
# Install Ambari 2.7
# Choose ZK, HDFS, YARN, Hive
# Double the *Minimum Container Size (Memory)* during the *All Configurations* step
# Observe that the configuration validations are mostly asking us to change values to what they already are",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 10:46:00,19
13160310,ZooKeepers Show As Down After EU to HDP 3.0 But They Are Not,"STR:

  * Perform an EU from HDP 2.6 to HDP 3.0

After, 2 of my 3 ZKs are shown as being down. However, they are actually alive
on my boxes:

    
    
    
    [root@c7402 ~]$ ps aux | grep [z]oo.cfg
    zookeep+ 22463  0.2  2.8 3064236 53728 ?       Sl   20:41   0:01 /usr/jdk64/jdk1.8.0_144/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.log.file=zookeeper-zookeeper-server-c7402.ambari.apache.org.log -Dzookeeper.root.logger=INFO,ROLLINGFILE -cp /usr/hdp/current/zookeeper-server/bin/../build/classes:/usr/hdp/current/zookeeper-server/bin/../build/lib/*.jar:/usr/hdp/current/zookeeper-server/bin/../lib/xercesMinimal-1.9.6.2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-provider-api-2.4.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-shared4-2.4.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-shared-1.0-beta-6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-lightweight-1.0-beta-6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-2.4.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-file-1.0-beta-6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/slf4j-api-1.6.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/plexus-utils-3.0.8.jar:/usr/hdp/current/zookeeper-server/bin/../lib/plexus-interpolation-1.11.jar:/usr/hdp/current/zookeeper-server/bin/../lib/plexus-container-default-1.0-alpha-9-stable-1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/netty-3.10.5.Final.jar:/usr/hdp/current/zookeeper-server/bin/../lib/nekohtml-1.9.6.2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-settings-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-repository-metadata-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-project-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-profile-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-plugin-registry-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-model-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-error-diagnostics-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-artifact-manager-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-artifact-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-ant-tasks-2.1.3.jar:/usr/hdp/current/zookeeper-server/bin/../lib/log4j-1.2.16.jar:/usr/hdp/current/zookeeper-server/bin/../lib/jsoup-1.7.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/jline-0.9.94.jar:/usr/hdp/current/zookeeper-server/bin/../lib/commons-logging-1.1.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/commons-io-2.2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/commons-codec-1.6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/classworlds-1.1-alpha-2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/backport-util-concurrent-3.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/ant-launcher-1.8.0.jar:/usr/hdp/current/zookeeper-server/bin/../lib/ant-1.8.0.jar:/usr/hdp/current/zookeeper-server/bin/../zookeeper-3.4.6.3.0.0.0-1250.jar:/usr/hdp/current/zookeeper-server/bin/../src/java/lib/*.jar:/usr/hdp/current/zookeeper-server/conf::/usr/share/zookeeper/*:/usr/share/zookeeper/* -Xmx1024m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/hdp/current/zookeeper-server/conf/zoo.cfg
    
    [root@c7402 ~]$ telnet localhost 2181
    Trying ::1...
    Connected to localhost.
    Escape character is '^]'.
    ^CConnection closed by foreign host.
    

But you can see that we clearly think it's down on c7402:

    
    
    
    {
      ""href"" : ""http://localhost:8080/api/v1/clusters/c1/hosts/c7402.ambari.apache.org/host_components/ZOOKEEPER_SERVER"",
      ""HostRoles"" : {
        ""cluster_name"" : ""c1"",
        ""component_name"" : ""ZOOKEEPER_SERVER"",
        ""desired_repository_version"" : ""3.0.0.0-1250"",
        ""desired_stack_id"" : ""HDP-3.0"",
        ""desired_state"" : ""STARTED"",
        ""display_name"" : ""ZooKeeper Server"",
        ""host_name"" : ""c7402.ambari.apache.org"",
        ""maintenance_state"" : ""OFF"",
        ""public_host_name"" : ""c7402.ambari.apache.org"",
        ""reload_configs"" : false,
        ""service_name"" : ""ZOOKEEPER"",
        ""stale_configs"" : false,
        ""state"" : ""INSTALLED"",
        ""upgrade_state"" : ""NONE"",
        ""version"" : ""3.0.0.0-1250"",
        ""actual_configs"" : { }
      },
      ""host"" : {
        ""href"" : ""http://localhost:8080/api/v1/clusters/c1/hosts/c7402.ambari.apache.org""
      },
      ""component"" : [
        {
          ""href"" : ""http://localhost:8080/api/v1/clusters/c1/services/ZOOKEEPER/components/ZOOKEEPER_SERVER"",
          ""ServiceComponentInfo"" : {
            ""cluster_name"" : ""c1"",
            ""component_name"" : ""ZOOKEEPER_SERVER"",
            ""service_name"" : ""ZOOKEEPER""
          }
        }
      ],
      ""processes"" : [ ]
    }
    

The PID file looks correct:

    
    
    
    [root@c7402 zookeeper]$ cat /var/run/zookeeper/zookeeper_server.pid
    22463
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-18 10:30:38,5
13160298,Using Configs.py throws <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:579) error,"{code}
[root@test ~]# /var/lib/ambari-server/resources/scripts/configs.py --port 8443 --protocol https --action get --host localhost --cluster cl1 --config-type hive-site --key hive.exec.post.hooks
2018-05-16 13:23:06,375 INFO ### Performing ""get"" content:
Traceback (most recent call last):
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 368, in <module>
sys.exit(main())
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 354, in main
return get_properties(cluster, config_type, action_args, accessor)
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 262, in get_properties
get_config(cluster, config_type, accessor, output)
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 214, in get_config
properties, attributes = get_current_config(cluster, config_type, accessor)
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 125, in get_current_config
config_tag = get_config_tag(cluster, config_type, accessor)
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 95, in get_config_tag
response = accessor(DESIRED_CONFIGS_URL.format(cluster))
File ""/var/lib/ambari-server/resources/scripts/configs.py"", line 90, in do_request
raise Exception('Problem with accessing api. Reason: {0}'.format(exc))
Exception: Problem with accessing api. Reason: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:579)>
{code}
configs.py script has no any option to disable SSL validation. Up from of Python 2.7.9, ssl validation is enabled by default at urllib2 library https://stackoverflow.com/a/19269164
So I'm adding a command line option to configs.py script that would allow to skip certificate validation

",pull-request-available,['ambari-server'],AMBARI,Improvement,Blocker,2018-05-18 09:37:42,13
13160200,[Logsearch UI] Column headers are missing in service logs tabular view,"Column headers are missing in service logs tabular view.

STR:
1. Login to Logsearch portal
2. Select valid time range with more than 10 log entries
3. Click on Tabular view of logs

4. Column headers should be displayed similar to the tabular view of Audit logs page.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Minor,2018-05-17 22:43:37,26
13160173,[Log Search UI] Exclude filter not working as expected.,"Exclude filter not working as expected.
STR:
1. Login to Logsearch portal
2. Select valid time range with more than 10 log entries
3. Click on the exclude filter button and try to select any filter option
4. The selected option should be displayed in the include/exclude search box and user should be able to add exclude keywords
5. Exclude option dropdown is not working and no keys are filtered.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-05-17 21:55:25,26
13160148,UI /validations request used wrong stack name and version,"Request to legacy /validations endpoint was using the default stack name and version, while the /recommendations request was using the first mpack's info. Evidently, this will break the installer when the new Mpack Advisor code is merged in, so fixing this as a stopgap measure.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-17 20:46:04,40
13160133,Remove webhcat server as co-hosted component for Hive,Since WEBHCAT_SERVER is deprecated in HDP 3.0 it results in a js error in Step 5 of installer wizard. We need to remove it as a coHostedComponent for 'HIVE_SERVER',pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-17 19:37:48,34
13160077,Service stop/start/restart times out when agent registration was not completed,Server should ensure that registration was successfully finished before applying registering/initial heartbeat timestamps.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-17 16:34:58,6
13160055,Remove dependency on marked.js 0.3.2 in Ambari Web,"Remove dependency on marked.js 0.3.2 in Ambari Web due to security concerns. See 
* https://nvd.nist.gov/vuln/detail/CVE-2015-1370
* https://nvd.nist.gov/vuln/detail/CVE-2017-1000427

{noformat}
[root@host ~]# ambari-server --version
2.7.0.0-519
{noformat}

{noformat}
[root@host ~]# find /usr/lib -name marked.js
/usr/lib/ambari-server/web/api-docs/lib/marked.js
{noformat}

Recommendation is to remove the dependency or upgrade to version 0.3.2-1 or the latest version, if possible. 
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-17 15:34:18,15
13160038,Install Wizard: fix markup issues,"Fix small markup issues on Install Wizard:
* decrease space after checkboxes on step3
* increase table width on step6
* change logic for bell animation on configs
* use scrollable tabs for service selection on All Configurations tab
* update warning color for bell and labels
* add title for deploy popup on step8
* change progress bar width on step9",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-17 14:22:45,19
13160014,Redo the Manage Journalnodes wizard in the context of federation changes,"STR: Deploy a cluster with multiple namespaces via Blueprint
Add JournalNode using the wizard
In the format JN step {code}sudo su cstm-hdfs -l -c 'hdfs namenode -initializeSharedEdits'{code} fails with 

{code}
Re-format filesystem in QJM to [<ip0>:8485, <ip1>:8485, <ip2>:8485] ? (Y or N) y
18/05/09 18:43:02 ERROR namenode.NameNode: Could not initialize shared edits dir
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Could not format one or more JournalNodes. 1 exceptions thrown:
<ip0>:8485: Directory /hadoop/hdfs/journal/ns1 is in an inconsistent state: Can't format the storage directory because the current directory is not empty.
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.checkEmptyCurrent(Storage.java:600)
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:683)
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.format(JNStorage.java:210)
	at org.apache.hadoop.hdfs.qjournal.server.Journal.format(Journal.java:235)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.format(JournalNodeRpcServer.java:181)
	at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.format(QJournalProtocolServerSideTranslatorPB.java:148)
	at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:27399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:286)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.format(QuorumJournalManager.java:228)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:426)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:1262)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1619)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
18/05/09 18:43:02 INFO util.ExitUtil: Exiting with status 1: ExitException
18/05/09 18:43:02 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at <host0>/<ip0>
************************************************************/
{code}

Solution
Reorganize wizard steps:
- Assign Journalnodes
- Save Namespace
- Add/Remove Journalnodes (no start JN)
- Copy Journalnode directories
- Start Journalnodes
- Start All Services",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-17 12:43:31,15
13159994,Service auto-start not working,"After enabling service auto start on all services, and then rebooting a host, none of the services were started even after waiting 10 minutes.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-17 11:25:54,1
13159971,Move user-related info to stack-level params,"User-related information (users, groups, user-group mapping) is currently sent to agents in {{clusterLevelParams}}.  The problem is that the data reflects only a single stack, so users etc. from other stacks are not created, causing deployment failures.  In addition, {{stack_version}} should be moved, too, similar to how {{stack_name}} was moved in AMBARI-23746.",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Critical,2018-05-17 08:58:03,21
13159970,Upgrade history arrow does not indicate downwards when expanded,When 'Upgrade History' panel is expanded - the small arrow to the left of 'Upgrade' text is still pointing to the right instead of bottom,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-17 08:53:40,7
13159945,New Alert JSON Is Invalid When Sent To Agents,"STR:

  * Set a simple cluster with HDFS
  * Attempt to create a new Alert:

    
    
    
    POST http://{{ambari-server}}:8080/api/v1/clusters/c1/alert_definitions
    
    {
      ""AlertDefinition"": {
        ""component_name"": ""NAMENODE"",
        ""description"": ""This service-level alert is triggered if the total number of volume failures across the cluster is greater than the configured critical threshold."",
        ""enabled"": true,
        ""help_url"": null,
        ""ignore_host"": false,
        ""interval"": 2,
        ""label"": ""NameNode Volume Failures"",
        ""name"": ""namenode_volume_failures"",
        ""scope"": ""ANY"",
        ""service_name"": ""HDFS"",
        ""source"": {
          ""jmx"": {
            ""property_list"": [
              ""Hadoop:service=NameNode,name=FSNamesystemState/VolumeFailuresTotal""
            ],
            ""value"": ""{0}""
          },
          ""reporting"": {
            ""ok"": {
              ""text"": ""There are {0} volume failures""
            },
            ""warning"": {
              ""text"": ""There are {0} volume failures"",
              ""value"": 1
            },
            ""critical"": {
              ""text"": ""There are {0} volume failures"",
              ""value"": 1
            },
            ""units"": ""Volume(s)""
          },
          ""type"": ""METRIC"",
          ""uri"": {
            ""http"": ""{{hdfs-site/dfs.namenode.http-address}}"",
            ""https"": ""{{hdfs-site/dfs.namenode.https-address}}"",
            ""https_property"": ""{{hdfs-site/dfs.http.policy}}"",
            ""https_property_value"": ""HTTPS_ONLY"",
            ""kerberos_keytab"": ""{{hdfs-site/dfs.web.authentication.kerberos.keytab}}"",
            ""kerberos_principal"": ""{{hdfs-site/dfs.web.authentication.kerberos.principal}}"",
            ""default_port"": 0,
            ""connection_timeout"": 5,
            ""high_availability"": {
              ""nameservice"": ""{{hdfs-site/dfs.internal.nameservices}}"",
              ""alias_key"": ""{{hdfs-site/dfs.ha.namenodes.{{ha-nameservice}}}}"",
              ""http_pattern"": ""{{hdfs-site/dfs.namenode.http-address.{{ha-nameservice}}.{{alias}}}}"",
              ""https_pattern"": ""{{hdfs-site/dfs.namenode.https-address.{{ha-nameservice}}.{{alias}}}}""
            }
          }
        }
      }
    }
    

This alert will not be scheduled on the agent correctly:

    
    
    
    ERROR 2018-05-16 20:11:55,186 AlertSchedulerHandler.py:307 - [AlertScheduler] Unable to load an invalid alert definition. It will be skipped.
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/AlertSchedulerHandler.py"", line 287, in __json_to_callable
        alert = MetricAlert(json_definition, source, self.config)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/alerts/metric_alert.py"", line 52, in __init__
        self.metric_info = JmxMetric(alert_source_meta['jmx'])
      File ""/usr/lib/ambari-agent/lib/ambari_agent/alerts/metric_alert.py"", line 288, in __init__
        self.property_list = jmx_info['property_list']
    KeyError: 'property_list'
    

Looking at `/var/lib/ambari-agent/cache/cluster_cache/alerts.json`, we can see
that `property_list` was changed into `propertyList`.

    
    
    
            ""name"": ""namenode_volume_failures"",
            ""componentName"": ""NAMENODE"",
            ""description"": ""This service-level alert is triggered if the total number of volume failures across the cluster is greater than the configured critical threshold."",
            ""interval"": 2,
            ""clusterId"": 2,
            ""label"": ""NameNode Volume Failures"",
            ""ignore_host"": false,
            ""source"": {
              ""jmx"": {
                ""urlSuffix"": ""/jmx"",
                ""propertyList"": [
                  ""Hadoop:service=NameNode,name=FSNamesystemState/VolumeFailuresTotal""
                ],
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-17 07:00:28,5
13159937,"Removal of huetoview migration, jobs view, hawq view, storm view from ambari	",These views cannot be maintained with the new release of Ambari and needs to be removed.,pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-05-17 06:19:19,38
13159915,Implement 3.0 Mpack Advisor's Host Component Layout Recommendation and Validations API Server code. ,https://github.com/apache/ambari/pull/1296 has the implementation details.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-05-17 03:47:47,43
13159894,Remove insecure dependencies from Ambari Server,"Remove insecure dependencies from Ambari Server

*Jetty: Java based HTTP, Servlet, SPDY, WebSocket Server 6.1.26*
* https://nvd.nist.gov/vuln/detail/CVE-2017-9735
* https://nvd.nist.gov/vuln/detail/CVE-2011-4461
* https://nvd.nist.gov/vuln/detail/CVE-2009-1523

{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Building Ambari Server 2.0.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
[INFO] org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT
[INFO] +- org.mortbay.jetty:jsp-api-2.1-glassfish:jar:2.1.v20100127:compile
[INFO] +- org.mortbay.jetty:jsp-2.1-glassfish:jar:2.1.v20100127:compile
[INFO] \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
[INFO]    \- org.mortbay.jetty:jetty:jar:6.1.26:compile{noformat}

Recommendation is to remove the dependency or upgrade to version 6.1.26.hwx or the latest version, if possible. 
",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2018-05-16 23:43:21,30
13159853,Log Search web alert gets 401 http status code if KNOX SSO is enabled,"with 2.7.0 changes root ( / ) is also protected on logsearch patch, there we should use an open endpoint like /api/v1/info",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-05-16 22:11:14,29
13159846,Kerberos Service Check failure due to kinit failure on random node,"We were seeing Kerberos Service checks failures in Ambari. Specifically it would fail during the first run of the day, succeed on the second, then fail on the next but succeed if run again and so forth.

Reviewing the operation log, it showed kinit failure from random node(s)
 {{kinit: Client XXXX not found in Kerberos database while getting initial credentials}}

Since AMBARI-9852
{quote}The service check must perform the following steps:
   1.Create a unique principal in the relevant KDC (server)
   2.Test that the principal can be used to authenticate via kinit (agent)
   3.Destroy the principal (server)
{quote}
Which is a very good check of services.

So what is happening...

In our environment we have multiple Kerberos Domain Controllers across multiple data centers all providing the same realm.

The creation of a unique principal occurs at a single KDC and is propagated to the others.

The agents were testing the principal at different KDC, i.e. before it had a change to propagate. This is why the second service check would succeed.

 ",pull-request-available,[],AMBARI,Improvement,Minor,2018-05-16 21:51:04,54
13159829,Upgrade Infra Solr to use Solr 7.3.1,upgrade infra solr to version 7.3.1 as it contains CVE-2016-1000031 fix,pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-05-16 20:54:01,29
13159808,Need to add new property for Ranger-Tagsync when enabling federation for Namenode-HA via Blueprints,"Need to dynamically add properties in config-type {{ranger-tagsync-site}} when cluster is being setup via blueprint . This should be done when Atlas and Ranger-Tagsync service is installed.
When federation is Enabled for  HDFS, in this case for each name-service, we will need to add a property in config-type {{ranger-tagsync-site}} using below format:
{noformat}ranger.tagsync.atlas.hdfs.instance.<cluster-name>.nameservice.<name-service>.ranger.service= <ranger service-name>{noformat}

The Ranger service name can be derived from logic as below:
If the value for property {{ranger.plugin.hdfs.service.name}} is default i.e. {{{{repo_name}}}} then the value for above property will be in the format: {noformat}<cluster-name>_hadoop_<name-service>{noformat}
for e.g if the cluster-name is {{testcl1}} and the name-services are {{ns1}} , {{ns2}} and the {{ranger.plugin.hdfs.service.name}} has value {{{{repo_name}}}}  then the property-value will be as below:
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns1.ranger.service=testcl1_hadoop_ns1
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns2.ranger.service=testcl1_hadoop_ns2

If the user has a custom-value for the property {{ranger.plugin.hdfs.service.name}} i.e. not {{{{repo_name}}}}, then we will need to use the value provided in for property {{ranger.plugin.hdfs.service.name}} and use the format as <custom-repo-name>_<name-service>.
for e.g if the cluster-name is {{testcl1}} and the name-services are {{ns1}} , {{ns2}} and the {{ranger.plugin.hdfs.service.name}} has value {{hadoop_service}}  then the property-value will be as below:
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns1.ranger.service=hadoop_service_ns1
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns2.ranger.service=hadoop_service_ns2

PS : Related NN-Federation UI-wizard changes are already handled in AMBARI-23424, This issue is created to handle addition of properties via blueprint.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-16 19:09:15,35
13159780,STOMP alert update was not sent when maintenance mode change,"Steps:
 - Shutdown a service, like AMS
 - Wait for alerts (about 10 showed up)
 - Turn in MM for AMS

Alerts don't clear until you hard refresh, even though the MM cleared them immediately.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-16 16:50:29,6
13159724,Service status is difficult to understand in the left nav of the new UI,"In previous versions of Ambari:
* A red dot that appears to the left of the service name meant that the service is down.  In the new UI, this red dot means there's an alert associated with the service, and if the service name turns red, then the service is down.  The semantics of the red dot has changed, and this is really confusing to the end user who is used to the old UI.  <- We need the red dot to mean the service is down.  
* We used to show a green dot to show that the service is up.  This gave assurance to the end user and this behavior has come to be expected.   <- We need to bring back the green dot.
* When there are associated service alerts, we used to show a red badge and the alert count to the right of the service name.  As mentioned above, a red dot shows up in the new UI and this is not easily understood as meaning there's an alert.  <- We need to bring back the alert badge w/ a count.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-16 13:02:07,19
13159561,UI changes for handling idempotent mpack registration,"The back end has been changed to send 201 created instead of exception 409 when same mpack is attempted to be registered, thus making this operation idempotent. The front end needs an update to account for this change to avoid treating repeated registrations as duplicate mpacks.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-15 22:39:23,40
13159525,Topology  cache on agent side is not actual after unsupported services removal during stack upgrade,Server should notify agent about unsupported services removal during stack upgrade.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-15 20:42:47,6
13159470,NN Federation wizard is stuck on step 3,"NameNode Federation wizard is stuck on Review step. Infinite spinner is displayed instead of config properties to be changed, and also JS error is thrown: {{app.js:5361 Uncaught TypeError: Cannot read property 'properties' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 17:04:36,15
13159453,Provide a Framework For Adding A Component During Upgrade,"During an upgrade it might be necessary to add a component automatically. For example, if a component is being replaced between stacks, then Ambari needs to remove the original component and add the new one.

This is a new situation which we've never had before. AMBARI-23553 was created to automatically remove services/components during an upgrade which didn't appear in the new stack. However, to replace a service component is something that involves more thought.

Perhaps we need an action that's similar to this:

{code}
<group xsi:type=""cluster"" name=""ADD_YARN_TIMELINE_READER"" title=""Replace ATS with Timeline Reader"">
  <execute-stage title=""Replace ATS with Timeline Reader"">
    <task xsi:type=""add_component"" service=""YARN"" component=""TIMELINE_READER"" host-service=""YARN"" host-component=""APP_TIMELINE_SERVER"">
      <summary>Add Timeline Reader to the cluster</summary>
    </task>
  </execute-stage>
</group>
{code}",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-05-15 15:54:18,31
13159451,No relationship between generic parameter and method argument,"Call to a generic collection method contains an argument with an incompatible class from that of the collection's parameter (i.e., the type of the argument is neither a supertype nor a subtype of the corresponding generic type argument). Therefore, it is unlikely that the collection contains any objects that are equal to the method argument used here. Most likely, the wrong value is being passed to the method.

* {{String}} is incompatible with expected argument type {{Long}} in {{onHostRemoved(String)}}
* {{Long}} is incompatible with expected argument type {{AlertDefinitionEntity}} in {{AlertGroupsUpdateListener.onAlertDefinitionDeleted(AlertDefinitionDeleteEvent)}}
* {{String}} is incompatible with expected argument type {{Long}} in {{HostConfigMappingDAO.removeByClusterAndHostName(long, String)}}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-15 15:47:51,21
13159424,Customise Services - All Configurations Tab - Issue with default dimensions of text box and toggle buttons colour,"The toggle buttons don't look like a button is present there. need to change the color of the toggle button base so that it is distinguishable from the background panel color. ( See Anonymous access allowed toggle button for Smartsense )

The issue is applicable to the web elements across all services under All Configurations Tab",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 14:02:08,19
13159408,User information is missing when upgrade wizard is launched by a custom user.,"1) Create two users - custom-user(With role as Administrator) , test-user (With any role , say service operator)
2) Login with custom-user and perform below steps.
2.a) Register a patch VDF (Attached the VDF too).
3) Install the new registered patch
4) Disable Auto-start and run service checks for all components in the UI (Not in the video). This is needed as a prerequisite for Upgrade to start
5) Start upgrade with user custom-admin
6) Login with user test-user and you will not see who initiated text.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 12:43:57,7
13159397,Manage Ambari UI style fixes,"* Add a tooltip on config version compare button: ""Compare this version with current""
* User edit page: remove extra space between columns
* Group edit: Group Access select alignment",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 12:09:00,19
13159392,Infra Solr: support for manage security.json manually,"security.json file (generated by Ambari) is uploaded (overriden) during Infra Solr startup, add a new option in order to turn this feature off and manage security.json manually: `infra-solr-security-json/infra_solr_security_manually_managed` ",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-05-15 11:50:15,29
13159388,unable to differentiate b/w new added service and existing service,"While adding new service, both existing components and new components on Assign Masters step are displayed in green.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 11:33:41,15
13159374,"Timeline service is shown as stopped, while being started.","
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 108, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 87, in status
        for pid_file in self.get_pid_files():
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 99, in get_pid_files
        import params
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/params.py"", line 29, in <module>
        from params_linux import *
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/params_linux.py"", line 473, in <module>
        repo_name = str(config['clusterName']) + '_yarn'
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'clusterName' was not found in configurations dictionary!
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-15 10:45:39,5
13159359,"ambari-server setup throwing Error ""/usr/lib/ambari-server/lib/ambari_commons/subprocess32.py:153: RuntimeWarning: The _posixsubprocess module is not being used""","
    
    [root@nat-r7-bjks-man-5 ~]# ambari-server setup --java-home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.ppc64le/
    Using python  /usr/bin/python
    Setup ambari-server
    /usr/lib/ambari-server/lib/ambari_commons/subprocess32.py:153: RuntimeWarning: The _posixsubprocess module is not being used. Child process reliability may suffer if your program uses threads.
      ""program uses threads."", RuntimeWarning)
    Checking SELinux...
    SELinux status is 'enabled'
    SELinux mode is 'permissive'
    WARNING: SELinux is set to 'permissive' mode and temporarily disabled.
    OK to continue [y/n] (y)?
    Customize user account for ambari-server daemon [y/n] (n)?
    Adjusting ambari-server permissions and ownership...
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-15 10:03:54,5
13159246,Show Limited Functionality warning for Ranger and Atlas during installer wizard,"Show the warning if Ranger/Atlas are not selected during the installer wizard ""Choose Services"" step",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-15 00:01:28,34
13159205,Amabri server gets NPE due to ambari agent failure while registration,"One of the ambari agent logs fails to handle registration with below stack trace
{code:java}
INFO 2018-05-14 20:38:02,377 HeartbeatThread.py:128 - Registration response received
ERROR 2018-05-14 20:38:02,378 HeartbeatThread.py:104 - Exception in HeartbeatThread. Re-running the registration
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 91, in run
    self.register()
  File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 131, in register
    self.handle_registration_response(response)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 189, in handle_registration_response
    raise Exception(error_message)
Exception: Registration failed
INFO 2018-05-14 20:38:02,379 transport.py:358 - Receiver loop ended{code}

Due to this failure, ambari server gets NPE.
{code}
2018-05-14 20:38:02,222  INFO [agent-register-processor-4] HeartBeatHandler:312 - agentOsType = centos7
2018-05-14 20:38:02,296  INFO [agent-register-processor-4] HostImpl:345 - Received host registration, host=[hostname=xx,fqdn=xx,domain=xxx,architecture=x86_64,processorcount=2,physicalprocessorcount=2,osname=centos,osversion=7.4.1708,osfamily=redhat,memory=16777216,uptime_hours=2238,mounts=(available=233881212,mountpoint=/,used=16772348,percent=7%,size=264091716,device=/dev/mapper/docker-253:5-3014660-4abca9bb09ce35db7850a7e6c617cd995ba5093a32b05ab6ac9374c858091864,type=ext4)]
, registrationTime=1526330282222, agentVersion=2.7.0.0
2018-05-14 20:38:02,308  INFO [agent-register-processor-4] TopologyManager:643 - TopologyManager.onHostRegistered: Entering
2018-05-14 20:38:02,308  INFO [agent-register-processor-4] TopologyManager:645 - TopologyManager.onHostRegistered: host = xxx is already associated with the cluster or is currently being processed
2018-05-14 20:38:02,309  INFO [agent-register-processor-4] HeartbeatController:105 -
java.lang.NullPointerException{code}",pull-request-available,[],AMBARI,Bug,Critical,2018-05-14 20:55:29,16
13159180,"Remove ""Stack and Versions"" view and related code","The ""Stack and Versions"" view is no longer relevant with the advent of mpacks and service groups for managing service installations.  All of this code must be removed.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-14 18:32:26,40
13159160,Remove unused/deprecated code,Removed more deprecated code related to stack and repo versions.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-14 17:55:01,40
13159088,Port is not cleared for Superset Database type if Mysql is selected than SQLLite is selected in Installer Wizard,"1) Launch Installer Wizard and select Superset in services to install
 2) In Databases tab of Customize Services tab , select Superset
 3) By default for , Superset Database type , SQLITE is selected and Port is empty.
 4) Select MYSQL and then reselect SQLLITE , not port is not cleared and still shows 3306. It should be cleared .",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-14 12:44:54,28
13159070,Rebalance HDFS fails,"The rebalance HDFS from Ambari is not working. 

DFS Usage before rebalance (threshold of 1%)
{code:bash}
[cstm-hdfs@ctr-e138-1518143905142-299595-01-000008 root]$ hadoop dfsadmin -report | grep 'DFS Used%'
WARNING: Use of this script to execute dfsadmin is deprecated.
WARNING: Attempting to execute replacement ""hdfs dfsadmin"" instead.

DFS Used%: 4.21%
DFS Used%: 1.89%
DFS Used%: 3.40%
DFS Used%: 7.91%
DFS Used%: 3.47%
DFS Used%: 4.67%
DFS Used%: 3.54%
DFS Used%: 2.80%
{code}

After invoking the rebalance HDFS from Ambari UI, the /system/balancer.id file is not present (meaning balancer is not running)
{code:bash}
[cstm-hdfs@ctr-e138-1518143905142-299595-01-000008 root]$ while true; do echo 'checking /system'; hadoop fs -ls /system; sleep 1; done
checking /system
ls: `/system': No such file or directory
checking /system
ls: `/system': No such file or directory
checking /system
ls: `/system': No such file or directory
checking /system
ls: `/system': No such file or directory
checking /system
ls: `/system': No such file or directory
checking /system
^Cchecking /system

ls: `/system': No such file or directory
^C
[cstm-hdfs@ctr-e138-1518143905142-299595-01-000008 root]$ hadoop dfsadmin -report | grep 'DFS Used%'
WARNING: Use of this script to execute dfsadmin is deprecated.
WARNING: Attempting to execute replacement ""hdfs dfsadmin"" instead.

DFS Used%: 4.21%
DFS Used%: 1.89%
DFS Used%: 3.40%
DFS Used%: 7.91%
DFS Used%: 3.47%
DFS Used%: 4.67%
DFS Used%: 3.54%
DFS Used%: 2.80%
[cstm-hdfs@ctr-e138-1518143905142-299595-01-000008 root]$
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-14 11:40:39,28
13159043,viewFS : Blueprint deployed clusters doesn't have xml include content in core-site.xml,"- Deployed a cluster with NN federation + viewFS via blueprint.
- viewFS is enabled by having all relevant configs in viewfs-mount-table
- Deploy was successful but upon accessing hdfs we got the error as below
{code}
# hdfs dfs -ls /
ls: ViewFs: Cannot initialize: Empty Mount table in config for viewfs://cl1/
{code}
Checked in core-site.xml and none of the core-site.xml has the xml include content for viewfs
They should have 
<xi:include href=""/usr/hdp/3.0.0.0-1312/hadoop/conf/viewfs-mount-table.xml""/>",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-14 09:43:04,28
13159039,Support Atlas HBase hook from Ambari,"Atlas hook for HBase is now available and can be enabled with patch for ATLAS-1805. This feature needs to be supported from Ambari, similar to existing hooks for Hive, Storm, Sqoop and Falcon.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-14 09:31:49,35
13158848,Enable Ranger Plugins for supported components if ranger is installed,Set them to 'Yes' before loading recommendations. For Kafka and Storm only set if the cluster is kerberized,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-12 01:07:44,34
13158763,Provide a command line utility in AMS scripts to upgrade the schema using the collector's tool.,"* This can look something like  
_su - ams -c '/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf/ upgrade <file> <startTime>'_
* Based on implementation of AMBARI-23780, this command line utility should be able to start and stop upgrade.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-11 16:20:31,28
13158713,Save button is inactive for changed filtered property after configs comparing	," # Go to HIVE configs.
 # Filter configs by hive.metastore.client.socket.timeout.
 # Compare current version with some previous.
 # Close comparing panel.
 # Change filtered property.

Result: ""Save"" button is inactive. Was reproduced not for any service/property.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-11 12:19:57,15
13158712,Log feeder fails to parse date,"2018-05-09 06:35:58,188 [file=hivemetastore.log] ERROR org.apache.ambari.logfeeder.util.LogFeederUtil (LogFeederUtil.java:134) - Error applying date transformation. isEpoch=false, targetDateFormat=yyyy-MM-dd HH:mm:ss,SSS, value=2018-05-09T06:35:57,174. mapClass=map_date, input=input:source=file, path=null, fieldName=logtime. Messages suppressed before: 1922
java.text.ParseException: Unparseable date: 2018-05-09T06:35:57,174
        at org.apache.commons.lang3.time.FastDateParser.parse(FastDateParser.java:367)
        at org.apache.commons.lang3.time.FastDateFormat.parse(FastDateFormat.java:550)
        at org.apache.ambari.logfeeder.mapper.MapperDate.apply(MapperDate.java:89)
        at org.apache.ambari.logfeeder.plugin.filter.Filter.apply(Filter.java:114)
        at org.apache.ambari.logfeeder.filter.FilterGrok.applyMessage(FilterGrok.java:264)
        at org.apache.ambari.logfeeder.filter.FilterGrok.apply(FilterGrok.java:192)
        at org.apache.ambari.logfeeder.plugin.input.Input.outputLine(Input.java:190)
        at org.apache.ambari.logfeeder.input.file.ProcessFileHelper.processFile(ProcessFileHelper.java:122)
        at org.apache.ambari.logfeeder.input.InputFile.processFile(InputFile.java:265)
        at org.apache.ambari.logfeeder.input.InputFile.start(InputFile.java:244)
        at org.apache.ambari.logfeeder.plugin.input.Input.run(Input.java:170)
        at java.lang.Thread.run(Thread.java:748)",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-05-11 12:16:50,16
13158700, Ambari schema upgrade failed while updating ams-site config,"Looks to be caused by the fix of AMBARI-23804

*STR*
 # Deployed cluster with Ambari version: 2.6.2.0-153 and HDP version: 2.6.2.0-205
 # Upgrade Ambari to 2.7.0.0-493

*Result*
Error during schema upgrade
{code:java}
2018-05-11 07:39:03,292  INFO [main] AbstractUpgradeCatalog:644 - No changes detected to config infra-solr-env. Skipping configuration properties update
2018-05-11 07:39:03,302  INFO [main] AbstractUpgradeCatalog:644 - No changes detected to config infra-solr-security-json. Skipping configuration properties update
2018-05-11 07:39:03,303 ERROR [main] SchemaUpgradeHelper:238 - Upgrade failed.
java.lang.NumberFormatException: null
        at java.lang.Integer.parseInt(Integer.java:542)
        at java.lang.Integer.parseInt(Integer.java:615)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateAmsConfigs(UpgradeCatalog270.java:1613)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:952)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:236)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:456)
2018-05-11 07:39:03,304 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: null
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:239)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:456)
Caused by: java.lang.NumberFormatException: null
        at java.lang.Integer.parseInt(Integer.java:542)
        at java.lang.Integer.parseInt(Integer.java:615)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateAmsConfigs(UpgradeCatalog270.java:1613)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:952)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:236)
        ... 1 more
2018-05-11 07:48:36,013  INFO [main] Configuration:2786 - Reading password from existing file
2018-05-11 07:48:36,029  INFO [main] Configuration:3435 - Hosts Mapping File null
2018-05-11 07:48:36,029  INFO [main] HostsMap:60 - Using hostsmap file null{code}
I don't see the config 'timeline.container-metrics.ttl' present in ams-site",upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-05-11 11:15:18,39
13158686,Remove hive and tez views from hive and tez page,User can get same functionality via DAS.,pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-05-11 09:14:08,36
13158669,Ambari Agent registration task is failing ,"Logs:

    
    
    
    INFO 2018-05-10 10:04:59,174 NetUtil.py:61 - Connecting to https://os-mv-07-test-3.openstacklocal:8440/ca
    WARNING 2018-05-10 10:04:59,174 NetUtil.py:92 - Failed to connect to https://os-mv-07-test-3.openstacklocal:8440/ca due to 'module' object has no attribute '_create_unverified_context'  
    WARNING 2018-05-10 10:04:59,174 NetUtil.py:115 - Server at https://os-mv-07-test-3.openstacklocal:8440 is not reachable, sleeping for 10 seconds...
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-11 07:49:09,5
13158651,Visualizing the Encrypted zones and Erasure coded zones in HDFS,For hadoop 3.0 Files view should show whether a folder or file is Encrypted or not and what Erasure coding policy is used for that.,pull-request-available,['ambari-views'],AMBARI,Bug,Critical,2018-05-11 06:25:59,38
13158569,NN Federation wizard: Restart Required Services should not restart JN and ZKFC,"STR:
Add namespace from UI. In the Restart All Services operation RM fails to start

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 54, in wrapper
 return function(*args, **kwargs)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 244, in wait_for_dfs_directory_created
 raise Fail(""DFS directory '"" + dir_path + ""' does not exist !"")
Fail: DFS directory '/ats/done/' does not exist !
{code}

The above exception was the cause of the following exception:

{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 261, in <module>
 Resourcemanager().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 142, in start
 self.wait_for_dfs_directories_created(params.entity_groupfs_store_dir, params.entity_groupfs_active_dir)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 211, in wait_for_dfs_directories_created
 self.wait_for_dfs_directory_created(dir_path, ignored_dfs_dirs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 62, in wrapper
 return function(*args, **kwargs)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 244, in wait_for_dfs_directory_created
 raise Fail(""DFS directory '"" + dir_path + ""' does not exist !"")
resource_management.core.exceptions.Fail: DFS directory '/ats/done/' does not exist !
{code}

But after closing the wizard and starting all the services it started.

Also restarting history server failed on another host

{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 134, in <module>
 HistoryServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 95, in start
 skip=params.sysprep_skip_copy_tarballs_hdfs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 502, in copy_to_hdfs
 replace_existing_files=replace_existing_files,
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 627, in action_create_on_execute
 self.action_delayed(""create"")
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 624, in action_delayed
 self.get_hdfs_resource_executor().action_delayed(action_name, self)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 333, in action_delayed
 self.action_delayed_for_nameservice(nameservice, action_name, main_resource)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 359, in action_delayed_for_nameservice
 self._create_resource()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 375, in _create_resource
 self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 490, in _create_file
 self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 178, in run_command
 return self._run_command(*args, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 251, in _run_command
 raise WebHDFSCallException(err_msg, result_dict)
resource_management.libraries.providers.hdfs_resource.WebHDFSCallException: Execution of 'curl -sS -L -w '%\{http_code}' -X PUT --data-binary @/var/lib/ambari-agent/tmp/mapreduce-native-tarball-staging/mapreduce-native.tar.gz -H 'Content-Type: application/octet-stream' --negotiate -u : -k 'https://<Host>:50470/webhdfs/v1/hdp/apps/3.0.0.0-1309/mapreduce/mapreduce.tar.gz?op=CREATE&overwrite=True&permission=444'' returned status_code=403. 
{
 ""RemoteException"": {
 ""exception"": ""IOException"", 
 ""javaClassName"": ""java.io.IOException"", 
 ""message"": ""Failed to find datanode, suggest to check cluster health. excludeDatanodes=null""
 }
}
{code}

 ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-10 20:51:50,15
13158536,"Service actions (Stop, Start) not enabled when individual components are stopped","Stop all components from the host detail page and navigate to the service actions. The start/stop button is not enabled. 

Only on refresh, the Start button is enabled.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-10 18:10:40,19
13158510,TimelineMetricsFilterTest failure if dir name contains @,"[PullRequest Builder|https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/] is occasionally running into failure in the following 2 test cases:

{noformat}
testHybridFilter(org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest)  Time elapsed: 0.725 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testHybridFilter(TimelineMetricsFilterTest.java:221)

testMetricWhitelisting(org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest)  Time elapsed: 0.035 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testMetricWhitelisting(TimelineMetricsFilterTest.java:78)
{noformat}

Looking at the logs, it only happens if two builds are running on the same Jenkins node concurrently (workspace dir has suffix {{@2}}), but timing doesn't matter, the two concurrent builds can be at completely different stages.  The problem is caused by not finding the whitelist file due to URL escaping ({{@}} is converted to {{%40}}):

{noformat}
FileNotFoundException: Ambari-Github-PullRequest-Builder%402/ambari-metrics/ambari-metrics-timelineservice/target/test-classes/test_data/metric_whitelist.dat (No such file or directory)
{noformat}

https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2225/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2220/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2214/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2195/",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-05-10 16:03:16,21
13158499,Alerts icon is absent in service page if no alerts present,Grayed-out bell should be displayed like in the top nav.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-10 15:06:38,15
13158476,Identical sets of capacity scheduler properties are displayed as unequal ones in configs comparison view,"While comparing different YARN config versions, capacity scheduler values are shown as different ones. The only displayed difference is the order of properties in those sets, though even the order is the same for both versions.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-10 13:55:09,15
13158432,Add users to the group Is not working In group Creation UI,"Steps to reproduce:-
1. login to ambari
2. move to Manage Ambari Page
3. Move to groups Tab
4. Create a group add few users to it and set role to the Group.



But U still can see 0 members in the Groups Dashboard.
But If u edit the group and add the users to it again. it shows number of members.",pull-request-available,['ambari-admin'],AMBARI,Bug,Blocker,2018-05-10 10:07:32,7
13158349,Refine AMS HBase region splitting calculation based on UUID work.,"* Implement collector side region splits for UUID based tables
* Refactor compaction policy and durability settings for metric tables.
* Replaced MD5 with Murmur3 Hash as the default UUID generation technique.
* Config cleanup. ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-10 01:54:29,39
13158292,Remove unused/deprecated installer code,"Removed unused code from the installer wizard, primarily the old step1 and step4. Also removed deprecated API calls and some stuff that just wasn't being used anywhere.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-09 20:54:30,40
13158264,Action not shown immediately in the BG Operations window,"In ambari-2.7.0.0-472, When an action is selected via the Web UI (From service/host), the action is not shown in the BG Operations window that pops up. 

It is shown only after a refresh.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-09 18:09:47,19
13158263,Unable to add Hive Metastore from Host detail Page,"In ambari-2.7.0.0-472, 

Adding Hive Metastore from Host detail page leads to the modal window being stuck and javascript errors:

Javascript errors: 
{code}
Uncaught TypeError: Cannot read property 'tag' of undefined
    at Class.loadHiveConfigs (app.js:24588)
    at Class.opt.success (app.js:181624)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-09 18:06:40,15
13158192,Regenerate Keytabs/Reenable security After Ambari Upgrade modifies hadoop.proxyuser.HTTP.hosts to an incorrect value,"value of the property hadoop.proxyuser.HTTP.hosts in HDFS is set to an incorrect value after regenerate keytabs operation or disable and then enable security is done post Ambari Upgrade to 2.7.0.0

The value contains hostnames repeated twice with ""\"" at the end.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-09 13:26:26,27
13158102,Handle null values for configurations in the Execution Command API,"/var/lib/ambari-agent/cache/cluster_cache/configurations.json
{noformat}
""zookeeper-env"": {
 ""content"": ""\nexport JAVA_HOME=\{{java64_home}}\nexport ZOOKEEPER_HOME=\{{zk_home}}\nexport ZOO_LOG_DIR=\{{zk_log_dir}}\nexport ZOOPIDFILE=\{{zk_pid_file}}\nexport SERVER_JVMFLAGS=\{{zk_server_heapsize}}\nexport JAVA=$JAVA_HOME/bin/java\nexport CLASSPATH=$CLASSPATH:/usr/share/zookeeper/*\n\n\{% if security_enabled %}\nexport SERVER_JVMFLAGS=\""$SERVER_JVMFLAGS -Djava.security.auth.login.config=\{{zk_server_jaas_file}}\""\nexport CLIENT_JVMFLAGS=\""$CLIENT_JVMFLAGS -Djava.security.auth.login.config=\{{zk_client_jaas_file}}\""\n\{% endif %}"",
 ""zk_server_heapsize"": ""1024m"",
 ""zk_user"": ""zookeeper"",
 ""zookeeper_keytab_path"": null,
 ""zk_log_dir"": ""/var/log/zookeeper"",
 ""zookeeper_principal_name"": null,
 ""zk_pid_dir"": ""/var/run/zookeeper""
 }
{noformat}
Related code
params.py
{noformat}
zk_principal_name = module_configs.get_property_value(module_name, 'zookeeper-env', 'zookeeper_principal_name', 'zookeeper@EXAMPLE.COM')
zk_principal = zk_principal_name.replace('_HOST', hostname.lower())
{noformat}

the api only picks up the default when no key exists
if zookeeper_principal_name does not exist
it will return default value
but now zookeeper_principal_name maps to value null
so api thinks null is a valid value",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-09 06:22:13,44
13158043,Allow Ranger and Ranger KMS installation via the Install Wizard,Currently we don't allow the installation of Ranger and Ranger KMS during the initial Install wizard. This jira is to track the dev work required to include them in the service selection for installer wizard,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-08 22:40:29,34
13157976,Server Error while Bulk Deleting Hosts,"Server Error while Bulk Deleting Hosts

STR:
1) Create a config group
2) Bulk delete some hosts from UI that are in the config group. Deletion of Host fails with.
UI does not handle this failure and the wizard is stuck.",pull-request-available,[],AMBARI,Bug,Blocker,2018-05-08 18:23:00,55
13157930,Fix WebHCat property in kerberize.json in HDP 2.5,Use hive_metastore_hosts instead of hive_metastore_host,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-08 15:38:29,36
13157929,MySQL Connector JAR distribution is broken,"When attempting to use Oozie, Hive, Superset, and Druid with an external MySQL database the jar is not properly placed in each service so startup fails. Even after running the ambari-server setup pointing to a MySQL connector jar prior to install, we still see exceptions where the jar is not in the class path.

The reason is that  db connection properties in ambariLevelParams are not updated after modifying ambari.properties.",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2018-05-08 15:37:50,18
13157900,Need Ranger Tagsync component to be started at end during restart all services task for Federation wizard,"Current implementation on Federation wizard start Ranger Tagsync before Namenode start during start task of Ranger Service. Need Ranger Tagsync component to be stopped when we are updating service name in restart of namenode. 

So following sequence of starting services needs to be considered.

# Stop Tagsync (as part of stop all services)
# Start Ranger Admin and Ranger Usersync
# Start Namenode 
# Start Ranger Tagsync 
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-08 13:51:58,9
13157868,Agent is not notified about cluster delete in runtime,Server should notify agent about cluster removal for appropriate updating topology/configs/metadata/hostlevelparams caches.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-08 11:09:28,6
13157866,Manage Ambari UI issues,"See screenshots

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-08 11:07:37,19
13157865,Regenerate keytabs on single host should be an experimental feature,Regenerating keytabs on songle host should be available with experimental flag checked only ({{regenerateKeytabsOnSingleHost}}),pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-08 11:03:48,15
13157848,Make server/agent connection with no cert verification possible with agent python 2.7.5,"By reading <https://bugzilla.redhat.com/show_bug.cgi?id=1173041> and also the
last update of AMBARI-14149, I think the python fix is also backported into
python 2.7.5  
If so, could you change ""(2, 7, 9)"" to a lower version please?

Editing /etc/python/cert-verification.cfg wouldn't be ideal workaround as it
would affect to all other python applications in the system.  
And today I had a case which system didn't have this file (SUSE and Anaconda2
python)

Thank you

",pull-request-available,[],AMBARI,Bug,Major,2018-05-08 09:59:22,5
13157844,Log rotation for ambari metrics monitor log,The ambari metrics monitor logs are not rotated as of today. We should rotate them.,pull-request-available,[],AMBARI,Improvement,Major,2018-05-08 09:42:37,55
13157801,Download client configs fails at Oozie client with AttributeError,"- Deploy cluster for ambari-270 and HDP-3.0
- Download client configs for the cluster
- This fails with below error
{code}
{
status: 500,
message: ""org.apache.ambari.server.controller.spi.SystemException: Execution of ""ambari-python-wrap /var/lib/ambari-server/resources/stacks/HDP/3.0/services/OOZIE/package/scripts/oozie_client.py generate_configs /var/lib/ambari-server/data/tmp/OOZIE_CLIENT2729746438603000677-configuration.json /var/lib/ambari-server/resources/stacks/HDP/3.0/services/OOZIE/package /var/lib/ambari-server/data/tmp/structured-out.json INFO /var/lib/ambari-server/data/tmp"" returned 1. java.lang.Throwable: 2018-04-30 21:20:41,087 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0 2018-04-30 21:20:41,109 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1279/hadoop/conf 2018-04-30 21:20:41,209 - Directory['/var/lib/ambari-server/data/tmp'] \{'create_parents': True} 2018-04-30 21:20:41,293 - XmlConfig['oozie-site.xml'] {} Traceback (most recent call last): File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/OOZIE/package/scripts/oozie_client.py"", line 74, in <module> OozieClient().execute() File ""/usr/lib/ambari-server/lib/resource_management/libraries/script/script.py"", line 353, in execute method(env) File ""/usr/lib/ambari-server/lib/resource_management/libraries/script/script.py"", line 1093, in generate_configs Directory(conf_tmp_dir, action=""delete"") File ""/usr/lib/ambari-server/lib/resource_management/core/base.py"", line 166, in __init__ self.env.run() File ""/usr/lib/ambari-server/lib/resource_management/core/environment.py"", line 160, in run self.run_action(resource, action) File ""/usr/lib/ambari-server/lib/resource_management/core/environment.py"", line 124, in run_action provider_action() File ""/usr/lib/ambari-server/lib/resource_management/libraries/providers/xml_config.py"", line 60, in action_create xml_config_dest_file_path = os.path.join(xml_config_provider_config_dir, filename) File ""/usr/lib64/python2.7/posixpath.py"", line 77, in join elif path == '' or path.endswith('/'): AttributeError: 'NoneType' object has no attribute 'endswith' ""
}
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-08 05:58:02,1
13157798,Update upgrade-config.xsd to support new conditional statements for insert operation,"Insert statement now supporting conditional ""if"" statements, but it will newer pass xml validation using xsd.",pull-request-available,['ambari-sever'],AMBARI,Bug,Blocker,2018-05-08 05:24:46,1
13157681,Remove dependency on com.fasterxml.jackson.core:jackson-databind 2.7.8 in Ambari Metrics Collector,"Remove dependency on com.fasterxml.jackson.core:jackson-databind 2.7.8 in Ambari Metrics Collector due to security concerns. See
 * [https://nvd.nist.gov/vuln/detail/CVE-2018-5968]
 * [https://nvd.nist.gov/vuln/detail/CVE-2018-7489]
 * [https://nvd.nist.gov/vuln/detail/CVE-2017-7525]
 * [https://nvd.nist.gov/vuln/detail/CVE-2017-17485]
 * [https://nvd.nist.gov/vuln/detail/CVE-2017-15095]

 
{noformat}
HW15069:ambari-metrics-timelineservice smolnar$ mvn dependency:tree -Dincludes=com.fasterxml.jackson.core:jackson-databind -Dverbose=true
[INFO] Scanning for projects...
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Building Ambari Metrics Collector 2.0.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
 
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-metrics-timelineservice ---
[INFO] org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT
[INFO] +- org.apache.phoenix:phoenix-core:jar:5.0.0.3.0.0.0-1181:compile
[INFO] |  +- org.apache.hbase:hbase-mapreduce:jar:2.0.0.3.0.0.0-1181:compile
[INFO] |  |  +- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] |  |  \- org.apache.hadoop:hadoop-hdfs:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |  |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for conflict with 2.7.8)
[INFO] |  +- org.apache.hbase:hbase-common:jar:2.0.0.3.0.0.0-1181:compile
[INFO] |  |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:compile - omitted for duplicate)
[INFO] |  +- org.apache.hbase:hbase-client:jar:2.0.0.3.0.0.0-1181:compile
[INFO] |  |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:compile - omitted for duplicate)
[INFO] |  \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |     +- org.apache.hadoop:hadoop-hdfs-client:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |     |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for conflict with 2.7.8)
[INFO] |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for conflict with 2.7.8)
[INFO] +- org.apache.hadoop:hadoop-common:jar:3.0.0.3.0.0.0-1181:provided (scope not updated to compile)
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - scope updated from provided; omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-common:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - scope updated from test; omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-yarn-common:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - scope updated from test; omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-yarn-common:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile
[INFO] |  +- com.fasterxml.jackson.module:jackson-module-jaxb-annotations:jar:2.7.8:compile
[INFO] |  |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] |  \- com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:jar:2.7.8:compile
[INFO] |     +- com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:jar:2.7.8:compile
[INFO] |     |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-yarn-server-common:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |  \- org.apache.hadoop:hadoop-yarn-registry:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] +- org.apache.hbase:hbase-it:jar:tests:2.0.0.3.0.0.0-1181:test
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:test - omitted for conflict with 2.7.8)
[INFO] \- org.apache.hbase:hbase-testing-util:jar:2.0.0.3.0.0.0-1181:test
[INFO]    +- org.apache.hbase:hbase-common:test-jar:tests:2.0.0.3.0.0.0-1181:test
[INFO]    |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:test - omitted for conflict with 2.7.8)
[INFO]    +- org.apache.hadoop:hadoop-hdfs:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO]    |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:test - omitted for duplicate)
[INFO]    \- org.apache.hadoop:hadoop-minicluster:jar:3.0.0.3.0.0.0-1181:test
[INFO]       +- org.apache.hadoop:hadoop-yarn-server-tests:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO]       |  \- org.apache.hadoop:hadoop-yarn-server-timelineservice:jar:3.0.0.3.0.0.0-1181:test
[INFO]       |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:test - omitted for duplicate)
[INFO]       \- org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.0.0.3.0.0.0-1181:test
[INFO]          \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:test - omitted for duplicate)
{noformat}
Recommendation is to remove the dependency or upgrade to version 2.8.11.1 or the latest, if possible.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-07 19:15:30,27
13157596,Implement an upgrade helper in Ambari Metrics collector that copies data from the old schema to the new. ,"* The upgrade helper can be a main class that is callable through command line.
* Inputs to the upgrade helper
    # Metric list file (Default -> metrics_whitelist file in conf. If not present, copy over all the metrics)
    # Start timestamp (Default -> Last 1 month) (Max 8640 points per metric in minute table)
* The upgrade helper runs a set of SQL queries to fetch data from all aggregate tables except METRIC_AGGREGATE and uses the API to write to the corresponding new tables.
* The tool marks the completion of every metric entry by using a marker file (list of processed metric names) or a simple hbase table.
* The next time the upgrade tool is invoked, the marker is used to identify whether the metric needs to be copied.
* Let's have the upgrade helper log the status to a different file than collector log as a mechanism to monitor the status.

We have to make a decision on whether the upgrade tool helper should be *synchronous* or *asynchronous*. If it is asynchronous, we should add the ability for it to '*stop performing upgrade*'. ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-07 14:04:00,28
13157583,[Log Search UI] limitation for number of URL paramater value length,"There are some URL parameters ({{hostList}}, {{clusters}}, {{mustBe}}, {{userList}}) whose values are comma-separated lists.

Where these parameters appears we use POST requests instead of GET because of the limitation of the URL length what the browsers can handle.",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-05-07 13:18:26,26
13157579,"Ambari assigns /home for NameNode, DataNode and NodeManager directories","Apparently Ambari is assigning following additional directories for given service components, which was not seen in earlier versions.
HDFS - NameNode Directories - /home/hadoop/hdfs/namenode
HDFS - DataNode Directories - /home/hadoop/hdfs/datanode
YARN - NodeManager local directories - /home/hadoop/yarn/local
YARN - NodeManager log directories - /home/hadoop/yarn/log",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-07 12:59:49,55
13157559,Service summary style tweaks,See screenshot,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-05-07 11:01:07,19
13157526,Remove unsecure dependencies from ambari-agent,"Remove - or upgrade to a recommended version - the following libraries in ambari-agent due to security concerns:
 * Remove dependency on com.jcraft:jsch 0.1.42 (or upgrade to version is 0.1.45 or greater)
 * Remove dependency on org.mortbay.jetty:jetty-util 6.1.26 (or upgrade to version is 6.1.26.hwx or greater)
 * Remove dependency on org.apache.zookeeper:zookeeper 3.4.9 (or upgrade to version 3.4.10, 3.5.3, and later. pre [CVE-2017-5637|https://nvd.nist.gov/vuln/detail/CVE-2017-5637])
 * Remove dependency on commons-httpclient:commons-httpclient 3.1 (or upgrade to version  5.0-alpha2-RC1)
 * Remove dependency on commons-beanutils:commons-beanutils-core 1.8.0 (or upgrade to version 1.9.2 or 1.9.3)

 ",black-duck pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-05-07 08:11:33,27
13157378,Suppress FindBugs warnings for EclipseLink-generated code,"FindBugs produces 1428 warnings for String comparisons in Ambari Server's statically weaved entities.  This amounts to almost 400KB in Jenkins build logs.

{noformat:title=}
[INFO] Comparison of String parameter using == or != in org.apache.ambari.server.orm.entities.AlertCurrentEntity._persistence_get(String)  [org.apache.ambari.server.orm.entities.AlertCurrentEntity] In AlertCurrentEntity.java ES_COMPARING_PARAMETER_STRING_WITH_EQ
[INFO] Comparison of String parameter using == or != in org.apache.ambari.server.orm.entities.AlertCurrentEntity._persistence_set(String, Object)  [org.apache.ambari.server.orm.entities.AlertCurrentEntity] In AlertCurrentEntity.java ES_COMPARING_PARAMETER_STRING_WITH_EQ
{noformat}

As we have no control over this kind of generated code, it should be suppressed, both to focus on more real problems pointed out by FindBugs, and to reduce size of our build logs.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-05-05 18:45:43,21
13157340,Background Ops modal breaks in some cases,"Click on BGO filter which doesn't have any operations. Eg FAILED(0). Click on All(10) again. Modal breaks.

!Screen Shot 2018-05-04 at 9.40.01 PM.png!",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-05 04:45:59,34
13157331,Customize Services - Alignment between Final/Set Recommended buttons and text boxes not proper in new tabs,"For customize services step in the Install Wizard:
The alignment between Final/Set Recommended/Override buttons and the corresponding text boxes is not proper in new DATABASES/DIRECTORIES tabs. There is lot of space in between them.

For ALL CONFIGURATIONS tab, it is proper and lies next to the text box.

The UI should be consistent across all tabs i.e. buttons should lie closer to the text box to enable user to modify intuitively.
Kindly check.

Issue#2 : The colour of the 'Set Recommended' Icon for All Configurations Tab and DIRECTORIES/DATABASES is not same. 
Issue#3: The Set Recommended and Final Buttons only appear on mouse over for Directories/Databases tab. However , they are stable on All Configurations Tab. Expected Behaviour is - the buttons should be visible always.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-05 01:54:52,34
13157263,Insert operation in upgrade pack extension ,"Insert operation in upgrade packs should be extended in such ways:
 * allow conditional operations

 

Conditions are extended as well, to enrich their functionality:
 * {{if-value-match-type}}, which allow exact or partial match. Default is exact
 * {{if-value-not-matched}}, allow operation when no matches found

 ",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-05-04 17:54:15,1
13157244,"Icon for ""start demo ldap"" is missing","Icon for ""Start Demo Ldap"" is missing from knox action list in ambari.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-04 16:13:42,15
13157173,Corrupt mapreduce/tez tar.gz may be uploaded to HDFS if parallel execution is enabled,"If parallel_execution on Ambari Agent is enabled, two components (History Server and Hive Server) may create and upload the MapReduce and Tez archives concurrently.  This could result in a corrupt uploaded file, preventing Hive Server from starting.

{noformat:title=output-32.txt}
2018-05-04 07:24:40,421 - Creating a new Tez tarball at /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz
...
2018-05-04 07:24:49,751 - Creating new file /hdp/apps/2.6.4.5-2/tez/tez.tar.gz in DFS
2018-05-04 07:24:49,753 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '""'""'%{http_code}'""'""' -X PUT --data-binary @/var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz -H '""'""'Content-Type: application/octet-stream'""'""' '""'""'http://localhost:50070/webhdfs/v1/hdp/apps/2.6.4.5-2/tez/tez.tar.gz?op=CREATE&user.name=hdfs&overwrite=True&permission=444'""'""' 1>/tmp/tmpoEINHo 2>/tmp/tmpkjlNxP''] {'logoutput': None, 'quiet': False}
2018-05-04 07:24:51,629 - call returned (0, '')
{noformat}

{noformat:title=output-30.txt}
2018-05-04 07:24:46,818 - Creating a new Tez tarball at /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz
...
2018-05-04 07:24:54,117 - DFS file /hdp/apps/2.6.4.5-2/tez/tez.tar.gz is identical to /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz, skipping the copying
{noformat}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-05-04 14:18:19,21
13157168,Remove topology info from non-blueprint commands,Server should sent topology info as a part of command only during blueprint commands posting.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-04 13:56:45,6
13157130,Subscriptions should be bound to WebSocket connection,"Stomp client subscription should happen after WebSocket connected.
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-04 12:19:59,19
13157126,Updating default Ranger Hdfs service name for NN Federation enabled cluster,Adding code logic in [ranger function file|https://github.com/apache/ambari/blob/trunk/ambari-common/src/main/python/resource_management/libraries/functions/ranger_functions_v2.py] for updating service name on Ranger Admin using Ranger REST API.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-04 11:51:52,25
13157125,Add organization / license data to ambari-infra and ambari-logsearch,Add organization / license data to ambari-infra and ambari-logsearch as those are not inherited from the top level parent anymore,pull-request-available,"['ambari-infra', 'ambari-logsearch']",AMBARI,Bug,Critical,2018-05-04 11:45:19,29
13157112,Dropdowns in Host Details Page do not conform to the new UI,"In ambari-2.7, the dropdowns in the following screens do not conform to the rest of the UI.

# Host Details Page -> Configs -> Change Config Group -> Groups
# Host Details Page -> Alerts -> Filters (Service, Status) 
# Host Details Page -> Versions -> Filters (Stack, Name, Status) 
# Host Details Page -> Logs -> Filters (Service, Component, Extension) 

Whereas dropdowns in  other pages, e.g. HDFS -> Configs -> Config Group have a different design.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-04 10:49:32,19
13156996,[Logsearch UI] Log graph gets filled with log level icons on clicking inside the graph,"[Logsearch] Log graph gets filled with log level icons on clicking inside the graph.
STR:
1. Login to logsearch UI
2. Use time range picker and select a valid time range (E.g Last 7 days)
3. Once logs are loaded, click on the bar graph 
4. Valid log graphs should be displayed with the custom range, but graph content is getting spammed with log level icons instead. Attaching screenshot.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Critical,2018-05-03 23:17:37,26
13156995,[Logsearch UI] Filter dropdown components not displayed properly,"[Logsearch UI] Filter dropdown components not displayed properly
STR:
1. Login to logsearch UI
2. Keep the graph hidden and date range 'Previous Week' (This is to make sure no logs are displayed) 
2. Try different filter options and user should be able to use all filter properly
3. Many filters are not displayed properly/completely, hence user will not able to interact with them properly.

E.g 
1. Custom time range picker calendar is displayed partially
2. Log filter drop down is only displayed partially",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Critical,2018-05-03 23:14:56,26
13156994,[Logsearch UI] Component filter 'All' not refreshing logs after selecting an individual component,"[Logsearch UI] Component filter 'All' not refreshing logs after selecting an individual component

STR:
1. Login to logsearch portal and select a valid date range
2. From the Component filter drop down, select an individual component e.g Ambari-agent
3. Now ambari-agent logs are filtered and displayed
4. Now select the 'All' from the drop down
5. The logs table should now refresh with logs from all components, but fails to refresh and displays only previously selected logs. User need to refresh the page or navigate away and come back to refresh the table",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-05-03 23:11:50,26
13156944,Ambari Infra Solr Service Check fails after Ambari Upgrade,"*STR*
1) Upgrade Ambari from 2.6.X to 2.7.0.0-435 ( Unkerberized cluster)
2) Upgrade Non Stack Services, like Infra Solr.
3) Restart all required services (which might have stale configs)
4) Run Service check on Ambari Infra Solr . It fails with below error

{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/service_check.py"", line 48, in <module>
    InfraServiceCheck().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/service_check.py"", line 27, in service_check
    import params
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/params.py"", line 109, in <module>
    infra_solr_java_stack_size = format(config['configurations']['infra-solr-env']['infra_solr_java_stack_size'])
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/format.py"", line 95, in format
    return ConfigurationFormatter().format(format_string, args, **result)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/format.py"", line 59, in format
    result_protected = self.vformat(format_string, args, all_params)
  File ""/usr/lib64/python2.7/string.py"", line 549, in vformat
    result = self._vformat(format_string, args, kwargs, used_args, 2)
  File ""/usr/lib64/python2.7/string.py"", line 558, in _vformat
    self.parse(format_string):
  File ""/usr/lib64/python2.7/string.py"", line 621, in parse
    return format_string._formatter_parser()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'infra_solr_java_stack_size' was not found in configurations dictionary!
{code}

*Cause*
While upgrading the name of the AMBARI_INFRA service is changed to AMBARI_INFRA_SOLR, but the old name is cached in the JPA entities cause a mismatch on services names at some point. 

*Solution*
Clear the JPA entity cache after changing the AMBARI_INFRA service name. 
",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-05-03 19:22:43,30
13156925,Exception: Expected confdir /usr/hdp/3.0.0.0-1252/accumulo/conf to be a symlink,"
Ambari: 2.7.0.0-389, HDP: HDP-3.0.0.0-1252

HDP version was not installed on one of the host. I attempted an re install, and the install succeeded. But the version is not set to current.

I see the following error in the install packages output:
{code}
2018-04-25 12:44:01,046 - checked_call[('ambari-python-wrap', u'/usr/bin/conf-select', 'set-conf-dir', '--package', u'accumulo', '--stack-version', u'3.0.0.0-1252', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}
2018-04-25 12:44:01,080 - Could not select the directory for package accumulo. Error: Execution of 'ambari-python-wrap /usr/bin/conf-select set-conf-dir --package accumulo --stack-version 3.0.0.0-1252 --conf-version 0' returned 1. Traceback (most recent call last):
File ""/usr/bin/conf-select"", line 178, in <module>
setConfDir(options.pname, options.sver, options.cver)
File ""/usr/bin/conf-select"", line 136, in setConfDir
raise Exception(""Expected confdir %s to be a symlink."" % confdir)
Exception: Expected confdir /usr/hdp/3.0.0.0-1252/accumulo/conf to be a symlink.
2018-04-25 12:44:01,081 - /etc/accumulo/conf is already linked to /usr/hdp/current/accumulo-client/conf
{code}

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-03 17:11:07,13
13156920,UI Issues for HDFS Metrics when having multiple namespaces,"Issue #1) 
 On Hdfs metrics page, add a widget. That is displayed in the top panel for the default namespace and then at the bottom for the namespace selected (ns1,ns2 or All)

Should the widget on the top show the aggregate metrics instead of metrics for just default namespace?
 OR 
 Should they not appear in the top section and be displayed only for the namespace selected at the bottom?
!image-2018-05-07-11-08-12-178.png!

Issue #2)
 Create custom widget for a namespace
 No Namespace name next to the custom widget



!image-2018-05-07-11-08-25-040.png!!image-2018-05-07-11-08-25-003.png!

 

Issue #3)
 Duplicate widgets in the Browse Widgets window where there are multiple namespaces

!image-2018-05-07-11-08-35-880.png!!image-2018-05-07-11-08-35-838.png!

 ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-03 16:59:50,9
13156873,Hide SmartSense master components in HDF stack,"Our assumption is that ambari should be able to allow zero node deployment of this master component. But unfortunately, UI does not have any provision to remove master components (which has cardinality 0-3). This causes dependency issues in HDF environment where the dependent component does not exists.
For the Activity Analyzer and Activity Explorer. We don’t want those to show up in assign masters for the HDF stack.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-03 13:47:12,19
13156864,Ambari ViewFS: Add ability to add XML inclusion node to a configuration file,"ViewFS mountable as recommended here: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ViewFs.html, should be added as XML inclusion node to core-site.

Currently Ambari does not support this. We should add a special field-type that can be serialized by the agent as not a key-value but write out the value as XML node in this case inclusion field.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-03 12:50:23,28
13156830,[Logsearch UI] Logout button does not navigate user to login/logged out page,"Logout button does not navigate user to login/logged out page.

STR:
1. Login to Logsearch portal
2. Click on logout button from the user menu
3. User should be taken back to login page with session cleared, but still stays on the same page, and on clicking on some links in the page, page displays distorted content.",pull-request-available,['ambari-logsearch'],AMBARI,Task,Critical,2018-05-03 10:47:02,26
13156722,"Creation of service, servicecomponents and host components during multimpack deployment has issues","Following issues found during multi mpack.
 * Create service with same name in two different service group throws validation error, services map error (Eg: HADOOP_CLIENTS from HDPCORE-default & ODS-default servicegroups)
 * Create service component with same name in two different service group throws error (Eg: HADOOP_CLIENT from HDPCORE-default & ODS-default servicegroups)
 * Create host component with same name in same host but two different service group throws error (Eg: HADOOP_CLIENT from HDPCORE-default & ODS-default servicegroups)",pull-request-available,[],AMBARI,Bug,Major,2018-05-02 22:57:00,56
13156676,Replace Multiple Matches Using Regex on Upgrades,"When upgrading from one stack to another, the {{regex-replace}} key can be used to find and replace a block of text which matches a given regular expression. This works by finding the literal string matching the regex and turning this into a regular find/replace. 

For example: 
{code}
<regex-replace find=""\d-foo-\d"" replace=""REPLACED""/>
{code}

on

{code}
1-foo-1
2-foo-2
3-foo-3
{code}

Would produce:
{code}
REPLACED
2-foo-2
3-foo-3
{code}

In order to replace all of the possible matches, we can extend the XSD of {{RegexReplace}} to create multiple literal {{Replace}} instances for every match.

{code}
<regex-replace find=""\d-foo-\d"" replace="""" match-all=""true""/>
{code}

Would produce:
{code}
REPLACED
REPLACED
REPLACED
{code}",pull-request-available,[],AMBARI,Task,Critical,2018-05-02 18:38:08,31
13156674,Fix UUID computation issues in AMS.,"* Fix UUID issues in Metadata manager
* Default to MD5 based UUID gen strategy
* Add ability to store and retrieve transient metrics from a separate table (without change in API). 
* Introduce metrics/summary endpoint for metadata and aggregation summary information.
* Change precision determination logic for GET queries.
* Fix issue in phoenix sqline packaged in AMS.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-02 18:29:46,39
13156631,Hive Service Summary View Links Need to be removed,"The ""Hive View 2.0"" and ""Debug Hive Query"" links need to be removed from the Hive Summary section as they are being removed from Ambari 2.7/HDP 3.0.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-02 15:05:22,19
13156630,Starting JHS Takes Too Long Due To Tarball Extraction,"Starting the Job History Server might time out on systems due to the use of Python's {{tarfile}} module. Instead, calling {{tar}} directly is much faster.

{code}
2018-05-01 17:28:44,102 - Extracting /usr/hdp/2.6.3.0-235/hadoop/mapreduce.tar.gz to /var/lib/ambari-agent/tmp/mapreduce-tarball-nqNJdx
2018-05-01 17:53:12,992 - Extracting /usr/hdp/2.6.3.0-235/tez/lib/tez.tar.gz to /var/lib/ambari-agent/tmp/tez-tarball-9CbDSh
2018-05-01 17:53:12,993 - Execute[('tar', '-xf', '/usr/hdp/2.6.3.0-235/tez/lib/tez.tar.gz', '-C', '/var/lib/ambari-agent/tmp/tez-tarball-9CbDSh/')] {'tries': 3, 'sudo': True, 'try_sleep': 1}
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-05-02 15:04:28,31
13156618,JS error after installing Ranger from Install Wizard - Smart Configs are broken - page refresh fixed the issue,"See attached.
Modifed FE code (doNotShowAndInstall computed property) so that Ranger can be installed from Install Wizard.  Installation succeeded.  However, post-Install, there were some JS errors (see attached) that prevented the Smart Config pages from loading.  See attached. !Screen Shot 2018-04-27 at 2.17.22 PM.png|thumbnail! 
After page refresh, the problem went away and Smart Configs can be viewed just fine.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-02 14:31:41,19
13156604,Namespace names are converted to uppercase after selecting from dropdown,Selected namespace names are shown in uppercase where as they are actually created in lowercase and in dropdown it's in lowercase,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-02 13:18:51,15
13156544,Need to add pre-upgrade check for Atlas service during stack upgrade,"While upgrading to stack 3.0, Atlas needs to migrate data from existing version to upgraded versions. To migrate the data we need to provide {{atlas.migration.data.filename}} property in the application-properties before upgrading the stack.

Hence prior to upgrading from existing stack to 3.0 stack we will need to check if the property {{atlas.migration.data.filename}} exists or not.
 Reference ATLAS-2460.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-02 09:18:54,25
13156394,JS error adding Ranger to a cluster,"The following error occurs in the UI while trying to add Ranger to a cluster with the following services:
 * HDFS
 * HBase
 * Atlas
 * Zookeeper
 * Infra Solar
 * Kafka

{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
    at Class.selectProperService (app.js:41945)
    at Class.applyServicesConfigs (app.js:41459)
    at Object.<anonymous> (app.js:41407)
    at fire (vendor.js:1141)
    at Object.add [as done] (vendor.js:1187)
    at Class.loadStep (app.js:41405)
    at Class.didInsertElement (app.js:202902)
    at Class.newFunc [as didInsertElement] (vendor.js:12954)
    at Class.trigger (vendor.js:25526)
    at Class.newFunc [as trigger] (vendor.js:12954)
{noformat}
!screenshot-1.png!

STR
 # Create cluster with HDFS, Atlas, Zookeeper, Infra Solar, Kafka, Ambari Metrics, Smart Sense
 # Add HBase (apparently required by Atlas)
 # Remove SmartSense
 # Remove Ambari Metrics (fails to start with some missing PID file)
 # Add Ranger, see failure",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-01 17:51:30,34
13156339,Expose Mpack Information As Individual Fields for Service Groups,"The service group endpoint does not expose the following fields:

- Mpack ID
- Mpack Name
- Mpack Version

Instead, it exposes a single string called {{version}} which represents the full Mpack: {{FOO-1.0.0-b1234}}.

This is not ideal for several reasons:
- It is not easy to determine the mpack associated with a service group without doing string parsing and string comparison.
- When creating a new service group, a string is used to query instead of an mpack ID.
",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-05-01 14:39:00,31
13156260,Customize Services - Tooltip issues on Accounts Tab and All Configuration Tabs,"1) Tooltips are seen twice for all text boxes on Accounts Tab under Customize Service Configuration Page.
Also seen under Misc Service Accounts Section of All Configurations Tab.

2) If length of the label is large , it is going outside of tooltip.This is applicable for All Configurations Tab.
See hadoop.http.authentication.simple.anonymous.allowed porperty tooltip under HDFS Service.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-30 23:47:48,34
13156164,Restart required services results in restart NN twice,"On large clusters restart NN can take 40 mins , if we start nn1 and nn2 in Start Namenodes step they should not be Restarted again.
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-30 17:40:22,9
13156117,"When Spark2 is selected in left pane, Spark also gets selected along with it","When Spark2 is selected in services in left pane, Spark gets selected along with it.
The pointer should only point to Spark2 (which is the selected service).",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-30 14:30:20,15
13156116,Start Namenodes takes a very long time when adding a namespace,"On newly added nn5 and nn6 I see the Start NN call waits for JMX to return, we are not starting Ranger so NN JMX does not respond. However there seems to be logic to skip the check if Ranger Admin is not up:

{code}
2018-04-20 18:34:29,072 - RangeradminV2: Skip ranger admin if it's down !
2018-04-20 18:34:29,093 - checked_call['/usr/bin/kinit -c /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_hdfs_cc_91f03cfb858cf8a975f355b846c7b45988d51543f0ec7ea2d1da3738 -kt /etc/security/keytabs/nn.service.keytab nn/ctr-e138-1518143905142-250606-01-000002.hwx.site@EXAMPLE.COM > /dev/null'] {'user': 'hdfs'}
2018-04-20 18:34:29,183 - checked_call returned (0, '')
{code}

Still we are waiting for a long time and them actually succeeding.

If Ranger being up is needed please assign this appropriately to UI team to make sure we start required services before Starting NN. If the skip check is desired behavior we should be able to proceed in that case.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-30 14:30:04,9
13156082,Group HDFS components into the components section of the ambari service summary page,The HDFS Service Page in the new Ambari UI page displays useful information about the service components. This information would be easier to digest if all the HDFS components were grouped together at the top of the page (in the components section) as they are for the rest of the services.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-30 09:46:48,15
13156057,Predicate evaluation does not work as expected for RequestResourceFilters,"The ""host_predicate"" added to the Request Resource by AMBARI-14394, works only for filtering provided by the ResourceProvider. The Query predicates are actually handled up in the API layers and hence to do something like restart all except Namenode kind of predicate does not work, although the intention of the previous change was that.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-30 02:11:42,42
13155791,Mpack Installation Response is not Updating State,{{mpack_host_state}} is not being updated when the custom action is getting invoked.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-04-27 20:09:41,33
13155777,Removal of Deprecated OperatingSystem/Repository/RepositoryVersion Logic,"There is no new functionality here. This is to track the removal of deprecated REST endpoints and logic which dealt with the following:

- Operating Systems off of a stack
- Repositories off of a stack
- Repositories off of a host
- Cluster creation via repositories
- VDF XML references
- Reliance on RepositoryVersionEntity",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-04-27 19:04:40,31
13155768,Ambari allows namespace name to be reused for new namespaces,"- Deployed a cluster with namespaces ns1 and ns2
- Try Adding new namespace via UI
- Name it ns1
- Next button should not be enabled and error message should be displayed to avoid conflicting namespaces",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 18:25:21,9
13155760,Adding and deleting widgets of NameNode section on dashboard isn't persisted,"*STR*
# Enable NameNode federation
# Add or remove some NameNode widgets on dashboard
# Go to other page and retirn to dashboard, or stay on dashboard and refresh the page

*Expected result*
NameNode widgets are displayed/hidden according to the adding/removing actions described above

*Actual result*
Default set of NameNode widgets is displayed",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 18:14:38,15
13155735,Ambari UI trying to create GPL repo with empty base url for the rest repos,"On cluster deploy, Ambari UI sending such request body: 

{code}
{""operating_systems"":[{""OperatingSystems"":{""os_type"":""redhat7"",""ambari_managed_repositories"":true},""repositories"":[{""Repositories"":{""base_url"":"""",""repo_id"":""HDP-3.0"",""repo_name"":""HDP"",""components"":null,""tags"":[],""distribution"":null}},{""Repositories"":{""base_url"":"""",""repo_id"":""HDP-3.0-GPL"",""repo_name"":""HDP-GPL"",""components"":null,""tags"":[""GPL""],""distribution"":null}},{""Repositories"":{""base_url"":"""",""repo_id"":""HDP-UTILS-1.1.0.21"",""repo_name"":""HDP-UTILS"",""components"":null,""tags"":[],""distribution"":null}}]}]}: 
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-27 15:42:14,19
13155704,hbase.zookeeper.property.clientPort config of AMS is empty after Ambari Upgrade,"STR

1). Install Ambari-2.6.0.0-267
2) Upgrade Ambari to 2.7.0.0-308 
3) Check for hbase.zookeeper.property.clientPort property under Advanced ams-hbase-site in UI. It is empty and is shown as required. But it has a value before Upgrade . 
",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-27 13:16:20,18
13155697,Stack installation command didn't fail properly when installed package didn't present in repository,"Log output 
{code}
2018-04-24 11:34:41,061 - Will install packages for repository version 3.0.0.0
2018-04-24 11:34:41,061 - Repository['HDP-3.0-repo-51'] \{'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-375', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname=\{{repo_id}}\n\{% if mirror_list %}mirrorlist=\{{mirror_list}}\{% else %}baseurl=\{{base_url}}\{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2018-04-24 11:34:41,069 - File['/etc/yum.repos.d/ambari-hdp-51.repo'] \{'content': '[HDP-3.0-repo-51]\nname=HDP-3.0-repo-51\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-375\n\npath=/\nenabled=1\ngpgcheck=0'}
2018-04-24 11:34:41,070 - Writing File['/etc/yum.repos.d/ambari-hdp-51.repo'] because contents don't match
2018-04-24 11:34:41,070 - Repository['HDP-3.0-GPL-repo-51'] \{'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1195', 'action': ['create'], 'components': [u'HDP-GPL', 'main'], 'repo_template': '[{{repo_id}}]\nname=\{{repo_id}}\n\{% if mirror_list %}mirrorlist=\{{mirror_list}}\{% else %}baseurl=\{{base_url}}\{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2018-04-24 11:34:41,074 - File['/etc/yum.repos.d/ambari-hdp-51.repo'] \{'content': '[HDP-3.0-repo-51]\nname=HDP-3.0-repo-51\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-375\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-51]\nname=HDP-3.0-GPL-repo-51\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1195\n\npath=/\nenabled=1\ngpgcheck=0'}
2018-04-24 11:34:41,074 - Writing File['/etc/yum.repos.d/ambari-hdp-51.repo'] because contents don't match
2018-04-24 11:34:41,074 - Repository['HDP-UTILS-1.1.0.22-repo-51'] \{'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname=\{{repo_id}}\n\{% if mirror_list %}mirrorlist=\{{mirror_list}}\{% else %}baseurl=\{{base_url}}\{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2018-04-24 11:34:41,078 - File['/etc/yum.repos.d/ambari-hdp-51.repo'] \{'content': '[HDP-3.0-repo-51]\nname=HDP-3.0-repo-51\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-375\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-51]\nname=HDP-3.0-GPL-repo-51\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1195\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-51]\nname=HDP-UTILS-1.1.0.22-repo-51\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
2018-04-24 11:34:41,078 - Writing File['/etc/yum.repos.d/ambari-hdp-51.repo'] because contents don't match
2018-04-24 11:34:41,079 - Yum non-completed transactions check passed
2018-04-24 11:34:41,079 - call[('ambari-python-wrap', u'/usr/bin/hdp-select', 'versions')] {}
2018-04-24 11:34:41,106 - call returned (0, '2.6.5.0-263')
2018-04-24 11:34:41,107 - Installing package hdp-select ('/usr/bin/yum -y install '--disablerepo=*' --enablerepo=HDP-3.0-GPL-repo-51,HDP-3.0-repo-51,HDP-UTILS-1.1.0.22-repo-51 hdp-select')
2018-04-24 11:34:44,747 - Looking for matching packages in the following repositories: HDP-3.0-repo-51, HDP-3.0-GPL-repo-51, HDP-UTILS-1.1.0.22-repo-51
2018-04-24 11:34:55,803 - Adding fallback repositories: HDP-UTILS-1.1.0.22-repo-1
2018-04-24 11:34:59,364 - Installing package gcc ('/usr/bin/yum -y install gcc')
2018-04-24 11:34:59,954 - Installing package python-kerberos ('/usr/bin/yum -y install python-kerberos')
2018-04-24 11:35:01,097 - No package found for hadoop_${stack_version}(hadoop_(\d|_)+$)
2018-04-24 11:35:01,097 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:01,099 - No package found for hadoop_${stack_version}-client(hadoop_(\d|_)+-client$)
2018-04-24 11:35:01,099 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:01,099 - Installing package snappy ('/usr/bin/yum -y install snappy')
2018-04-24 11:35:01,724 - Installing package snappy-devel ('/usr/bin/yum -y install snappy-devel')
2018-04-24 11:35:02,713 - No package found for hadoop_${stack_version}-libhdfs(hadoop_(\d|_)+-libhdfs$)
2018-04-24 11:35:02,713 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,715 - No package found for hadoop_${stack_version}-mapreduce(hadoop_(\d|_)+-mapreduce$)
2018-04-24 11:35:02,715 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,716 - No package found for tez_${stack_version}(tez_(\d|_)+$)
2018-04-24 11:35:02,716 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,717 - No package found for zookeeper_${stack_version}(zookeeper_(\d|_)+$)
2018-04-24 11:35:02,718 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,719 - No package found for zookeeper_${stack_version}-server(zookeeper_(\d|_)+-server$)
2018-04-24 11:35:02,719 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,720 - No package found for hadoop_${stack_version}-yarn(hadoop_(\d|_)+-yarn$)
2018-04-24 11:35:02,720 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,721 - No package found for hadoop_${stack_version}-mapreduce(hadoop_(\d|_)+-mapreduce$)
2018-04-24 11:35:02,722 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,723 - No package found for hadoop_${stack_version}-hdfs(hadoop_(\d|_)+-hdfs$)
2018-04-24 11:35:02,723 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:02,724 - No package found for hbase_${stack_version}(hbase_(\d|_)+$)
2018-04-24 11:35:02,724 - Installing package None ('/usr/bin/yum -y install ''')
2018-04-24 11:35:04,147 - Attempting to determine actual version with build number.
2018-04-24 11:35:04,147 - Old versions: ['2.6.5.0-263']
2018-04-24 11:35:04,147 - call[('ambari-python-wrap', u'/usr/bin/hdp-select', 'versions')] {}
2018-04-24 11:35:04,179 - call returned (0, '2.6.5.0-263')
2018-04-24 11:35:04,179 - New versions: ['2.6.5.0-263']
2018-04-24 11:35:04,179 - Deltas: set([])
2018-04-24 11:35:04,179 - Cannot determine actual version installed by checking the delta between versions before and after installing package
2018-04-24 11:35:04,179 - Will try to find for the actual version by searching for best possible match in the list of versions installed
2018-04-24 11:35:04,179 - Failure while computing actual version. Error: Could not determine actual version installed. Try reinstalling packages again.
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 451, in install_packages
 self.compute_actual_version()
 File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 296, in compute_actual_version
 raise Fail(msg)
Fail: Could not determine actual version installed. Try reinstalling packages again.

Command failed after 1 tries
{code}

No real installation command should be passed, additionally would be great to add check and info for the user about real issue",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-27 12:50:05,1
13155678,Kafka Quick Links Shows all Views links,"The Kafka Service Summary screen shows all Ambari View instances under the ""Views"" Quick Link.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 10:51:16,15
13155677,Config Version Comparison tool shows diff text merging into each other if config values are long.,"While comparing two config versions , if config values are long , than the diff shows values merging into each other and text is not wrapped.

See Attached Screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 10:51:16,19
13155653,Ambari Schema Upgrade Failing source 2.6.2.0 target 2.7.0.0,"When upgrading Ambari from 2.6.x to 2.7.0 an error occured in SchemaUpgradeHelper:
{code:java}
15 Apr 2018 17:12:06,307 INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE repo_tags (repo_definition_id BIGINT NOT NULL, tag VARCHAR(255) NOT NULL) ENGINE=INNODB
15 Apr 2018 17:12:06,333 INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE repo_tags ADD CONSTRAINT FK_repo_tag_definition_id FOREIGN KEY (repo_definition_id) REFERENCES repo_definition (id)
15 Apr 2018 17:12:06,349 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.lang.NullPointerException
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.migrateRepoData(UpgradeCatalog270.java:443)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeRepoTables(UpgradeCatalog270.java:323)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:291)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
15 Apr 2018 17:12:06,350 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.lang.NullPointerException
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.migrateRepoData(UpgradeCatalog270.java:443)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeRepoTables(UpgradeCatalog270.java:323)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:291)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 ... 1 more{code}
This issue occurred only when we had data in _repo_version_ and the _repositories_ JSON contained missing data (_OperatingSystems/ambari_managed_repositories_ was missing).

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-27 08:26:18,27
13155547,Customize Services - The selected service/Modified configurations are not remembered when navigating away/to the configuration tabs,"Customize service improvements:

#1 There should be 'data will be lost warning' on clicking the 'Back' button or clicking on step previous to this one. The same should be for 'Add Service' and 'Add Host' wizard.

#2 When navigating between different tabs (Directories, all configs, etc.)UI should remember which service was active last time.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-26 22:06:12,34
13155460,Agent uses compressed topology upon retry,"HiveServer 2 fails to start due to:

{noformat}
2018-04-26 07:54:04,971 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server 0:2181,2:2181,4:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
2018-04-26 07:54:05,693 - call returned (1, 'Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
{noformat}

ZooKeeper connection string contains host indexes instead of hostnames.  This only happens upon command retry, initial run uses hostnames.",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-04-26 16:45:59,21
13155435,NN Federation Wizard: move out of wizard create widgets operaions,"As we should handle installing Ambari with federation from blueprint, we should move create widgets logic out of wizard and run it on first dashboard page visit.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-26 15:15:45,9
13155421,NameNode HA wizard style changes,"We should use the standard gray in the background. rgba(233, 233, 233, 0.5).
See screenshots",pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-04-26 14:11:17,19
13155413,Need better xpaths for quicklinks when multiple namespaces are present,Need better xpaths for quicklinks when multiple namespaces are present. It would be better if quicklinks are grouped by namespaces,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-26 13:42:40,15
13155399,"While adding HDFS Namespace from UI, Timeline service fails to start","While adding HDFS Namespace from UI, Timeline service fails to start in the last step (""Restart All Services"") because the active Namenode(from older namespace) is in Safe Mode

In Restart all services, Timeline Service starts right after Old Active NN starts 
This might be intermittent timing issue.",pull-request-available,[],AMBARI,Bug,Critical,2018-04-26 12:46:25,55
13155355,Adding new namespace fails at Reconfigure Services for HDFS 2,"Add New HDFS Namespace wizard fails at Reconfigure Services step when trying to add new namespace ""ns2"" (existing namespace is ""TEST"") in a HDP 2.6 cluster.

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_client.py"", line 78, in <module>
    HdfsClient().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_client.py"", line 35, in install
    import params
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py"", line 330, in <module>
    if hostname.lower() in nn_host.lower() or public_hostname.lower() in nn_host.lower():
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.rpc-address.TEST,ns2.nn1' was not found in configurations dictionary!
{noformat}",pull-request-available,['stacks'],AMBARI,Bug,Critical,2018-04-26 09:52:58,21
13155342,Stack scripts should only try to reach corresponding Namenode pair in the context rather than all NNs,"A good example is when we start up a specific Namenode, say NN1. In that case,
we try to get the HAState of all the NNs (even NN3 and NN4) to find out the
active namenode. This is not really needed since starting up NN1 should not
care about NN3 and NN4's state.

I see usages in following places.

  * ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py
  * ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode_ha_state.py
  * ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py

Some of the above might not need any change.

",pull-request-available,[],AMBARI,Bug,Major,2018-04-26 08:52:22,5
13155263,Remove unsecure dependencies from ambari-utility,"# Remove dependency on com.fasterxml.jackson.dataformat:jackson-dataformat-xml 2.4.5 in ambari-utility due to security concerns. Recommendation is to remove the dependency or upgrade to version 2.6.3.redhat-3 or the latest version, if possible.
 # Remove dependency on com.fasterxml.jackson.core:jackson-databind 2.9.4 in ambari-utility due to security concerns. Recommendation is to remove the dependency or upgrade to version 2.9.5 or the latest, if possible.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-25 22:27:29,27
13155247,Replace Cluster install_packages with an Upgrade Plan Installation,"Add functionality to install an mpack for upgrade purposes.

Does not include tests.  Add an annotation marker for this purpose.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-04-25 20:37:42,33
13155235,Symlinks are not followed when requesting resources from Ambari's resources entry point,"Symlinks are not followed when requesting resources from Ambari's resources entry point.

For example if the Beacon mpack is installed, a symlink is made from /var/lib/ambari-server/resources/mpacks/beacon-engine.mpack-1.1.0.0/addon-services/BEACON/1.1.0 to /var/lib/ambari-server/resources/stacks/HDP/2.6/services/BEACON:
{noformat}
# ls -ltr /var/lib/ambari-server/resources/stacks/HDP/2.6/services/BEACON
lrwxrwxrwx 1 root root 95 Apr 25 07:03 /var/lib/ambari-server/resources/stacks/HDP/2.6/services/BEACON -> /var/lib/ambari-server/resources/mpacks/beacon-engine.mpack-1.1.0.0/addon-services/BEACON/1.1.0
{noformat}

{noformat}
# curl -i -X GET http://c7401.ambari.apache.org:8080/resources/stacks/HDP/2.6/services/BEACON/metainfo.xml
HTTP/1.1 404 Not Found
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Pragma: no-cache
Content-Type: text/plain;charset=ISO-8859-1
Content-Length: 45

{
  ""status"": 404,
  ""message"": ""Not Found""
}
{noformat}

When this occurs, the stack definitions from BEACON are not able to be cached by the Ambari agents. 
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-25 19:53:26,30
13155170,The UI should ignore Kerberos identity references when setting the user-supplied Kerberos descriptor,"The UI should ignore Kerberos identity references when setting the user-supplied Kerberos descriptor.  

For example, any Kerberos identity entry that contains a ""reference"" attribute, should not be added to the JSON data stored in the cluster artifact table, which represents the _user-supplied_ Kerberos descriptor. 

Currently, there are a few errors in the current process:

*Variables are being replaced*
{code}
{
  ""keytab"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-keytab-file"",
    ""file"": ""/etc/security/keytabs/spnego.service.keytab""
  },
  ""name"": ""mapreduce2_historyserver_spnego"",
  ""principal"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-principal"",
    ""local_username"": null,
    ""type"": null,
    ""value"": ""HTTP/_HOST@EXAMPLE.COM""
  },
  ""reference"": ""/spnego""
}
{code}

This should be 

{code}
{
  ""keytab"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-keytab-file"",
    ""file"": ""${keytab_dir}/spnego.service.keytab""
  },
  ""name"": ""mapreduce2_historyserver_spnego"",
  ""principal"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-principal"",
    ""local_username"": null,
    ""type"": null,
    ""value"": ""HTTP/_HOST@EXAMPLE.COM""
  },
  ""reference"": ""/spnego""
}
{code}

But really should be 

{code}
{
  ""keytab"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-keytab-file""
  },
  ""name"": ""mapreduce2_historyserver_spnego"",
  ""principal"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-principal""
  },
  ""reference"": ""/spnego""
}
{code}

*Incorrect variable replacement*
Some replacement issue has occurred where the keytab _file_ and the principal _name_ values have been swapped:

{code}
{
  ""keytab"": {
    ""configuration"": ""hive-site/hive.server2.authentication.spnego.keytab"",
    ""file"": ""HTTP/_HOST@EXAMPLE.COM""
  },
  ""name"": ""hive_hive_server_spnego"",
  ""principal"": {
    ""configuration"": ""hive-site/hive.server2.authentication.spnego.principal"",
    ""local_username"": null,
    ""type"": null,
    ""value"": ""/etc/security/keytabs/spnego.service.keytab""
  },
  ""reference"": ""/spnego""
},
{code}

And error that has resulted from this occurred while installing Hive into a cluster where Kerberos was enabled:
{noformat}
2018-04-18 19:30:24,557 - Failed to create principal, /etc/security/keytabs/spnego.service.keytab - Failed to create service principal for /etc/security/keytabs/spnego.service.keytab
STDOUT: Authenticating as principal admin/admin@EXAMPLE.COM with existing credentials.
Principal ""/etc/security/keytabs/spnego.service.keytab@EXAMPLE.COM"" created.

STDERR: WARNING: no policy specified for /etc/security/keytabs/spnego.service.keytab@EXAMPLE.COM; defaulting to no policy

Administration credentials NOT DESTROYED.
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-25 16:20:30,19
13155168,Installing Packages Fails Silently When Dependencies Are Not Resolved,"STR:

Stack installation not failing even when some problem happen with package installation

 

{code}
--> Running transaction check
---> Package hadoop_3_0_0_0_1245.x86_64 0:3.0.0.3.0.0.0-1245 will be installed
--> Processing Dependency: system-lsb for package: hadoop_3_0_0_0_1245-3.0.0.3.0.0.0-1245.x86_64
---> Package ranger_3_0_0_0_1245-hdfs-plugin.x86_64 0:1.0.0.3.0.0.0-1245 will be installed
---> Package ranger_3_0_0_0_1245-yarn-plugin.x86_64 0:1.0.0.3.0.0.0-1245 will be installed
---> Package spark2_3_0_0_0_1245-yarn-shuffle.noarch 0:2.3.0.3.0.0.0-1245 will be installed
--> Finished Dependency Resolution
Error: Package: hadoop_3_0_0_0_1245-3.0.0.3.0.0.0-1245.x86_64 (HDP-3.0-repo-51)
 Requires: system-lsb
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
{code}",pull-request-available,[],AMBARI,Bug,Blocker,2018-04-25 16:01:08,1
13155141,Remove unsecure dependencies from ambari-server,"* Remove dependency on org.springframework.security:spring-security-core 4.2.2.RELEASE in Ambari Server
* Remove dependency on org.springframework.ldap:spring-ldap-core 2.0.4.RELEASE in Ambari Server	
* Remove dependency on org.springframework.security:spring-security-config 4.2.2.RELEASE in Ambari Server
* Remove dependency on com.nimbusds:nimbus-jose-jwt 3.9 in Ambari Server
* Remove dependency on org.apache.ant:ant-launcher 1.7.1 in Ambari Server
* Remove dependency on org.springframework.security:spring-security-ldap 4.0.4.RELEASE in Ambari Server
* Remove dependency on org.springframework.security:spring-security-web 4.2.2.RELEASE in Ambari Server
* Remove dependency on com.jcraft:jsch 0.1.42 in Ambari Server
* Remove dependency on com.fasterxml.jackson.dataformat:jackson-dataformat-xml 2.4.5 in Ambari Server",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-25 14:20:25,18
13155124,Log Search UI: the UI does not load the initial data on logs screen loading,"When the log screen loads the app does not request the initial data from the server, so that the user can't see any information in the loaded page.",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-04-25 13:49:41,26
13155109,HDFS metrics page: list of namespaces is misaligned,"After enabling NameNode federation, the section with namespace-scoped widgets on HDFS metric page appears. Namespaces list is misaligned relatively to select control.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-25 12:59:37,15
13155099,Rename metric tables to support schema upgrade.,"METRIC_RECORD -> METRIC_RECORD_V2
METRIC_AGGREGATE -> METRIC_AGGREGATE_V2
and so on...",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-04-25 12:20:46,28
13155094,Deselect NFS Gateway (and Phoenix Query Server) by default in Ambari UI deployment,UI deployment via Ambari-2.6 has components NFSGateway & 'Phoenix Query Server' disabled (i.e. the checkbox is not checked) on 'Assign Slaves and Clients' page. In Ambari 2.7 these check boxes are active but they should not be.,pull-request-available,[],AMBARI,Bug,Critical,2018-04-25 11:55:21,27
13155076,dfs_ha_initial_* properties should be removed after installation,{{dfs_ha_initial_namenode_active}} and {{dfs_ha_initial_namenode_standby}} properties are not removed from {{hadoop-env}} config after installing a cluster with NameNode HA using blueprints.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-25 09:57:25,21
13155068,Ambari Metrics Service Check Fails Post Ambari Upgarde with error : Configuration parameter not found in configurations dictionary!,"In an unkerberized cluster , after ambari upgrade from 2.6.X to 2.7.0.0 , the
service check for Ambari Metrics fail with below error.

    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/service_check.py"", line 304, in <module>
        AMSServiceCheck().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/service_check.py"", line 170, in service_check
        import params
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/params.py"", line 119, in <module>
        'files', 'grafana-dashboards', stack_name))
      File ""/usr/lib64/python2.7/posixpath.py"", line 75, in join
        if b.startswith('/'):
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'service_package_folder' was not found in configurations dictionary!
",pull-request-available,[],AMBARI,Bug,Major,2018-04-25 09:15:09,5
13155064,Ensure all API Reference names have the first letter capitalized,"Things like Actions, Alerts, Blueprints, are all good, we just need to ensure the the following are capitalized:
 * hosts should be Hosts
 * services should be Services
 * settings should be Settings
 * clusters should be Clusters

Fix errors shown using swagger-ui (not shown in generated HTML file):
{code:java}
Semantic error at paths./services/{serviceName}/hosts
Declared path parameter ""serviceName"" needs to be defined as a path parameter at either the path or operation level
Jump to line 3312
Semantic error at paths./services/{serviceName}/hosts/{hostName}
Declared path parameter ""serviceName"" needs to be defined as a path parameter at either the path or operation level
Jump to line 3343
Semantic error at paths./views/{viewName}/versions/{version}/instances/{instanceName}/privileges
Declared path parameter ""version"" needs to be defined within every operation in the path (missing in ""delete""), or moved to the path-level parameters object
Jump to line 6267{code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-25 08:52:54,10
13155062,"Installing hive with ""New mysql database"" fails if mysql jar does not have exact name as shown on ui	",As a solution ambari-server script should accept any jar name that matches the regex of mysql jar name as it is when downloaded from the link shown on ui.,pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2018-04-25 08:43:30,55
13154988,Set appropriate s3a properties in core-site per best practice,Add S3-related properties to core-site.xml per best practice.,pull-request-available,['stacks'],AMBARI,Bug,Critical,2018-04-25 00:18:41,52
13154845,"In Select Target Host page as part of Move Master wizard, the host holding the component is not present in list of options","In Select Target Hosts page, the current host for the component is not shown in the list of available options for the component. This leads to a blank being shown in the combo box.

The current host was part of the options in earlier ambari versions.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-24 15:58:56,19
13154840,Ability to edit coordinators and bundles from dashboard view,"Today, when I go to the Workflow Manager, Dashboard view, and select a workflow I then get an ""Edit Workflow"" link on the right side to open the XML in the GUI viewer (and edit).

This however is not offered for Coordinators or Bundles and would be awesome to visualize to properties, time settings and the like.",ambari-views,['ambari-views'],AMBARI,Improvement,Major,2018-04-24 15:34:40,57
13154835,Available packages reader speed is to slow for ubuntu systems,"For ubuntu, where we reading full package list in the system, we need switch from {{ReaderStrategy.BufferedQueue}} to {{ReaderStrategy.BufferedChunks}} to make it faster

It should drop execution time from 1+ minute to around 10 seconds",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-24 15:11:52,1
13154811,UI Styling Is Incorrect On Upgrade Repositories Page,"Some of the following areas of the UI look a bit off WRT styling. 

- Upgrade repositories
-- Button size
-- Repository placement / size

- Upgrade History
- General styling and alignment",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-24 13:59:09,15
13154789,issue with service auto restart service categorization,"The service and component categorization are incorrect for e.x.

NameNode, Nodemanager and and Nimbus are under ""Ambari Metrics"".",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-24 12:38:49,19
13154765,Heatmaps page style changes,"See screenshots

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-24 11:14:11,19
13154651,Hive Summary page does not have the correct 'Go To View' links.,"- When user access the Hive Summary page in the ambari 7.0 UI and  click on the following ""Go To View"" links then they do not work properly and opens the Ambari UI home page instead of taking to the Hive View UI or the Tez View UI.

{code}
  HIVE VIEW 2.0   ""Go To View""
  DEBUG HIVE QUERY ""Go To View""
{code}

Screenshots are attached:
1. Before_Clicking_on_Go_To_View_Link.png 
2. When user clicks on the ""Go To View"" then we see ""Hive_View_Go_To_Link_Issue.png"". The the URL changes back to Ambari Dashboard.   http://$AMBARI_HOST:8080/#/main/dashboard/metrics
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-23 23:07:21,22
13154552,Ensure URLs to Ambari server resources are valid,"Ensure URLs to Ambari server resources are valid.  For example, fix URLs like 
{noformat}
http://ambari_server_host:8080/resources//DBConnectionVerification.jar
{noformat}
To
{noformat}
http://ambari_server_host:8080/resources/DBConnectionVerification.jar
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-23 18:08:10,30
13154512,Log Search UI: subscription error on route changing,Changing route from Shipper Configuration to Logs causes break of the application.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-04-23 15:36:51,26
13154503,Hosts topology can be incomplete for components install during blueprint deploy,Execution command should contain actual cluster topology information. Temporary fix.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-23 14:53:05,6
13154441,No widgets displayed on dashboard after adding third NameNode namespace,"After enabling NameNode federation and adding two additional namespaces, dashboards has no widgets displayed, with infinite spinner instead of them. Also, JS error is thrown: {{Uncaught TypeError: Cannot read property '1' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-23 10:38:11,15
13154434,Ambari-agent should handle delivery status responses from server,"This is required so that if connection between agent and server is lost, agent
knows what was lost and can resend it later.

",pull-request-available,[],AMBARI,Bug,Major,2018-04-23 10:05:35,5
13154433,NameNode namespaces aren't sorted by name,Namespaces should be displayed in alphabetical order.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-23 09:54:40,15
13154362,Install wizard issues,"When install wizard is running , reloading Ambari fails to load the page completely. 

Reason:
 Selected stack from VDF was not saved (or was saved but it is not present on response)

Stack creation:
 POST api/v1/version_definitions?dry_run=true
 Request body:
{code:java}
{""VersionDefinition"":{""version_url"":""http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1223/HDP-3.0.0.0-1223.xml""}}
{code}
Response:
 status: 201 created
 body:
{code:java}
{
  ""resources"" : [
    {
      ""href"" : ""http://104.196.80.21:8080/api/v1/version_definitions/"",
      ""operating_systems"" : [
        {
          ""OperatingSystems"" : {
            ""ambari_managed_repositories"" : true,
            ""os_type"" : ""redhat7"",
            ""stack_name"" : ""HDP"",
            ""stack_version"" : ""3.0""
          },
          ""repositories"" : [
            {
              ""Repositories"" : {
                ""base_url"" : ""http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1223"",
                ""os_type"" : ""redhat7"",
                ""repo_id"" : ""HDP-3.0"",
                ""repo_name"" : ""HDP"",
                ""distribution"" : null,
                ""components"" : null,
                ""stack_name"" : ""HDP"",
                ""stack_version"" : ""3.0"",
                ""tags"" : [ ]
              }
            },
            {
              ""Repositories"" : {
                ""base_url"" : ""http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1223"",
                ""os_type"" : ""redhat7"",
                ""repo_id"" : ""HDP-3.0-GPL"",
                ""repo_name"" : ""HDP-GPL"",
                ""distribution"" : null,
                ""components"" : null,
                ""stack_name"" : ""HDP"",
                ""stack_version"" : ""3.0"",
                ""tags"" : [
                  ""GPL""
                ]
              }
            },
            {
              ""Repositories"" : {
                ""base_url"" : ""http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7"",
                ""os_type"" : ""redhat7"",
                ""repo_id"" : ""HDP-UTILS-1.1.0.22"",
                ""repo_name"" : ""HDP-UTILS"",
                ""distribution"" : null,
                ""components"" : null,
                ""stack_name"" : ""HDP"",
                ""stack_version"" : ""3.0"",
                ""tags"" : [ ]
              }
            }
          ]
        }
      ],
      ""VersionDefinition"" : {
        ""display_name"" : ""HDP-3.0.0.0-1223"",
        ""id"" : null,
        ""max_jdk"" : ""1.8"",
        ""min_jdk"" : ""1.8"",
        ""repository_version"" : ""3.0.0.0-1223"",
        ""services"" : [
          {
            ""name"" : ""SQOOP"",
            ""versions"" : [
              {
                ""version"" : ""1.4.7"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Sqoop""
          },
          {
            ""name"" : ""DRUID"",
            ""versions"" : [
              {
                ""version"" : ""0.12.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Druid""
          },
          {
            ""name"" : ""OOZIE"",
            ""versions"" : [
              {
                ""version"" : ""4.2.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Oozie""
          },
          {
            ""name"" : ""MAPREDUCE2"",
            ""versions"" : [
              {
                ""version"" : ""3.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""MapReduce2""
          },
          {
            ""name"" : ""TEZ"",
            ""versions"" : [
              {
                ""version"" : ""0.9.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Tez""
          },
          {
            ""name"" : ""HDFS"",
            ""versions"" : [
              {
                ""version"" : ""3.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""HDFS""
          },
          {
            ""name"" : ""ACCUMULO"",
            ""versions"" : [
              {
                ""version"" : ""1.7.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Accumulo""
          },
          {
            ""name"" : ""ZOOKEEPER"",
            ""versions"" : [
              {
                ""version"" : ""3.4.6"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""ZooKeeper""
          },
          {
            ""name"" : ""HBASE"",
            ""versions"" : [
              {
                ""version"" : ""2.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""HBase""
          },
          {
            ""name"" : ""YARN"",
            ""versions"" : [
              {
                ""version"" : ""3.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""YARN""
          },
          {
            ""name"" : ""RANGER_KMS"",
            ""versions"" : [
              {
                ""version"" : ""1.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Ranger KMS""
          },
          {
            ""name"" : ""PIG"",
            ""versions"" : [
              {
                ""version"" : ""0.16.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Pig""
          },
          {
            ""name"" : ""KNOX"",
            ""versions"" : [
              {
                ""version"" : ""1.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Knox""
          },
          {
            ""name"" : ""ATLAS"",
            ""versions"" : [
              {
                ""version"" : ""1.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Atlas""
          },
          {
            ""name"" : ""RANGER"",
            ""versions"" : [
              {
                ""version"" : ""1.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Ranger""
          },
          {
            ""name"" : ""STORM"",
            ""versions"" : [
              {
                ""version"" : ""1.2.1"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Storm""
          },
          {
            ""name"" : ""HIVE"",
            ""versions"" : [
              {
                ""version"" : ""3.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Hive""
          },
          {
            ""name"" : ""ZEPPELIN"",
            ""versions"" : [
              {
                ""version"" : ""0.8.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Zeppelin Notebook""
          },
          {
            ""name"" : ""KAFKA"",
            ""versions"" : [
              {
                ""version"" : ""1.0.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Kafka""
          },
          {
            ""name"" : ""SPARK2"",
            ""versions"" : [
              {
                ""version"" : ""2.3.0"",
                ""components"" : [ ]
              }
            ],
            ""display_name"" : ""Spark2""
          }
        ],
        ""stack_name"" : ""HDP"",
        ""stack_services"" : [
          {
            ""name"" : ""SPARK2"",
            ""display_name"" : ""Spark2"",
            ""comment"" : ""Apache Spark 2.0 is a fast and general engine for large-scale data processing. This service is <b>Technical Preview</b>."",
            ""versions"" : [
              ""2.3.0""
            ]
          },
          {
            ""name"" : ""RANGER_KMS"",
            ""display_name"" : ""Ranger KMS"",
            ""comment"" : ""Key Management Server"",
            ""versions"" : [
              ""1.0.0""
            ]
          },
          {
            ""name"" : ""ATLAS"",
            ""display_name"" : ""Atlas"",
            ""comment"" : ""Atlas Metadata and Governance platform"",
            ""versions"" : [
              ""1.0.0""
            ]
          },
          {
            ""name"" : ""KAFKA"",
            ""display_name"" : ""Kafka"",
            ""comment"" : ""A high-throughput distributed messaging system"",
            ""versions"" : [
              ""1.0.0""
            ]
          },
          {
            ""name"" : ""SUPERSET"",
            ""display_name"" : ""Superset"",
            ""comment"" : ""Superset is a data exploration platform designed to be visual, intuitive and interactive."",
            ""versions"" : [
              ""0.15.0""
            ]
          },
          {
            ""name"" : ""TEZ"",
            ""display_name"" : ""Tez"",
            ""comment"" : ""Tez is the next generation Hadoop Query Processing framework written on top of YARN."",
            ""versions"" : [
              ""0.9.0""
            ]
          },
          {
            ""name"" : ""AMBARI_METRICS"",
            ""display_name"" : ""Ambari Metrics"",
            ""comment"" : ""A system for metrics collection that provides storage and retrieval capability for metrics collected from the cluster\n      "",
            ""versions"" : [
              ""0.1.0""
            ]
          },
          {
            ""name"" : ""ACCUMULO"",
            ""display_name"" : ""Accumulo"",
            ""comment"" : ""Robust, scalable, high performance distributed key/value store.\n      "",
            ""versions"" : [
              ""1.7.0""
            ]
          },
          {
            ""name"" : ""SMARTSENSE"",
            ""display_name"" : ""SmartSense"",
            ""comment"" : ""SmartSense - Hortonworks SmartSense Tool (HST) helps quickly gather configuration, metrics, logs from common HDP\n                services that aids to quickly troubleshoot support cases and receive cluster-specific recommendations.\n            "",
            ""versions"" : [
              ""1.5.0.2.7.0.0-362""
            ]
          },
          {
            ""name"" : ""SQOOP"",
            ""display_name"" : ""Sqoop"",
            ""comment"" : ""Tool for transferring bulk data between Apache Hadoop and\n        structured data stores such as relational databases\n      "",
            ""versions"" : [
              ""1.4.7""
            ]
          },
          {
            ""name"" : ""RANGER"",
            ""display_name"" : ""Ranger"",
            ""comment"" : ""Comprehensive security for Hadoop"",
            ""versions"" : [
              ""1.0.0""
            ]
          },
          {
            ""name"" : ""HDFS"",
            ""display_name"" : ""HDFS"",
            ""comment"" : ""Apache Hadoop Distributed File System"",
            ""versions"" : [
              ""3.0.0""
            ]
          },
          {
            ""name"" : ""OOZIE"",
            ""display_name"" : ""Oozie"",
            ""comment"" : ""System for workflow coordination and execution of Apache Hadoop jobs.  This also includes the installation of the optional Oozie Web Console which relies on and will install the <a target=\""_blank\"" href=\""http://www.sencha.com/legal/open-source-faq/\"">ExtJS</a> Library.\n      "",
            ""versions"" : [
              ""4.2.0""
            ]
          },
          {
            ""name"" : ""AMBARI_INFRA_SOLR"",
            ""display_name"" : ""Infra Solr"",
            ""comment"" : ""Core shared service used by Ambari managed components."",
            ""versions"" : [
              ""0.1.0""
            ]
          },
          {
            ""name"" : ""YARN"",
            ""display_name"" : ""YARN"",
            ""comment"" : ""Apache Hadoop NextGen MapReduce (YARN)"",
            ""versions"" : [
              ""3.0.0""
            ]
          },
          {
            ""name"" : ""MAPREDUCE2"",
            ""display_name"" : ""MapReduce2"",
            ""comment"" : ""Apache Hadoop NextGen MapReduce (YARN)"",
            ""versions"" : [
              ""3.0.0""
            ]
          },
          {
            ""name"" : ""ZEPPELIN"",
            ""display_name"" : ""Zeppelin Notebook"",
            ""comment"" : ""A web-based notebook that enables interactive data analytics. It enables you to\n        make beautiful data-driven, interactive and collaborative documents with SQL, Scala\n        and more.\n      "",
            ""versions"" : [
              ""0.8.0""
            ]
          },
          {
            ""name"" : ""ZOOKEEPER"",
            ""display_name"" : ""ZooKeeper"",
            ""comment"" : ""Centralized service which provides highly reliable distributed coordination"",
            ""versions"" : [
              ""3.4.6""
            ]
          },
          {
            ""name"" : ""DRUID"",
            ""display_name"" : ""Druid"",
            ""comment"" : ""A fast column-oriented distributed data store."",
            ""versions"" : [
              ""0.12.0""
            ]
          },
          {
            ""name"" : ""KERBEROS"",
            ""display_name"" : ""Kerberos"",
            ""comment"" : ""A computer network authentication protocol which works on\n        the basis of 'tickets' to allow nodes communicating over a\n        non-secure network to prove their identity to one another in a\n        secure manner.\n      "",
            ""versions"" : [
              ""1.10.3-30""
            ]
          },
          {
            ""name"" : ""KNOX"",
            ""display_name"" : ""Knox"",
            ""comment"" : ""Provides a single point of authentication and access for Apache Hadoop services in a cluster"",
            ""versions"" : [
              ""1.0.0""
            ]
          },
          {
            ""name"" : ""LOGSEARCH"",
            ""display_name"" : ""Log Search"",
            ""comment"" : ""Log aggregation, analysis, and visualization for Ambari managed services. This service is <b>Technical Preview</b>."",
            ""versions"" : [
              ""0.5.0""
            ]
          },
          {
            ""name"" : ""PIG"",
            ""display_name"" : ""Pig"",
            ""comment"" : ""Scripting platform for analyzing large datasets"",
            ""versions"" : [
              ""0.16.0""
            ]
          },
          {
            ""name"" : ""HIVE"",
            ""display_name"" : ""Hive"",
            ""comment"" : ""Data warehouse system for ad-hoc queries & analysis of large datasets and table & storage management service"",
            ""versions"" : [
              ""3.0.0""
            ]
          },
          {
            ""name"" : ""STORM"",
            ""display_name"" : ""Storm"",
            ""comment"" : ""Apache Hadoop Stream processing framework"",
            ""versions"" : [
              ""1.2.1""
            ]
          },
          {
            ""name"" : ""HBASE"",
            ""display_name"" : ""HBase"",
            ""comment"" : ""Non-relational distributed database and centralized service for configuration management &\n        synchronization\n      "",
            ""versions"" : [
              ""2.0.0""
            ]
          }
        ],
        ""stack_version"" : ""3.0"",
        ""type"" : ""STANDARD"",
        ""validation"" : [ ],
        ""release"" : {
          ""build"" : ""1223"",
          ""compatible_with"" : ""3\\.\\d+\\.\\d+\\.\\d+"",
          ""notes"" : ""http://example.com"",
          ""version"" : ""3.0.0.0""
        }
      }
    }
  ]
}
{code}
Request for getting stacks after creation:
 request:
 GET api/v1/version_definitions?fields=VersionDefinition/stack_default,VersionDefinition/stack_repo_update_link_exists,VersionDefinition/max_jdk,VersionDefinition/min_jdk,operating_systems/repositories/Repositories/*,operating_systems/OperatingSystems/*,VersionDefinition/stack_services,VersionDefinition/repository_version&VersionDefinition/show_available=true&VersionDefinition/stack_name=HDP

Response:
 Is too long, but it has no created stack",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-23 03:02:06,7
13154334,Ensure URLs to Ambari server resources are valid,"Ensure URLs to Ambari server resources are valid.  For example, fix URLs like 
{noformat}
http://ambari_server_host:8080/resources//DBConnectionVerification.jar
{noformat}
To
{noformat}
http://ambari_server_host:8080/resources/DBConnectionVerification.jar
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-22 18:44:01,30
13154218,Log Search UI: Various fixes for Shipper Configuration,"* Validation for the component name in test form should revalidate when the config JSON has changed
 * The navigation got extra two URL when the cluster and/or the service name is not provided. It should redirect directly to the right URL
 * Notifications styling should be better",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-04-21 11:26:48,26
13154004,Missing tooltip/indicator for unsupported services in Versions page when HDP-3.0 VDF is registered,"*STR*
# Deploy HDP-2.6.1.0 cluster with Ambari-2.6.x
# Upgrade Ambari to 2.7.0.0
# Register HDP-3.0 VDF
# Go to Version page and observe the small icon against services that are unsupported in 3.0 like Falcon, Flume, Spark, Mahout, Slider
# Hover mouse over them - it does not indicate any message

*Expected Result*
It would be good to say something like: This service is unsupported in the current version of the stack",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-20 11:31:40,15
13154002,Compatibility repository version wrong,"GET http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions

{code}

  ""href"" : ""http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions"",
  ""items"" : [
    {
      ""href"" : ""http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions/51"",
      ""CompatibleRepositoryVersions"" : {
        ""id"" : 51,
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""2.6"",
        ""upgrade_types"" : [
          ""NON_ROLLING"",
          ""HOST_ORDERED"",
          ""ROLLING""
        ]
      }
    },
    {
      ""href"" : ""http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions/1"",
      ""CompatibleRepositoryVersions"" : {
        ""id"" : 1,
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""3.0"",
        ""upgrade_types"" : [
          ""NON_ROLLING""
        ]
      }
    }
  ]
}
{code}

The version in the second href is incorrect.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-20 11:22:40,18
13153870,"Review calls GET, PUT, DELETE for Host components and make sure that we call them with ID as endpoint instead of name.","Given that Host Component APIs are Id based now to support multi-component instances [https://github.com/apache/ambari/pull/477/files], UI need to update the calls so that whenever Host Component API calls (GET, PUT, DELETE etc) are made, they should refer HOST COMPONENTS via their ID, wherever applicable.

Example:

The below calls breaks for component ZOOKEEPER_SERVER, when we do a PUT.
 - PUT http://<AmbariServer>:8080/api/v1/clusters/c1/hosts/<host_name>/host_components/ZOOKEEPER_SERVER?

 - Body :

{code:java}
{""RequestInfo"":{""context"":""Start ZooKeeper Server"",""operation_level"":{""level"":""HOST_COMPONENT"",""cluster_name"":""c1"",""host_name"":<host_name>,""service_name"":""ZOOKEEPER""}},""Body"":{""HostRoles"":{""state"":""STARTED""}}}
{code}
Thus, we need to visit all Host Component API calls from UI perspective.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-19 22:41:46,34
13153844,Enable Ambari SSO to be enabled without impacting other service sso configs,"*Scenario*
Ranger and Atlas are SSO enabled via BP deploys and Ambari is not SSO enabled. 
Later Ambari SSO has to be enabled without changing existing configs(so restart will not be required) for Atlas and Ranger. 
Now this is not possible with ""Enable for the selected services"" option. 
This was possible in previous versions but with the latest changes from AMBARI-23253, even if SSO was enabled for services earlier we still have to opt SSO for Ranger and Atlas in the list. When services are specified in the list, this would prompt for service restart.
So,
---If we enable SSO for Ambari and not the other services via the CLI, then any previous SSO setting for those services will be cleared
---If we enable SSO for Ambari and the other services via the CLI, then any previous SSO setting for those services will be potentially updated and this cause services to need to restart. But since data is the same no restart should be needed for those services

*Solution*
Add new prompts to separate Ambari's SSO configuration from the managed service's SSO configs so they can be managed separately:
* Use SSO for Ambari ({{--sso-enabled-ambari}})
* Manage SSO configurations for eligible services ({{--sso-manage-services}})

{noformat}
[root@c7401 ~]# ambari-server setup-sso --help
Using python  /usr/bin/python
Setting up SSO authentication properties...
Usage: ambari-server.py action [options]

Options:
  -h, --help            show this help message and exit
  -v, --verbose         Print verbose status messages
  -s, --silent          Silently accepts default prompt values. For db-cleanup
                        command, silent mode will stop ambari server.
  --sso-enabled=SSO_ENABLED
                        Indicates whether to enable/disable SSO
  --sso-enabled-ambari=SSO_ENABLED_AMBARI
                        Indicates whether to enable/disable SSO authentication
                        for Ambari, itself
  --sso-manage-services=SSO_MANAGE_SERVICES
                        Indicates whether Ambari should manage the SSO
                        configurations for specified services
  --sso-enabled-services=SSO_ENABLED_SERVICES
                        A comma separated list of services that are expected
                        to be configured for SSO (you are allowed to use '*'
                        to indicate ALL services)
  --sso-provider-url=SSO_PROVIDER_URL
                        The URL of SSO provider; this must be provided when
                        --sso-enabled is set to 'true'
  --sso-public-cert-file=SSO_PUBLIC_CERT_FILE
                        The path where the public certificate PEM is located;
                        this must be provided when --sso-enabled is set to
                        'true'
  --sso-jwt-cookie-name=SSO_JWT_COOKIE_NAME
                        The name of the JWT cookie
  --sso-jwt-audience-list=SSO_JWT_AUDIENCE_LIST
                        A comma separated list of JWT audience(s)
  --ambari-admin-username=AMBARI_ADMIN_USERNAME
                        Ambari administrator username for accessing Ambari's REST API
  --ambari-admin-password=AMBARI_ADMIN_PASSWORD
                        Ambari administrator password for accessing Ambari's REST API
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-19 21:16:58,30
13153780,Update Spring dependencies to fix CVE-2018-1270,"Spring dependencies should be updated to fix remote code execution vulnerabilities:
[https://pivotal.io/security/cve-2018-1270]
[https://pivotal.io/security/cve-2018-1275]",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-19 17:37:05,6
13153778,Progress bar is missing in UI during package install,For patch upgrades the mini install progress bar is missing,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-19 17:32:38,34
13153708,Additional realm config change not working,"1) go to https://<host_name>/#/main/admin/kerberos
2) click edit
3) add Additional Realms config in ambari kerberos config tab
4) click save
5) Click OK from the popup which ask for the restart of the components
In network tab we got this error:
Request URL:https://<host_name>/api/v1/clusters/<cluster_name>/artifacts/kerberos_descriptor
Request Method:PUT
Status Code:404 Not Found
{ ""status"" : 404, ""message"" : ""org.apache.ambari.server.controller.spi.NoSuchResourceException: The requested resource doesn't exist: Artifact not found, Artifacts/cluster_name=<cluster_name> AND Artifacts/artifact_name=kerberos_descriptor"" }",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-19 13:56:27,15
13153679,Enable NameNode HA fails if Ranger is set up with Ambari-managed Hive MySQL DB instance,"To repro:
* Install a cluster with Hive's database type ""New MySQL""
* Add Ranger via Add Service Wizard, and have its database point to the Hive's Ambari-managed MySQL database
* Attempt to enable HA via ""Enable NameNode HA Wizard""
This fails when Ranger is being started up, because the Ambari-managed MySQL server is down

To fix this, we can start the Ambari-managed MySQL server before starting Ranger",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-19 12:40:28,19
13153668,JWT cookie name and audiences not queried for during ambari-server setup-sso,"JWT cookie name and audiences not queried for during ambari-server setup-sso

Example:
{noformat}
[root@c7401 ~]# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Enter Ambari Admin login: admin
Enter Ambari Admin password:

SSO is currently not configured
Do you want to configure SSO authentication [y/n] (y)? y
Provider URL (https://knox.example.com:8443/gateway/knoxsso/api/v1/websso):https://c7401.ambari.apache.org:8443/gateway/knoxsso/api/v1/websso
Public Certificate PEM (empty line to finish input):
MIICVTCCAb6gAwIBAgIILf1Tx+q3QEMwDQYJKoZIhvcNAQEFBQAwbTELMAkGA1UE
...
6crsjbE33yYbJ1mZCpLGtM7mCj0liitItA==

Use SSO for all services [y/n] (n):
Use SSO for AMBARI [y/n] (y):
Use SSO for ATLAS [y/n] (y):
Use SSO for RANGER [y/n] (y): n
Ambari Server 'setup-sso' completed successfully.
{noformat}

The following queries are expected before setup is complete:
{noformat}
JWT Cookie name (hadoop-jwt):
JWT audiences list (comma-separated), empty for any ():
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-19 12:10:42,30
13153612,Ambari-server setup fails on Amazonlinux2,"Ambari server setup fails with the following error:

    
    
    
    Traceback (most recent call last):
      File ""/usr/sbin/ambari-server.py"", line 1054, in <module>
        mainBody()
      File ""/usr/sbin/ambari-server.py"", line 1024, in mainBody
        main(options, args, parser)
      File ""/usr/sbin/ambari-server.py"", line 974, in main
        action_obj.execute()
      File ""/usr/sbin/ambari-server.py"", line 79, in execute
        self.fn(*self.args, **self.kwargs)
      File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1199, in setup
        _setup_database(options)
      File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1005, in _setup_database
        dbmsAmbari = factory.create(options, properties, ""Ambari"")
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 511, in create
        dbmsConfig = desc.create_config(options, properties, dbId)
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 81, in create_config
        return self.fn_create_config(options, properties, self.storage_key, dbId)
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 884, in createPGConfig
        return PGConfig(options, properties, storage_type)
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 421, in __init__
        PGConfig.PG_HBA_DIR = get_postgre_hba_dir(OS_FAMILY)
      File ""/usr/lib/ambari-server/lib/ambari_server/utils.py"", line 268, in get_postgre_hba_dir
        get_pg_hba_init_files())
    OSError: [Errno 17] File exists
    

As I checked this function /usr/lib/ambari-server/lib/ambari_server/utils.py
get_postgre_hba_dir determines incorrectly the postgres data dir. If I change
it to return ""/var/lib/pgsql/9.5/data/"" the setup finishes successfully.
Verbose log attached. Used Ambari version: 2.7.0.0-330

",pull-request-available,[],AMBARI,Bug,Major,2018-04-19 07:28:37,5
13153600,Oozie - JDBC Driver class does not change when the Database type is changed,"In ambari 2.7.0.0-330, when the database type is changed for Oozie service, the JDBC server class does not change. This happens in the install wizard as well as post-install.

The behavior is different in Hive, where the JDBC server class changes when the database type changes
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-19 06:39:08,27
13153537,Requesting non-existing metric (including wildcard) to AMS gets HTTP 500,"Below links are throwing HTTP 500:

http://<host>:6188/ws/v1/timeline/metrics?appId=nimbus&hostname=&metricNames=topology.streamline-1-simple-yelp-review.3-SINK.%25.--fail-count.%25&startTime=1522734868701&endTime=1522736668701

http://<host>:3000/api/datasources/proxy/1/ws/v1/timeline/metrics?metricNames=topology.streamline-1-simple-yelp-review.%.--fail-count.%._avg&appId=nimbus&startTime=1522715768&endTime=1522737368&seriesAggregateFunction=sum

Please note that such kind of metrics are not in metadata, but I guess we are returning empty map here, not throwing HTTP 500. 
",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-04-19 00:01:23,39
13153486,Header missing in Step2 Select hosts page in NN Federation Wizard,Header missing in Step2 Select hosts page in NN Federation Wizard. All other pages have proper headers,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-18 21:12:05,15
13153412,Move NameNode wizard is stuck on Review step after enabling federation,"- Infinite spinner is displayed
- 'Next' button is disabled
- JS error is thrown: {{app.js:31501 Uncaught TypeError: Cannot read property 'indexOf' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-18 16:52:50,15
13153397,View instances not created by default,"In ambari-2.7.0, the view instances are not created by default. 

In previous versions, the instances of YARN, Files, Hive, Hive 2.0, Smartsense and Tez  were created automatically. ",pull-request-available,['ambari-views'],AMBARI,Bug,Blocker,2018-04-18 16:23:38,6
13153367,Add Service Group to various start/stop/restart API calls,"The UI exposes various Start, Stop, and Restart API calls which now must be updated to include Service Groups. The functions in scope for this issue are:

* Start/Stop All Services
* Restart All Stale Services
* Restart Service on Selected Hosts
* Restart Service (on all hosts)
* Restart Component (on specific Host)
* Restart Stale Components (on specific Host)
* Restart All Components (on specific Host)
* Configure Client (restart client component)
* Service Check

For these functions, in the body of the {{RequestInfo, resource_filters}} are used to actually target the intended recipients of the command (when not using a specific URL). Any time that the resource_filter contains a service_name, we will add service_group_name. If multiple service groups are in scope, then the resource_filter will be duplicated for each service group.

There are two exceptions to this pattern that are also in scope:
 * Start/Stop Service - these requests are made to the specific service endpoint, and the service group will be indicated in the URL
 * Start/Stop (Host) Component - these requests are made to the specific host component endpoint, and the specific component is targeted either by including its ID in the URL (which obviates the need to indicate the service group) or by including identifying information (including the service group) in the request body.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-18 14:36:56,40
13153361,Enable NameNode HA fails after Ambari Upgrade due to AttributeError,"STR

# Install a Ambari Cluster with Ambari 2.6.1
# Upgrade to Ambari 2.7
# Try to enable NameNode HA

Result:

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/journalnode.py"", line 143, in <module>
    JournalNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/journalnode.py"", line 39, in install
    import params
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py"", line 330, in <module>
    if hostname.lower() in nn_host.lower() or public_hostname.lower() in nn_host.lower():
AttributeError: 'NoneType' object has no attribute 'lower'
{noformat}

Problem:

{noformat:title=command*json}
""public_hostname"": null
{noformat}",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-04-18 14:21:44,21
13153341,Credential store is not working,"The command json element ""configuration_credentials"" are not present in the command jsons on the agents, though they are present on the server side command json (at least it is present in the json persisted at HostRoleCommand.constructExecutionCommandEntity(HostRoleCommand:239)). This causes that the credential store is not working, as this command json element is essential.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-18 12:57:16,28
13153314,Unable to update credentials of a remotely registered cluster,"*STR*
# Register a remote cluster with Ambari server
# Go to Admin - Remote Cluster page and select the cluster
# Hit 'Update Credentials' and supply new/existing admin credentials of remote cluster
# Hit Update

Result: Credentials are not updated and UI shows an error",pull-request-available,['ambari-admin'],AMBARI,Bug,Critical,2018-04-18 10:40:53,15
13153147,Lifecycle-Bulk: Components can be added even when ambari agent is down,Skip hosts that do not have heartbeat while adding components,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-17 21:34:18,34
13153094,Add Host wizard not working,Add Host wizard is broken due to changes in shared code with the Install wizard.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-17 18:53:19,40
13153076,ParseException line 1:116 mismatched input 'ROW' expecting INTO near ')' in table buckets specification when uploading table using Hive view 2.0.,"*Problem:*
When uploading table using CSV file as transactional table, below is the error see:
{code:java}
java.lang.Exception: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:116 mismatched input 'ROW' expecting INTO near ')' in table buckets specification

java.lang.Exception: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:116 mismatched input 'ROW' expecting INTO near ')' in table buckets specification
	at org.apache.ambari.view.hive20.resources.jobs.JobService.getOne(JobService.java:147)
	at sun.reflect.GeneratedMethodAccessor831.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
{code}
*Steps to replicate:*

1. Upload the attached file.
2. Select any of the columns as 'clustered'.
3. Under Advanced options, enable transactional and buckets=4.
4. Click on create table.",pull-request-available,['ambari-views'],AMBARI,Bug,Critical,2018-04-17 17:42:37,38
13153051,NN Federation Lower priority changes,"- The add metric in the HDFS Metrics needs more padding around it
- Create metric - because of menu depth the name of the metric is hard to see, we should wrap them and do anything to make sure the metric names are easily readable
- In the move master wizard we need to add the namespace before the word ""NameNode"", like ""ns1 NameNode""",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-17 16:02:26,15
13153049,NN Federation Wizard: Format NameNode fails due to stopped NameNodes,"NameNodes are stopped in first step of the flow. This prevents _Format NameNode_ step from getting clusterId of existing NameNodes.

NameNodes should be started, too, after JournalNodes.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-17 15:48:33,9
13153024,cluster name validation issue in ADMIN view,Cluster name is limited to 100 characters. Need to remove all another UI validation,pull-request-available,['ambari-admin'],AMBARI,Bug,Critical,2018-04-17 14:24:11,7
13153020,It is hectic whether or not a setting a property attribute in the service advisor gets into the recommendations,"Expected behavior of the field ""HA HSI namespace"":
When the switch is off, it should not be visible.
When the switch is on, and 1 host is selected, it should not be visible.
When the switch is on, and 2 hosts are selected then it should be visible.

Experienced behavior:
At first it is not visible. If I select two hosts, it is still not visible, though the service advisor sets the visibility attribute to true. When I discard it, it is visible, as for some reason the service advisor sees that there are two HSI hosts, though there are none, I've just discarded it.

I've added a message in the service advisor log starting ""Setting the visibily of hive.server2.active.passive.ha.registry.namespace to"" to make it easier to see what decision it made.
 ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-17 14:05:54,7
13153018,Config's tags should be cached,"UI often make calls to fetch current tags, which can be cached and updated on WebSocket event.

",pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-04-17 13:58:03,19
13152988,Ambari web should escape slash in config names,"Should escape property's name ""a/b"" => ""a\u002Fb"".

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-17 12:34:18,19
13152985,Ambari to replace HOSTGROUP variable in blueprints.,These variables are replaced with fqdn's in some cases and not in other cases.,pull-request-available,[],AMBARI,Bug,Critical,2018-04-17 12:20:42,20
13152979,LZO Libraries Are Not Installed Correctly During Upgrade,"*STR*
# Deployed cluster with Ambari version: 2.6.1.5-3 and HDP version: 2.6.1.0-129
# Upgrade Ambari to Target Version: 2.7.0.0-312, then upgrade AMS
# Delete unsupported services like Flume, Falcon
# Try Express Upgrade to HDP-3.0.0.0-1192
 
*Result*
Oozie server start failed
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/OOZIE/package/scripts/oozie_server.py"", line 154, in <module>
    OozieServer().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/OOZIE/package/scripts/oozie_server.py"", line 70, in configure
    oozie(is_server=True)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/OOZIE/package/scripts/oozie.py"", line 196, in oozie
    Execute(format('{sudo} cp {hadoop_lib_home}/hadoop-lzo*.jar {oozie_lib_dir}'),
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 308, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'ambari-sudo.sh cp /usr/hdp/3.0.0.0-1192/hadoop/lib/hadoop-lzo*.jar /usr/hdp/current/oozie-server' returned 1. cp: cannot stat '/usr/hdp/3.0.0.0-1192/hadoop/lib/hadoop-lzo*.jar': No such file or directory
{code}",pull-request-available,[],AMBARI,Bug,Blocker,2018-04-17 12:07:09,31
13152971,Logsearch UI: Fix Log Feeder Config issues,"* Change the name of the parameters.
 * Handle the server response correctly (it will return status 200 even if the validation is failed, but it contains errorMessage property in this case)
 * Fix the notifications.",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-04-17 11:45:49,26
13152786,Return mpack info even when attempting to register duplicate mpack,"When registering an mpack, if the mpack has already been registered, the server returns a 409. However, we also need the server to return the same data that it returns when the mpack is initially registered (most importantly, the registered mpack ID). This is required so that we can have the UI behave the same way – the operation needs to appear idempotent, even if the response code is a 409.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-16 18:02:59,56
13152757,NN Federation Wizard Changes,"There are a few required changes to the ""Add New HDFS Namespace"" wizard:
* Remove the ""If you have HBase running"" warning and replace it with a red warning similar to what we have for move name node that says: ""You should plan a cluster maintenance window and prepare for cluster downtime when adding a new HDFS Namespace as this Wizard will restart all services.
* During the wizard if you hit the Esc key you bounce completely out of the wizard, is there anyway we can prompt the user to say are you sure you want to exit this wizard if the user hits the Esc key?
* In the Configure Components step, we need to change the info text to: ""Please wait while your new HDFS Namespace is being deployed""
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-16 15:41:41,9
13152744,Ambari Schema Upgrade Failing source 2.6.1.0 target 2.7.0.0,"{code:title=ambari-server.log}
16 Apr 2018 11:04:47,676 INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD status VARCHAR2(255) NULL
16 Apr 2018 11:04:47,695 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.sql.SQLSyntaxErrorException: ORA-00904: ""PENDING"": invalid identifier

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)
 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)
 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)
 at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:227)
 at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531)
 at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:195)
 at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1036)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1336)
 at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1845)
 at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1810)
 at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:294)
 at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:829)
 at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:632)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateStageTable(UpgradeCatalog270.java:842)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:283)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
16 Apr 2018 11:04:47,695 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: ORA-00904: ""PENDING"": invalid identifier

at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.sql.SQLSyntaxErrorException: ORA-00904: ""PENDING"": invalid identifier

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)
 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)
 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)
 at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:227)
 at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531)
 at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:195)
 at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1036)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1336)
 at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1845)
 at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1810)
 at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:294)
 at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:829)
 at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:632)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateStageTable(UpgradeCatalog270.java:842)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:283)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 ... 1 more
{code}",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-04-16 14:48:05,21
13152679,Register VDF failing for AmazonLinux2 ,"
    
    -bash-4.2# curl -v -k -u admin:admin -H ""X-Requested-By:ambari"" -X POST http://ctr-e138-1518143905142-226218-01-000002.hwx.site:8080/api/v1/version_definitions -d '{ ""VersionDefinition"": { ""version_url"": ""http://s3.amazonaws.com/dev.hortonworks.com/HDP/amazonlinux2/3.x/BUILDS/3.0.0.0-1189/HDP-3.0.0.0-1189.xml"" } }'
    Note: Unnecessary use of -X or --request, POST is already inferred.
    *   Trying 172.27.80.4...
    * TCP_NODELAY set
    * Connected to ctr-e138-1518143905142-226218-01-000002.hwx.site (172.27.80.4) port 8080 (#0)
    * Server auth using Basic with user 'admin'
    > POST /api/v1/version_definitions HTTP/1.1
    > Host: ctr-e138-1518143905142-226218-01-000002.hwx.site:8080
    > Authorization: Basic YWRtaW46YWRtaW4=
    > User-Agent: curl/7.55.1
    > Accept: */*
    > X-Requested-By:ambari
    > Content-Length: 151
    > Content-Type: application/x-www-form-urlencoded
    >
    * upload completely sent off: 151 out of 151 bytes
    < HTTP/1.1 500 Internal Server Error
    < Date: Fri, 13 Apr 2018 10:09:15 GMT
    < X-Frame-Options: DENY
    < X-XSS-Protection: 1; mode=block
    < X-Content-Type-Options: nosniff
    < Cache-Control: no-store
    < Pragma: no-cache
    < X-Content-Type-Options: nosniff
    < X-Frame-Options: DENY
    < Set-Cookie: AMBARISESSIONID=node018hn2df5vsjqjuo7g1yr9mvno8.node0;Path=/;HttpOnly
    < Expires: Thu, 01 Jan 1970 00:00:00 GMT
    < User: admin
    < Content-Type: text/plain;charset=utf-8
    < Transfer-Encoding: chunked
    <
    {
      ""status"" : 500,
      ""message"" : ""An internal system exception occurred: Could not load url from http://s3.amazonaws.com/dev.hortonworks.com/HDP/amazonlinux2/3.x/BUILDS/3.0.0.0-1189/HDP-3.0.0.0-1189.xml.  null""
    * Connection #0 to host ctr-e138-1518143905142-226218-01-000002.hwx.site left intact
    }-bash-4.2#
    -bash-4.2#
    

Build Used:

    
    
    
    -bash-4.2# cat /etc/yum.repos.d/ambari.repo
    #VERSION_NUMBER=2.7.0.0-309
    [ambari-2.7.0.0-309]
    name=ambari Version - ambari-2.7.0.0-309
    baseurl=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-309
    gpgcheck=1
    gpgkey=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-309/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    enabled=1
    priority=1
    

",pull-request-available,[],AMBARI,Bug,Major,2018-04-16 10:22:33,5
13152421,Pre Upgrade checks look for kafka service validation even when the service is deleted,"STR
1. Install Kafka.
2. Delete kafka and try to upgrade.

Pre upgrades checks for kafka service validation without checking if it is even installed or not",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-13 21:36:49,1
13152390,UI reads service versions from base stack repo,After HDP upgrade to HDP-2.6.5.0 Kafka and Spark is supposed to have versions 1.0 and 2.3 as per the latest changes. But Ambari UI shows it incorrectly,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-13 19:35:50,34
13152387,Fix permissions of logsarch/logfeeder scripts for debian packages,"{code:java}
-su: /usr/lib/ambari-logsearch-portal/bin/logsearch.sh: Permission denied
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/LOGSEARCH/0.5.0/package/scripts/logsearch.py"", line 62, in <module>
    LogSearch().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/LOGSEARCH/0.5.0/package/scripts/logsearch.py"", line 44, in start
    user=params.logsearch_user
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in _init_
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 308, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/lib/ambari-logsearch-portal/bin/logsearch.sh start' returned 126.
-su: /usr/lib/ambari-logsearch-portal/bin/logsearch.sh: Permission denied
Attempt Count :1
logsearch start fail
{code}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-04-13 19:17:38,29
13152363,Failed to stop AMS components after Ambari-only upgrade,"STR:
# Install Ambari 2.6.x  with HDP 2.6 (AMS is the only important service)
# Enable Kerberos
# Upgrade to Ambari 2.7.0
# Stop/Restart AMS

{noformat:title=Metrics Monitor Stop}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_monitor.py"", line 72, in <module>
    AmsMonitor().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_monitor.py"", line 46, in stop
    import params
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/params.py"", line 347, in <module>
    kinit_cmd = '%s -kt %s %s' % (kinit_path_local, config['configurations']['ams-hbase-security-site']['ams.monitor.keytab'], config['configurations']['ams-hbase-security-site']['ams.monitor.principal'].replace('_HOST',_hostname_lowercase))
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'ams.monitor.principal' was not found in configurations dictionary!
{noformat}

Other AMS components failed to stop too. 

Update: The issue persists even after AMS is upgraded, hence bumped to blocker
",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-04-13 18:13:30,39
13152342,Ambari shows invalid passwords as plaintext,"While installing an HDF cluster, if you provide a password which is not valid for NiFi toolkit (eg. it is less than 12 characters) the password you entered is displayed in the error message as plaintext.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-13 16:55:25,15
13152313,NN Federation Service Pages Changes,"* In HDFS Service Summary, we need to remove the ""Service Metrics"" label, and remove the horizontal whitespace we've added between the service metrics  and the ns metrics.
* In the HDFS Metrics section, we need to add ""NAMESPACE"" to the left of the Namespace drop down menu.
* We need to also make sure we have the namespace label in the metric names, just like we did for the Dashboard HDFS metrics",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-13 15:11:29,15
13152311,Modify federation wizard steps to Stop All services and Start ZK and JN,"Restart JN with services up and running is not a good idea and can result in potential data loss.
Since we do not have server side ability to Stop All except ZK, here is new steps needed:

- Stop All Services
- Reconfigure
- Install NN
- Install ZKFC
- Start ZK
- Start JN
....",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-13 15:06:44,9
13152307,Support for auto-deployment of MPacks,"The Blueprint deployment code must be updated in order to support the automatic downloading and installation of management packs.  

If the user specifies a Management Pack (or set of Management Packs) in the Blueprint or Cluster Creation Template, and that MPack isn't already installed, the Blueprint processor must: 

1. Download the MPack's tarball at the URL location specified in the Blueprint or Cluster Creation Template. 

2. Install the MPack, using the Ambari Server APIs. 

3. Continue with the normal Blueprint deployment process.  ",pull-request-available,[],AMBARI,Bug,Major,2018-04-13 14:52:21,20
13152288,HDFS install fails at before-ANY hook due to no created instances,"HDFS install fails with Ambari 3.0.0.0-1527 and HDPCORE-1.0.0-b227 due to:
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py"", line 26, in hook
    import params
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/params.py"", line 227, in <module>
    components_instance_type=HADOOP_CLIENT_COMPONENT_TYPE)
ValueError: There are no created instances. Use create-mpack-instance command to add them.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-13 13:41:48,28
13152252,Navigation sub-menu style changes,"The additional top and bottom padding is for the sub-menu container should be added (5px).
There needs to be more padding between the Service name and the status indicator, they're too tight now

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-13 10:45:49,19
13152223,Hosts page: Combo filter should use hostname as default filter,"if a user enters a value without specifying category we should treat as a hostname (Hostname as default category).

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-13 09:04:27,19
13152097,Provide Mpack Package Name to Install in metainfo.xml,"In order to support parallel installations of mpacks on a single host, Ambari will be need to have the exact package name, including build number, of the mpack to install. This is a direct translation of what Ambari 2.x does when attempting to install multiple versions of a component.

In the case of Ambari 2.x, there was no way to provide the package information to Ambari beyond a wildcard, which caused problems if there were 2 builds close to each other on the host. Because of this (and because Ambari needed to discover the version being installed), we had to perform a lot of fragile trickery with yum/apt-get and {{hdp-select}}(and other similar tools).

The {{mpack.json}} file should provide this to us now. There are a few caveats:
 - If there are mutliple tarballs built (one for each OS), then we only need to provide the package name.
 - If there is only 1 tarball built, then the {{mpack.json}} must have a package name listing for every OS family. This is to allow for the differences between package managers (like hdpcore_3_0_0_0_b1 and hdpcore-3-0-0-0-b2).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-12 21:46:28,56
13152031,Restore configs does not work,"Steps:
 # Create custom property.
 # Restart all recommended services.
 # Roll back the version number to the one noted earlier.
 # Issue 1 - restart icon was not appeared.
 # Force restart required services.
 # Compare the difference in configs between current and previous version and notice the config has been removed as expected.
 # Issue 2 - check service's config file was not updated on hosts. ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-12 16:47:43,6
13152008,Start All Services Fails after ambari upgrade as Livy(Spark2) tries to access Namenode which is not yet started.,"STR
1) Install Ambari-2.6.0.0-267
2) Stop All Services 
3) Upgrade Ambari from 2.6.0.0-267 to 2.7.0.0-292
4) Upgrade Non Stack Services (Smartsense , Ambari Infra , Logsearch)
5) try to start all services.
Cause - Livy for Spark2 Server Start operation tries to access namenode whereas Namenode start hasn't happened yet. Looks to be some issue with RCO sequence of operations. Hence all other operations are aborted.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-12 15:02:06,46
13152001,Alerts Are Not Updated in the UI,"Alerts in the UI are not updated automatically. You must hard refresh the page or constantly navigate around to get them to show up correctly.

STR:
- Deploy a simple cluster, just with ZK. 
- Kill one of the ZK servers manually (or just stop it) and wait.
- The APIs will update with the newly triggered alert, but the UI will not

In some cases, navigating around will show inconsistent alerts:
- The alerts hidden under the alert icon never change without a hard refresh
- Visiting the page of an alert definition does show updated data, but it no longer changes to reflect the current state.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-12 14:35:23,19
13152000,Regenerate Keytabs After Ambari Upgrade leads to AMS start failure due to change in one core-site.xml config.,"1) Upgrade Ambari from 2.6.0.0-267 to 2.7.0.0-292
2) Upgrade Non Stack Services ( Smartsense , Ambari Infra , Logsearch)
3) Regenerate Keytabs 
At this step , hadoop.proxyuser.HTTP.hosts property in HDFS core-site undergoes a change. Due to this , AMS start fails with below error.


{code:java}
2018-04-10 19:34:41,668 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2801)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:64)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:87)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2816)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2799)
	... 5 more
Caused by: java.lang.IllegalArgumentException: Could not parse [${clusterHostInfo/webhcat_server_host|append(core-site/hadoop.proxyuser.HTTP.hosts]
	at org.apache.commons.net.util.SubnetUtils.calculate(SubnetUtils.java:220)
	at org.apache.commons.net.util.SubnetUtils.<init>(SubnetUtils.java:52)
	at org.apache.hadoop.util.MachineList.<init>(MachineList.java:109)
	at org.apache.hadoop.util.MachineList.<init>(MachineList.java:83)
	at org.apache.hadoop.util.MachineList.<init>(MachineList.java:75)
	at org.apache.hadoop.security.authorize.DefaultImpersonationProvider.init(DefaultImpersonationProvider.java:97)
	at org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration(ProxyUsers.java:75)
	at org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration(ProxyUsers.java:85)
	at org.apache.hadoop.hbase.security.HBasePolicyProvider.init(HBasePolicyProvider.java:54)
	at org.apache.hadoop.hbase.ipc.RpcServer.start(RpcServer.java:2090)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.start(RSRpcServices.java:1042)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:628)
	... 10 more
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-12 14:29:51,27
13151981,Ambari (HSI HA): Missing option to add another 'Hiveserver Interactive' component under actions menu for Active/Passive HSI.,Ambari (HSI HA): Missing option to add another 'Hiveserver Interactive' component under actions menu for Active/Passive HSI. ,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-12 13:29:28,7
13151975,Refactor base_alert to support multiple nameservices,"_NameNode Web UI_ alert is in critical state for all NameNodes with the
    
    
    
    [Alert][namenode_webui] HA nameservice value is present but there are no aliases for {{hdfs-site/dfs.ha.namenodes.ns1,ns2}}
    
",pull-request-available,[],AMBARI,Bug,Major,2018-04-12 13:25:03,5
13151974,Journalnode restart step is required when edits dir is changed,"Journal node does not create the edits directory in the right place because it is not restarted and hence does not load the new configurations resulting in the following error:

{code}
2018-04-11 23:19:39,835 FATAL namenode.FSEditLog (JournalSet.java:mapJournalsAndReportErrors(390)) - Error: recoverUnfinalizedSegments failed for required journal (JournalAndSt
ream(mgr=QJM to [10.240.0.23:8485, 10.240.0.32:8485, 10.240.0.95:8485], stream=null))
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:
10.240.0.23:8485: Journal Storage Directory root= /hadoop2/hdfs/journal/ns2; location= null not formatted
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkFormatted(Journal.java:500)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.getLastPromisedEpoch(Journal.java:262)
{code}

Need to add Restart All Journal nodes step right *before* the Format Namenode step in the wizard.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-12 13:21:54,9
13151926,Fix constraint issue during logfeeder log entry testing,"@NotBlank cannot be used for Map type, use @NotEmpty",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-04-12 10:59:46,29
13151919,NN federation: switching namespaces for dashboard widgets doesn't work,"After switching NN namespace for dashboard widgets nothing happens on UI, and also JS error is thrown:
{noformat}
Uncaught Error: assertion failed: Emptying a view in the inBuffer state is not allowed and should not happen under normal circumstances. Most likely there is a bug in your application. This may be due to excessive property change notifications.
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-12 10:42:21,15
13151884,"Unable to delete Slider after Ambari upgrade, due to Hive dependency","*STR*
 # Deploy HDP-2.6.3.0 with Ambari-2.6.0.0 (that stack includes services such as Slider, Mahout etc. which are unsupported in HDP-3.0 stack)
 # Upgrade Ambari to 2.7.0.0
 # Try to delete Slider (this is required since stack upgrade to HDP-3.0 needs the unsupported services to be removed)

*Result*
Slider prompts to delete Hive due to dependency

We need to remove the dependency, else asking customer to delete Hive may not be viable",express_upgrade pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-12 08:40:32,18
13151796,"Switch to using Surrogate PK in Ambari DB tables, wherever applicable.","The {{clusterservices}} table was given a new surrogate, auto-incrementing primary key:

{code}
@Entity
@TableGenerator(name = ""service_id_generator"",
  table = ""ambari_sequences"", pkColumnName = ""sequence_name"", valueColumnName = ""sequence_value""
  , pkColumnValue = ""service_id_seq""
  , initialValue = 1
)
{code}

However, the table doesn't use this for its PK. Instead, it combines it with 2 other columns. This would allow a single Service Group to be a part of 2 clusters and still be considered unique (which is incorrect). Compound PKs also present a problem in slower cloud-based databases as they can cause table locks on read which lead to deadlocks in the database:

{code}
CREATE TABLE clusterservices (
  id BIGINT NOT NULL,
  service_name VARCHAR(255) NOT NULL,
  service_type VARCHAR(255) NOT NULL,
  cluster_id BIGINT NOT NULL,
  service_group_id BIGINT NOT NULL,
  service_enabled INTEGER NOT NULL,
  CONSTRAINT PK_clusterservices PRIMARY KEY (id, service_group_id, cluster_id),
  CONSTRAINT UQ_service_id UNIQUE (id),
  CONSTRAINT FK_clusterservices_cluster_id FOREIGN KEY (service_group_id, cluster_id) REFERENCES servicegroups (id, cluster_id));
{code}

By not using the surrogate PK, we also cause other tables, like {{serviceconfig}} to have to create compound FKs as well:

{code}
  CONSTRAINT FK_serviceconfig_clstr_svc FOREIGN KEY (service_id, service_group_id, cluster_id) REFERENCES clusterservices (id, service_group_id, cluster_id),
{code}

This should just be a single FK to the surrogate ID.

Same for some other other tables, too, like {{servicegroups}}:

{code}
CREATE TABLE servicegroups (
  id BIGINT NOT NULL,
  service_group_name VARCHAR(255) NOT NULL,
  cluster_id BIGINT NOT NULL,
  CONSTRAINT PK_servicegroups PRIMARY KEY (id, cluster_id),
  CONSTRAINT FK_servicegroups_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id));
{code}

It uses a surrogate auto-incrementing ID, but it's PK is a compound.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-11 22:02:46,43
13151777,Ambari Metrics references outdated JARs,"GitHub PR builder is failing on some Jenkins nodes due to references to outdated Hadoop, etc. JARs that are no longer available.

{noformat}
[ERROR] Failed to execute goal on project ambari-metrics-timelineservice: Could not resolve dependencies for project org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.phoenix:phoenix-core:jar:5.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-common:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-annotations:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-common:jar:tests:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-common:jar:tests:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-common:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-api:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-server-common:jar:3.0.0.3.0.0.2-97, org.apache.phoenix:phoenix-core:jar:tests:5.0.0.3.0.0.2-97, org.apache.hbase:hbase-it:jar:tests:2.0.0.3.0.0.2-97, org.apache.hbase:hbase-testing-util:jar:2.0.0.3.0.0.2-97
{noformat}

https://builds.apache.org/view/A/view/Ambari/job/Ambari-Github-PullRequest-Builder/1744/console
https://builds.apache.org/view/A/view/Ambari/job/Ambari-Github-PullRequest-Builder/1791/console
etc.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-04-11 20:56:32,21
13151701,NameNode namespaces aren't displayed after HDFS page refresh,"*STR*
# Enable NameNode federation
# Go to HDFS section (summary or any other tab)
# Refresh page

*Expected result*
Namespaces data is displayed (sections on summary page, items of Service actions dropdown)

*Actual result*
- No namespaces data is displayed
- Service actions dropdown is empty
- JS error thrown: {{app.js:68529 Uncaught TypeError: Cannot read property 'contains' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-11 17:01:40,15
13151654,Comparing config versions style tweaks,"* Make current ""selected"" font color and button border color look incorrect
* Fix bottom padding
* See screenshots",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-11 13:45:00,19
13151644,UI Performance Tuning,"Since we've made a ton of change to the UI, I'm noticing some slowness in page load times and interaction. 
Specific Areas I'm noticing slowness in:
* Initial Ambari UI Page Load
* Initial Dashboard UI Page Load
* Service Actions Menu Loading
* Quicklinks Loading
* Service Configurations Tab Loading",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-11 13:06:36,19
13151629,Sometimes host checks never complete,"During the install wizard, the host checks that get run immediately after hosts are registered sometimes never complete.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-11 12:19:22,6
13151596,Not able to add a user in ambari,"1) login to ambari ui with admin:admin

2) select ADMIN -> Manage Ambari
 3) click on users section from the left pane
 4) click on Add users button
 5) fill in required details:

username: cloudbreak

password: admin

Add Roles For This user: None

is this admin user? : YES

Deactivate this user? : ACTIVE

 

6) click save

User creation error has been thrown (see screenshot)",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-11 09:34:23,6
13151588,Log Search UI: Log Index Filter notifications,# Add store result notification (success or failed) and handle errors.,pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-04-11 08:46:19,26
13151568,Fix TestCustomServiceOrchestrator.py and TestRegistration.py on branch-3.0-perf,"TestCustomServiceOchestrator requires rewriting since configs building routine
changed quite a lot on branch-3.0-perf

",pull-request-available,[],AMBARI,Bug,Major,2018-04-11 07:07:39,5
13151543,Log Search UI: refine log actions,"* Add alerts for Copy action
 * Histogram shouldn't be displayed in Open Log mode.
 * Disable the Host and Components dropdown
 * Context view: Fix buttons paddings. Create more user friendly layout.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-04-11 02:03:43,26
13151540,Log Search UI: service logs chart fixes,"# Chart should fit the parent container width on window resize
 # Bars overlap the Y axis
 # Change the background to white.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-04-11 01:30:36,26
13151464,Debian9: Atlas client installation failed due to unsupported OS family error,"Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 37, in <module>
    BeforeInstallHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 34, in hook
    install_packages()
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/shared_initialization.py"", line 37, in install_packages
    retry_count=params.agent_stack_retry_count)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 125, in __new__
    cls(names_list.pop(0), env, provider, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 119, in run_action
    provider = provider_class(resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 26, in __init__
    self._pkg_manager = ManagerFactory.get()
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/__init__.py"", line 46, in get
    cls.__repo_manager = cls.get_new_instance(OSCheck.get_os_family())
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/__init__.py"", line 71, in get_new_instance
    raise RuntimeError(""Not able to create Repository Manager object for unsupported OS family {0}"".format(os_family))
RuntimeError: Not able to create Repository Manager object for unsupported OS family debian
Attempt Count :1",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-04-10 19:08:46,18
13151403,Document enabling SSO via CLI,"Document enabling SSO via CLI

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-04-10 14:54:56,30
13151389,Utility function to get namenode logical hostname and namespace for a given hostname,The goal is to create a utility function to get namenode-logical name and namespace for a given hostname.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-10 13:58:17,35
13151388,Usability: Generate blueprint should download a single Zip file and not 2 files.,"During the installation wizard, just before deploy, the user has the opportunity to export a blueprint. This will download two files, but the behavior in the browser is odd as the user has to accept that multiple files will be downloaded. It would be better if it just downloaded a single zip file with both the blueprint and cluster creation template included in it. Much like our client configuration export.",pull-request-available,['ambari-web'],AMBARI,Improvement,Critical,2018-04-10 13:55:21,19
13151375,Slash not unescaped in property name,"After saving config with name ""a/b"" displayed as ""a\u002Fb"".",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-10 13:25:29,19
13151367,Missing unlimited JCE policy installation on ambari-agent side,"*STR:*
1. install Ambari 2.7.0.0 (build #220) server/agents on Centos7 machines (this issue is not OS dependent)
2. install an HDP 3.0 cluster with HDFS
3. check if unlimited JCE policy has been installed on the hosts where only ambari-agent has been installed:
{code}
[root@c7402 ~]# /usr/jdk64/jdk1.8.0_112/bin/java -jar /var/lib/ambari-agent/tools/jcepolicyinfo.jar -tu
{code}
*Result:*
{code}
[root@c7402 ~]# /usr/jdk64/jdk1.8.0_112/bin/java -jar /var/lib/ambari-agent/tools/jcepolicyinfo.jar -tu
Unlimited Key JCE Policy: false
{code}
*Expected result:*
{code}
[root@c7402 ~]# /usr/jdk64/jdk1.8.0_112/bin/java -jar /var/lib/ambari-agent/tools/jcepolicyinfo.jar -tu
Unlimited Key JCE Policy: true
{code}
*Additional information:*
unlimited JCE policy in agent side is triggered by a hook: ambari-server/src/main/resources/stack-hooks/before-START/scripts/hook.py:38 -> ambari-server/src/main/resources/stack-hooks/before-START/scripts/shared_initialization.py

In this Python code we check if we really need to install unlimited JCE:
{code}
  elif not params.unlimited_key_jce_required:
    Logger.debug(""Skipping unlimited key JCE policy check and setup since it is not required"")
{code}
This parameter is being set in params_linux.py:
{code}
unlimited_key_jce_required = default(""/componentLevelParams/unlimited_key_jce_required"", False)
{code}
However org/apache/ambari/server/controller/AmbariManagementControllerImpl.java:2590 sets it in the host level parameters (and not in component level params)",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-04-10 12:54:12,27
13151360,There is no alerts for logfeeder,"h1. There is no alerts for logfeeder.

when one logfeeder is down,there is no alert.",patch pull-request-available,['logsearch'],AMBARI,Bug,Major,2018-04-10 12:01:22,58
13151321,Fix mysql-collector-java installation for HIVE,"After mysql-collector-java was put back to the installed packages of hive it installs it only to the host of the database, whil",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-10 09:15:43,36
13151201,dfs.journalnode.edits.dir nameservice property being displayed UNDEFINED,Since it is non-editable and invalid wizard is not allowing to go to the next step.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-09 21:46:01,34
13151179,Investigate upgrade changes if any needed for federated NN cluster,Upgraded a 3.0.0 NN Federated cluster to a newer build.,pull-request-available,['ambari-upgrade'],AMBARI,Bug,Critical,2018-04-09 20:14:15,39
13151163,Removal of Repository Version State,"{{RepositoryVersionState}} is no longer used since Ambari does not track installation of a single repository anymore. This is because multiple mpacks may come from the state repository. The replacement, {{MpackInstallState}} was already introduced in AMBARI-23205.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-04-09 19:26:28,31
13151133,Install wizard summary shows wrong os/repo info,"The summary page of the install wizard always shows all OSes and their default repos, even if the user selects only some OSes and/or changes the repo locations. This problem stems from the mpack information being stored and retrieved in more than one place by the wizard. This needs to be consolidated.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-09 17:55:56,40
13151120,Ambari can not recognize slash in property name in xml file,"When specifying property name in XML files in Amabri code base, if the property name contains slash ('/'), the name will not be recognized by ambari and can not show up on ambari UI nor XML files. Here is how to reproduce this issue.

For instance, we add property name ""yarn.resource-types.yarn.io/gpu.maximum-allocation"" in resource-type.xml under ambari ""hdp_ambari_definitions"", we should suppose to get this property shown in XML file after we make this change. However, we can not see it. Similarly for UI, if we want this property to show up in UI section, we add a property in XML file and theme.json, again, we can not see this property either.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-09 16:56:17,34
13151072,Rearranging configuration file creation for Ranger Plugins.,"When enabling Ranger plug-ins for Storm, Knox and Kafka certain configuration files are created for the plug-in enabled components, need to re-arrange logic to create the configuration files to allow better flexibility.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-09 14:42:49,35
13151046,Log Search UI: various fixes for History feature,"# Implement handling of case when there's nothing to undo or redo
 # Refine styles and labels for History dropdown
 # Handle case of large history dropdown items count",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-04-09 13:45:07,26
13151019,Log Search UI: search box fixes,"# Apply the specified query on Enter press as well as on Search button click
 # Provide an ability to turn query parameter from exclude to include and vice versa (something like toggle button)
 # When user comes back to search box, it sometimes doesn't receive focus
 # Add ""clear all"" button to the search box",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-04-09 12:04:56,26
13150783,Format ZKFC fails while adding new HDFS namespace through UI in secure environment,"Tried to add new HDFS namespace with latest available build from yesterday
evening #262  
Format ZKFC is failing with below error

    
    
    
    18/04/05 16:30:10 INFO zookeeper.ClientCnxn: Session establishment complete on server ctr-e138-1518143905142-202626-01-000006.hwx.site/172.27.29.151:2181, sessionid = 0x26293f2ead15e74, negotiated timeout = 9000
    18/04/05 16:30:10 INFO ha.ActiveStandbyElector: Session connected.
    18/04/05 16:30:10 ERROR ha.ZKFailoverController: The failover controller encounters runtime error
    java.io.IOException: Couldn't create /hadoop-ha/ns1
    	at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:358)
    	at org.apache.hadoop.ha.ZKFailoverController.formatZK(ZKFailoverController.java:286)
    	at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:216)
    	at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:60)
    	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:175)
    	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:171)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:360)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1662)
    	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:480)
    	at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:171)
    	at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:195)
    Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hadoop-ha/ns1
    	at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
    	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
    	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
    	at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)
    	at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)
    	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)
    	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)
    	at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)
    	at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)
    	... 11 more
    18/04/05 16:30:11 INFO zookeeper.ZooKeeper: Session: 0x26293f2ead15e74 closed
    18/04/05 16:30:11 FATAL tools.DFSZKFailoverController: DFSZKFailOverController exiting due to earlier exception java.io.IOException: Couldn't create /hadoop-ha/ns1
    18/04/05 16:30:11 INFO zookeeper.ClientCnxn: EventThread shut down
    18/04/05 16:30:11 INFO util.ExitUtil: Exiting with status 1: java.io.IOException: Couldn't create /hadoop-ha/ns1
    18/04/05 16:30:11 INFO tools.DFSZKFailoverController: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down DFSZKFailoverController at ctr-e138-1518143905142-202626-01-000005.hwx.site/172.27.26.14
    ************************************************************/
    
",pull-request-available,[],AMBARI,Bug,Major,2018-04-07 15:26:39,5
13150730,"FluentPropertyBeanIntrospector from CLI operation log output, when running hdfs commands","Extra logger - ""FluentPropertyBeanIntrospector"" is seen while accessing Data Connectors. 
18/03/01 18:43:54 INFO beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.",pull-request-available,[],AMBARI,Bug,Major,2018-04-07 00:26:20,59
13150716,Fix the unhandled http request promise error,There is an unhandled promise rejection at app initialisation.,pull-request-available,['ambari-logsearch'],AMBARI,Bug,Critical,2018-04-06 22:03:22,26
13150714,Update execution_command module to handle data type conversion issue,Use expect() to do type conversion,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-06 21:42:45,44
13150694,Cluster version is not updated after stack upgrade ,"While testing a case where we upgrade, downgrade, then upgrade HDF stack, we noticed that the cluster version value was not updated (expected to go from HDF 3.1 to HDF 3.2).  Ambari did complete the upgrade process without error, and new components libraries were installed, however the newer service advisors for the stack were not executing and expected new configurations did not appear.  Upon further investigation we saw that after upgrade the cluster version had not been updated from HDF 3.1 to HDF 3.2",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-06 20:12:19,31
13150687,Pre-Upgrade Configuration Check for Kafka should only apply to HDP 2.6.5 -> HDP 2.6.x (Not HDF),{{KafkaPropertiesCheck}} check should be optional and executed only from upgradePack instructions,pull-request-available,['ambari-sever'],AMBARI,Bug,Blocker,2018-04-06 19:30:33,1
13150652,Fixes for NameNode widgets on dashboard,"- Make widgets sortable
- Fix Edit Delete features
- Make widgets able to be added
- Fix pie chart colors",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-06 17:12:00,15
13150620,Default shared edits location should be preserved,Currently the wizard asks for new locations for journalnode edits dirs. The location for ns1 should be the original one because that location has the edits already we cannot change that. In fact it should be greyed out.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-06 15:08:29,9
13150606,NN Federation Wizard state not restored after closing session,"Wizard should restore state after Ambari tab was closed and then opened again.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-06 14:30:46,9
13150603,Disable Add New HDFS Namespace option if cluster has no NN HA,If cluster doesn't have NameNode HA enabled there is no way to add new namespace via wizard.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-06 14:10:22,9
13150597,Incorrect servicerpc address configs being generated by NN Federation wizard.,It is seems that the servicerpc-address for nn1 points to nn2 host and vice versa.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-06 13:52:11,9
13150587,Decommission Datanode doesn't work,"On an Ambari270 cluster, try to decommission a datanode.
You could see bgop is triggerred and shows as sucsessful.
But Datanode seems to be still in started state and Decommission op is still available in dropdown.

The issue is caused by recent perf changes.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-06 13:09:25,28
13150579,Server should acknowledge api endpoint about host-component state before deletion,Server should send last state of removed host-component to api topology stomp endpoint.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-06 12:13:04,6
13150573,User information is missing in Upgrade wizard when checked in another session,"The user information is missing in 'Upgrade wizard' when opened from another session (see attached)
The information shows up fine for other wizards like enable NN HA",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-06 11:36:29,7
13150538,NN Federation Wizard: Bootstrap NameNode failed,"Bootstrap NameNode command fails due to stopped NameNode. Actually NameNode
should be started and command to start it was sent correctly from UI, but was
ignored by BE due to:

    
    
    Ignoring ServiceComponentHost as the current state matches the new desired state, clusterName=c, serviceName=HDFS, componentName=NAMENODE, hostname=c7404.ambari.apache.org, currentState=STARTED, newDesiredState=STARTED

This is because Format NameNode command starts NameNode for a little period of
time and then stops it back, but component's current state is not updated
immediately and BE thinks that NameNode is still started.  
From Format NameNode log:

    
    
    
    18/04/04 11:44:31 INFO namenode.NameNode: STARTUP_MSG: 
    /************************************************************
    STARTUP_MSG: Starting NameNode
    
    ...
    
    18/04/04 11:44:35 INFO namenode.NameNode: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down NameNode at c7404.ambari.apache.org/192.168.74.104
    

",pull-request-available,[],AMBARI,Bug,Major,2018-04-06 08:30:15,5
13150480,Move SSO-related properties from ambari.properties into the Ambari DB,"Move SSO-related properties from ambari.properties into the Ambari DB so that Ambari is able to use the SSO-related properties without needing to be restarted.

The following properties should be moved:

||Original Name||New Name||
|authentication.jwt.providerUrl|ambari.sso.provider.url|
|authentication.jwt.publicKey|ambari.sso.provider.publicKey|
|authentication.jwt.originalUrlParamName|ambari.sso.provider.originalUrlParamName|
|authentication.jwt.enabled|ambari.sso.authentication.enabled|
|authentication.jwt.audiences|ambari.sso.jwt.audiences|
|authentication.jwt.cookieName|ambari.sso.jwt.cookieName|
",pull-request-available sso,['ambari-server'],AMBARI,Task,Major,2018-04-06 00:09:00,30
13150449,Fix Kerberos service documentation for Ambari 2.6.x,Fix Kerberos service documentation for Ambari 2.6.x,pull-request-available,['ambari-server'],AMBARI,Task,Minor,2018-04-05 21:54:13,30
13150446,Update Kerberos service documentation for Ambari 2.7.0,Update Kerberos service documentation for Ambari 2.7.0,pull-request-available,['ambari-server'],AMBARI,Task,Minor,2018-04-05 21:45:20,30
13150406,NN Federation: service summary widgets should show correct metrics data,After enabling NameNode Federation HDFS service summary widgets should display correct data according to NameNode nameservice.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-05 18:48:06,9
13150405,Selected Hosts dropdown is disabled after navigating away and coming back to hosts page,"Selected Hosts dropdown is disabled after navigating away and coming back to hosts page
STR:
-> Go to hosts page and select some hosts
-> Now navigate away and come back to hosts page. Selected Hosts in the drop down is be disabled and does not show all the selected hosts",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-05 18:46:51,34
13150370,Error encountered while disabling Kerberos on HDF 3.2 using Ambari 2.7.0.0,"Ambari version is 2.7.0.0-182

1. I installed HDF 3.2 on an openstack using mpack
2. Enabled Kerberos which went okay and services were all restarted
3. Disabling Kerberos throws following exception

{code}
21 Mar 2018 23:10:08,794 ERROR [Server Action Executor Worker 266] AmbariJpaLocalTxnInterceptor:180 - [DETAILED ERROR] Rollback reason: 
Local Exception Stack: 
Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
Error Code: 0
Call: DELETE FROM kerberos_principal WHERE (principal_name = ?)
bind => [1 parameter bound]
at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1620)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:900)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:964)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:633)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1845)
at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4300)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5592)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:285)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:134)
at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:153)
at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)
at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)
at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$fe23de42.remove(<generated>)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:143)
at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:435)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:88)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
at java.lang.Thread.run(Thread.java:745)
Caused by: org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255)
at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:363)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:892)
... 22 more
21 Mar 2018 23:10:08,795 ERROR [Server Action Executor Worker 266] AmbariJpaLocalTxnInterceptor:188 - [DETAILED ERROR] Internal exception (1) :
org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255)
at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:363)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:892)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:964)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:633)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1845)
at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4300)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5592)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:285)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:134)
at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:153)
at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)
at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)
at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$fe23de42.remove(<generated>)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:143)
at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:435)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:88)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
at java.lang.Thread.run(Thread.java:745)
21 Mar 2018 23:10:08,795 WARN [Server Action Executor Worker 266] DestroyPrincipalsServerAction:173 - Failed to remove identity for amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM from the Ambari database - Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
Error Code: 0
Call: DELETE FROM kerberos_principal WHERE (principal_name = ?)
bind => [1 parameter bound]
21 Mar 2018 23:10:08,795 INFO [Server Action Executor Worker 266] DestroyPrincipalsServerAction:117 - Destroying identity, nimbus/priyank-3-2-1.openstacklocal@EXAMPLE.COM
21 Mar 2018 23:10:08,824 INFO [Server Action Executor Worker 266] DestroyPrincipalsServerAction:117 - Destroying identity, ambari-qa-cl1@EXAMPLE.COM
21 Mar 2018 23:10:08,864 INFO [Server Action Executor Worker 266] KerberosServerAction:456 - Processing identities completed.
{code}

This happens when the same principal is stored in multiple keytabs.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-05 16:07:58,18
13150337,YARN Cluster CPU Usage Graph Always Shows High CPU Usage,"h3. ISSUE
In Ambari, YARN's Cluster CPU widget always shows relatively high CPU usage, when NodeManager in a cluster is more than one.
 !image-2018-03-19-20-26-44-325.png|thumbnail! 
 !image-2018-03-19-20-27-19-160.png|thumbnail! 
(started another node at around 19:00)

h3. REPRODUCE STEPS
# Install a cluster with one NodeManager and AMS.
# Confirm ""Cluster CPU"" widget looks OK
# Add one more node with NodeManager, and wait for a while

h3. INVESTIGATION
AMS side looks OK
{code}
curl -s -k http://sandbox-hdp.hortonworks.com:6188/ws/v1/timeline/metrics -G --data-urlencode metricNames=cpu_idle._sum --data-urlencode appId=NODEMANAGER --data-urlencode startTime=1521454794 --data-urlencode endTime=1521455394 --data-urlencode precision=MINUTES 
...
{
    ""metrics"": [
        {
            ""appid"": ""nodemanager"",
            ""metadata"": {},
            ""metricname"": ""cpu_idle._sum"",
            ""metrics"": {
                ""1521454800000"": 198.99000000000001,
                ""1521455100000"": 192.56999999999999
            },
            ""starttime"": 1521454800000,
            ""timestamp"": 1521454800000
        }
    ]
}
{code}

But via Ambari, cpu_idle._sum becomes *{color:#d04437}100 times{color}* smaller
{code}
curl -s -k -u admin:admin http://sandbox-hdp.hortonworks.com:8080/api/v1/clusters/Sandbox/services/YARN/components/NODEMANAGER -G --data-urlencode 'fields=metrics/cpu/cpu_idle._sum[1521454950,1521455550,15]'
...(snip)...
  ""metrics"" : {
    ""cpu"" : {
      ""cpu_idle._sum"" : [
        [
          1.8686666666666667,
          1521454950
        ],
        [
          1.9843333333333333,
          1521454980
        ],
        [
          1.9,
          1521455010
        ],
        [
          1.9846666666666664,
          1521455040
        ],
        [
          1.8926666666666665,
          1521455070
        ],
...(snip)...
{code}

Somehow 'cpu_idle._sum' is always wrong for this Widget:

{code}
curl -s -k -u admin:admin http://sandbox-hdp.hortonworks.com:8080/api/v1/clusters/Sandbox/services/YARN/components/NODEMANAGER -G --data-urlencode 'fields=metrics/cpu/cpu_nice._sum[1521196167,1521199767,15],metrics/cpu/cpu_idle._avg[1521196167,1521199767,15],metrics/cpu/cpu_wio._sum[1521196167,1521199767,15],metrics/cpu/cpu_idle._sum[1521196167,1521199767,15],metrics/cpu/cpu_user._sum[1521196167,1521199767,15],metrics/cpu/cpu_system._sum[1521196167,1521199767,15]' -o ./ambari_NODEMANAGER_metrics.json

[root@sandbox-hdp ~]# grep -E -B1 '""cpu_|1521450000' ambari_NODEMANAGER_metrics.json | grep -vE -- '(--|\],)'
    ""cpu"" : {
      ""cpu_idle._avg"" : [
          85.54999999999998,
          1521199500
      ""cpu_idle._sum"" : [
          1.7109999999999996,     <<< need to multiply 100
          1521199500
      ""cpu_nice._sum"" : [
          0.0,
          1521199500
      ""cpu_system._sum"" : [
          21.900000000000002,
          1521199500
      ""cpu_user._sum"" : [
          6.666666666666666,
          1521199500
      ""cpu_wio._sum"" : [
          0.2,
          1521199500
{code}",pull-request-available,[],AMBARI,Bug,Major,2018-04-05 15:20:55,16
13150326,Utility function to parse initial active namenode hostnames for blueprint deployment,The goal of this task is to provide a utility function in the resource management libraries to parse initial active namenode hostnames from {{hadoop-env}} for blueprint deployment.  These properties may be specified by the user or be set by {{BlueprintConfigurationProcessor}} (AMBARI-23467).  The utility function can be used by stack scripts.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-04-05 15:00:57,21
13150305,Install wizard issues,"In the above cluster, when install wizard is running , reloading Ambari fails to load the page ",pull-request-available,[],AMBARI,Bug,Critical,2018-04-05 13:59:44,7
13150283,UI option to stop HDFS service doesnt work - JS Error,"HDFS Stop option from UI doesn't work. No operation is triggered

Below Js Error is seen in the console.
Ambari Build - 2.7.0.0-263

{code:java}
app.js:28031 Uncaught TypeError: Cannot read property 'set' of undefined
    at app.js:28031
    at Class.onPrimary (app.js:211716)
    at Class.newFunc [as onPrimary] (vendor.js:12954)
    at handler (vendor.js:31554)
    at HTMLButtonElement.<anonymous> (vendor.js:23346)
    at HTMLDivElement.dispatch (vendor.js:3178)
    at HTMLDivElement.elemData.handle (vendor.js:2854)
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-05 12:37:45,19
13150260,NameNode HA: QuickLinks section issues,"When NameNode HA is enabled, the QuickLink section shows the hostname of each NameNode as a hyperlink.  This is confusing since these are not meant to be clickable; they should be displayed as labels, not hyperlinks.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-05 10:19:10,15
13150212,"Ubuntu16: Install components exited with code '100' with message: debconf: delaying package configuration, since apt-utils is not installed","{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/infra_solr.py"", line 137, in <module>
 InfraSolr().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/infra_solr.py"", line 38, in install
 self.install_packages(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 810, in install_packages
 retry_count=agent_stack_retry_count)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 30, in action_install
 self._pkg_manager.install_package(package_name, self.__create_context())
 File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/apt_manager.py"", line 35, in wrapper
 return function_to_decorate(self, name, *args[2:])
 File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/apt_manager.py"", line 245, in install_package
 shell.repository_manager_executor(cmd, self.properties, context, env=self.properties.install_cmd_env)
 File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 743, in repository_manager_executor
 raise RuntimeError(message)
RuntimeError: Failed to execute command '['/usr/bin/apt-get', '-o', 'Dpkg::Options::=--force-confdef', '--allow-unauthenticated', '--assume-yes', 'install', u'ambari-infra-solr-client']', exited with code '100' with message: debconf: delaying package configuration, since apt-utils is not installed

W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/ambari-hdp-1.list:1 and /etc/apt/sources.list.d/hdp.list:2

W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/ambari-hdp-1.list:1 and /etc/apt/sources.list.d/hdp.list:2

W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/ambari-hdp-1.list:3 and /etc/apt/sources.list.d/hdp.list:3

W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/ambari-hdp-1.list:3 and /etc/apt/sources.list.d/hdp.list:3

E: Sub-process /usr/bin/dpkg returned an error code (2)

{code}",pull-request-available,['ambari-sever'],AMBARI,Bug,Blocker,2018-04-05 04:47:35,1
13150199,None of the Ambari View works because of  'X-Frame-Options' to 'deny' error,"In Ambari 2.7.0,  While accessing any View like HiveView 2.0 we see a blank page.
On te Browser Debugger Console we see the XFrame related error as following:

{code}
Refused to display 'http://latest1.example.com:8080/#/login?targetURI=/views/HIVE/2.0.0/Hive20/' in a frame because it set 'X-Frame-Options' to 'deny'.

Uncaught DOMException: Blocked a frame with origin ""http://latest1.example.com:8080"" from accessing a cross-origin frame.
    at Class.resizeFunction (http://latest1.example.com:8080/javascripts/app.js:237835:78)
    at http://latest1.example.com:8080/javascripts/app.js:237810:12
{code} ",pull-request-available,"['ambari-views', 'ambari-web']",AMBARI,Bug,Major,2018-04-05 02:26:22,6
13150191,HostCleanup.py script is failing with AttributeError: 'NoneType' object has no attribute 'get',"- While running HostCleanup.pu on Ambari 2.7.0 then it fails with the following error:
{code}
# /usr/lib/ambari-agent/lib/ambari_agent/HostCleanup.py  --silent --verbose
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_agent/HostCleanup.py"", line 710, in <module>
    main()
  File ""/usr/lib/ambari-agent/lib/ambari_agent/HostCleanup.py"", line 701, in main
    h.do_cleanup(propMap)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/HostCleanup.py"", line 144, in do_cleanup
    procList = proc_map.get(PROCESS_KEY)
AttributeError: 'NoneType' object has no attribute 'get'
{code}

- Used Python version is:
{code}
# python --version
Python 2.7.5
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-04-05 00:39:42,0
13150131,Blueprint configuration support for multiple NameNode HA deployments in a Federated cluster,"This new requirement was discovered while investigating the failures around Blueprint Deployments of HA NameNodes in a Federated cluster.  

In previous versions of Ambari (from Ambari 2.0 up to Ambari 2.6), HDFS NameNode HA deployments with Blueprints relied on some custom configuration in order to start up the Active and StandBy namenodes properly. In particular, we had to introduce the following configuration properties in the ""hadoop-env"" configuration type:
 # *dfs_ha_initial_namenode_active* - This property should contain the hostname for the “active” NameNode in this cluster.
 # *dfs_ha_initial_namenode_standby* - This property should contain the host name for the “passive” NameNode in this cluster.

These properties could be set by users to determine the initial state of the NameNode cluster, meaning which NameNode would be Active vs. Standby in the initial startup. This was required, since the startup commands for a NameNode in a Blueprint deployment, which occurs from scratch), are different for the Active and Standby cases. By default (the most common case), users did not set this property, and the BlueprintConfigurationProcessor would choose the Active and Standby nodes, and pass this information down to the ambari-agent via the properties listed above. The agents would then use this configuration to determine which NameNode commands to run on each node.

Based on information provided by [~swagle], 

in a new Federated cluster, there will be an HA deployment within each HDFS nameservice, consisting of a pair of Active/Standby nodes.

The current Blueprint configuration properties mentioned above assume a single nameservice, and so we'll need to introduce some new configuration in order to configure the ambari-agents to start each Active/Standby NameNode pair within each configured nameservice.

Since it appears to be possible to have an arbitrary number of nameservices defined, I propose that we add some new properties to ""hadoop_env"", with the express purpose of allowing either users or the Blueprint configuration processor (by default) to specify the set of Active and Standby NameNodes for the initial install:
 # *dfs_ha_initial_namenode_active_set* : A comma-separated list of Active Namenode hosts, across all known nameservices defined.
 # *dfs_ha_initial_namenode_standby_set*: A comma-separated list of Standy NameNode hosts, across all known nameservices defined.

There should be no intersections between these two sets, meaning that a host can only be listed in one of these properties. The Blueprint config processor should verify this, in the case that this property is customized by a user. Generally, this property should only be set by the BlueprintConfiguration processor, but for the sake of flexibility we should provide support for customization if the need arises.

Since we'd like to preserve backwards compatibility with the original Blueprint HA support, I think we should keep the original properties, and use them in the non-Federated case. When multiple HDFS nameservices are defined, then the new properties above should be used.

This JIRA tracks the work to properly configure these properties, and add them to the cluster configuration at deployment time. I'll file a separate companion JIRA to track the work required in the HDFS stack scripts to parse out the hostnames in these new properties, and use that information to determine the Active/Standby status of a host.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-04 20:03:20,51
13150118,the weather view example does not work?,"I was practicing the weather view example:

[https://github.com/apache/ambari/blob/trunk/ambari-views/examples/weather-view/docs/index.md]

I could only get the following (also shown in the attached picture), instead of a nice web page as described/shown in the above link.
h1. Directory: /views/WEATHER/1.0.0/EUROPE/
|[META-INF/ |http://127.0.0.1:8080/views/WEATHER/1.0.0/EUROPE/META-INF/]|4096 bytes |Mar 23, 2018 11:16:00 PM|
|[WEB-INF/ |http://127.0.0.1:8080/views/WEATHER/1.0.0/EUROPE/WEB-INF/]|4096 bytes |Mar 23, 2018 11:16:00 PM|
|[org/ |http://127.0.0.1:8080/views/WEATHER/1.0.0/EUROPE/org/]|4096 bytes |Mar 23, 2018 11:16:00 PM|
|[view.xml |http://127.0.0.1:8080/views/WEATHER/1.0.0/EUROPE/view.xml]|2782 bytes |Mar 23, 2018 11:16:00 PM|

 

Versions I used:

Apache Ambari version 2.6.0.0

openjdk version ""1.8.0_151""

Apache Maven 3.2.5 

 

Is there anything wrong with the versions I used? Or, there are other problems?

Thanks in advance for any help!

Yanyan",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-04-04 19:15:09,22
13150077,Server should support saving configs with names which contains slash,"PUT request to url: /api/v1/clusters/c1
Data:
{noformat}
[{""Clusters"":{""desired_config"":[{""type"":""zoo.cfg"",""properties"":{""autopurge.purgeInterval"":""24"",""autopurge.snapRetainCount"":""30"",""dataDir"":""/hadoop/zookeeper"",""tickTime"":""3000"",""initLimit"":""10"",""syncLimit"":""5"",""clientPort"":""2181"",""prop/my"":""1""},""service_config_version_note"":""""}]}}]
{noformat}
was successful, but property ""prop/my"" wasn't saved.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-04 17:06:41,6
13150065,NN federation related fixes for host details page,"- Fix colors for pie chart
- Add NameNode uptime widget
- Content of widgets isn't updated",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-04 16:28:57,15
13150036,Fix TestHeartbeatHandler.testComponents NPE,"This issue is intermittent:

{noformat}
ERROR] testComponents(org.apache.ambari.server.agent.TestHeartbeatHandler)  Time elapsed: 1.116 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.ambari.server.agent.TestHeartbeatHandler.testComponents(TestHeartbeatHandler.java:1351)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-04 15:19:37,30
13150015,ServiceAdvisor KeyError during kerberization of OneFS,"{code:java}
Error message: Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'onefs'

StdOut file: /var/run/ambari-server/stack-recommendations/5/stackadvisor.out

 

StdErr file: /var/run/ambari-server/stack-recommendations/5/stackadvisor.err

 

Stackadvisor.err shows the following traceback:

 

2018-04-03 16:09:09,711 ERROR HDP26StackAdvisor getCapacitySchedulerProperties: - Couldn't retrieve 'capacity-scheduler' from services.

Traceback (most recent call last):

  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 167, in <module>

    main(sys.argv)

  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main

    result = stackAdvisor.recommendConfigurations(services, hosts)

  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1565, in recommendConfigurations

    serviceAdvisor.getServiceConfigurationRecommendations(configurations, clusterSummary, services, hosts)

  File ""/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ONEFS/service_advisor.py"", line 112, in getServiceConfigurationRecommendations

    onefs_host = Uri.onefs(services)

  File ""/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ONEFS/service_advisor.py"", line 41, in onefs

    return self.from_config(configs, 'onefs', 'onefs_host')

  File ""/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ONEFS/service_advisor.py"", line 45, in from_config

    return Uri(configs['configurations'][config_type]['properties'][property_name])

KeyError: 'onefs'{code}
 ",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-04-04 14:15:15,18
13150012,Creating component instances shouldn't fail if instance already exists when called from the agent,"Reinstall operation fails because the component instance already exists.
ValueError: The instance /usr/hwx/instances/hdpcore/HDPCORE/default/zookeeper_client already exist. To change the version use set-mpack-instance command

Need to add fail_if_exists flag to instance creation command and  set it to False when called from mpack_manager_helper.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-04-04 14:02:21,28
13150008,Hive-server-interactive service needs to be started after Ranger-Admin.,"When {{Hive-Server-Interactive}} is enabled for Hive service and {{Ranger-Hive plugin}} is enabled, {{Hive-Server-Interactive}} is started before {{Ranger-Admin}} starts. This can cause start of Hive-Server-Interactive start with errors. Hence need to start {{Hive-Server-Interactive}} after {{Ranger-Admin}} starts.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-04 13:47:31,35
13149983,Improve parallel start performance,Reduce the amount of work performed while holding {{configs_lock}}.,pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-04-04 12:28:48,21
13149967,Agent can not to connect to server during blueprint deploy,"Blueprint deploy was hunged on hosts assignments.
ambari-agent.log:
INFO 2018-03-27 15:23:58,132 NetUtil.py:61 - Connecting to https://ctr-e138-1518143905142-164693-01-000002.hwx.site:8440/connection_info
INFO 2018-03-27 15:23:58,155 security.py:59 - Server require two-way SSL authentication. Use it instead of one-way...
INFO 2018-03-27 15:23:58,155 security.py:61 - Connecting to wss://ctr-e138-1518143905142-164693-01-000002.hwx.site:8441/agent/stomp/v1
INFO 2018-03-27 15:23:58,155 security.py:207 - Server certicate exists, ok
INFO 2018-03-27 15:23:58,155 security.py:215 - Agent key exists, ok
INFO 2018-03-27 15:23:58,156 security.py:223 - Agent certificate exists, ok
WARNING 2018-03-27 15:23:58,161 security.py:99 - Could not connect to wss://ctr-e138-1518143905142-164693-01-000002.hwx.site:8441/agent/stomp/v1. [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:661)
INFO 2018-03-27 15:23:58,161 HeartbeatThread.py:99 - Connection error ""[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:661)"". Re-running the registration",pull-request-available,['ambari-agent'],AMBARI,Task,Blocker,2018-04-04 11:35:20,28
13149939,Fix connection drop on ambari-agent by locking the write code,"This solution can possibly work to fix connection drop

    
    
    ERROR 2018-04-04 00:22:26,771 websocket.py:272 - Failed to receive data
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 394, in once
        b = self.sock.recv(self.reading_buffer_size)
      File ""/usr/lib64/python2.7/ssl.py"", line 759, in recv
        return self.read(buflen)
      File ""/usr/lib64/python2.7/ssl.py"", line 653, in read
        v = self._sslobj.read(len or 1024)
    SSLError: [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:1783)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-04-04 09:43:37,5
13149934,Refactor metrics_alert to support multiple nameservices,"The HDFS metrics based alerts defined in alert_metrics_deviation.py is agnostic of federated Namenodes.

Tasks:

When federation is enabled we should add an alert instance per nameservice for the HDFS metric based alerts.
Currently, we track metrics based alerts for:
RPC processing latency
RPC queue latency
NN Heap usage
There are multiple alerts based on whether service port is enabled and also different time windows. 
For every alert type, there should be an instance of the alert per nameservice.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-04 09:26:42,55
13149908,Allow Custom Hooks on a Per-Role Basis," Expand hooks with a  service-specific hooks. In addition, we must use custom command parameters to help with this. For examples:
{{before-KAFKA-INSTALL}}
{{after-STORM-START}}
{{after-KAFKA-RESTART}} <-- this is a custom command, not a ""regular"" command

 

The more-specific hooks should execute tightly to the command script. For example:
{{before-INSTALL}}
{{before-KAFKA-INSTALL}}
install script for Kafka
{{after-KAFKA-INSTALL}}
{{after-INSTALL}}",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Critical,2018-04-04 05:55:00,1
13149845,Load StackService mappings from registered mpacks,"Previously, StackService was being loaded with data from VDFs. Now that we are not using VDFs, we need to fill the StackService model from mpack data after the mpacks are registered. This will keep the data available where StackService is used.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-03 22:49:31,40
13149813,API Addition for Upgrade Plan for Config Changes,Add config changes per service group detail,pull-request-available,[],AMBARI,Task,Major,2018-04-03 19:02:01,33
13149811,404 error while setting up SSO on a new cluster ,"When setting up SSO via the Ambari server CLI using a new Ambari server instance, the following error is encountered:
{noformat}
[root@c7401 ~]# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Enter Ambari Admin login: admin
Enter Ambari Admin password:

Fetching SSO configuration from DB.ERROR: Exiting with exit code 1.
REASON: Error while fetching SSO configuration. Error details: HTTP Error 404: Not Found
{noformat}
This is caused by the lack of the {{sso-configuration}} category in the Ambari configuration set - which will be a common scenario for new Ambari clusters.
{noformat}
GET /api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/sso-configuration
{noformat}
{noformat}
{
  ""status"" : 404,
  ""message"" : ""The requested resource doesn't exist: RootServiceComponentConfiguration not found where Configuration/service_name=AMBARI AND Configuration/component_name=AMBARI_SERVER AND Configuration/category=sso-configuration.""
}
{noformat}
The CLI handle this response and assume the following default values:
{noformat}
ambari.sso.manage_services = false
ambari.sso.enabled_services = 
{noformat}
 ",SSO pull-request-available security,['ambari-server'],AMBARI,Bug,Critical,2018-04-03 18:49:22,27
13149802,Overflowing list of frozen hosts in Assign Slaves and Clients page,"When more than 10 hosts are present in a deployed cluster, the frozen hosts column in assign slaves and clients page overflows the boundary and is not scrollable. ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-03 18:10:11,34
13149782,Fix failing metrics unit tests on trunk.,"{code}
org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.source.RawMetricsSourceTest.testRawMetricsSourcedAtFlushInterval
org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.source.RawMetricsSourceTest.testRawMetricsCachedAndSourced

org.apache.ambari.server.controller.internal.StackDefinedPropertyProviderTest.testStackDefinedPropertyProviderAsClusterAdministrator
org.apache.ambari.server.controller.internal.StackDefinedPropertyProviderTest.testStackDefinedPropertyProviderAsAdministrator
org.apache.ambari.server.controller.internal.StackDefinedPropertyProviderTest.testStackDefinedPropertyProviderAsServiceAdministrator

org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testRbacForAMSPropertyProvider
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForSingleHostMetric
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForSingleHostMetricPointInTime
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForMultipleHostMetricscPointInTime
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForMultipleHostMetrics
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForRegexpMetrics
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForSingleComponentMetric
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateMetricsForEmbeddedHBase
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testAggregateFunctionForComponentMetrics
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testFilterOutOfBandMetricData
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForHostComponentHostMetrics
org.apache.ambari.server.controller.metrics.timeline.AMSPropertyProviderTest.testPopulateResourcesForHostComponentMetricsForMultipleHosts

org.apache.ambari.server.controller.metrics.timeline.AMSReportPropertyProviderTest.testPopulateResources
org.apache.ambari.server.controller.metrics.timeline.AMSReportPropertyProviderTest.testPopulateResourceWithAggregateFunction

org.apache.ambari.server.state.ServicePropertiesTest.validatePropertySchemaOfServiceXMLs
{code}",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-04-03 15:54:42,39
13149744,Update install wizard to use service groups,"There are a number of API calls used in the install wizard that currently have a \{defaultServiceGroupName} placeholder. These API calls need to be updated to be compatible with multiple service groups. Everywhere they are used, the code that uses them must also be updated to be multiple service group compatible.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-03 13:57:38,40
13149733,Fix stack issues in HDFS to support Namenode Federation setup,"For example, here are 2 things I found to be probably wrong.

1\. Journal node restart failed because we cannot find hdfs-site :
dfs.journalnode.edits.dir. We delete that property in the wizard. We may have
to change that to dfs.journalnode.edits.dir.&lt;nameservice&gt;

2\. The following snippet in params_linux.py on HDFS 3.0 stack seems wrong. It
has been designed to work with only 1 nameservice.

    
    
    
    dfs_ha_enabled = False
    dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.internal.nameservices', None)
    if dfs_ha_nameservices is None:
      dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.nameservices', None)
    dfs_ha_namenode_ids = default(format(""/configurations/hdfs-site/dfs.ha.namenodes.{dfs_ha_nameservices}""), None)
    

3\. After setting up NN Fed, when I restart namenodes, I see the following
error.

    
    
    
        main_resource.resource.security_enabled, main_resource.resource.logoutput)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 154, in __init__
        security_enabled, run_user)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/namenode_ha_utils.py"", line 204, in get_property_for_active_namenode
        if INADDR_ANY in value and rpc_key in hdfs_site:
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.https-address.ns2.nn1' was not found in configurations dictionary!
    

This is probably because the namenode_ha_utils is not equipped to handle
multiple nameservices.

We may have to create to fix such stack errors when the wizard is done.

",pull-request-available,[],AMBARI,Bug,Major,2018-04-03 12:10:51,5
13149730,Remove unnecessary properties from the Ranger SSO configuration updates via the stack advisor,"Remove unnecessary properties from the Ranger SSO configuration updates via the stack advisor

* {{ranger.sso.cookiename}}
* {{ranger.sso.query.param.originalurl}}",pull-request-available,['ambari-admin'],AMBARI,Task,Major,2018-04-03 12:07:42,30
13149723,Cannot save edited repository urls in Ambari Version UI,"When attempting to Save an edited Repository URL the browser reports the following error (Chrome):


{noformat}
vendor.js:26942 TypeError: Cannot read property 'primaryClass' of undefined
    at new $modal.open.controller (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/main.js:7972:41)
    at Object.instantiate (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:17529:14)
    at $controller (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:23350:28)
    at resolveSuccess (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:52701:32)
    at processQueue (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:29439:28)
    at http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:29455:27
    at Scope.$eval (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:30737:28)
    at Scope.$digest (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:30551:31)
    at Scope.$apply (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:30845:24)
    at done (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:24825:47) undefined
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-03 11:36:26,19
13149717,"After enabling Kerberos, the Ambari JAAS file is not updated","After enabling Kerberos, the Ambari JAAS file is not updated. This leads to various errors like collecting JXM data from services:
{noformat}
28 Mar 2018 15:40:29,041  WARN [ambari-metrics-retrieval-service-thread-4] RequestTargetAuthentication:88 - NEGOTIATE authentication error: No valid credentials provided (Mechanism level: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials fai
led! (null)))
28 Mar 2018 15:40:29,042 ERROR [ambari-metrics-retrieval-service-thread-4] AppCookieManager:122 - SPNego authentication failed, can not get hadoop.auth cookie for URL: http://c7401.ambari.apache.org:50070/jmx
28 Mar 2018 15:40:29,042 ERROR [ambari-metrics-retrieval-service-thread-5] AppCookieManager:122 - SPNego authentication failed, can not get hadoop.auth cookie for URL: http://c7401.ambari.apache.org:50070/jmx?get=Hadoop:service=NameNode,name=FSNamesystem::tag.HAState
2
{noformat}
The JAAS file as {{/etc/ambari-server/conf/krb5JAASLogin.conf}} is expected to be updated to match the created Kerberos identity for the Ambari server, but is not:

The default values of
{noformat}
    ...
    keyTab=""/etc/security/keytabs/ambari.keytab""
    principal=""ambari@EXAMPLE.COM""
    ...
{noformat}
Should have been changed to
{noformat}
    ...
    keyTab=""/etc/security/keytabs/ambari.server.keytab""
    principal=""ambari-server-c1@EXAMPLE.COM""
    ...
{noformat}
After manually fixing this and restarting Ambari, the JMX requests authenticated properly.",kerberos pull-request-available security,['ambari-server'],AMBARI,Bug,Critical,2018-04-03 11:09:45,27
13149679,Not able to install METRICS_MONITOR,"ambari version - 2.7.0.0-246

metrics monitor installation failed due to below error

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_monitor.py"", line 72, in <module>
    AmsMonitor().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_monitor.py"", line 28, in install
    import params
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/params.py"", line 376, in <module>
    if hbase_wal_dir and re.search(""^file://|/"", hbase_wal_dir): #If wal dir is on local file system, create it.
NameError: name 're' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_monitor.py"", line 72, in <module>
    AmsMonitor().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_monitor.py"", line 28, in install
    import params
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/params.py"", line 376, in <module>
    if hbase_wal_dir and re.search(""^file://|/"", hbase_wal_dir): #If wal dir is on local file system, create it.
NameError: name 're' is not defined
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-03 07:16:58,32
13149614,Metrics Collector stops after starting,"{code:title=ambari-metrics-collector.out }
Mar 28, 2018 8:18:10 AM java.util.logging.LogManager$RootLogger log
SEVERE: Failed to resolve default logging config file: config/java.util.logging.properties
java.lang.NumberFormatException
        at java.math.BigDecimal.<init>(BigDecimal.java:494)
        at java.math.BigDecimal.<init>(BigDecimal.java:383)
        at java.math.BigDecimal.<init>(BigDecimal.java:806)
        at java.math.BigDecimal.valueOf(BigDecimal.java:1274)
        at org.apache.phoenix.schema.types.PDouble.getMaxLength(PDouble.java:79)
        at org.apache.phoenix.expression.LiteralExpression.newConstant(LiteralExpression.java:211)
        at org.apache.phoenix.expression.LiteralExpression.newConstant(LiteralExpression.java:174)
        at org.apache.phoenix.expression.LiteralExpression.newConstant(LiteralExpression.java:161)
        at org.apache.phoenix.compile.UpsertCompiler$UpdateColumnCompiler.visit(UpsertCompiler.java:871)
        at org.apache.phoenix.compile.UpsertCompiler$UpdateColumnCompiler.visit(UpsertCompiler.java:855)
        at org.apache.phoenix.parse.BindParseNode.accept(BindParseNode.java:47)
        at org.apache.phoenix.compile.UpsertCompiler.compile(UpsertCompiler.java:744)
        at org.apache.phoenix.jdbc.PhoenixStatement$ExecutableUpsertStatement.compilePlan(PhoenixStatement.java:772)
        at org.apache.phoenix.jdbc.PhoenixStatement$ExecutableUpsertStatement.compilePlan(PhoenixStatement.java:758)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:389)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:379)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:378)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:366)
        at org.apache.phoenix.jdbc.PhoenixPreparedStatement.executeUpdate(PhoenixPreparedStatement.java:199)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.PhoenixHBaseAccessor.commitMetrics(PhoenixHBaseAccessor.java:355)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.PhoenixHBaseAccessor.commitMetricsFromCache(PhoenixHBaseAccessor.java:305)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.MetricsCacheCommitterThread.run(MetricsCacheCommitterThread.java:35)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}
",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-04-02 23:05:25,39
13149585,Unable to parse task structured output when disabling Kerberos,"The following error is seen in the log when disabling Kerberos:

{noformat}
Unable to parse task structured output: /var/lib/ambari-agent/data/structured-out-5.json
{noformat}

This appears to be caused by an empty value being returned from {{resource_management.libraries.script.script.Script#disable_security}}.  Rather then returning an empty value ({{None}}), return {{""\{\}""}} instead. 

",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-04-02 21:01:15,30
13149573,Remove desired_stack from POST /service call,"Remove ""desired_stack"" from the call that creates a service during installation. With the introduction of mpacks, we are no longer on a stack-centric model, so the ""desired_stack"" concept no longer makes sense.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-02 20:26:23,40
13149548,Need to add new property for Ranger-Tagsync when enabling federation for Namenode-HA via UI wizard,"Need to dynamically add properties in config-type {{ranger-tagsync-site}} from UI for Federation wizard. This should be done when Atlas and Ranger-Tagsync service is installed.
When federation is Enabled for  HDFS, in this case for each name-service, we will need to add a property in config-type {{ranger-tagsync-site}} using below format:
{noformat}ranger.tagsync.atlas.hdfs.instance.<cluster-name>.nameservice.<name-service>.ranger.service= <ranger service-name>{noformat}

The Ranger service name can be derived from logic as below:
If the value for property {{ranger.plugin.hdfs.service.name}} is default i.e. {{{{repo_name}}}} then the value for above property will be in the format: {noformat}<cluster-name>_hadoop_<name-service>{noformat}
for e.g if the cluster-name is {{testcl1}} and the name-services are {{ns1}} , {{ns2}} and the {{ranger.plugin.hdfs.service.name}} has value {{{{repo_name}}}}  then the property-value will be as below:
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns1.ranger.service=testcl1_hadoop_ns1
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns2.ranger.service=testcl1_hadoop_ns2

If the user has a custom-value for the property {{ranger.plugin.hdfs.service.name}} i.e. not {{{{repo_name}}}}, then we will need to use the value provided in for property {{ranger.plugin.hdfs.service.name}} and use the format as <custom-repo-name>_<name-service>.
for e.g if the cluster-name is {{testcl1}} and the name-services are {{ns1}} , {{ns2}} and the {{ranger.plugin.hdfs.service.name}} has value {{hadoop_service}}  then the property-value will be as below:
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns1.ranger.service=hadoop_service_ns1
ranger.tagsync.atlas.hdfs.instance.testcl1.nameservice.ns2.ranger.service=hadoop_service_ns2",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-02 18:46:53,9
13149525,JS error while deploying new service,"{noformat}
""<DS.StateManager:ember27929> could not respond to event setProperty in state rootState.loading.""
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-02 16:47:33,19
13149469,Dashboard: add Yarn Containers widget,"Title: ""YARN containers""
Content:   <#allocated>/<#pending>/<#reserved>",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-02 11:29:25,19
13149371,Add solr-to-solr archive operation for solrDataManager.py,"Add solr to solr archive/save option for solrDataManager.py
new fields: 
- `solr-output-collection`: the collection where we will archive data
- `exclude-fields`: exclude fields from solr response (at least _version_ is required)
example:
{code:java}
infra-solr-data-manager -m archive -v -c old_ranger_audits -s http://c7402.ambari.apache.org:8886/solr -z none -r 10000 -w 100000 -f evtTime -e 2018-03-31T01:00:00.00Z --solr-output-collection ranger_audits -k /etc/security/keytabs/ambari-infra-solr.service.keytab -n infra-solr/$(hostname -f) --exclude-fields _version_,_ttl_,_expire_at_
{code}
",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-04-01 16:19:26,29
13149305,Refine pre-check message to remove unsupported services before upgrade to HDP-3.0,"Services such as Falcon. Flume, Mahout, Slider would be removed from HDP-3.0 stack. The pre-check message to remove them should be refined to say that the services would be removed and cannot be re-installed after upgrade",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Major,2018-03-31 14:48:11,46
13149304,Refine pre-check message to remove unsupported services before upgrade to HDP-3.0,"Services such as Falcon. Flume, Mahout, Slider would be removed from HDP-3.0 stack. The pre-check message to remove them should be refined to say that the services would be removed and cannot be re-installed after upgrade",upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-03-31 14:47:58,46
13149186,Fix and cleanup ClientConfigResourceProviderTest and TestHeartbeatHandler,"Fix and cleanup ClientConfigResourceProviderTest.  At least the following error occurs when running this unit test:

{noformat:title=org.apache.ambari.server.controller.internal.ClientConfigResourceProviderTest#testGetResources}
java.lang.AssertionError: 
  Unexpected method call Configuration.getExternalScriptThreadPoolSize():
    Configuration.getResourceDirPath(): expected: 1, actual: 1
    Configuration.getJavaHome(): expected: 1, actual: 0
{noformat}

Fix and cleanup TestHeartbeatHandler.  At least the following error occurs when running this unit test:

{noformat:title=org.apache.ambari.server.agent.TestHeartbeatHandler#testComponents}
 TestHeartbeatHandler.testComponents:1351 » NullPointer
{noformat}



",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-30 15:33:43,30
13149169,Upgrade Requires a Pre-Upgrade Configuration Validation Check for Kafka,"kafka-broker have two new properties:

- inter.broker.protocol.version
- log.message.format.version

These configurations must exist and have a value before proceeding with upgrade (the value for both should be aligned with the Kafka version on the current stack). We need validation at minimum to check for the existence of these parameters . Additional validation includes the following

For inter.broker.protocol.version :
value is not empty
value is set to current Kafka version in the stack (e.g. for HDP 2.6.x value should be 0.10.1, HDP 2.5.x should be 0.10.0, HDP 2.3x - 2.4x should be 0.9.0)

For log.message.format.version:
value is not empty (version can vary from current stack version)",pull-request-available,[],AMBARI,Task,Blocker,2018-03-30 14:11:08,1
13149166,Background Operations: icons have incorrect color,See screenshot,pull-request-available,['ambari-web'],AMBARI,Bug,Minor,2018-03-30 13:55:22,19
13149165,Remove unwanted trailing slash at the end of hbase_pid_dir path in ams-hbase-env ,"- While starting the AMS collector we can see that it has two slash in the PID file path ""/var/run/ambari-metrics-collector{color:red}//{color}hbase-ams-regionserver.pid"" . Also ""/var/run/ambari-metrics-collector{color:red}//{color}hbase-ams-master.pid""  . which does not look good and seems error prone. It should be removed. 

{code}
Execute['/usr/lib/ams-hbase/bin/hbase-daemon.sh --config /etc/ams-hbase/conf stop regionserver'] {'on_timeout': 'ls /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid >/dev/null 2>&1 && ps `cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid` >/dev/null 2>&1 && ambari-sudo.sh -H -E kill -9 `ambari-sudo.sh cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid`', 'timeout': 30, 'user': 'ams'}
{code}",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-03-30 13:47:54,22
13149155,Refreshing Ambari Dashboard or HDFS Dashboard shows console error Cannot read property 'hosts' of undefined,"In a NameNode HA enabled cluster, When we refresh the Ambari UI page then on next time refresh we see the following error on the Browser console :

{code}
Uncaught TypeError: Cannot read property 'hosts' of undefined
    at app.js:213145
    at Array.filter (<anonymous>)
    at app.js:213144
    at Array.map (<anonymous>)
    at Class.setMultipleGroupLinks (app.js:213143)
    at Class.setQuickLinksSuccessCallback (app.js:212862)
    at Class.opt.success (app.js:180150)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
{code}

This happens because the expression ""this.get('shouldSetGroupedLinks')"" returns ""false"" first time but next time it returns ""true"" but the possibility is that the expression ""this.get('masterGroups.length')"". is still returns ""false"". (means the ""masterGroups"" array is still having no or one element).",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2018-03-30 12:33:32,22
13149151,Moving Hive Metastore gets hanged in Ambari due to Uncaught TypeError: Cannot read property 'tag' of undefined,"In Ambari 2.7, when we try to move the Hive Metastore to a new host then at Wizard Step-3 it hangs. When we look at the  ambari UI Browser Developer Console then we notice the following error:
{code}
app.js:31183 Uncaught TypeError: Cannot read property 'tag' of undefined
    at app.js:31183
    at Array.forEach (<anonymous>)
    at Class.getConfigUrlParams (app.js:31182)
    at Class.onLoadConfigsTags (app.js:31167)
    at Class.opt.success (app.js:180694)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}

On further debugging it is found that when the site = webhcat-site then it shows this error because this site is not present inside the desired_configs. Hence there should be a check before attempting to read it's tag information.

*Steps to Reproduce the issue:*
- Install Hive Service to the cluster.
- Now attempt to move the Hive MetaStore to another host using ambari UI.

",ambari-web pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2018-03-30 11:43:56,22
13149134,Invalid websocket client port when connected over https,"Websocket client has not ability to connect to endpoint with non-secure port:
{code}
WebSocket connection to 'wss://172.27.25.217:8080/api/stomp/v1/websocket' failed: Error in connection establishment: net::ERR_CONNECTION_REFUSED
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-30 10:10:29,19
13148948,Issues for no NN federation case appearing after federation support implementation,"- HDFS links dropdown on dashboard isn't displayed with JS error thrown: {{Uncaught TypeError: Cannot read property 'hosts' of undefined}}
- 'N/A' / 'Not Running' is displayed for NameNode Uptime instead of actual value
- Incorrect values in Service metrics section of service summary page
- Order of dashboard widgets is not restored
- Incorrect value of NameNode RPC on host summary page",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-29 17:22:12,15
13148894,NN Federation: replace start/stop operation with restart operation,The last operation in the wizard should be Restart All Service. Also we need to provide a check before opening wizard that ZooKeeper servers and JournalNodes are running.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-29 14:32:35,9
13148892,Add ability to use latest configs for execution commands,Generally each execution command contains hash of configuration used with. Blueprint deploy workflow presumes posting configs updates after execution commands. Server should have ability to order agent to use last available configuration instead specified in command for command execution.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-29 14:30:13,6
13148877,"Customize Services - when user is changed in accounts tab no warning shown to adjust other dependent properties , though shown when navigate to Misc Tab	","1) Modify the property Yarn user under Accounts tab. 
2) Click Next to Navigate to All Configurations Tab . Chose Misc from Service Selection Dropdown. User will see a pop up to adjust a property yarn.admin.acl which is affected because of user change.
If user doesn't navigate to Misc tab , he is able to proceed with this effect going unchanged and unnoticed.
This is not even shown in Review Recommendations.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-29 13:44:39,7
13148876,Upgrade throws NullPointerException when configuration type is not selected,"
{code}
java.lang.NullPointerException
at org.apache.ambari.server.state.UpgradeHelper.processConfigurationsIfRequired(UpgradeHelper.java:1032)
at org.apache.ambari.server.state.UpgradeHelper.updateDesiredRepositoriesAndConfigs(UpgradeHelper.java:849)
at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
at org.apache.ambari.server.serveraction.upgrades.UpdateDesiredRepositoryAction.updateDesiredRepositoryVersion(UpdateDesiredRepositoryAction.java:166)
at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
at org.apache.ambari.server.serveraction.upgrades.UpdateDesiredRepositoryAction.execute(UpdateDesiredRepositoryAction.java:104)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
at java.lang.Thread.run(Thread.java:745)
{code}

The main issue here is that if any configuration types in the ""clusterconfig"" table (in the ambari-server DB) happen to not be selected (no versions of that config type have ""selected""= ""1"", then this NullPointerException is thrown.
The Ambari Server Upgrade code should probably handle this exception case more gracefully, by adding a check for null in the {code}UpgradeHelper{code} class, and returning a more useful error message.  Perhaps the customer should get a message indicating the problem with the config types.


",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-29 13:44:00,13
13148845,Problems while selecting the property uniquely in Configurations screen while installation,"While scripting the automation code for the new UI installer, could not select some properties uniquely and some problems faced:
# Unable to select some properties by their label in xpath:
{panel}
* In Advanced zeppelin-config panel under Zeppelin configurations, there are a bunch of configurations related to zeppelin.ssl. As seen in screenshot-1, all the labels have zeppelin.ssl in them and if one were to type textbox with label zeppelin.ssl using XPATH strict text match, i.e. //*[text()=’zeppelin.ssl’], it fails. This is because within the label there are some html elements (see screenshot-1) like <wbr> which do not allow a strict match to the label.
* A simple workaround would be to use xpath contains() operator, but it would return multiple fields for the same label, which is not good.
{panel}
# Try to select the property by ‘data-qa’ also has similar problems to above
{panel}
As per screenshot-2, the data-qa attribute has the value ‘service-config-zeppelin-ssl-zeppelin-config-xml-default’ which can only be covered by xpath //*[contains(@data-qa,’zeppelin-ssl’)]. But this would again lead to multiple matches in the UI.
{panel}

Two asks from a test automation point of view:
# Remove any HTML entity from in between the label text
{panel}
Example: zeppelin.<wbr>ssl to zeppelin.ssl
{panel}
# Set the data-qa attribute for the input fields to just the property name as specified in Blueprint. For example, if the Blueprint property is ‘zeppelin.ssl’, the data-qa attribute can be ‘zeppelin-ssl’.
 
This would enable the automation code to uniquely identify the property fields.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-29 11:17:41,19
13148820,HDFS metrics data missing in Ambari 2.7.0,"n Ambari 2.7 with HDFS, Zookeeper, AMS, and Smart Sense (HDP 3.0) installed, the following alert is seen:
{noformat}
NameNode Directory Status
Failed directory count: None{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-03-29 09:32:54,18
13148819,PasswordUtilsTest fails if running as 'root',"If org.apache.ambari.server.utils.PasswordUtilsTest.shouldReadDefaultPasswordIfPasswordPropertyIsPasswordFilePathButItIsNotReadable() is running by _root_ user the File.setReadable does not work -> the build will fail.

We should consider using
java.nio.file.attribute.PosixFilePermissions
I'll disable this use case until it's fixed.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-29 09:30:29,27
13148719,Blueprint deployments for NameNode Federation Fail,"Ambari 2.7.0 will include support for HDFS NameNode Federation deployments, as detailed in:  

AMBARI-22916

Blueprint deployments that attempt to use NameNode Federation currently fail with the following exception: 

 
{code:java}
{u'status': 400, u'message': u'Topology validation failed: org.apache.ambari.server.topology.InvalidTopologyException: NAMENODE HA requires exactly 2 hosts running NAMENODE but there are: 4 Hosts: [ctr-e138-1518143905142-139109-01-000005.hwx.site, ctr-e138-1518143905142-139109-01-000006.hwx.site, ctr-e138-1518143905142-139109-01-000003.hwx.site, ctr-e138-1518143905142-139109-01-000004.hwx.site]'}{code}
 

The Blueprint configuration processor needs to be updated in order to properly handle HDFS HA deployments that include Federated nameservices.  This will generally mean that HA deployments can now support more than 2 NameNode instances.  ",pull-request-available,['ambari-sever'],AMBARI,Bug,Critical,2018-03-28 21:29:55,51
13148648,Group_list should also look for properties in cluster settings besides desired configs.,"The desiredconfigs contains only service configs.
In the previous ambari version, it also used to have cluster-env configs but now since we have moved to cluster settings, the properties in cluster settings are not part of the desired configs.
Hence, while searching for the group_list and user_list for setting in the command.json, we need to explicitly look for the properties in cluster settings as well.
cc [~sshridhar], [~jluniya]",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-28 18:00:23,56
13148592,Ambari Requests Too Much Data From Hosts When Logging In,"When first logging into Ambari, the web client makes the following request:
GET api/v1/clusters/<clusterName>/hosts?fields=
- Hosts/cpu_count
- Hosts/host_name
- Hosts/host_status
- Hosts/ip
- Hosts/last_agent_env
- Hosts/last_heartbeat_time
- Hosts/maintenance_state
- Hosts/ph_cpu_count
- Hosts/public_host_name
- Hosts/rack_info
- Hosts/total_mem
- alerts_summary
- host_components/HostRoles/desired_admin_state
- host_components/HostRoles/display_name
- host_components/HostRoles/maintenance_state
- host_components/HostRoles/service_name
- host_components/HostRoles/stale_configs
- host_components/HostRoles/state
- host_components/logging
- stack_versions/HostStackVersions
- stack_versions/repository_versions/RepositoryVersions/display_name
- stack_versions/repository_versions/RepositoryVersions/id
- stack_versions/repository_versions/RepositoryVersions/repository_version

In a cluster with 100 hosts, this can cause a payload of over 200MB to be returned. The culprit seems to be the {{Hosts/last_agent_env}} property, specifically the {{activeJavaProcs}} key. If the hosts in the cluster are running a lot of Java processes, this can account for over 90% of the payload (roughly 180MB). This data takes 30 or more seconds for the server to serialize and stream to the socket.

It looks like {{Hosts/last_agent_env}} isn't even needed after initially provisioning a host during the host checks, so it probably can be removed.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-28 14:40:59,19
13148583,Duplicate websocket subscription on Configs page,"Steps to reproduce:
# Open ambari web
# Go to HDFS
# Open Configs page
# Leave page

Actual Result: UI has subscribed to all topics again.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-28 14:23:16,19
13148580,Remove FK Relationship On Repo Versions for Services & Components,"As part of AMBARI-23009, this will remove the reliance that services and components have on the {{repo_version}} table. It will also attempt to address some of the many, many unit test failures in the branch.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-03-28 14:10:27,31
13148550,Dashboard: rationalize default widgets,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-28 11:59:36,19
13148540,Install Packages button on the versions screen fails if there is a service installed which was removed,"I've installed several services with Ambari 2.6.1 - HDP 2.6.4, then upgraded Ambari to 2.7. After this I've added a new HDP 3.0 repo. As Slider was installed which was removed from HDP 3.0, it displayed a white ""i"" in a blue circle in the 3.0 column (see the screenshot). As I'v clicked on the ""Install Packages"" button it returned an error message (see screenshot). I've checked the log, and it seems that the absence of the Slider service caused the issue:

{code}
org.apache.ambari.server.controller.spi.SystemException: Cannot obtain stack information for HDP-3.0
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:305)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.getHostVersionInstallCommand(ClusterStackVersionResourceProvider.java:723)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrchestration(ClusterStackVersionResourceProvider.java:634)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrUpdateHostVersions(ClusterStackVersionResourceProvider.java:532)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createResourcesAuthorized(ClusterStackVersionResourceProvider.java:474)
 at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.createResources(AbstractAuthorizedResourceProvider.java:231)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
 at org.apache.ambari.server.api.services.ClusterStackVersionService.createRequests(ClusterStackVersionService.java:120)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:564)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
 at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
 at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ambari.server.StackAccessException: Stack data, stackName=HDP, stackVersion=3.0, serviceName=SLIDER
 at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:540)
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:303)
 ... 114 more
org.apache.ambari.server.controller.spi.SystemException: Cannot obtain stack information for HDP-3.0
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:305)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.getHostVersionInstallCommand(ClusterStackVersionResourceProvider.java:723)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrchestration(ClusterStackVersionResourceProvider.java:634)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrUpdateHostVersions(ClusterStackVersionResourceProvider.java:532)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createResourcesAuthorized(ClusterStackVersionResourceProvider.java:474)
 at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.createResources(AbstractAuthorizedResourceProvider.java:231)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
 at org.apache.ambari.server.api.services.ClusterStackVersionService.createRequests(ClusterStackVersionService.java:120)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:564)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
 at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
 at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ambari.server.StackAccessException: Stack data, stackName=HDP, stackVersion=3.0, serviceName=SLIDER
 at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:540)
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:303)
 ... 114 more
{code}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-03-28 11:22:43,18
13148538,Hostlist view selection bug,"* open hostlist view
* select a node with the checkbox
* top right actions list: ""Selected Hosts (1)""  (ok)
* switch to any other view
* switch back to hostlist view
* node is *still* selected
* top right actions list ""Selected Hosts (0)"" 

the list should be either cleared from the selection; or the action should be available.... ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-28 11:04:35,22
13148503,PQS start fails after Ambari upgrade due to Bad file descriptor,"*STR*
PQS start after Ambari upgrade to 2.7.0 failed with below error:
{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/phoenix_queryserver.py"", line 81, in 
 PhoenixQueryServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 356, in execute
 self.execute_prefix_function(self.command_name, 'post', env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute_prefix_function
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 419, in post_start
 raise Fail(""Pid file \{0} doesn't exist after starting of the component."".format(pid_file))
resource_management.core.exceptions.Fail: Pid file /var/run/hbase/phoenix-hbase-server.pid doesn't exist after starting of the component.
{code}

The log says the following:
{code}
2018-03-26 16:33:13.563112 launching /base/tools/jdk1.8.0_112/bin/java -cp /usr/hdp/current/hbase-client/conf:/etc/hadoop/conf:/grid/0/hdp/2.6.4.0-91/phoenix/bin/../phoenix-4.7.0.2.6.4.0-91-client.jar:/grid/0/hdp/2.6.4.0-91/phoenix/bin/../phoenix-4.7.0.2.6.4.0-91-queryserver.jar:/usr/hdp/2.6.4.0-91/hadoop/conf:/usr/hdp/2.6.4.0-91/hadoop/lib/*:/usr/hdp/2.6.4.0-91/hadoop/.//*:/usr/hdp/2.6.4.0-91/hadoop-hdfs/./:/usr/hdp/2.6.4.0-91/hadoop-hdfs/lib/*:/usr/hdp/2.6.4.0-91/hadoop-hdfs/.//*:/usr/hdp/2.6.4.0-91/hadoop-yarn/lib/*:/usr/hdp/2.6.4.0-91/hadoop-yarn/.//*:/usr/hdp/2.6.4.0-91/hadoop-mapreduce/lib/*:/usr/hdp/2.6.4.0-91/hadoop-mapreduce/.//*::mysql-connector-java-5.1.33-bin.jar:mysql-connector-java.jar:/grid/0/hdp/2.6.4.0-91/tez/*:/grid/0/hdp/2.6.4.0-91/tez/lib/*:/grid/0/hdp/2.6.4.0-91/tez/conf
 -Dproc_phoenixserver -Dlog4j.configuration=file:/grid/0/hdp/2.6.4.0-91/phoenix/bin/log4j.properties -Dpsql.root.logger=INFO,DRFA -Dpsql.log.dir=/grid/0/log/hbase -Dpsql.log.file=phoenix-hbase-server.log org.apache.phoenix.queryserver.server.Main
close failed in file object destructor:
IOError: [Errno 9] Bad file descriptor
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/grid/0/hdp/2.6.4.0-91/phoenix/phoenix-4.7.0.2.6.4.0-91-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/grid/0/hdp/2.6.4.0-91/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
starting Query Server, logging to /grid/0/log/hbase/phoenix-hbase-server.log
Query Server already running, PID file found: /var/run/hbase/phoenix-hbase-server.pid
close failed in file object destructor:
IOError: [Errno 9] Bad file descriptor
{code}",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-03-28 07:28:40,18
13148283,"Fix intermittent ""No such file or directory"" error in TestHostCleanup","Fix intermittent ""No such file or directory"" error in TestHostCleanup

{noformat}
======================================================================
ERROR: test_do_cleanup_all (TestHostCleanup.TestHostCleanup)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/test/python/ambari_agent/TestHostCleanup.py"", line 233, in test_do_cleanup_all
    self.hostcleanup.do_cleanup(propertyMap)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/main/python/ambari_agent/HostCleanup.py"", line 168, in do_cleanup
    self.do_delete_by_owner(userIds, FOLDER_LIST)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/main/python/ambari_agent/HostCleanup.py"", line 514, in do_delete_by_owner
    stat = os.stat(fileToCheck)
OSError: [Errno 2] No such file or directory: '/tmp/symlink2120964127269148256test_link'
{noformat}",pull-request-available unit-test,['ambari-agent'],AMBARI,Bug,Major,2018-03-27 17:42:49,30
13148255,UI loading stuck after deleting service,"{noformat}
Uncaught TypeError: Cannot read property 'toLowerCase' of null
    at Class.<anonymous> (app.js:234426)
    at ComputedPropertyPrototype.get (vendor.js:14954)
    at get (vendor.js:13360)
    at getPath (vendor.js:13482)
    at get (vendor.js:13353)
    at getWithGlobals (vendor.js:15987)
    at Binding._sync (vendor.js:16174)
    at invoke (vendor.js:15382)
    at iter (vendor.js:15429)
    at Array.forEach (<anonymous>)
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-27 15:48:59,19
13148230,Closing 'Add New HDFS Namespace' wizard shows confirmation for RM HA wizard,"- Navigate to Add New HDFS Namespace wizard to enable federation
- At configure components one step had failed
- Tried to close the wizard by choosing 'x' on right hand side of the wizard page
- Confirmation popup shows message for RM HA",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-27 13:43:00,9
13148206,"Create topic handler for small agent actions. Like restart_agent, clean_caches etc.","For bigger actions containing a lot of info or special workflow and a new
topic would be required. Small actions like restart_agent/clean_cache make
sense to be sent in one general topic

",pull-request-available,[],AMBARI,Bug,Major,2018-03-27 12:18:37,5
13148199,Add Service Wizard will not reset after adding a service,"After adding a service using the Add Service Wizard and then trying to add another service using the Add Service Wizard, the user interface will not reset to the beginning of the wizard.  

Steps to reproduce:
# Create Ambari 2.6.1 cluster with ZK, HDFS, and Yarn (HDP 2.6)
# Upgrade to Ambari 2.7.0
# Add Atlas
# Add Knox (or try to)

Note: The upgrade or stack may not be a factor here

Note: The dashboard does not refresh after the first add... so Atlas is not displayed in the service list until and manual refresh of the view. ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-27 11:50:56,19
13148198,Fix formatZKFC,"resource_management.core.exceptions.Fail: Script '/var/lib/ambari-
agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py' has no
method 'format'

",pull-request-available,[],AMBARI,Bug,Major,2018-03-27 11:41:47,5
13148194,Hosts not populated in Assign Master Page in Move Master Wizard,q,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-27 11:04:44,7
13148185,Download client configs fails due to 'clusterLevelParams' not found,"# Install Ambari 2.6.0 with ZooKeeper
# Upgrade to Ambari 2.7.0 (tested 2.7.0.0-115 with fix from [apache/ambari#608|https://github.com/apache/ambari/pull/608])
# Try to _Download All Client Configs_

Result: 500 Server Error

{noformat}
Execution of ""ambari-python-wrap /var/lib/ambari-server/resources/common-services/ZOOKEEPER/3.4.5/package/scripts/zookeeper_client.py generate_configs /var/lib/ambari-server/data/tmp/ZOOKEEPER_CLIENT3308027289386152157-configuration.json /var/lib/ambari-server/resources/common-services/ZOOKEEPER/3.4.5/package /var/lib/ambari-server/data/tmp/structured-out.json INFO /var/lib/ambari-server/data/tmp"" returned 1.
...
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/common-services/ZOOKEEPER/3.4.5/package/scripts/zookeeper_client.py"", line 81, in <module>
    ZookeeperClient().execute()
  File ""/usr/lib/ambari-server/lib/resource_management/libraries/script/script.py"", line 377, in execute
    method(env)
  File ""/usr/lib/ambari-server/lib/resource_management/libraries/script/script.py"", line 1072, in generate_configs
    import params
  File ""/var/lib/ambari-server/resources/common-services/ZOOKEEPER/3.4.5/package/scripts/params.py"", line 26, in <module>
    from params_linux import *
  File ""/var/lib/ambari-server/resources/common-services/ZOOKEEPER/3.4.5/package/scripts/params_linux.py"", line 21, in <module>
    import status_params
  File ""/var/lib/ambari-server/resources/common-services/ZOOKEEPER/3.4.5/package/scripts/status_params.py"", line 53, in <module>
    stack_version_unformatted = str(config['clusterLevelParams']['stack_version'])
  File ""/usr/lib/ambari-server/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
{noformat}",pull-request-available,[],AMBARI,Bug,Blocker,2018-03-27 09:53:54,55
13148153,Livy server fails to start after Ambari upgrade due to missing argument for nameservice,"*STR*
 # Deployed a cluster with HDP-2.6.4 and Ambari-2.6.1
 # Upgrade Ambari to 2.7.0.0-202
 # Try to start Spark service

*Result*
Spark livy server start fails with:
{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/livy2_server.py"", line 144, in 
    LivyServer().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/livy2_server.py"", line 59, in start
    self.wait_for_dfs_directories_created([params.entity_groupfs_store_dir, params.entity_groupfs_active_dir])
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/livy2_server.py"", line 91, in wait_for_dfs_directories_created
    self.wait_for_dfs_directory_created(dir_path, ignored_dfs_dirs)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 54, in wrapper
    return function(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/livy2_server.py"", line 115, in wait_for_dfs_directory_created
    util = WebHDFSUtil(params.hdfs_site, params.hdfs_user, params.security_enabled)
TypeError: __init__() takes at least 5 arguments (4 given)
{code}
 

The issue is because a new 'nameservice' argument is added to WebHDFSUtil constructor

The problem needs a fix in both livy_server and livy2_server scripts, as the below call is common in these two scripts
{code:java}
util = WebHDFSUtil(params.hdfs_site, params.hdfs_user, params.security_enabled){code}
 

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-27 07:29:21,46
13148118,"Initialize 'topologyHolder', 'hostLevelParamsHolder', 'recoveryConfigHelper' in HostResourceProvider.","- After *trunk* merge into *branch-feature-AMBARI-14714*

{code}
commit 65f010cfce4af4eeffc72a125bf2307f7fbc1e87
Merge: 8e5b0de 0ed3485
Author: Swapan Shridhar <sshridhar@hortonworks.com>
Date:   Mon Mar 26 16:32:54 2018 -0700

    Merging Trunk to branch : 'branch-feature-AMBARI-14714'.
{code}

- POST hosts started failing with NPE at https://github.com/apache/ambari/blob/branch-feature-AMBARI-14714/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java#L585

Stack Trace: 

{code:title=ambari-server.log}
27 Mar 2018 03:03:08,284  WARN [ambari-client-thread-95] HttpChannel:507 - /api/v1/clusters/c1/hosts
java.lang.NullPointerException
        at org.apache.ambari.server.controller.internal.HostResourceProvider.createHosts(HostResourceProvider.java:585)
        at org.apache.ambari.server.controller.internal.HostResourceProvider$1.invoke(HostResourceProvider.java:255)
        at org.apache.ambari.server.controller.internal.HostResourceProvider$1.invoke(HostResourceProvider.java:252)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:288)
        at org.apache.ambari.server.controller.internal.HostResourceProvider.createResourcesAuthorized(HostResourceProvider.java:252)
        at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.createResources(AbstractAuthorizedResourceProvider.java:231)
{code}

*Code Line:*

{code:title=https://github.com/apache/ambari/blob/branch-feature-AMBARI-14714/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java#L585}
           for (HostLevelParamsUpdateEvent hostLevelParamsUpdateEvent : hostLevelParamsUpdateEvents) {
      hostLevelParamsHolder.updateData(hostLevelParamsUpdateEvent);
    }
{code} 

*Reason :* With perf->trunk->feature branch merge, we have started using *hostLevelParamsHolder*, which has not been initialized/injected. Similarly for others : 'topologyHolder' and 'recoveryConfigHelper'.

*Fix* : Added initialization code for them.

CC [~mradhakrishnan] | [~sduan] | [~jluniya]",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-27 03:56:20,43
13148104,XMLConfig files written by setup_ranger_plugin_xml.py should use 0644 instead of 0744 for XML files,"- When we enable Ranger Plugin for knox we see that ambari pushes the following files with 744 permission which should be 644 (as the XML files do not require execute permission).

{code}
# ls -l /etc/knox/conf/ranger-*
-rwxr--r--. 1 knox knox 1322 Mar 27 00:20 /etc/knox/conf/ranger-knox-audit.xml
-rwxr--r--. 1 knox knox  944 Mar 27 00:20 /etc/knox/conf/ranger-knox-security.xml
-rwxr--r--. 1 knox knox 1027 Mar 27 00:20 /etc/knox/conf/ranger-policymgr-ssl.xml
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-27 02:20:28,22
13147985,Streamline Application manager's UI link is not visible in Ambari-2.7.0.0,"SAM's UI link is not available on side bar using Ambari-2.7.0.0. Independently, you can open SAM's UI at port 7777 (default) just fine.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-26 17:16:10,15
13147959,Log Search UI: filter dropdown lists fixes,"# Add Select All and Select None controls
 # Items of Levels list should look like entries of service logs table (i.e. with icons and colors)
 # Add 'filter list' feature.",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-03-26 15:34:50,26
13147912,Ambari Agent Creates World Writable Files with 666 permissions,"*ambari agent is creating world writable files with 666 permission*

So when ambari-agent is run as root , the file's are created with 666 permission and file owners are root.

Ambari-agent shouldn't be creating files owned by root and with world writable permissions. - this is issue related to security.

",ambari-agent permissions pull-request-available security,['ambari-agent'],AMBARI,Bug,Major,2018-03-26 11:17:48,3
13147911,When credential store is enabled status commands should not generate jceks,"Status commands should not generate jceks as they don't need them. This was
the behavior in previous release.

Currently jceks are generated for every status command, flooding ambari-agent
logs and making status commands run unnecessary longer.

",pull-request-available,[],AMBARI,Bug,Major,2018-03-26 11:12:19,5
13147886,Service page layout updates,See screenshots,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-26 09:34:16,19
13147871,Update combo filter style,"Remove shadow outside input box.

",pull-request-available,['ambari-web'],AMBARI,Task,Minor,2018-03-26 08:24:15,19
13147829,Add Hive Service wizard shows incorrect path for mysql driver in ambari-server setup command,"
While installing Hive on Ambari it shows the message for installing driver with incorrect Driver JAR name in the setup command.

{code}
To use MySQL with Hive, you must download the https://dev.mysql.com/downloads/connector/j/ from MySQL. Once downloaded to the Ambari Server host, run: 
ambari-server setup --jdbc-db=mysql --jdbc-driver=/path/to/mysql/com.mysql.jdbc.Driver
{code}

Notice the path is *""/path/to/mysql/com.mysql.jdbc.Driver""*,  which is supposed to be *""/path/to/mysql/mysql-connector-java.jar""*

So if the users will follow the instruction as it is, then it will cause the following error:
{code}
# ambari-server setup --jdbc-db=mysql --jdbc-driver=/path/to/mysql/com.mysql.jdbc.Driver
Using python  /usr/bin/python
Setup ambari-server
ERROR: Exiting with exit code 1. 
REASON: File /path/to/mysql/com.mysql.jdbc.Driver does not exist!
{code}",ambari-web pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-26 02:06:40,22
13147818,Enable or disable SSO for services upon setting sso-configuration values ,"Enable or Disable SSO for services upon setting sso-configuration values.

The action performed by the user via the REST API to set value for the Ambari server sso-configuration value should trigger a call to the stack advisor and, using the returned recommendations, set properties needed to enable or disable SSO integration for the relevant services.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-25 23:07:00,30
13147689,"During ambari upgrade, DB backup choice default option should be 'n' instead of 'y'","When we run the ""ambari-server upgrade"" option then at the following point it asks if the user has already taken Database dump or not?  

{code}
[root@jnode1 ~]# ambari-server upgrade
.
.
INFO: Fixing database objects owner
Ambari Server configured for Embedded Postgres. Confirm you have made a backup of the Ambari Server database [y/n] (y)? 
{code}


There is a possibility that the user might mistakenly press ENTER (which means 'y')  without actually collecting the Ambari DB backup.

So in order to avoid this risk the default option should be ""n'  so that we can avoid ignorance and user will have to explicitly enter ""y' to proceed (to confirm they collected Db dump already).

Expected value:
{code}
Ambari Server configured for Embedded Postgres. Confirm you have made a backup of the Ambari Server database [y/n] (n)? 
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-24 12:42:00,22
13147631,The Ambari-DDL-Postgres-EMBEDDED-CREATE.sql file contains incorrect filename inside it.,"There seems to be some typo issue inside the ""Ambari-DDL-Postgres-EMBEDDED-CREATE.sql"" which is pointing to some incorrect file name (due to typo).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-24 00:46:54,22
13147604,"ServiceConfig API call uses ""MpackName-Vesion"" for SG instead of just ""MpackName"".","1. ServiceConfig API call uses ""MpackName-Vesion"" for SG instead of just ""MpackName"".

- With ""[AMBARI-23269] Removed mpack version from service group name (#692)"" we have started using just the *Mpack* Name to create the SG.
- But we haven't updated the Service Configs API POST calls, which still continue to use *MpackName-Vesion* for referring SG. This needs to be updated.

Failure : 

POST http://<Server>:8080/api/v1/clusters/c1/servicegroups/HDPCORE-1.0.0-b140/services/HDFS/configurations

 [^Screen Shot 2018-03-23 at 1.05.13 PM.png]


",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2018-03-23 21:52:17,43
13147563,Add NameNode federation support for Host Details page,"- Display namespace id for NameNode entries on Host Details page
- Perform DataNode decommission status check using data of the correct NameNode
- Add NameNode widgets to Host Metrics section",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-23 18:59:41,15
13147547,service advisor has the option to calculate colocatServices with more information from the services list,"Besides the current colocateService method in service_advisor.py, we propose to add one more method that takes in the full services list so the script may calculate colocate components with more information.

We are adding a new method so the existing service advisor code in HDP 3.0 remain compatible, while 3rd party services, such as Big SQL can implement the new method.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-03-23 17:40:56,60
13147506,Ambari server start exits with non zero,"On slower machines, ambari server (2.7.0) start command fails because the web UI availability check times out at 50 seconds.

{code}
Restarting ambari-server
Waiting for server stop...
Ambari Server stopped
Ambari Server running with administrator privileges.
Organizing resource files at /var/lib/ambari-server/resources...
Ambari database consistency check started...
Server PID at: /var/run/ambari-server/ambari-server.pid
Server out at: /var/log/ambari-server/ambari-server.out
Server log at: /var/log/ambari-server/ambari-server.log
Waiting for server start............................................................
DB configs consistency check: no errors and warnings were found.
ERROR: Exiting with exit code 1. 
REASON: Server not yet listening on http port 8080 after 50 seconds. Exiting.
 failed and return code :1
{code}

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-23 15:44:17,39
13147505,Install Wizard: Add a Cancel button to go back to Admin View.,"See screenshot.

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-23 15:40:34,19
13147463,Server should acknowledge agent about received reports,Server should sent reply for requests to agent reports stomp endpoint.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-23 12:47:18,6
13147453,Admin View: Users and Groups style tweaks,"See screenshots
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-23 12:13:37,19
13147432,Blueprints with an installed cluster should be read-only,Deleting a blueprint with a cluster installed can cause heartbeat issues with agents. Such blueprints should be read-only,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-23 10:45:10,20
13147419,Remove python3 files from ws4py since they can cause build failure,"On some envs (not reproducible for me) it can result in failure like this:

    
    
    2018/03/22 23:49:21 INFO    : [INFO]   File ""/usr/lib/ambari-agent/lib/ambari_ws4py/async_websocket.py"", line 85
    2018/03/22 23:49:21 INFO    : [INFO]     yield from self.proto.writer.drain()
    2018/03/22 23:49:21 INFO    : [INFO]              ^
    2018/03/22 23:49:21 INFO    : [INFO] SyntaxError: invalid syntax
    2018/03/22 23:49:21 INFO    : [INFO] 
    2018/03/22 23:49:21 INFO    : [INFO] Compiling /grid/0/jenkins/workspace/Zuul_Ambari_Build_Job/build-support/SOURCES/ambari/ambari-agent/target/rpm/ambari-agent/buildroot/usr/lib/ambari-agent/lib/ambari_ws4py/server/tulipserver.py ...
    2018/03/22 23:49:21 INFO    : [INFO]   File ""/usr/lib/ambari-agent/lib/ambari_ws4py/server/tulipserver.py"", line 101
    2018/03/22 23:49:21 INFO    : [INFO]     request_line = yield from self.next_line()
    2018/03/22 23:49:21 INFO    : [INFO]                             ^
    2018/03/22 23:49:21 INFO    : [INFO] SyntaxError: invalid syntax
    2018/03/22 23:49:21 INFO    : [INFO] 
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-23 10:03:31,5
13147408,Editing configurations usings configs.py doesnt have options to pass service config version note,"Editing configurations usings configs.py doesnt have options to pass service config version note

But we can pass service config version note via curl command and in ambari-ui 

This feature should be implemented for actions 'SET' and 'DELETE'

Suggested give '-b' as option to give service config version note 

ex : 

{code:java}
/var/lib/ambari-server/resources/scripts/configs.py -u admin -p admin -t 8080 -n asnaik -l localhost -a set -c capacity-scheduler -k ""yarn.scheduler.capacity.resource-calculator"" -v ""org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator"" -b ""Creating a new version with CPU scheduling off""
{code}

and the Help of the configs.py should be 
Please edit the help page : https://cwiki.apache.org/confluence/display/AMBARI/Modify+configurations#Modifyconfigurations-Editconfigurationusingconfigs.py

{code:java}
[root@pwd1 scripts]#  /var/lib/ambari-server/resources/scripts/configs.py --help
Usage: configs.py [options]

Options:
  -h, --help            show this help message and exit
  -t PORT, --port=PORT  Optional port number for Ambari server. Default is
                        '8080'. Provide empty string to not use port.
  -s PROTOCOL, --protocol=PROTOCOL
                        Optional support of SSL. Default protocol is 'http'
  -a ACTION, --action=ACTION
                        Script action: <get>, <set>, <delete>
  -l HOST, --host=HOST  Server external host name
  -n CLUSTER, --cluster=CLUSTER
                        Name given to cluster. Ex: 'c1'
  -c CONFIG_TYPE, --config-type=CONFIG_TYPE
                        One of the various configuration types in Ambari. Ex:
                        core-site, hdfs-site, mapred-queue-acls, etc.
  -b VERSION_NOTE, --version-note=VERSION_NOTE
                        Version change notes which will help to know what has
                        been changed in this config , this value is optional
                        and is used for action set

  To specify credentials please use ""-e"" OR ""-u"" and ""-p'"":
    -u USER, --user=USER
                        Optional user ID to use for authentication. Default is
                        'admin'
    -p PASSWORD, --password=PASSWORD
                        Optional password to use for authentication. Default
                        is 'admin'
    -e CREDENTIALS_FILE, --credentials-file=CREDENTIALS_FILE
                        Optional file with user credentials separated by new
                        line.

  To specify property(s) please use ""-f"" OR ""-k"" and ""-v'"":
    -f FILE, --file=FILE
                        File where entire configurations are saved to, or read
                        from. Supported extensions (.xml, .json>)
    -k KEY, --key=KEY   Key that has to be set or deleted. Not necessary for
                        'get' action.
    -v VALUE, --value=VALUE
                        Optional value to be set. Not necessary for 'get' or
                        'delete' actions.
{code}


",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-23 09:33:53,3
13147397,400 server error when Cluster Administrator tries to add service and move components,"STR:
 # Deploy a HDP-3.0 cluster
 # Create a user with role as ""Cluster Administrator""
 # Login as the user
 # Click Add Service from the dashboard
 # Click Next button in the wizard

The same error occurs while trying the Move Components wizard too.

Issue also present for Cluster Operator Role while moving components.

The page is stuck at this stage even after dismissing the popup.
{code:java|title=ambari-server.log}
17 Mar 2018 16:13:51,740  WARN [ambari-client-thread-524] StackAdvisorCommand:204 - Error occurred during retrieving ldap configuration, status=403, response={
  ""status"" : 403,
  ""message"" : ""The authenticated user does not have the appropriate authorizations to get the requested resource(s)""
}
17 Mar 2018 16:13:51,741  WARN [ambari-client-thread-524] StackAdvisorCommand:182 - Error parsing services.json file content: Error occurred during retrieving ldap configuration, status=403, response={
  ""status"" : 403,
  ""message"" : ""The authenticated user does not have the appropriate authorizations to get the requested resource(s)""
}
org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Error occurred during retrieving ldap configuration, status=403, response={
  ""status"" : 403,
  ""message"" : ""The authenticated user does not have the appropriate authorizations to get the requested resource(s)""
}
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.populateLdapConfiguration(StackAdvisorCommand.java:205)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.adjust(StackAdvisorCommand.java:177)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:352)
	at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:132)
	at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:145)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
	at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:564)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
	at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
	at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:748)
17 Mar 2018 16:13:51,744 ERROR [ambari-client-thread-524] CreateHandler:84 - Caught a runtime exception while attempting to create a resource: null
javax.ws.rs.WebApplicationException
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.adjust(StackAdvisorCommand.java:183)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:352)
	at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:132)
	at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:145)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
	at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:564)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
	at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
	at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:748)

{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-23 09:03:58,18
13147382,Provider URL validator incorrect when setting up SSO via Ambari CLI,"When setting the URL for the SSO provider while running {{ambari-server setup-sso}}, valid URLs are rejected.
{noformat}
[root@c7402 ~]# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Do you want to configure SSO authentication [y/n] (y)?y
Provider URL [URL] (http://example.com):https://c7402.ambari.apache.org:8443/gateway/knoxsso/api/v1/websso
Invalid provider URL
Provider URL [URL] (http://example.com):c7402.ambari.apache.org:8443
Public Certificate pem (empty) (empty line to finish input):
{noformat}
Cause:
The RegEx validating the URL is only allowing hostname and port values, when it should allow for absolute URLs.
{code:java|title=ambari_server/setupSso.py:72}
provider_url = get_validated_string_input(""Provider URL [URL] ({0}):"".format(provider_url), provider_url, REGEX_HOSTNAME_PORT,
                                                ""Invalid provider URL"", False){code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-23 08:13:24,27
13147336,Recommend SSO configuration values for ATLAS and RANGER in the stack advisor,"*Recommend SSO configuration values for ATLAS in the stack advisor*
The following values in {{application-properties}} need to be set:

* {{atlas.sso.knox.enabled}}
** SSO enabled (true) or disabled (false)

* {{atlas.sso.knox.providerurl}}
** SSO provider url. Example: https://KNOX_HOST:KNOX_PORT/gateway/TOPOLOGY_NAME/knoxsso/api/v1/websso

* {{atlas.sso.knox.publicKey}}
** SSO provider public key. The base64-encoded x509 Public key data (without the certificate header and footer) used for SSO provider cookie verification

* {{atlas.sso.knox.providerurl}}
** SSO browser useragents (comma-delimted).  Default: Mozilla,chrome

Also, update the Atlas metainfo.xml file to indicate it supports SSO configuration by Ambari

*Recommend SSO configuration values for RANGER in the stack advisor*
The following values in {{ranger-admin-site}} need to be set:

* {{ranger.sso.enabled}}
** SSO enabled (true) or disabled (false)

* {{ranger.sso.providerurl}}
** SSO provider url. Example: https://KNOX_HOST:KNOX_PORT/gateway/TOPOLOGY_NAME/knoxsso/api/v1/websso

* {{ranger.sso.publicKey}}
** SSO provider public key. The base64-encoded x509 Public key data (without the certificate header and footer) used for SSO provider cookie verification

* {{ranger.sso.cookiename}}
** SSO cookie name

* {{ranger.sso.query.param.originalurl}}
** Query name for appending original url in SSO url

* {{ranger.sso.browser.useragent}}
** SSO browser useragents (comma-delimted).  Default: Mozilla,chrome

Also, update the Ranger metainfo.xml file to indicate it supports SSO configuration by Ambari",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-23 01:11:54,30
13147217,"Ambari: set ""cloud storage"" usage tracking properties for AWS, Azure object stores","Set the following properties for HDP stack:
{noformat}
fs.azure.user.agent.prefix
fs.s3a.user.agent.prefix
{noformat}",pull-request-available,[],AMBARI,Task,Critical,2018-03-22 17:11:44,33
13147198,Remove LDAP Synchronization Process,"The existing LDAP synchronization process has the following challenges:

* Common annoyance among Operators
* Difficult to schedule as it’s interactive
* Introduces delay in entitlements being granted to users (in LDAP), and more importantly from revocation.
* Introduces issues with remove users that are no longer active, or with the company which == compliance concerns

and benefits: 
* Simplifies auto-complete for adding users/groups to cluster roles and view permissions
* Shields users from LDAP performance issues on login, and during user/group permission mapping

Given that, Ambari's LDAP sync process should be removed to allow for users and groups to be dynamically synchronized with a configured LDAP server.  Users should be added to the Ambari DB if necessary and groups are to be dynamically assigned and mapped to Ambari roles upon successful authentication with the configured LDAP server (via Ambari).  A scheduled job may need to execute to clean out any orphaned data.

The requirements will be broken out into categories of capabilities:

1.) Permission Mapping
2.) Permission Resolution
3.) User management

*Permission Mapping*: Ability to map an individual LDAP User DN to a permission, as well as an individual Group DN to a permission

*Permission Resolution*: Ability to resolve DN of user, DN of all directly mapped groups, and DN of all in-directly mapped groups (nested groups)
* If the user logging in has no permissions they should not be allowed to login, but shown a message stating that they have no mapped permissions in Ambari and to contact their administrator, no Ambari DB user should be auto-created
* If the user logging in has permissions, we should:
** Auto-create the Ambari user in the DB if it does not exist
** Check if we were asked to auto-create home directories on login, and if so check if the user has a home directory, if they don't, then auto-create it

*User Management*: Because users will be auto-created in the Ambari DB, and because they will be authenticated against LDAP before being able to login we have to appropriately deal with two types of users:

# Orphaned Users: Users without any mapped permissions - users enter this state by being in a group ""HadoopOps"" lets say and then six months later they are removed from this group. Or were directly mapped to a permission by name, and were removed from that permission. If they try logging into Ambari they will not be allowed to login and will be shown the message stating that they have no permissions.  These users need to be removed from Ambari individually through the Ambari UI with a ""Remove LDAP User"" button on the user.
# Deprovisioned Users: Users who have been either removed or inactivated in the upstream LDAP server due to termination or other reasons - In most situations we'll never see these users again.

For both types of users having the following capabilities would be extremely helpful:

* Ability to remove individual LDAP users from Ambari ""Remove LDAP User"" button
* Ability to remove all users who haven't logged in in more than x days.
",authentication ldap,['ambari-server'],AMBARI,Epic,Major,2018-03-22 15:50:53,30
13147146,Add custom context root support to quicklink engine,"Adding placeholder replacement ability to quicklink engine.

For example:
{code:java}
""url"":""%@://%@:%@/${gateway-site/gateway.path}/knoxsso/knoxauth/login.html""{code}
Here the ${config-type/propety-name} should be replaced based on the appropriate configuration.",pull-request-available,['ambari-web'],AMBARI,Improvement,Major,2018-03-22 13:19:22,18
13147133,Locking configure to single process causes deadlock,"1\. Component1 needs component2 to be started to do configure  
2\. Component1 grabs the lock and retries in configure  
3\. Component2 cannot grab the lock waiting forever

This causes variety of timeout errors in different components.  
Than finally one component will fail with at the end

    
    
    2018-03-22 08:53:01,593 - Trying to acquire a lock on /var/lib/ambari-agent/tmp/link_configs_lock_file
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-22 12:41:32,5
13147112,Missing tooltips for two fields in 'Add User' page,"No tooltip for 'Is this user an Ambari Admin', 'Deactivate this user' in Add user page",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2018-03-22 10:42:18,7
13146942,"Provide for mpack definition tarballs crc32 or md5 checksums, to validate downloaded files","from the start, not fully downloaded or become broken after some hardware failure. The idea is, to provide crc32 or md5 checksum to verify them before start working with them, as it happen with rpm files and yum.

Checksum could be provided as separate file: definition_name.tar.gz.crc32 or inside mpack.json",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-21 19:17:18,44
13146863,Prevent Multiple OS Entries For a Single Mpack,"The operating systems endpoint allows for creation and deletion of repositories which are associated with a single management pack:

{code}
PUT http://{{ambari-server}}:8080/api/v1/mpacks/1/operating_systems/redhat7
{
  ""OperatingSystems"" : {
    ""is_ambari_managed"" : true,
    ""repositories"" : [
      {
        ""distribution"" : null,
        ""components"" : null,
        ""unique"" : true,
        ""tags"" : [ ],
        ""base_url"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96/foo"",
        ""os_type"" : ""redhat7"",
        ""repo_id"" : ""HDPCORE-1.0.0-b96"",
        ""repo_name"" : ""HDPCORE"",
        ""mirrors_list"" : null,
        ""default_base_url"" : null,
        ""ambari_managed"" : true
      },
      {
        ""distribution"" : null,
        ""components"" : null,
        ""unique"" : false,
        ""tags"" : [ ],
        ""base_url"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDP-UTILS-1.1.0.22/foo"",
        ""os_type"" : ""redhat7"",
        ""repo_id"" : ""HDP-UTILS-1.1.0.21"",
        ""repo_name"" : ""HDP-UTILS"",
        ""mirrors_list"" : null,
        ""default_base_url"" : null,
        ""ambari_managed"" : true
      }
    ]
  }
}
{code}

However, multiple PUTs causes duplicate entries for the same OS. This should be prevented.",pull-request-available,[],AMBARI,Bug,Critical,2018-03-21 14:43:22,31
13146844,JS error on service stop/restart after config changes,"Steps:
# Change the value of fs.trash.interval for HDFS service.
# Save configs
# Notice Restart required icon
# Try to apply 'Restart All Affected'

No op was triggered. Could see js error error in console:
{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
pullNnCheckPointTime @ app.js:27950
checkNnLastCheckpointTime @ app.js:27905
restartAllStaleConfigComponents @ app.js:26478
ActionHelper.registeredActions.(anonymous function).handler @ vendor.js:31554
(anonymous function) @ vendor.js:23346
jQuery.event.dispatch @ vendor.js:3178
elemData.handle @ vendor.js:2854
app.js:56248 
{noformat}

Also for 'Stop' service operation:
{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
pullNnCheckPointTime @ app.js:27950
checkNnLastCheckpointTime @ app.js:27905
startStopPopup @ app.js:27842
stopService @ app.js:28176
doAction @ app.js:28662
ActionHelper.registeredActions.(anonymous function).handler @ vendor.js:31554
(anonymous function) @ vendor.js:23346
jQuery.event.dispatch @ vendor.js:3178
elemData.handle @ vendor.js:2854
app.js:56248 
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-21 14:04:34,15
13146826,Regression : Adding Atlas Metadata server' for HA on an host takes forever.,"Atlas is already installed in the cluster. To make Atlas HA , tried to add a new instance of Atlas Metadata server on host summary page. if Atlas Metadata server is added, ""Confirmation"" window keeps buffering forever.

refer to screenshot  provided

Workaround :
1. Inspect on the Confirm button
2. Remove the ""disabled"" attribute (Now Confirm button is enabled)
3. Submit the confirm button",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-21 13:05:37,19
13146822,Use Ambari CLI to specify which services should be setup for SSO integration,"Use Ambari CLI to specify which services should be setup for SSO integration.
{noformat:title=Example}
# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Do you want to configure SSO authentication [y/n] (y)?y
Enter Ambari Admin login: admin
Enter Ambari Admin password: admin
Provider URL [URL] (http://example.com):http://knox.ambari.apache.org:8080
Public Certificate pem (stored) (empty line to finish input):
AAAAB3NzaC1yc2EAAAADAQABAAABAQD....

Use SSO for all services [y/n] (y)? n
Use SSO for Ambari [y/n] (y)? y
Use SSO for HDFS [y/n] (y)? y
Use SSO for YARN [y/n] (y)? y
...
Use SSO for ZOOKEEPER [y/n] (y)? n
Do you want to configure advanced properties [y/n] (n) ?
Ambari Server 'setup-sso' completed successfully.
{noformat}
NOTE: this will require obtaining an Ambari administrator username and password to GET, PUT, and POST to the Ambari REST API.",pull-request-available security sso,['ambari-server'],AMBARI,Task,Major,2018-03-21 12:50:33,27
13146815,Add SSO-related configuration recommendations to the stack advisor,"Add SSO-related configuration recommendations to the stack advisor.

# Add a new action - {{recommend-configurations-for-sso}} - to query services for only SSO-related configuration changes
# Append to the stack advisor input data, the Ambari-stored SSO integration data (list of services that should enable SSO integration, proxy url, public key details, etc...).

",pull-request-available stack_advisor,['ambari-server'],AMBARI,Task,Major,2018-03-21 12:19:58,30
13146810,YARN cluster memory graph is not loading in Ambari 2.6,"Users are not able to see cluster memory graph under YARN. This was working before upgrade.

We were able to reproduce this internally on Ambari 2.6. We checked across 5 clusters and all of them exhibited same behaviour.

Seems like problem with the compute : mem_total._sum - mem_free._sum fails but mem_total._sum + mem_free._sum succeeds.

Also if we load mem_total._sum AND mem_free._sum individually, they come up fine.

Seems like issue when we do a 'subtract'.

Attaching UI Screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-21 11:35:47,19
13146797,Ticket cache file is delete before balancer access it during rebalance HDFS operation,"Since the rebalance command is run into background without waiting for finish, delete tgt cache file is executed right after subprocess start.

The subprocess fails due to missing tgt cache file because the file is deleted before the subprocess access it.",pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-03-21 10:35:33,28
13146586,"Ambari Upgrade:  Start All Services fails after AU at MapReduce2 Client Install with error ""Configuration parameter 'container-executor' was not found in configurations dictionary!""","After Ambari Upgrade from 2.6.0.0 to 2.7.0.0 , start all services operation fails at MapReduce2 Client Install with below error.

{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/mapreduce2_client.py"", line 91, in <module>
    MapReduce2Client().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/mapreduce2_client.py"", line 41, in install
    self.configure(env)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 122, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/mapreduce2_client.py"", line 50, in configure
    yarn(config_dir=config_dir)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/yarn.py"", line 166, in yarn
    content=InlineTemplate(params.container_executor_cfg_template)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/source.py"", line 150, in __init__
    super(InlineTemplate, self).__init__(name, extra_imports, **kwargs) 
  File ""/usr/lib/ambari-agent/lib/resource_management/core/source.py"", line 137, in __init__
    self.template = self.template_env.get_template(self.name)     
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/environment.py"", line 716, in get_template
    return self._load_template(name, self.make_globals(globals))
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/environment.py"", line 686, in _load_template
    template = self.cache.get(name)
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/utils.py"", line 405, in get
    return self[key]
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/utils.py"", line 448, in __getitem__
    rv = self._mapping[key]
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'container-executor' was not found in configurations dictionary!
{code}",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-03-20 16:16:00,27
13146561,Add namespace-specific layout for HDFS metrics,The layout of HDFS metrics section should look like as follows: non-namespace specific metrics on top and then a selector for namespace specific metrics below.,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-03-20 14:23:28,9
13146558,Use NameNode Upgrade Timeout Override In All Upgrade Packs,The upgrade timeout parameter for NameNode restarts is only defined in rolling upgrade packs. It should be used in all upgrade packs.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-20 14:13:16,31
13146549,Ambari Upgrade from 2.6.0.0 - 2.7.0.0 - Dashboard is unable to load and spinner keeps spinning,"After Ambari Upgrade from 2.6.0.0 to 2.7.0.0 , the Ambari UI Dashboard is unable to load. Spinner keeps spinning,
Seems like Wizard-Data API calls are returning null and not being handled properly on the UI. Ideally, the dashboard should be shown with widgets with No data (if AMS is down)
Additional Information:
Adding few more observed issues. These might be related to the originally raised issue.
# Quick Links are not shown for services. Spinner keeps spinning.
# Component information is not shown for some services. e.g. Oozie.
# For HDFS, YARN -> DataNodes, NodeManagers keep showing Loading and never loads.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-20 13:53:00,15
13146510,Python UT failure,"
    ERROR: test_get_service_base_dir (TestFileCache.TestFileCache)
    ----------------------------------------------------------------------
    ERROR 2018-03-20 06:58:54,744 - Python unit tests failed
    Traceback (most recent call last):
      File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
        return func(*args, **keywargs)
      File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/test/python/ambari_agent/TestFileCache.py"", line 72, in test_get_service_base_dir
        res = fileCache.get_service_base_dir(command, ""server_url_pref"")
      File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/main/python/ambari_agent/FileCache.py"", line 77, in get_service_base_dir
        if 'service_package_folder' in command['commandParams']:
    KeyError: 'commandParams'
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-20 10:33:28,5
13146475,Ambari Files view throws 500 ERROR while trying to upload/download to/from HDFS encrypted zone,"The problem is that the error does not show up properly on UI.

So it becomes difficult to understand.",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-03-20 07:30:50,38
13146455,Fix the following : 1. Host Component deletes are broken 2. Use resource Type in for getting resources from Mpack Modules.,"After Fix:

*1. Host Component Delete*:


{code:title=DELETE http://<AmbariServer>:8080/api/v1/clusters/c1/hosts/<HostName>/host_components/13}
{
  ""deleteResult"" : [
    {
      ""deleted"" : {
        ""key"" : ""component_id: 13""
      }
    }
  ]
}
{code}

*2. Use resource Type in for getting resources from Mpack Modules.*

*Reason:* When we are creating a Service, Host Component and Service Component when *name* provided is not same as the *type*, we need to use the ResourceType to refer the mpack to get the resource component from stack.

Eg: 

{code}
 {  
      ""ServiceInfo"":{  
         ""service_group_name"":""dev"",
         ""service_type"":""ZOOKEEPER"",
         ""service_name"":""ZOOKEEPER_dev""
      }
   }
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-20 05:00:18,43
13146415,livy.superusers is not getting configured correctly when Spark2 and Zeppelin are added as a service later on,"This bug was found during bugbash. I had a cluster where Spark2 and Zeppelin was not present. I added both these services via Add service wizard in Ambari. 

The string '$

{zeppelin-env/zeppelin_user}

$

{principal_suffix}

' is not getting replaced by correct zeppelin principal value and hence the %livy2 paragraphs are failing",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-20 00:02:36,24
13146369,Add SSO integration support information to service information via Ambari's REST API,"Add SSO integration support information to service information via Ambari's REST API.  This information should be usable by Ambari's search predicate feature.

New _read-only_ properties for (stack) services should be:
* *{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
** Information is expected to be determined by service's meta info (see BUG-98626)

New _read-only_ properties for installed services should be:
* *{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
** Information is expected to be determined by the service's meta info (see BUG-98626)
* *{{sso_integration_enabled}}* - Indicates whether the service is configured for SSO integration or not
** Information is expected to be determined by a value indicated in the service's meta info (see BUG-98626)
* *{{sso_integration_desired}}* - Indicates whether the service is chosen for SSO integration or not
** Information is expected to be in {{cluster-env/sso_enabled_services}} (see BUG-98451)

Examples:
{noformat:title=Get stack service details}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME"",
{
  ""href"" : "":URL"",
  ""StackServices"" : {
     ...
     ""sso_integration_supported"": ""false"",
     ...
  },
  ...
{noformat}

{noformat:title=Get installed service information}
GET /api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME
{
  ""href"" : "":URL"",
  ""ServiceInfo"" : {
    ""cluster_name"" : "":CLUSTER_NAME"",
    ...
    ""sso_integration_supported"": ""true"",
    ""sso_integration_enabled"": ""false"",
    ""sso_integration_desired"": ""false"",
     ...
    },
    ...
{noformat}

{noformat:title=List installed services that support SSO integration}
GET /api/v1/clusters/:CLUSTER_NAME/services?ServiceInfo/sso_integration_supported=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}

{noformat:title=List stack services that support SSO integration}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services?StackServices/credential_store_enabled=false
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-19 20:49:33,18
13146306,Add Ambari configuration data to stack/service advisor input data,"Add Ambari configuration data to stack/service advisor input data. Currently the {{ldap-configuration}} data is passed in to the stack/service advisor. However, generically all Ambari configuration data should be passed in.

For example, the current data set (in service.json) contains
{code:java}
{
  ...  ""services"" : [],
  ... ""configurations"" : \{ ... }, ""ambari-server-properties"" : \{ ... },
  ...  ""ldap-configurations"" : \{ ... }
}
 

{code}
 

""\{{ldap-configurations}}"" may or may not exist depending on if LDAP authentication/sync was setup. This should be treated generically and part of a larger block of data named ""\{{ambari-server-configuration}}"". For example:
{code:java}
{
...
""services"" : [],
...
""configurations"" :

{ ... },
""ambari-server-properties"" : { ... }

,
...
""ambari-server-configuration"" : {
""ldap-configurations"" :

{ ... }

,
...
}
}

 

 {code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-19 16:59:12,18
13146270,Install Wizard > Select Version page: the user cannot proceed with Redhat Satellite option,"STR:
* Launch Install Wizard
* On Select Version page, select ""Use Local Repository"" and then ""Use RedHat Satellite/Spacewalk""
* The Next button is disabled.  The Next button must be enabled in this scenario.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-19 15:19:16,19
13146254,Ambari should not set world-readable permissions for atlas-application.properties file,"Atlas uses the atlas-application.properties file and a clear-text-password can be found there. Therefore, OS permissions should be set to just allowing the atlas user to access it, but ambari resets them every time, making it not possible to restrict world-read access to the file.",pull-request-available security,['ambari-server'],AMBARI,Bug,Minor,2018-03-19 14:25:22,27
13146233,Enable Kerberos Wizard style edits,"# Reduce top padding for modal of wizard
# Test KDC connection success icon should be green
# the retry button on the 'Kerberize Cluster' page does not conform to the retry button styles found in other wizards

See screenshots.
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-19 14:02:19,19
13146180,Description for yarn.nodemanager.bind-host under YARN Configs needs to be improved,"Description for yarn.nodemanager.bind-host under YARN Configs says ""I think"" which won't give confidence to end users when reading this.

{code}
Default value is 0.0.0.0, when this is set the service will bind on all interfaces. I think these two options (blank, ""0.0.0.0"" sans quotes) should be the two available values, with blank as the default.
{code}

",pull-request-available,['ambari-sever'],AMBARI,Bug,Minor,2018-03-19 10:11:43,3
13146159, Wait to leave Safemode failed on EU ,"I can confirm that {{config _hostLevelParams/stack_version_ }}is not present at command.json
{code:java}
2018-03-15 17:17:15,891 - Task. Type: EXECUTE, Script: scripts/namenode.py - Function: wait_for_safemode_off
2018-03-15 17:17:16,026 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1040/hadoop/conf
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 383, in <module>
    NameNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 194, in wait_for_safemode_off
    wait_for_safemode_off(self.get_hdfs_binary(), afterwait_sleep=30, execute_kinit=True)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 72, in get_hdfs_binary
    return get_hdfs_binary(""hadoop-hdfs-namenode"")
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/utils.py"", line 397, in get_hdfs_binary
    import params
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/params_linux.py"", line 59, in <module>
    stack_version_formatted = format_stack_version(stack_version_unformatted)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/version.py"", line 42, in format_stack_version
    if value:
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in getattr
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'stack_version' was not found in configurations dictionary!{code}
May be related to mpack work",upgrade,['ambari-server'],AMBARI,Task,Blocker,2018-03-19 08:50:20,27
13146045,Maven cleanup,Clean up some of the duplications in {{pom.xml}}.,pull-request-available,[],AMBARI,Task,Major,2018-03-18 13:17:57,21
13146020,Upgrade maven dependency for jdeb and fic rpm autorequire tag value,Update jdeb from 1.0.1 to 1.6,pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-03-18 05:51:25,61
13145932,Database script issue when running MYSQL DDL scripts,"running the [script|https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/Ambari-DDL-MySQL-CREATE.sql] on MySQL, leads to error where user_authentication table is not present.

With this user cannot login to Ambari

{code:bash}
mysql> CREATE TABLE user_authentication (
    ->   user_authentication_id INTEGER,
    ->   user_id INTEGER NOT NULL,
    ->   authentication_type VARCHAR(50) NOT NULL,
    ->   authentication_key TEXT,
    ->   create_time TIMESTAMP NOT NULL DEFAULT 0,
    ->   update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    ->   CONSTRAINT PK_user_authentication PRIMARY KEY (user_authentication_id),
    ->   CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
    -> );
ERROR 1067 (42000): Invalid default value for 'create_time'
{code}

This is because the DEFAULT value is set to 0 for 'create_time' field.

Fix: Setting the DEFAULT to {{CURRENT_TIMESTAMP}}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-17 06:59:05,32
13145836,Make HDFS widgets namespace-scoped,Widgets on dashboard and HDFS summary page should be namespace-scoped.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-16 19:30:39,15
13145826,Remove mpack version from generated Service Group name,"Per back end request, do not include mpack version in service group name when automatically creating service groups.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-16 19:00:03,40
13145791,Use Mpack Operating System Endpoint for Mpack Repositories,"AMBARI-23147 has exposed the CRUD management of repositories based off of the <mpack/<id>/operating_system} endpoint since operating systems and repos are now directly tied to the {{repoinfo.xml}} which is shipped with the mpack. This replaces the VDF-centric mechanism which was used in the Ambari 2.6 line since repositories now contain multiple versions of mpacks and modules. The web client needs to begin using the new endpoint to add, edit, and remove operating systems.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-16 17:12:11,40
13145788, Add more logging for config update process,Additional logging is required to investigate intermittent issue with config updates.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-16 17:09:20,6
13145780,Wrong service_package_folder in serviceLevelParams during Express Upgrade from HDP 2.6 to HDP 3.0,"Upgrade HDP 2.6 to HDP 3.0 in Ambari 2.7; EU is supported only (i.e. no RU).

At upgrade time, when the services are starting we should use the new stack version's _service_package_folder_ instead of the old one. This was ok, until the change in FileCache.get_service_base_dir to get the base directory from 'serviceLevelParams' instead of 'commandParams'.

{code:java}
 ...
 ""serviceLevelParams"": {
 ""credentialStoreEnabled"": false, 
 ""status_commands_timeout"": 300, 
 ""version"": ""2.7.3"", 
 ""service_package_folder"": ""common-services/YARN/2.1.0.2.0/package""
 },
 ...
 ""commandParams"": {
 ""service_package_folder"": ""stacks/HDP/3.0/services/YARN/package"", 
 ""hooks_folder"": ""stack-hooks"", 
 ""clusterName"": ""cluster1"", 
 ""custom_command"": ""RESTART"", 
 ""upgrade_direction"": ""upgrade"", 
 ""upgrade_type"": ""nonrolling_upgrade"", 
 ""script"": ""scripts/historyserver.py"", 
 ""version"": ""3.0.0.2-155"", 
 ""forceRefreshConfigTagsBeforeExecution"": ""true"", 
 ""request_id"": ""17"", 
 ""command_timeout"": ""900"", 
 ""script_type"": ""PYTHON""
 },
{code}

The _service_package_folder_ is different; the one in _commandParams_ is the correct value.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-16 16:39:33,6
13145777,Provide Original Operating Systems from Mpack repoinfo.xml In Responses,"Management packs now have a response which includes the operating systems associated with the mpack:

{code:title=GET api/v1/mpacks/1}
{
  ""href"" : ""http://c7401.ambari.apache.org:8080/api/v1/mpacks/1"",
  ""MpackInfo"" : {
    ""id"" : 1,
    ""mpack_description"" : ""Hortonworks Data Platform Core"",
    ""mpack_id"" : ""hdpcore"",
    ""mpack_name"" : ""HDPCORE"",
    ""mpack_uri"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96/mpack.json"",
    ""mpack_version"" : ""1.0.0-b96"",
    ""registry_id"" : null,
    ""stack_name"" : """",
    ""stack_version"" : """"
  },
  ""operating_systems"" : [
    {
      ""href"" : ""http://c7401.ambari.apache.org:8080/api/v1/mpacks/1/operating_systems/redhat7"",
      ""OperatingSystems"" : {
        ""is_ambari_managed"" : true,
        ""mpack_id"" : 1,
        ""os_type"" : ""redhat7"",
        ""repositories"" : [
          {
            ""distribution"" : null,
            ""components"" : null,
            ""unique"" : true,
            ""tags"" : [ ],
            ""base_url"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96/foo"",
            ""os_type"" : ""redhat7"",
            ""repo_id"" : ""HDPCORE-1.0.0-b96"",
            ""repo_name"" : ""HDPCORE"",
            ""mirrors_list"" : null,
            ""default_base_url"" : null,
            ""ambari_managed"" : true
          },
          {
            ""distribution"" : null,
            ""components"" : null,
            ""unique"" : false,
            ""tags"" : [ ],
            ""base_url"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDP-UTILS-1.1.0.22/foo"",
            ""os_type"" : ""redhat7"",
            ""repo_id"" : ""HDP-UTILS-1.1.0.21"",
            ""repo_name"" : ""HDP-UTILS"",
            ""mirrors_list"" : null,
            ""default_base_url"" : null,
            ""ambari_managed"" : true
          }
        ]
      }
    }
  ],
  ""version"" : [ ]
}
{code}

These repositories are editable. If the UI (or any client for that matter) needed to see the original OS's and repositories which shipped with the mpack, they would not be able to. We will now expose a new key call {{default_operating_systems}} which will include this information.",pull-request-available,[],AMBARI,Task,Blocker,2018-03-16 16:27:03,31
13145776,Fix TestMySqlServer error while executing unit tests on MacOS/Darwin,"Fix TestMySqlServer error while executing unit tests on MacOS/Darwin.

 
{noformat}
ERROR: test_configure_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
File ""/Users/rlevas/github/ambari/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
return func(*args, **keywargs)
File ""/Users/rlevas/github/ambari/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 84, in test_configure_secured
self.assert_configure_secured()
File ""/Users/rlevas/github/ambari/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 168, in assert_configure_secured
mode = 0755,
File ""/Users/rlevas/github/ambari/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 345, in assertResourceCalled
self.assertEquals(kwargs, resource.arguments)
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 515, in assertEqual
assertion_func(first, second, msg=msg)
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 831, in assertDictEqual
if d1 != d2:
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/resource_management/core/source.py"", line 59, in __eq__
and ((self.name.startswith(os.sep) and self.name == other.name) or self.get_content() == other.get_content()))
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/resource_management/core/source.py"", line 78, in get_content
return self.read_file(path)
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_family_impl.py"", line 84, in thunk
fn_id = fn_id_base + ""."" + OSCheck.get_os_family()
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 277, in get_os_family
os_family = OSCheck.get_os_type()
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 242, in get_os_type
return OSCheck.get_alias(OSCheck._get_os_type(), OSCheck._get_os_version())[0]
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 248, in _get_os_type
dist = OSCheck.os_distribution()
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 202, in os_distribution
distribution = platform.linux_distribution()
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/platform.py"", line 329, in linux_distribution
return _dist_try_harder(distname,version,id)
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/platform.py"", line 205, in _dist_try_harder
info = open('/var/adm/inst-log/info').readlines()
IOError: [Errno 2] No such file or directory: '/var/adm/inst-log/info'{noformat}
 
*Cause*
This is caused by the following mock
{code:java}
@patch(""os.path.exists"", MagicMock(return_value=True))
{code}
 Which allow a later call on the Python 2.7 library to return {{True}}, causing {{/var/adm/inst-log/info}} to appear to exist and then be read. Since {{/var/adm/inst-log/info}} does not exist, this fails, as seen above. 

*Solution*
Mock the OsCheck function to avoid this.
 ",pull-request-available unit-test,['ambari-server'],AMBARI,Task,Major,2018-03-16 16:24:18,30
13145775,hdfs-site.xml is not getting updated after config change and restart via Ambari,"1. Changed 'dfs.datanode.data.dir' in hdfs-site.xml to new value.
2. Restarted HDFS service.
3. The value of 'dfs.datanode.data.dir' is not updated in hdfs-site.xml.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-16 16:23:15,6
13145750,Create sso-configuration category in Ambari Configurations data,"Create \{{sso-configuration}} category in Ambari Configurations data and allow for the {{ambari.sso.enabled_services}} property to be added to it. This property is used to declare what services are expected to be configured for SSO and is expected to be a comma-delimited list of services or ""{{*}}"" to indicate all services.

Examples:
{code:title=All services}
""ambari.sso.enabled_services"":""*""
{code}
{code:title=Only Ambari}
""ambari.sso.enabled_services"":""Ambari""
{code}
{code:title=Ambari, Ranger, and Atlas}
""ambari.sso.enabled_services"":""Ambari, Ranger, Atlas""
{code}

Each service in the set of services should have indicated it supports SSO (see AMBARI-23252) else it will silently be ignored.

This value should be set via Ambari's REST API or a Blueprint.

Upon setting this value via the Ambari REST API, it is expected that internal logic will be triggered to ensure the relevant services in the list are configured for SSO or not configured for SSO as the case may be.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-16 15:09:59,30
13145726,"Ambari web - Install wizard ""Review"" step does not show HDP-GPL repo","Steps to reproduce: 
# Install latest ambari server (2.7.0.2-30)
# Upload a custom vdf in step2
# Proceed till the ""Review"" step. The summary misses ""HDP-GPL"" repo url.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-16 14:14:15,19
13145687,For Ranger Service /spnego reference principal is not getting updated in config,"For UI based installation, /spnego reference principal used in Ranger Service kerberos descriptor is not getting updated in ranger.spnego.kerberos.keytab configuration.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-16 12:45:17,19
13145599,Ambari-agent setting permission of .hash files in /var/lib/ambari-agent/cache to 666,"Ambari-agent is setting permission of some files in /var/lib/ambari-agent/cache folder to 666 

{code:java}

[root@asnaik3 ambari-agent]# find cache -printf '%m %p\n' |grep 666
666 cache/custom_actions/.hash
666 cache/common-services/AMBARI_INFRA/0.1.0/package/.hash
666 cache/common-services/AMBARI_METRICS/0.1.0/package/.hash
666 cache/common-services/HDFS/2.1.0.2.0/package/.hash
666 cache/common-services/HIVE/0.12.0.2.0/package/.hash
666 cache/common-services/LOGSEARCH/0.5.0/package/.hash
666 cache/common-services/OOZIE/4.0.0.2.0/package/.hash
666 cache/common-services/PIG/0.12.0.2.0/package/.hash
666 cache/common-services/SLIDER/0.60.0.2.2/package/.hash
666 cache/common-services/SPARK/1.2.1/package/.hash
666 cache/common-services/SPARK2/2.0.0/package/.hash
666 cache/common-services/TEZ/0.4.0.2.1/package/.hash
666 cache/common-services/YARN/2.1.0.2.0/package/.hash
666 cache/common-services/ZOOKEEPER/3.4.5/package/.hash
666 cache/stacks/HDP/2.0.6/hooks/.hash
666 cache/host_scripts/.hash
{code}


root cause : https://github.com/apache/ambari/blob/trunk/ambari-agent/src/main/python/ambari_agent/FileCache.py
{code:java}
hash_file = os.path.join(directory, self.HASH_SUM_FILE)
    try:
      with open(hash_file, ""w"") as fh:
        fh.write(new_hash)
      os.chmod(hash_file, 0o666)
{code}

Need to change the permission to os.chmod(hash_file, 0o644)
",ambari-agent pull-request-available python,['ambari-agent'],AMBARI,Bug,Major,2018-03-16 06:59:15,3
13145506,Load modules and components into a map when reading the mpack.json,Load the modules and components into a map when reading the mpack.json,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-15 22:13:19,56
13145477,Mpack should have both displayName and description,update the mpack and software registry schema as well as update the mpack and software registry apis to expose these fields.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-15 20:15:55,56
13145465,Allow Ambari Server to Setup SSO for the entire stack using the CLI,"Today enabling SSO requires visiting each component that supports SSO and adding configuration entries to each. This task is to enable a single entry point via the Ambari CLI to configure SSO for each service that supports it.

Changes to the ambari-server setup-sso CLI are needed allow configuration of all SSO-capable services using that single CLI. This facility can be used to enable, disable, and reconfigure SSO integration.

*Proposed implementation:*

Services are to declare they support SSO integration by indicating in the service's \{{metainfo.xml}} file as follows: 
{code}
<sso>
 <supported>true</supported>
 <enabledConfiguration>config-type/sso.enabled.property</enabledConfiguration>
</sso>
{code}

The stack/service advisor will be used to retrieve the recommended configurations needed by a service to set up SSO integration. A special stack advisor action will be added to ensure only SSO-related recommendations are returned upon request. The new action name is ""{{recommend-configurations-for-sso}}"". Ambari (or common) SSO information will be provided to the stack advisor via the input data under the label ""sso-configuration"". This information may be used by the stack advisor when creating recommendations.

Ambari will store details on which services should be enabled for SSO so it _knows_ how to behave when SSO integration is enabled and new services are added. This data will be stored within Ambari's configuration data under the category of {{sso-configuration}}. The list of services to have SSO integration turned on will be stored in the property named {{ambari.sso.enabled_services}}. The value will be a comma-delimited list of service names, or ""{{*}}"" to indicate all services that support SSO integration.

The Ambari REST API entry point for installed services ({{/api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME}}) is to be enhance by adding the following properties:
* *\{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
* *\{{sso_integration_enabled}}* - Indicates whether the service is configured for SSO integration or not
* *\{{sso_integration_desired}}* - Indicates whether the service is chosen for SSO integration or not

The Ambari REST API entry point for stack services ({{/api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME}}) is to be enhance by adding the following properties:
* *\{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not

When producing a list of installed services that support SSO integration in the CLI, the Ambari REST API is to be used to query for the relevant service names. Once the user selects the set of services to enable SSO for (or all), the Ambari REST API is to be used to set the value of the Ambari configuration \{{sso-configuration/ambari.sso.enabled_services}}. Upon setting this, logic is triggered in the backend to query the stack advisor for SSO-related configuration recommendations which will be automatically applied. This will potentially yield new configuration versions and require services to be manually restarted.

When adding new services, the \{{sso-configuration/ambari.sso.enabled_services}} value is to be checked to see if the new service is on the list of services to have SSO integration enabled. If so, and the service has a SSO descriptor, its configuration will be updated as needed before the service is started.

In a Blueprint scenario, it is expected that the user first sets up Ambari for SSO integration using the {{ambari-server setup-sso}} CLI. The Blueprint is expected to set the relevant properties needed to enable SSO integration per service. However, if SSO details were set up, the stack advisor may recommend relevant changes which may be applied depending on the Blueprint settings.",SSO sso,['ambari-server'],AMBARI,Epic,Major,2018-03-15 19:16:06,30
13145464,Update service metainfo to declare SSO integration support,"Update service metainfo to declare SSO integration support. The following tag may be optionally set in a service's {{metainfo.xml}} file:
{code:java}
<sso>
 <supported>true</supported>
 <enabledConfiguration>config-type/sso.enabled.property</enabledConfiguration>
</sso>
{code}
 ",pull-request-available sso,['ambari-server'],AMBARI,Task,Critical,2018-03-15 19:10:50,30
13145461,Convert a cluster-env API call to use the cluster settings API.,Updated a couple places where an API call was used to load cluster settings to the client relying on cluster-env to use the cluster settings API.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-15 18:45:43,40
13145428,Integrate Instance manager into HDPCORE mpack,"HDFS, HADOOP-CLIENTS, Zookeeper and zookeeper clients should use instance manager to determine bin and conf directories location
 ",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-15 16:35:42,28
13145416,Ambari Setup with external postgres db failed on configuring database.,"Suse deployments fails during ambari-server setup with error:

{noformat}
Configuring ambari database...
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Enter full path to custom jdbc driver:
Traceback (most recent call last):
 File ""/usr/sbin/ambari-server.py"", line 1042, in <module>
 mainBody()
 File ""/usr/sbin/ambari-server.py"", line 1012, in mainBody
 main(options, args, parser)
 File ""/usr/sbin/ambari-server.py"", line 962, in main
 action_obj.execute()
 File ""/usr/sbin/ambari-server.py"", line 79, in execute
 self.fn(*self.args, **self.kwargs)
 File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1199, in setup
 _setup_database(options)
 File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1012, in _setup_database
 dbmsAmbari.setup_database()
 File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 154, in setup_database
 self._setup_remote_database()
 File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 162, in _setup_remote_database
 if self.ensure_jdbc_driver_installed(properties):
 File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 177, in ensure_jdbc_driver_installed
 path_to_custom_jdbc_driver = get_validated_string_input(""Enter full path to custom jdbc driver: "", None, None, None, False, False)
 File ""/usr/lib/ambari-server/lib/ambari_server/userInput.py"", line 87, in get_validated_string_input
 if not input.strip():
AttributeError: 'NoneType' object has no attribute 'strip'
{noformat}


Executed command during test:
{noformat}
ambari-server setup --database=postgres --databasehost=<hostName> --databaseport=<port> --databasename=<databaseName> --databaseusername=<userName> --databasepassword=<password> --java-home=/base/tools/jdk1.8.0_112 -s
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-15 15:41:15,6
13145410,Pass Mpack ID In Agent Commands For Installation Updates,"When an mpack is initially registered in Ambari, we will create {{NOT_INSTALLED}} records for it on every host. During a normal installation, the resource provider will set the correct rows to a value of {{INSTALLING}}.

However, on a blueprint deployment or initial cluster deployment, this doesn't happen. As a result, the value stays at {{NOT_INSTALLED}} until the installation commands come back.

If for some reason the installation command fails (aborts, times out, etc), then there will be no structured output indicating the mpack which was attempted. Therefore, we cannot find the right values to lookup. 

This Jira will allow the mpack ID to be set on the command so that it doesn't need to be specifically reported by an agent.",pull-request-available,[],AMBARI,Task,Critical,2018-03-15 15:13:07,31
13145376,Invalid value for zeppelin.config.fs.dir property,"Property zeppelin.config.fs.dir should not be available for zeppelin, if it was installed with stack < HDP-2.6.3. Also it should be added with value ""conf"", if zeppelin installed with stack HDP-2.6.3 and higher.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-15 13:30:52,24
13145308,Adding missing properties to OneFS mpack,"Following properties should be added to the appropriate config
 * hadoop.security.token.service.use_ip should be false when kerberos is enabled
 * dfs.checksum.type should always be null

 * dfs.datanode.http.address/dfs.datanode.https.address = 0.0.0.0:8082/0.0.0.0:8080 [?]

 * -yarn.scheduler.capacity.node-locality-delay = 0-

 * dfs.client-write-packet-size = 131072",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-15 09:13:07,18
13145182,"Add ""tag"" property for widget resource in API",Needed for NameNode Federation development.,pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-03-14 21:12:28,55
13145156,Incorrect property for NN namespace value is used,Currently {{ClusterId}} property is used on UI to identify the namespaces. Values from {{hdfs-site}} configs should be used instead.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-14 19:54:46,15
13145086,Connection dropped during task execution caused stage abort,We should tolerate agent reconnect for now when agent was not restarted.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-14 16:25:12,6
13145085,SQL errors in Oracle schema create script,"During deploy on debian 7 with oracle database ambari server reported  many errors related to SQL queries:

{noformat}
14 Mar 2018 10:51:08,721 ERROR [ExecutionScheduler_QuartzSchedulerThread] JobStoreTX:3652 - Couldn't rollback jdbc connection. No more data to read from socket
java.sql.SQLRecoverableException: No more data to read from socket
{noformat}

{noformat}
Internal Exception: java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist

Error Code: 942
Call: INSERT INTO hostcomponentstate (id, current_state, last_live_state, upgrade_state, version, host_id, service_name, cluster_id, component_name) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        bind => [9 parameters bound]
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-14 16:25:10,21
13145075,Issue with zeppelin after upgrade,"I performed the express upgrade to latest fenton build which has the fix for AMBARI-23113

Observed below issues after upgrading cluster from ambari-2.5.1.0 with 2.6.1.0-129 to ambari-2.6.2.0-96+HDP-2.6.5.0-162

1) Interpreter settings are not stored in HDFS. '/user/zeppelin/conf/interpreter.json' doesnt exist in hdfs.

2) Notebook authorisations are not stored in HDFS '/user/zeppelin/conf/notebook-authorization.json' doesnt exist in hdfs.

3) Look at the interpreter page on Zeppelin and see that there are duplicate interpreters

4) Previously created notebooks are not copied into hdfs.
found root cause, there is no property""zeppelin.config.fs.dir"" = ""conf"" in zeppelin-config. After i've added it, ""conf"" directory was created in HDFS and interpreter.json was there. Also tested zeppelin, it really using this interpreter.json from HDFS. Looks like it works fine. For fresh install it will work fine because ""zeppelin.config.fs.dir"" by default is added to zeppelin 0.7.0 (zeppelin-config) but for upgrade we also should add this property.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-14 16:08:26,13
13145057,Broken view of Configs after saving changes,See screenshot,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-14 15:10:12,19
13145027,Set full name (cn) when creating user accounts in FreeIPA server,"Set full name (cn) when creating user accounts in FreeIPA server.

{code:title=Example IPA CLI}
ipa user-add user1 --principal user1@AMBARI.APACHE.ORG --first user1 --last user1 --cn ""user1""
{code}

The new argument is ""\{{--cn ""user1""}}.

This may allow for compatibility with IPA server version 3.x",freeipa kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-03-14 13:23:50,30
13145022,Server service_check does not get executed for new mpacks,"During install wizard, service checks for clients were being called but not for servers.
 We need it to be the other way around. We have disabled the client service check but need to investigate why the server checks are disabled.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-14 12:57:47,28
13144887,Ambari Server login fails due to TimelineMetricsCacheSizeOfEngine error in AMS perf branch.,"{code}
java.lang.ClassCastException: org.apache.ambari.server.controller.metrics.timeline.cache.TimelineMetricsCacheSizeOfEngine cannot be cast to net.s
f.ehcache.pool.SizeOfEngine
        at net.sf.ehcache.CacheManager.createSizeOfEngine(CacheManager.java:2068)
        at net.sf.ehcache.CacheManager.doInit(CacheManager.java:453)
{code}",pull-request-available,['ambari-metrics'],AMBARI,Bug,Blocker,2018-03-13 23:19:37,39
13144834,Dependency check should ignore unknown services,"{{BlueprintValidatorImpl}} throws NPE if encounters an unknown component.

{noformat}
	at org.apache.ambari.server.topology.BlueprintValidatorImpl.validateHostGroup(BlueprintValidatorImpl.java:250)
	at org.apache.ambari.server.topology.BlueprintValidatorImpl.validateTopology(BlueprintValidatorImpl.java:70)
	at org.apache.ambari.server.topology.BlueprintImpl.validateTopology(BlueprintImpl.java:332)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-13 19:58:09,21
13144833,Stack Mpack link broken in stacks api,"http://{{ambari_server}}:8080/api/v1/mpacks/3
version is empty

GET [http://ambari_server:8080/api/v1/stacks/<mpack>/versions/1.0.0-b141|http://172.22.102.182:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b141]
mpack is empty",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-13 19:52:59,56
13144820,Ambari-agent fails to connect to server with two_way_auth enabled,"
    ERROR 2018-03-13 17:15:04,264 security.py:122 - Could not connect to wss://ctr-e138-1518143905142-94896-01-000002.hwx.site:8441/agent/stomp/v1
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/security.py"", line 113, in establish_connection
        conn.start()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/connect.py"", line 46, in start
        self.transport.start()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/transport.py"", line 109, in start
        self.attempt_connection()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/adapter/websocket.py"", line 89, in attempt_connection
        self.ws.connect()
      File ""/usr/lib/ambari-agent/lib/ambari_ws4py/client/__init__.py"", line 216, in connect
        self.sock.connect(self.bind_addr)
      File ""/usr/lib64/python2.7/ssl.py"", line 869, in connect
        self._real_connect(addr, False)
      File ""/usr/lib64/python2.7/ssl.py"", line 860, in _real_connect
        self.do_handshake()
      File ""/usr/lib64/python2.7/ssl.py"", line 833, in do_handshake
        self._sslobj.do_handshake()
    SSLError: [SSL: SSLV3_ALERT_BAD_CERTIFICATE] sslv3 alert bad certificate (_ssl.c:579)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-13 18:55:45,5
13144767,Customise Services : Alignment not proper for Select Config group Pop up and overriden text box,"There are two issues w.r.t alignment

*Issue#1*
1) Go to All Configurations Tab under Customise Services Step of Install Wizard.
2) Select any property say under HDFS 'DataNode directories permission' . Click '+' to override.
3) Select a new config group
4) The new text box which appears to provide overridden value is misaligned with original one.

*Issue#2*
Following same steps as in Issue#1. Check the warning 'You are changing not default group, please select config group to which you want to save dependent configs from other services +Show Details+' at page top. 
Click Show Details link.
A pop up opens. The text which appears on the pop up  is not aligned properly.

Attaching screenshots for both issues.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-13 15:47:32,19
13144551,"Update logic to skip ""*_CLIENT"" services Service Checks (SC), as there is no SC defined for them. ","- With advent of Mpacks, we have separated the *CLIENT* components from *SERVER* counterparts and will be separate services itself. 
 -- For example: *ZOOKEEPER_CLIENT* will be a separate service having category *CLIENT* and *ZOOKEEPER_SERVER* will be a separate service having category *MASTER*. (Similarly, for others like HDFS_CLEINT and so on.)

- As there are no Service Checks explicitly defined for the CLIENT Services, we need to update the code to skip running any SC calls for them.

[~dsen] Thanks for the pointers.

CC [~jluniya] | [~mradhakrishnan]








",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-12 22:54:49,43
13144454,StackAdvisor Error while enabling back security : invalid literal for int() with base 10,"StackAdvisor Error while enabling back security : invalid literal for int() with base 10

 

STR: 
1) Install cluster with Ambari 2.6.1.0
2) Upgrade to Ambari 2.6.2.0
3) Regenerate keytabs for missing components and start all services
4) Disable security
5) Enable NN and RM HA
6) Enable Security. On enabling security. On configure identities page, there is a stackadvisor error

 

{code}

12 Mar 2018 06:29:08,144 INFO [ambari-client-thread-67] StackAdvisorRunner:71 - advisor script stderr: Traceback (most recent call last):
 File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 173, in <module>
 main(sys.argv)
 File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 121, in main
 result = stackAdvisor.recommendConfigurations(services, hosts)
 File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 776, in recommendConfigurations
 calculation(configurations, clusterSummary, services, hosts)
 File ""/var/lib/ambari-server/resources/scripts/./../stacks/HDP/2.6/services/stack_advisor.py"", line 125, in recommendZEPPELINConfigurations
 if compare_versions(full_stack_version, '2.6.3.0') >= 0:
 File ""/usr/lib/ambari-server/lib/resource_management/libraries/functions/version.py"", line 83, in compare_versions
 return cmp(_normalize(v1, desired_segments=max_segments), _normalize(v2, desired_segments=max_segments))
 File ""/usr/lib/ambari-server/lib/resource_management/libraries/functions/version.py"", line 34, in _normalize
 return [int(x) for x in v_list]
ValueError: invalid literal for int() with base 10: ''
12 Mar 2018 06:29:08,145 WARN [ambari-client-thread-67] AbstractResourceProvider:97 - Error occured during recommendation
org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Stack Advisor reported an error: ValueError: invalid literal for int() with base 10: ''
StdOut file: /var/run/ambari-server/stack-recommendations/4/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/4/stackadvisor.err
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner.runScript(StackAdvisorRunner.java:96)
 at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:297)
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:113)
 at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:92)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:298)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:37)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:73)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:90)
 at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:59)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:287)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:132)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:125)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
 at org.eclipse.jetty.server.Server.handle(Server.java:370)
 at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
 at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
 at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
 at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
 at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
 at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
 at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
 at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
 at java.lang.Thread.run(Thread.java:745)
12 Mar 2018 06:29:08,147 ERROR [ambari-client-thread-67] BaseManagementHandler:61 - Caught a system exception while attempting to create a resource: Stack Advisor reported an error: ValueError: invalid literal for int() with base 10: ''
StdOut file: /var/run/ambari-server/stack-recommendations/4/stackadvisor.out

{code}

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-12 20:58:40,7
13144440,Enable or disable SSO using Ambari CLI with options,"To enable or disable SSO using the Ambari CLI, the user needs to answer each prompt. This is not convenient for automated tools. So the following command line options should be available for use. If any mandatory option is not specified via the command line, the user should be prompted for a value.

Global SSO options:
 * {{--sso-enabled}}

if {{sso-enabled}} is ""true"":
 * {{--sso-provider-url}}
 * {{--sso-public-cert-file}}
 * {{--sso-jwt-cookie-name}}
 ** optional, if not supplied, the documented default value will be used
 * {{--sso-jwt-audience-list}}
 ** optional, if not supplied, the documented default value will be used

{noformat:title=Examples}
ambari-server setup-sso --help

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem --sso-jwt-cookie-name=ambari-jtw 

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem --sso-jwt-cookie-name=ambari-jtw --sso-jwt-audience-list=ambari

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem

ambari-server setup-sso --sso-enabled=false
{noformat}",SSO pull-request-available security,['ambari-server'],AMBARI,Task,Critical,2018-03-12 20:30:14,27
13144384,Customise Services: Alignment issue while adding new custom password property,"When a new custom property is added whose Type is Password , the two textboxes that appear are not properly aligned with Remove button.
The length of the text boxes should be reduced so that both textboxes(password/confirm password) fit in the same line along with remove button.

Attached screenshot from old and new UI.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-12 17:35:48,19
13144370,Kerberos 'Edit' Properties not conforming to the new style,"The 'Edit' link of Kerberos properties page does not confirm to the new UI design styles. All the actions are buttons on this page, and it is easy to miss the edit link.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-12 16:49:33,19
13144352,NN Federation Wizard: rename custom command,Rename BOOTSTRAP custom command to BOOTSTRAP_STANDBY.,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-03-12 15:58:31,9
13144342,The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text,"The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text
Also if there are more than one properties overriden then the they are not shown in new lines.


Attached Screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-12 15:33:13,19
13144332,Use Agent Command Response To Update Mpack Install State,"A continuation of AMBARI-23205, this will allow commands (like {{INSTALL}} and install_packages) to update the management pack installation state. They currently are stuck at {{NOT_INSTALLED}}.",pull-request-available,[],AMBARI,Task,Critical,2018-03-12 15:07:50,31
13144330,Refactor host_version Tracking for MPacks,"As part of the ongoing effort to remove repository version, the ability to track installation of management packs must change as well. We previously were tracking the installation of a repository on every host, marking it as {{OUT_OF_SYNC}}, {{CURRENT}}, etc, depending on the state.

With management pack meta-RPMs, things get quite a bit easier. We no longer have to worry about specific versions being reported back per install nor do we need to worry about per-component installation states. Instead, there is only the need to track which mpacks have been installed on a given host. 

This also allows us to simplify the states that we track. The proposal is to track the following states:
- {{NOT_REQUIRED}}
- {{NOT_INSTALLED}}
- {{INSTALLING}}
- {{INSTAL_FAILED}}
- {{INSTALLED}}

The {{NOT_REQUIRED}} state might actually not be needed. It's going to depend on how what queries might be needed to retrieve information about service groups (and their associated mpacks). ",pull-request-available,[],AMBARI,Task,Critical,2018-03-12 15:00:54,31
13144305,"Ambari Upgrade : Schema Upgrade Fails with error ""Foreign key constraint is incorrectly formed""","Ambari Schema Upgrade from 2.6.X to 2.7.0.0 fails with below exception . 
A blocker for Ambari Upgrades testing.

{code:java}
12 Mar 2018 00:51:37,879  INFO [main] SchemaUpgradeHelper:424 - Upgrading schema to target version = 2.7.0.0
12 Mar 2018 00:51:37,883  INFO [main] SchemaUpgradeHelper:433 - Upgrading schema from source version = 2.6.1
12 Mar 2018 00:51:37,888  INFO [main] SchemaUpgradeHelper:163 - Upgrade path: [{ upgradeCatalog: sourceVersion = 2.6.1, targetVersion = 2.6.2 }, { upgradeCatalog: sourceVersion = 2.6.2, targetVersion = 2.7.0 }, { upgradeCatalog: sourceVersion = null, targetVersion = 2.7.0 }, { upgradeCatalog: sourceVersion = null, targetVersion = 2.7.0 }]
12 Mar 2018 00:51:37,889  INFO [main] SchemaUpgradeHelper:200 - Executing DDL upgrade...
12 Mar 2018 00:51:37,911  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE topology_host_request ADD status VARCHAR(255)
12 Mar 2018 00:51:37,948  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE topology_host_request ADD status_message VARCHAR(1024)
12 Mar 2018 00:51:37,982  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD status VARCHAR(255) NOT NULL DEFAULT 'PENDING'
12 Mar 2018 00:51:38,017  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD display_status VARCHAR(255) NOT NULL DEFAULT 'PENDING'
12 Mar 2018 00:51:38,055  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE request ADD display_status VARCHAR(255) NOT NULL DEFAULT 'PENDING'
12 Mar 2018 00:51:38,091  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE request ADD user_name VARCHAR(255)
12 Mar 2018 00:51:38,127  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE host_role_command ADD ops_display_name VARCHAR(255)
12 Mar 2018 00:51:38,314  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE hostcomponentdesiredstate DROP COLUMN security_state
12 Mar 2018 00:51:38,351  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE hostcomponentstate DROP COLUMN security_state
12 Mar 2018 00:51:38,387  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE servicedesiredstate DROP COLUMN security_state
12 Mar 2018 00:51:38,426  INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE ambari_configuration (category_name VARCHAR(100) NOT NULL, property_name VARCHAR(100) NOT NULL, property_value VARCHAR(255)) ENGINE=INNODB
12 Mar 2018 00:51:38,435  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE ambari_configuration ADD CONSTRAINT PK_ambari_configuration PRIMARY KEY (category_name,property_name)
12 Mar 2018 00:51:38,443  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE hostcomponentstate ADD last_live_state VARCHAR(255) DEFAULT 'UNKNOWN'
12 Mar 2018 00:51:38,476  WARN [main] DBAccessorImpl:965 - user_authentication_tmp table doesn't exists, skipping
12 Mar 2018 00:51:38,477  INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE user_authentication_tmp (user_authentication_id BIGINT NOT NULL, user_id BIGINT NOT NULL, authentication_type VARCHAR(50) NOT NULL, authentication_key LONGTEXT, create_time DATETIME, update_time DATETIME) ENGINE=INNODB
12 Mar 2018 00:51:38,483  INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE user_authentication (user_authentication_id BIGINT NOT NULL, user_id BIGINT NOT NULL, authentication_type VARCHAR(50) NOT NULL, authentication_key LONGTEXT, create_time DATETIME, update_time DATETIME) ENGINE=INNODB
12 Mar 2018 00:51:38,490  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE user_authentication ADD CONSTRAINT PK_user_authentication PRIMARY KEY (user_authentication_id)
12 Mar 2018 00:51:38,555  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE user_authentication ADD CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
12 Mar 2018 00:51:38,572 ERROR [main] DBAccessorImpl:882 - Error executing query: ALTER TABLE user_authentication ADD CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
12 Mar 2018 00:51:38,572 ERROR [main] DBAccessorImpl:882 - Error executing query: ALTER TABLE user_authentication ADD CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
java.sql.SQLException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:996)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:303)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:280)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
12 Mar 2018 00:51:38,572  WARN [main] DBAccessorImpl:521 - Add FK constraint failed, constraintName = FK_user_authentication_users, tableName = user_authentication
12 Mar 2018 00:51:38,572 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.sql.SQLException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:996)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:303)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:280)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
12 Mar 2018 00:51:38,573 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.sql.SQLException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:996)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:303)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:280)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
        ... 1 more
{code}

Possibly the issue is in different datatypes of user_id(int(11)) column in users and user_id(bigint(20)) in user_authentication table : See below:

{code:java}

MariaDB [ambaricustom]> desc user_authentication;
+------------------------+-------------+------+-----+---------+-------+
| Field                  | Type        | Null | Key | Default | Extra |
+------------------------+-------------+------+-----+---------+-------+
| user_authentication_id | bigint(20)  | NO   | PRI | NULL    |       |
| user_id                | bigint(20)  | NO   |     | NULL    |       |
| authentication_type    | varchar(50) | NO   |     | NULL    |       |
| authentication_key     | longtext    | YES  |     | NULL    |       |
| create_time            | datetime    | YES  |     | NULL    |       |
| update_time            | datetime    | YES  |     | NULL    |       |
+------------------------+-------------+------+-----+---------+-------+
6 rows in set (0.01 sec)

MariaDB [ambaricustom]> desc users;
+-----------------------+---------------+------+-----+---------------------+-------+
| Field                 | Type          | Null | Key | Default             | Extra |
+-----------------------+---------------+------+-----+---------------------+-------+
| user_id               | int(11)       | NO   | PRI | NULL                |       |
| principal_id          | bigint(20)    | NO   | MUL | NULL                |       |
| create_time           | timestamp     | NO   |     | current_timestamp() |       |
| ldap_user             | int(11)       | NO   |     | 0                   |       |
| user_type             | varchar(100)  | NO   |     | LOCAL               |       |
| user_name             | varchar(100)  | NO   | MUL | NULL                |       |
| user_password         | varchar(255)  | YES  |     | NULL                |       |
| active                | int(11)       | NO   |     | 1                   |       |
| active_widget_layouts | varchar(1024) | YES  |     | NULL                |       |
+-----------------------+---------------+------+-----+---------------------+-------+
9 rows in set (0.00 sec)
{code}
",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-03-12 14:09:45,18
13144298,Add mpack-instance-manager package dependency for ambari-agent,mpack-instance-manager package should be installed with ambari-agent package,pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-03-12 13:38:47,28
13144284,Websocket client should use secure endpoint when connected over https,See screenshot.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-12 12:38:22,19
13144203,Infra Solr - utility script to run infra-solr custom commands (migrate/backup/restore),"Added new cli tool for infra solr client:
/usr/lib/ambari-infra-solr-client/migrationHelper.py
{code:java}
Usage: migrationHelper.py [options]

Options:
  -h, --help            show this help message and exit
  -H HOST, --host=HOST  hostname for ambari server
  -P PORT, --port=PORT  port number for ambari server
  -c CLUSTER, --cluster=CLUSTER
                        name cluster
  -s, --ssl             use if ambari server using https
  -u USERNAME, --username=USERNAME
                        username for accessing ambari server
  -p PASSWORD, --password=PASSWORD
                        password for accessing ambari server
  -a ACTION, --action=ACTION
                        backup | restore | migrate
  -f, --force           force index upgrade even if it's the right version
  --index-location=INDEX_LOCATION
                        location of the index backups
  --backup-name=BACKUP_NAME
                        backup name of the index
  --collection=COLLECTION
                        solr collection
  --version=INDEX_VERSION
                        lucene index version for migration (6.6.2 or 7.2.1)
  --request-tries=REQUEST_TRIES
                        number of tries for BACKUP/RESTORE status api calls in
                        the request
  --request-time-interval=REQUEST_TIME_INTERVAL
                        time interval between BACKUP/RESTORE status api calls
                        in the request
  --request-async       skip BACKUP/RESTORE status api calls from the command
  --shared-fs           shared fs for storing backup (will create index
                        location to <path><hostname>)
  --solr-hosts=SOLR_HOSTS
                        comma separated list of solr hosts
  --disable-solr-host-check
                        Disable to check solr hosts are good for the
                        collection backups
  --core-filter=CORE_FILTER
                        core filter for replica folders
{code}
Example: (backup)
{code:java}
/usr/lib/ambari-infra-solr-client/migrationHelper.py --action backup -H c7301.ambari.apache.org -c cl1 --index-location /tmp/ranger --collection ranger_audits --backup-name ranger
{code}
",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-03-12 01:31:43,29
13144201,Infra Solr - add status check for backup and restore custom commands,Follow-up change for AMBARI-23182 ,pull-request-available,"['ambari-infra', 'ambari-server']",AMBARI,Bug,Major,2018-03-12 01:30:39,29
13144038,Ambari Agent unit test failures on Python 2.7,"The following 2 tests consistently fail on CentOS7 with Python 2.7.5:

{noformat:title=mvn -am -pl ambari-agent -Drat.skip clean test}
======================================================================
FAIL: test_load_definitions_noFile (TestAlertSchedulerHandler.TestAlertSchedulerHandler)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py"", line 314, in test_load_definitions_noFile
    self.assertEquals(definitions, [])
AssertionError: Lists differ: [<ambari_agent.alerts.port_ale... != []

First list contains 1 additional elements.
First extra element 0:
<ambari_agent.alerts.port_alert.PortAlert object at 0x384c990>

- [<ambari_agent.alerts.port_alert.PortAlert object at 0x384c990>]
+ []

======================================================================
FAIL: test_generation (TestCertGeneration.TestCertGeneration)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""ambari-agent/src/test/python/ambari_agent/TestCertGeneration.py"", line 49, in test_generation
    self.assertTrue(os.path.exists(self.certMan.getAgentKeyName()))
AssertionError: False is not true

----------------------------------------------------------------------
Ran 342 tests in 17.726s

FAILED (failures=2)
{noformat}

The first one also fails on Ubuntu 16 with Python 2.7.12:

{noformat}
FAIL: test_load_definitions_noFile (TestAlertSchedulerHandler.TestAlertSchedulerHandler)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py"", line 314, in test_load_definitions_noFile
    self.assertEquals(definitions, [])
AssertionError: Lists differ: [<ambari_agent.alerts.port_ale... != []

First list contains 1 additional elements.
First extra element 0:
<ambari_agent.alerts.port_alert.PortAlert object at 0x7fbc86c56fd0>

- [<ambari_agent.alerts.port_alert.PortAlert object at 0x7fbc86c56fd0>]
+ []

----------------------------------------------------------------------
Ran 342 tests in 18.052s

FAILED (failures=1)
{noformat}

Also seen in Jenkins:
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/1083/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/950/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/938/",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-03-10 10:24:26,21
13143998,Add reverted changes back (reverted in : AMBARI-23183) after merging branch 'branch-feature-AMBARI-14714-ui' into 'branch-feature-AMBARI-14714' (AMBARI-23193),"UI changes:
- AMBARI-22945 added code for passing servoce_name and service_group-name while creating Host Components. But, in AMBARI-23183, we reverted the UI related changes to have a smooth merge from Ui branch.
  -- We are bringing that change in with a slight tweak of reading *mpackname* to be passed as Service_group-name directly, instead of earlier where we were passing the default '*core*' SG read from *App.get('defaultServiceGroupName')*.


- AMBARI-23177 :  Yarn-MR separation and minor deployment fixes

",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2018-03-09 23:29:42,43
13143984,Missing LdapFacade in ambari-server check-database,"{noformat:title=ambari-server check-database}
Using python  /usr/bin/python
Checking database
ERROR: Exiting with exit code 1.
REASON: Database check failed to complete: No errors and warnings were found.
...
Caused by: com.google.inject.CreationException: Unable to create injector, see the following errors:

1) No implementation for org.apache.ambari.server.ldap.service.LdapFacade was bound.
  while locating org.apache.ambari.server.ldap.service.LdapFacade
    for field at org.apache.ambari.server.controller.internal.AmbariServerLDAPConfigurationHandler.ldapFacade(AmbariServerLDAPConfigurationHandler.java:43)
  at org.apache.ambari.server.controller.ControllerModule.bindByAnnotation(ControllerModule.java:568)

1 error
	at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)
	at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)
	at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)
	at com.google.inject.Guice.createInjector(Guice.java:99)
	at com.google.inject.Guice.createInjector(Guice.java:73)
	at com.google.inject.Guice.createInjector(Guice.java:62)
	at org.apache.ambari.server.checks.DatabaseConsistencyChecker.main(DatabaseConsistencyChecker.java:101)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-09 21:48:29,30
13143974,Values instance has no attribute 'master_key' during ambari-server reset,"{noformat:title=ambari-server reset -s}
Using python  /usr/bin/python
Resetting ambari-server
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
**** WARNING **** You are about to reset and clear the Ambari Server database. This will remove all cluster host and configuration information from the database. You will be required to re-configure the Ambari server and re-run the cluster wizard.
Are you SURE you want to perform the reset [yes/no] (yes)?
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 1033, in <module>
    mainBody()
  File ""/usr/sbin/ambari-server.py"", line 1003, in mainBody
    main(options, args, parser)
  File ""/usr/sbin/ambari-server.py"", line 953, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1326, in reset
    _reset_database(options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1052, in _reset_database
    dbmsAmbari = factory.create(options, properties)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 501, in create
    dbmsConfig = desc.create_config(options, properties, dbId)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 81, in create_config
    return self.fn_create_config(options, properties, self.storage_key, dbId)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 881, in createPGConfig
    return PGConfig(options, properties, storage_type)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 391, in __init__
    super(PGConfig, self).__init__(options, properties, storage_type)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 94, in __init__
    self.database_password = DBMSConfig._read_password_from_properties(properties, options)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 222, in _read_password_from_properties
    database_password = decrypt_password_for_alias(properties, JDBC_RCA_PASSWORD_ALIAS, options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 940, in decrypt_password_for_alias
    return read_passwd_for_alias(alias, masterKey, options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 911, in read_passwd_for_alias
    if options is not None and options.master_key is not None and options.master_key:
AttributeError: Values instance has no attribute 'master_key'
{noformat}

{noformat:title=grep passwd /etc/ambari-server/conf/ambari.properties}
server.jdbc.user.passwd=${alias=ambari.db.password}
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-09 20:57:23,30
13143940,NoClassDefFoundError on Files View on S3,"{code:java}
06 Mar 2018 08:52:07,987 ERROR [ambari-client-thread-6083] ContainerResponse:423 - The exception contained within MappableContainerException could not be mapped to a response, re-throwing to the HTTP container
java.lang.NoClassDefFoundError: Could not initialize class com.amazonaws.ClientConfiguration
	at org.apache.hadoop.fs.s3a.S3ClientFactory$DefaultS3ClientFactory.createS3Client(S3ClientFactory.java:75)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:205)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2795)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2829)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2811)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:179)
	at org.apache.ambari.view.utils.hdfs.HdfsApi$1.run(HdfsApi.java:77)
	at org.apache.ambari.view.utils.hdfs.HdfsApi$1.run(HdfsApi.java:75)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.execute(HdfsApi.java:513)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.execute(HdfsApi.java:489)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.<init>(HdfsApi.java:75)
	at org.apache.ambari.view.utils.hdfs.HdfsUtil.getHdfsApi(HdfsUtil.java:157)
	at org.apache.ambari.view.utils.hdfs.HdfsUtil.connectToHDFSApi(HdfsUtil.java:129)
	at org.apache.ambari.view.commons.hdfs.HdfsService.getApi(HdfsService.java:98)
	at org.apache.ambari.view.commons.hdfs.FileOperationService.listdir(FileOperationService.java:100)
	at sun.reflect.GeneratedMethodAccessor443.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205){code}
 

This is caused by an incorrect version from jackson-core.

In /var/lib/ambari-server/resources/views/work/FILES\{1.0.0}/WEB-INF/lib/ there is jackson-core-2.2.3.jar (instead of 2.9.4).

In 2.2.3 there was no requiresPropertyOrdering() method in com.fasterxml.jackson.core.JsonFactory.

jackson-core, jackson-databind and jackson-annotations should all have the same version (2.9.4).",pull-request-available,"['ambari-server', 'ambari-views']",AMBARI,Bug,Major,2018-03-09 18:29:28,18
13143917,Use service configuration API in install wizard.,Update the install wizard to use the new service level configuration API to configure services.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-09 16:03:26,40
13143901,Background Ops: Make the operations column wider (shrink the other columns to make space),"Current implementation attached. 
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-09 14:19:02,19
13143885,UI Changes required for Manage Versions /Register Versions Page,"# Space required between Name: and Version number on Register Versions Page (Screen Shot 2018-02-09 at 12.12.06 PM.png)
# Space required between '-' and Remove text next to Base URL text boxes.(Screen Shot 2018-02-09 at 12.13.09 PM.png)
# Alignment between Version Details and Service Details  in that version needs to be corrected in Versions Page (Screen Shot 2018-02-09 at 12.13.51 PM.png)
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-09 12:43:23,19
13143878,"Add custom actions for: format NN, format ZK and bootStrapStandBy","Automate the actions needed for the wizard so UI can make a call to perform
the commands on the new NN hosts and setup federation.

Favoring automation here over manual steps like in the past NN HA wizard
forced user to do this instead of Ambari doing it for them.

",pull-request-available,[],AMBARI,Bug,Major,2018-03-09 11:57:15,5
13143861,Provide a new CLI option for setup-ldap to bypass question when there is no auth method configured (or different than LDAP),"As part of the job in AMBARI-22905 a change has been made to ask a question in case there is no configured authentication method or the one is different than LDAP (previously that question has been asked only if PAM was configured).

To allow any automated tests to bypass that question we should introduce a new CLI option. ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-09 10:46:51,27
13143856,Metrics Collector Install failed on HA cluster,"We have the very first run on Atlantic run using Ambari 2.7.0.2-60 and
HDP-3.0.0.2-130.  
Noticed in HA cluster that deploy has failed at Metrics collector install with
below error

    
    
    
    Caught an exception while executing custom service command: <class 'ambari_agent.AgentException.AgentException'>: 'Script /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py does not exist'; 'Script /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py does not exist'
    
    

Artifacts can be found
[here](http://logserver.eng.hortonworks.com/?prefix=qelogs/nat/86939/yarn-
ha/split-1/nat-yc-r7-schs-yarn-ha-re-
re/deploy_logs/logs/ctr-e138-1518143905142-62501-01-000002.hwx.site/)  
Live cluster: <http://172.27.15.145:8080/> (lifetime 48hrs)  
Could you please help take a look

",pull-request-available,[],AMBARI,Bug,Major,2018-03-09 10:23:15,5
13143717,"Revert UI changes made to branch : ""branch-feature-AMBARI-14714"".","- As we are going for a merge of ""branch-feature-AMBARI-14714"" (Backend changes) and ""branch-feature-AMBARI-14714-ui "" (UI changes) branch to and fro, removing the bare minimum changes made in  ""branch-feature-AMBARI-14714"" ambari-web code base, to make the merge smooth.

- UI code changes undone:
   -- AMBARI-22945 : Added UI code in *step8_controller.js* to pass the *default* service_group_name (=core) and service_name for Host component creation, as the ""branch-feature-AMBARI-14714"" branch assumes that service_group created would be names *""core""*. But, ""branch-feature-AMBARI-14714-ui"" branch has moved forward using *stack_name* and *version* as the name for the service_group (eg: HDPCore-version). Thus, this UI related changes anyways needs to be revisited.
  -- AMBARI-23177 : YARN and MR separation changes made in *ambari-web/app/models/stack_service.js*. We will bring it back after the merge.

CC [~mradhakrishnan] | [~jluniya]",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-08 23:10:29,43
13143576,Hide downgrade option from stack upgrade wizard,Downgrade button should be removed from the stack upgrade wizard for upgrade paths from HDP-2.6 to 3.0,express_upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-03-08 15:04:38,18
13143453,Hive view 2.0 does not parse the TAB delimited CSV files while using 'Upload Table',"Problem:
When trying to Upload table from Hive view 2.0 from a TAB delimited file, the data is not parsed as expected.
Same feature works as expected in Hive view 1.5.0.

Attached output from Hive view 2.0 and output from Hive view 1.5.0.

From the code, we do see difference in the code for src/main/java/org/apache/ambari/view/hive20/resources/uploads/UploadService.java.

Debug from Hive view 2.0:
{code:java}
27 Feb 2018 20:27:28,169 DEBUG [HiveViewActorSystem-akka.actor.default-dispatcher-4] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] OperationController:53 - } java.util.HashMap
27 Feb 2018 20:27:28,169 DEBUG [HiveViewActorSystem-akka.actor.default-dispatcher-4] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] HiveActor:41 - Message submitted: 927343c0-5ed6-435c-be60-37448ac1a8b0
27 Feb 2018 20:27:30,925  INFO [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] UploadService:484 - isFirstRowHeader : false, inputFileType : CSV
27 Feb 2018 20:27:30,934  INFO [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] OpenCSVParser:50 - setting delimiter as T
27 Feb 2018 20:27:30,934  INFO [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] OpenCSVParser:56 - setting Quote char : ""
27 Feb 2018 20:27:30,934  INFO [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] OpenCSVParser:62 - setting escapeChar : \
27 Feb 2018 20:27:30,939  INFO [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] Parser:75 - generating preview for : ParseOptions{options={OPTIONS_CSV_QUOTE="", OPTIONS_CSV_DELIMITER=T, HEADER=NONE, FILE_TYPE=CSV, OPTIONS_CSV_ESCAPE_CHAR=\}}
27 Feb 2018 20:27:30,939 DEBUG [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] Parser:83 - Illegal number of preview columns supplied null
27 Feb 2018 20:27:30,943 DEBUG [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] ParseUtils:152 - error while parsing as timestamp string column1	column2	column3
java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]
	at java.sql.Timestamp.valueOf(Timestamp.java:204)
.....
27 Feb 2018 20:27:30,948  INFO [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] Parser:149 - datatype detected for column 0 : STRING
27 Feb 2018 20:27:30,949 DEBUG [ambari-client-thread-38] [HIVE 2.0.0 AUTO_HIVE20_INSTANCE] Parser:159 - return headers : [ColumnInfo{name='column1', type='STRING', precision=null, scale=null, comment='null'}]
{code}
Debug from Hive view 1.5.0:
{code:java}
27 Feb 2018 20:13:15,881  INFO [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] UploadService:499 - isFirstRowHeader : true, inputFileType : CSV
27 Feb 2018 20:13:15,918  INFO [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] OpenCSVParser:50 - setting delimiter as 	
27 Feb 2018 20:13:15,919  INFO [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] OpenCSVParser:56 - setting Quote char : ""
27 Feb 2018 20:13:15,919  INFO [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] OpenCSVParser:62 - setting escapeChar : \
27 Feb 2018 20:13:15,928  INFO [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] Parser:75 - generating preview for : ParseOptions{options={OPTIONS_CSV_QUOTE="", OPTIONS_CSV_DELIMITER=	, HEADER=FIRST_RECORD, FILE_TYPE=CSV, OPTIONS_CSV_ESCAPE_CHAR=\}}
27 Feb 2018 20:13:15,929 DEBUG [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] Parser:83 - Illegal number of preview columns supplied null
27 Feb 2018 20:13:15,936 DEBUG [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] ParseUtils:152 - error while parsing as timestamp string AD
java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]
	at java.sql.Timestamp.valueOf(Timestamp.java:204)
.........
27 Feb 2018 20:13:15,961  INFO [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] Parser:149 - datatype detected for column 2 : STRING
27 Feb 2018 20:13:15,962 DEBUG [ambari-client-thread-38] [HIVE 1.5.0 AUTO_HIVE_INSTANCE] Parser:159 - return headers : [ColumnDescriptionImpl[name : column1, type : STRING, position : 0, precision : null, scale : null], ColumnDescriptionImpl[name : column2, type : STRING, position : 1, precision : null, scale : null], ColumnDescriptionImpl[name : column3, type : STRING, position : 2, precision : null, scale : null]] 
{code}
 ",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-03-08 06:32:42,38
13143428,AMS restart failed in embedded mode,"While ams use embedded mode,you can start it for the first time.But,when you restart it,it starts failed.The log of hbase master is shown above.The reason is due to the existed hbase.zookeeper.property.dataDir before.So,we must delete it everytime before you want to restart ams in embedded mode.",pull-request-available,[],AMBARI,Bug,Major,2018-03-08 03:13:13,47
13143398,Some Minor Fixes For Failing Tests Due to Repo Version Refactor,The ongoing work for AMBARI-14714 has left the feature branch tests in a wreck. This is a commit which fixes a bunch of tests temporarily to help us prepare for a merge with trunk in the future.,pull-request-available,[],AMBARI,Task,Major,2018-03-08 00:10:45,31
13143376,Fix for PUT Host Component API failing because validation check fails.,"- Validation check for HOST COMPONENT PUT API looks for having cluster_name, component_name, component_type, service_name, service_group_name, and host_name.

- But, as of now, *service_name*, *service_group* names are not fetched/set during find operation.  ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-07 22:27:57,43
13143366,Add storm topology metric expressions for custom downsampling config.,"This is a minor config change that needs to be made for the work done in  AMBARI-23008  to have storm metrics aggregated differently.

",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-03-07 21:38:23,39
13143357,Assign Client/Slaves page pagination control static,pagination controls should not be hidden behind the horizontal scroll,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-07 21:01:46,34
13143349,Build fails for Debian (jdeb plugin not found),"{code:java}
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] No plugin found for prefix 'jdeb' in the current project and in the plugin groups [org.sonatype.plugins, org.apache.maven.plugins, org.codehaus.mojo] available from the repositories [local (/grid/0/jenkins/.m2/repository), public (http://nexus-private.hortonworks.com/nexus/content/groups/public)] -> [Help 1]
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] 
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] 
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:08:15 2018/03/07 06:08:14 INFO : [ERROR] [Help 1]{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-07 20:47:31,42
13143327,Ambari logo is missing.,See the screenshot.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-07 20:17:52,6
13143278,StackOverflowError thrown during cluster creation,"Seeing a StackOverflowException when creating clusters from blueprints:

{code}
.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)
	at org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilter(AmbariBasicAuthenticationFilter.java:116)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:121)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:210)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:199)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:137)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.StackOverflowError
	at org.eclipse.persistence.mappings.DatabaseMapping.getAttributeValueFromObject(DatabaseMapping.java:657)
	at org.eclipse.persistence.mappings.ForeignReferenceMapping.getAttributeValueFromObject(ForeignReferenceMapping.java:1010)
	at org.eclipse.persistence.mappings.ForeignReferenceMapping.buildClone(ForeignReferenceMapping.java:309)
	at org.eclipse.persistence.internal.descriptors.ObjectBuilder.populateAttributesForClone(ObjectBuilder.java:4165)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.populateAndRegisterObject(UnitOfWorkImpl.java:3676)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.cloneAndRegisterObject(UnitOfWorkImpl.java:1026)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.cloneAndRegisterObject(UnitOfWorkImpl.java:955)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.registerExistingObject(UnitOfWorkImpl.java:3965)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.registerExistingObject(UnitOfWorkImpl.java:3894)
	at org.eclipse.persistence.mappings.ObjectReferenceMapping.buildUnitofWorkCloneForPartObject(ObjectReferenceMapping.java:111)
	at org.eclipse.persistence.mappings.ObjectReferenceMapping.buildCloneForPartObject(ObjectReferenceMapping.java:73)
	at org.eclipse.persistence.internal.indirection.NoIndirectionPolicy.cloneAttribute(NoIndirectionPolicy.java:79)
	at org.eclipse.persistence.mappings.ForeignReferenceMapping.buildClone(ForeignReferenceMapping.java:311)
	at org.eclipse.persistence.internal.descriptors.ObjectBuilder.populateAttributesForClone(ObjectBuilder.java:4165)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.populateAndRegisterObject(UnitOfWorkImpl.java:3676)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.cloneAndRegisterObject(UnitOfWorkImpl.java:1026)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.cloneAndRegisterObject(UnitOfWorkImpl.java:955)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.registerExistingObject(UnitOfWorkImpl.java:3965)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.registerExistingObject(UnitOfWorkImpl.java:3894)
	at org.eclipse.persistence.mappings.CollectionMapping.buildElementUnitOfWorkClone(CollectionMapping.java:308)
	at org.eclipse.persistence.mappings.CollectionMapping.buildElementClone(CollectionMapping.java:321)
	at org.eclipse.persistence.internal.queries.ContainerPolicy.addNextValueFromIteratorInto(ContainerPolicy.java:215)
	at org.eclipse.persistence.mappings.CollectionMapping.buildCloneForPartObject(CollectionMapping.java:223)
	at org.eclipse.persistence.internal.indirection.NoIndirectionPolicy.cloneAttribute(NoIndirectionPolicy.java:79)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-07 17:41:07,31
13143263,NN Federation Wizard: implement step4,Provide step4 with ability to see progress of operations performed to add new namespace.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-07 16:16:47,9
13143233,Config dependencies alert bar not displayed properly while scrolling down the page,"Config dependencies alert bar not displayed properly while scrolling down the page.

Attaching screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-07 14:02:35,19
13143205,Unable to add a service in second go from Ambari-UI,"Installed Ambari using build 2.7.0.2-70 and HDP-3.0.0.2-150.
Added services HDFS, Yarn, Hive, HBase, Zookeeper and Solr.
Now trying to Add Knox / Ranger, after customizing the configurations and clicked on the {{Next}} button UI is stuck and unable to reach the deploy screen.
Browser console shows error {{unable to load modification handler for RANGER}}.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-07 12:33:42,19
13143034,Invalid storage property value for zeppelin in stack HDP 2.6,"For stacks lower than HDP 2.6.3, property {{zeppelin.notebook.storage}} should have value {{org.apache.zeppelin.notebook.repo.VFSNotebookRepo}}. For stacks HDP 2.6.3 and higher this property should have value {{org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo}}. As of now for all stacks HDP 2.6.x by default storage property will have value FileSystemNotebookRepo, it's not correct. So it should be fixed.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-06 23:46:43,24
13142969,Fix NPE in Host Components query.,"*serviceComponentHostRequest* may not get instantiated if *HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID* is null.

{code:title=/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostComponentResourceProvider.java}
  private ServiceComponentHostRequest getRequest(Map<String, Object> properties) {
    ServiceComponentHostRequest serviceComponentHostRequest = null;
    if (properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID) != null) {
      Long hostComponentId = properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID) instanceof String ?
              Long.parseLong((String) properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID)) :
              (Long) properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID);

      serviceComponentHostRequest = new ServiceComponentHostRequest(
              (String) properties.get(HOST_COMPONENT_CLUSTER_NAME_PROPERTY_ID),
              (String) properties.get(HOST_COMPONENT_SERVICE_GROUP_NAME_PROPERTY_ID),
              (String) properties.get(HOST_COMPONENT_SERVICE_NAME_PROPERTY_ID),
              hostComponentId,
              (String) properties.get(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID),
              (String) properties.get(HOST_COMPONENT_COMPONENT_TYPE_PROPERTY_ID),
              (String) properties.get(HOST_COMPONENT_HOST_NAME_PROPERTY_ID),
              (String) properties.get(HOST_COMPONENT_DESIRED_STATE_PROPERTY_ID));
    }
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-06 20:48:48,43
13142953,Typo in configs.py,"Typo in configs.py


ERROR File extension .sh """"""doesn't"""""" supported",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-03-06 19:41:10,60
13142915,Remove Required Group Name and Expose Lifecycle to API,The group name is no longer a required field.  Expose lifecycle via API,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-03-06 16:42:37,33
13142890,Use cluster settings API in install wizard,Update install wizard to use the new cluster settings API to configure the cluster.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-06 15:39:10,40
13142875,BootStrap Standby NameNode page does not conform to new UI,"While navigating through the Manage Journal Nodes wizard, the 'BootStrap Standby NameNode' is not as per the new Ambari style.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-06 14:56:26,19
13142866,Clean up dup vars in params in Slider,Clean up dup vars in params in Slider,pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-03-06 14:24:11,60
13142865,Ambari agent should trust Ambari server's SSL certificate,"Ambari agent should trust Ambari server's SSL certificate.  

When using Python 2.7 and above, the agent tends to fail connecting with the Ambari server with a {{CERTIFICATE_VERIFY_FAILED}} error.   

To solve this, else tell Python to no verify certificates (which is insecure):
{noformat:title=/etc/python/cert-verification.cfg}
[https]
verify=disable
{noformat}
See https://access.redhat.com/articles/2039753

Or import the Ambari server's SSL cert into the truststore used by Python, which is more secure. 

",security ssl,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2018-03-06 14:22:48,27
13142855,Configurations attributes reset after running Add Service wizard,"h3. STEPS TO REPRODUCE
# Install the latest Sandbox docker (to re-reproduce the issue docker is easier)
# Add ""hadoop.security.group.mapping.ldap.bind.password"" in Custom core-site with the value ""admin-password"" with Property Type *""PASSWORD""*, and save
!image-2018-02-27-20-00-12-533.png|thumbnail! 
# From Add Service Wizard, add some service (I tested with Nifi and Solr but any service should be OK)
# After Wizard, go to HDFS => Configs and search ldap.bind.password, it shows text input box rather than password input box",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-06 13:43:28,19
13142846,"Installation of MySQL fails on CentOS 7.2, should install mariadb","I'm seeing the following error during HDP 3 beta installation.

    
    
    
    Traceback (most recent call last):
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 268, in _call_with_retries
        code, out = func(cmd, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    
    The above exception was the cause of the following exception:
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/HIVE/3.0.0.3.0/package/scripts/mysql_server.py"", line 68, in <module>
        MysqlServer().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/HIVE/3.0.0.3.0/package/scripts/mysql_server.py"", line 37, in install
        self.install_packages(env)
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 831, in install_packages
        retry_count=agent_stack_retry_count)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 53, in action_install
        self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py"", line 251, in install_package
        self.checked_call_with_retries(cmd, sudo=True, logoutput=self.get_logoutput())
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 251, in checked_call_with_retries
        return self._call_with_retries(cmd, is_checked=True, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 268, in _call_with_retries
        code, out = func(cmd, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    stdout:   /var/lib/ambari-agent/data/output-49.txt
    2018-02-16 18:25:54,593 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-16 18:25:54,605 - Using hadoop conf dir: /usr/hdp/3.0.0.0-814/hadoop/conf
    2018-02-16 18:25:54,608 - Group['livy'] {}
    2018-02-16 18:25:54,609 - Group['spark'] {}
    2018-02-16 18:25:54,610 - Group['hdfs'] {}
    2018-02-16 18:25:54,610 - Group['zeppelin'] {}
    2018-02-16 18:25:54,610 - Group['hadoop'] {}
    2018-02-16 18:25:54,611 - Group['users'] {}
    2018-02-16 18:25:54,611 - Group['knox'] {}
    2018-02-16 18:25:54,612 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,614 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,615 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,617 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,618 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,620 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-16 18:25:54,621 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,623 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,624 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,626 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-16 18:25:54,627 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,629 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,630 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,632 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,633 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,635 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-16 18:25:54,636 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,639 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-16 18:25:54,650 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-16 18:25:54,651 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-16 18:25:54,653 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,657 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,658 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-16 18:25:54,672 - call returned (0, '1015')
    2018-02-16 18:25:54,674 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1015'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-16 18:25:54,683 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1015'] due to not_if
    2018-02-16 18:25:54,684 - Group['hdfs'] {}
    2018-02-16 18:25:54,685 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-16 18:25:54,687 - FS Type: 
    2018-02-16 18:25:54,687 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-16 18:25:54,731 - File['/usr/hdp/3.0.0.0-814/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-16 18:25:54,732 - Writing File['/usr/hdp/3.0.0.0-814/hadoop/conf/hadoop-env.sh'] because contents don't match
    2018-02-16 18:25:54,733 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-16 18:25:54,771 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-16 18:25:54,786 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-16 18:25:54,787 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-16 18:25:54,788 - Repository with url http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-814 is not created due to its tags: set([u'GPL'])
    2018-02-16 18:25:54,788 - Repository['HDP-UTILS-1.1.0.22-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-16 18:25:54,796 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-1]\nname=HDP-UTILS-1.1.0.22-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-16 18:25:54,796 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-16 18:25:54,798 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,943 - Skipping installation of existing package unzip
    2018-02-16 18:25:54,944 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,960 - Skipping installation of existing package curl
    2018-02-16 18:25:54,961 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,976 - Skipping installation of existing package hdp-select
    2018-02-16 18:25:54,995 - Skipping stack-select on MYSQL_SERVER because it does not exist in the stack-select package structure.
    2018-02-16 18:25:55,435 - Using hadoop conf dir: /usr/hdp/3.0.0.0-814/hadoop/conf
    2018-02-16 18:25:55,471 - call['ambari-python-wrap /usr/bin/hdp-select status hive-server2'] {'timeout': 20}
    2018-02-16 18:25:55,523 - call returned (0, 'hive-server2 - 3.0.0.0-814')
    2018-02-16 18:25:55,525 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-16 18:25:55,567 - File['/var/lib/ambari-agent/cred/lib/CredentialUtil.jar'] {'content': DownloadSource('http://will-hdp-1.field.hortonworks.com:8080/resources/CredentialUtil.jar'), 'mode': 0755}
    2018-02-16 18:25:55,569 - Not downloading the file from http://will-hdp-1.field.hortonworks.com:8080/resources/CredentialUtil.jar, because /var/lib/ambari-agent/tmp/CredentialUtil.jar already exists
    2018-02-16 18:25:55,570 - checked_call[('/usr/jdk64/jdk1.8.0_112/bin/java', '-cp', u'/var/lib/ambari-agent/cred/lib/*', 'org.apache.ambari.server.credentialapi.CredentialUtil', 'get', 'javax.jdo.option.ConnectionPassword', '-provider', u'jceks://file/var/lib/ambari-agent/cred/conf/mysql_server/hive-site.jceks')] {}
    2018-02-16 18:25:56,859 - checked_call returned (0, 'SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\nFeb 16, 2018 6:25:56 PM org.apache.hadoop.util.NativeCodeLoader <clinit>\nWARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nhive')
    2018-02-16 18:25:56,873 - Package['mysql-community-release'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:57,038 - Installing package mysql-community-release ('/usr/bin/yum -d 0 -e 0 -y install mysql-community-release')
    2018-02-16 18:25:58,091 - Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    2018-02-16 18:25:58,091 - Failed to install package mysql-community-release. Executing '/usr/bin/yum clean metadata'
    2018-02-16 18:25:58,386 - Retrying to install package mysql-community-release after 30 seconds
    2018-02-16 18:26:37,003 - Skipping stack-select on MYSQL_SERVER because it does not exist in the stack-select package structure.
    
    Command failed after 1 tries
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-06 13:22:56,5
13142633,Expose Repository CRUD via the Mpack Endpoint,"With the removal of version definitions and repository versions, we need a new way of exposing and managing repositories. Since repositories are now associated with management packs, it makes sense to expose these off of the {{mpack}} endpoint.

The question is whether we want to continue our model of having OperatingSystems read-only and adding sub-resources of repositories off of the OS or changing to something simpler like having a single object serialized which contains all of the information. 

h5. Scenario 1: Operating Systems are 1st Class Resources
I think that having each repository as a sub-resource is a bit of an overkill. We certainly don't need that level of granularity. But we could make the {{operating_system}} the lowest level so that you could update all repositories for a given operating system at once. For example:

{code:title=http://c7001.ambari.apache.org:8080/api/v1/mpacks/1}
{
{
  ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/mpacks/1"",
  ""MpackInfo"": {
    ""id"": 1,
    ""mpack_description"": ""Hortonworks Data Platform Core"",
    ""mpack_id"": ""hdpcore"",
    ""mpack_name"": ""HDPCORE"",
    ""mpack_uri"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96/mpack.json"",
    ""mpack_version"": ""1.0.0-b96"",
    ""operating_systems"": [
      {
        ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7"",
        ""OperatingSystems"": {
          ""os_type"": ""redhat7"",
          ""stack_name"": ""HDPCORE"",
          ""stack_version"": ""1.0.0-b96""
        }
      },
      {
        ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/ubuntu12"",
        ""OperatingSystems"": {
          ""os_type"": ""ubuntu12"",
          ""stack_name"": ""HDPCORE"",
          ""stack_version"": ""1.0.0-b96""
        }
      }
    ]
  }
}
{code}

This allows us to use the operating system type (like {{redhat7}} or {{ubuntu12}}) as a key so we can get more information about the OS:
{code:title=http://c7401.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7}
{
  ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7?fields=repositories/*"",
  ""OperatingSystems"": {
    ""os_type"": ""redhat7"",
    ""stack_name"": ""HDPCORE"",
    ""stack_version"": ""1.0.0-b96""
  },
  ""repositories"": [
    {
      ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7/repositories/HDP-UTILS-1.1.0.21"",
      ""Repositories"": {
        ""base_url"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDP-UTILS-1.1.0.22/"",
        ""components"": null,
        ""default_base_url"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDP-UTILS-1.1.0.22/"",
        ""distribution"": null,
        ""mirrors_list"": null,
        ""os_type"": ""redhat7"",
        ""repo_id"": ""HDP-UTILS-1.1.0.21"",
        ""repo_name"": ""HDP-UTILS"",
        ""stack_name"": ""HDPCORE"",
        ""stack_version"": ""1.0.0-b96"",
        ""tags"": [],
        ""unique"": false
      }
    },
    {
      ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7/repositories/HDPCORE-1.0.0-b96"",
      ""Repositories"": {
        ""base_url"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96"",
        ""components"": null,
        ""default_base_url"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96"",
        ""distribution"": null,
        ""mirrors_list"": null,
        ""os_type"": ""redhat7"",
        ""repo_id"": ""HDPCORE-1.0.0-b96"",
        ""repo_name"": ""HDPCORE"",
        ""stack_name"": ""HDPCORE"",
        ""stack_version"": ""1.0.0-b96"",
        ""tags"": [],
        ""unique"": false
      }
    }
  ]
}
{code}

This maintains the current structure of {{operating_systems}} being the wrapper around each repository. Each {{operating_system}} contains the actual repositories:

{code:title=http://c7001.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7/repositories/HDPCORE-1.0.0-b96}
{
  ""href"" : ""http://c7001.ambari.apache.org:8080/api/v1/stacks/HDPCORE/versions/1.0.0-b96/operating_systems/redhat7/repositories/HDPCORE-1.0.0-b96"",
  ""Repositories"" : {
    ""base_url"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96"",
    ""components"" : null,
    ""default_base_url"" : ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96"",
    ""distribution"" : null,
    ""mirrors_list"" : null,
    ""os_type"" : ""redhat7"",
    ""repo_id"" : ""HDPCORE-1.0.0-b96"",
    ""repo_name"" : ""HDPCORE"",
    ""stack_name"" : ""HDPCORE"",
    ""stack_version"" : ""1.0.0-b96"",
    ""tags"" : [ ],
    ""unique"" : false
  }
}
{code}

- Allows querying by the OS type (not sure if we need or want that)
- Allows updating all repositories for an OS type in 1 call
- Requires 1 call per operating system to update all repos

h5. Scenario 2: Serialized Object without ResourceProvider Support
The other option is just to serialize the information we have directly as part of the mpack endpoint. 

{code:title=http://c7401.ambari.apache.org:8080/api/v1/mpacks/1}
{
  ""href"": ""http://c7401.ambari.apache.org:8080/api/v1/mpacks/1"",
  ""MpackInfo"": {
    ""id"": 1,
    ""mpack_description"": ""Hortonworks Data Platform Core"",
    ""mpack_id"": ""hdpcore"",
    ""mpack_name"": ""HDPCORE"",
    ""mpack_uri"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96/mpack.json"",
    ""mpack_version"": ""1.0.0-b96"",
    ""operating_systems"": [
      {
        ""repositories"": [
          {
            ""distribution"": null,
            ""components"": null,
            ""unique"": true,
            ""tags"": [],
            ""base_url"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDPCORE-1.0.0-b96"",
            ""os_type"": ""redhat7"",
            ""repo_id"": null,
            ""repo_name"": null,
            ""mirrors_list"": null,
            ""default_base_url"": null,
            ""ambari_managed"": true
          },
          {
            ""distribution"": null,
            ""components"": null,
            ""unique"": false,
            ""tags"": [],
            ""base_url"": ""http://repo.ambari.apache.org/hdpcore/centos7/HDP-UTILS-1.1.0.22/"",
            ""os_type"": ""redhat7"",
            ""repo_id"": null,
            ""repo_name"": null,
            ""mirrors_list"": null,
            ""default_base_url"": null,
            ""ambari_managed"": true
          }
        ],
        ""os_type"": ""redhat7"",
        ""ambari_managed"": true
      },
      {
        ""repositories"": [
          {
            ""distribution"": null,
            ""components"": null,
            ""unique"": false,
            ""tags"": [],
            ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu12"",
            ""os_type"": ""ubuntu12"",
            ""repo_id"": null,
            ""repo_name"": null,
            ""mirrors_list"": null,
            ""default_base_url"": null,
            ""ambari_managed"": true
          },
          {
            ""distribution"": null,
            ""components"": null,
            ""unique"": true,
            ""tags"": [],
            ""base_url"": ""http://dev.hortonworks.com.s3.amazonaws.com/HDPCORE/ubuntu12/1.x/BUILDS/1.0.0-b96"",
            ""os_type"": ""ubuntu12"",
            ""repo_id"": null,
            ""repo_name"": null,
            ""mirrors_list"": null,
            ""default_base_url"": null,
            ""ambari_managed"": true
          }
        ],
        ""os_type"": ""ubuntu12"",
        ""ambari_managed"": true
      }
    ],
    ""registry_id"": null,
    ""stack_name"": """",
    ""stack_version"": """"
  },
  ""operating_system"": [],
  ""version"": []
}
{code}

- Operating systems and repositories are no longer really ""resources"". You can't query by them and can't update them individually
- Much simpler to implement
- Requires updating all repos together, but can be done in 1 call",pull-request-available,[],AMBARI,Task,Critical,2018-03-05 19:02:26,31
13142588,Alignment issues for checkboxes and radio buttons,The alignment of the radio buttons and checkboxes with the labels are messed up.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-05 16:40:40,19
13142556,Stack Advisor Should not Use 'accessible-node-labels' as a Queue Name,"Ambari UI can not save changes due to Ambari StackAdvisor failing if accessible-node-labels is used as a queue name:

2018-02-20 05:16:30,497 - Entered __getSelectedQueueTotalCap fn() with llap_daemon_selected_queue_name= 'llap'. 
2018-02-20 05:16:30,498 - DBG: Selected queue name as: yarn.scheduler.capacity.root.accessible-node-labels.llap.capacity 
2018-02-20 05:16:30,498 - Queue list : ['root', 'accessible-node-labels', 'llap'] 
2018-02-20 05:16:30,498 - DBG: Selected queue name as: yarn.scheduler.capacity.root.capacity 
2018-02-20 05:16:30,498 - Total capacity available for queue root is : 1187840.0 
Error occured in stack advisor. 
Error details: float() argument must be a string or a number 
===",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-05 15:04:25,18
13142543,Remove obsolete modules from the Ambari source code,"The following obsolete modules should be removed from Ambari.  Since they are not used or maintained there is no need to keep them around:  

* Groovy Shell (ambari-shell/ambari-groovy-shell) [last updated 9 months ago*]
* Groovy Client (ambari-client/groovy_client) [last updated 2 years ago]
* Python Shell (ambari-shell/ambari-python-shell) [last updated 4 years ago*]
*  Python Client (ambari-client/python_client)  [last updated 2 years ago]
",pull-request-available,"['ambari-client', 'ambari-shell']",AMBARI,Task,Major,2018-03-05 14:26:51,30
13142530,Failure while starting Spark2 service after performing ambari upgrade,"Test steps :
1) Deploy a cluster with ambari-2.5.1.0-159 + HDP-2.6.1.0-129
2) Upgrade ambari to 2.6.2.0-82
3) Now try to restart all the services. The operation is failing with below exception while trying to start Spark2
{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/hook.py"", line 37, in <module>
    AfterInstallHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/hook.py"", line 31, in hook
    setup_stack_symlinks(self.stroutfile)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/shared_initialization.py"", line 49, in setup_stack_symlinks
    stack_packages = stack_select.get_packages(stack_select.PACKAGE_SCOPE_INSTALL)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/stack_select.py"", line 235, in get_packages
    raise Fail(""The package {0} is not supported by this version of the stack-select tool."".format(package))
resource_management.core.exceptions.Fail: The package livy2-client is not supported by this version of the stack-select tool.{code}
livy2-client package was added in HDP 2.6.3 and was not present before that.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-05 13:32:32,13
13142440,Fix BlackDuck found security issues in Ambari Functional Tests,Fix BlackDuck found security issues in Ambari Functional Tests,black-duck pull-request-available security,['test'],AMBARI,Bug,Blocker,2018-03-05 07:45:05,27
13142439,Fix typo in AmbariHostsTest in groovy-client,"In our groovy-client project the Python unit tests are failing in Jenkins due to the following typo in 

AmbariHostsTest.test get host components as map when there is no cluster yet:

ambari.metaClass.getHostComponenets ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-05 07:32:23,27
13142224,Allow users to skip backup steps on Express Upgrades to save time,Large clusters with say 1000+ nodes have problems with EU phases like HBase snapshot backup of Namenode namedirs taking a very long time that actually increases the downtime for the upgrade window. This Jira intends to allow parameterization of these skips using cluster configurations.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-02 23:38:49,42
13142218,Create an interface to read command.json properties via API instead of direct access,"*Use APIs to access the command.json file*
 * ambari-common/src/main/python/resource_management/core/resources/klist.py
Fetch executable_search_path using library function instead of direct access.
 * ambari-common/src/main/python/resource_management/libraries/functions/get_architecture.py
 * ambari-common/src/main/python/resource_management/libraries/functions/get_config.py
Revisit logic to fetch config. Currently from params.
 * ambari-common/src/main/python/resource_management/libraries/functions/get_not_managed_resources.py
 * ambari-common/src/main/python/resource_management/libraries/functions/package_conditions.py",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-03-02 23:25:07,44
13142187,Install wizard revisions for updated APIs,The back end team made changes to server behavior affecting the registry and mpacks. This updates the install wizard to accommodate these changes.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-02 20:00:09,40
13142122,Fix ConfigUpgradeValidityTest.testConfigurationDefinitionsExist test failure,"Fix ConfigUpgradeValidityTest.testConfigurationDefinitionsExist test failure which is caused since no active HDP stacks are available in the core Ambari source code tree. 

Thanks [~smolnar] for pointing out the issue.
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-02 15:35:11,30
13142114,Config version not shown after overriding properties with a config group,"STR:
# Navigate to HDFS -> Config Tab
# Create new Config group 'group1' and add 2 hosts with DataNode to the Config group
# Select this new group
# Override properties in Advanced Tab for this config group and Click Save

Observed: After the config change is saved, the Version is not populated in the version dropdown and also there is a ""Make Current"" Button which is not present in earlier releases.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-02 14:47:00,19
13142025,Implement persistence support for the cluster creation template,Implement persistence for the cluster creation template.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-03-02 07:57:45,20
13141949,Multiple issues while executing Ambari server upgrade to Ambari 2.7.0,"*1. An NPE is thrown will initializing the Ambari server upgrade catalog*

{code:title=com.google.inject.persist.jpa.JpaPersistService#begin}
    public void begin() {
        Preconditions.checkState(null == this.entityManager.get(), ""Work already begun on this thread. Looks like you have called UnitOfWork.begin() twice without a balancing call to end() in between."");
        this.entityManager.set(this.emFactory.createEntityManager());
    }
{code}

{{this.emFactory}} is {{null}}.

*Cause*
{{com.google.inject.persist.jpa.JpaPersistService#start()}} was not called before {{com.google.inject.persist.jpa.JpaPersistService#begin}} to do order of operations in {{org.apache.ambari.server.upgrade.SchemaUpgradeHelper#main}}.  

*Solution*
Ensure {{com.google.inject.persist.jpa.JpaPersistService#start()}} is being called before {{com.google.inject.persist.jpa.JpaPersistService#begin}} in {{org.apache.ambari.server.upgrade.SchemaUpgradeHelper#main}}.  

-----

*2. Missing repo_os, repo_definition, and repo_tags tables*

The {{repo_os}}, {{repo_definition}}, and {{repo_tags}} tables were  never added to the UpgradeCatalog implementation.  The migration logic is also needed.
*Solution*

Add the missing tables and logic while executing {{org.apache.ambari.server.upgrade.UpgradeCatalog270#executeDDLUpdates}}. 

-----

*3. Entity classes are initialized before the schema of the underlying database is updated*

*Solution*
Notify relevant classes that the persistence infrastructure is ready after DDL updates have been applied. 

The {{org.apache.ambari.server.events.publishers.AmbariEventPublisher}} is to be used for issuing  a {{org.apache.ambari.server.events.JpaInitializedEvent}}.

-----

*4. JVM does not exit after performing upgrade*

After executing {{ambari-server upgrade}}, the JVM process hangs and does not exit.  According to the logs, no errors have occurred.

*Cause*
The cause of this is several non-daemon threads not being shutdown by Ambari code. 

*Solution*
Change relevant non-daemon threads to daemon threads and ensure any thread polls are not forcing one thread to be alive at all times. 



",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-01 23:55:43,30
13141920,Allow default value of number of parallel tasks run during Upgrade to be customizable,"On large clusters of 1000+ nodes the default value of 100 for max degree of parallelism results in Stop and Start command batches of 100 hosts and resulting operation taking longer because of the default batch size. This Jira adds a possible override through ambari.properties.

Property:
{noformat}
stack.upgrade.default.parallelism{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-01 22:13:19,42
13141892,Fix BlackDuck found security issues in Ambari Server,Fix BlackDuck found security issues in Ambari Server,black-duck pull-request-available security,['ambari-server'],AMBARI,Bug,Blocker,2018-03-01 19:54:44,27
13141858,Yarn Queue Manager is not case sensitive,"YARN Queue Manager does not allow two queues with ""abc"" and ""ABC"" whereas YARN Capacity Scheduler allows the same.

Attached screenshots of YARN Queue Manager and RM UI.",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-03-01 17:22:35,3
13141768,Missing 'hash' in alert update message.,"{noformat}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 130, in register
 listener.on_event({}, response)
 File ""/usr/lib/ambari-agent/lib/ambari_agent/listeners/AlertDefinitionsEventListener.py"", line 51, in on_event
 self.alert_definitions_cache.rewrite_cache(message['clusters'], message['hash'])
KeyError: 'hash'
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-01 11:46:19,6
13141752,oozie config change throws Consistency Check Failed Error,"========
Ambari : 2.6.2
HDP : HDP 2.6.5
========


* Login to cluster and goto oozie config
* Update any oozie configuration for e.x update oozie.service.WorkflowAppService.WorkflowDefinitionMaxLength and save

* Attached screenshot error is displayed

Below error is noticed in logs.

{code}
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Stack Advisor reported an error: TypeError: argument of type 'NoneType' is not iterable
StdOut file: /var/run/ambari-server/stack-recommendations/6/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/6/stackadvisor.err
	at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner.runScript(StackAdvisorRunner.java:96)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:297)
	at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.validate(StackAdvisorHelper.java:78)
	at org.apache.ambari.server.controller.internal.ValidationResourceProvider.createResources(ValidationResourceProvider.java:85)
	... 90 more

{code}
",pull-request-available,[],AMBARI,Bug,Critical,2018-03-01 10:21:58,55
13141628,Notebook storage still configured to VFSNotebookRepo in upgraded cluster,"For upgrade runs, I am seeing that notebook storage is still configured to VFSNotebookRepo and hence notebook remote storage functionality is not working. 

Test configuration:
CentOS6 + Ambari-2.5.1 + HDP-2.6.1 -> AU to Ambari-2.6.2 -> Full EU to HDP-2.6.5.0-74 -> Run stack tests

as i understood, property zeppelin.notebook.storage should be updated. In Zeppelin 0.6.0 this property has value org.apache.zeppelin.notebook.repo.VFSNotebookRepo, in Zeppelin 0.7.0 org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo, so during upgrade from Zeppelin 0.6.0 to Zeppelin 0.7.0 this value should be changed.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-28 20:27:17,13
13141626,Some libraries are not relocated in ambari-metrics-common jar,"Some libraries are not relocated in ambari-metrics-common jar, that means some classes are exists in the jar, and those can be on the classpath as well (with different versions), which can cause issues.

Libaries which are not shaded: (not sure that conventional, but i think we should not include those dependencies in the uberjar if those are not shaded):

* commons-lang
* commons-math3
* commons-codec
* jackson-xc
* jackson-core-asl / jackson-mapper-asl",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-02-28 20:17:33,39
13141595,Additional fixes for service summary page to support NameNode Federation,"- Add mapping of namespace id property from API
- Display NameNodes as active and standby ones
- Remove tabs to select certain namespace
- Add namespace-scoped HDFS checkpoint check and warning message before stopping and restarting NameNodes
- Too long namespace id should be cropped with adding ellipsis and beung displayed fully in tooltip",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-28 18:24:26,15
13141569,livy2-client version is not updated as part of Patch Upgrade.,"
*STR*

1) Deployed cluster with Ambari version: 2.6.1.0-143 and HDP version: 2.6.4.0-91
2) Upgrade Ambari to Target Version: 2.6.2.0-67
3) Patch Express Upgrade Spark2 to HDP-2.6.5.0-120

Observe the version of livy2-client after  PU

result of hdp-select command
{code:java}
[root@ctr-e138-1518143905142-45389-01-000002 ~]# hdp-select | grep livy2-client
livy2-client - 2.6.4.0-91
[root@ctr-e138-1518143905142-45389-01-000002 ~]#
{code}

hdp-select outputs <See the livy2-client versions on all hosts , should have been 2.6.4.0-120>

{code:java}
===6===
livy2-client - 2.6.4.0-91
livy2-server - 2.6.5.0-120
===2===
livy2-client - 2.6.4.0-91
livy2-server - 2.6.4.0-91
===5===
livy2-client - 2.6.4.0-91
livy2-server - 2.6.4.0-91
===4===
livy2-client - 2.6.4.0-91
livy2-server - 2.6.4.0-91
{code}


*Note : The livy2-client version was set correctly during Full stack Express Upgrade. The issue is seen only for Patch Upgrade*

LIVY2_CLIENT isn't actually a component which Ambari manages. That's why it's never orchestrated. I suppose that we can fix this by having the SPARK2_CLIENT also update the pointers for LIVY2_CLIENT by changing the stack_packages.json.

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-28 16:44:38,13
13141562,Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Functional Tests,"Remove dependency on org.apache.httpcomponents:httpclient:jar before version 4.3.5.1 due to security concerns. See
 * CVE-2015-5262 - [https://nvd.nist.gov/vuln/detail/CVE-2015-5262]
 * CVE-2014-3577 - [https://nvd.nist.gov/vuln/detail/CVE-2014-3577]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-funtest ---
 org.apache.ambari:ambari-funtest:jar:2.6.1.0.0
 +- org.apache.httpcomponents:httpclient:jar:4.5.2:compile
 +- org.apache.ambari:ambari-metrics-common:jar:2.6.1.0.0:compile
 |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for conflict with 4.5.2)
 \- org.apache.ambari:ambari-server:jar:2.6.1.0.0:compile
    +- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for conflict with 4.5.2)
    +- org.apache.hadoop:hadoop-auth:jar:2.7.2:compile
    |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for conflict with 4.5.2)
    \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
       \- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
          \- (org.apache.httpcomponents:httpclient:jar:4.1.2:compile - omitted for conflict with 4.5.2){noformat}",black-duck pull-request-available,['test'],AMBARI,Bug,Blocker,2018-02-28 15:45:44,27
13141555,Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Functional Tests,"Remove dependency on commons-collections:commons-collections before version 3.2.2 due to security concerns. See
 * CVE-2015-6420 - [https://nvd.nist.gov/vuln/detail/CVE-2015-6420]
 * CVE-2015-7501 - [https://nvd.nist.gov/vuln/detail/CVE-2015-7501]
 * CVE-2017-15708 - [https://nvd.nist.gov/vuln/detail/CVE-2017-15708]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-funtest ---
 org.apache.ambari:ambari-funtest:jar:2.6.1.0.0
 +- org.apache.directory.server:apacheds-server-annotations:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-core-annotations:jar:2.0.0-M19:test
 |  |  +- org.apache.directory.server:apacheds-xdbm-partition:jar:2.0.0-M19:test
 |  |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  |  \- org.apache.directory.mavibot:mavibot:jar:1.0.0-M6:test
 |  |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.api:api-ldap-codec-core:jar:1.0.0-M26:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.directory.server:apacheds-core-integ:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-interceptors-authn:jar:2.0.0-M19:test
 |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.server:apacheds-interceptors-hash:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M19:compile
 |  \- org.apache.directory.api:api-ldap-model:jar:1.0.0-M26:compile
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - omitted for duplicate)
 +- org.apache.directory.server:apacheds-core:jar:2.0.0-M19:test
 |  \- org.apache.directory.server:apacheds-interceptors-exception:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.shared:shared-ldap:jar:0.9.17:test
 |  \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.velocity:velocity:jar:1.7:compile{noformat}",black-duck pull-request-available,['test'],AMBARI,Bug,Blocker,2018-02-28 15:25:50,27
13141550,Deprecate Repo Versions And Start Tracking Repo OS's For MPacks,"Repository Versions are no longer going to be needed since a single repository now can contain multiple versions of a management pack and its RPMs. Since the {{mpack.json}} file contains all the necessary version information, we no longer need to keep track of reported versions coming from a repo. However, repository versions are very tightly coupled now and to remove them without having a working replacement would be quite difficult. This Jira covers the following work items:

- Begin marking deprecated classes and methods related to removal of repo versions and host versions
- Start tracking mpack installations on a per-host basis
- Associate repository OS and definition entities with the management pack (instead of a repo version)",pull-request-available,[],AMBARI,Task,Blocker,2018-02-28 15:18:20,31
13141470,Exception caught while installing Timeline Service V2.0 ,"While installing Timeline Service V2.0 during CI run, the following error is
captured:

    
    
    Caught an exception while executing custom service command: <type 'exceptions.Exception'>: Command requires configs with timestamp=1519782088331 but configs on agent have timestamp=1519783511993; Command requires configs with timestamp=1519782088331 but configs on agent have timestamp=1519783511993
    

See <http://172.27.30.209:8080> (up until 8pm ET 3/1)

",pull-request-available,[],AMBARI,Bug,Major,2018-02-28 09:18:36,5
13141436,Adding Stack feature constant for Ranger and Ranger KMS services,Need to add a constant in Stack feature for {{RANGER}} and {{RANGER_KMS}} services. This constant will be used during upgrade of stack 2.6 to 3.0.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-28 06:49:16,25
13141331,Let custom service users to access Log Search collections,Add new {{logsearch-env/logsearch_kerberos_service_users}} property to logsearch which can be used to let other service users to manipulate logsearch collections,pull-request-available,"['ambari-infra', 'ambari-logsearch']",AMBARI,Bug,Major,2018-02-27 19:59:24,29
13141314,Log Search: knox proxy support,Add knox proxy support and update knox docker env with that.,pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-02-27 18:23:20,29
13141224,Ambari does not manage repositories,"While running CI tests

    
    
     
    Traceback (most recent call last): 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/package/__init__.py"", line 283, in _call_with_retries 
        code, out = func(cmd, **kwargs) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner 
        result = function(command, **kwargs) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call 
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper 
        result = _call(command, **kwargs_copy) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 303, in _call 
        raise ExecutionFailed(err_msg, code, out, err) 
    ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install hadooplzo_3_0_0_2_87' returned 1. Error: Nothing to do 
    
    
    
    2018-02-26 12:16:14,479 - Repository for HDP/3.0.0.2-87/HDP-3.0 is not managed by Ambari
    2018-02-26 12:16:14,479 - Repository for HDP/3.0.0.2-87/HDP-3.0-GPL is not managed by Ambari
    2018-02-26 12:16:14,480 - Repository for HDP/3.0.0.2-87/HDP-UTILS-1.1.0.22 is not managed by Ambari
    

I checked in DB and indeed manage_repos was false, however I didn't found any
signs in logs etc. that it was set to false during cluster setup.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-27 13:52:27,5
13141219,Remove dependency on org.apache.zookeeper:zookeeper before version 3.4.6.2.0.0.0-579 for Ambari Server,"Remove dependency on org.apache.zookeeper:zookeeper before version 3.4.6.2.0.0.0-579 due to security concerns. See
 * CVE-2017-5637 - [https://nvd.nist.gov/vuln/detail/CVE-2017-5637]
 * CVE-2016-5017 - [https://nvd.nist.gov/vuln/detail/CVE-2016-5017]

{noformat}
 --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.ambari:ambari-metrics-common:jar:2.6.1.0.0:compile
 |  \- org.apache.curator:curator-framework:jar:2.7.1:compile
 |     \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitted for duplicate)
 +- org.apache.hadoop:hadoop-auth:jar:2.7.2:compile
 |  \- org.apache.zookeeper:zookeeper:jar:3.4.6:compile
 \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
    +- org.apache.curator:curator-client:jar:2.7.1:compile
    |  \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitted for duplicate)
    +- org.apache.curator:curator-recipes:jar:2.7.1:compile
    |  \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitted for duplicate)
    \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitte{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-27 13:24:34,27
13141187,Zeppelin Notebook SSL credentials in Ambari UI are in plain text rather than being hidden,Zeppelin Notebook keystore & truststore passwords appear in plain text rather than being hidden in Ambari,pull-request-available,['ambari-sever'],AMBARI,Bug,Critical,2018-02-27 10:55:18,24
13141007,Upgrade Plan Wizard framework,"Create basic infrastructure for a new wizard with the steps that we need per the mockups. Copy any reusable screens from the install wizard
h2.  ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-26 19:07:20,34
13141000,Target host shows all the hosts available in Move Wizard,The target host selection in the Move Wizard shows all the available hosts in the cluster instead of the hosts where the component is not installed. ,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-26 18:40:43,19
13140982,NN Federation Wizard: implement step3,Provide step3 with ability for user to review selections from previous steps and configs to be changed.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-26 17:09:10,9
13140961,Background Ops Modal Updates - Get user name,"clusters/clusterName/requests/... API call to provide user name associated with the request.
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-26 15:56:59,55
13140910,Missing permission for 'others' when Ambari is configured with two way SSL and https enabled,"# Deploy Ambari-2.6.2.0 server on machine A
# Manually install and register agents on other machines (including machine A)
# Enable 2 way SSL between server and agents
# Enable https at Ambari server
# Deploy a cluster via blueprints with HDP-2.6.5.0

After cluster is deployed, observed that the permission of files such as hadoop-env.sh is '-rw-r-----'
Complete output:
{code}
[root@ctr-e138-1518143905142-36503-01-000002 logs]# ls -lhrt /etc/hadoop/conf/
total 176K
-rw-r--r-- 1 cstm-hdfs hadoop 8.9K Feb 22 09:30 core-site.xml
-rw-r----- 1 cstm-hdfs hadoop 333 Feb 22 09:35 hdfs_dn_jaas.conf
-rw-r----- 1 cstm-hdfs hadoop 333 Feb 22 09:35 hdfs_nn_jaas.conf
-rw-r----- 1 cstm-hdfs hadoop 1.3K Feb 22 09:35 hadoop-policy.xml
-rw-r----- 1 cstm-hdfs hadoop 884 Feb 22 09:35 ssl-client.xml
drwxr-xr-x 2 root hadoop 4.0K Feb 22 09:35 secure
-rw-r----- 1 cstm-hdfs hadoop 1000 Feb 22 09:35 ssl-server.xml
-rw-r--r-- 1 cstm-hdfs hadoop 8.7K Feb 22 09:35 hdfs-site.xml
-rw-r--r-- 1 cstm-mr hadoop 7.5K Feb 22 09:37 mapred-site.xml
-rw-r--r-- 1 cstm-hdfs hadoop 2.3K Feb 22 09:37 capacity-scheduler.xml
-rw-r--r-- 1 root hadoop 1.1K Feb 22 09:37 container-executor.cfg
-rwxr-xr-x 1 root root 984 Feb 22 09:37 mapred-env.sh
-rw-r--r-- 1 root hadoop 947 Feb 22 09:37 taskcontroller.cfg
-rw-r----- 1 cstm-yarn hadoop 571 Feb 22 09:37 yarn_jaas.conf
-rw-r----- 1 cstm-yarn hadoop 337 Feb 22 09:37 yarn_ats_jaas.conf
-rw-r----- 1 cstm-yarn hadoop 333 Feb 22 09:37 yarn_nm_jaas.conf
-rw-r----- 1 cstm-mr hadoop 320 Feb 22 09:37 mapred_jaas.conf
-rw-r----- 1 root root 1020 Feb 22 09:48 commons-logging.properties
-rw-r----- 1 root root 1.6K Feb 22 09:48 health_check
-rw-r--r-- 1 cstm-hdfs hadoop 11K Feb 22 09:48 log4j.properties
-rwxr-xr-x 1 root root 4.2K Feb 22 09:48 task-log4j.properties
-rwxr-xr-x 1 root root 2.4K Feb 22 09:48 topology_script.py
-rw-r----- 1 root root 241 Feb 22 10:10 slaves
-rw-r----- 1 root hadoop 6.3K Feb 22 10:10 hadoop-env.sh
-rw-r--r-- 1 cstm-yarn hadoop 24K Feb 22 10:10 yarn-site.xml
-rwxr-xr-x 1 cstm-yarn hadoop 5.5K Feb 22 10:10 yarn-env.sh
-rw-r----- 1 cstm-hdfs hadoop 2.6K Feb 22 10:12 hadoop-metrics2.properties
-rw-r--r-- 1 cstm-hdfs hadoop 467 Feb 22 10:12 topology_mappings.data
-rw-r----- 1 cstm-hdfs hadoop 1 Feb 22 10:13 dfs.exclude
{code}

 


When compared this with a non-SSL cluster the permission is '-rw-r--r--' i.e. read permission is available for other users",system_test,['ambari-server'],AMBARI,Bug,Critical,2018-02-26 12:24:43,27
13140527,Service Auto Start cannot be disabled,"*STR*
# navigate / Admin / Service Auto Start
# disable Auto Start
# save 
# refresh page",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-23 15:36:48,19
13140522,Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Server,"Remove dependency on commons-collections:commons-collections before version 3.2.2 due to security concerns. See
 * CVE-2015-6420 - [https://nvd.nist.gov/vuln/detail/CVE-2015-6420]
 * CVE-2015-7501 - [https://nvd.nist.gov/vuln/detail/CVE-2015-7501]
 * CVE-2017-15708 - [https://nvd.nist.gov/vuln/detail/CVE-2017-15708]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.directory.server:apacheds-server-annotations:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-core-annotations:jar:2.0.0-M19:test
 |  |  +- org.apache.directory.server:apacheds-xdbm-partition:jar:2.0.0-M19:test
 |  |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  |  \- org.apache.directory.mavibot:mavibot:jar:1.0.0-M6:test
 |  |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.api:api-ldap-codec-core:jar:1.0.0-M26:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.directory.server:apacheds-core-integ:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-interceptors-authn:jar:2.0.0-M19:test
 |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.server:apacheds-interceptors-hash:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M19:compile
 |  \- org.apache.directory.api:api-ldap-model:jar:1.0.0-M26:compile
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - omitted for duplicate)
 +- org.apache.directory.server:apacheds-core:jar:2.0.0-M19:test
 |  \- org.apache.directory.server:apacheds-interceptors-exception:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.shared:shared-ldap:jar:0.9.17:test
 |  \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.velocity:velocity:jar:1.7:compile
 |  \- commons-collections:commons-collections:jar:3.2.1:compile
 +- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
 |  +- (commons-collections:commons-collections:jar:3.2.2:compile - omitted for conflict with 3.2.1)
 |  \- commons-configuration:commons-configuration:jar:1.6:compile
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - omitted for duplicate)
 \- utility:utility:jar:1.0.0.0-SNAPSHOT:test
    \- com.puppycrawl.tools:checkstyle:jar:6.19:test
       \- (commons-collections:commons-collections:jar:3.2.2:test - omitted for conflict with 3.2.1){noformat}",black-duck pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-23 15:10:37,27
13140511,Freeze the hostname column in Assign Slaves/Clients page (Install and Add Service Wizards),"we just want to freeze the hostname column so that when the user scrolls horizontally, the hostname column remains in place. ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-23 14:30:03,34
13140495,Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Server,"Remove dependency on org.apache.httpcomponents:httpclient:jar before version 4.3.5.1 due to security concerns. See
 * CVE-2015-5262 - [https://nvd.nist.gov/vuln/detail/CVE-2015-5262]
 * CVE-2014-3577 - [https://nvd.nist.gov/vuln/detail/CVE-2014-3577]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.httpcomponents:httpclient:jar:4.2.5:compile
 +- org.apache.ambari:ambari-metrics-common:jar:2.6.1.0.0:compile
 |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for duplicate)
 +- org.apache.hadoop:hadoop-auth:jar:2.7.2:compile
 |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for duplicate)
 \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
    \- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
       \- (org.apache.httpcomponents:httpclient:jar:4.1.2:compile - omitted for conflict with 4.2.5)
 {noformat}
 * 
 * 
[Options|https://hortonworks.jira.com/browse/BUG-97133?filter=54432]

h2.  ",black-duck pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-23 13:14:53,27
13140436,Remove dependency on com.fasterxml.jackson.core:jackson-databind before version 2.9.4 for Ambari Server,"Remove dependency on com.fasterxml.jackson.core:jackson-databind before version 2.9.4 due to security concerns. See
 * CVE-2018-5968 - [https://nvd.nist.gov/vuln/detail/CVE-2018-5968]
 * CVE-2017-17485 - [https://nvd.nist.gov/vuln/detail/CVE-2017-17485]

{noformat}
 --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 \- com.networknt:json-schema-validator:jar:0.1.10:test
    \- com.fasterxml.jackson.core:jackson-databind:jar:2.8.7:test
{noformat}",black-duck pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-23 08:59:48,27
13140359,yum installation fails if there is any transaction files,"Starting from 2.6.1 Ambari started checking transaction files during the repo/service installation and it fails with below error.


{noformat}
2018-02-16 16:12:21,639 - File['/etc/yum.repos.d/ambari-hdp-104.repo'] {'content': '[HDP-2.6-repo-104]\nname=HDP-2.6-repo-104\nbaseurl=http://xxxxxxx/vcm_hadoop/HDP/centos6/2.6.4.0-91\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-104]\nname=HDP-UTILS-1.1.0.22-repo-104\nbaseurl=http://xxxxxx/vcm_hadoop/HDP-UTILS-1.1.0.22\n\npath=/\nenabled=1\ngpgcheck=0'} 
2.  2018-02-16 16:12:21,639 - Writing File['/etc/yum.repos.d/ambari-hdp-104.repo'] because contents don't match 
3.  2018-02-16 16:12:21,639 - Yum non-completed transactions check failed, found 1 non-completed transaction(s): 
4.  2018-02-16 16:12:21,640 - [2015-03-12.17:13.28.disabled] Packages broken: hue-common-2.6.1.2.2.0.0-2041.el6.x86_64; Packages not-installed hue-hcatalog-2.6.1.2.2.0.0-2041.el6.x86_64, hue-2.6.1.2.2.0.0-2041.el6.x86_64, hue-pig-2.6.1.2.2.0.0-2041.el6.x86_64, hue-oozie-2.6.1.2.2.0.0-2041.el6.x86_64, hue-beeswax-2.6.1.2.2.0.0-2041.el6.x86_64, hue-server-2.6.1.2.2.0.0-2041.el6.x86_64 
5.  2018-02-16 16:12:21,640 - *** Incomplete Yum Transactions *** 
6.  2018-02-16 16:12:21,640 - 
7.  2018-02-16 16:12:21,640 - Ambari has detected that there are incomplete Yum transactions on this host. This will interfere with the installation process and must be resolved before continuing. 
8.  2018-02-16 16:12:21,640 - 
9.  2018-02-16 16:12:21,640 - - Identify the pending transactions with the command 'yum history list <packages failed>' 
10. 2018-02-16 16:12:21,640 - - Revert each pending transaction with the command 'yum history undo' 
11. 2018-02-16 16:12:21,640 - - Flush the transaction log with 'yum-complete-transaction --cleanup-only' 
After cleaning up with 'yum-complete-transaction --cleanup-only' still Ambari could not able to proceed further with same error.

When user tried to do yum install it was going through successfully.

Finally had to delete /var/lib/yum/transaction* manually for ambari to proceed further with installation. these transactions files were very old and nothing to do with any latest installation but still ambari does not proceed further.
{noformat}


What is expected: If this validation was done intentionally then it should throw proper message guiding the user to remove those files.

OptionsAttachments",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-02-22 23:32:10,5
13140348,Upgrade fails because of Stale alert definitions.,"Upgrade fails with below exception
{noformat}
06 Nov 2017 04:31:26,826 ERROR [main] SchemaUpgradeHelper:437 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: Service not found, clusterName=testcluster, serviceName=GANGLIA
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:234)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:424)
Caused by: org.apache.ambari.server.ServiceNotFoundException: Service not found, clusterName=testcluster, serviceName=GANGLIA
	at org.apache.ambari.server.state.cluster.ClusterImpl.getService(ClusterImpl.java:864)
	at org.apache.ambari.server.api.services.AmbariMetaInfo.reconcileAlertDefinitions(AmbariMetaInfo.java:1240)
	at org.apache.ambari.server.upgrade.UpdateAlertScriptPaths.executeDMLUpdates(UpdateAlertScriptPaths.java:46)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:938)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:231)
{noformat}
few users are reporting this problem -reporting these stale alert definitions in DBConsistency check would avoid upgrade failures.",pull-request-available,['ambari-upgrade'],AMBARI,Bug,Major,2018-02-22 22:42:37,14
13140258,GPL repo is not hidden when default version is chosen in UI,"Ambari server has gpl license disabled i.e.
{code}
cat /etc/ambari-server/conf/ambari.properties | grep -i gpl
gpl.license.accepted=false
{code}

- On register version page, upon choosing HDP-2.6.4.0 from the drop-down, the GPL repo field is not masked (see default-version.png)
- However upon choosing to upload the version from VDF URL, the GPL field is correctly masked (see version-via-vdf.png)

Looks like the problem is [showRepo()|https://github.com/apache/ambari/blob/branch-2.6/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js#L50] function which calls [isGPLRepo|https://github.com/apache/ambari/blob/branch-2.6/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js#L46] function where if we do not load version fromVDF URL the showRepo return true

 ",pull-request-available system_test,['ambari-server'],AMBARI,Bug,Critical,2018-02-22 17:23:15,6
13140247,Remove dependency on commons-beanutils:commons-beanutils before version 1.9.2 for Ambari Server,"Remove dependency on commons-beanutils:commons-beanutils before version 1.9.2 due to security concerns. See CVE-2014-0114 - [https://nvd.nist.gov/vuln/detail/CVE-2014-0114]
{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
 |  \- commons-configuration:commons-configuration:jar:1.6:compile
 |     +- commons-digester:commons-digester:jar:1.8:compile
 |     |  \- commons-beanutils:commons-beanutils:jar:1.9.2:compile
 |     \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
 \- utility:utility:jar:1.0.0.0-SNAPSHOT:test
    \- com.puppycrawl.tools:checkstyle:jar:6.19:test
       \- (commons-beanutils:commons-beanutils:jar:1.9.2:compile -{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-22 16:29:07,27
13140244,Rename NON-ROLLING to EXPRESS,"The term ""non-rolling"" was coined before we used ""Express"". The enum should be called EXPRESS.

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-02-22 16:19:18,33
13140213,Extend service actions dropdowns with namespace-specific items,"- User should be able to apply the service action (if it can be executed for certain components only) to the given namespace
- Add Restart NameNodes item",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-22 14:45:18,15
13140193,Background Ops during wizard pages do not show all the ops running by default,The Background Ops opened inside an install wizard do not show all the operations running by default. They are shown only after selecting the dropdown.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-22 13:47:15,19
13140178,Hosts selection not shown while adding Zookeeper server,"STR:
# Login and navigate to Zookeeper service
# Click actions button
# Select Add Zookeeper Server option

In the confirmation window that appears, the host selection is not present.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-22 12:19:54,19
13140155,Remove unsecure dependencies from Ambari Groovy Shell,"* Remove dependency on org.apache.zookeeper:zookeeper before version 3.4.6.2.0.0.0-579 for Ambari Groovy Shell

* Remove dependency on xerces:xercesImpl before version 2.11.2 for Ambari Groovy Shell

* Remove dependency on tomcat:jasper-compiler and tomcat:jasper-runtime before version 6.0.20.0 for Ambari Groovy Shell

* Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Groovy Shell

* Remove dependency on commons-httpclient:commons-httpclient before version 3.1.0 for Ambari Groovy Shell

* Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Groovy Shell

* Remove dependency on commons-beanutils:commons-beanutils-core before version 1.9.2 for Ambari Groovy Shell",pull-request-available,['ambari-shell'],AMBARI,Bug,Major,2018-02-22 10:10:05,18
13140140,Remove unsecure dependencies from Ambari Groovy Client,"Remove dependency on xerces:xercesImpl before version 2.11.2 for Ambari Groovy Client

Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Groovy Client

Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Groovy Client

Remove dependency on commons-beanutils:commons-beanutils-core before version 1.9.2 for Ambari Groovy Client",pull-request-available,['ambari-client'],AMBARI,Bug,Major,2018-02-22 09:06:50,18
13139921,'Table or view not found error' with livy/livy2 interpreter on upgraded cluster,"The test has been performed as below:
 CentOS6 + Ambari-2.5.1 + HDP-2.6.1 -> AU to Ambari-2.6.2 -> Full EU to HDP-2.6.5.0-74  -> Run stack tests

I see that with livy2 interpreter, anytime we register a temporary view or table - the corresponding query on that table will fail with 'Table or view not found error'
{code:java}
org.apache.spark.sql.AnalysisException: Table or view not found: word_counts; line 2 pos 24
 at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:649)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:601)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:631)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:624)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:61)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:624)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:570)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
 at scala.collection.immutable.List.foldLeft(List.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
 at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)
 at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:637)
 ... 50 elided
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-21 14:53:04,24
13139918,Save/discard button is not found for ranger configuration,"STR:
1) go to ambari ui
2) open ranger configs

ER: there should be save/discard button
AR: no save/discard button found",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-21 14:41:04,19
13139880,Deploy fails during Atlas Client installation,"Deploy jobs are failing with following exception during installation of Atlas
metadata client:

    
    
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ATLAS/package/scripts/atlas_client.py"", line 53, in 
        AtlasClient().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ATLAS/package/scripts/atlas_client.py"", line 41, in install
        self.install_packages(env)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 823, in install_packages
        name = self.format_package_name(package['name'])
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 567, in format_package_name
        raise Fail(""Cannot match package for regexp name {0}. Available packages: {1}"".format(name, self.available_packages_in_repos))
    resource_management.core.exceptions.Fail: Cannot match package for regexp name atlas-metadata_${stack_version}. Available packages: ['openblas', 'openblas-Rblas', 'openblas-devel', 'openblas-openmp', 'openblas-openmp64', 'openblas-openmp64_', 'openblas-serial64', 'openblas-serial64_', 'openblas-static', 'openblas-threads', 'openblas-threads64', 'openblas-threads64_', 'snappy', 'snappy', 'snappy-devel', 'snappy-devel']
    
",pull-request-available,[],AMBARI,Bug,Major,2018-02-21 11:52:47,5
13139878,'Current' Tag not shown in the Versions page for the currently installed version,"The 'Current' tag is not shown against the installed cluster.

Also, on the Stacks & Versions page, there are lines shown when an older version of HDP is registered:",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-21 11:46:19,19
13139803,Alert filter with services having space is not working,"Alert filter with services having space is not working.
STR:
1. Navigate to Alerts page
2. Try to filter using Service as filter key with services like ""Ambari Infra""/""Log Search""
3. Alerts are not filtered.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-21 01:29:45,19
13139802,Admin user can delete own user from Users page,"Admin user can delete own user from Users page.

 

STR:
 # Login to ambari UI as admin user
 # Navigate to Users page
 # Try to delete the same user (admin) from the users list

The delete option is disabled in the individual user page, but not from the all Users page.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-21 01:27:23,19
13139765,Create Lifecycle changes for Upgrade Packs,"In Upgrade Packs, abstract the {{<order>}} element further and wrap them with a {{<lifecycle>}} element.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-20 21:22:43,33
13139719,HDFS Balancer via Ambari fails when FIPS mode is activated on the OS,"This is similar issue as reported on AMBARI-22417, but for service checks. 

The original issue of ambari check is resolved. But the same issue is seen when running the HDFS balancer via Ambari.

*Solution*
MD5 is disabled on the OS, Code needs to be updated to use SHA

This is required when FIPS mode is enabled on the RHEL OS",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-20 17:02:35,30
13139708,Create HDP 2.6 to HDP 3.0 express upgrade pack,"
Create HDP 2.6. to HDP 3.0 express upgrade pack

",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-20 16:01:19,13
13139679,Support for Blueprint exports for MPack-based clusters,"The Blueprints export feature needs to be updated to support a deployment based on multiple MPacks. The Blueprint export processor will need to integrate with the MPack set of APIs in order to properly export a cluster created with the new mechanism.

",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-20 14:42:39,20
13139671,View name is not displayed in ambari-views page,"*Steps to reporduce:-*
1. go to ambari-views tab
2. create as many view you can create 
3. see the views dashboard

Screenshot: [^Screen Shot 2018-02-09 at 11.44.53 AM.png]

The Name field is blank

the Name Only get populated add a ""Short URL"".",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-20 14:20:13,19
13139646,Not able to delete livy component from a host,"I am not able to delete livy(only one instance of livy is present on the cluster) from a host.
After stopping livy, when I try to delete, its showing below message :
""WARNING! Delete the last Livy for Spark2 Server component in the cluster?
Deleting the last component in the cluster could result in permanent loss of service data."" (Attaching the screenshot)

But there's not way to enable 'Confirm Delete' button.

Note : In earlier versions of ambari there used to be a check box and user has to check it to enable delete button.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-20 12:29:58,19
13139577,Add backup/copy znode command for infra-solr-client,"Usage: 
{code:java}
infra-solr-cloud-cli --transfer-znode -z host1:2181,host2:2181 --copy-src /infra-solr -copy-dest /infra-solr-backup

{code}
Src/Dist can be local, but both not (both can be znode of course)",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-02-20 06:04:33,29
13139569,WEB type alerts authentication in Kerberos secured cluster,"In a Kerberized cluster some web endpoints (App Timeline Web UI, ResourceManger Web UI, etc.) require authentication. Any Ambari alerts checking those endpoints must then be able to authenticate.

This was addressed in AMBARI-9586, however the default principal and keytab used in the alerts.json is that of the ""bare"" SPNEGO principal HTTP/_HOST@REALM. 
 My understanding is that the HTTP service principal is used to authenticate users to a service, not used to authenticate to another service.

1. Since most endpoints involved are Web UI, would it be more appropriate to use the smokeuser in the alerts?

2. This was first observed in Ranger Audit, the YARN Ranger Plug-in showed many access denied from HTTP user. [This post|https://community.hortonworks.com/content/supportkb/150206/ranger-audit-logs-refers-to-access-denied-for-http.html] provided some direction as to where those requests were coming from. We have updated the ResourceManger Web UI alert definition to use cluster-env/smokeuser_keytab and cluster-env/smokeuser_principal_name and this has resolved the initial HTTP access denied. 
 Would it also be advisable to make the change in the other secure Web UI alert definitions?",pull-request-available,['alerts'],AMBARI,Bug,Minor,2018-02-20 04:54:40,27
13139498,NN Federation Wizard: implement step2,Provide step2 with ability for user to select hosts for new host components.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-19 18:02:07,9
13139460,Log Serach UI: Implement Metadata patterns screen,Implement a feature where the user can list all the metadata patterns and can add new or edit an existing one.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-19 15:36:42,26
13139445,Ambari UI deploy fails during startup of Ambari Metrics,"{noformat}
HDP version:    HDP-3.0.0.0-702
Ambari version: 2.99.99.0-77
{noformat}

/var/lib/ambari-agent/data/errors-52.txt:
{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py"", line 90, in <module>
    AmsCollector().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 371, in execute
    self.execute_prefix_function(self.command_name, 'post', env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 392, in execute_prefix_function
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 434, in post_start
    raise Fail(""Pid file {0} doesn't exist after starting of the component."".format(pid_file))
resource_management.core.exceptions.Fail: Pid file /var/run/ambari-metrics-collector//hbase-ams-master.pid doesn't exist after starting of the component.
{noformat}

/var/lib/ambari-agent/data/output-52.txt:
{noformat}
2018-01-11 13:03:40,753 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-702 -> 3.0.0.0-702
2018-01-11 13:03:40,755 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:40,884 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-702 -> 3.0.0.0-702
2018-01-11 13:03:40,885 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:40,886 - Group['hdfs'] {}
2018-01-11 13:03:40,887 - Group['hadoop'] {}
2018-01-11 13:03:40,887 - Group['users'] {}
2018-01-11 13:03:40,887 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,890 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,891 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,892 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,893 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,893 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
2018-01-11 13:03:40,894 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,894 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
2018-01-11 13:03:40,895 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
2018-01-11 13:03:40,895 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,896 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,897 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,897 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,898 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
2018-01-11 13:03:40,903 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
2018-01-11 13:03:40,903 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:40,904 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,905 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,906 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
2018-01-11 13:03:40,913 - call returned (0, '1002')
2018-01-11 13:03:40,914 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1002'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
2018-01-11 13:03:40,917 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1002'] due to not_if
2018-01-11 13:03:40,918 - Group['hdfs'] {}
2018-01-11 13:03:40,918 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', 'hdfs']}
2018-01-11 13:03:40,919 - FS Type: 
2018-01-11 13:03:40,919 - Directory['/etc/hadoop'] {'mode': 0755}
2018-01-11 13:03:40,932 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,933 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
2018-01-11 13:03:40,947 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}
2018-01-11 13:03:40,962 - Directory['/var/log/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:40,964 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}
2018-01-11 13:03:40,964 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'create_parents': True, 'cd_access': 'a'}
2018-01-11 13:03:40,967 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}
2018-01-11 13:03:40,969 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'hdfs'}
2018-01-11 13:03:40,973 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:40,982 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/hadoop-metrics2.properties'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,983 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
2018-01-11 13:03:40,983 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,987 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:40,991 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
2018-01-11 13:03:40,996 - Skipping stack-select on AMBARI_METRICS because it does not exist in the stack-select package structure.
2018-01-11 13:03:41,142 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:41,144 - checked_call['hostid'] {}
2018-01-11 13:03:41,147 - checked_call returned (0, '007f0100')
2018-01-11 13:03:41,150 - Directory['/etc/ams-hbase/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,151 - Changing owner for /etc/ams-hbase/conf from 0 to ams
2018-01-11 13:03:41,151 - Changing group for /etc/ams-hbase/conf from 0 to hadoop
2018-01-11 13:03:41,151 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,159 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] since it doesn't exist.
2018-01-11 13:03:41,159 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp from 0 to ams
2018-01-11 13:03:41,160 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:41,160 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] since it doesn't exist.
2018-01-11 13:03:41,160 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 0 to ams
2018-01-11 13:03:41,160 - Changing group for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 0 to hadoop
2018-01-11 13:03:41,160 - Changing permission for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 755 to 775
2018-01-11 13:03:41,161 - File['/etc/ams-hbase/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,161 - File['/etc/ams-hbase/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,161 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,170 - Generating config: /etc/ams-hbase/conf/hbase-site.xml
2018-01-11 13:03:41,170 - File['/etc/ams-hbase/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,198 - Writing File['/etc/ams-hbase/conf/hbase-site.xml'] because contents don't match
2018-01-11 13:03:41,198 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,198 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool'] since it doesn't exist.
2018-01-11 13:03:41,199 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool from 0 to ams
2018-01-11 13:03:41,199 - Changing group for /var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool from 0 to hadoop
2018-01-11 13:03:41,199 - XmlConfig['hbase-policy.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {}, 'configurations': {'security.masterregion.protocol.acl': '*', 'security.admin.protocol.acl': '*', 'security.client.protocol.acl': '*'}}
2018-01-11 13:03:41,205 - Generating config: /etc/ams-hbase/conf/hbase-policy.xml
2018-01-11 13:03:41,205 - File['/etc/ams-hbase/conf/hbase-policy.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,207 - Writing File['/etc/ams-hbase/conf/hbase-policy.xml'] because contents don't match
2018-01-11 13:03:41,213 - File['/etc/ams-hbase/conf/hbase-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,215 - Writing File['/etc/ams-hbase/conf/hbase-env.sh'] because contents don't match
2018-01-11 13:03:41,219 - File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties.j2'), 'owner': 'ams', 'group': 'hadoop'}
2018-01-11 13:03:41,220 - Writing File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] because contents don't match
2018-01-11 13:03:41,220 - TemplateConfig['/etc/ams-hbase/conf/regionservers'] {'owner': 'ams', 'template_tag': None}
2018-01-11 13:03:41,222 - File['/etc/ams-hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'ams', 'group': None, 'mode': None}
2018-01-11 13:03:41,223 - Writing File['/etc/ams-hbase/conf/regionservers'] because it doesn't exist
2018-01-11 13:03:41,223 - Changing owner for /etc/ams-hbase/conf/regionservers from 0 to ams
2018-01-11 13:03:41,223 - Directory['/var/run/ambari-metrics-collector/'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,224 - Creating directory Directory['/var/run/ambari-metrics-collector/'] since it doesn't exist.
2018-01-11 13:03:41,224 - Changing owner for /var/run/ambari-metrics-collector/ from 0 to ams
2018-01-11 13:03:41,225 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,225 - Creating directory Directory['/var/log/ambari-metrics-collector'] since it doesn't exist.
2018-01-11 13:03:41,225 - Changing owner for /var/log/ambari-metrics-collector from 0 to ams
2018-01-11 13:03:41,225 - Directory['/var/lib/ambari-metrics-collector/hbase'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,225 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase'] since it doesn't exist.
2018-01-11 13:03:41,226 - Changing owner for /var/lib/ambari-metrics-collector/hbase from 0 to ams
2018-01-11 13:03:41,226 - File['/var/run/ambari-metrics-collector//distributed_mode'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,228 - File['/etc/ams-hbase/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,229 - Writing File['/etc/ams-hbase/conf/log4j.properties'] because contents don't match
2018-01-11 13:03:41,230 - Directory['/etc/ams-hbase/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,230 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,230 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:41,231 - File['/etc/ams-hbase/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,231 - File['/etc/ams-hbase/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,231 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,237 - Generating config: /etc/ams-hbase/conf/hbase-site.xml
2018-01-11 13:03:41,237 - File['/etc/ams-hbase/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,264 - XmlConfig['hbase-policy.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {}, 'configurations': {'security.masterregion.protocol.acl': '*', 'security.admin.protocol.acl': '*', 'security.client.protocol.acl': '*'}}
2018-01-11 13:03:41,270 - Generating config: /etc/ams-hbase/conf/hbase-policy.xml
2018-01-11 13:03:41,270 - File['/etc/ams-hbase/conf/hbase-policy.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,280 - File['/etc/ams-hbase/conf/hbase-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,283 - File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties.j2'), 'owner': 'ams', 'group': 'hadoop'}
2018-01-11 13:03:41,283 - TemplateConfig['/etc/ams-hbase/conf/regionservers'] {'owner': 'ams', 'template_tag': None}
2018-01-11 13:03:41,284 - File['/etc/ams-hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'ams', 'group': None, 'mode': None}
2018-01-11 13:03:41,285 - Directory['/var/run/ambari-metrics-collector/'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,285 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,287 - File['/etc/ams-hbase/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,288 - Directory['/etc/ambari-metrics-collector/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,288 - Changing owner for /etc/ambari-metrics-collector/conf from 0 to ams
2018-01-11 13:03:41,288 - Changing group for /etc/ambari-metrics-collector/conf from 0 to hadoop
2018-01-11 13:03:41,289 - Directory['/var/lib/ambari-metrics-collector/checkpoint'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,289 - Creating directory Directory['/var/lib/ambari-metrics-collector/checkpoint'] since it doesn't exist.
2018-01-11 13:03:41,289 - Changing owner for /var/lib/ambari-metrics-collector/checkpoint from 0 to ams
2018-01-11 13:03:41,289 - Changing group for /var/lib/ambari-metrics-collector/checkpoint from 0 to hadoop
2018-01-11 13:03:41,289 - XmlConfig['ams-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {}, 'configurations': ...}
2018-01-11 13:03:41,296 - Generating config: /etc/ambari-metrics-collector/conf/ams-site.xml
2018-01-11 13:03:41,296 - File['/etc/ambari-metrics-collector/conf/ams-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,339 - Writing File['/etc/ambari-metrics-collector/conf/ams-site.xml'] because contents don't match
2018-01-11 13:03:41,339 - XmlConfig['ssl-server.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {}, 'configurations': ...}
2018-01-11 13:03:41,346 - Generating config: /etc/ambari-metrics-collector/conf/ssl-server.xml
2018-01-11 13:03:41,346 - File['/etc/ambari-metrics-collector/conf/ssl-server.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,351 - Writing File['/etc/ambari-metrics-collector/conf/ssl-server.xml'] because it doesn't exist
2018-01-11 13:03:41,351 - Changing owner for /etc/ambari-metrics-collector/conf/ssl-server.xml from 0 to ams
2018-01-11 13:03:41,351 - Changing group for /etc/ambari-metrics-collector/conf/ssl-server.xml from 0 to hadoop
2018-01-11 13:03:41,351 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,357 - Generating config: /etc/ambari-metrics-collector/conf/hbase-site.xml
2018-01-11 13:03:41,357 - File['/etc/ambari-metrics-collector/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,384 - Writing File['/etc/ambari-metrics-collector/conf/hbase-site.xml'] because contents don't match
2018-01-11 13:03:41,385 - File['/etc/ambari-metrics-collector/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,387 - Writing File['/etc/ambari-metrics-collector/conf/log4j.properties'] because contents don't match
2018-01-11 13:03:41,390 - File['/etc/ambari-metrics-collector/conf/ams-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,391 - Writing File['/etc/ambari-metrics-collector/conf/ams-env.sh'] because contents don't match
2018-01-11 13:03:41,392 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,392 - Changing group for /var/log/ambari-metrics-collector from 0 to hadoop
2018-01-11 13:03:41,392 - Directory['/var/run/ambari-metrics-collector'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,392 - Changing group for /var/run/ambari-metrics-collector from 0 to hadoop
2018-01-11 13:03:41,393 - File['/usr/lib/ams-hbase/bin/hadoop'] {'owner': 'ams', 'mode': 0755}
2018-01-11 13:03:41,393 - Writing File['/usr/lib/ams-hbase/bin/hadoop'] because it doesn't exist
2018-01-11 13:03:41,394 - Changing owner for /usr/lib/ams-hbase/bin/hadoop from 0 to ams
2018-01-11 13:03:41,394 - Changing permission for /usr/lib/ams-hbase/bin/hadoop from 644 to 755
2018-01-11 13:03:41,394 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}
2018-01-11 13:03:41,397 - File['/etc/security/limits.d/ams.conf'] {'content': Template('ams.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}
2018-01-11 13:03:41,397 - Writing File['/etc/security/limits.d/ams.conf'] because it doesn't exist
2018-01-11 13:03:41,398 - Execute['/usr/lib/ams-hbase/bin/hbase-daemon.sh --config /etc/ams-hbase/conf stop regionserver'] {'on_timeout': 'ls /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid >/dev/null 2>&1 && ps `cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid` >/dev/null 2>&1 && ambari-sudo.sh -H -E kill -9 `ambari-sudo.sh cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid`', 'timeout': 30, 'user': 'ams'}
2018-01-11 13:03:41,448 - File['/var/run/ambari-metrics-collector//hbase-ams-regionserver.pid'] {'action': ['delete']}
2018-01-11 13:03:41,449 - Execute['/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf stop'] {'user': 'ams'}
2018-01-11 13:03:41,505 - Execute['ambari-sudo.sh rm -rf /var/lib/ambari-metrics-collector/hbase-tmp/*.tmp'] {}
2018-01-11 13:03:41,511 - File['/etc/ambari-metrics-collector/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,511 - File['/etc/ambari-metrics-collector/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,512 - Execute['/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf start'] {'user': 'ams'}
2018-01-11 13:08:55,668 - Skipping stack-select on AMBARI_METRICS because it does not exist in the stack-select package structure.

Command failed after 1 tries
{noformat}",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-02-19 14:27:12,39
13139436,ServiceInfo: credential_store_supported attempts to overwrite maintenance_state,"Try to update {{credential_store_supported}} property of a service:

{noformat}
$ curl -X PUT -d @- ""http://$AMBARI_SERVER:8080/api/v1/clusters/TEST/services/HDFS"" <<EOF
{ ""ServiceInfo"": { ""credential_store_supported"": ""true"" } }
EOF
HTTP/1.1 400 Bad Request
...
  ""message"" : ""java.lang.IllegalArgumentException: No enum constant org.apache.ambari.server.state.MaintenanceState.true""
{noformat}

Expected response:

{{IllegalArgumentException: Invalid arguments, cannot update credential_store_supported as it is set only via service definition.}}

The response code is the same as expected due to a coincidence.

The problem is setting the wrong property:

{noformat}
 414     o = properties.get(SERVICE_CREDENTIAL_STORE_SUPPORTED_PROPERTY_ID);
 415     if (null != o) {
 416       svcRequest.setMaintenanceState(o.toString());
 417     }
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-02-19 13:55:53,21
13139417,Missing LDAP configuration property warning should be a debug statement,"Missing LDAP configuration property warning should be a debug statement. The Ambari server log is being filled with statements like:
{noformat}
  
16 Feb 2018 14:01:36,617  WARN [main] AmbariLdapConfiguration:47 - Ldap configuration property [ambari.ldap.connectivity.server.host] hasn't been set; using default value
{noformat}
The log level for these messages should be lowered to at least DEBUG.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-19 12:57:47,27
13139371,[PERF] Deployment of PERF stack leads to Out of Sync,"While deploying PERF stack, PERF 1.0 stack is not installed and shows up as
Out of sync  

",pull-request-available,[],AMBARI,Bug,Major,2018-02-19 08:33:47,5
13139041,Error in Metrics retrieval service due to bad JSON formatting,"Some jmx metrics JSON contains NaN and it causes an exception.
{code}
org.codehaus.jackson.JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow
 at [Source: sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@3292d496; line: 916, column: 28]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.Utf8StreamParser._handleUnexpectedValue(Utf8StreamParser.java:2079)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:555)
	at org.codehaus.jackson.map.deser.std.MapDeserializer._readAndBind(MapDeserializer.java:304)
	at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize(MapDeserializer.java:249)
	at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize(MapDeserializer.java:33)
	at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:217)
	at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:194)
	at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:30)
	at org.codehaus.jackson.map.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:299)
	at org.codehaus.jackson.map.deser.SettableBeanProperty$MethodProperty.deserializeAndSet(SettableBeanProperty.java:414)
	at org.codehaus.jackson.map.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:697)
	at org.codehaus.jackson.map.deser.BeanDeserializer.deserialize(BeanDeserializer.java:580)
	at org.codehaus.jackson.map.ObjectReader._bindAndClose(ObjectReader.java:768)
	at org.codehaus.jackson.map.ObjectReader.readValue(ObjectReader.java:434)
	at org.apache.ambari.server.state.services.MetricsRetrievalService$JMXRunnable.processInputStreamAndCacheResult(MetricsRetrievalService.java:559)
	at org.apache.ambari.server.state.services.MetricsRetrievalService$MetricRunnable.run(MetricsRetrievalService.java:439)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,[],AMBARI,Bug,Critical,2018-02-16 14:29:29,55
13139038,Unable to proceed to step1 on Install Wizard,Got js error when tried to proceed form step0 to step1.,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-02-16 14:12:56,9
13138991,Ignore python UT failing on branch-3.0-perf,"As discussed with Sid Wagle for now we should ignore UT failing on branch-3.0-perf as they take up a lot of
time rework.

The reason for this is that we need to merge into trunk soon, so the feature
code gets enough test cycles.  
The tests will be unignored and fixed after the merge is done.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-16 08:48:52,5
13138982,"""Stale Alerts"" alert appears on cluster after some time","Previously. Every x seconds any alert was updated with information from agent.
Even if nothing changed.  
Due to perf reasons this behavior was changed so that agent reports alerts
only when their status changes.

Due this change, ""stale alerts"" alert started appearing on clusters, showing
that alerts were not updated for a long time.

We should consider alert up-to-date to the point of last heartbeat. Which
shows that the connection between server and agent is okay due to
last_hb_timestamp, and so agent could send alert updates.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-16 07:33:11,5
13138942,Ranger LB URL is pointing to ranger port by default when we click on Quick link,"For Ranger HA, when we give External url as LB url and port (LB:port), but when we click on quick link is pointing to LB url, but it is using ranger port(6080 or 6182 depend it's http or https).

Below code snippet - reading the host name from policymgr_external_url and then using default ranger port.


{noformat}
} else if (serviceName === 'RANGER') {
      var siteConfigs = this.get('configProperties').findProperty('type', 'admin-properties').properties;
      if (siteConfigs['policymgr_external_url']) {
        host = siteConfigs['policymgr_external_url'].split('://')[1].split(':')[0];
         var newItem = {};
        newItem.url = siteConfigs['policymgr_external_url'];
        newItem.label = link.label;
         return newItem;
        }
    }
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-16 00:24:27,19
13138892,AMS: SUM downsampling function works incorrectly,"AMS, by default does not sum up across time dimension. However, there are certain metric cases where this is useful.

*Aggregation Scenario - How it should work*

Let's say in Host H1 - the metric values for 'topology.streamline-2-test-clone.3-KAFKA.TASK1.--emit-count.STREAM1' are 1,1,0,1,2 in time 10:01 to 10:05 (1 minute values) respectively

And, in Host H2 - the metric values for the same metric are 1,0,2,2,1 respectively

AMS will first aggregate across hosts. 'Sum' aggregation -> 2,1,2,3,3 (10:01 - 10:05) and then across time (downsampling) for larger windows. In time based downsampling, the 'sum' value will be 11 (2 + 1 + 2 + 3 + 3) at 10:05.

Also, such metrics should not be interpolated, because the injected values will cause wrong summed up values.

* Work*
A new custom downsampler was added which implements the above logic. ",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-02-15 20:25:47,39
13138869,Create Upgrade Plan menu item and popup,Implements the action for the Create Upgrade Plan menu item on the Service Groups admin screen and the popup that appears when the menu item is clicked.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-15 19:12:26,40
13138867,Upgrade plan admin view,"This is the ""Upgrade"" part of the admin screen that shows the state of the saved upgrade plan and progress of the upgrade process, including action history log.",pull-request-available,[],AMBARI,Bug,Major,2018-02-15 19:11:01,40
13138853,Failed to Add Big SQL to HDP 3.0 due to a stack advisor error on calculating slave component dependencies,"Failed to Add Big SQL to HDP 3.0 due to a stack advisor error on calculating slave component dependencies. 


Error printed by Ambari server



Error occured in stack advisor.
Error details: 'StackServiceComponents'
14 Feb 2018 12:32:54,260 INFO [ambari-client-thread-94] StackAdvisorRunner:164 - Advisor script stderr: Traceback (most recent call last):
 File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 167, in <module>
 main(sys.argv)
 File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 108, in main
 result = stackAdvisor.recommendComponentLayout(services, hosts)
 File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 608, in recommendComponentLayout
 layoutRecommendations = self.createComponentLayoutRecommendations(services, hosts)
 File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 794, in createComponentLayoutRecommendations
 filteredHosts = self.getFilteredHostsBasedOnDependencies(services, component, hostsList, hostsComponentsMap)
 File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 928, in getFilteredHostsBasedOnDependencies
 componentName = component[""StackServiceComponents""][""component_name""]
KeyError: 'StackServiceComponents'
14 Feb 2018 12:32:54,260 WARN [ambari-client-thread-94] AbstractResourceProvider:97 - Error occured during recommendation
org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'StackServiceComponents'
StdOut file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.err
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner.processLogs(StackAdvisorRunner.java:146)
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner.runScript(StackAdvisorRunner.java:86)
 at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:345)
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:132)
 at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:92)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:45)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:76)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:166)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:130)
 at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:60)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:288)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:133)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:93)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:213)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:202)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
 at org.eclipse.jetty.server.Server.handle(Server.java:370)
 at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
 at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
 at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
 at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
 at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
 at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
 at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
 at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
 at java.lang.Thread.run(Thread.java:745)
14 Feb 2018 12:32:54,262 ERROR [ambari-client-thread-94] BaseManagementHandler:69 - Caught a system exception while attempting to create a resource: Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'StackServiceComponents'
StdOut file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.err
org.apache.ambari.server.controller.spi.SystemException: Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'StackServiceComponents'
StdOut file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.err
 at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:98)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:45)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:76)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:166)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:130)
 at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:60)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:288)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:133)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:93)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:213)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:202)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
 at org.eclipse.jetty.server.Server.handle(Server.java:370)
 at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
 at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
 at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
 at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
 at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
 at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
 at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
 at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'StackServiceComponents'
StdOut file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/32/stackadvisor.err
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner.processLogs(StackAdvisorRunner.java:146)
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner.runScript(StackAdvisorRunner.java:86)
 at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:345)
 at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:132)
 at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:92)
 ... 90 more",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-15 18:01:38,60
13138849,Create initial version of InstanceManager to create instance layout ,"We need an instance manager tool for managing the lifecycle of mpack instances, service instances, component instances. This will be the replacement for hdp-select, conf-select

Examples of few operations that we will need
# create-mpack-instance
# create-component-instance
# set-component-instance-version
# get-component-instance-version
# get-component-instance-confdir",pull-request-available,['ambari-agent'],AMBARI,Task,Major,2018-02-15 17:36:09,28
13138790,NN Federation Wizard: implement step1,Provide step1 with ability for user to specify new namespace name. Also user should be able to see existing namespaces.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-15 15:13:18,9
13138778,Cancel Button in Upload table doesnt works in Hive views 2.0,"*Steps to reproduce :*

1) Open Hive Views 2.0
2) navigate to *Tables*  Tab
3) click on *+* button
4) click on *Upload Table* button 
5) Click on *cancel* button that appears along with create button , its throws a *script error* and no action is performed 

Attached screen shot of the error
[screenshot of issue|https://issues.apache.org/jira/secure/attachment/12910750/hive%20views%20cancel%20issue.png]",Ambari ambari-views hive-view pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-02-15 14:17:15,3
13138775,"HA, Move wizard shows error modal on selection","Move NameNode, Enable HA wizards on HDFS service page throws up an error that at least 2 hosts are to be present even though more than 2 hosts are present.

!move-wizard.png!",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-15 14:12:35,7
13138758,Timeline Service V2.0 reader install fails if wget is not already installed on the host,"Encountered while testing Atlantic Beta 1.  
Timeline Service V2.0 reader install fails if wget is already not installed on
the host beforehand.

    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 101, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 45, in install
        hbase_service.install_hbase(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/hbase_service.py"", line 82, in install_hbase
        Execute(hbase_download_cmd, user=""root"", logoutput=True)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
        tries=self.resource.tries, try_sleep=self.resource.try_sleep)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase' returned 127. -bash: wget: command not found
     stdout:
    2018-02-13 07:39:31,705 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:31,711 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:31,713 - Group['livy'] {}
    2018-02-13 07:39:31,714 - Group['spark'] {}
    2018-02-13 07:39:31,714 - Group['hdfs'] {}
    2018-02-13 07:39:31,714 - Group['zeppelin'] {}
    2018-02-13 07:39:31,714 - Group['hadoop'] {}
    2018-02-13 07:39:31,715 - Group['users'] {}
    2018-02-13 07:39:31,715 - Group['knox'] {}
    2018-02-13 07:39:31,716 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,717 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,718 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,719 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,720 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,721 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,722 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,723 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,725 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,726 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,727 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,728 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,729 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,731 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-13 07:39:31,732 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,735 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-13 07:39:31,744 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-13 07:39:31,744 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-13 07:39:31,746 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,748 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,749 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-13 07:39:31,759 - call returned (0, '1014')
    2018-02-13 07:39:31,760 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-13 07:39:31,766 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] due to not_if
    2018-02-13 07:39:31,766 - Group['hdfs'] {}
    2018-02-13 07:39:31,767 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-13 07:39:31,767 - FS Type: 
    2018-02-13 07:39:31,767 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-13 07:39:31,786 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-13 07:39:31,787 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-13 07:39:31,803 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,811 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,812 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,813 - Repository['HDP-UTILS-1.1.0.21-repo-1'] {'append_to_file': True, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,816 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.21-repo-1]\nname=HDP-UTILS-1.1.0.21-repo-1\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,817 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,817 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,902 - Skipping installation of existing package unzip
    2018-02-13 07:39:31,902 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,912 - Skipping installation of existing package curl
    2018-02-13 07:39:31,912 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,922 - Skipping installation of existing package hdp-select
    2018-02-13 07:39:32,207 - Looking for matching packages in the following repositories: HDP-3.0-repo-1, HDP-UTILS-1.1.0.21-repo-1
    2018-02-13 07:39:34,268 - Package['hadoop_3_0_0_0_809-yarn'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,352 - Skipping installation of existing package hadoop_3_0_0_0_809-yarn
    2018-02-13 07:39:34,354 - Package['hadoop_3_0_0_0_809-mapreduce'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,363 - Skipping installation of existing package hadoop_3_0_0_0_809-mapreduce
    2018-02-13 07:39:34,365 - Package['hadoop_3_0_0_0_809-hdfs'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,380 - Skipping installation of existing package hadoop_3_0_0_0_809-hdfs
    2018-02-13 07:39:34,393 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,394 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:34,394 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-yarn-resourcemanager'] {'timeout': 20}
    2018-02-13 07:39:34,421 - call returned (0, 'hadoop-yarn-resourcemanager - 3.0.0.0-809')
    2018-02-13 07:39:34,461 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,472 - Execute['umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase'] {'logoutput': True, 'user': 'root'}
    -bash: wget: command not found
    
    Command failed after 1 tries
    
    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 101, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 45, in install
        hbase_service.install_hbase(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/hbase_service.py"", line 82, in install_hbase
        Execute(hbase_download_cmd, user=""root"", logoutput=True)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
        tries=self.resource.tries, try_sleep=self.resource.try_sleep)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase' returned 127. -bash: wget: command not found
     stdout:
    2018-02-13 07:39:31,705 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:31,711 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:31,713 - Group['livy'] {}
    2018-02-13 07:39:31,714 - Group['spark'] {}
    2018-02-13 07:39:31,714 - Group['hdfs'] {}
    2018-02-13 07:39:31,714 - Group['zeppelin'] {}
    2018-02-13 07:39:31,714 - Group['hadoop'] {}
    2018-02-13 07:39:31,715 - Group['users'] {}
    2018-02-13 07:39:31,715 - Group['knox'] {}
    2018-02-13 07:39:31,716 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,717 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,718 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,719 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,720 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,721 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,722 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,723 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,725 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,726 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,727 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,728 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,729 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,731 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-13 07:39:31,732 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,735 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-13 07:39:31,744 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-13 07:39:31,744 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-13 07:39:31,746 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,748 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,749 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-13 07:39:31,759 - call returned (0, '1014')
    2018-02-13 07:39:31,760 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-13 07:39:31,766 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] due to not_if
    2018-02-13 07:39:31,766 - Group['hdfs'] {}
    2018-02-13 07:39:31,767 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-13 07:39:31,767 - FS Type: 
    2018-02-13 07:39:31,767 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-13 07:39:31,786 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-13 07:39:31,787 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-13 07:39:31,803 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,811 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,812 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,813 - Repository['HDP-UTILS-1.1.0.21-repo-1'] {'append_to_file': True, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,816 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.21-repo-1]\nname=HDP-UTILS-1.1.0.21-repo-1\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,817 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,817 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,902 - Skipping installation of existing package unzip
    2018-02-13 07:39:31,902 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,912 - Skipping installation of existing package curl
    2018-02-13 07:39:31,912 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,922 - Skipping installation of existing package hdp-select
    2018-02-13 07:39:32,207 - Looking for matching packages in the following repositories: HDP-3.0-repo-1, HDP-UTILS-1.1.0.21-repo-1
    2018-02-13 07:39:34,268 - Package['hadoop_3_0_0_0_809-yarn'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,352 - Skipping installation of existing package hadoop_3_0_0_0_809-yarn
    2018-02-13 07:39:34,354 - Package['hadoop_3_0_0_0_809-mapreduce'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,363 - Skipping installation of existing package hadoop_3_0_0_0_809-mapreduce
    2018-02-13 07:39:34,365 - Package['hadoop_3_0_0_0_809-hdfs'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,380 - Skipping installation of existing package hadoop_3_0_0_0_809-hdfs
    2018-02-13 07:39:34,393 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,394 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:34,394 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-yarn-resourcemanager'] {'timeout': 20}
    2018-02-13 07:39:34,421 - call returned (0, 'hadoop-yarn-resourcemanager - 3.0.0.0-809')
    2018-02-13 07:39:34,461 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,472 - Execute['umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase'] {'logoutput': True, 'user': 'root'}
    -bash: wget: command not found
    
    Command failed after 1 tries
    

",pull-request-available,[],AMBARI,Bug,Major,2018-02-15 12:24:12,5
13138694,Ambari Hive View 2.0 'Upload Table' does not support UTF8 files with BOM,"Creating this Jira to request same fix as AMBARI-18583 for Ambari Hive 2.0 View.

 ",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-02-15 02:57:16,38
13138662,Wrong user used to execute the Spark/Livy Server service check,"{code:title=common-services/SPARK/1.2.1/package/scripts/service_check.py:36}
 livy_kinit_cmd = format(""{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal}; "")
 Execute(livy_kinit_cmd, user=params.livy_user)
{code}
Notice the Kerberos identity is for the smoke user, but the exec is for the livy user. This will replace the livy user's interactive Kerberos ticket cache.

This should be 
{code}
smoke_user_kinit_cmd = format(""{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal}; "")
Execute(smoke_user_kinit_cmd, user=params.smoke_user)
{code}
Where {{smoke_user}} is
{code}
smoke_user =  config['configurations']['cluster-env']['smokeuser']
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-15 00:32:00,30
13138585,Remove deprecated Upgrade Packs,The old upgrade packs and test should be removed to make way for the new format that supports lifecycles.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-14 20:58:01,33
13138584,Management Packs: Upgrade Pack Changes,This epic is used to track all the work necessary to change Upgrade Packs to work with the architecture changes for Management Packs,pull-request-available,['ambari-server'],AMBARI,Epic,Critical,2018-02-14 20:53:57,33
13138580,Update error handling during mpack installation,"Remove stale code and update error handling in MpackManager. At present the code supports refreshing a stack definition when an mpack is installed. Given we are will map stack to mpack 1:1, we need to remove this stale code and replace with error handling. Also the old mpack json schema used to contain stack-id that we no longer need.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-14 20:13:41,56
13138561,Grafana shows incorrect info for HBase Average Regions per RegionServer,"Problem

These are number widgets on Grafana and is expected to show point in time information for ""Average Regions per RegionServer"" 
ambari-web that shows expected correct information gets the data from ""Hadoop:service=HBase,name=Master,sub=Server.averageLoad"" jmx metric via ambari-server metrics API resource.

It seems having aggregator as ""avg"" looks to be causing this issue. when there are more than one HBase master than the metric value is averaged across all masters",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-02-14 18:35:19,39
13138538,master-key option is missing from ambari-server setup action,"{{ambari-server setup --enable-lzo-under-gpl-license}} failing with {{Unexpected AttibuteError}}

{noformat}
# ambari-server setup --enable-lzo-under-gpl-license -s
Using python /usr/bin/python
Setup ambari-server
Checking SELinux...
SELinux status is 'disabled'
Customize user account for ambari-server daemon [y/n] (n)?
Adjusting ambari-server permissions and ownership...
Checking firewall status...
Checking JDK...
Do you want to change Oracle JDK [y/n] (n)?
Check JDK version for Ambari Server...
JDK version found: 8
Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.
Checking GPL software agreement...
Completing setup...
Configuring database...
Enter advanced database configuration [y/n] (n)?
Configuring database...
ERROR: Unexpected AttributeError: Values instance has no attribute 'master_key'
For more info run ambari-server with -v or --verbose option
{noformat}

*Note*: This occurs after encrypting passwords in the {{ambari_server.properties}} file:
{noformat}
# ambari-server setup-security
Using python  /usr/bin/python
Security setup options...
===========================================================================
Choose one of the following options:
  [1] Enable HTTPS for Ambari server.
  [2] Encrypt passwords stored in ambari.properties file.
  [3] Setup Ambari kerberos JAAS configuration.
  [4] Setup truststore.
  [5] Import certificate to truststore.
===========================================================================
Enter choice, (1-5): 2
Please provide master key for locking the credential store:
Re-enter master key:
Do you want to persist master key. If you choose not to persist, you need to provide the Master Key while starting the ambari server as an env variable named AMBARI_SECURITY_MASTER_KEY or the start will prompt for the master key. Persist [y/n] (y)? y
Adjusting ambari-server permissions and ownership...
Ambari Server 'setup-security' completed successfully.
{noformat}

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-14 17:09:18,30
13138515,Fix Ambari doc site to fix errors from https://whimsy.apache.org/site/project/ambari,Fix errors shown in https://whimsy.apache.org/site/project/ambari,pull-request-available,['site'],AMBARI,Bug,Major,2018-02-14 15:18:36,62
13138476,Quick links should be grouped by namespace,Currently quick links are grouped by host. These groups in their part should be grouped by namespace (if any).,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-14 11:43:34,15
13138474,Old mpacks broken ,"Old mpacks or custom stacks may not have ""component"" filed stack_packages.json and package install will be broken. We need to fallback to old behavior if this field is missing",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-02-14 11:24:10,63
13138448,"Add Hosts is Failing with - Error while bootstrapping: Cannot run program ""/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py""","Add Hosts(UI/API) is broken and the following error is seen .
{code:java}
Error while bootstrapping:
Cannot run program ""/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py"": error=2, No such file or directory
{code}
 ",pull-request-available,[],AMBARI,Bug,Blocker,2018-02-14 10:01:31,5
13138422,Update Hadoop RPC Encryption Properties During Upgrade,"When *HDP 3.0.0* is installed, clients should have the ability to choose encrypted communication over RPC when talking to core hadoop components. Today, the properties that control this are:
 - {{core-site.xml : hadoop.rpc.protection = authentication}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication}}

The new value of {{privacy}} enables clients to choose an encrypted means of communication. By keeping {{authentication}} first, it will be taken as the default mechanism so that wire encryption is not automatically enabled by accident.

The following properties should be changed to add {{privacy}}:
 - {{core-site.xml : hadoop.rpc.protection = authentication,privacy}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication,privacy}}

The following are cases when this needs to be performed:
 - During Kerberization (this case is covered by AMBARI-22803)
 - During a stack upgrade to any version of *HDP 3.0.0*, they should be automatically merged

Blueprint deployment is not a scenario being covered here.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-14 07:53:37,27
13138219,NullPointerException in KerberosHelperImpl.addIdentities,"If a group name or group access is missing from a kerberos descriptor ambari will throw an NPE during enable kerberos
{code:java}

13 Feb 2018 11:27:37,466  WARN [Server Action Executor Worker 39] ServerActionExecutor:471 - Task #39 failed to complete execution due to thrown exception: java.lang.NullPointerException:null
java.lang.NullPointerException
	at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1601)
	at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:177)
	at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:87)
	at org.apache.ambari.server.serveraction.kerberos.PrepareKerberosIdentitiesServerAction.execute(PrepareKerberosIdentitiesServerAction.java:128)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
	at java.lang.Thread.run(Thread.java:748){code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-13 15:54:33,18
13138138,Add namespace-specific layout for service summary page,"- HDFS components should be grouped by namespace
- Service summary page should have separate tab for each namespace",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-13 11:40:54,15
13138132,Ambari API should expose the nameserivceId for NAMENODE host component,"The clusterId can be obtained with this call: http://104.196.73.142:50070/jmx
Look for MBean name Hadoop:service=NameNode,name=NameNodeInfo
And property: ClusterId

This needs to be exposed through Ambari API for the UI to utilize.

",pull-request-available,['ambari-server'],AMBARI,Technical task,Major,2018-02-13 11:01:16,55
13138117,'Assign Slaves & Clients' page is empty when 3 or more services selected while adding multiple services,"STR:
 # In the cluster, click add service
 # Select the remaining services to be added (Pig, Knox, Logsearch, RangerKMS, Zeppelin) and click Next
 # Accept Defaults and Select Next on Assign Masters page

In the Assign Slaves and Clients page, there is an empty page.

Adding services individually does not hit this issue.

There are some error logs in the console:
{code:java}
Uncaught TypeError: Cannot read property 'get' of undefined
at app.js:38709
at Array.forEach (<anonymous>)
at app.js:38707
at Class.forEach (vendor.js:18038)
at app.js:38706
at Array.forEach (<anonymous>)
at Class.enableCheckboxesForDependentComponents (app.js:38705)
at Class.renderSlaves (app.js:38671)
at Class.render (app.js:38646)
at Class.loadStep (app.js:38569)
{code}
Calls to server made during while navigating to Assign Slaves and Clients page",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-02-13 09:27:11,19
13138031,"When host is registering while adding it, Remove Selected button shows up on selecting host","When host is registering(or installing agent) while adding it, Remove Selected button shows up when you select a host

STR:
Try to add host to a cluster. When it is registering, the remove icon(on the right side of the host name) is disabled, which is expected. But at that time, if you select the host checkbox, then Remove Selected button is available which allows user to delete the host while it is registering to the server ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-13 01:26:04,7
13137976,Remove class duplications from Log Search / Log Feeder classpath,"Things like that can happen at runtime:
(Multiple javax.el implementation on Log Search runtime/compile classpath)
{code:java}
Feb 12, 2018 8:03:55 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: Unknown HK2 failure detected:
MultiException stack 1 of 1
javax.validation.ValidationException: HV000183: Unable to initialize 'javax.el.ExpressionFactory'. Check that you have the EL dependencies on the classpath, or use ParameterMessageInterpolator instead
	at org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.buildExpressionFactory(ResourceBundleMessageInterpolator.java:102)
	at org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.<init>(ResourceBundleMessageInterpolator.java:45)
	at org.hibernate.validator.internal.engine.ConfigurationImpl.getDefaultMessageInterpolator(ConfigurationImpl.java:423)
	at org.hibernate.validator.internal.engine.ConfigurationImpl.getDefaultMessageInterpolatorConfiguredWithClassLoader(ConfigurationImpl.java:575)
	at org.hibernate.validator.internal.engine.ConfigurationImpl.getMessageInterpolator(ConfigurationImpl.java:364)
	at org.hibernate.validator.internal.engine.ValidatorFactoryImpl.<init>(ValidatorFactoryImpl.java:144)
	at org.hibernate.validator.HibernateValidator.buildValidatorFactory(HibernateValidator.java:38)
	at org.hibernate.validator.internal.engine.ConfigurationImpl.buildValidatorFactory(ConfigurationImpl.java:331)
	at org.glassfish.jersey.server.validation.internal.ValidationBinder$DefaultValidatorFactoryProvider.provide(ValidationBinder.java:164)
	at org.glassfish.jersey.server.validation.internal.ValidationBinder$DefaultValidatorFactoryProvider.provide(ValidationBinder.java:157)
	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:153)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:487)
	at org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:83)
	at org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:71)
	at org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture$1.call(Cache.java:97)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture.run(Cache.java:154)
	at org.glassfish.hk2.utilities.cache.Cache.compute(Cache.java:199)
	at org.jvnet.hk2.internal.SingletonContext.findOrCreate(SingletonContext.java:122)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2022)
	at org.jvnet.hk2.internal.ServiceHandleImpl.getService(ServiceHandleImpl.java:114)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:695)
	at org.jvnet.hk2.internal.ThreeThirtyResolver.resolve(ThreeThirtyResolver.java:78)
	at org.jvnet.hk2.internal.ClazzCreator.resolve(ClazzCreator.java:212)
	at org.jvnet.hk2.internal.ClazzCreator.resolveAllDependencies(ClazzCreator.java:235)
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:358)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:487)
	at org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:83)
	at org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:71)
	at org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture$1.call(Cache.java:97)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture.run(Cache.java:154)
	at org.glassfish.hk2.utilities.cache.Cache.compute(Cache.java:199)
	at org.jvnet.hk2.internal.SingletonContext.findOrCreate(SingletonContext.java:122)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2022)
	at org.jvnet.hk2.internal.ServiceHandleImpl.getService(ServiceHandleImpl.java:114)
	at org.jvnet.hk2.internal.ServiceHandleImpl.getService(ServiceHandleImpl.java:88)
	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:135)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:487)
	at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:70)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2022)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:774)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getUnqualifiedService(ServiceLocatorImpl.java:786)
	at org.jvnet.hk2.internal.IterableProviderImpl.get(IterableProviderImpl.java:111)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker$Builder.build(ResourceMethodInvoker.java:153)
	at org.glassfish.jersey.server.internal.routing.RuntimeModelBuilder.createInflector(RuntimeModelBuilder.java:128)
	at org.glassfish.jersey.server.internal.routing.RuntimeModelBuilder.createMethodRouter(RuntimeModelBuilder.java:115)
	at org.glassfish.jersey.server.internal.routing.RuntimeModelBuilder.createResourceMethodRouters(RuntimeModelBuilder.java:309)
	at org.glassfish.jersey.server.internal.routing.RuntimeModelBuilder.buildModel(RuntimeModelBuilder.java:173)
	at org.glassfish.jersey.server.internal.routing.Routing$Builder.buildStage(Routing.java:196)
	at org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:587)
	at org.glassfish.jersey.server.ApplicationHandler.access$500(ApplicationHandler.java:184)
	at org.glassfish.jersey.server.ApplicationHandler$3.call(ApplicationHandler.java:350)
	at org.glassfish.jersey.server.ApplicationHandler$3.call(ApplicationHandler.java:347)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
	at org.glassfish.jersey.internal.Errors.processWithException(Errors.java:255)
	at org.glassfish.jersey.server.ApplicationHandler.<init>(ApplicationHandler.java:347)
	at org.glassfish.jersey.servlet.WebComponent.<init>(WebComponent.java:392)
	at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:177)
	at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:369)
	at javax.servlet.GenericServlet.init(GenericServlet.java:244)
	at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:637)
	at org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:421)
	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:760)
	at org.springframework.boot.context.embedded.jetty.JettyEmbeddedWebAppContext$JettyEmbeddedServletHandler.deferredInitialize(JettyEmbeddedWebAppContext.java:46)
	at org.springframework.boot.context.embedded.jetty.JettyEmbeddedWebAppContext.deferredInitialize(JettyEmbeddedWebAppContext.java:36)
	at org.springframework.boot.context.embedded.jetty.JettyEmbeddedServletContainer.handleDeferredInitialize(JettyEmbeddedServletContainer.java:205)
	at org.springframework.boot.context.embedded.jetty.JettyEmbeddedServletContainer.start(JettyEmbeddedServletContainer.java:138)
	at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.startEmbeddedServletContainer(EmbeddedWebApplicationContext.java:297)
	at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.finishRefresh(EmbeddedWebApplicationContext.java:145)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546)
	at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:303)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:134)
	at org.apache.ambari.logsearch.LogSearch.main(LogSearch.java:44)
Caused by: java.lang.NoSuchMethodError: javax.el.ExpressionFactory.newInstance()Ljavax/el/ExpressionFactory;
	at org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.buildExpressionFactory(ResourceBundleMessageInterpolator.java:98)
	... 77 more

{code}",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-02-12 21:48:59,29
13137933,ambari-agent fails to start,"ambari-agent start gives the following exception:

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/ambari_agent/main.py"", line 92, in <module>
 import ProcessHelper
 File ""/usr/lib/ambari-agent/lib/ambari_agent/ProcessHelper.py"", line 25, in <module>
 from ambari_commons.shell import getTempFiles
 File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 30, in <module>
 from resource_management.core import sudo
 File ""/usr/lib/ambari-agent/lib/resource_management/__init__.py"", line 23, in <module>
 from resource_management.libraries import *
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/__init__.py"", line 23, in <module>
 from resource_management.libraries.functions import *
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/__init__.py"", line 48, in <module>
 from resource_management.libraries.functions.log_process_information import *
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/log_process_information.py"", line 22, in <module>
 from ambari_commons.shell import shellRunner
ImportError: cannot import name shellRunner
{code}

Most likely because of a circular dependency that was added by AMBARI-22888. Removing the sudo import from shell makes the problem go away.

sudo -> resource_management.libraries.* -> resource_management.libraries.functions.* -> esource_management.libraries.functions.log_process_information.* -> ambari_commons.shell.shellRunner -> resource_management.core.sudo",pull-request-available,['ambari-agent'],AMBARI,Task,Blocker,2018-02-12 18:41:08,1
13137828,Add NameNode Federation wizard,Basic implementation of NN Federation wizard.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-12 11:04:39,9
13137795,Blacklist by default /boot/efi path as mount point reported by Ambari Agent,"On EFI Based systems, efi partition mounting under /boot/efi path. It causing Agent to report this mount point as available partition location. On new cluster deploy, this partition could be proposed by stack_adviser as datanode storage location, which is wrong.

Workaround for versions without this patch: add \{{/boot/efi}} to \{{ignore_mount_points}} property, \{{agent}} section in ambari-agent.ini configuration file.

Problem source:
- https://community.hortonworks.com/questions/118551/namenode-can-not-be-started-1.html 
- ASF mailing list",pull-request-available,['ambari-agent'],AMBARI,Task,Major,2018-02-12 08:39:51,1
13137750,Ambari agent kills parent process on stop,"If ambari-agent was started from bash script (for example), this script will be killed on ambari-agent stop.
Issue prevents some automatization scenarios.",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-02-11 23:53:42,6
13137634,Upgrade Apache Rat to 0.12,"Apache Rat 0.12 can automatically exclude patterns from SCM ignore files ({{.gitignore}} in Ambari's case) from its check.  See RAT-171 for details.

Benefits:

# No need to duplicate the exclude information in the {{apache-rat-plugin}}'s configuration.
# Improved run time for non-clean workdir, since it can exclude everything under {{target}}.
",pull-request-available,[],AMBARI,Improvement,Major,2018-02-10 15:45:43,21
13137576,Stack Metainfo.xml should contain the osSpecifics tag. Add as part of metainfo generation.,"{noformat}
<osSpecifics>
      <osSpecific> 
         <osFamily>any</osFamily>
         <packages>
           <package>
              <name>hdpcore</name>
              <condition></condition>
              <skipUpgrade>false</skipUpgrade>
           </package>
         </packages>
      </osSpecific>
    </osSpecifics>
{noformat}
This should be added to metainfo.xml for each stack during its generation. The only package that we now install is yum install <mpackname> which takes dependencies on all modules of the mpack.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-09 23:12:59,56
13137542,Bulk operation to turn on Maintanance Mode on hosts does not work,"It is possible in some circumstances for a single-repository cluster to be OUT_OF_SYNC, so the web client should still be able to handle this situation if it arises.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-09 20:37:51,34
13137456,"Install: Central configuration of DB's, Passwords, Log Directories, Users","During installation, users commonly have to configure DB's, Passwords, and Log Directories. Instead of forcing our users to click into every individual service to find the appropriate fields for those three common configurations we should create a separate step in the installation where users can easily set these three things, and the rest of the configuration we will consider as 'Advanced' configuration.

We need to ensure that users can see all required credentials, databases, service accounts, log directories (data, log, pid) for each service in a single section.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-09 15:10:33,9
13137446,Heartbeat gets lost due to subprocess lock,"Subprocess has problem when run in multithreaded environment. As stated by developers it should not be used in such env.
As a result Ambari done multiple patches to subprocess. However still we are having multithreading problems with it.

This jira targets moving from Subprocess to Subprocess32 (a port of Python3.0 subprocess) which support multithreaded execution.",pull-request-available,[],AMBARI,Bug,Major,2018-02-09 14:08:49,5
13137432,UI Install is unable to work with 'Redhat Satellite/Spacewalk' as local repository,"The UI must allow changing the repo_id field when enabling RH Satellite. it just needs to be a text box, don't allow (trimmed) empty strings or blanks. Each repo must have an ID. Text inputs only count for redhat/centos. (Ubuntu, suse, etc don't have this).",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-09 12:53:15,19
13137429,Export JAVA_HOME env variable in shell script for Ranger,"Need to export JAVA_HOME variable in ranger-admin-env.sh, so that [ranger-admin-services.sh|https://github.com/apache/ranger/blob/master/embeddedwebserver/scripts/ranger-admin-services.sh] knows which java to be used for processing.",pull-request-available,[],AMBARI,Bug,Major,2018-02-09 12:32:54,25
13137326,SPNEGO service keytab is getting deleted upon deleting component from host,"spnego.service.keytab is getting deleted upon deleting components.

Steps to reproduce :
# Add additional ""livy"" component to some host in the cluster
# Delete added ""livy"" component
# Deletion of livy is deleting /etc/security/keytabs/spnego.service.keytab as well


The cause of this is due to an invalid check to determine if a Kerberos identity is a reference or no at 
{code:title=org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptorContainer#getIdentitiesSkipReferences}
  public List<KerberosIdentityDescriptor> getIdentitiesSkipReferences() {
    return nullToEmpty(getIdentities())
      .stream()
      .filter(identity -> !identity.getReferencedServiceName().isPresent() && identity.getName() != null && !identity.getName().startsWith(""/""))
      .collect(toList());
  }
{code}

The fixed code should be

{code:title=org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptorContainer#getIdentitiesSkipReferences}
  public List<KerberosIdentityDescriptor> getIdentitiesSkipReferences() {
    return nullToEmpty(getIdentities())
      .stream()
.filter(identity -> !identity.getReferencedServiceName().isPresent() && !identity.isReference())      .collect(toList());
  }
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-09 02:19:16,30
13137290,All Container-executor.cfg properties should be managed by ambari ui,"Currently, only few properties from Container-executor.cfg can be managed via ambari UI. 
Container-executor.cfg should be handled similar to yarn-env.sh in ambari.
This way ambari ui can manage all config properties for Container-executor.cfg",pull-request-available,[],AMBARI,Bug,Major,2018-02-08 23:06:22,27
13137277,Download client configs API failing with 500 server error,"Download client configs API failing with 500 server error
{code:java}
http://<AMBARI_SERVER>:<PORT>/api/v1/clusters/cl1/components?format=client_config_tar
http://<AMBARI_SERVER>:<PORT>/api/v1/clusters/cl1/hosts/<HOST_NAME>/host_components?format=client_config_tar

{ ""status"": 500, ""message"": ""org.apache.ambari.server.controller.spi.SystemException: Execution of \""ambari-python-wrap /var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py generate_configs /var/lib/ambari-server/data/tmp/KERBEROS_CLIENT2999359502875665328-configuration.json /var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package /var/lib/ambari-server/data/tmp/structured-out.json INFO /var/lib/ambari-server/data/tmp\"" returned 1. java.lang.Throwable: Traceback (most recent call last):\n File \""/var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py\"", line 20, in <module>\n from kerberos_common import *\n File \""/var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_common.py\"", line 32, in <module>\n from ambari_agent import Constants\nImportError: No module named ambari_agent\n"" }


{code}",pull-request-available,['ambari-sever'],AMBARI,Bug,Major,2018-02-08 22:11:03,18
13137275,Enhance host components API to support multiple host component instances.,"*BACKGROUND:*

Given that in Ambari, we refer components of a services in 2 ways: 

  - *1.* One from the Service component APIs : 
    *http://<AmbariServerHost>:8080/api/v1/clusters/<clusterName>/services/<ServiceName>/components/<componentName>*

  - *2.*From the host component APIs (which tell us the host on which the current component is resides)
    *http://<AmbariServerHost>:8080/api/v1/clusters/<clusterName>/hosts/<hostName>/host_components/<hostComponentName>*

_Note that, we of now, we are referring both 1and 2 using the component names_

But in the multi-component world for Ambari, we will come up with a situation like this, as shown below:

 - *A.* If *core* _Service Group_ has its *HDFS_CLIENT* type component named as *HDFS_CLIENT* 
    *Core -> HDFS -> HDFS_CLIENT*
 - *B.* And at the same time, we have *edw* _Service Group_ having its *HDFS_CLIENT* type component named as *HDFS_CLIENT* : 
    *edw -> HDFS -> HDFS_CLIENT*

There is  no good way to distinguish a given host component in *A* and *B*, if we continue to refer them with the component name end point (No. *2* API call). 
However, No. *1* call is still fine as we will not allow 2 names for the same component within a given service, thus making it unique and allowing us to continue using the component names endpoint.   




*****************************************************************WORK DONE :*******************************************************************



In order to support multi-instance for components of a given service, we need to enhance the *Host Components API* so that they can distinguish *one component instance of a service* from *another component instance of a service* with the same Name.

The way to achieve this is to move Host Components API to be *ID* (number) based end point, compared to earlier being a name based endpoint. This will allow us to distinguish  *Core -> HDFS -> HDFS_CLIENT* from *edw -> HDFS -> HDFS_CLIENT*, as an example.

Thus, following changes are required:

  - New field : *component_type* (eg: HDFS_CLIENT, HIVE_SERVER, ZOOKEEPER_SERVER etc)
  - Existing field : *component_name* will now hold the actual name given for the component at the time of install. For example: 
     - *HDFS_CLIENT_EDW* for component_type *HDFS_CLIENT*
     - *HIVE_SERVER* FOR component_type *HIVE_SERVER*  _(name can be same as type also)_
     - *HIVE_SERVER1* for component_type *HIVE_SERVER*
     - *ZOOKEEPER_SERVER_FOR_KAFKA* for component_type *ZOOKEEPER_SERVER*, and so forth.
     
  - A way to identify the HOST component uniquely via API calls, by way of referring them by *ID* instead of name.


*MODIFIED API CALLs:*

1. POST SERVICE COMPONENT API call 
 
- CHANGE : New field *component_type*
- POST http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/servicegroups/core/services/HDFS/components

  Body :

  {code}
  [{""ServiceComponentInfo"":{""component_name"":""HDFS_CLIENT2"",""component_type"":""HDFS_CLIENT""}}]
  {code}  

  Response:
  
  {code}
{
  ""resources"" : [
    {
      ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/servicegroups/core/services/HDFS/components/HDFS_CLIENT2"",
      ""ServiceComponentInfo"" : {
        ""category"" : null,
        ""cluster_id"" : 2,
        ""cluster_name"" : ""c1"",
        ""component_name"" : ""HDFS_CLIENT2"",
        ""component_type"" : ""HDFS_CLIENT"",
        ""desired_stack"" : ""HDP-2.6"",
        ""desired_version"" : ""NOT_REQUIRED"",
        ""display_name"" : ""HDFS Client"",
        ""id"" : 51,
        ""recovery_enabled"" : false,
        ""service_group_id"" : 2,
        ""service_group_name"" : ""core"",
        ""service_id"" : 3,
        ""service_name"" : ""HDFS"",
        ""service_type"" : ""HDFS"",
        ""state"" : ""STARTED"",
        ""total_count"" : {
          ""installFailedCount"" : 0,
          ""unknownCount"" : 0,
          ""installedCount"" : 0,
          ""initCount"" : 0,
          ""installedAndMaintenanceOffCount"" : 0,
          ""startedCount"" : 0,
          ""totalCount"" : 0
        }
      }
    }
  ]
}
  {code}
  

2. POST HOST COMPONENT API call 

- CHANGE : New field *component_type*
- POST http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>/host_components
  Body
  {code}
   {""HostRoles"":{""cluster_name"": ""c1"",""component_name"":""HDFS_CLIENT2"",""component_type"":""HDFS_CLIENT""}}
  {code}

- Response:
  {code}
   {
  ""resources"" : [
    {
      ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>/host_components/51"",
      ""HostRoles"" : {
        ""actual_configs"" : { },
        ""cluster_id"" : 2,
        ""cluster_name"" : ""c1"",
        ""component_name"" : ""HDFS_CLIENT2"",
        ""component_type"" : ""HDFS_CLIENT"",
        ""desired_admin_state"" : null,
        ""desired_repository_version"" : ""2.6.4.0-91"",
        ""desired_stack_id"" : ""HDP-2.6"",
        ""display_name"" : ""HDFS_CLIENT2"",
        ""host_name"" : ""<host1>"",
        ""id"" : 51,
        ""maintenance_state"" : null,
        ""public_host_name"" : ""<host1>"",
        ""service_group_id"" : 2,
        ""service_group_name"" : ""core"",
        ""service_id"" : 3,
        ""service_name"" : ""HDFS"",
        ""service_type"" : ""HDFS"",
        ""stale_configs"" : false,
        ""state"" : ""INIT"",
        ""upgrade_state"" : ""NONE"",
        ""version"" : ""UNKNOWN""
      },
      ""host"" : {
        ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>""
      }
    }
  ]
}
  {code}


3. GET SERVICE COMPONENT:

   - CHANGE: *host_components* sub-resource will be referenced with end point as ID one.

   - GET http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/servicegroups/core/services/HDFS/components/HDFS_CLIENT2

   - Response: 
     {code}
      {
  ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/servicegroups/core/services/HDFS/components/HDFS_CLIENT2"",
  ""ServiceComponentInfo"" : {
    ""category"" : null,
    ""cluster_id"" : 2,
    ""cluster_name"" : ""c1"",
    ""component_name"" : ""HDFS_CLIENT2"",
    ""component_type"" : ""HDFS_CLIENT"",
    ""desired_stack"" : ""HDP-2.6"",
    ""desired_version"" : ""2.6.4.0-91"",
    ""display_name"" : ""HDFS Client"",
    ""id"" : 51,
    ""init_count"" : 1,
    ""install_failed_count"" : 0,
    ""installed_and_maintenance_off_count"" : 0,
    ""installed_count"" : 0,
    ""recovery_enabled"" : ""false"",
    ""repository_state"" : ""NOT_REQUIRED"",
    ""service_group_id"" : 2,
    ""service_group_name"" : ""core"",
    ""service_id"" : 3,
    ""service_name"" : ""HDFS"",
    ""service_type"" : ""HDFS"",
    ""started_count"" : 0,
    ""state"" : ""STARTED"",
    ""total_count"" : 1,
    ""unknown_count"" : 0
  },
  ""host_components"" : [
    {
      ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>/host_components/51"",
      ""HostRoles"" : {
        ""cluster_name"" : ""c1"",
        ""component_name"" : ""HDFS_CLIENT2"",
        ""host_name"" : ""<host1>"",
        ""id"" : 51,
        ""service_group_name"" : ""core"",
        ""service_name"" : ""HDFS""
      }
    }
  ]
}
     {code}



4. GET HOST COMPONENT:

   - CHANGE: Called with ID based end point

   - GET http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>/host_components/51

   - Response:

    {code}
     {
  ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>/host_components/51"",
  ""HostRoles"" : {
    ""cluster_name"" : ""c1"",
    ""component_name"" : ""HDFS_CLIENT2"",
    ""component_type"" : ""HDFS_CLIENT"",
    ""desired_repository_version"" : ""2.6.4.0-91"",
    ""desired_stack_id"" : ""HDP-2.6"",
    ""desired_state"" : ""INIT"",
    ""display_name"" : ""HDFS_CLIENT2"",
    ""host_name"" : ""<host1>"",
    ""id"" : 51,
    ""maintenance_state"" : ""OFF"",
    ""public_host_name"" : ""<host1>"",
    ""reload_configs"" : false,
    ""service_group_name"" : ""core"",
    ""service_name"" : ""HDFS"",
    ""stale_configs"" : false,
    ""state"" : ""INIT"",
    ""upgrade_state"" : ""NONE"",
    ""version"" : ""UNKNOWN"",
    ""actual_configs"" : { }
  },
  ""host"" : {
    ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/hosts/<host1>""
  },
  ""processes"" : [ ],
  ""component"" : [
    {
      ""href"" : ""http://{{AmbariServer}}:8080/api/v1/clusters/<clusterName>/servicegroups/core/services/HDFS/components/HDFS_CLIENT2"",
      ""ServiceComponentInfo"" : {
        ""cluster_name"" : ""c1"",
        ""component_name"" : ""HDFS_CLIENT2"",
        ""service_group_name"" : ""core"",
        ""service_name"" : ""HDFS""
      }
    }
  ]
}
    {code}   

5. UPDATE SERVICE COMPONENTS
    - NO CHANGE

6. UPDATE HOST COMPONENTS 
   - CHANGE: API end point is ID based now.

 7. DELETE SERVICE COMPONENT:
     - No CHANGE

8. DELETE HOST COMPONENT: 
    - CHANGE: API end point is ID based now.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-08 21:58:09,43
13137257,Wrong variable in KerberosKeytabDescriptor#toMap(),"{code:title=org/apache/ambari/server/state/kerberos/KerberosKeytabDescriptor.java:466}
    if (!owner.isEmpty()) {
      map.put(KEY_GROUP, group);
    }
{code}
The if statement checks wrong variable.

Should read
{code}
    if (!group.isEmpty()) {
      map.put(KEY_GROUP, group);
    }
{code}
",pull-request-available,[],AMBARI,Bug,Major,2018-02-08 20:52:17,27
13137130,"Debian stackdeploys failing with ""ambari-agent: command not found"" errors","Debian stackdeploys failing with ""ambari-agent: command not found"" errors even
though ambari-agent is installed.

From a debian cluster:

    
    
    
    root@ctr-e137-1514896590304-63273-01-000006:~# dpkg-query -l | grep ambari
    ii  ambari-agent                                  2.6.2.0-45                        amd64        Ambari Agent
    root@ctr-e137-1514896590304-63273-01-000006:~# find / -name ambari-agent
    /run/ambari-agent
    /usr/lib/ambari-agent
    /var/log/ambari-agent
    /var/lib/ambari-agent
    /var/lib/ambari-agent/bin/ambari-agent
    /etc/init.d/ambari-agent
    /etc/ambari-agent
    /grid/0/log/ambari-agent
",pull-request-available,[],AMBARI,Bug,Major,2018-02-08 11:37:11,5
13137126,Service-auto start page update of UI,See screenshot,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-08 11:18:22,19
13137085,Ambari throws NPE when deleting a service,"When a config group exists with service_name = null, ambari will throw a NullPointerException after removing a service. Ambari tries to remove config groups after deleting a service which causes the NPE",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-08 08:14:10,18
13136968,Deactivate unsupported stack definitions in Ambari,"Deactivate unsupported stack definitions in Ambari.  This includes HDP versions before 2.6.

{noformat:title=Active Stacks}
./3.0/metainfo.xml:    <active>true</active>
./2.6/metainfo.xml:    <active>true</active>
./2.5/metainfo.xml:    <active>true</active>
./2.4/metainfo.xml:    <active>true</active>
./2.3/metainfo.xml:    <active>true</active>
{noformat}

There is no enforcement of minimum stack level during the upgrade process. The user is expected to have performed the necessary actions to meet the _documented_ minimum stack level. 

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-02-07 21:51:34,30
13136953,[API] Updating current stack repo without GPL repos in the body does not throw any error,"# Install HDP 2.6.4.0 stack with Ambari 2.6.1
 # Update the current repository versions with the following body on ambari-2.6.1

{code:java}
PUT http://<ambari-server>:8080/api/v1/stacks/HDP/versions/2.6/repository_versions/1
{
 ""operating_systems"": [
 {
 ""OperatingSystems"": {
 ""os_type"": ""debian7"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/debian7/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian6"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""redhat6"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.2.0/"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos6"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""redhat7"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""suse11"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/suse11sp3"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""suse12"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/suse11sp3"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""ubuntu12"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu12"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""ubuntu14"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu14"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""ubuntu16"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/ubuntu16/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu16"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 }
 ]
}
{code}
Response: 200 OK

Navigating to the UI results in GPL fields missing.

Expected: Error to be thrown when trying to update repos without the GPL repo when stack >= HDP-2.6.4.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-07 20:29:12,6
13136950,Create UpgradeCatalog270 to migrate data,"Create UpgradeCatalog270 to migrate data.

This will entail renaming \{{org.apache.ambari.server.upgrade.UpgradeCatalog300}} to \{{org.apache.ambari.server.upgrade.UpgradeCatalog270}} and making adjustments as needed.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-02-07 20:19:22,30
13136931,Pre-Upgrade Checks Should Have Structured Output,"The UI needs to know in which host component installations have failed in order to add opportunity re-install them from pre-upgrade dialog. For now, we have this info only in ""reason"" string.
Components installation info should have map between host and failed components. 
Now it is:
{code}
 {
      ""href"" : ""http://104.196.88.201:8080/api/v1/clusters/test/rolling_upgrades_check/COMPONENTS_INSTALLATION"",
      ""UpgradeChecks"" : {
        ""check"" : ""All service components must be installed"",
        ""check_type"" : ""SERVICE"",
        ""cluster_name"" : ""test"",
        ""failed_detail"" : [ ],
        ""failed_on"" : [
          ""ATLAS"",
          ""HIVE"",
          ""SLIDER"",
          ""TEZ"",
          ""MAPREDUCE2"",
          ""KAFKA"",
          ""YARN"",
          ""PIG""
        ],
        ""id"" : ""COMPONENTS_INSTALLATION"",
        ""reason"" : ""Found service components in INSTALL_FAILED state. Please re-install these components. Service components in INSTALL_FAILED state: [KAFKA:KAFKA_BROKER on aantonenko252-1.c.pramod-thangali.internal], [ATLAS:ATLAS_SERVER on aantonenko252-1.c.pramod-thangali.internal], [YARN:RESOURCEMANAGER on aantonenko252-1.c.pramod-thangali.internal], [PIG:PIG on aantonenko252-1.c.pramod-thangali.internal], [HIVE:HIVE_SERVER on aantonenko252-1.c.pramod-thangali.internal], [TEZ:TEZ_CLIENT on aantonenko252-1.c.pramod-thangali.internal], [YARN:NODEMANAGER on aantonenko252-1.c.pramod-thangali.internal], [MAPREDUCE2:MAPREDUCE2_CLIENT on aantonenko252-1.c.pramod-thangali.internal], [HIVE:HCAT on aantonenko252-1.c.pramod-thangali.internal], [HIVE:HIVE_METASTORE on aantonenko252-1.c.pramod-thangali.internal], [HIVE:WEBHCAT_SERVER on aantonenko252-1.c.pramod-thangali.internal], [YARN:YARN_CLIENT on aantonenko252-1.c.pramod-thangali.internal], [HIVE:HIVE_CLIENT on aantonenko252-1.c.pramod-thangali.internal], [SLIDER:SLIDER on aantonenko252-1.c.pramod-thangali.internal]."",
        ""repository_version_id"" : 4,
        ""status"" : ""FAIL"",
        ""upgrade_type"" : ""ROLLING""
      }
    }
{code}

The proposal is to add structured output to each of the upgrade pre-checks which could have automated UI actions associated with them. For example:

{code}
""failed_detail"" : [ 
  {
  ""host_name"" : aantonenko252-1.c.pramod-thangali.internal,
  ""service_name"" : ""HIVE"",
  ""component_name"" : ""HIVE_SERVER""
  },
  {
  ""hostname_name"" : aantonenko252-1.c.pramod-thangali.internal,
  ""service_name"" : ""HIVE"",
  ""component_name"" : ""HIVE_CLIENT""
  },
...
],
{code}",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-07 19:06:56,31
13136922,Restart failure due to Library scripts not able to handle multiple nameservices,"Currently, we assume there is single shared edits dir and other one exists only for DR cluster.

{code}
stderr: 
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 969, in restart
    self.status(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/historyserver.py"", line 117, in status
    check_process_status(status_params.mapred_historyserver_pid_file)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 43, in check_process_status
    raise ComponentIsNotRunning()
ComponentIsNotRunning

The above exception was the cause of the following exception:

Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/historyserver.py"", line 132, in 
    HistoryServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 368, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 980, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/historyserver.py"", line 87, in start
    self.configure(env) # FOR SECURITY
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 121, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/historyserver.py"", line 54, in configure
    yarn(name=""historyserver"")
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/yarn.py"", line 89, in yarn
    setup_historyserver()
  File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/yarn.py"", line 298, in setup_historyserver
    recursive_chmod=True
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 604, in action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 601, in action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 321, in action_delayed
    main_resource.resource.security_enabled, main_resource.resource.logoutput)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 154, in __init__
    security_enabled, run_user)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/namenode_ha_utils.py"", line 173, in get_property_for_active_namenode
    if INADDR_ANY in value and rpc_key in hdfs_site:
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.https-address' was not found in configurations dictionary!
 stdout:
{code}

",pull-request-available,[],AMBARI,Technical task,Blocker,2018-02-07 18:17:00,55
13136765,Hive View 1.5 Upload Table - not able to change datatype from the dropdown for a wide file,"Steps:
 # Go to Hive view 1.5,
 # Select type CSV
 # Upload the attached file
 # Try to change the datatype of last column
 # The UI comes back to the leftmost column, showing the first few columns",pull-request-available,['ambari-views'],AMBARI,Bug,Blocker,2018-02-07 07:01:09,45
13136717,UI changes in the Background Ops modal,Tweak the UI for the background ops modal.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-07 01:03:56,34
13136645,Mpack/service group admin screen,Create Management Pack admin screen accessible from main menu. This will show information about currently installed mpacks and is the entry point for upgrading mpacks and performing other mpack related administrative actions.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-06 19:07:58,40
13136578,Style edits of Service Configuration Group modal,"Move and align checkboxes to the left in Select configuration Group Hosts modal.
See screenshot.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-06 14:37:44,19
13136552,"Unable to enable hive interactive, LLAP, on our Ambari 2.5.2 managed cluster","While trying to enable the hive interactive plugin identified
that Ambari was looking in the incorrect directory when trying to locate the
users keytab.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-06 12:26:37,5
13136498,Decommission RegionServer fails when kerberos is enabled,"When kerberos is enabled, Decommission RegionServer fails with the following errors:

stderr:
{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py"", line 114, in <module>
    HbaseMaster().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 329, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py"", line 55, in decommission
    hbase_decommission(env)
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_decommission.py"", line 84, in hbase_decommission
    logoutput=True
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/hbase.service.keytab hbase/master2@EXAMPLE.COM; /usr/hdp/current/hbase-master/bin/hbase --config /usr/hdp/current/hbase-master/conf -Djava.security.auth.login.config=/usr/hdp/current/hbase-master/conf/hbase_master_jaas.conf org.jruby.Main /usr/hdp/current/hbase-master/bin/draining_servers.rb add worker1' returned 1. Error: Could not find or load main class org.jruby.Main{code}
stdout:
{code:java}
2018-02-06 07:25:03,453 - Stack Feature Version Info: Cluster Stack=2.6, Cluster Current Version=2.6.2.0-205, Command Stack=None, Command Version=2.6.2.0-205 -> 2.6.2.0-205
2018-02-06 07:25:03,476 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
2018-02-06 07:25:03,484 - checked_call['hostid'] {}
2018-02-06 07:25:03,490 - checked_call returned (0, '1aacc56c')
2018-02-06 07:25:03,502 - File['/usr/hdp/current/hbase-master/bin/draining_servers.rb'] {'content': StaticFile('draining_servers.rb'), 'mode': 0755}
2018-02-06 07:25:03,504 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/hbase.service.keytab hbase/master2@EXAMPLE.COM; /usr/hdp/current/hbase-master/bin/hbase --config /usr/hdp/current/hbase-master/conf -Djava.security.auth.login.config=/usr/hdp/current/hbase-master/conf/hbase_master_jaas.conf org.jruby.Main /usr/hdp/current/hbase-master/bin/draining_servers.rb add worker1'] {'logoutput': True, 'user': 'hbase'}
Error: Could not find or load main class org.jruby.Main

Command failed after 1 tries{code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-06 08:10:38,64
13136350,Dashboard-Metrics page style edits,See screenshots,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-05 18:35:04,19
13136308,Oracle DDL is broken at trunk,"Looks like Oracle DDL is broken
{code}
ERROR at line 1:
ORA-00904: ""CREATE_TIMESTAMP"": invalid identifier


insert into user_authentication(user_authentication_id, user_id, authentication_type, authentication_key, create_time, update_time)
*
ERROR at line 1:
ORA-02291: integrity constraint (AMBARI.FK_USER_AUTHENTICATION_USERS) violated
- parent key not found
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-05 16:42:06,30
13136298,Add ability to MasterHostResolver to resolve by namespace at a time,The upgrade behavior which figures out the restart order for the Namenodes needs to become namespace aware to support NN federation in Ambari.,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-02-05 16:05:48,18
13136285,Host details page: components not reconfigured after deleting their host,"# When deleting a host, after components deletion config changes not triggered, as opposed to when they removed individually.
 # When adding/deleting component config modification triggered before actual call to add/delete a component, so even if the call fails then configs would be modified",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-05 14:21:19,19
13136277,Log Search UI: move Capture button to top menu,"- Move 'Capture' button from filters panel to top menu
- Buttons order: Undo, Redo, History, Filter, Capture, Refresh",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-05 13:59:09,15
13136237,Add Ambari admin/pw CLI options for setup-ldap tool,"Since we use Amabri's REST API in the _setup-ldap_ tool to persists its outcome into the DB we require the end-user to type his/her Ambari username and password during the user input phase.
 This makes automated integration tests very hard to implement.

The solution is to add two new CLI option names for _setup-ldap_ (similarly to the _sync-ldap_ tool):
{code:java}
--ldap-setup-admin-name
--ldap-setup-admin-password{code}",ldap pull-request-available,[],AMBARI,Improvement,Critical,2018-02-05 10:16:48,27
13136235,Log Search UI: implement filter by username for access logs,User should be able to filter access logs by username,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-05 10:14:26,15
13135889,Fix bugs in install wizard,"This fixes the following bugs:
 * Tooltip not activated on Select Mpacks screen.
 * Verify Repos step not being skipped when using Public Repos.
 * Data load timing issues when moving to steps 5 and 6.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-02 21:43:18,40
13135824,Not able to register new HDP version after upgrading to Ambari2.6.1,"Steps to reproduce the issue

1. Install Ambari2.5.x and install HDP on that.

2. Now Upgrade ambari to 2.6.1 version.

3. Now try to register HDP version with VDF file - Save button is not enabled (also UI does not load repo URLs and)

4. In JS it fails with below error

Cannot read property 'gpl.license.accepted' of undefined
What is expected: UI should properly show the message that gpl license needs to be enabled. also Upgrade should take care of adding this property if it is needed.

Workaround for this is : add ""gpl.license.accepted=true"" to ambari.proeprties and then restart should fix the problem",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-02 17:57:18,5
13135750,Support old option names in setup-ldap tool,"While working on AMBARI-22797 we modified the following option names for the setup-ldap tool in AMBARI server:
 # {{ldap-url}} became {{ldap-primary-host}} and {{ldap-primary-port}}
 # {{ldap-secondary-url}} became {{ldap-secondary-host}} and {{ldap-secondary-port}}

To support backward compatibility we need to support old option names too.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-02 11:48:46,27
13135601,Mpack and Stack REST API Changes,Revise the APIs based on new layout,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-01 21:15:18,56
13135513,Update kerberos-env/admin_server_host description to specify KCD admin host must be a FQDN,"Update kerberos-env/admin_server_host description to specify KCD admin host must be a FQDN.

The following needs to be changed in
* {{common-services/KERBEROS/1.10.3-10/configuration/kerberos-env.xml:168}}
* {{common-services/KERBEROS/1.10.3-30/configuration/kerberos-env.xml:168}}
{code}
The IP address or FQDN for the KDC Kerberos administrative host. Optionally a port number may be included.
{code}
To 
{code}
The FQDN for the KDC Kerberos administrative host. Optionally a port number may be included.
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-01 18:13:12,27
13135486,View Log feature for failed mpack downloads,"Implement ""view log"" feature for failed mpacks downloads, which will allow the user to view the error message in the server response when an mpack registration fails.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-01 16:46:40,40
13135482,Log Search UI: implement 'History' functionality,History of user's actions should be available for viewing and undoing/redoing.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-01 16:36:40,15
13135470,Upgrade Infra Solr version to 7.2.1,"Upgrade Solr version to at least 7.2.1

That will require to make Atlas and Ranger work with the new Solr as well.

For Log Search, as its not GA yet, its ok to drop the collections. Preferred way: rename the old collections to a new one",pull-request-available,"['ambari-infra', 'ambari-logsearch']",AMBARI,Bug,Major,2018-02-01 16:01:50,29
13135445,Tez view button doesnt works in ambari hive views 1.5,"*Issue :* 
Login to ambari version 2.6.1 navigate to hive view 1.5

When click on tez button or visualization or visual explain  or settings button
the page throws a script error and it become a blank page 

*Root Cause :*

when click on the tez button, visualization or any other side button , 
the following error is thrown from ember :
Assertion Failed: You attempted to render into 'index' but it was not found

attached the image error

",ambari-views,['ambari-views'],AMBARI,Bug,Major,2018-02-01 14:37:28,3
13135406,Fix TestAlertSchedulerHandler.py and TestAlerts.py,"How the configs and alerts definitions are consumed by the alerts framework
has completely changed on branch-3.0-perf so most alert-related tests require
rewriting.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-01 12:42:01,5
13135369,Mysql connection fails for Ranger Admin configuration during service installation," 
{code:java}
 
 2018-02-01 15:53:09,969 - Host checks started.
 2018-02-01 15:53:09,969 - Check execute list: db_connection_check
 2018-02-01 15:53:09,969 - DB connection check started.
 2018-02-01 15:53:09,970 - Custom java is not available on host. Please install it. Java home should be the same as on server.
2018-02-01 15:53:09,971 - Host checks completed.
 2018-02-01 15:53:09,971 - Check db_connection_check was unsuccessful. Exit code: 1. Message: Custom java is not available on host. Please install it. Java home should be the same as on server.
Command failed after 1 tries
{code}
 ",pull-request-available,[],AMBARI,Bug,Major,2018-02-01 10:26:16,65
13135295,Install zk error when cluster just only installed zk.,"cluster with only the service of zookeeper.

 

when start zk, error is like this:
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 40, in <module>
    BeforeStartHook().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 314, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 28, in hook
    import params
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/params.py"", line 320, in <module>
    if namenode_rpc:
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'core-site' was not found in configurations dictionary!
Error: Error: Unable to run the custom hook script ['/usr/bin/python', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py', 'START', '/var/lib/ambari-agent/data/command-31.json', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START', '/var/lib/ambari-agent/data/structured-out-31.json', 'INFO', '/var/lib/ambari-agent/tmp', 'PROTOCOL_TLSv1']",pull-request-available trunk,['ambari-server'],AMBARI,Bug,Major,2018-02-01 02:46:40,58
13135243,Logging improvement during the Upgrade when there is invalid Alert definition,"If there is invalid definition/json in alert definition then Upgrade would fail with below exception. there is no way to determine which alert is causing the issue here.

its not clear how this alert definition gets corrupted but we have seen this with 3 to 4 customers already - printing the alert definition name would he helpful in support point of view to determine the root cause.. other wise it is very difficult to find it out - had a give a dev patch to customers to find it out.


{noformat}
29 Jan 2018 19:58:50,173 ERROR [main] AlertDefinitionFactory:199 - Unable to deserialize the alert definition source during coercion
com.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected STRING but was BEGIN_OBJECT
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:176)
	at com.google.gson.Gson.fromJson(Gson.java:795)
	at com.google.gson.Gson.fromJson(Gson.java:859)
	at com.google.gson.Gson$2.deserialize(Gson.java:131)
	at org.apache.ambari.server.state.alert.AlertDefinitionFactory$AlertDefinitionSourceAdapter.deserialize(AlertDefinitionFactory.java:373)
	at org.apache.ambari.server.state.alert.AlertDefinitionFactory$AlertDefinitionSourceAdapter.deserialize(AlertDefinitionFactory.java:313)
	at com.google.gson.TreeTypeAdapter.read(TreeTypeAdapter.java:58)
	at com.google.gson.Gson.fromJson(Gson.java:795)
	at com.google.gson.Gson.fromJson(Gson.java:761)
	at com.google.gson.Gson.fromJson(Gson.java:710)
	at com.google.gson.Gson.fromJson(Gson.java:682)
	at org.apache.ambari.server.state.alert.AlertDefinitionFactory.coerce(AlertDefinitionFactory.java:196)
	at org.apache.ambari.server.api.services.AmbariMetaInfo.reconcileAlertDefinitions(AmbariMetaInfo.java:1150)
	at org.apache.ambari.server.upgrade.UpdateAlertScriptPaths.executeDMLUpdates(UpdateAlertScriptPaths.java:46)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:946)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
Caused by: java.lang.IllegalStateException: Expected STRING but was BEGIN_OBJECT
	at com.google.gson.internal.bind.JsonTreeReader.nextString(JsonTreeReader.java:154)
	at com.google.gson.internal.bind.TypeAdapters$13.read(TypeAdapters.java:349)
	at com.google.gson.internal.bind.TypeAdapters$13.read(TypeAdapters.java:337)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172)
	... 16 more
29 Jan 2018 19:58:50,176 ERROR [main] SchemaUpgradeHelper:239 - Upgrade failed. 
java.lang.NullPointerException
	at org.apache.ambari.server.api.services.AmbariMetaInfo.reconcileAlertDefinitions(AmbariMetaInfo.java:1163)
	at org.apache.ambari.server.upgrade.UpdateAlertScriptPaths.executeDMLUpdates(UpdateAlertScriptPaths.java:46)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:946)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
29 Jan 2018 19:58:50,177 ERROR [main] SchemaUpgradeHelper:464 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:240)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
Caused by: java.lang.NullPointerException
	at org.apache.ambari.server.api.services.AmbariMetaInfo.reconcileAlertDefinitions(AmbariMetaInfo.java:1163)
	at org.apache.ambari.server.upgrade.UpdateAlertScriptPaths.executeDMLUpdates(UpdateAlertScriptPaths.java:46)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:946)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
	... 1 more
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-31 22:35:09,14
13135165,LDAP configuration is not reloaded in Guice,"+*Steps to reproduce:*+

run _ambari-server setup-ldap_ and set primary host:port to ""myHost:1""

run _ambari-server sync-ldap --all_

+*Actual results:*+

the synch process will fail due to a CommunicationException saying that *localhost:33389* is not reachable.

+*Expected results:*+

the synch process will fail due to a CommunicationException saying that *myHost:1* is not reachable.

+*Additional information:*+

in ambari-server.log I saw that AmbariConfigurationChangedEvent has been triggered and processed when setup-ldap updated the DB (see org.apache.ambari.server.ldap.service.AmbariLdapConfigurationProvider; however the get() method does not seem to be called when needed; it's maybe a Guice injection scope issue)",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-31 18:34:17,27
13135088,Cancel operation during package deployment causing repository manager to be broken,"During any failure on cluster deployment, server posting cancel operation to all pending and in progress operations. 

 

This causing package manager to be killed and enter in a bad state. ",pull-request-available,['ambari-sever'],AMBARI,Bug,Major,2018-01-31 13:55:31,1
13135012,LDAP sync fails with 'LDAP is not configured' error after configuring LDAP,"LDAP sync fails with 'LDAP is not configured' error after configuring LDAP.

*Steps to Reproduce*
 # ambari-server setup-ldap
 # ambari-server sync-ldap --all

{noformat:title=Example}
[root@c6404 ~]# ambari-server sync-ldap --all
Using python  /usr/bin/python
Syncing with LDAP...
ERROR: Exiting with exit code 1.
REASON: LDAP is not configured. Run 'ambari-server setup-ldap' first.
{noformat}
*Cause*
The ambari-server script is looking for the value of {{ambari.ldap.authentication.enabled}} (from the the {{ambari.properties}} file. The value is expected to be ""true"". Since {{ambari.ldap.authentication.enabled}} is not being set in the ambari.properties file, the check is failing and thus, the unexpected error.

See {{ambari-server/src/main/python/ambari_server/setupSecurity.py:298}}.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-31 08:14:47,27
13134866,Create hosts per operating system count summary,"In the install wizard, after host registration, we need to know the number of hosts running each possible operating system.

*Example request:*
{code:java}
 GET
http://localhost:8080/api/v1/hosts?format=summary
{code}

Example response:

{code:java}
{    ""operating_systems"": [        ""redhat6"": 75,        ""debian7"": 100
    ]
}
{code}

Note that if there are zero hosts for an operating system type, then it is not included in the array.

This structure allows us to add additional summary information as siblings to ""operating_systems"" in the future.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-30 19:44:06,44
13134848,Long cannot be cast to String error when changing a user's password,"Long cannot be cast to String error when changing a user's password:
{noformat}
30 Jan 2018 18:21:11,308 ERROR [ambari-client-thread-38] AbstractResourceProvider:353 - Caught AmbariException when modifying a resource
org.apache.ambari.server.AmbariException: java.lang.Long cannot be cast to java.lang.String
at org.apache.ambari.server.controller.internal.UserResourceProvider.addOrUpdateLocalAuthenticationSource(UserResourceProvider.java:559)
at org.apache.ambari.server.controller.internal.UserResourceProvider.updateUsers(UserResourceProvider.java:486)
at org.apache.ambari.server.controller.internal.UserResourceProvider.access$200(UserResourceProvider.java:69)
at org.apache.ambari.server.controller.internal.UserResourceProvider$3.invoke(UserResourceProvider.java:264)
at org.apache.ambari.server.controller.internal.UserResourceProvider$3.invoke(UserResourceProvider.java:261)
at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:346)
at org.apache.ambari.server.controller.internal.UserResourceProvider.updateResources(UserResourceProvider.java:261)
at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:317)
...
{noformat}

*Steps to reproduce*
 # Create a {{LOCAL}} user account (using either the Ambari UI or REST API)
{noformat}
POST /api/v1/users
{noformat}
{code:title=Payload}
{ 
  ""Users"" : {
    ""user_name"" : ""myuser"",
    ""password"" : ""hadoop""
  }
}
{code}
 # Change the user's password (using either the Ambari UI or REST API via the users entry point)
{noformat}
PUT /api/v1/users/myuser
{noformat}
{code:title=Payload}
{ 
  ""Users"" : {
    ""old_password"" : ""hadoop""
    ""password"" : ""hadoop1234""
  }
}
{code}
{code:title=Response}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: java.lang.Long cannot be cast to java.lang.String""
}
{code}

*Cause*
When building the internal request to set the user's password via the UserAuthenticationSource resource provider, the authentication source key is set as a {{Long}}. The UserAuthenticationSource resource provider expects this value to be a {{String}}.

*Solution*
The User resource provider should set the {{AuthenticationSourceInfo/source_id}} as a {{String}} value.

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-30 18:37:45,30
13134830,Missing user_authentication_id_seq in ambari_sequences table upon upgrade,"Missing user_authentication_id_seq in ambari_sequences table upon upgrade.

The missing sequence should be added using a value greater than the largest {{user_authentication.user_authentication_id}}. For example:
{code:sql}
insert into ambari_sequences (sequence_name, sequence_value) 
  select 'user_authentication_id_seq', max(user_authentication_id) + 1 from user_authentication;{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-30 17:44:02,27
13134730,Ambari makes unrelated changes to zookeeper quorum config in all services when delete host action fails,"*STR:*
# Delete a host from ambari-web ui that has only slave components like datanode, nodemanager, etc 
# This action will make API calls to delete host components and then delete host api
# Somehow environment should be configured in a way such that delete host component API succeeds but delete host api errors out

*Expected Result:* Host should still exist with no components. NO config changes should happen as none were previewed and slave component deletion generally does not result in config changes

*Actual Result:* ZK quorum related config changes happen in all services",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-01-30 11:52:20,19
13134658,Update Service Group API to take list of mpack name associated with the service group,"Today service group API is not associated with any mpacks. We need to store this association by adding ""mpacks"" field to the request. Note at present we will restrict a service group to be associated to a single mpack (i.e. EDW-MKTG service group will be associated with only EDW mpack and so all services in EDW-MKTG service group should be installed from EDW mpack. However note that from the API and DB schema perspective we need to future proof so that we can support installing services from multiple mpacks in the same service group.

{code}
POST /clusters/c1/servicegroups
[
  {
    ""ServiceGroupInfo"": {
      ""service_group_name"": ""CORE"",
      ""mpacks"": [
        {
          ""name"": ""HDPCORE""
        }
      ]
    }
  },
  {
    ""ServiceGroupInfo"": {
      ""service_group_name"": ""EDW-MKTG"",
      ""mpacks"": [
        {
          ""name"": ""EDW""
        }
      ]
    }
  },
  {
    ""ServiceGroupInfo"": {
      ""service_group_name"": ""EDW-FINANCE"",
      ""mpacks"": [
        {
          ""name"": ""EDW""
        }
      ]
    }
  }
]
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-30 06:17:38,44
13134591,Disable consecutive authentication failure account lockout feature by default,"Disable consecutive authentication failure account lockout feature by default. This feature locks an account after a configured number of failed authentication attempts. By defaulting {{authentication.local.max.failures}} to {{0}}, this feature will be disabled by default.

If enabled, a user account may be locked out due to number of authentication failures. There is a REST API call to unlock the user; however a user interface change will not be available until Ambari 3.x.",authentication pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-29 22:35:50,30
13134576,Blueprint cluster creation using manually installed mpacks,Deploy a cluster via blueprint based on one or more MPacks that have already been manually installed.,blueprints pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-01-29 21:49:26,21
13134563,Log Search: Return with 500 error for shipper config testing if an exception occurred,"Right now test api for shipperconfig returns with 200 status code anyway.
We should return with 500 when an error happened.
Also return 400 if the request is invalid",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-01-29 21:01:22,29
13134553,Remove HDP 3.0 stack from Ambari,Remove HDP-3.0 stack definition from Ambari. Management Packs will replace stacks in Ambari 3.0.,pull-request-available,['stacks'],AMBARI,Bug,Critical,2018-01-29 20:25:13,41
13134543,Register/Confirm Host for modular deployment,"UI implementation of ""Register Host"" step for mpack installer wizard",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-29 19:52:21,34
13134517,Move Stack OS and Repository to DB,"
This jira is to address technical debt.  Move from storing OS and Repository data as embedded json and make them proper tables/entities instead.

We are starting to add data to OS and Repository over just what is defined on the stack.  These tables should allow for both  {{stack}} and {{repo_version}} to use them.

new: repo_os
|id|bigint|PK|
|family|varchar(255), non-null, default ''|the os family (eg redhat6)|
|ambari_managed|smallint default 1|whether ambari manages the repos|

new: repo_definition
|id|bigint|PK|
|repo_os_id|bigint|FK to repo_os|
|repo_name|varchar(255) non-null|name of the repository (HDP-2.5, HDP-UTILS)|
|repo_id|varchar(255) non-null|freeform id of the repository (HDP-2.5-2.5.0.0)|
|base_url|varchar(2048) non-null|base url|
|default_url|varchar(2048) non-null|(verify this is still needed)|
|mirrors|text|mirrors to check|

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-01-29 18:04:38,13
13134510,Ranger fails to install (branch-3.0-perf),"jdbc related properties like:
{noformat}
previous_custom_mysql_jdbc_name, custom_mysql_jdbc_name, custom_postgres_jdbc_name, previous_custom_postgres_jdbc_name
{noformat}
also
{noformat}
gpl_license_accepted
{noformat}
are missing.

We should sent them in metadata/ambariLevelParams in order for Ranger andother db dependent services to work properly.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-29 17:52:55,6
13134473,"Expose ability to configure ""done-flag"" for Coordinator Datasets in Workflow Designer","AMBARI-18691 added support for Coordinators to the Workflow Designer, but does not expose configuration of dataset done-flag in the UI.

 

done-flag needs to support the following in generated xml:
* absent
* present, but empty
* present with text value",WFD WFM pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-01-29 15:29:18,45
13134465,Add new state for Not Available data in Heatmap widget,"# We should add a new state ""Data Not Available""
# Roundup metric values based on data type",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-29 15:04:43,19
13134444,Log Search: Add userList query for audit log requests,"add an extra userList property (comma separated list) for audit requests, like: 
/api/v1/audit/logs?userList=joe,steven
",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-01-29 13:09:57,29
13134432,Error while installing Hive 2.1.0.3.0 from Ambari 3.0.0,"The following error occurred in Ambari 3.0.0 when I tried to install Hive from HDP 3.0 (v2.1.0.3.0):

 
{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 167, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1565, in recommendConfigurations
    serviceAdvisor.getServiceConfigurationRecommendations(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/common-services/HIVE/2.1.0.3.0/service_advisor.py"", line 126, in getServiceConfigurationRecommendations
    recommender.recommendHiveConfigurationsFromHDP21(configurations, clusterData, services, hosts)
  File ""/var/lib/ambari-server/resources/common-services/HIVE/2.1.0.3.0/service_advisor.py"", line 209, in recommendHiveConfigurationsFromHDP21
    meta = self.get_service_component_meta(""HIVE"", ""WEBHCAT_SERVER"", services)
AttributeError: 'HiveRecommender' object has no attribute 'get_service_component_meta'
{code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-29 12:21:03,27
13134423,Agent commands hang even after freeing up disk space on the host,"*STR*
# Install a cluster with Ambari-2.6.2 and HDP-2.6.4.0
# Go the host (say host1) running Nimbus component and restart Nimbus
# Fill up the disk space on host1 (in my test, the disk space was filled up on the host running Nimbus component)
# Try to restart Nimbus. Nimbus restart expectedly fails with error:
{code}
Caught an exception while executing custom service command: <type 'exceptions.IOError'>: [Errno 28] No space left on device; [Errno 28] No space left on device
{code}
# Now free up the disk space on host1 and try to restart Nimbus

 

*Result*
Nimbus restart command hangs and eventually times out

Looks like the issue is because the action queue is unable to create new command for Nimbus restart.",pull-request-available system_test,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2018-01-29 11:33:52,5
13134402,Hive should handle a customized Zookeeper service principal name,"Hive should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} should be added to {{HIVE_OPTS}}
 * Other changes may be needed to handle Hive-related tool and JDBC/ODBC implementations",kerberos pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-01-29 09:46:32,18
13134250,Support external zookeeper for Infra Solr and Log Search,"Supporting external ZooKeeper from Infra Solr.

Security: If Infra Solr is secured we can assume that ZooKeeper is secure as well, same for non-secured environment

Addition to that:
Ambari Infra (Solr) should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * {{ambari/ambari-logsearch/ambari-logsearch-solr-client/src/main/resources/solrCloudCli.sh}} must be changed to set {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}}
 * {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} should be added to{{SOLR_OPTS}}",pull-request-available,['ambari-infra'],AMBARI,Bug,Major,2018-01-27 21:09:06,29
13134115,First prereq not displaying for Free IPA method in Enable Kerberos Wizard,"First prereq not displaying for Free IPA method in Enable Kerberos Wizard.

 !image-2018-01-26-16-32-14-802.png! 

This is due to a typo at 
{code:title=ambari-web/app/controllers/main/admin/kerberos/step1_controller.js:88}
          dsplayText: Em.I18n.t('admin.kerberos.wizard.step1.option.ipa.condition.1'),
{code}

The code should be 
{code:title=ambari-web/app/controllers/main/admin/kerberos/step1_controller.js:88}
          displayText: Em.I18n.t('admin.kerberos.wizard.step1.option.ipa.condition.1')
{code}

Notice ""dsplayText"" vs ""displayText"".
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-26 21:33:53,30
13134085,missing && operator while checking the state inmaintainence in ambari-server,"**missing && operator while checking the state inmaintainence in ambari-server

_if (isInMaintenance & !componentInfo.isClient()) {_
 _hasMM = true;_
 _if ( maxMMState == null || state.ordinal() > maxMMState.ordinal()) {_
 _maxMMState = state;_
 _}_
_}_",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-26 19:18:49,61
13134027,Fix running service checks with new mpack definitions,"*Background*
# New mpack composition forces us to break down our service definitions into client and server modules. This is because EDW mpack needs to be composed of HIVE, HIVE-CLIENT, ZOOKEEPER-CLIENT. EDW mpack doesnt really contain ZOOKEEPER service just ZOOKEEPER-CLIENT. Hence the new mpack structure we will have to break the ZOOKEEPER service definition into ""zookeeper"" server module and ""zookeeper-client"" client module definitions. 

However, breaking the service definition into client and server modules would require the logic for calling service check to be updated because now to run service check for zookeeper, we will need to call ZOOKEEPER-CLIENT/ZOOKEEPER-CLIENT to run the service check. 


cc: [~mradhakrishnan] who has more context on this.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-26 14:56:49,28
13134015,NFS Gateway is not logging at the correct location,"/grid/0/log/**hdfs//hadoop-hdfs-root-
nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out**

    
    
    
    [root@ctr-e137-1514896590304-8236-01-000002 ~]# cd /grid/0/log/hdfs/
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# ls -ltr
    total 500
    -rw------- 1 root root        0 Jan  9 22:52 hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out
    -rw------- 1 hdfs hadoop      0 Jan  9 22:52 SecurityAuth.audit
    -rw-r--r-- 1 root root      714 Jan  9 22:52 privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out
    -rw------- 1 root root      117 Jan  9 23:00 privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err
    drwxrwxr-x 2 root hadoop   4096 Jan  9 23:54 root
    drwxr-xr-x 2 hdfs hadoop   4096 Jan  9 23:59 hdfs
    -rw------- 1 hdfs hadoop 493804 Jan 10 00:58 hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# ps -ef| grep nfs
    root       10637       1  0 Jan09 ?        00:00:00 jsvc.exec -Dproc_nfs3 -outfile /grid/0/log/hdfs//hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out -errfile /grid/0/log/hdfs//privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err -pidfile /var/run/hadoop//hadoop-hdfs-root-nfs3.pid -nodetach -user hdfs -cp /usr/hdp/3.0.0.0-691/hadoop/conf:/usr/hdp/3.0.0.0-691/hadoop/lib/*:/usr/hdp/3.0.0.0-691/hadoop/.//*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/./:/usr/hdp/3.0.0.0-691/hadoop-hdfs/lib/*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/.//*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/lib/*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/.//*:/usr/hdp/3.0.0.0-691/hadoop-yarn/./:/usr/hdp/3.0.0.0-691/hadoop-yarn/lib/*:/usr/hdp/3.0.0.0-691/hadoop-yarn/.//*:/usr/hdp/3.0.0.0-691/tez/*:/usr/hdp/3.0.0.0-691/tez/lib/*:/usr/hdp/3.0.0.0-691/tez/conf:/usr/hdp/3.0.0.0-691/tez/doc:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-2.8-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib:/usr/hdp/3.0.0.0-691/tez/man:/usr/hdp/3.0.0.0-691/tez/tez-api-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-common-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-dag-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-examples-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-ext-service-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-history-parser-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-javadoc-tools-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-job-analyzer-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-mapreduce-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-internals-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-library-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-cache-plugin-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-acls-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-fs-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/ui:/usr/hdp/3.0.0.0-691/tez/lib/RoaringBitmap-0.4.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/async-http-client-1.8.16.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-cli-1.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-codec-1.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections-3.2.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections4-4.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-io-2.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-lang-2.6.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-math3-3.1.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/guava-11.0.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-hdfs-client-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-common-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-core-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-client-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-json-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jettison-1.3.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jsr305-3.0.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/metrics-core-3.1.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/netty-3.6.2.Final.jar:/usr/hdp/3.0.0.0-691/tez/lib/protobuf-java-2.5.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/servlet-api-2.5.jar:/usr/hdp/3.0.0.0-691/tez/lib/slf4j-api-1.7.10.jar:/usr/hdp/3.0.0.0-691/tez/lib/tez.tar.gz -Dhdp.version=3.0.0.0-691 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.0.0.0-691 -Xmx1024m -Dhadoop.security.logger=ERROR,DRFAS -jvm server -Dyarn.log.dir=/grid/0/log/hdfs/ -Dyarn.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dyarn.home.dir=/usr/hdp/3.0.0.0-691/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=:/usr/hdp/3.0.0.0-691/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.0.0.0-691/hadoop/lib/native -Dhadoop.log.dir=/grid/0/log/hdfs/ -Dhadoop.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dhadoop.home.dir=/usr/hdp/3.0.0.0-691/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
    hdfs       25658   10637  0 Jan09 ?        00:06:37 jsvc.exec -Dproc_nfs3 -outfile /grid/0/log/hdfs//hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out -errfile /grid/0/log/hdfs//privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err -pidfile /var/run/hadoop//hadoop-hdfs-root-nfs3.pid -nodetach -user hdfs -cp /usr/hdp/3.0.0.0-691/hadoop/conf:/usr/hdp/3.0.0.0-691/hadoop/lib/*:/usr/hdp/3.0.0.0-691/hadoop/.//*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/./:/usr/hdp/3.0.0.0-691/hadoop-hdfs/lib/*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/.//*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/lib/*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/.//*:/usr/hdp/3.0.0.0-691/hadoop-yarn/./:/usr/hdp/3.0.0.0-691/hadoop-yarn/lib/*:/usr/hdp/3.0.0.0-691/hadoop-yarn/.//*:/usr/hdp/3.0.0.0-691/tez/*:/usr/hdp/3.0.0.0-691/tez/lib/*:/usr/hdp/3.0.0.0-691/tez/conf:/usr/hdp/3.0.0.0-691/tez/doc:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-2.8-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib:/usr/hdp/3.0.0.0-691/tez/man:/usr/hdp/3.0.0.0-691/tez/tez-api-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-common-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-dag-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-examples-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-ext-service-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-history-parser-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-javadoc-tools-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-job-analyzer-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-mapreduce-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-internals-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-library-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-cache-plugin-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-acls-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-fs-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/ui:/usr/hdp/3.0.0.0-691/tez/lib/RoaringBitmap-0.4.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/async-http-client-1.8.16.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-cli-1.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-codec-1.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections-3.2.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections4-4.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-io-2.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-lang-2.6.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-math3-3.1.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/guava-11.0.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-hdfs-client-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-common-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-core-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-client-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-json-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jettison-1.3.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jsr305-3.0.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/metrics-core-3.1.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/netty-3.6.2.Final.jar:/usr/hdp/3.0.0.0-691/tez/lib/protobuf-java-2.5.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/servlet-api-2.5.jar:/usr/hdp/3.0.0.0-691/tez/lib/slf4j-api-1.7.10.jar:/usr/hdp/3.0.0.0-691/tez/lib/tez.tar.gz -Dhdp.version=3.0.0.0-691 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.0.0.0-691 -Xmx1024m -Dhadoop.security.logger=ERROR,DRFAS -jvm server -Dyarn.log.dir=/grid/0/log/hdfs/ -Dyarn.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dyarn.home.dir=/usr/hdp/3.0.0.0-691/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=:/usr/hdp/3.0.0.0-691/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.0.0.0-691/hadoop/lib/native -Dhadoop.log.dir=/grid/0/log/hdfs/ -Dhadoop.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dhadoop.home.dir=/usr/hdp/3.0.0.0-691/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
    root      449951  449877  0 21:36 pts/0    00:00:00 grep --color=auto nfs
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# 
    


",pull-request-available,[],AMBARI,Bug,Major,2018-01-26 13:49:12,5
13134011,Ambari Infra should handle a customized Zookeeper service principal name,"Ambari Infra (Solr) should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * {{ambari/ambari-logsearch/ambari-logsearch-solr-client/src/main/resources/solrCloudCli.sh}} must be changed to set {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}}
 * {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} should be added to{{SOLR_OPTS}}",kerberos,['ambari-server'],AMBARI,Improvement,Major,2018-01-26 13:39:42,29
13133985,Host Details page style fixes,"# Remove back button
# Use a table to display components
# Restart component notification should be at page level",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-26 11:21:07,19
13133803,HBase should handle a customized Zookeeper service principal name,"HBase should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * Add {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} to {{HBASE_OPTS}} in {{hbase-env/template}}",kerberos pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-01-25 19:28:41,27
13133650,Update service metainfo schema,"An mpack is composed on multiple modules. Modules could be categorized as ""SERVER"" module or a ""CLIENT"" module. In Ambari an mpack represents a stack and a module represents a service definition. We need to update the service metainfo schema to be able to specify the module category.
 
{code}
<service>
   <name>MODULE-NAME</name>
    <category>LEGACY|SERVER|CLIENT</category>
</service>
{code}
LEGACY category should be the default to support old stacks (HDP-2.6). 

Also an mpack could ship with different version of datanode and namenode (hotfix release for a component). Apart from having a service version, we should also be able to specify the component version in the mpack. If component version is not specified then componentVersion == serviceVersion

{code}
<service>
  <name>HDFS</service>
  <category>SERVER</category>
  <version>3.0.0.0-b123</version>
  <components>
    <component>
        <name>DATANODE</name>
        <version>3.0.0.0-h7-b5</version>
    </component>
    <component>
        <name>NAMENODE</name>
    </component>
  </components>
</service>
{code}

In the example above DATANODE is 3.0.0.0-h7-b5 whereas all other HDFS components are 3.0.0.0-b123.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-01-25 10:51:57,28
13133463,Install wizard mpack/service filter on selection screen,Implement ability to filter available mpacks or services by name on the selection screen of the install wizard.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-24 18:59:05,40
13133422,Provide field / component UI metadata from backend,"Adding new options for the backend to get field / component metadata:
{code:java}
logsearch.web.service_logs.group.labels=k1:v1,k2:v2
logsearch.web.service_logs.component.labels=k1:v1,k2:v2
logsearch.web.service_logs.field.labels=k1:v1,k2:v2
logsearch.web.service_logs.field.excludes=v1,v2,v3
logsearch.web.service_logs.field.visible=v1,v2,v3
logsearch.web.service_logs.field.filterable.excludes=v1,v2
logsearch.web.audit_logs.component.labels=k1:v1,k2:v2
logsearch.web.audit_logs.field.labels=K1#k1:v1,k2:v2;K2#k1:v1,k3:v3
logsearch.web.audit_logs.field.common.labels=k1:v1,k2:v2
logsearch.web.audit_logs.field.visible=k1:v1,v2,v3;k2:v1,v2
logsearch.web.audit_logs.field.common.visible=v1,v2,v3
logsearch.web.audit_logs.field.excludes=k1:v1,v2,v3;k2:v1,v2
logsearch.web.audit_logs.field.common.excludes=v1,v2,v3
logsearch.web.audit_logs.field.filterable.excludes==k1:v1,v2,v3;k2:v1,v2
logsearch.web.audit_logs.field.common.filterable.common.excludes=v1,v2,v3
logsearch.web.labels.fallback.enabled=true
logsearch.web.labels.service_logs.field.fallback.prefixes=ws_,std_,sdi_
{code}

Field metadata:
- name: real field name
- label (visible name on the UI for the field)
- visible (by default it should be visible in tables)
- filterable (it can be filterable from the searchbox) - always true by default, if not set, contains everything which is not excluded
(audit field metadata has common values, because different metadata can be used for different audit types)
with exclude properties you can remove fields from the response, those are also not filtered, but you can set fields to non-filtered if they are in the response with filterable.exclude options

Component metadata:
- name - label map: real component name with the name that should be seen on the UI
for service component there is an additional field: group, different components can be grouped together (like: metrics_collector and metrics_grafana can be in the same AMS group)

`api/v1/service/logs/schema/fields` and `api/v1/audit/logs/schema/fields` will contain field metadata
`api/v1/service/logs/components` and `api/v1/audit/logs/components` will contain component metadata
",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-01-24 16:17:26,29
13133401,Standardize precision when expressing durations,"Duration should have the following format ""1d 3h 46m"".",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-01-24 15:17:21,19
13133340,Fix TestRecoveryManager ClusterConfigurationCache and a bunch of small tests.,"Fix the below tests on branch-3.0-perf:

    
    
    TestClusterConfigurationCache.py
    TestFileCache.py
    TestRecoveryManager.py
    TestScript.py
    

",pull-request-available,[],AMBARI,Bug,Major,2018-01-24 11:28:57,5
13133170,"Stack Feature Updates - SAM, KAFKA, NiFi, and Schema Registry","The following stack feature changes are required:

1) Remove Kafka Stack Feature kafka_extended_sasl_support  (doesn't apply to current version)

2) Remove registry entries: registry_allowed_resources_support, registry_rewriteuri_filter_support

3) Remove sam entries: sam_storage_core_in_registry

4) Add NiFi encrypted authorizers
{noformat}
{
 ""name"": ""nifi_encrypted_authorizers_config"",
 ""description"": ""Support encrypted authorizers.xml configuration for version 3.1 onwards"",
 ""min_version"": ""2.6.5.0”
}{noformat}",pull-request-available,['stacks'],AMBARI,Bug,Major,2018-01-23 20:31:24,49
13133133,Log Search UI: implement log level filter,User should be able to choose the log levels visible to log feeder.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-01-23 17:46:31,15
13133095,change commons-collections-3.2.1.jar being used by ambari views to commons-collections-3.2.2.jar,"This library should be replaced by newer version of commons-collection or atleast 2.3.2, should be used.",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2018-01-23 16:00:39,38
13133087,Log Search UI: Add flexible environment handling and environment depended API Url config.,"As a developer or tester I should be able to set different environment with different API URLs, so that I can test my application with realtime data without build it which would mean a quicker debugging and development.",pull-request-available,['ambari-logsearch'],AMBARI,Improvement,Minor,2018-01-23 15:43:50,26
13133045,Ambari-agent puts Python scripts in 2.6 directory on OSes that use python 2.7.x by default,"User needs to upgrade all OSes for security purposes, and will need to remove all directories related to Python2.6 due to
TLS violations, they need this ASAP to test, as they have a hard deadline.  
OS: Red Hat Enterprise Linux Server release 7.4  
JDK version: 1.8.0_144  

",pull-request-available,[],AMBARI,Bug,Major,2018-01-23 13:26:55,5
13132910,db-purge-history operation fails with large DB size in postgres.,"When the ambari DB size is too large (around 1+ GB) then the db-purge-history may fail with the postgres error Tried to send an out-of-range integer as a 2-byte value

 

Following error trace is from Ambari 2.5.2 however the same might cause in higher version as well.

{code}
Internal Exception: org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
Error Code: 0
Call: SELECT DISTINCT host_task_id FROM topology_logical_task WHERE (physical_task_id IN (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,
....
....
.....
......
?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?))

Query: ReportQuery(name=""TopologyLogicalTaskEntity.findHostTaskIdsByPhysicalTaskIds"" referenceClass=TopologyLogicalTaskEntity sql=""SELECT DISTINCT host_task_id FROM topology_logical_task WHERE (physical_task_id IN ?)"")
 at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1620)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:676)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:560)
 at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:2055)
 at org.eclipse.persistence.sessions.server.ServerSession.executeCall(ServerSession.java:570)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:242)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:228)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeSelectCall(DatasourceCallQueryMechanism.java:299)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.selectAllRows(DatasourceCallQueryMechanism.java:694)
 at org.eclipse.persistence.internal.queries.ExpressionQueryMechanism.selectAllRowsFromTable(ExpressionQueryMechanism.java:2740)
 at org.eclipse.persistence.internal.queries.ExpressionQueryMechanism.selectAllReportQueryRows(ExpressionQueryMechanism.java:2677)
 at org.eclipse.persistence.queries.ReportQuery.executeDatabaseQuery(ReportQuery.java:852)
 at org.eclipse.persistence.queries.DatabaseQuery.execute(DatabaseQuery.java:904)
 at org.eclipse.persistence.queries.ObjectLevelReadQuery.execute(ObjectLevelReadQuery.java:1134)
 at org.eclipse.persistence.queries.ReadAllQuery.execute(ReadAllQuery.java:460)
 at org.eclipse.persistence.queries.ObjectLevelReadQuery.executeInUnitOfWork(ObjectLevelReadQuery.java:1222)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.internalExecuteQuery(UnitOfWorkImpl.java:2896)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1857)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1839)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1804)
 at org.eclipse.persistence.internal.jpa.QueryImpl.executeReadQuery(QueryImpl.java:258)
 at org.eclipse.persistence.internal.jpa.QueryImpl.getResultList(QueryImpl.java:473)
 at org.apache.ambari.server.orm.dao.DaoUtils.selectList(DaoUtils.java:60)
 at org.apache.ambari.server.orm.dao.TopologyLogicalTaskDAO.findHostTaskIdsByPhysicalTaskIds(TopologyLogicalTaskDAO.java:56)
 at org.apache.ambari.server.orm.AmbariLocalSessionInterceptor.invoke(AmbariLocalSessionInterceptor.java:53)
 at org.apache.ambari.server.orm.dao.RequestDAO.cleanup(RequestDAO.java:403)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
 at org.apache.ambari.server.cleanup.CleanupServiceImpl.cleanup(CleanupServiceImpl.java:82)
 at org.apache.ambari.server.cleanup.CleanupDriver.main(CleanupDriver.java:89)
Caused by: org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
 at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:281)
 at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
 at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417)
 at org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:302)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeSelect(DatabaseAccessor.java:1009)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:644)
... 43 more
Caused by: java.io.IOException: Tried to send an out-of-range integer as a 2-byte value: 43222
 at org.postgresql.core.PGStream.SendInteger2(PGStream.java:196)
 at org.postgresql.core.v3.QueryExecutorImpl.sendParse(QueryExecutorImpl.java:1242)
 at org.postgresql.core.v3.QueryExecutorImpl.sendOneQuery(QueryExecutorImpl.java:1547)
 at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1100)
 at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:253)
... 48 more
{code}


Looks like Postgres side limitation of around 32k parameters. However Ambari can send the large number of parameters in batch instead of sending it as bunch.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-23 02:16:32,27
13132860,Mpack related changes to Add Hosts Page,"Changes to be made for the Add Hosts Page including changes to the manual registration of hosts, registration using ssh and table display for the pre-registered hosts.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-22 20:46:50,34
13132626,Yarn/MR2 should handle a customized Zookeeper service principal name,"Yarn and MapReduce2 should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * common-services/YARN/2.1.0.2.0/package/scripts/params_linux.py must be changed to not hardcode the Zookeeper principal name in
{code:java}
m_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=zookeeper -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')
{code}

 * {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} should be added to {{YARN_OPTS}}",kerberos pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-01-22 09:55:12,27
13132577,"On upgrade all Zeppelin's interpreters are listed twice, with the exact same configuration.","On upgrade all Zeppelin's interpreters are listed twice, with the exact same configuration.",pull-request-available,[],AMBARI,Bug,Major,2018-01-22 05:17:35,66
13132573,Zeppelin's notebook-authorization.json doesn't get copied to HDFS on upgrade,Zeppelin's notebook-authorization.json doesn't get copied to HDFS on upgrade,pull-request-available,[],AMBARI,Bug,Major,2018-01-22 04:08:21,66
13132317,Issues with storm jaas files and recursive ch_(mod/own) calls,"Post disabling the kerberos Ambari is not deleting the storm_jaas.conf and client_jaas.conf files from storm conf directory.

Offset lag section in Storm UI is not populating because of the jaas conf.

{code}
Unable to get offset lags for kafka. Reason: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /brokers/topics/hortonworks/partitions at org.apache.zookeeper.KeeperException.create(KeeperException.java:123) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) at org.apache.curator.shaded.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) at org.apache.curator.shaded.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) at org.apache.curator.shaded.RetryLoop.callWithRetry(RetryLoop.java:109) at org.apache.curator.shaded.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) at org.apache.curator.shaded.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) at org.apache.curator.shaded.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.getLeadersAndTopicPartitions(KafkaOffsetLagUtil.java:323) at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.getOffsetLags(KafkaOffsetLagUtil.java:257) at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.main(KafkaOffsetLagUtil.java:126)

{code}

Storm's /usr/hdp/current/storm-client/bin/storm-kafka-monitor checks if the storm_jaas.conf exists in storm configuration directory and then uses it.

{code:java}
STORM_JAAS_CONF_PARAM=""""
JAAS_FILE=""${STORM_CONF_DIR}/storm_jaas.conf""
if [ -f $JAAS_FILE ]; then
 STORM_JAAS_CONF_PARAM=""-Djava.security.auth.login.config=${JAAS_FILE}""
fi
{code}

Issue will be resolved if we delete the storm_jaas.conf file manually.

As well storm jaas configs must be readable by world to allow some mpacks work properly.",pull-request-available,['security'],AMBARI,Bug,Major,2018-01-19 23:46:50,63
13132232,Update backend code to handle new versioning schema,"The new module and mpack meta rpms will have be versioned as follows

*Mpack Versioning*
{code}
<MAJOR>.<MINOR>.<MAINT>-b<BUILDNUM>
<MAJOR>.<MINOR>.<MAINT>-h<HOTFIXNUM>-b<BUILDNUM>
{code}

*Examples*
{code}
hdpcore 3.0.0-b123
hdpcore 3.0.0-h7-b111

edw 1.0.0-b234
edw 1.0.0-h15-b7
{code}

*Module Versioning*
{code}
<APACHE-MAJOR>.<APACHE-MINOR>.<HWX-MINOR>.<HWX-MAINT>-b<BUILDNUM>
<APACHE-MAJOR>.<APACHE-MINOR>.<HWX-MINOR>.<HWX-MAINT>-h<HOTFIXNUM>-b<BUILDNUM>
{code}

*Examples*
{code}
hdfs 3.0.0.1-b123
hdfs 3.0.1.0-h77-b11

zookeeper 3.5.1.0-b111
zookeeper 3.5.1.1-h21-b10
{code}

We need the BE code (java, python) to handle this versioning schema while comparing versions, formatting versions etc.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-19 17:01:29,24
13132229,"Use of OS family instead of OS type in hybrid setup for JAVA_HOME, Ambari and HDP repos","As HDP repos are configured using OS family as OS type we need to maintain similar assumption for other areas as well eg: Java home, Ambari repo and HDP repo as well.",pull-request-available,[],AMBARI,Improvement,Major,2018-01-19 16:47:23,65
13132175,Change requiredServices in metainfo.xml,"Today we define requiredServices as follows

{code}
<requiredServices>
    <service>ZOOKEEPER</service>
</requiredServices>
{code}

In the mpack model we need to categorize the scope of service dependency. We could have an INSTALL time dependency (i.e. we should also install the dependent service) or a RUNTIME dependency (i.e. there should be a running instance of the service in the cluster). 

For example HIVE in EDW mpack, will have an install-time dependency on ZOOKEEPER-CLIENT but a RUNTIME dependency on ZOOKEEPER.

{code}
<requiredServices>
  <service>
      <name>ZOOKEEPER-CLIENT</name>
      <scope>INSTALL</scope>
   </service>
  <service>
      <name>ZOOKEEPER</name>
      <scope>RUNTIME</scope>
   </service>
</requiredServices>
{code}

We should also check how we maintain compatibility with legacy stack definitions.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-19 13:05:40,28
13132162,Add different options of component name display,"The ams_collector should show up as “AMS: Metrics Collector”, and hdfs_namenode should show up as “HDFS: NameNode” - the mapping should be mutable, but we should show it everywhere, and we should have a long name “Service: Component” and a short name “Component”

Example:
Log Feeder: hdfs_namenode
Long Name: HDFS: NameNode
Short Name: NameNode
Abbreviation: NN",pull-request-available,['logsearch'],AMBARI,Bug,Major,2018-01-19 12:14:57,26
13131970,Fix checkstyle error in UpgradeCatalog300Test,"Fix checkstyle error in UpgradeCatalog300Test 
{noformat}
[ERROR] .../ambari/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog300Test.java:20: Using the '.*' form of import should be avoided - org.apache.ambari.server.upgrade.UpgradeCatalog300.*. [AvoidStarImport]
{noformat}",pull-request-available,[],AMBARI,Bug,Major,2018-01-18 18:46:46,30
13131874,Rewrite TestActionQueue,"Because of branch-3.0-perf changes ActionQueue had some big chunks completely
rewritten.  
Which causes all TestActionQueue unit tests to fail.

For these changes the TestActionQueue tests have to be rewritten as well. Some
deleted due to already non-existant functionality like the one related to
status commands execution.

",pull-request-available,[],AMBARI,Bug,Major,2018-01-18 13:16:28,5
13131862,Tez shown in red in the left nav for no apparent reason,"Tez is showing in red because it is in ""INSTALLED"" state, but it is client only service.",pull-request-available,['ambari-client'],AMBARI,Bug,Major,2018-01-18 12:49:20,7
13131695,Blueprints do not handle some failures properly,"Failures in the cluster configuration task and topology host tasks during blueprint cluster deployment or upscaling are not visible via request status. The logical request stays PENDING even after Ambari Server gave up retrying. Both the fact that it no longer makes progress and any reason of the failure can be seen only in {{ambari-server.log}}.

Some ways to reproduce (all via blueprints):
 * Create cluster with ZooKeeper and HDFS, but omit the NAMENODE component
 * Create a secure cluster with wrong Kerberos credentials
 * Create a secure cluster without storing Kerberos credentials, restart Ambari Server, add a new node",blueprints pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-17 20:46:30,21
13131693,Install Wizard final deployment of services from multiple mpacks,"Orchestrate final steps to install services from multiple mpacks using latest API, including creating service groups, creating version definitions, and deploying services.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-17 20:38:19,40
13131636,Styling for install wizard,"Styling for new and updated Install Wizard components. This includes:
 * Download config screen including fancy radio buttons
 * Mpack selection screen including mode selection behavior
 * Custom repo and download progress screens for mpacks and service repos
 * Updated left nav styling
 * Updated container styling and responsive behavior as needed
 * Tooltips",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-17 17:06:49,40
13131577,Configuration overwritten during Add Host/Add Service in Kerberized Cluster,"There are side effects of Add Host/Add Service in kerberized clusters that have common properties customized. In this situation the customer installs HDP, kerberizes the cluster, makes their configuration property customizations for their environment, and then after that if they add a host or service their configuration customizations are overwritten.

Here are examples of the properties that are commonly customized and have been known to be overwritten during add host/service:
* yarn.admin.acl
* yarn.resourcemanager.proxy-user-privileges.enabled
* yarn.timeline-service.http-authentication.cookie.domain
* yarn.timeline-service.http-authentication.signature.secret.file
* capacity-scheduler.xml configuration
* auth_to_local customizations

Note: This may also occur during the ""Regenerate Keytabs"" operation. 

 ",kerberos,['ambari-server'],AMBARI,Bug,Major,2018-01-17 14:00:25,30
13131329,Role authorization AMBARI.MANAGE_CONFIGURATION is not added to AMBARI.ADMINISTRATOR role during Ambari upgrade,"Role authorization \{{AMBARI.MANAGE_CONFIGURATION}} is not added to \{{AMBARI.ADMINISTRATOR}} role during Ambari upgrade. This privilege needs to be added to the AMBARI.ADMINISTRATOR role upon upgrade.

Note: A fresh install of Ambari contains the role authorization association.",pull-request-available,['ambari-sever'],AMBARI,Bug,Critical,2018-01-16 19:51:43,30
13131265,ambari-server setup-ldap should set configurations in the Ambari database,"{{ambari-server setup-ldap}} should set configurations in the Ambari database rather than the ambari.properties file.

Similar to the following, but updated to collect the relevant data for Ambari 3.0.0:
{noformat}
# ambari-server setup-ldap
Using python  /usr/bin/python
Setting up LDAP properties...
Primary URL* {host:port} : c6409.ambari.apache.org
Invalid characters in the input!
Primary URL* {host:port} : c6409.ambari.apache.org:363
Secondary URL {host:port} :
Use SSL* [true/false] (false):
User object class* (posixAccount):
User name attribute* (uid):
Group object class* (posixGroup):
Group name attribute* (cn):
Group member attribute* (memberUid):
Distinguished name attribute* (dn):
...{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-16 15:22:42,27
13131237,LogSearch Fixes for LogList Display,"1. Remove Whitespace from first two columns. We'll need these columns to conserve more visible space on the display, so we'll need the padding on these columns to be decreased.

2. Log message seems to trail off on 15-inch monitor display. Probably need a way to truncate the data, and allow the user to expand the log message. Maybe some kind of dynamic truncation. (Note: This might only be a problem in the initial state after resizing down).

3. Currently, elipses (""..."") are used on both sides of the display of the log list section of the UI. ",pull-request-available,['logsearch'],AMBARI,Bug,Major,2018-01-16 13:42:37,26
13131229,LogSearch Filter Bar Fixes,"Increase the opacity for the filter bar background to .9 and add a box shadow.
background-color: hsla(0,0%,100%,.90);
box-shadow: 0 2px 2px rgba(0,0,0,.1);",pull-request-available,['ambari-logsearch'],AMBARI,Bug,Major,2018-01-16 13:10:27,26
13131221,Notification popover layout issues,"The notification popover looks like it has a lot of dead space.
Need to move filter up",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-16 12:25:14,7
13131199,Extending Blueprint Parser for 3.0/3.1 syntax additions,"The Blueprint syntax parsing code will need to be updated in order to properly parse/handle the syntax additions for Ambari 3.0/3.1.

This will also include any parsing updates necessary for the Cluster Creation Template.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-16 10:42:12,20
13131066,Fix Log Search / Log Feeder / Infra Manager start scripts,JAVA_HOME variable is not set properly in logsearch/logfeeder/infra manager start scripts. Therefore those scripts not working if the default system java points to jdk7-,pull-request-available,"['ambari-infra', 'ambari-logsearch', 'ambari-server']",AMBARI,Bug,Major,2018-01-15 16:19:23,29
13131063,Unsightly artifacts during Login,See attached.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-15 16:03:55,19
13131035,Ambari should setup tcp preference in krb5.conf,"Ambari template should conditionally force to TCP by setting the following in the krb5.conf template if configured to do so:
{code:java}
{%- if force_tcp %}
  udp_preference_limit = 1
{%- endif -%}{code}",kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-15 14:09:43,27
13131023,Login page lost Ambari branding,"See attached.

1. There's no Ambari branding on trunk. There's nothing that says this is a login page for Ambari.
2. The spacing is odd.

See attached for trunk vs 2.6.1 comparison.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-01-15 13:23:00,19
13130991,Fix Namenode alerts broken due to enabling federation,"Namnode alerts need to be fixed.

<http://104.196.73.142:8080/#/main/alerts>

",pull-request-available,[],AMBARI,Bug,Major,2018-01-15 10:57:21,5
13130968,Cannot scale cluster if Ambari Server restarted since blueprint cluster creation,"STR:
# Create cluster using blueprint
# Restart Ambari Server
# Install Ambari Agent on new host and register with server
# Add the new host via API request
# Start Ambari Agent on the new host

Result: Ambari Server accepts the scale request, but does not proceed to install/start components on the new host

The problem is that AMBARI-22012 fixed a timing/ordering issue by introducing an executor, which is started on ""cluster configured"" event during blueprint cluster creation. If Ambari Server is restarted afterwards, the executor will stay stopped, hence tasks for scale requests are never executed.",blueprints pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-01-15 09:24:34,21
13130653,Blueprint 3.0/3.1 Database Table Design,"This JIRA tracks the work required to design any new database tables, or any modifications to existing tables, that will be required in order to support the Blueprint 3.0/3.1 requirements (multiple mpacks, multi-*, etc).",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-12 21:35:45,20
13130549,ambari-web unit test is failing at apache ambari jenkins job,"{code}
Error loading resource file:///api/v1/clusters/c1/upgrades?fields=Upgrade&_=1515682880321 (203). Details: Error opening /api/v1/clusters/c1/upgrades: No such file or directory

  30514 passing (31s)
  157 pending
  1 failing

  1) Ambari Web Unit tests test/utils/date/timezone_test timezoneUtils #detectUserTimezone Detect UTC+1:
     expected '0-60|Africa' to include '0-60|Atlantic'
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-01-12 15:25:25,19
13130538,Handle configs update in the middle of request (RU),"Async events should support RU scenario.
Main concern: RU process involves server execution commands which may change configs in the middle of request processing, so all new tasks should be executed with new configs.
Currently this is achieved by mutating config with tag used on request construction.

We may achieve similar result by implementing upgrade mode (with flag or by analyzing upgrades in progress similar to UI) and when it is enabled intercept execution commands events and extend them with actual configs just before sending to message broker.

In this case we can also consider to force agent refresh configs when RU finishes.

Current upgrade process relies on disabling auto-start of components prior to upgrade and doesn't care about status commands (as their configs usually don't change dramatically)",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-12 14:09:32,6
13130496,Ambari loads ambari.properties using ISO 8859-1 encoding,"Ambari Server loads {{ambari.properties}} using {{Properties.load(InputStream)}}, which [uses ISO 8859-1 character encoding|https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html#load-java.io.InputStream-].  This causes a problem for non-Latin1 characters that can occur in several properties, eg. in any directory or file name.

STR:

# Install and setup Ambari Server
# Update {{ambari.properties}} changing {{server.jdbc.user.passwd}} to {{/etc/ambari-server/conf/password.őőő}}
# {{mv -iv /etc/ambari-server/conf/password.dat /etc/ambari-server/conf/password.őőő}}
# Try to start Ambari Server

Result:

{noformat}
...
ERROR: Exiting with exit code 1.
REASON: Database check failed to complete. Please check /var/log/ambari-server/ambari-server.log and /var/log/ambari-server/ambari-server-check-database.log for more information.
{noformat}

{noformat:title=/var/log/ambari-server/ambari-server-check-database.log}
...
FileNotFoundException: File '/etc/ambari-server/conf/password.ÅÅÅ' does not exist
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-12 11:16:01,21
13130117,[Hive Views 2.0] Deleting a Saved query is Buggy when Mutliple Queries exist in same Name,"Reproduction scenario :

Save some queries under same name .
!https://issues.apache.org/jira/secure/attachment/12905639/hive_duplicate_saved_query_issue.png!

delete one of the saved query 
*hive views is deleting some other query randomly and the query i deleted still exists in saved query even after refresh of page.*

Root Cause : 
After the Fix of AMBARI-19958, Hive views is querying the saved Queries with FilterBy queryId and server is responding mutiple queries as all the queries has same name,
then Web client is deleting the first record from these savedQueries due to which this issue is happening.



",newbie patch pull-request-available usability,['ambari-views'],AMBARI,Bug,Major,2018-01-11 09:00:57,45
13129560,Show OneFS JMX metrics on the UI,"There are some metrics like a (NameNode uptime/StartTime and HDFS disk usage/Capacity*) which cannot be shown in a widget because widgets only support AMS based metrics.
Normally these metrics are shown in a service specific dashboard which is hardcoded in the ui (summary.js/serviceCustomViewsMap).

{code}
serviceCustomViewsMap: function() {
    return {
      HBASE: App.MainDashboardServiceHbaseView,
      HDFS: App.MainDashboardServiceHdfsView,
      STORM: App.MainDashboardServiceStormView,
      YARN: App.MainDashboardServiceYARNView,
      RANGER: App.MainDashboardServiceRangerView,
      FLUME: App.MainDashboardServiceFlumeView,
      HIVE:  App.MainDashboardServiceHiveView
    }
  }.property('serviceName'),
{code}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-01-09 15:33:38,18
13129538,Fix existing unit tests after STOMP protocol implementation,Fix failing unit tests,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-09 14:22:21,6
13129251,"Loosely store users authenticating from remote sources (LDAP, PAM, etc)","Loosely store users authenticating from remote sources (LDAP, PAM, etc) such that minimal information is store in the Ambari database, relying on information from the remote sources to provide details like group membership and username.

Group membership, consecutive authentication failure count, and etc... should not be stored in the Ambari database for user accounts that are not authenticated locally. 

To do this, convert the {{users}} table into the following tables:

*user*
* user_id  (primary key)
* principal_id (foreign key to adminprincipal table)
* user_name
* authentication_source (LOCAL, LDAP, PAM)
* active_widget_layouts
* create_time

*local_user_authentication*
* user_id (foreign key to user table)
* password
* active
* consecutive_failures
* create_time
* update_time
",authentication,['ambari-server'],AMBARI,Bug,Major,2018-01-08 14:01:16,30
13128892,Rename ambari metrics collector package to org.apache.ambari.metrics,"Fixing all integration tests in AMS timeline-service module. 
Renamed 'org.apache.hadoop.yarn.server.applicationhistoryservice' package to 'org.apache.ambari.metrics'.",pull-request-available,['ambari-metrics'],AMBARI,Task,Critical,2018-01-05 19:10:05,39
13128824,alert_definitions topic doesn't emit any events to client,"/alert_definitions topic doesn't emit any events to the client when definition properties are modified.
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-05 15:03:58,6
13128599,Expose Conditional Elements For Tasks on Upgrade,"Some upgrade tasks, such as configuration tasks, need to leverage the conditional elements which change the flow of control if the cluster is Kerberized (or based on other configuration values). For example, today we have this:
{code:java}
<execute-stage service=""RANGER_KMS"" component=""RANGER_KMS_SERVER"" title=""Calculating Proxy Properties under kms-site"">
  <condition xsi:type=""security"" type=""kerberos""/>
  <task xsi:type=""server_action"" class=""org.apache.ambari.server.serveraction.upgrades.RangerKmsProxyConfig""/>
</execute-stage>{code}
 

Where {{condition}} elements can be added to an {{execute-stage}} or a {{group}}. However, since {{execute-stage}} may only contain one task, it makes doing this work on a per-task level impossible. This particularly impacts the {{processing}} element. We want something like this:
{code:java}
<task xsi:type=""configure"" id=""hdp_2_6_hadoop_rpc_protection"">
  <condition xsi:type=""security"" type=""kerberos""/>
</task>

{code}
 

So that {{task}} elements defined in the {{pre-upgrade}} section of {{processing}} can also be conditionally invoked.",pull-request-available,[],AMBARI,Task,Critical,2018-01-04 20:02:04,31
13127965,Update install Wizard layout,Fix styles and markup issues.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-01-02 11:21:51,9
13127947,Ambari Versions Page is having JS error  When using redhat  satellite server for repos,"Ambari Versions page not updating when using Redhat satellite server for repos

Versions page was showing up like below

!https://issues.apache.org/jira/secure/attachment/12904190/Ambari_versions_page.png!


",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-02 10:00:52,3
13127147,Throw exception when keytab creation fails due to wrong configuration of key encryption types ,"When kerberos key encryption type is wrong, kerberos service check fails by kinit failure. Now it is difficult to find that the root cause (misconfiguration of key encryption type) because keytabs look to be created successfully. (Actually keytabs are not created successfully, they are only 2-byte files)
I'm thinking we should display error message or throw exception when keytab creation fails due to wrong key encryption type.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2017-12-26 02:57:59,67
13125821,"Upon upgrade, move LDAP properties from ambari.properties file to internal Ambari configuration storage","Upon upgrade to Ambari 3.0.0, move the relevant LDAP properties from ambari.properties file to internal Ambari configuration storage under the ""ldap-configuration"" category. 

||ambari.properties||ldap-configuration||
|ambari.ldap.isConfigured|ambari.ldap.authentication.enabled|
|authentication.ldap.useSSL|ambari.ldap.connectivity.use_ssl|
|authentication.ldap.primaryUrl|ambari.ldap.connectivity.server.host, ambari.ldap.connectivity.server.port|
|authentication.ldap.secondaryUrl|ambari.ldap.connectivity.secondary.server.host, ambari.ldap.connectivity.secondary.server.port|
|authentication.ldap.baseDn|ambari.ldap.attributes.user.search_base, ambari.ldap.attributes.group.search_base|
|authentication.ldap.bindAnonymously|ambari.ldap.connectivity.anonymous_bind|
|authentication.ldap.managerDn|ambari.ldap.connectivity.bind_dn|
|authentication.ldap.managerPassword|ambari.ldap.connectivity.bind_password|
|authentication.ldap.dnAttribute|ambari.ldap.attributes.dn_attr|
|authentication.ldap.usernameAttribute|ambari.ldap.attributes.user.name_attr|
|authentication.ldap.username.forceLowercase|ambari.ldap.advanced.force_lowercase_usernames|
|authentication.ldap.userBase|ambari.ldap.attributes.search_user_base|
|authentication.ldap.userObjectClass|ambari.ldap.attributes.user.object_class|
|authentication.ldap.groupBase|ambari.ldap.attributes.group.search_group_base|
|authentication.ldap.groupObjectClass|ambari.ldap.attributes.group.object_class|
|authentication.ldap.groupNamingAttr|ambari.ldap.attributes.group.name_attr|
|authentication.ldap.groupMembershipAttr|ambari.ldap.attributes.group.member_attr|
|authorization.ldap.adminGroupMappingRules|ambari.ldap.advanced.group_mapping_rules|
|authentication.ldap.userSearchFilter|ambari.ldap.advanced.user_search_filter|
|authentication.ldap.alternateUserSearchEnabled|ambari.ldap.advanced.alternate_user_search_enabled|
|authentication.ldap.alternateUserSearchFilter|ambari.ldap.advanced.alternate_user_search_filter|
|authorization.ldap.groupSearchFilter|ambari.ldap.advanced.group_search_filter|
|authentication.ldap.referral|ambari.ldap.advanced.referrals|
|authentication.ldap.pagination.enabled|ambari.ldap.advanced.pagination_enabled|
|authentication.ldap.sync.userMemberReplacePattern|ambari.ldap.advanced.user_member_replace_pattern|
|authentication.ldap.sync.groupMemberReplacePattern|ambari.ldap.advanced.group_member_replace_pattern|
|authentication.ldap.sync.userMemberFilter|ambari.ldap.advanced.user_member_filter|
|authentication.ldap.sync.groupMemberFilter|ambari.ldap.advanced.group_member_filter|
|ldap.sync.username.collision.behavior|ambari.ldap.advance.collision_behavior|",ldap pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-12-19 08:34:41,27
13125820,Use internal LDAP configuration values rather than ambari.properties values when accessing the configured LDAP server,"Use internal LDAP configuration values rather than ambari.properties values when accessing the configured LDAP server for LDAP sync and authentication. 

* Deprecate {{setup-ldap}} from the {{ambari-server}} script.  
** Rather then perform any operations, alert user to configure LDAP integration from the Ambari UI
* Lookup LDAP-specific properties from the Ambari configuration data under the ""ldap-configuration"" category.
* Remove relevant properties from {{org.apache.ambari.server.configuration.Configuration}}
** ambari.ldap.isConfigured
** authentication.ldap.useSSL
** authentication.ldap.primaryUrl
** authentication.ldap.secondaryUrl
** authentication.ldap.baseDn
** authentication.ldap.bindAnonymously
** authentication.ldap.managerDn
** authentication.ldap.managerPassword
** authentication.ldap.dnAttribute
** authentication.ldap.usernameAttribute
** authentication.ldap.username.forceLowercase
** authentication.ldap.userBase
** authentication.ldap.userObjectClass
** authentication.ldap.groupBase
** authentication.ldap.groupObjectClass
** authentication.ldap.groupNamingAttr
** authentication.ldap.groupMembershipAttr
** authorization.ldap.adminGroupMappingRules
** authentication.ldap.userSearchFilter
** authentication.ldap.alternateUserSearchEnabled
** authentication.ldap.alternateUserSearchFilter
** authorization.ldap.groupSearchFilter
** authentication.ldap.referral
** authentication.ldap.pagination.enabled
** authentication.ldap.sync.userMemberReplacePattern
** authentication.ldap.sync.groupMemberReplacePattern
** authentication.ldap.sync.userMemberFilter
** authentication.ldap.sync.groupMemberFilter
** ldap.sync.username.collision.behavior
 ",ldap pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-12-19 08:27:14,27
13124855,Library for querying cluster_settings and stack_settings in command*.json.,"Background : AMBARI-22198 added ""stack settings"", and AMBARI-22196 introduced ""cluster settings"" in Ambari.

*=========================================================================*
*Library for querying _clusterSettings_ and _stackSettings_ for its contents in command\*.json.*
*=========================================================================*

One should be able to query for a given *clusterSettings* or *stackSettings*:
 -  by passing in the setting name(one or more) in order to get it back as key-value map, or
 -  just get the value back for a passed-in setting.


*Functions for clusterSettings:*
*-----------------------------------------*
  - get_cluster_setting_entries(setting_names) : 
    -- Retrieves the passed-in cluster setting entr(y/ies) and their values as a map.
       If 'setting_names' is passed-in as None : all the settings names and their corresponding values will be returned as map.
       If 'setting_names' is passed-in as empty set : None will be returned.

  - get_cluster_setting_value(setting_name) :
    -- Retrieves the passed-in cluster setting entry's value.

  - is_security_enabled() : 
    -- Retrieves the cluster's security status.


*Functions for stackSettings:* 
*-----------------------------------------*

Stack settings as of now has 5 settings : stack_name, stack_root, stack_features, stack_tools, stack_packages. stack_name, stack_root have string as values, whereas stack_features, stack_tools, stack_packages have values as JSON. Further there already exists python functions in files : *stack_features.py*, *stack_tools.py* and *stack_select.py*.

   - get_stack_setting_entries(setting_names) : 
      --   Retrieves the passed-in stack setting entr(y/ies) and their values as a map.
            If 'setting_names' is passed-in as None, all the settings names and their corresponding values will be returned as map.
            If 'setting_names' is passed-in as empty set : None will be returned.

   - get_stack_setting_value(setting_name):
    -- Retrieves the passed-in stack setting entry's value.

- get_stack_name():
    -- Retrieves the stack name.

- get_stack_root():
   -- Retrieves the stack root.

 

*Modifications in  _stack_features.py, stack_tools.py and stack_select.py_ files:*
*--------------------------------------------------------------------------------------------------------------*

- Given that these already exist and as of now they read the relevant stack setting from *configurations/cluster_env*. 
- Thus, code has been added to try reading from /stackSettings first by calling the new fn.() get_stack_setting_value(). if setting not found, go for the fall back  *configurations/cluster_env* (which would be removed soon, when we remove cluster_env).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-12-14 01:58:24,43
13124713,AmbariServer will throw internal server error in case of post existing version_definition,"The following curl will result in 500 if post it more than once:
{noformat}
curl -vvv -u admin:admin -k -H ""X-Requested-By:ambari"" -X POST https://$AMBARI_SERVER/api/v1/version_definitions -d '{
  ""VersionDefinition"": {
   ""version_url"":
""http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0/HDP-2.6.3.0-235.xml""
    }
  }'
{noformat}

The http response is:

{noformat}
{
  ""status"" : 500,
  ""message"" : ""An internal system exception occurred: Base url http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0 is already defined for another repository version. Setting up base urls that contain the same versions of components will cause stack upgrade to fail.""
}
{noformat}

It would be better, if in this case the http response status would be {noformat}409 - Conflict{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-12-13 15:49:45,21
13124673,LDAPS sync Connection Refused ,"Ambari server configured to use ""secure"" ldap authentication. 
authentication.ldap.primaryUrl=********:636
authentication.ldap.useSSL=true

 We call the ldap_sync_events REST endpoint frequently to synchronize existing groups and a specific list groups.  We had no issues with this until mid-October at which point we began to see:
{code}
    ""status"" : ""ERROR"",
    ""status_detail"" : ""Caught exception running LDAP sync. simple bind failed: **********:636; nested exception is javax.naming.CommunicationException: simple bind failed: **********:636 [Root exception is java.net.SocketException: Connection reset]"",
{code}

Troubleshooting: 
* We saw random success and failure when attempting to sync a single group. 
* With useSSL=false and an updated port ldap sync was consistently successful.

Cause:
* By default, ldap connection only uses pooled connections when connecting to a directory server over LDAP. Enabling SSL causes it to disable the pooling, resulting in poorer performance and failures due to connection resets. 
* Around mid-October we increased the number of groups defined on the system (50+), this pushed us outside the ""safe zone"".

Fix:
Enable the SSL connections pooling by adding the below argument to startup options.
-Dcom.sun.jndi.ldap.connect.pool.protocol='plain ssl'

Reference: 
[https://confluence.atlassian.com/jirakb/connecting-jira-to-active-directory-over-ldaps-fails-with-connection-reset-763004137.htm]
[https://docs.oracle.com/javase/jndi/tutorial/ldap/connect/config.html]

  ",easyfix patch pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2017-12-13 13:49:31,54
13124400,Fix misuses of os.path.dirname(path) in yarn.py,"As I written in AMBARI-22632, yarn.py wrongly sets 755 permission to '/ats/done' and '/ats/active' by default. This is because the default value of yarn.timeline-service.entity-group-fs-store.done-dir is '/ats/done/' and the default value of yarn.timeline-service.entity-group-fs-store.active-dir is '/ats/active/', and both of the default value have trailing '/'.
Next yarn.py sets 700 permission to the directories, so finally the permissions become correct.
This issue causes redundant WebHDFS calls every time when launching ATS.",pull-request-available,['ambari-server'],AMBARI,Sub-task,Major,2017-12-12 16:18:15,67
13124206,Disabling Kerberos after enabled during Blueprint install fails with missing data directory error,"Disabling Kerberos after enabled during Blueprint install fails with missing data directory error:
{noformat}
The data directory has not been set.  Generated data can not be stored.
{noformat}

!screenshot-error-dialog.png!

This is caused by an invalid security state set for the installed components since the appropriate state is not set while enabling Kerberos during the installation process:

{noformat}
ambari=> select * from hostcomponentstate;
 id | cluster_id |  component_name  |   version    | current_state | host_id | service_name | upgrade_state | security_state
----+------------+------------------+--------------+---------------+---------+--------------+---------------+----------------
  1 |          2 | KERBEROS_CLIENT  | UNKNOWN      | INSTALLED     |       1 | KERBEROS     | NONE          | UNSECURED
  2 |          2 | ZOOKEEPER_CLIENT | 2.5.0.0-1245 | INSTALLED     |       1 | ZOOKEEPER    | NONE          | UNSECURED
  3 |          2 | ZOOKEEPER_SERVER | 2.5.0.0-1245 | STARTED       |       1 | ZOOKEEPER    | NONE          | UNSECURED
{noformat}

The expected state for each component is {{SECURED}}, not {{UNSECURED}}. Because of this, Ambari _thinks_ there is no work to be done, causing this issue. 

*Steps to reproduce*:
# Setup Ambari, ensure KDC is installed on some host and Kerberos client libs are installed on the Ambari server host with the krb5.conf setup properly
# Install Blueprint - [^blueprint_single_node_zk.json]
# Create clister - [^cluster_template_single_node_zk.json]
# When cluster is created, Kerberos should be enabled and all services up
# Disable Kerberos - error occurs during Unkerberize Cluster task.",blueprints kerberos,['ambari-server'],AMBARI,Bug,Major,2017-12-11 20:18:28,63
13124128,Non DFS Used from HDFS Namenode UI and HDFS summary in Ambari  is different," 'NON DFS Used' value  in Services -> HDFS -> Summary if different from the Value shown in NameNode UI (see Picture).

in NAMENODE UI -->
!https://issues.apache.org/jira/secure/attachment/12901501/namenode.png!
In Services -> HDFS -> Summary
!https://issues.apache.org/jira/secure/attachment/12901499/ambari-2.png!




In NameNode UI the Non DFS Used is taken from 'NonDfsUsedSpace' variable from  REST API call : http://host1:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo 
Where host1 is the Name Node Server FQDN

but in Ambari we are calculating the same value from (Total Allocated - DFS USed - DFS Remaining) .

due to which this issue is happening.


as a Fix we need to use the 'capacityNonDfsUsed' which comes from Server in NameNode metrics
'FSNamesystem > CapacityNonDFSUsed'

attached the patch in the JIRA.
Please fix this bug

",pull-request-available,['ambari-web'],AMBARI,Bug,Minor,2017-12-11 13:12:28,3
13123800,"User should be able to install Yarn DNS server using ""+Add"" utility from Host page","Scenario:
Install Yarn component without selecting ""Yarn DNS registry""
After installation, go to Host page and click on +ADD. 
Yarn DNS registry service option should be present to allow the user to add the service to host.

Currently, Yarn DNS registry service can not be ADDED to host directly.",pull-request-available,[],AMBARI,Bug,Critical,2017-12-08 21:12:59,7
13123504,Fix unit tests in feature branch to make them workable,Fix unit tests in feature branch to make them workable,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2017-12-07 16:56:25,24
13122616,Fix the wording on IPA integration requirements in the Enable Kerberos Wizard,"
Fix the wording of the IPA requirements in the Enable Kerberos Wizard to read as follows

* All cluster hosts are joined to the IPA domain and hosts are registered in DNS
* A password policy is in place that sets no expiry for created principals
* If you do not plan on using Ambari to manage the krb5.conf, ensure the following is set in each krb5.conf file in your cluster: default_ccache_name = /tmp/krb5cc_%{uid}
* The Java Cryptography Extensions (JCE) have been setup on the Ambari Server host and all hosts in the cluster
",freeipa kerberos,['ambari-server'],AMBARI,Bug,Major,2017-12-04 16:14:25,30
13122614,Ambari should not force accounts created in IPA to be added a user named 'ambari-managed-principals',"When creating user principals while enabling Kerberos using FreeIPA, Ambari should not force accounts to be added a user named 'ambari-managed-principals'. 

This occurs because the default value of {{kerberos-env/ipa_user_group}} is ""ambari-managed-principals"". To stop forcing this, the default value should be empty.",freeipa kerberos,['ambari-server'],AMBARI,Bug,Major,2017-12-04 16:07:34,30
13122291,Migrate user data for upgrade to improved user account management,"Migrate data from the {{users}} table (pre-Ambari 3.0.0) to the updated {{users}} table and {{user_authentication}} tables.

See [^user_management_db_schema_upgrade.png]

!user_management_db_schema_upgrade.png|thumbnail!",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-12-01 22:00:37,30
13122288,LDAP sync should be based off of DN rather than username,LDAP sync should be based off of DN rather than username.,ldap,['ambari-server'],AMBARI,Task,Major,2017-12-01 21:57:17,30
13122151,Handle passwords/sensitive data in Ambari configuration properties,"Passwords and other sensitive data stored as values to properties in Ambari configurations need to be masked or not stored in cleartext.

For example, {{ldap-configuration/ambari.ldap.connectivity.trust_store.password}} and ldap-{{configuration/ambari.ldap.connectivity.bind_password}}.

If the Ambari credential store is enabled (which might be by default as of Ambari 3.0.0), the sensitive date can be stored there like we do when sensitive data is to be stored in the ambari.properties file - see {{org.apache.ambari.server.security.encryption.CredentialStoreService}}.",config security,['ambari-server'],AMBARI,Task,Minor,2017-12-01 11:05:06,27
13121915,Need to address HDP-GPL repo update after user accepts license in post-install scenario,"  * User denies the GPL license agreement, UI will still issue PUT call to create the repo
  * Ambari should not write HDP-GPL repo info to the ambari-hdp-1.repo because it will break yum in local repo world.
  * Then the user accepts the license by running ambari-server setup
  * The repo file does not get updated on existing hosts
  * We need to push the HDP-GPL repo to the repo files on existing hosts only when the license is accepted
",pull-request-available,[],AMBARI,Bug,Major,2017-11-30 13:28:10,5
13121898,Remove obsolete hack to set KDC admin credentials via Cluster session API,"Remove hack to set KDC admin credential via the API to set session attribute via the Cluster resource.
Near *org/apache/ambari/server/controller/AmbariManagementControllerImpl.java:1469*

{code:java}
      // TODO: Once the UI uses the Credential Resource API, remove this block to _clean_ the
      // TODO: session attributes and store any KDC administrator credentials in the secure
      // TODO: credential provider facility.
      // For now, to keep things backwards compatible, get and remove the KDC administrator credentials
      // from the session attributes and store them in the CredentialsProvider. The KDC administrator
      // credentials are prefixed with kdc_admin/. The following attributes are expected, if setting
      // the KDC administrator credentials:
      //    kerberos_admin/principal
      //    kerberos_admin/password
      if((sessionAttributes != null) && !sessionAttributes.isEmpty()) {
        Map<String, Object> cleanedSessionAttributes = new HashMap<>();
        String principal = null;
        char[] password = null;

        for(Map.Entry<String,Object> entry: sessionAttributes.entrySet()) {
          String name = entry.getKey();
          Object value = entry.getValue();

          if (""kerberos_admin/principal"".equals(name)) {
            if(value instanceof String) {
              principal = (String)value;
            }
          }
          else if (""kerberos_admin/password"".equals(name)) {
            if(value instanceof String) {
              password = ((String) value).toCharArray();
            }
          } else {
            cleanedSessionAttributes.put(name, value);
          }
        }

        if(principal != null) {
          // The KDC admin principal exists... set the credentials in the credentials store
          credentialStoreService.setCredential(cluster.getClusterName(),
              KerberosHelper.KDC_ADMINISTRATOR_CREDENTIAL_ALIAS,
              new PrincipalKeyCredential(principal, password), CredentialStoreType.TEMPORARY);
        }

        sessionAttributes = cleanedSessionAttributes;
      }
      // TODO: END
{code}


This is no longer needed once the UI uses the new Credential Resource REST API - see  AMBARI-13292",kdc_credentials kerberos,['ambari-server'],AMBARI,Bug,Minor,2017-11-30 12:06:30,27
13121621,Integrate component state counters with websocket events,"Use /events/hostcomponents and /events/ui_topologies topics to track component counters. Events will update counters of component: TOTAL, INSTALLED, STARTED, INIT, INSTALL_FAILED, UNKNOWN.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2017-11-29 11:39:49,19
13119977,Moving Metrics Collector Forces ZooKeeper Server Install on Target Host,"When moving a Metrics Collector, a ZooKeeper Server is installed on the target host along with the Metrics Collector. This ZooKeeper Server is added and quorum can be upset in large clusters, and the customer was not intending to have a ZooKeeper server on the target directory.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2017-11-21 15:41:48,7
13118390,Ambari Incorrectly Warns about tez.tez-ui.history-url.base,"In the ambari cluster where SSL in enabled, and ambari port is 8443, it wrongly displays the warning as below:
***
tez.tez-ui.history-url.base:
It is recommended to set value https://<host>:8765/#/main/view/TEZ/tez_cluster_instance for property tez.tez-ui.history-url.base
***
Before enabling SSL, Ambari was running on port 8765, and looks like ""tez.tez-ui.history-url.base"" is still looking for older port.
Though we were able to connect Tez UI using: https://<host>:8443/#/main/view/TEZ/tez_cluster_instance
Hence, here ambari is providing a wrong warning.

Code snippet that is causing this issue is 

https://github.com/apache/ambari/blob/release-2.5.1/ambari-server/src/main/resources/stacks/HDP/2.3/services/stack_advisor.py

In below code we are always checking client.api.port for port - if https is enabled that it should check the port at ""client.api.ssl.port"" instead of ""client.api.port""
{code:java}
    if serverProperties:
      if 'client.api.port' in serverProperties:
        server_port = serverProperties['client.api.port']
      if 'views.dir' in serverProperties:
        views_dir = serverProperties['views.dir']
      if 'api.ssl' in serverProperties:
        if serverProperties['api.ssl'].lower() == 'true':
          server_protocol = 'https'
{code}
",triage,['ambari-sever'],AMBARI,Bug,Minor,2017-11-14 22:36:17,14
13117569,"Fix tar_archive.py: 1. remove verbosity flags, 2. fix archive_dir() and archive_directory_dereference() methods","There are two issues with tar_archive.py which caused a HDP upgrade to fail at a customer:

# The verbose flag (-v) is switched on. This could result in long archival time and timeouts
# The full path of the files is preserverved in the archive without the leading slash. This results in doubled paths when the archive is expanded. 

E.g:
The contents of _/hadoop/falcon/_ directory is archived.
All files are archived with full path but no leading slash: _hadoop/falcon/<directories>/<filename>_
After unpacking the unpacked file will look like: _/hadoop/falcon/hadoop/falcon/<directories>/<filename>_.
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2017-11-10 13:49:19,20
13111504,Improve KDC integration,"Improve KDC integration by making the interfaces more consistent with each other.

*Notes:*
* When using the MIT KDC or IPA options, the {{kerberos-env/admin_server_host}} value *must be the fully qualified domain name* (FQDN) of the host were the KDC administrator service is. 
* When connecting to the MIT KDC and IPA server, a username a password is not used to authenticate using the kadmin utility.  A Kerberos ticket is first acquired and that is used for authentication.
* When creating Kerberos identities using the MIT KDC and IPA handlers, the Ambari-generated password is not used.  All password's for principals in the MIT KDC and IP server are generated randomly by the KDC.
* Removed {{kerberos-env/set_password_expiry}} and {{kerberos-env/password_chat_timeout}} properties since they are no longer needed
* Changed {{kerberos-env/groups}} to {{kerberos-env/ipa_user_groups}} to be more explicit in how the property is used.
* The setPassword implementation for the MIT KDC and IPA handlers do nothing except check to see if the relevant principal exists. This is to maintain backward compatibility with previous implementations.  

",kerberos,['ambari-server'],AMBARI,Task,Major,2017-10-23 18:31:39,30
13111475,CLONE - HDP + HDF installation fails,"*Issue:*
With recent changes for patch upgrade, we end up creating installing ZOOKEEPER, KAFKA, STORM RPMs from HDF repo instead of HDP repo

*Fix:*
Add logic to not use custom service repos when deploying HDP services

",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2017-10-23 16:43:42,41
13111018,falcon quicklinks.json added in AMBARI-19059 has typos,"ambari-server/src/main/resources/common-services/FALCON/0.5.0.2.1/quicklinks/quicklinks.json added in AMBARI-19059 has below typos.
{code}
""property"":""falcon.enableTLS"" should be ""property"":""*.falcon.enableTLS"",

""site"":""falcon-startup-properties"" should be ""site"":""falcon-startup.properties""
{code}

Due to this quicklink for falcon is not working after enabling ssl for falcon.

I have verified that making above changes on ambari server solve the problem.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-10-20 15:37:44,52
13110781,"""META DATA STORAGE CONFIG"" tab is disabled in the install wizard>Customize service page>Druid when superset is also selected to install","""META DATA STORAGE CONFIG"" tab is disabled in the install wizard>Customize service page>Druid when superset is also selected to install.

STR:
1. Navigate to Ambari admin page and start install wizard
2. Select Druid and Superset to be installed from Choose Services page along with other required services
3. Navigate to Customize services page step in the wizard
4. ""META DATA STORAGE CONFIG"" tab in Druid config tab should be enabled but is disabled now.
Note: This behavior is only when Superset is chosen, and works fine without Superset.",deployment,['ambari-web'],AMBARI,Bug,Blocker,2017-10-19 20:00:12,68
13110462,Version registration failure during patch upgrade + Debian + Oracle run,"Live cluster :http://172.27.58.135:8080
Test steps :
1) Install ambari 2.5.2 with HDP-2.6.2.0-205
2) Upgrade ambari to 2.6.0.0-236
3) Run EU and upgrade stack to 2.6.3.0-205
4) Now try to register the VDF for PU and version registration is failing.
read version info api : http://172.27.58.135:8080/api/v1/version_definitions?skip_url_check=true&dry_run=true is throwing 500 server error and failing with below error :

{code}
Error Code: 932
Call: SELECT DISTINCT t0.repo_version_id, t0.display_name, t0.hidden, t0.legacy, t0.repositories, t0.resolved, t0.repo_type, t0.version, t0.version_url, t0.version_xml, t0.version_xsd, t0.parent_id, t0.stack_id FROM repo_version t0, servicedesiredstate t1 WHERE ((t1.desired_repo_version_id IN (?,?)) AND (t0.repo_version_id = t1.desired_repo_version_id))
	bind => [2 parameters bound]
Query: ReportQuery(name=""findByServiceDesiredVersion"" referenceClass=ServiceDesiredStateEntity sql=""SELECT DISTINCT t0.repo_version_id, t0.display_name, t0.hidden, t0.legacy, t0.repositories, t0.resolved, t0.repo_type, t0.version, t0.version_url, t0.version_xml, t0.version_xsd, t0.parent_id, t0.stack_id FROM repo_version t0, servicedesiredstate t1 WHERE ((t1.desired_repo_version_id IN ?) AND (t0.repo_version_id = t1.desired_repo_version_id))"")
18 Oct 2017 19:50:10,308  WARN [ambari-client-thread-17969] ServletHandler:561 - Error Processing URI: /api/v1/version_definitions - (javax.persistence.PersistenceException) Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLSyntaxErrorException: ORA-00932: inconsistent datatypes: expected - got CLOB
{code}

Attaching the complete log and vdf file.",patch-upgrade,['ambari-upgrade'],AMBARI,Bug,Blocker,2017-10-18 20:15:25,31
13109010,host and hostname built-in variables are not populated when performing Kerberos-related operations,"The host and hostname built-in variables are not populated when performing Kerberos-related operations.  These variables may be used like 

{code}
service-${host}
{code}

If the current host being processed is host1.example.com, the value should be converted to

{code}
service-host1.example.com
{code}

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2017-10-12 18:30:02,30
13108387,Prevent Patch Upgrades With Known Service Incompatibilities,"Some stacks, such as HDP, do not support service-level upgrades for some services since they have hard coded dependencies with other services which might not be in the upgrade. 

For example, if YARN is configured to use Tez, then an upgrade of YARN or Tez must also include Tez or YARN respectively. This is because the path is hard coded inside of YARN to use its version and cannot be changed via any external environment variable.",pull-request-available,[],AMBARI,Bug,Blocker,2017-10-10 19:37:52,31
13107937,Not able to perform revert after deleting the upgraded service,"Live cluster : http://172.27.19.201:8080/#/main/admin/stack/versions
Steps to reproduce:
1) Perform the patch upgrade for ATLAS(or any other service)
2) Delete ATLAS from cluster(or patch upgraded service)
3) Now try to revert the upgrade
4) Revert is failing as the upgraded service is already deleted. This is also blocking the revert for previous upgrades.

{code}
An internal system exception occurred: Service not found, clusterName=cl1, serviceName=ATLAS
{code}

Exception from ambari-server.log :

{code}
09 Oct 2017 10:02:59,142 ERROR [ambari-client-thread-22258] AbstractResourceProvider:285 - Caught AmbariException when creating a resource
org.apache.ambari.server.ServiceNotFoundException: Service not found, clusterName=cl1, serviceName=ATLAS
	at org.apache.ambari.server.state.cluster.ClusterImpl.getService(ClusterImpl.java:863)
	at org.apache.ambari.server.state.UpgradeContext.<init>(UpgradeContext.java:403)
	at org.apache.ambari.server.state.UpgradeContext$$FastClassByGuice$$6e2e5fe5.newInstance(<generated>)
	at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)
	at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:60)
	at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
	at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)
	at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)
	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1024)
	at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)
	at com.google.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:632)
	at com.sun.proxy.$Proxy31.create(Unknown Source)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:336)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:331)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:455)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:278)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createResources(UpgradeResourceProvider.java:331)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:298)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:37)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:73)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:90)
	at org.apache.ambari.server.api.services.UpgradeService.createUpgrade(UpgradeService.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:287)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:132)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:125)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
09 Oct 2017 10:02:59,144 ERROR [ambari-client-thread-22258] BaseManagementHandler:61 - Caught a system exception while attempting to create a resource: An internal system exception occurred: Service not found, clusterName=cl1, serviceName=ATLAS
org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Service not found, clusterName=cl1, serviceName=ATLAS
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:287)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider.createResources(UpgradeResourceProvider.java:331)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:298)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:37)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:73)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:90)
	at org.apache.ambari.server.api.services.UpgradeService.createUpgrade(UpgradeService.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:287)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:132)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:125)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ambari.server.ServiceNotFoundException: Service not found, clusterName=cl1, serviceName=ATLAS
	at org.apache.ambari.server.state.cluster.ClusterImpl.getService(ClusterImpl.java:863)
	at org.apache.ambari.server.state.UpgradeContext.<init>(UpgradeContext.java:403)
	at org.apache.ambari.server.state.UpgradeContext$$FastClassByGuice$$6e2e5fe5.newInstance(<generated>)
	at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)
	at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:60)
	at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
	at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)
	at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)
	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1024)
	at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)
	at com.google.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:632)
	at com.sun.proxy.$Proxy31.create(Unknown Source)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:336)
	at org.apache.ambari.server.controller.internal.UpgradeResourceProvider$1.invoke(UpgradeResourceProvider.java:331)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:455)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:278)
	... 93 more
{code}",patch-upgrade,['ambari-upgrade'],AMBARI,Bug,Blocker,2017-10-09 10:07:46,31
13107059,"When regenerating keytab files for a service, non-service-specific principals are affected","When regenerating keytab files for a service, non-service-specific principals are affected. For example, when regenerating the keytab files for HDFS using the following ReST API call:

{code:title=PUT /api/v1/clusters/c1?regenerate_keytabs=all&regenerate_components=HDFS}
{
  ""Clusters"": {
    ""security_type"": ""KERBEROS""
  }
}
{code}

The following principals are affected:
* HTTP/c6402.ambari.apache.org@EXAMPLE.COM
* ambari-qa-c1@EXAMPLE.COM
* nn/c6402.ambari.apache.org@EXAMPLE.COM
* hdfs-c1@EXAMPLE.COM
* HTTP/c6403.ambari.apache.org@EXAMPLE.COM
* dn/c6403.ambari.apache.org@EXAMPLE.COM
* HTTP/c6401.ambari.apache.org@EXAMPLE.COM
* nn/c6401.ambari.apache.org@EXAMPLE.COM
* ambari-server-c1@EXAMPLE.COM

However only the following principals *should be*  affected:
* nn/c6402.ambari.apache.org@EXAMPLE.COM
* hdfs-c1@EXAMPLE.COM
* dn/c6403.ambari.apache.org@EXAMPLE.COM
* nn/c6401.ambari.apache.org@EXAMPLE.COM
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2017-10-04 20:52:34,30
13106881,Package install fails on Debian7 with 'EMPTY_FILE' global variable not defined,"*STR*
# Deployed cluster with Ambari version: 2.5.1.0-159 and HDP version: 2.5.6.0-40
# Upgrade Ambari to 2.6.0.0-173
# Register HDP Version 2.6.3.0-151 and try to install the packages

*Result*
Package install fails with below error
{code}
2017-10-03 05:27:09,454 - Will install packages for repository version 2.6.3.0-151
2017-10-03 05:27:09,455 - Repository['HDP-2.6-repo-51'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2017-10-03 05:27:09,460 - File['/tmp/tmpPriIC1'] {'content': 'deb http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151 HDP main'}
2017-10-03 05:27:09,461 - Writing File['/tmp/tmpPriIC1'] because contents don't match
2017-10-03 05:27:09,461 - File['/tmp/tmpWBU9KY'] {'content': StaticFile('/etc/apt/sources.list.d/ambari-hdp-51.list')}
2017-10-03 05:27:09,462 - Writing File['/tmp/tmpWBU9KY'] because contents don't match
2017-10-03 05:27:09,462 - File['/etc/apt/sources.list.d/ambari-hdp-51.list'] {'content': StaticFile('/tmp/tmpPriIC1')}
2017-10-03 05:27:09,463 - Writing File['/etc/apt/sources.list.d/ambari-hdp-51.list'] because contents don't match
2017-10-03 05:27:09,463 - checked_call[['apt-get', 'update', '-qq', '-o', 'Dir::Etc::sourcelist=sources.list.d/ambari-hdp-51.list', '-o', 'Dir::Etc::sourceparts=-', '-o', 'APT::Get::List-Cleanup=0']] {'sudo': True, 'quiet': False}
2017-10-03 05:27:10,166 - checked_call returned (0, '')
2017-10-03 05:27:10,169 - Repository['HDP-UTILS-1.1.0.21-repo-51'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian7', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2017-10-03 05:27:10,176 - File['/tmp/tmpnjTvFL'] {'content': 'deb http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151 HDP main\ndeb http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian7 HDP-UTILS main'}
2017-10-03 05:27:10,176 - Writing File['/tmp/tmpnjTvFL'] because contents don't match
2017-10-03 05:27:10,177 - File['/tmp/tmprvUQz1'] {'content': StaticFile('/etc/apt/sources.list.d/ambari-hdp-51.list')}
2017-10-03 05:27:10,178 - Writing File['/tmp/tmprvUQz1'] because contents don't match
2017-10-03 05:27:10,179 - File['/etc/apt/sources.list.d/ambari-hdp-51.list'] {'content': StaticFile('/tmp/tmpnjTvFL')}
2017-10-03 05:27:10,180 - Writing File['/etc/apt/sources.list.d/ambari-hdp-51.list'] because contents don't match
2017-10-03 05:27:10,181 - checked_call[['apt-get', 'update', '-qq', '-o', 'Dir::Etc::sourcelist=sources.list.d/ambari-hdp-51.list', '-o', 'Dir::Etc::sourceparts=-', '-o', 'APT::Get::List-Cleanup=0']] {'sudo': True, 'quiet': False}
2017-10-03 05:27:11,641 - checked_call returned (0, '')
2017-10-03 05:27:11,642 - call[('ambari-python-wrap', '/usr/bin/hdp-select', 'versions')] {}
2017-10-03 05:27:11,681 - call returned (0, '2.5.6.0-40')
2017-10-03 05:27:11,683 - Package['hdp-select'] {'retry_on_repo_unavailability': True, 'retry_count': 5, 'use_repos': ['HDP-2.6-repo-51', 'HDP-UTILS-1.1.0.21-repo-51'], 'action': ['upgrade']}
2017-10-03 05:27:11,683 - Temporal sources directory was created: /tmp/tmpN_6yuF-ambari-apt-sources-d
2017-10-03 05:27:11,683 - Package Manager failed to install packages. Error: global name 'EMPTY_FILE' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 335, in install_packages
    retry_count=agent_stack_retry_count)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 57, in action_upgrade
    self.upgrade_package(package_name, self.resource.use_repos, self.resource.skip_repos)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 74, in wrapper
    return function_to_decorate(self, name, *args[2:])
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 385, in upgrade_package
    return self.install_package(name, use_repos, skip_repos, is_upgrade)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 74, in wrapper
    return function_to_decorate(self, name, *args[2:])
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 352, in install_package
    cmd = cmd + ['-o', 'Dir::Etc::SourceList=%s' % EMPTY_FILE]
NameError: global name 'EMPTY_FILE' is not defined
{code}",express_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-10-04 07:51:08,21
13105806,Druid start component via ambari doesn't wait to check if the component is successfully started,"Druid start component via ambari doesn't wait to check if the component is successfully started.

Any component start for Druid service will just  execute the start command and comes out with exit status 0. It will not further check if the process is running, or any port if its listening.

say for example Druid Coordinator
{code}
2017-09-19 18:48:47,076 - Execute['/usr/lib/jvm/java-openjdk/bin/java -cp /usr/lib/ambari-agent/DBConnectionVerification.jar:/usr/hdp/current/druid-coordinator/extensions/mysql-metadata-storage/mysql-connector-java.jar org.apache.ambari.server.DBConnectionVerification 'jdbc:mysql://ctr-e134-1499953498516-160569-01-000007.hwx.site:3306/druid?createDatabaseIfNotExist=true' druid [PROTECTED] com.mysql.jdbc.Driver'] {'tries': 5, 'user': 'druid', 'try_sleep': 10}
2017-09-19 18:48:47,554 - Execute['source /usr/hdp/current/druid-coordinator/conf/druid-env.sh ; /usr/hdp/current/druid-coordinator/bin/node.sh coordinator start'] {'user': 'druid'}

Command completed successfully!
{code}

The start command via Ambari-UI shows as passed on Ambari-ops even if the component fails to start. this gives false impression to check the service.
",druid,['ambari-server'],AMBARI,Bug,Major,2017-09-28 17:20:38,68
13105489,Use A Scoped Repo When Installing stack-tools,"Ambari does not write out repositories with a ""priority"" attribute since this is a plugin and is not guaranteed to be installed. Also, when installing packages with versions ""in the middle"" of two other installed stacks, priority would be incorrect as the most recent stack-tools should always be used.

The following is seen when trying to distribute a stack for a patch upgrade:
{code}
2017-09-26 16:28:58,652 - Package Manager failed to install packages. Error: Execution of '/usr/bin/yum -d 0 -e 0 -y install hive_2_6_3_1_1' returned 1. Error: Package: hadoop_2_6_3_1_1-2.7.3.2.6.3.1-1.x86_64 (HDP-2.6-repo-2)
           Requires: hdp-select >= 2.6.3.1-1
           Installed: hdp-select-2.6.3.0-122.noarch (@HDP-2.6.3.0-122)
               hdp-select = 2.6.3.0-122
{code}

The cause of this seems to be that the repository for HDP-2.6-repo-2 does not take priority over the existing installed repository for {{hdp-select}}. That's actually good - it shouldn't take priority over it arbitrarily. 

However, this means that the {{hdp-select}} which is installed did not get taken from {{HDP-2.6-repo-2}}. So, when installing {{hdp-select}}, we should scope the install to the repo we are trying to calculate the version for.

This works because the stack-select tools do not have their package names versioned ({{hdp-select}} as opposed to {{hdp-select_x_y_z}}.",pull-request-available,[],AMBARI,Bug,Blocker,2017-09-27 15:47:03,31
13104399,deleting and uploading files Using the HDFS View,"When using HDFS File View when I do a file delete, followed by another upload and delete of the same file, I'm getting the error shown below. (Failed to move the file to .Trash).
One workaround is when you get the Delete pop-up, you can check the ""delete permanently"" box.
This only occurs if the file is already present in .Trash
Works fine when using the command line. 
admin@vnagarajan-gvklab1 ~$ hadoop fs -touchz /user/admin/testdelete
admin@vnagarajan-gvklab1 ~$ hadoop fs -rm /user/admin/testdelete
17/09/20 22:46:38 INFO fs.TrashPolicyDefault: Moved: 'hdfs://gvklab1.openstacklocal:8020/user/admin/testdelete' to trash at: hdfs://gvklab1.openstacklocal:8020/user/admin/.Trash/Current/user/admin/testdelete
admin@vnagarajan-gvklab1 ~$ hadoop fs -touchz /user/admin/testdelete
admin@vnagarajan-gvklab1 ~$ hadoop fs -rm /user/admin/testdelete
17/09/20 22:46:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://gvklab1.openstacklocal:8020/user/admin/testdelete' to trash at: hdfs://gvklab1.openstacklocal:8020/user/admin/.Trash/Current/user/admin/testdelete1505947609951
",pull-request-available,['ambari-views'],AMBARI,Bug,Minor,2017-09-22 20:42:28,3
13103766,Patch upgrade is going into incorrect state during package installation with invalid VDF ,"Patch upgrade is going into incorrect state when invalid VDF is used during package installation.
Steps to reproduce :
1) Create a Centos7 HDP cluster with ambari
2) Register the VDF for Centos6 (mistakenly I registered the invalid VDF)
3) Click on install packages. 
Install packages throws 500 (""Repositories for os type redhat7 are not defined. Repo version=2.6.0.2-81, stackId=HDP-2.6""), but in the UI it still shows the state as 'installing'. Also Its not allowing to deregister the VDF as the state is installing.
Its better to check the validity of the VDF during registration, rather than failing during package installation.
Workaround : I was able to come out of this state by restarting ambari server and I was able to deregister the version as well.",patch-upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2017-09-20 23:35:16,33
13103613,Addition of service component after patching a service still keeps the component at base version,"*STR*
# Deploy HDP-2.6.0.0 cluster with Ambari-2.6.0.0 (2.6.0.0-102)
# Register VDF for Storm PU to 2.6.0.3-8
# Perform a patch RU for Storm
# Revert the patch
# Perform a patch EU for Storm
# Add Supervisor component on one of the hosts 
# Observe the version of Supervisor on newly added host

*Result:*
Supervisor still shows its version as the base version
Looks like the issue is because while installing packages for Supervisor, hdp-select sets it at base-version
{code}
2017-09-19 11:20:33,011 - Execute[('ambari-python-wrap', '/usr/bin/hdp-select', 'set', 'storm-supervisor', '2.6.0.0-598')] {'sudo': True}
2017-09-19 11:20:33,061 - After ('ambari-python-wrap', '/usr/bin/hdp-select', 'set', 'storm-supervisor', '2.6.0.0-598'), reloaded module params
{code}",patch-upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-09-20 14:14:10,31
13101728,Ambari schema upgrade from any version older than 2.5.1 fails due to incorrect version check ,"Perform an upgrade from Ambari version older than 2.5.1 (like 2.5.0 or 2.4.x) to 2.6.0

Fails with below error
{code}
12 Sep 2017 02:16:35,727  INFO [main] TransactionalLock$LockArea:121 - LockArea HRC_STATUS_CACHE is enabled
12 Sep 2017 02:16:36,091  INFO [main] LockFactory:53 - Lock profiling is disabled
12 Sep 2017 02:16:36,112  INFO [main] SchemaUpgradeHelper:391 - Upgrading schema to target version = 2.6.0.0
12 Sep 2017 02:16:36,115  INFO [main] SchemaUpgradeHelper:400 - Upgrading schema from source version = 2.5.0
12 Sep 2017 02:16:36,118 ERROR [main] SchemaUpgradeHelper:434 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: Database version does not meet minimal upgrade requirements. Expected version should be not less than 2.5.1, current version is 2.5.0
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:405)
12 Sep 2017 02:16:40,419  INFO [main] Configuration:3044 - Reading password from existing file
12 Sep 2017 02:16:40,439  INFO [main] Configuration:3529 - Hosts Mapping File null
12 Sep 2017 02:16:40,439  INFO [main] HostsMap:60 - Using hostsmap file null
12 Sep 2017 02:16:41,290  INFO [main] ControllerModule:221 - Detected MYSQL as the database type from the JDBC URL
{code}",upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-09-12 18:13:31,13
13101703,Validate kerberos.json files to ensure they meet the expected schema,Validate kerberos.json files to ensure they meet the expected schema.,kerberos_descriptor,['ambari-server'],AMBARI,Improvement,Major,2017-09-12 16:34:28,30
13101685,Ambari server schema upgrade failed while creating DRUID_SUPERSET component,"*STR*
# Deployed cluster with Ambari version: 2.5.2.0-298 and HDP version: 2.6.2.0-205
# Upgrade Ambari packages and then schema (ambari-server upgrade) to target Version: 2.6.0.0-77 | Hash: 450aae3dceabb270e0e267c16e2bb198db809541

*Result*
Schema upgrade failed with below error:
{code}
12 Sep 2017 01:09:19,339  INFO [Stack Version Loading Thread] LatestRepoCallable:106 - Loaded uri http://s3.amazonaws.com/dev.hortonworks.com/HDP/hdp_urlinfo.json in 177ms
12 Sep 2017 01:09:19,361 ERROR [main] SchemaUpgradeHelper:230 - Upgrade failed.
com.google.inject.ProvisionException: Guice provision errors:

1) Error injecting method, java.lang.RuntimeException: Trying to create a ServiceComponent not recognized in stack info, clusterName=cl1, serviceName=DRUID, componentName=DRUID_SUPERSET, stackInfo=HDP-2.6
  at org.apache.ambari.server.state.cluster.ClustersImpl.loadClustersAndHosts(ClustersImpl.java:173)
  at org.apache.ambari.server.state.cluster.ClustersImpl.class(ClustersImpl.java:95)
  while locating org.apache.ambari.server.state.cluster.ClustersImpl
  while locating org.apache.ambari.server.state.Clusters

1 error
        at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:987)
        at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1013)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.addNewConfigurationsFromXml(AbstractUpgradeCatalog.java:365)
        at org.apache.ambari.server.upgrade.UpgradeCatalog260.executeDMLUpdates(UpgradeCatalog260.java:392)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:938)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:228)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:421)
Caused by: java.lang.RuntimeException: Trying to create a ServiceComponent not recognized in stack info, clusterName=cl1, serviceName=DRUID, componentName=DRUID_SUPERSET, stackInfo=HDP-2.6
        at org.apache.ambari.server.state.ServiceComponentImpl.updateComponentInfo(ServiceComponentImpl.java:141)
        at org.apache.ambari.server.state.ServiceComponentImpl.<init>(ServiceComponentImpl.java:170)
        at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)
        at com.google.inject.internal.ProxyFactory$ProxyConstructor.newInstance(ProxyFactory.java:260)
        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)
        at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)
        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031)
        at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)
        at com.google.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:632)
        at com.sun.proxy.$Proxy19.createExisting(Unknown Source)
        at org.apache.ambari.server.state.ServiceImpl.<init>(ServiceImpl.java:162)
        at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)
        at com.google.inject.internal.ProxyFactory$ProxyConstructor.newInstance(ProxyFactory.java:260)
        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)
        at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)
        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031)
        at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)
        at com.google.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:632)
        at com.sun.proxy.$Proxy15.createExisting(Unknown Source)
        at org.apache.ambari.server.state.cluster.ClusterImpl.loadServices(ClusterImpl.java:427)
        at org.apache.ambari.server.state.cluster.ClusterImpl.<init>(ClusterImpl.java:318)
        at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)
        at com.google.inject.internal.ProxyFactory$ProxyConstructor.newInstance(ProxyFactory.java:260)
        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)
{code}

Looks like the issue is due to newly introduced service DRUID_SUPERSET, that was part of Druid service itself earlier

",upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-09-12 15:23:18,68
13101018,"Kerberos identity references should use the ""reference"" attribute","Kerberos identity references should use the ""reference"" attribute rather than rely on the ""name"" attribute to indicate the identity descriptor references some other identity descriptor.  

Either method should work on the backend, however the UI appears to not fully handle the ""named"" reference properly. 

The solution is to change 
{code}
            {
              ""name"": ""/HDFS/NAMENODE/namenode_nn"",
              ""principal"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
            }
{code}

by changing the ""name"" attribute to ""reference"" and adding a new ""name"" reference with a unique name relative to the scope of the identity descriptor. For example:

{code}
            {
              ""name"":""ranger_hdfs_audit""
              ""reference"": ""/HDFS/NAMENODE/namenode_nn"",
              ""principal"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
            }
{code}
",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2017-09-09 10:48:24,30
13098649,"Not able to enable hive ""Interactive Query""","when try to enable Hive interactive query it fails with below JS error and Save button does not get activated and does not start any wizard.

{noformat}
app.js:42734 Uncaught TypeError: Cannot read property 'forEach' of undefined(…)(anonymous function) @ app.js:42734pendingBatchRequestsAjaxSuccess @ app.js:42733opt.success @ app.js:175507o @ vendor.js:106fireWith @ vendor.js:106w @ vendor.js:108d @ vendor.js:108
{noformat}


Workaround: Update requestschedule table to mark the entry as ""COMPLETED"" to resolve the issue.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2017-08-30 20:06:29,3
13097579,test_kms_server timing issue,"Python unit test in {{test_kms_server}} occasionally fails due to timing.  Expected and actual output use two separate calls to get current time, which may be different if happens to be executed around the turn of a second.

{noformat}
FAIL: test_start_secured (test_kms_server.TestRangerKMS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/stacks/2.5/RANGER_KMS/test_kms_server.py"", line 522, in test_start_secured
    mode = 0644
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 330, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'owner': 'kms', 'content': '<ranger>\n<enabled>2017-08-25 11:02:46</enabled>\n< [truncated]... != {'content': '<ranger>\n<enabled>2017-08-25 11:02:45</enabled>\n</ranger>', 'owne [truncated]...
- {'content': '<ranger>\n<enabled>2017-08-25 11:02:46</enabled>\n</ranger>',
?                                                   ^

+ {'content': '<ranger>\n<enabled>2017-08-25 11:02:45</enabled>\n</ranger>',
?                                                   ^

-  'group': 'kms',
+  'group': u'kms',
?           +

   'mode': 420,
-  'owner': 'kms'}
+  'owner': u'kms'}
?           +

{noformat}

Affected code: {{test_start_default}} and {{test_start_secured}} in {{ambari-server/src/test/python/stacks/2.5/RANGER_KMS/test_kms_server.py}}.",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2017-08-25 12:05:02,21
13097078,Incorrect 'End Time' and 'Duration' shown in Upgrade History,"*STR*
# Deploy BI-4.2.0 cluster with Ambari 2.2.0
# Upgrade Ambari to 2.5.2.0-274 and perform post upgrade tasks like delete unsupported services (Solr, Titan)
# Perform EU to HDP-2.6.2.0-194 - while EU was in progress, I paused the upgrade few times; Later the EU was finalized
# Observe Upgrade History page

*Result*
The End time and duration of upgrade are incorrect. ",express_upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-08-23 16:41:42,31
13095264,OS specific package manager logic need to be moved out from Package provider to ambari_commons,"Package manager logic implemented on resource_management.core.providers.package.* package need to be moved to ambari_common package
This should create a unified package to work with package managers and remove all custom code from scripts",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2017-08-17 08:13:36,1
13093766,Add check for import from relocated packages,"Occasionally an import from {{org.apache.hadoop.metrics2.sink.relocated....}} creeps into {{ambari-server}} source code, causing compile errors:

{noformat}
$ mvn -am -pl ambari-server clean test
...
[ERROR] ambari-server/src/test/java/org/apache/ambari/server/checks/AbstractCheckDescriptorTest.java:[39,71] package org.apache.hadoop.metrics2.sink.relocated.google.common.collect does not exist
[ERROR] ambari-server/src/test/java/org/apache/ambari/server/checks/AbstractCheckDescriptorTest.java:[114,34] cannot find symbol
[ERROR]   symbol:   variable Sets
[ERROR]   location: class org.apache.ambari.server.checks.AbstractCheckDescriptorTest
{noformat}

The problem is that the same code can be compiled if dependencies are already installed in one's local Maven repository.

{noformat}
$ mvn -am -pl ambari-metrics/ambari-metrics-common,ambari-serviceadvisor,ambari-views clean install
...
$ mvn -pl ambari-server clean test
...
[INFO] BUILD SUCCESS
{noformat}

This succeeds because {{ambari-metrics-common}} installs a shaded uber jar including the {{..relocated..}} packages, hence they are available when compiling {{ambari-server}}.  On the other hand, when building from scratch (selectively with {{-am -pl ...}}, or the entire multimodule project) classpath contains {{ambari-metrics-common}} classes and individual dependencies without relocation.

The goal of this change is to add a checkstyle check to catch such imports at build-time with both compilation methods.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2017-08-10 12:49:01,21
13093694,Error starting client components on RedHat7 and BI 4.2.5 after Ambari upgrade,"*STR*
# Deployed IOP-4.2.5 cluster with Ambari-2.4.2
# Upgrade Ambari to 2.5.2
# Stop Hbase service and delete HBASE_REST_SERVER component via API
# Try to start HBase service

Result
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/hook.py"", line 37, in <module>
    BeforeInstallHook().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 329, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/hook.py"", line 33, in hook
    install_repos()
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/repo_initialization.py"", line 68, in install_repos
    _alter_repo(""create"", params.repo_info, template)
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/repo_initialization.py"", line 35, in _alter_repo
    repo_dicts = json.loads(repo_string)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/__init__.py"", line 307, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 335, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 353, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
{code}",system_test,['ambari-server'],AMBARI,Bug,Blocker,2017-08-10 08:00:28,21
13091162,Dynamically determine what keytab files have been distributed,"Dynamically determine what keytab files have been distributed to hosts. A custom command should be available via the KERBEROS_CLIENT to query for the keytab files installed on the relevant host. The communication between the Ambari server and the agents should generate data needed to determine what keytab files exist.
",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2017-07-31 11:10:55,18
13090068,"Kafka service failed to start during regenerate keytab after upgrade from Biginsights 4.2.5,4.2.0 to HDP 2.6.2","*Steps to Reproduce*
# Install IOP 4.2.5 with Kafka
# Kerberize the cluster
# Upgrade Ambari to 2.5.2
# Register HDP 2.6
# Remove iop-select
# Install HDP 2.6.
# Express Upgrade from BigInsights to HDP
# Regenerate Keytab

After regenerate keytab, Kafka will go down.

The problem is that kafka-broker config type has 
{noformat}
listeners: ""SASL_PLAINTEXT://localhost:6667""
security.inter.broker.protocol: ""SASL_PLAINTEXT""
{noformat}

and it needs to replace ""SASL_PLAINTEXT"" with ""PLAINTEXTSASL""",AMBARI-21348,['express-upgrade'],AMBARI,Bug,Blocker,2017-07-25 22:51:53,69
13088590,"Zookeeper server has incorrect memory setting, missing m in Xmx value","Repro Steps:

* Installed BI 4.2.0 cluster on IBM Ambari 2.2.2 with Zookeeper
* Upgraded Ambari to 2.5.2.0-146
* Registered HDP 2.6.2.0 repo, installed packages
* Ran service checks
* Started Express Upgrade

Result: _Service Check ZooKeeper_ step failed with {{KeeperErrorCode = ConnectionLoss for /zk_smoketest}}

This was caused by Zookeeper dying immediately during restart:
{noformat}
Error occurred during initialization of VM
Too small initial heap
{noformat}

{noformat:title=zookeeper-env.sh before upgrade}
export JAVA_HOME=/usr/jdk64/java-1.8.0-openjdk-1.8.0.77-0.b03.el7_2.x86_64
export ZOOKEEPER_HOME=/usr/iop/current/zookeeper-server
export ZOO_LOG_DIR=/var/log/zookeeper
export ZOOPIDFILE=/var/run/zookeeper/zookeeper_server.pid
export SERVER_JVMFLAGS=-Xmx1024m
export JAVA=$JAVA_HOME/bin/java
export CLASSPATH=$CLASSPATH:/usr/share/zookeeper/*
{noformat}

{noformat:title=zookeeper-env.sh after upgrade}
export JAVA_HOME=/usr/jdk64/java-1.8.0-openjdk-1.8.0.77-0.b03.el7_2.x86_64
export ZOOKEEPER_HOME=/usr/hdp/current/zookeeper-client
export ZOO_LOG_DIR=/var/log/zookeeper
export ZOOPIDFILE=/var/run/zookeeper/zookeeper_server.pid
export SERVER_JVMFLAGS=-Xmx1024
export JAVA=$JAVA_HOME/bin/java
export CLASSPATH=$CLASSPATH:/usr/share/zookeeper/*
{noformat}

Note missing ""m"" in memory setting.

zookeeper-env template contains,
{noformat}
export SERVER_JVMFLAGS={{zk_server_heapsize}}
{noformat}

In this cluster, zookeeper-env contains,
zk_server_heapsize: ""1024""

While the params_linux.py file has some inconsistencies with appending the letter ""m"".
{noformat}
zk_server_heapsize_value = str(default('configurations/zookeeper-env/zk_server_heapsize', ""1024m""))
zk_server_heapsize = format(""-Xmx{zk_server_heapsize_value}"")
{noformat}

Instead, it should be,
{noformat}
zk_server_heapsize_value = str(default('configurations/zookeeper-env/zk_server_heapsize', ""1024""))
zk_server_heapsize_value = zk_server_heapsize_value.strip()
if len(zk_server_heapsize_value) > 0 and not zk_server_heapsize_value[-1].isdigit():
  zk_server_heapsize_value = zk_server_heapsize_value + ""m""
zk_server_heapsize = format(""-Xmx{zk_server_heapsize_value}"")
{noformat}",AMBARI-21348,['stacks'],AMBARI,Bug,Blocker,2017-07-19 22:59:38,69
13087707,"Cross-stack migration from BigInsights to HDP, EU needs to set hive-site custom.hive.warehouse.mode to 0770","During cross stack upgrade from BigInsights, if user has not set custom.hive.warehouse.mode in hive-site, as the default value is changed in HDFS from BigInsights default (0770) to HDP default (0777).
EU should set the value of custom.hive.warehouse.mode to 0770.",AMBARI-21348,"['express-upgrade', 'stacks']",AMBARI,Bug,Blocker,2017-07-17 19:33:35,69
13087337,Upgrading IOP cluster with Spark2 to Ambari 2.5.2 fails on start because config mapping spark2-javaopts-properties is never selected,"STR:
* Install BigInsights 4.2.5 with Spark2
* Upgrade to Ambari 2.5.2
* Start Ambari, which will fail in the DB consistency check

{code}
2017-07-07 01:07:01,881 ERROR - You have non selected configs: spark2-javaopts-properties for service SPARK2 from cluster c1!
{code}

Looks like this config is never selected,

{noformat}
ambari=> SELECT sc.service_name, sc.version, sc.user_name, TO_TIMESTAMP(sc.create_timestamp/1000), sc.stack_id, sc.note,
c.version AS type_version, c.version_tag, c.type_name, c.stack_id, TO_TIMESTAMP(c.create_timestamp/1000) AS clusterconfig_created,
TO_TIMESTAMP(ccm.create_timestamp/1000) AS clusterconfigmapping_created, ccm.selected, ccm.user_name
FROM serviceconfig sc JOIN serviceconfigmapping m ON sc.service_config_id = m.service_config_id
JOIN clusterconfig c ON m.config_id = c.config_id
JOIN clusterconfigmapping ccm ON c.type_name = ccm.type_name AND c.version_tag = ccm.version_tag
WHERE sc.service_name = 'SPARK2' and c.type_name = 'spark2-javaopts-properties'
ORDER BY sc.version, c.type_name;
 service_name | version |   user_name    |      to_timestamp      | stack_id |               note                | type_version | version_tag |         type_name          | stack_id | clusterconfig_created  | clusterconfigmapping_created | selected | user_name
--------------+---------+----------------+------------------------+----------+-----------------------------------+--------------+-------------+----------------------------+----------+------------------------+------------------------------+----------+-----------
 SPARK2       |       1 | admin          | 2017-07-11 20:17:01+00 |        3 | Initial configurations for Spark2 |            1 | version1    | spark2-javaopts-properties |        3 | 2017-07-11 20:17:00+00 | 2017-07-11 20:17:01+00       |        0 | admin
 SPARK2       |       2 | ambari-upgrade | 2017-07-13 20:33:50+00 |        3 |                                   |            1 | version1    | spark2-javaopts-properties |        3 | 2017-07-11 20:17:00+00 | 2017-07-11 20:17:01+00       |        0 | admin
(2 rows)
{noformat}

This is because the config type has a single config with an empty value, so perhaps UI never selects it.
{code}
<configuration supports_final=""true"">
  <property>
    <name>content</name>
    <description>Spark2-javaopts-properties</description>
    <value> </value>
    <on-ambari-upgrade add=""true""/>
  </property>
</configuration>
{code}

Fix will be for Ambari 2.5.2 upgrade to select exactly once config for each config type if none are selected.
The only config type to hit this issue is spark2-javaopts-properties
",AMBARI-21348,"['ambari-server', 'stacks']",AMBARI,Bug,Blocker,2017-07-15 00:10:45,69
13087272,"NPE during ""Update Kerberos Descriptor""","Ambari-server.log:- 
{code}
java.lang.NullPointerException
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processIdentity(KerberosDescriptorUpdateHelper.java:360)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processIdentities(KerberosDescriptorUpdateHelper.java:321)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processComponent(KerberosDescriptorUpdateHelper.java:230)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processService(KerberosDescriptorUpdateHelper.java:195)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processServices(KerberosDescriptorUpdateHelper.java:122)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.updateUserKerberosDescriptor(KerberosDescriptorUpdateHelper.java:78)
	at org.apache.ambari.server.serveraction.upgrades.UpgradeUserKerberosDescriptor.execute(UpgradeUserKerberosDescriptor.java:139)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:517)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:454)
	at java.lang.Thread.run(Thread.java:748)

{code}

*Cause*
This is caused by having a _custom/unexpected_ identity in the user-supplied Kerberos descriptor that is not in the Kerberos descriptor from the initial stack.  

*Solution*
Ignore the _custom/unexpected_ identity since the user must have added that manually and it is expected that it should remain untouched after the upgrade process. 

",express_upgrade rolling_upgrade upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-07-14 18:47:10,30
13086757,"Cross-stack upgrade, Oozie restart fails with ext-2.2.zip missing error, stack_tools.py is missing get_stack_name in __all__, disable BigInsights in UI","Oozie Server restart failed due to this: Unable to copy /usr/share/HDP-oozie/ext-2.2.zip because it does not exist

Doesn't look like HDP rpms created this path:
{code}
[root@sid-test-2 ~]# ls -l /var/lib/oozie/ext-2.2.zip
-rwxr-xr-x. 1 oozie hadoop 6800612 Jul  5 18:03 /var/lib/oozie/ext-2.2.zip
[root@sid-test-2 ~]# ls -l /usr/hdp/2.6.1.0-129/oozie/libext/ext-2.2.zip
-rw-r--r--. 1 oozie hadoop 6800612 Jul  6 16:36 /usr/hdp/2.6.1.0-129/oozie/libext/ext-2.2.zip
{code}

The ext2js rpm seems to come from IOPUtils:
{code}
[root@sid-test-2 oozie]# yum list | grep extjs
extjs.noarch                              2.2_IBM_2-1                @IOP-UTILS-1.3

[root@sid-test-2 oozie]# rpm -qa | grep extjs
extjs-2.2_IBM_2-1.noarch
{code}

We should swap the source from
{noformat}
/usr/share/HDP-oozie/ext-2.2.zip
{noformat}
to
{noformat}
/usr/share/BIGINSIGHTS-oozie/ext-2.2.zip
{noformat}
since the latter does exist.

Also, restarting Oozie Clients during EU is failing because stack_tools.py is missing the ""get_stack_name"" function in the __all__ variable.
Lastly, disable showing the BigInsights stack by default in the UI.",AMBARI-21348,[],AMBARI,Bug,Blocker,2017-07-13 00:05:51,69
13086731,"Readd TITAN, R4ML, SYSTEMML, JNBG to BigInsights and fix HBase backup during EU and imports","Services to be deleted (in-order of delete after Ambari upgrade)

TITAN
R4ML
SYSTEMML
JNBG

Fix imports in HBase and the ""take_snapshot"" function called during EU.",AMBARI-21348,['stacks'],AMBARI,Bug,Blocker,2017-07-12 22:35:00,69
13086642,Remove unnecessary services from BigInsights stack,"Remove services that are not needed in BigInsights stack such as Titan and SYSTEMML, and all references to JNBG, R4ML.",AMBARI-21348,['stacks'],AMBARI,Bug,Critical,2017-07-12 17:20:46,69
13086596,Expected Values Like original_stack Are Missing On Downgrades,"*STR*
# Deployed cluster with Ambari version: 2.4.3.0-30 and HDP version: 2.5.5.0-157
# Upgrade Ambari to 2.5.2.0-114 | hash: d2ee45733e0c63d589f535866e6584b42d91126e
# Start EU to 2.6.2.0-79 and reach till Finalize
# Downgrade back to 2.5

*Result*
Observed errors at Livy and Spark client restarts:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/SPARK/1.2.1/package/scripts/spark_client.py"", line 88, in <module>
    SparkClient().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 329, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 806, in restart
    self.pre_upgrade_restart(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/SPARK/1.2.1/package/scripts/spark_client.py"", line 79, in pre_upgrade_restart
    import params
  File ""/var/lib/ambari-agent/cache/common-services/SPARK/1.2.1/package/scripts/params.py"", line 62, in <module>
    stack_version_unformatted = config['commandParams']['original_stack'].split(""-"")[1]
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'original_stack' was not found in configurations dictionary!
{code}",express_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-07-12 14:54:43,31
13086106,"Cross-stack migration, package supplemental stack in RPM","When trying to upgrade ambari-server, it fails with error: 
{noformat}
Exception in thread ""main"" org.apache.ambari.server.AmbariException: Stack data, Stack BigInsights 4.2.5 is not found in Ambari metainfo
{noformat}

For the server, package /var/lib/ambari-server/resources/stacks/BigInsights
For the agent, package /var/lib/ambari-agent/cache/stacks/BigInsights",AMBARI-21348,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2017-07-10 23:28:40,69
13085012,Backend - Run EU/RU PreChecks during a cross-stack upgrade,"During a cross-stack upgrade, the PreChecks need to be able to run.

STR:
* Install BigInsights 4.2.5
* Upgrade Ambari to 2.5
* Register bits for HDP 2.6
* Install bits for HDP 2.6
* Attempt to run PreChecks

{noformat}
curl -u admin:admin -H ""X-Requested-By: ambari"" -i -X GET http://localhost:8080/api/v1/clusters/c1/rolling_upgrades_check?fields=*&UpgradeChecks/repository_version=2.6.1.0-129&UpgradeChecks/upgrade_type=NON_ROLLING&UpgradeChecks/upgrade_pack=nonrolling-upgrade-to-hdp-2.6
{noformat}

This returns,
{noformat}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: Repository version null was not found""
}
{noformat}

because in UpgradeHelper.java it cannot find the upgrade pack.

The new API will requiring passing in a new argument, e.g., UpgradeChecks/target_stack=HDP-2.6",AMBARI-21348,['ambari-server'],AMBARI,Bug,Major,2017-07-06 00:18:25,69
13083121,DB consistency checker throws errors for missing 'parquet-logging' and 'product-info' configs after Ambari upgrade,"*STR*
# Deployed cluster with Ambari version: 2.5.1.0-159 and HDP version: 2.6.1.0-129
# Upgrade Ambari to 2.5.2.0-74 (hash: fd30644590991deb41241454d6e9091ed7a38e92)
# Run ""ambari-server start""

{code}
root@ctr-e133-1493418528701-156570-01-000005:/hwqe/hadoopqe# ambari-server restart
Using python  /usr/bin/python
Restarting ambari-server
Waiting for server stop...
Ambari Server stopped
Ambari Server running with administrator privileges.
Organizing resource files at /var/lib/ambari-server/resources...
Ambari database consistency check started...
Server PID at: /var/run/ambari-server/ambari-server.pid
Server out at: /var/log/ambari-server/ambari-server.out
Server log at: /var/log/ambari-server/ambari-server.log
Waiting for server start..................
DB configs consistency check failed. Run ""ambari-server start --skip-database-check"" to skip. You may try --auto-fix-database flag to attempt to fix issues automatically. If you use this ""--skip-database-check"" option, do not make any changes to your cluster topology or perform a cluster upgrade until you correct the database consistency issues. See /var/log/ambari-server/ambari-server-check-database.log for more details on the consistency issues.
ERROR: Exiting with exit code -1.
REASON: Ambari Server java process has stopped. Please check the logs for more information.
{code}

DB log: ambari-server-check-database.log
{code}
2017-06-27 13:51:38,743  INFO - Executing query 'GET_SERVICES_WITH_CONFIGS'
2017-06-27 13:51:38,748  INFO - Comparing service configs from stack with configs that we got from db
2017-06-27 13:51:38,748  INFO - Getting services from metainfo
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / KAFKA
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / PIG
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / ZEPPELIN
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / LOGSEARCH
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / MAPREDUCE2
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / SLIDER
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / HIVE
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / TEZ
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / HBASE
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / OOZIE
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / FLUME
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / MAHOUT
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / HDFS
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / DRUID
2017-06-27 13:51:38,748  INFO - Processing HDP-2.6 / AMBARI_METRICS
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / SPARK
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / SMARTSENSE
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / AMBARI_INFRA
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / YARN
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / FALCON
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / SPARK2
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / ZOOKEEPER
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / ATLAS
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / SQOOP
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / STORM
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / KNOX
2017-06-27 13:51:38,749  INFO - Processing HDP-2.6 / KERBEROS
2017-06-27 13:51:38,749  INFO - Comparing required service configs from stack with mapped service configs from db
2017-06-27 13:51:38,751 ERROR - Required config(s): product-info is(are) not available for service SMARTSENSE with service config version 2 in cluster cl1
2017-06-27 13:51:38,751 ERROR - Required config(s): parquet-logging is(are) not available for service HIVE with service config version 7 in cluster cl1
2017-06-27 13:51:38,751  INFO - Getting services which has mapped configs which are not selected in clusterconfigmapping
2017-06-27 13:51:38,755  INFO - Checking Topology tables
2017-06-27 13:51:38,757  INFO - Checking for tables with large physical size
2017-06-27 13:51:38,759  INFO - The database table host_role_command is currently 16.703 MB and is within normal limits (3000.000)
2017-06-27 13:51:38,760  INFO - The database table execution_command is currently 12.516 MB and is within normal limits (3000.000)
2017-06-27 13:51:38,761  INFO - The database table stage is currently 0.109 MB and is within normal limits (3000.000)
2017-06-27 13:51:38,761  INFO - The database table request is currently 0.078 MB and is within normal limits (3000.000)
2017-06-27 13:51:38,762  INFO - The database table alert_history is currently 0.563 MB and is within normal limits (3000.000)
2017-06-27 13:51:38,762  INFO - ******************************* Check database completed *******************************
{code}


Looks like the issue is because the DB query does not return 'parquet-logging' and 'product-info' in type_name, while they exist in the respective metainfo files. See attached SQL query output and below:

{code}
root@ctr-e133-1493418528701-156570-01-000005:/var/lib/ambari-server/resources/common-services/HIVE/0.12.0.2.0# cat /var/lib/ambari-server/resources/common-services/HIVE/0.12.0.2.0/metainfo.xml | grep -C2 parquet
            <configFile>
              <type>env</type>
              <fileName>parquet-logging.properties</fileName>
              <dictionaryName>parquet-logging</dictionaryName>
            </configFile>
          </configFiles>
--
        <config-type>webhcat-site</config-type>
        <config-type>webhcat-env</config-type>
        <config-type>parquet-logging</config-type>
        <config-type>ranger-hive-plugin-properties</config-type>
        <config-type>ranger-hive-audit</config-type>

{code}",upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-06-28 12:09:46,13
13083116,ORA-00911 error during Ambari server schema upgrade due to incorrect syntax of Update statement,"Looks due to the fix of AMBARI-21291 

*STR*
# Deployed cluster with Ambari version: 2.2.2.0 and HDP version: 2.4.2.0-258 (Oracle DB)
# Upgrade Ambari to target Version: 2.5.2.0-77 | Hash: 3ebf403f9d5065bfc83fae5d699bc4a88f67d179

*Result*
{code}
28 Jun 2017 04:42:23,733  INFO [main] DBAccessorImpl:848 - Executing query: ALTER TABLE request ADD cluster_host_info BLOB NULL
28 Jun 2017 04:42:23,763  INFO [main] DBAccessorImpl:878 - Executing prepared query: UPDATE REQUEST SET CLUSTER_HOST_INFO=? WHERE CLUSTER_HOST_INFO IS NULL;
28 Jun 2017 04:42:23,770 ERROR [main] DBAccessorImpl:895 - Error executing prepared query: UPDATE REQUEST SET CLUSTER_HOST_INFO=? WHERE CLUSTER_HOST_INFO IS NULL;
java.sql.SQLSyntaxErrorException: ORA-00911: invalid character

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
        at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
        at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:3677)
        at oracle.jdbc.driver.OraclePreparedStatementWrapper.execute(OraclePreparedStatementWrapper.java:1374)
        at org.apache.ambari.server.orm.DBAccessorImpl.executePreparedQuery(DBAccessorImpl.java:892)
        at org.apache.ambari.server.orm.DBAccessorImpl.executePreparedUpdate(DBAccessorImpl.java:911)
        at org.apache.ambari.server.orm.DBAccessorImpl.moveColumnToAnotherTable(DBAccessorImpl.java:1428)
        at org.apache.ambari.server.upgrade.UpgradeCatalog251.moveClusterHostColumnFromStageToRequest(UpgradeCatalog251.java:131)
        at org.apache.ambari.server.upgrade.UpgradeCatalog251.executeDDLUpdates(UpgradeCatalog251.java:90)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:925)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:426)
28 Jun 2017 04:42:23,773 ERROR [main] SchemaUpgradeHelper:210 - Upgrade failed.
java.sql.SQLSyntaxErrorException: ORA-00911: invalid character

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
        at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
        at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:3677)
        at oracle.jdbc.driver.OraclePreparedStatementWrapper.execute(OraclePreparedStatementWrapper.java:1374)
        at org.apache.ambari.server.orm.DBAccessorImpl.executePreparedQuery(DBAccessorImpl.java:892)
        at org.apache.ambari.server.orm.DBAccessorImpl.executePreparedUpdate(DBAccessorImpl.java:911)
        at org.apache.ambari.server.orm.DBAccessorImpl.moveColumnToAnotherTable(DBAccessorImpl.java:1428)
        at org.apache.ambari.server.upgrade.UpgradeCatalog251.moveClusterHostColumnFromStageToRequest(UpgradeCatalog251.java:131)
        at org.apache.ambari.server.upgrade.UpgradeCatalog251.executeDDLUpdates(UpgradeCatalog251.java:90)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:925)
{code}


Issue appears Oracle specific possibly due to missing quotes in ? of the Update SQL statement
",regression upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-06-28 12:04:35,1
13082439,Styling Issues with newly implemented workflow manager file browser ,"Issue 1: Input box to provide file input path is not visible as part of file browser modal window without scrolling.
Need to make it more prominent.

Issue 2 : Folder listing is distorted in firefox browsers. Need to left align these subdirectories. 
Attaching the screenshot.
",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-06-26 08:36:46,45
13081417,Python tests fail for ambari-server and ambari-agent on ppc64le,"There are 78 python test failures in Ambari agent for ppc64le

Error:

{code:java}
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-agent/src/test/python/resource_management/TestXmlConfigResource.py"", line 68, in test_action_create_empty_xml_config
    configuration_attributes={}
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/environment.py"", line 118, in run_action
    resource.provider)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/providers/__init__.py"", line 93, in find_provider
    if resource in os_family_provider:
UnboundLocalError: local variable 'os_family_provider' referenced before assignment
{code}

In Ambari-server, the below tests fail: 


{code:java}
ERROR: test_configure_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 37, in test_configure_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_install_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 79, in test_install_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 41, in install
    self.configure(env)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_start_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 48, in test_start_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 49, in start
    self.configure(env)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_start_default_22_with_phoenix_enabled (test_hbase_regionserver.TestHbaseRegionServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HBASE/test_hbase_regionserver.py"", line 427, in test_start_default_22_with_phoenix_enabled
    target = RMFTestCase.TARGET_COMMON_SERVICES)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 93, in start
    self.configure(env) # for security
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 49, in configure
    hbase(name='regionserver')
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase.py"", line 224, in hbase
    Package(params.phoenix_package,
AttributeError: 'module' object has no attribute 'phoenix_package'

FAIL: test_clean_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 124, in test_clean_default
    self.assert_clean_default()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 182, in assert_clean_default
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/removeMysqlUser.sh mysql hive c6402.ambari.apache.org' != u'bash -x /tmp/removeMysqlUser.sh mysqld hive c6402.ambari.apache.org'

FAIL: test_clean_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 135, in test_clean_secured
    self.assert_clean_secured()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 193, in assert_clean_secured
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/removeMysqlUser.sh mysql hive c6402.ambari.apache.org' != u'bash -x /tmp/removeMysqlUser.sh mysqld hive c6402.ambari.apache.org'

FAIL: test_configure_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 38, in test_configure_default
    self.assert_configure_default()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 153, in assert_configure_default
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/addMysqlUser.sh mysql hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org' != u'bash -x /tmp/addMysqlUser.sh mysqld hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org'

FAIL: test_configure_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 81, in test_configure_secured
    self.assert_configure_secured()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 171, in assert_configure_secured
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/addMysqlUser.sh mysql hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org' != u'bash -x /tmp/addMysqlUser.sh mysqld hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org'

FAIL: test_start_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 53, in test_start_default
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'start') != ('service', 'mysqld', 'start')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'start')
+ ('service', 'mysqld', 'start')
?                   +


FAIL: test_start_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 96, in test_start_secured
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'start') != ('service', 'mysqld', 'start')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'start')
+ ('service', 'mysqld', 'start')
?                   +


FAIL: test_stop_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 68, in test_stop_default
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'stop') != ('service', 'mysqld', 'stop')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'stop')
+ ('service', 'mysqld', 'stop')
?                   +


FAIL: test_stop_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 112, in test_stop_secured
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'stop') != ('service', 'mysqld', 'stop')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'stop')
+ ('service', 'mysqld', 'stop')
?                   +


FAIL: test_service_check_default (test_service_check.TestServiceCheck)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 40, in test_service_check_default
    self.assert_service_check()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 153, in assert_service_check
    try_sleep = 5,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: '/tmp/oozieSmoke2.sh suse /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False' != u'/tmp/oozieSmoke2.sh suse-ppc /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False'

FAIL: test_service_check_secured (test_service_check.TestServiceCheck)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 53, in test_service_check_secured
    self.assert_service_check()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 153, in assert_service_check
    try_sleep = 5,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: '/tmp/oozieSmoke2.sh suse /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False' != u'/tmp/oozieSmoke2.sh suse-ppc /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False'

FAIL: test_hook_default (test_before_install.TestHookBeforeInstall)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/hooks/before-INSTALL/test_before_install.py"", line 42, in test_hook_default
    repo_template='[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 282, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6 [truncated]... != {'base_url': u'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0. [truncated]...
  {'action': ['create'],
-  'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0',
+  'base_url': u'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0',
?              +

-  'components': ['HDP', 'main'],
+  'components': [u'HDP', 'main'],
?                 +

   'mirror_list': None,
-  'repo_file_name': 'HDP',
+  'repo_file_name': u'HDP',
?                    +

-  'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'}
+  'repo_template': u'{{package_type}} {{base_url}} {{components}}'}

FAIL: test_hook_default_repository_file (test_before_install.TestHookBeforeInstall)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/hooks/before-INSTALL/test_before_install.py"", line 80, in test_hook_default_repository_file
    append_to_file=False)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 282, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'append_to_file': False, 'base_url': 'http://repo1/HDP/centos5/2.x/updates/2.2. [truncated]... != {'append_to_file': False, 'base_url': u'http://repo1/HDP/centos5/2.x/updates/2.2 [truncated]...
  {'action': ['create'],
   'append_to_file': False,
-  'base_url': 'http://repo1/HDP/centos5/2.x/updates/2.2.0.0',
+  'base_url': u'http://repo1/HDP/centos5/2.x/updates/2.2.0.0',
?              +

-  'components': ['HDP', 'main'],
+  'components': [u'HDP', 'main'],
?                 +

   'mirror_list': None,
   'repo_file_name': 'ambari-hdp-4',
-  'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'}
+  'repo_template': u'{{package_type}} {{base_url}} {{components}}'}

FAIL: testTransparentHugePage (TestCheckHost.TestCheckHost)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/custom_actions/TestCheckHost.py"", line 407, in testTransparentHugePage
    self.assertEquals(structured_out_mock.call_args[0][0], {'transparentHugePage' : {'message': 'never', 'exit_code': 0}})
AssertionError: {'transparentHugePage': {'message': '', 'exit_code': 0}} != {'transparentHugePage': {'message': 'never', 'exit_code': 0}}
- {'transparentHugePage': {'exit_code': 0, 'message': ''}}
+ {'transparentHugePage': {'exit_code': 0, 'message': 'never'}}
?                                                      +++++

{code}



",powerpc ppc64le,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2017-06-21 13:03:56,5
13078503,Upgrade PreUpgradeCheck NullPointerException ,"
java.lang.NullPointerException
        at org.apache.ambari.server.state.stack.UpgradePack.getPrerequisiteChecks(UpgradePack.java:151)

",upgrade,['ambari-server'],AMBARI,Bug,Major,2017-06-09 03:24:19,70
13076202,Eliminate Maven warnings,"Get rid of as many Maven warnings as possible:

{noformat}
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-web:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:exec-maven-plugin @ line 161, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-admin:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 91, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-common:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-hadoop-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-flume-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-kafka-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-storm-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-storm-sink-legacy:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 252, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-host-monitoring:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 86, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 110, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-grafana:pom:2.1.0.0.0
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 64, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-assembly:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-host-aggregator:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-antrun-plugin @ line 699, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-antrun-plugin @ line 735, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:exec-maven-plugin @ line 824, column 15
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:properties-maven-plugin is missing. @ line 469, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-funtest:jar:2.0.0.0-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.httpcomponents:httpclient:jar -> version 4.2.5 vs 4.5.2 @ line 559, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-failsafe-plugin is missing. @ line 52, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-agent:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:properties-maven-plugin is missing. @ line 207, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-05-31 14:24:34,21
13076196,Code cleanup,Clean up Ambari Server source code warnings.,pull-request-available,['ambari-server'],AMBARI,Epic,Major,2017-05-31 14:00:24,21
13075924,Update Database Access Layer to Support New Database Schema for Improved User Account Management,"Update Database Access Layer to Support New Database Schema for Improved User Account Management.  

* Update {{org.apache.ambari.server.orm.entities.UserEntity}}
* Update {{org.apache.ambari.server.orm.dao.UserDAO}}
* Add {{org.apache.ambari.server.orm.entities.UserAuthenticationEntity}}
* Add {{org.apache.ambari.server.orm.dao.UserAuthenticationDAO}}
",user_management,['ambari-server'],AMBARI,Bug,Major,2017-05-30 18:40:55,30
13074764,Ambari STS2 checker should use principal in secure cluster,"In the secure cluster, Ambari STS checker for Spark2 seems to generate misleading error every one minute.

{code}
17/05/24 19:24:35 ERROR TThreadPoolServer: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type PLAIN
 
17/05/24 19:25:35 ERROR TThreadPoolServer: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type PLAIN
 
17/05/24 19:26:36 ERROR TThreadPoolServer: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type PLAIN
 
17/05/24 19:27:35 ERROR TThreadPoolServer: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type PLAIN
{code}",spark,[],AMBARI,Bug,Major,2017-05-25 07:22:17,71
13074103,Integrate Titan into Ambari,"Titan is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is Titan’s foundational benefit.
This story is aimed to integrate Titan into Ambari so that it can be installed and managed by Ambari from web UI.",titan,[],AMBARI,Story,Major,2017-05-23 07:28:08,72
13073934,HSI start failed due to Unrecognized VM option 'UseParallelGC-Xss512k' during EU (missing space),"STR
1. Deploy HDP-2.5 cluster with HSI enabled and Ambari-2.4.2.0
2. Upgrade Ambari to 2.5.1.0-140
3. Start EU to HDP-2.6.1.0-105

Result
Error during HSI start
From yarn log
{code}
org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon
Unrecognized VM option 'UseParallelGC-Xss512k'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code}


Looks to be caused by https://issues.apache.org/jira/browse/AMBARI-20537",express_upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2017-05-22 17:09:59,73
13073346,Kafka broker goes down after Ambari upgrade from 2.5.0 to 2.5.1 due to missing 'kafka.timeline.metrics.instanceId' property,"STR:
1. Deploy HDP-2.6.0 cluster with Ambari-2.5.0.3
2. Upgrade Ambari to 2.5.1.0-130 (hash: e7745e411638405dd0a406305e1da222c6879daa)
3. Observed that Kafka broker went down

Kafka logs indicate:
{code}
[2017-05-19 03:42:17,105] FATAL  (kafka.Kafka$)
java.lang.IllegalArgumentException: requirement failed: Missing required property 'kafka.timeline.metrics.instanceId'
        at scala.Predef$.require(Predef.scala:233)
        at kafka.utils.VerifiableProperties.getString(VerifiableProperties.scala:177)
        at org.apache.hadoop.metrics2.sink.kafka.KafkaTimelineMetricsReporter.init(KafkaTimelineMetricsReporter.java:169)
        at kafka.metrics.KafkaMetricsReporter$$anonfun$startReporters$1.apply(KafkaMetricsReporter.scala:66)
        at kafka.metrics.KafkaMetricsReporter$$anonfun$startReporters$1.apply(KafkaMetricsReporter.scala:64)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at kafka.metrics.KafkaMetricsReporter$.startReporters(KafkaMetricsReporter.scala:64)
        at kafka.server.KafkaServerStartable$.fromProps(KafkaServerStartable.scala:27)
        at kafka.Kafka$.main(Kafka.scala:58)
        at kafka.Kafka.main(Kafka.scala)
{code}

Did not see this property being added on ambari_upgrade_config_changes.json file",upgrade,['ambari-upgrade'],AMBARI,Bug,Blocker,2017-05-19 07:44:29,28
13072699,Pause Upgrade button is present in the wizard even after Upgrade is finalized,"Once the Express Upgrade is finalized the wizard still shows 'Pause Upgrade' button.

Expected Result: Pause Upgrade should be hidden or grayed out at this stage

Two side-effects of this issue:
1. Bad user experience - upgrade has completed, we still let them pause the upgrade
2. If user pauses the upgrade, it brings Ambari into a state where it thinks that Upgrade is actually paused and locks the screen for any actions. The UI still shows ""Upgrade:Action Required"" but there is no way to open the EU wizard again. As a result, Ambari functionality cannot be used",express_upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-05-17 06:22:31,7
13071484,Need SLA monitor for WFM dashboard.,WFM needs SLA monitoring on UI on Dashboard. It should be similar to the one in the older Oozie UI.,WFD WFM,['ambari-views'],AMBARI,Improvement,Major,2017-05-12 09:14:15,74
13071408,Hive 1.5.0 view does not load in Internet Explorer 11,"1. Create a Hive 1.5.0 view instance execute it in IE 11.
2. A blank page is displayed with the following error message in the web console:
SCRIPT1002: Syntax error
File: hive.js, Line: 8374, Column: 23

This is because the ES6 arrow (=>) functions are not supported in IE

",pull-request-available,['ambari-views'],AMBARI,Bug,Major,2017-05-12 01:32:16,75
13070949,ambari/dev-support/docker build is broken on trunk,"Several issues need to be fixed to get the container working;
1. Version of maven needs to be updated
2. JDK should be version 7
3. Set bower flag to run as root in container

{code}
docker build -t ambari/build ./dev-support/docker/docker
....
[INFO] Reactor Summary:
[INFO] 
[INFO] Ambari Main ....................................... SUCCESS [4:58.949s]
[INFO] Apache Ambari Project POM ......................... SUCCESS [6.050s]
[INFO] Ambari Web ........................................ FAILURE [15.348s]
[INFO] Ambari Views ...................................... SKIPPED
[INFO] Ambari Admin View ................................. SKIPPED
[INFO] utility ........................................... SKIPPED
[INFO] ambari-metrics .................................... SKIPPED
[INFO] Ambari Metrics Common ............................. SKIPPED
[INFO] Ambari Metrics Hadoop Sink ........................ SKIPPED
[INFO] Ambari Metrics Flume Sink ......................... SKIPPED
[INFO] Ambari Metrics Kafka Sink ......................... SKIPPED
[INFO] Ambari Metrics Storm Sink ......................... SKIPPED
[INFO] Ambari Metrics Storm Sink (Legacy) ................ SKIPPED
[INFO] Ambari Metrics Collector .......................... SKIPPED
[INFO] Ambari Metrics Monitor ............................ SKIPPED
[INFO] Ambari Metrics Grafana ............................ SKIPPED
[INFO] Ambari Metrics Assembly ........................... SKIPPED
[INFO] Ambari Server ..................................... SKIPPED
[INFO] Ambari Functional Tests ........................... SKIPPED
[INFO] Ambari Agent ...................................... SKIPPED
[INFO] Ambari Client ..................................... SKIPPED
[INFO] Ambari Python Client .............................. SKIPPED
[INFO] Ambari Groovy Client .............................. SKIPPED
[INFO] Ambari Shell ...................................... SKIPPED
[INFO] Ambari Python Shell ............................... SKIPPED
[INFO] Ambari Groovy Shell ............................... SKIPPED
[INFO] ambari-logsearch .................................. SKIPPED
[INFO] Ambari Logsearch Appender ......................... SKIPPED
[INFO] Ambari Logsearch Config Api ....................... SKIPPED
[INFO] Ambari Logsearch Config Zookeeper ................. SKIPPED
[INFO] Ambari Logsearch Web .............................. SKIPPED
[INFO] Ambari Logsearch Server ........................... SKIPPED
[INFO] Ambari Logsearch Log Feeder ....................... SKIPPED
[INFO] Ambari Logsearch Assembly ......................... SKIPPED
[INFO] Ambari Logsearch Integration Test ................. SKIPPED
[INFO] ambari-infra ...................................... SKIPPED
[INFO] Ambari Infra Solr Client .......................... SKIPPED
[INFO] Ambari Infra Solr Plugin .......................... SKIPPED
[INFO] Ambari Infra Solr Assembly ........................ SKIPPED
[INFO] Ambari Infra Manager .............................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6:01.681s
[INFO] Finished at: Wed May 10 19:58:42 UTC 2017
[INFO] Final Memory: 21M/225M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.4:install-node-and-yarn (install node and yarn) on project ambari-web: The plugin com.github.eirslett:frontend-maven-plugin:1.4 requires Maven version 3.1.0 -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.4:install-node-and-yarn (install node and yarn) on project ambari-web: The plugin com.github.eirslett:frontend-maven-plugin:1.4 requires Maven version 3.1.0
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:170)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)
Caused by: org.apache.maven.plugin.PluginIncompatibleException: The plugin com.github.eirslett:frontend-maven-plugin:1.4 requires Maven version 3.1.0
        at org.apache.maven.plugin.internal.DefaultMavenPluginManager.checkRequiredMavenVersion(DefaultMavenPluginManager.java:283)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:166)
        ... 19 more
{code}",docker,[],AMBARI,Bug,Minor,2017-05-10 20:52:10,76
13070833,Hive 2.0 View Table Statistics overflows Integer Type convert to Long,"When testing with our extremely large datasets the statistics data returns a failure and the following stack trace. Using Integer for Hive Table Stats unfortunately is not going to work. It needs to be a long. I've provided a patch to fix this.


09 May 2017 10:24:34,474 ERROR [ambari-client-thread-55044] ContainerResponse:419 - The RuntimeException could not be mapped to a response, re-throwing to the HT
TP container
java.lang.NumberFormatException: For input string: ""12375183159""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:583)
        at java.lang.Integer.valueOf(Integer.java:766)
        at org.apache.ambari.view.hive20.internal.parsers.TableMetaParserImpl.getTableStats(TableMetaParserImpl.java:115)
        at org.apache.ambari.view.hive20.internal.parsers.TableMetaParserImpl.parse(TableMetaParserImpl.java:62)
        at org.apache.ambari.view.hive20.resources.browser.DDLProxy.getTableProperties(DDLProxy.java:157)
        at org.apache.ambari.view.hive20.resources.browser.DDLService.getTableInfo(DDLService.java:278)
        at sun.reflect.GeneratedMethodAccessor253.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)",views,['contrib'],AMBARI,Bug,Major,2017-05-10 14:36:18,38
13070565,Workflow Designer View: Provide sorting of workflows on dashboard,"The workflows displayed on the worflow designer view dashboard are not sortable. The ability to sort via Name, Status, User, Created Time, End Time and jobId should be included",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-05-09 19:44:56,74
13070487,Pig and CapSched views are not loading in 2.4.3,"After ""ember-precompile-brunch"" package has been changed from ""1.5.1"" to ""0.1.0"", ember fails to resolve view templates results in pig and capsched views fails to render and goes blank. This patch adds custom resolver to fix the issue.

Pig and Cap sched views are not loading :
Cap Sched : https://172.22.106.180:8443/#/main/views/CAPACITY-SCHEDULER/1.0.0/AUTO_CS_INSTANCE
Pig : https://172.22.106.180:8443/#/main/views/PIG/1.0.0/Pig

Pig View error:
{code:java}
ember.js:15373 Error while processing route: splash Assertion Failed: Could not find ""splash"" template or view. Error: Assertion Failed: Could not find ""splash"" template or view.
    at EmberError (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:30819:23)
    at Object.Ember.assert (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:20433:15)
    at Class.render (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:41957:17)
    at Class.renderTemplate (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/app.js:3438:10)
    at apply (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:36242:27)
    at Class.superWrapper [as renderTemplate] (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:35813:15)
    at Class.setup (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:41223:16)
    at applyHook (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:63552:30)
    at callHook (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:63546:14)
    at handlerEnteredOrUpdated (https://172.22.106.182:8443/views/PIG/1.0.0/PIG/static/javascripts/vendor.js:62306:7)

{code}",system_test,['ambari-views'],AMBARI,Bug,Blocker,2017-05-09 16:41:33,77
13069536,Configuration parameter 'hdfs-site' was not found error when AMS rootdir is on s3a,"When I specify AMS rootdir to be on s3a and restart AMS, I would get the following error:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py"", line 86, in <module>
    AmsCollector().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 315, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 817, in restart
    self.start(env)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py"", line 48, in start
    self.configure(env, action = 'start') # for security
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 118, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py"", line 43, in configure
    hbase('master', action)
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/hbase.py"", line 222, in hbase
    dfs_type=params.dfs_type
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 119, in run_action
    provider = provider_class(resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 503, in __init__
    self.assert_parameter_is_set('hdfs_site')
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 575, in assert_parameter_is_set
    if not getattr(self.resource, parameter_name):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'hdfs-site' was not found in configurations dictionary!
{code}",s3,[],AMBARI,Bug,Major,2017-05-05 17:50:39,39
13069272,LDAPS connections to an Active Directory when enabling Kerberos should validate the server's SSL certificate,"LDAPS connections to an Active Directory when enabling Kerberos should validate the server's SSL certificate.  The current implementation skips validation checks to help avoid SSL issues; however this is not secure. Also the _trusting_ SSL connection may not support the more secure SSL protocols - TLSv1.2.

A flag in the {{ambari.properties}} file ({{kerberos.operation.verify.kdc.trust}}) should be available to allow for the user to select either a _trusting_  SSL connection or a validating (non-trusting) SSL connection to be used.  The default should be to use the standard (non-trusting) SSL connection. 
",active-directory active_directory kerberos ssl,['ambari-server'],AMBARI,Bug,Major,2017-05-04 20:39:13,30
13069181,Metrics Collector stopped by itself after enabling security [Upgrade from 2.4.0.0 to 2.4.3.0],"STR:
1) Deploy cluster - 2.4.0.0
2) Make ambari only upgrade to 2.4.3.0
3) Enable security

Actual result: Metrics Collector stopped by itself after enabling security.",upgrade,['ambari-metrics'],AMBARI,Bug,Critical,2017-05-04 15:39:00,42
13068235,Create Database Schema for Improved User Account Management,"User management tables in the DB should be:

*{{users}}*
||Name||Type||Description||
|user_id|INTEGER|Internal unique identifier|
|principal_id|INTEGER|Foreign key from adminprincipal table|
|user_name|VARCHAR|Unique, case-insensitive, login identifier expected to be used when logging into Ambari|
|active|BOOLEAN|Active/not active flag|
|consecutive_failures|INTEGER|The number a failed authorization attempts since the last successful authentication|
|active_widgets_layout|VARCHAR| |
|display_name|VARCHAR|Cosmetic name value to show the user in user interfaces|
|local_username|VARCHAR|Case-sensitive username to use when impersonating user in facilities like Ambari Views|
|create_time|TIMESTAMP|Creation time for this account in Ambari|
* Primary Key: {{user_id}}
* Foreign Key: {{principal_id}} -> {{adminprincipal.principal_id}}

*{{user_authentication}}*
||Name||Type||Description||
|user_authentication_id|INTEGER|Primary key for this table|
|user_id|INTEGER|Foreign key from users table|
|authentication_type|VARCHAR|Type of authentication system - LOCAL, LDAP,  KERBEROS, JTW, PAM, etc...
|authentication_key|VARCHAR|Type-specific key (or identifier):
* LOCAL: the user's password (digest)
* LDAP: the user’s distinguished name
* KERBEROS: the user’s principal
* etc...|
|create_time|TIMESTAMP|Creation time of this record
|update_time|TIMESTAMP|Update time for this record, can be used to enforce password retention times|
* Primary Key: {{user_authentication_id}}
* Foreign Key: {{user_id}} -> {{users.user_id}}
",user_management,['ambari-server'],AMBARI,Task,Critical,2017-05-01 20:50:29,30
13068164,WFM: Include an option to clear filters in workflow dashboard,"Currently the filters can be cleared only manually by removing the contents, having a button to clear the filters on one click would be helpful.",WFD WFM,['ambari-views'],AMBARI,Bug,Minor,2017-05-01 16:32:29,74
13067713,WFM view build failure with jsplumb reference,build failure in latest build. remove all references to jsplumb instance,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-28 15:04:37,53
13067658,Create idempotent Ambari DB Schema SQL script for AzureDB,The schema file should be idempotent so that we can retry the script in case of exception.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-04-28 11:10:18,21
13067461,Custom RM principal causes zookeeper HA state store to be inaccessible,"HDP 2.6 stack introduced settings for ACLs on the Yarn Resource Manager HA state store. In `yarn-site/yarn.resourcemanager.zk-acl` the ACL user is set to `rm`. 

If this user name does not match the primary component of the Yarn RM Kerberos principal in `yarn-site/yarn.resourcemanager.principal`, then Yarn is unable to access the state store and RM will stop immediately after start.

During the Kerberos wizard there needs to be a check to see if these settings are out of sync. Or, the zk-acl setting needs to somehow reference the principal and extract the primary root through a variable.",security,['ambari-shell'],AMBARI,Bug,Major,2017-04-27 17:52:45,18
13067299,Cleanup Jsplumb specific code from WFM,Clean up code and dependencies related JSPlumb which is no longer used.,WFD WFM,['ambari-views'],AMBARI,Improvement,Major,2017-04-27 09:16:42,78
13067086,BE: Extend Ambari REST API to Support User Account Management Improvements,"Update the Ambari REST API to allow for GET, POST, PUT, and DELETE operations on the authentication sources related to an Ambari user account.

* * {{/api/v1/users/:USERNAME/sources}}
** List a user’s authentication sources
** Add a new authentication source for a user

* {{/api/v1/users/:USERNAME/sources/:SOURCE_ID}}
** Get details on a specific authentication source for a user
** Modify details for a specific authentication source for a user

Update the following entry points, ensuring backwards compatibility where possible:

* {{/api/v1/users}}
** List all users
** Add a new user
** Backward compatibility: Set password should create or update the appropriate user_authentication record. 

",rest_api security,['ambari-server'],AMBARI,Task,Major,2017-04-26 15:24:44,30
13067078,BE: Improve User Account Management ,"Update the backend for improved user management.  

User management tables in the DB should be:

*{{users}}*
||Name||Type||Description||
|user_id|INTEGER|Internal unique identifier|
|principal_id|INTEGER|Foreign key from adminprincipal table|
|user_name|VARCHAR|Unique, case-insensitive, login identifier expected to be used when logging into Ambari|
|create_time|TIMESTAMP|Creation time for this account in Ambari|
|active|BOOLEAN|Active/not active flag|
|consecutive_failed_auth_attemps|INTEGER|The number a failed authorization attempts since the last successful authentication|
|active_widgets_layout|VARCHAR| |
|display_name|VARCHAR|Cosmetic name value to show the user in user interfaces|
|local_username|VARCHAR|Case-sensitive username to use when impersonating user in facilities like Ambari Views|
* Primary Key: {{user_id}}
* Foreign Key: {{principal_id}} -> {{adminprincipal.principal_id}}

*{{user_authentication}}*
||Name||Type||Description||
|user_authentication_id|INTEGER|Primary key for this table|
|user_id|INTEGER|Foreign key from users table|
|authentication_type|VARCHAR|Type of authentication system - LOCAL, LDAP,  KERBEROS, JTW, PAM, etc...
|authentication_key|VARCHAR|Type-specific key (or identifier):
* LOCAL: the user's password (digest)
* LDAP: the user’s distinguished name
* KERBEROS: the user’s principal
* etc...|
|create_time|TIMESTAMP|Creation time of this record
|update_time|TIMESTAMP|Update time for this record, can be used to enforce password retention times|
* Primary Key: {{user_authentication_id}}
* Foreign Key: {{user_id}} -> {{users.user_id}}

Java code needs to change accordingly.",authentication security,['ambari-server'],AMBARI,Task,Major,2017-04-26 14:52:52,30
13067077,Improve User Account Management Within Ambari,"As of Ambari 2.4, user management is confusing and tends to lead to inconsistent results during synchronization and authentication.  With the addition of new mechanisms such as Kerberos and PAM, this will only get worse.  Therefore, there is a need to rework how Ambari manages users to ensure that new authentication facilities are easily integrated.

The following problems need to be solved:

* *Case-sensitivity*
Some authentication sources are case sensitive and some are not.  Ambari inconsistently handles the case of user names leading to confusing where user metadata is being created or being overwritten.  This issue extends from the front end through the backend and to the database layer.   

* *Username Collisions*
There are several cases where username collisions occur.  One is where a username exists as a local user as well as an external user.  For example, the initial administrator account has is a local user account with the username of ""admin"".  There may also be an external user account with the username ""admin"". In some cases Ambari will treat both accounts as the same user, converting the local account during synchronization operation to an LDAP account. However in other cases, Ambari will treat the accounts as separate users and create a separate account.  

* *REST API*
Due to the implementation of the user resource in the REST API, there is no way to distinguish between user accounts with the same username and different data sources. For example usera/LOCAL vs usera/LDAP.  This is because the primary key for user resources is only the username field.  This make managing users confusing since the REST API entrypoint for user resources is /api/v1/users/:USERNAME and there is no way to retrieve or set the details for a specific user. 
",authentication pull-request-available security user_management,"['ambari-server', 'ambari-web']",AMBARI,Epic,Major,2017-04-26 14:51:45,30
13066864,WFM: Clicking search icon in workflow dashboard throws error,Clicking on search icon when the filter is empty in workflow dashboard throws the error Remote API failed as in screenshot,WFD WFM,['ambari-views'],AMBARI,Bug,Minor,2017-04-25 22:12:00,74
13066466,WFM: Date filter in workflow dashboard is not working properly,Date filter in workflow dashboard is not filtering the values according to the date entered. Also AM/PM is not calculated for the time while filtering.,WFD WFM,['ambari-views'],AMBARI,Bug,Minor,2017-04-24 20:11:12,74
13066396,Unable to proceed from manual prompt in EU wizard due to IllegalArgumentException,"*STR*
# Deployed cluster with Ambari version: 2.2.2.0 and HDP version: 2.4.2.0-258
# Upgrade Ambari to target Version: 2.5.1.0-42 | Hash: ac1f26f309bf23d684089baa6d3db9fa96b92ecc
# Register HDP-2.6.1.0-22 version and install the bits
# Start Express Upgrade and hit Proceed at first manual prompt to 'Stop slider apps'

*Result*
Request fails:
{code}

{
  ""status"" : 400,
  ""message"" : ""java.lang.IllegalArgumentException: Can not transition a stage from COMPLETED to COMPLETED""
}


{code}

",express_upgrade,['ambari-sever'],AMBARI,Bug,Blocker,2017-04-24 16:44:14,33
13065499,Need to change the arrangement of the action buttons in project manager,"Need to change the arrangement of the action buttons in project manager. While asking for confirmation to delete the workflow from history, 'Delete' button is placed at left and 'Close' is on the right.
But its a usual practice to keep the action button on the right and less important 'Close' button on the left.",WFD WFM,['ambari-views'],AMBARI,Bug,Minor,2017-04-20 11:55:29,53
13065443,Need to show appropriate error message while deleting the workflow history in project manager,"Need to show appropriate error message while deleting the workflow history in project manager.
Currently when user clicks on deleting the workflow history, he is shown with the confirmation message ""Do you want to delete the draft"" and once deletion is successful, he is shown the message ""Workflow Successfully Deleted""

Since this operation is not deleting the workflow, instead just deletes the workflow history, messages should be changed appropriately to convey the meaning of his action.

",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-20 08:42:37,53
13065435,Project manager window is not opening for the second time,"Project manager window is not opening for the second time.

Steps to reproduce :
1) Open the project manager(Recent) window by clicking on 'More' link.
2) When modal is opened click 'esc' button or click outside the modal window so that modal gets closed.
3) Now try again to open the recent tab by clicking on 'More' link. Window is not getting opened.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-20 08:29:51,45
13064400,FE: Characters used in usernames should be constrained ,"Characters used in usernames should be constrained such that they cannot contain the following characters:

* Less than symbols ( < )
* Greater than symbols ( > )
* Ampersand ( & )
* Back slashes ( \ )
* Backtick ( ` )
* Pipe ( | )


",security,['ambari-web'],AMBARI,Bug,Critical,2017-04-17 13:54:18,7
13064399,BE: Characters used in usernames should be constrained ,"Characters used in usernames should be constrained such that they cannot contain the following characters:

* Less than symbols ( < )
* Greater than symbols ( > )
* Ampersand ( & )
* Back slashes ( \ )
* Backtick ( ` )
* Pipe ( | )

",security,['ambari-server'],AMBARI,Bug,Critical,2017-04-17 13:48:32,18
13063097,Need capability to swipe designer windows within workflow manager,"This has been an ask from the customer that workflow manager should have the capability to swipe the designer windows as supported by browser windows.

Creating this jira to track the request and this feature addition would be an enhancement for the user experience.",WFD WFM,['ambari-views'],AMBARI,Improvement,Major,2017-04-11 07:24:21,45
13063095,Need way to explicitly ask for workflow name ,"This is a ask from lot of users where they get confused between workflow name and workflow xml file name.
Since workflow manager is auto populating workflow name, usually forgets to enter this manually.
And once he saves the workflow at specific path, he expects the file name to be workflow name.
This behaviour will also damage the user experience in project manager view. 

To avoid this ambiguity, workflow manager should explicitly ask for workflow name.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-04-11 07:18:29,45
13063088,Need more appropriate text in project management window,"Project management window is referring the text as 'My Workflows' and 'Type Workflow Name and Path'.
Since this contains list of workflows, coordinators and bundles, just specifying 'workflows' would be misleading.
So need more appropriate text to represent the group of entities.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-11 06:30:45,79
13062876,Need to show decision conditions for decision node in Flow Graph tab,Decision conditions are not shown in 'Flow Graph' tab in workflow dashboard. Need to show these details when user clicks on decision node.,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-10 12:19:30,80
13062863,User should not be allowed to validate/Submit the workflow in case of duplicate action nodes,"User should not be allowed to validate/Submit the workflow in case of duplicate action nodes. As of now, workflow manager shows error message saying that workflow has duplicate action node. But it still allows the user to submit the workflow and workflow validation/Submission fails. Its better to prompt the user to correct the workflow before allowing him to Validate/Submit.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-10 11:03:51,45
13062859,Need to disable upload file option from file browser window in workflow manager,"Need to disable upload file option from file browser window in workflow manager, as this should only be used to read the files from HDFS.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-10 10:54:51,45
13062381,Incorrect job completion state for pig - Intermittent issue,"Job state is shown incorrectly after running the pig script. Even though job has completed successfully and script has returned the result, job state is being shown as SUBMIT_FAILED.

I dont see any error in pig logs.",pig-view,['ambari-views'],AMBARI,Bug,Major,2017-04-07 09:46:06,38
13062379,Need to show appropriate error when views are not able to read the encrypted file,"Need to show appropriate error message on the UI when ambari views are not able to read the encrypted file.
Issue Scenario 1 : Try to upload/download file from encrypted zone using 'Files view'
Issue Scenario 2 : Try to create a table from using a file from encrypted zone using 'Hive View'
Issue Scenario 3 : Try to access workflows from encrypted zone in 'Workflow Manager View'
Steps to reproduce :
1) Encrypt a folder using the steps as defined in https://github.com/abajwa-hw/security-workshops/blob/master/Setup-TDE-23.md
2) Add a new file to this location.
3) Now try to access this file from different views as described in above scenario.
UI simply fails without showing any error message. Need to show appropriate message to the user to indicating the cause of failure.",HiveView2.0,['ambari-views'],AMBARI,Bug,Critical,2017-04-07 09:42:21,81
13062377,Hive Visual explain is not working on IE browser,"Hive Visual explain is not working on IE 10 browser . On running the visual explain command, only start node is shown and other nodes are not being rendered.
",HiveView2.0,['ambari-views'],AMBARI,Bug,Critical,2017-04-07 09:40:08,50
13062374,Workflow manager is not prompting for custom properties if its part of an expression,"Workflow manager is not prompting for custom properties if its part of an expression. 
Workflow manager should identify all the properties and prompt them during workflow submission.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-04-07 09:37:06,79
13062372,Issue while adding custom job.properties,"Additional job.properties value is not taken unless use clicks on ""+"" button. If user doesn't click on , though its present in the submission modal window, this property is not considered during workflow submission.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-07 09:33:15,80
13062371,Need a way to indicate the action node type in workflow manager,"Currently there's no way for the user to know the action node type from workflow designer page.
Need to add this ability to indicate the action node type in the designer.",WFD WFM,['ambari-views'],AMBARI,Improvement,Major,2017-04-07 09:28:23,45
13062370,Bulk action is not functional in workflow dashboard,"Bulk action is not functional in workflow dashboard. 
Steps to reproduce :
1) Submit couple of workflows.
2) Navigate to dashboard.
3) Select 2workflow using check box.
4) Now try to perform bulk actions.
Issue 1 : Actions should be disabled based on the current state of the workflow
Issue 2 : Rerun option is not available in the action list
Issue 3 : Even after suspending, workflow state is still SUCCESS instead in suspended",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-07 09:26:15,45
13062369,User should be prompted for confirmation before closing the browser window,"User should be prompted for confirmation before closing the browser window. If user is in designer page and in middle of workflow creation and he clicks on closing browser window or browser instance, he should be prompted for confirmation before actually closing the window.",WFD WFM,['ambari-web'],AMBARI,Bug,Major,2017-04-07 09:24:10,50
13062232,Stack advisor code in 2.3 refactor logic for ranger Kafka Plugin," if ranger_plugin_enabled:
      # If ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled,
      # determine if the Ranger/Kafka plug-in enabled enabled or not
      if 'ranger-kafka-plugin-properties' in configurations and \
          'ranger-kafka-plugin-enabled' in configurations['ranger-kafka-plugin-properties']['properties']:
        ranger_plugin_enabled = configurations['ranger-kafka-plugin-properties']['properties']['ranger-kafka-plugin-enabled'].lower() == 'yes'
      # If ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled was not changed,
      # determine if the Ranger/Kafka plug-in enabled enabled or not
      elif 'ranger-kafka-plugin-properties' in services['configurations'] and \
          'ranger-kafka-plugin-enabled' in services['configurations']['ranger-kafka-plugin-properties']['properties']:
        ranger_plugin_enabled = services['configurations']['ranger-kafka-plugin-properties']['properties']['ranger-kafka-plugin-enabled'].lower() == 'yes'

    # Determine the value for kafka-broker/authorizer.class.name
    if ranger_plugin_enabled:
      # If the Ranger plugin for Kafka is enabled, set authorizer.class.name to
      # ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer"" whether Kerberos is
      # enabled or not.
      putKafkaBrokerProperty(""authorizer.class.name"", 'org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer')
    elif security_enabled:
      putKafkaBrokerProperty(""authorizer.class.name"", 'kafka.security.auth.SimpleAclAuthorizer')
    else:
      putKafkaBrokerAttributes('authorizer.class.name', 'delete', 'true')

In the above code after ranger_plugin_enabled is true and inside conditions don't match then also we set authorizer.class.name to RangerKafakAuthorizer.  So, to avoid this after checking ranger_plugin_enabled set to false and then continue with code checking",stack stackadvisor,['ambari-server'],AMBARI,Bug,Major,2017-04-06 20:11:12,59
13061997,Zooming workflows should happen more smoothly,"Zooming workflows should happen more smoothly. As of now if user clicks on +/- , zoom is happening in bigger steps and creates a bad user experience.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-06 06:29:46,80
13061885,Allow compilation with maven >=2.2,"Compilation fails with recent maven due to empty id.

Here is a pull request including a patch correcting it:

https://github.com/apache/ambari/pull/52",easyfix,[],AMBARI,Bug,Minor,2017-04-05 22:17:54,82
13061676,Centering workflows for zoom breaks when multiple tabs exists,"If multiple tabs exists, only the tab that got highlighted at the end has the workflow at center. In remaining tabs, workflow is not at the center. ",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-05 09:31:26,80
13061670,User should be able to visualize inherited properties while submitting the workflow,"User should be able to visualize inherited properties while submitting the workflow.
As asked by one of the user (Artem) its better to show inherited properties like “queueName”, “resourceManager”, “namenode” etc which are added by workflow designer by default.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-05 09:11:39,80
13061110,Need to auto populate the workflow parameters if its already defined in the global space,"Need to auto populate the workflow parameters if its already defined in the global space.
ie If user has already specified workflow parameters in the global space then while submitting the 
workflow, these parameters should be auto populated.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-03 12:07:49,80
13061068,Coordinator and bundle should retain job.properties in submission modal window,"Coordinator should retain job.properties in submission modal window. If user is submitting the same coordinator second time, then initially provided details should be auto populated in the submission modal window. And also similar behavior for bundle too.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-03 09:16:26,80
13061062,Need to improve the grouping of the nodes available in transition section of WFM,"Need to improve the grouping of the nodes available in transition section(Error To and OK to) of WFM. As of now its grouped under 'Kill Nodes' and 'Other Nodes'
But 'Kill Nodes' and 'Other Nodes' look like list items available for selection.
Need to make it more intuitive.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-03 08:40:47,80
13061040,Workflow should retain job.properties in submission modal window,"Workflow should retain job.properties in submission modal window. If user is submitting the same workflow second time, then initially provided details should be auto populated in the submission modal window.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-03 06:59:23,80
13061038,Bad user experience in workflow credential creation,"While adding workflow credentials, 'Add' button is not at all visible in the modal window and only 'Save' button is visible.
Since even after adding all the credential configs 'Add' button is not visible, user is tend to click on Save button and modal window just closes. 
This give the user the wrong impression that credential is created.
Need to change this behavior and make it more intuitive.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-04-03 06:57:12,80
13059479,EU/RU Auto-Retry does not reschedule task when host is not heartbeating before task is scheduled and doesn't have a start time,"STR:
1) Install ambari 2.5.0.1
In the ambari.properties file, set
stack.upgrade.auto.retry.timeout.mins=6
stack.upgrade.auto.retry.check.interval.secs=30

2) Install HDP with any set of services
3) Add NameNode HA
4) Register and install new HDP stack version
5) Start RU
5) Corrupt one step from Core Masters group (e.g., stop ambari-agent on a node while the command is running)
Ambari will restart Restarting NN Batch 1 
6) Fix corrupted step (e.g., start ambari-agent again)
7) Corrupt another step from before the command is scheduled (e.g., stop ambari-agent on a node)
8) Fix corrupted step (e.g., start ambari-agent agent)

The expectation is that Ambari Server should schedule the command on the 2nd node. However, because the command never got an original_start_time and start_time, the RetryUpgradeActionService was not able to retry it since it didn't have any timestamps to compare against.",rolling_upgrade,['ambari-server'],AMBARI,Bug,Major,2017-03-27 18:23:22,69
13058951,Perf: Asynchronous event handling server/agent,"Ambari's internal event model is heavily fragmented with some of the events that result in state transitions being synchronous in nature. The proposal of this Epic is to standardize event handling on the server-side and utilize its asynchronous nature to make architecture changes that are geared to towards scaling up the performance of Ambari server. The goal is to support 5K active nodes with this new architecture.

To summarize the entailment:
- Write a Global Event Observer to allow clients to subscribe to server side events
- Use websocket protocol to reduce network chatter between agent and server
- Reduce the work server has to do to generate STATUS commands for heartbeats.
- Make execution commands leaner replacing the need for providing topology info and configs with every command, instead rely on event model and agent in-memory state.
- Re-architect the agent to make it more intelligent towards maintaining topology info, daemon status and configs.
- Reduce computation requirements for every heartbeat and allow for parallel processing of agent side events

!event-arch.png!


",performance,"['ambari-agent', 'ambari-server']",AMBARI,Epic,Critical,2017-03-24 17:05:43,42
13058663,Remove the use of legacy SSL and TLS protocol versions,"I notice that the explicit enabling of various protocols still includes SSLv2Hello and SSLv3, which are severely broken protocols with numerous known vulnerabilities and not necessary for legacy compatibility. Even TLSv1 and TLSv1.1 have been [discouraged since February 2014|https://community.qualys.com/thread/12421], when all modern browsers supported TLSv1.2. Is there any reason Ambari still needs to enable support for these legacy protocols, and are there any other mitigating controls put in place to prevent downgrade, brute force, padding oracle, and weak parameter attacks against these protocols? Thanks. 

",security ssl tls,"['ambari-server', 'security']",AMBARI,Bug,Major,2017-03-23 17:00:56,30
13058220,Make home directory check as optional in wfmanager view,This is required because some filesystem do not properly support home directory location.,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-22 13:20:27,80
13057756,Storm alerts appear after disabling security [upgrade],"1)Deploy old version Cluster (with AD Security) 
2)Make ambari only upgrade
3) disable security
4) enable security (AD)
5) disable security

*Actual result:* Storm alerts appear after disabling security",upgrade,['ambari-server'],AMBARI,Bug,Major,2017-03-21 01:36:19,83
13057423,Disabling security fails with AttributeError,"Disabling security is failing with :
{code}
Stack Advisor reported an error: AttributeError: 'NoneType' object has no attribute 'replace'
StdOut file: /var/run/ambari-server/stack-recommendations/26/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/26/stackadvisor.err
{code}

Error file shows :
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 166, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 775, in recommendConfigurations
    calculation(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/./../stacks/HDP/2.5/services/stack_advisor.py"", line 480, in recommendStormConfigurations
    storm_nimbus_impersonation_acl.replace('{{storm_bare_jaas_principal}}', storm_bare_jaas_principal)
AttributeError: 'NoneType' object has no attribute 'replace'
{code}",kerberos stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2017-03-20 04:52:24,30
13056623,Workflow manager is only showing 10 actions for a workflow in dashboard,"Workflow manager is only showing 10 actions for a workflow. If a workflow has more than 10 nodes, then information about remaining nodes are not shown in workflow dashboard.

Steps to reproduce :
1) Create a workflow with more than 10nodes.
2) Submit the workflow.
3) Navigate to dashboard and select 'Action' tab.
4) See that only first 10 nodes are being shown.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-03-16 10:59:03,80
13056558,Issue with cut/copy functionality in workflow manager,"
Action node configurations are getting copied across  copied nodes even after pasting the original node.

Steps to reproduce :
1) Create an action node.
2) Copy the node
3) Create a new node of the same type in the workflow by pasting the node.
4) Now modify the configurations for one node. Then configuration is also getting changed for the second node.",WFD WFM,['ambari-views'],AMBARI,Bug,Blocker,2017-03-16 06:44:52,78
13056369,Duplicate entries in DB for auto_<view>_instance privileges upon Ambari server restart,"When I create a new user from Ambari UI and let’s say give him ‘Cluster User’ role; later go to Users page it shows the permissions as seen in the screenshot

The UI display is fine, however when I make an API call like below
/api/v1/users/tom/privileges?fields=*
 
It shows three entries for each auto_<view>_instance privilege. As an example: for ‘AUTO_FILES_INSTANCE’ I see three entries like:
api/v1/users/tom/privileges/6
api/v1/users/tom/privileges/56
api/v1/users/tom/privileges/106
 
and so on, so we have 16 entries (One for Cluster.User + 3 * privileges for each of five View instances)
 
The same behavior is seen for groups too like: /api/v1/groups/gp1/privileges?fields=* 

*Example*
It is expected that only one of the following rows exists:
{noformat}
ambaricustom=> select * from adminprivilege where privilege_id in (6, 56, 106);
privilege_id | permission_id | resource_id | principal_id
--------------+---------------+-------------+--------------
            6 |             4 |          54 |            8
           56 |             4 |          54 |            8
          106 |             4 |          54 |            8
(3 rows)
{noformat}
 
* permission_id (4): VIEW.USER
* resource_id (54): AUTO_FILES_INSTANCE
* principal_id (8): CLUSTER.USER

*Cause*
When Ambari server restarts, it installs the automatically created view instances without first checking to see if they have been previously created. Each restart of Ambari server will create a new set of duplicated records.
",system_test,['ambari-server'],AMBARI,Bug,Critical,2017-03-15 17:34:06,30
13056232,Need to improve log viewing on workflow manager,"Need to restrict the logs within the modal and need to make it more appealing.
As of now logs are going outside the workflow container. ",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-15 09:33:16,80
13050442,Not able to validate/submit for one particular workflow,"Not able to validate/submit for one particular workflow. Attaching the workflow.
Functionality is working fine for other workflows.

Live cluster : https://172.27.31.131:8443

While running the workflow, seeing the below error in oozie logs :

{code}
2017-03-13 09:06:50,182  WARN ParameterVerifier:523 - SERVER[ctr-e129-1487033772569-42714-01-000003.hwx.site] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] The application does not define formal parameters in its XML definition
2017-03-13 09:07:00,224  WARN V1JobsServlet:523 - SERVER[ctr-e129-1487033772569-42714-01-000003.hwx.site] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] URL[GET https://ctr-e129-1487033772569-42714-01-000003.hwx.site:11443/oozie/v2/jobs?action=dryrun&doAs=admin] error[E0302], E0302: Invalid parameter [action]
org.apache.oozie.servlet.XServletException: E0302: Invalid parameter [action]
	at org.apache.oozie.servlet.JsonRestServlet.validateRestUrl(JsonRestServlet.java:448)
	at org.apache.oozie.servlet.JsonRestServlet.service(JsonRestServlet.java:294)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.apache.oozie.servlet.AuthFilter$2.doFilter(AuthFilter.java:171)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:617)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:576)
	at org.apache.oozie.servlet.AuthFilter.doFilter(AuthFilter.java:176)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.apache.oozie.servlet.HostnameFilter.doFilter(HostnameFilter.java:86)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:563)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)
	at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:620)
	at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)
	at java.lang.Thread.run(Thread.java:745)
{code}",WFD WFM,['ambari-views'],AMBARI,Bug,Blocker,2017-03-13 10:02:38,79
13050439,"The Zoom feature in WFM, hides the WF completely either with Maximum and Min","Steps to repro:
Create a WF and add a action
Now use the Zoom feature and Maximize it completely and return to normal. You will not be able to see the WF information.
The same is applied for complete Minimize as well.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-13 09:58:02,80
13050320,Atlas MetaData server start fails while granting permissions to HBase tables after unkerberizing the cluster,"STR
1. Deploy HDP-2.5.0.0 with Ambari-2.5.0.0 (secure MIT cluster installed via blueprint)
2. Express Upgrade the cluster to 2.6.0.0
3. Disable Kerberos
4. Observed that Atlas Metadata server start failed with below errors:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata_server.py"", line 249, in <module>
    MetadataServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 720, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata_server.py"", line 102, in start
    user=params.hbase_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
atlas_titan
ATLAS_ENTITY_AUDIT_EVENTS
atlas
TABLE
ATLAS_ENTITY_AUDIT_EVENTS
atlas_titan
2 row(s) in 0.2000 seconds

nil
TABLE
ATLAS_ENTITY_AUDIT_EVENTS
atlas_titan
2 row(s) in 0.0030 seconds

nil
java exception
ERROR Java::OrgApacheHadoopHbaseIpc::RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered coprocessor service found for name AccessControlService in region hbase:acl,,1480905643891.19e697cf0c4be8a99c54e39aea069b29.
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:7692)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:1897)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:1879)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32299)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2141)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:187)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:167)
{code}

*Cause*
When disabling Kerberos, the stack advisor recommendations are not properly applied due to the order of operations and various conditionals.

*Solution*
Ensure that the stack advisor recommendations are properly applied when disabling Kerberos. 
",system_test,['ambari-server'],AMBARI,Bug,Critical,2017-03-12 12:07:52,30
13049835,Inappropriate kafka log4j configuration results in too much kafka logs ,"Inappropriate kafka log4j configuration results in too much kafka logs which are constantly increasing and never be removed.
Besides,DailyRollingFileAppender does not support property:maxBackupIndex and maxFileSize, Only RollingFileAppender supports it.

Suggestion:
Change kafka log4j appender mode from DailyRollingFileAppender to RollingFileAppender.
In this way, it will automatically remove the useless kafka log file.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-03-10 02:02:48,41
13049685,Not able to view the error log details on UI ,"Not able to view the error logs on UI. When user clicks on 'Details' link, UI is failing with below error:

{code}
Uncaught TypeError: Cannot read property 'nodes' of null
    at n.isWorkflowValid (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-89dda89e1533d3f32ee5166954811095.js:8:25204)
    at n.validate (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-89dda89e1533d3f32ee5166954811095.js:58:12130)
    at http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:65:30047
    at Array.map (native)
    at n.<anonymous> (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:65:29848)
    at n.<anonymous> (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:65:24894)
    at d.p.get (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:9:31289)
    at s (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:11:3104)
    at c (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:11:3694)
    at s (http://172.22.73.134:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:11:3093)
{code}

Steps to reproduce :
1) Try to import a workflow which user doesn't have read permission.
2) 'Permission Denied' message is shown on the UI. But when user clicks on 'Details' link, UI is failing.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-03-09 17:51:12,78
13049651,"User is not able to import workflows, coordinators and bundles","User is not able to import workflows, coordinators and bundles. Seeing below error in UI console.log

{code}
TypeError: Cannot read property 'workflow-app' of null
    at n.processWorkflowXml (oozie-designer-89dda89….js:9)
    at n.importWorkflow (oozie-designer-89dda89….js:9)
    at n.importWorkflowFromString (oozie-designer-89dda89….js:3)
    at n.<anonymous> (oozie-designer-89dda89….js:3)
    at y (vendor-b25c8db….js:19)
    at b (vendor-b25c8db….js:19)
    at g (vendor-b25c8db….js:19)
    at vendor-b25c8db….js:13
    at invoke (vendor-b25c8db….js:5)
    at n.flush (vendor-b25c8db….js:5)
{code}

Facing the same issue while importing the asset from hdfs as well.",WFD WFM,['ambari-views'],AMBARI,Bug,Blocker,2017-03-09 16:21:30,78
13049547,Need hdfs-site for saving ranger audits to hdfs in namenode HA env,"For {{KNOX}} and {{RANGER_KMS}} services which supports ranger plugin, need to have hdfs-site.xml available in respective services conf directory for saving ranger audits to hdfs in namenode HA env.

Below error logs are found, if hdfs-site.xml is not available,
{noformat}
2017-03-01 18:48:50,150 ERROR provider.BaseAuditHandler (BaseAuditHandler.java:logError(327)) - Error writing to log file.
java.lang.IllegalArgumentException: java.net.UnknownHostException: mycluster
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:438)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:311)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:690)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:631)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:160)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2795)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2829)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2811)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:390)
	at org.apache.ranger.audit.destination.HDFSAuditDestination.getLogFileStream(HDFSAuditDestination.java:271)
	at org.apache.ranger.audit.destination.HDFSAuditDestination.access$000(HDFSAuditDestination.java:43)
	at org.apache.ranger.audit.destination.HDFSAuditDestination$1.run(HDFSAuditDestination.java:157)
	at org.apache.ranger.audit.destination.HDFSAuditDestination$1.run(HDFSAuditDestination.java:154)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.ranger.audit.provider.MiscUtil.executePrivilegedAction(MiscUtil.java:523)
	at org.apache.ranger.audit.destination.HDFSAuditDestination.logJSON(HDFSAuditDestination.java:154)
	at org.apache.ranger.audit.queue.AuditFileSpool.sendEvent(AuditFileSpool.java:880)
	at org.apache.ranger.audit.queue.AuditFileSpool.runLogAudit(AuditFileSpool.java:828)
	at org.apache.ranger.audit.queue.AuditFileSpool.run(AuditFileSpool.java:758)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: mycluster
	... 24 more
2017-03-01 18:48:50,151 ERROR queue.AuditFileSpool (AuditFileSpool.java:logError(710)) - Error sending logs to consumer. provider=knox.async.multi_dest.batch, consumer=knox.async.multi_dest.batch.hdfs{{noformat}",Ambari,['ambari-server'],AMBARI,Bug,Major,2017-03-09 09:33:40,25
13049288,Stack Upgrade tests fail during Hive Metastore restart due to missing hive-site.jceks file,"*STR*
# Deploy HDP-2.5.0.0 with Ambari-2.5.x
# Register HDP-2.6 version and install the bits
# Start Express Upgrade

*Result:*
Failed with below error during restart of Hive Metastore:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 260, in <module>
    HiveMetastore().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 313, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 761, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 58, in start
    self.configure(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 116, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 74, in configure
    hive(name = 'metastore')
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 228, in hive
    params.user_group
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/security_commons.py"", line 55, in update_credential_provider_path
    content = StaticFile(src_provider_path)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 123, in action_create
    content = self._get_content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 160, in _get_content
    return content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/source.py"", line 52, in __call__
    return self.get_content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/source.py"", line 76, in get_content
    raise Fail(""{0} Source file {1} is not found"".format(repr(self), path))
resource_management.core.exceptions.Fail: StaticFile('/usr/hdp/current/hive-metastore/conf/conf.server/hive-site.jceks') Source file /usr/hdp/current/hive-metastore/conf/conf.server/hive-site.jceks is not found
{code}",express_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-03-08 15:22:21,31
13049255,Need to show appropriate error message in cases of UI failures,"Need to show appropriate error message in cases of UI failures.

Scenario 1 : When user tries to import invalid workflow xml. For example, if user imports coordinator xml instead of workflow, then appropriate error message should be displayed. Same should be handled for bundles and coordinators.

Scenario 2 : When user tries to import larger workflows. Currently workflow manager supports workflows with 400 nodes. If user imports workflows with more nodes, then appropriate error message should be shown.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-03-08 12:30:18,80
13049249,Not able to import workflow with fs action node,"Not able to import workflow with fs action node. 
Seeing below error in UI console log :

{code}
Uncaught TypeError: Cannot read property '0' of undefined
    at n.handleImport (http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:7:28237)
    at n [as handleImport] (http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-b25c8db62aa353ea5ab0a4f9e228c623.js:11:18180)
    at n.handleImportNode (http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:9:13846)
    at http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:9:27314
    at Array.forEach (native)
    at http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:9:27286
    at Array.forEach (native)
    at n.setupNodeMap (http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:9:27188)
    at n.processWorkflowXml (http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:9:26231)
    at n.importWorkflow (http://172.27.32.136:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-35b7698efcbc6722853be0273625a4ef.js:9:25548)
{code}
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-03-08 12:08:11,80
13049180,Log error while importing the workflow from encrypted path,"Facing issue while importing workflow where api /api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFM/resources/proxy/readWorkflow?workflowPath=/tmp/encrypt/workflow.xml&jobType=WORKFLOW is failing with 500 status code.
Steps to reproduce : 
Try to import workflow /tmp/encrypt/workflow.xml and its failing.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-08 07:10:26,80
13049174,Need to show timer while loading coordinator name while bundle creation,"Need to show timer while loading coordinator name while bundle creation. 
Issue 1 : Some times coordinator name is not fetch even after 5secs. So its better to show timer to indicate that name is getting loaded.
Issue 2: If user provides invalid coordinator path, then appropriate error message should be shown.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-08 06:58:35,80
13049026,"When SPNEGO authentication is enabled for Hadoop in a cluster with NN HA, PXF Process alert fails","When SPNEGO authentication is enabled for Hadoop in a cluster where NN HA is enabled, PXF Process alert fails with the following errors in the ambari-agent.log file 

{noformat}
ERROR 2017-03-07 18:03:58,417 jmx.py:44 - Getting jmx metrics from NN failed. URL: http://c6401.ambari.apache.org:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesy
stem
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/jmx.py"", line 41, in get_value_from_jmx
    data_dict = json.loads(data)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/__init__.py"", line 307, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 335, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 353, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
INFO 2017-03-07 18:04:02,769 logger.py:71 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '""'""'http://c6402.ambari.apache.org:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'""'""' 1>/tmp/tmphTXg76 2>/tmp/tmp5bm2nM''] {'quiet': False}
INFO 2017-03-07 18:04:02,797 logger.py:71 - call returned (0, '')
ERROR 2017-03-07 18:04:02,798 jmx.py:44 - Getting jmx metrics from NN failed. URL: http://c6402.ambari.apache.org:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/jmx.py"", line 41, in get_value_from_jmx
    data_dict = json.loads(data)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/__init__.py"", line 307, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 335, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 353, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
{noformat}

*Cause*
During the test for the {{PXF Process}} alert, the Active NN is found using a JMX call.  This call requires SPNEGO authentication since SPNEGO authentication is turned on for the Hadoop web interfaces. However, a valid Kerberos ticket is not found in the configured user's Kerberos ticket cache. In this case, the configured users is the HDFS user - which technically is not necessary. 

This occurs in 
{code:title=common-services/PXF/3.0.0/package/alerts/api_status.py:137}
    if CLUSTER_ENV_SECURITY in configurations and configurations[CLUSTER_ENV_SECURITY].lower() == ""true"":
      if 'dfs.nameservices' in configurations[HDFS_SITE]:
        namenode_address = get_active_namenode(ConfigDictionary(configurations[HDFS_SITE]), configurations[CLUSTER_ENV_SECURITY], configurations[HADOOP_ENV_HDFS_USER])[1]
      else:
        namenode_address = configurations[HDFS_SITE]['dfs.namenode.http-address']

      token = _get_delegation_token(namenode_address,
                                     configurations[HADOOP_ENV_HDFS_USER],
                                     configurations[HADOOP_ENV_HDFS_USER_KEYTAB],
                                     configurations[HADOOP_ENV_HDFS_PRINCIPAL_NAME],
                                     None)
      commonPXFHeaders.update({""X-GP-TOKEN"": token})
{code}

Inside the call at 

{code}
namenode_address = get_active_namenode(ConfigDictionary(configurations[HDFS_SITE]), configurations[CLUSTER_ENV_SECURITY], configurations[HADOOP_ENV_HDFS_USER])[1]
{code}

*Solution*
Ensure the configured user's Kerberos ticket cache contains a valid ticket before querying for the active NN. Possibly change the acting user to one executing the PXF component. 

",PHD PXF kerberos,['ambari-server'],AMBARI,Bug,Major,2017-03-07 21:15:20,30
13048891,WFM:Performance improvement for adding node on a decision node in large workflow.,"Performance improvement on adding node to large workflow on lower part of decision path.
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-03-07 13:26:10,78
13048800,Need to show timer while loading coordinator name while bundle creation,"Need to show timer while loading coordinator name while bundle creation. 
Issue 1 : Some times coordinator name is not fetch even after 5secs. So its better to show timer to indicate that name is getting loaded.
Issue 2: If user provides invalid coordinator path, then appropriate error message should be shown.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-07 09:02:05,80
13048699,Kerberos identity reference not working for ranger-audit property in hbase,"From stack 2.5 onwards {{xasecure.audit.jaas.Client.option.principal/ranger-hbase-audit}} needs to have principal value available under {{hbase.master.kerberos.principal/hbase-site}}

To achieve that added below block of code under hbase [kerberos.json|https://github.com/apache/ambari/blob/branch-2.5/ambari-server/src/main/resources/stacks/HDP/2.5/services/HBASE/kerberos.json]
{noformat}
{
              ""name"": ""/HBASE/HBASE_MASTER/hbase_master_hbase"",
              ""principal"": {
                ""configuration"": ""ranger-hbase-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hbase-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
}
{noformat}

But on test cluster, {{xasecure.audit.jaas.Client.option.principal/ranger-hbase-audit}} property is not showing the expected value. It is showing the principal/keytab values of {{ams_hbase_master_hbase}} identity. 

Because of wrong reference of principal audit to solr is not working in kerberos environment, as security.json have below entry instead of {{hbase@EXAMPLE.COM}}
{noformat}
""amshbase@EXAMPLE.COM"":[
        ""ranger_audit_user"",
        ""dev""]
{noformat}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2017-03-06 23:57:35,30
13048104,HBase Master CPU Utilization Alert is in unknown state due to kinit error,"HBase Master CPU Utilization Alert is in unknown state due to kinit error:

{noformat}
Execution of '/usr/bin/kinit -c /var/lib/ambari-agent/tmp/curl_krb_cache/metric_alert_ambari-qa_cc_56787c2122a8214ca9775f3433361f8b -kt HTTP/_HOST@EXAMPLE.COM /etc/security/keytabs/spnego.service.keytab > /dev/null' returned 1. kinit: Client not found in Kerberos database while getting initial credentials
{noformat}

This issue is also seen in /var/log/krb5kdc.log:

{noformat}
Mar 03 16:43:06 c6401.ambari.apache.org krb5kdc[4749](info): AS_REQ (4 etypes {18 17 16 23}) 192.168.64.101: CLIENT_NOT_FOUND: /etc/security/keytabs/spnego.service.keytab@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM, Client not found in Kerberos database
{noformat}

*Cause*
It appears that the HBASE alerts.json file ({{common-services/HBASE/0.96.0.2.0/alerts.json}}) has swapped values for the {{kerberos_keytab}} and {{kerberos_principal}} properties.

{code}
      {
        ""name"": ""hbase_master_cpu"",
        ""label"": ""HBase Master CPU Utilization"",
        ""description"": ""This host-level alert is triggered if CPU utilization of the HBase Master exceeds certain warning and critical thresholds. It checks the HBase Master JMX Servlet for the SystemCPULoad property. The threshold values are in percent."",
        ""interval"": 5,
        ""scope"": ""ANY"",
        ""enabled"": true,
        ""source"": {
          ""type"": ""METRIC"",
          ""uri"": {
            ""http"": ""{{hbase-site/hbase.master.info.port}}"",
            ""default_port"": 60010,
            ""connection_timeout"": 5.0,
            ""kerberos_keytab"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.principal}}"",
            ""kerberos_principal"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.keytab}}""
          },
          ""reporting"": {
            ""ok"": {
              ""text"": ""{1} CPU, load {0:.1%}""
            },
            ""warning"": {
              ""text"": ""{1} CPU, load {0:.1%}"",
              ""value"": 200
            },
            ""critical"": {
              ""text"": ""{1} CPU, load {0:.1%}"",
              ""value"": 250
            },
            ""units"" : ""%"",
            ""type"": ""PERCENT""
          },
          ""jmx"": {
            ""property_list"": [
              ""java.lang:type=OperatingSystem/SystemCpuLoad"",
              ""java.lang:type=OperatingSystem/AvailableProcessors""
            ],
            ""value"": ""{0} * 100""
          }
        }
      }
{code}

Notice:
{code}
            ""kerberos_keytab"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.principal}}"",
            ""kerberos_principal"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.keytab}}""
{code}

*Solution*
Fix values for the {{kerberos_keytab}} and {{kerberos_principal}} properties in {{common-services/HBASE/0.96.0.2.0/alerts.json}}:

{code}
            ""kerberos_principal"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.principal}}"",
            ""kerberos_keytab"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.keytab}}""
{code}
",alerts kerberos,['ambari-server'],AMBARI,Bug,Major,2017-03-03 16:49:05,30
13048099,Atlas service check fails during EU on wire encrypted cluster,"STR
1. Deployed cluster with Ambari version: 2.4.2.0-136 and HDP version: 2.5.3.0-37 (wire encrypted cluster)
2. Upgrade Ambari to 2.5.0.0-1030 and then start EU to 2.6
3. Observed following failure at Atlas service check
{code}
2017-03-02 13:22:41,729 - ATLAS service check failed for host atlas_host with error Execution of 'curl -k --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt -s -o /dev/null -w ""%{http_code}"" https://atlas_host:21443/' returned 35. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
000
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/service_check.py"", line 52, in <module>
    AtlasServiceCheck().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 313, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/service_check.py"", line 48, in service_check
    raise Fail(""All instances of ATLAS METADATA SERVER are down."")
resource_management.core.exceptions.Fail: All instances of ATLAS METADATA SERVER are down.
{code}",express_upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-03-03 16:29:04,31
13048044,Ember should take node version specific to wfmanager,Ember should take node version specific to wfmanaer project instead of global one.,WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-03-03 13:39:23,80
13048013,Workflow Manger support for pointing to kill node in decision editor. ,"Workflow Manger support for pointing to kill node in decision editor..
In decision editor , the user should be able to point to a different node for a condition than the pre configured ones.
Also in case of condition-node combination pointing to an structural error in workflow, user should be warned .",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-03 12:01:42,78
13048008,Workflow manager view is not loading,"Workflow manager is not loading and I am seeing below error in UI console log

{code}
Uncaught SyntaxError: Illegal break statement
    at vendor-1b1cf83….js:56
    at vendor-1b1cf83….js:56
(anonymous) @ vendor-1b1cf83….js:56
(anonymous) @ vendor-1b1cf83….js:56
vendor-1b1cf83….js:1 Uncaught Error: Could not find module `ember-resolver` imported from `oozie-designer/resolver`
    at o (vendor-1b1cf83….js:1)
    at a (vendor-1b1cf83….js:1)
    at r.findDeps (vendor-1b1cf83….js:1)
    at a (vendor-1b1cf83….js:1)
    at r.findDeps (vendor-1b1cf83….js:1)
    at a (vendor-1b1cf83….js:1)
    at requireModule (vendor-1b1cf83….js:1)
    at oozie-designer-3f49278….js:58
{code}",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-03 11:57:36,80
13047985,WFM FS editor needs to maintain order of FS operations.,WFM FS editor needs to maintain order of FS operations.,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-03 10:32:08,79
13047670,Failed task during EU is not reported upfront causing Upgrade to show 'Aborted' after Finalize step,"STR
1. Started EU from HDP-2.5.3 to 2.6.0.0
2. Reach till ‘Restart HBASE Master’ task, let it complete,
3. Stop Ambari server, start it back in a minute
4. Now open EU wizard again

Result:
It shows two tasks under HBASE upgrade group as aborted and continues to next steps. 
EU reached till Finalize screen and allowed to hit Finalize. Thereafter an error for HBase Client was thrown

Expected Result:  Fail EU with ‘HOLDING_TIMEOUT’ status at 'HBASE' Upgrade Group and let user retry the failed task and then move forward",express_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-03-02 13:03:20,6
13047636,WFM generates duplicate kill nodes in XML if decision paths have multiple transition to kill node.,The generate XML contains duplicate kill nodes if decision paths have multiple transition to kill node.,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-02 10:16:29,78
13047615,Duplicate node error is shown if kill node transition exists from decision node in WFM,"If there's a transition to kill node from a decision node, node already exists error is shown.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-02 09:03:01,78
13047264,"Tez UI throwing 404 error, Button to minimise TEZ UI, Worksheet save issues, creation of duplicate tabs and other tab issues","Tez UI throwing 404 error, Button to minimise TEZ UI, Worksheet save issues, creation of duplicate tabs and other tab issues",HiveView2.0,['ambari-views'],AMBARI,Bug,Major,2017-03-01 10:08:58,45
13047225,Stopping spark2 doesn't generate any alerts,"STR :
1. Create cluster with spark2
2. Stop spark2

Expected : An alert to signify stop of a service
Actual : No alert",system_test,['ambari-server'],AMBARI,Bug,Major,2017-03-01 08:38:15,71
13047197,Show full error while importing the workflow from encrypted file,"Live cluster : https://172.22.98.248:8443/#/main/views/WORKFLOW_MANAGER/1.0.0/WFM
Facing issue while importing workflow where api /api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFM/resources/proxy/readWorkflow?workflowPath=/user/hrt_qa/test_WfKillSubwf1/workflow.xml&jobType=WORKFLOW is failing with 500 status code.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-03-01 06:21:31,80
13047192,Not able to preview the xml for coordinator,"Issue 1:
Not able to preview the xml for coordinator. I am not seeing any error in UI console log.

Steps to reproduce :
1) Import the coordinator.
2) Import the coordinator.
3) Try to preview the coordinator and UI is not showing preview.

Issue 2:
After issue 1 is encountered, if user tries to perform any other operations, then UI is getting hung.",WFD WFM,['ambari-views'],AMBARI,Bug,Blocker,2017-03-01 05:13:22,78
13047053,a running workflow cannot be killed from WFM ui immediately,"Steps:
1. log into ambari ui
2. go to wfm
3. upload workflow from file system:
https://github.com/hortonworks/certification/blob/pixie-dust/HDPTests/tests/oozie/oozie_1/regression-two/test_WfKillSubwf1/workflow.xml
4. submit the workflow
5. kill the workflow from WFM ui
Error: 
workflow ui is still showing running after the kill. Wait 5 min and refresh the page, we can see it is killed.
if you check the log of wf on ui, it showed it is killed several times{noformat}
2017-02-16 18:25:08,356  INFO DagEngine:520 - SERVER[bugbash-r7-oozie-tde-we-2-1.openstacklocal] USER[ambari-server-cl1 doAs admin] GROUP[-] TOKEN[-] APP[-] 
JOB[0000003-170215185313202-oozie-oozi-W] ACTION[] User admin killed the WF job 
0000003-170215185313202-oozie-oozi-W

2017-02-16 18:25:18,534  INFO DagEngine:520 - SERVER[bugbash-r7-oozie-tde-we-2-
1.openstacklocal] USER[ambari-server-cl1 doAs admin] GROUP[-] TOKEN[-] APP[-] 
JOB[0000003-170215185313202-oozie-oozi-W] ACTION[] User admin killed the WF job 
0000003-170215185313202-oozie-oozi-W

2017-02-16 18:25:57,166  INFO DagEngine:520 - SERVER[bugbash-r7-oozie-tde-we-2-
1.openstacklocal] USER[ambari-server-cl1 doAs admin] GROUP[-] TOKEN[-] APP[-] 
JOB[0000003-170215185313202-oozie-oozi-W] ACTION[] User admin killed the WF job 
0000003-170215185313202-oozie-oozi-W

2017-02-16 18:26:02,047  INFO DagEngine:520 - SERVER[bugbash-r7-oozie-tde-we-2-
1.openstacklocal] USER[ambari-server-cl1 doAs admin] GROUP[-] TOKEN[-] APP[-] 
JOB[0000003-170215185313202-oozie-oozi-W] ACTION[] User admin killed the WF job 
0000003-170215185313202-oozie-oozi-W{noformat}
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-28 18:19:56,78
13047049,After regenerate keytabs post Ambari upgrade yarn.nodemanager.linux-container-executor.cgroups.mount-path property got added with blank value ,"STR
1. Deployed cluster with Ambari version: 2.4.1.0-22 and HDP version: 2.5.0.0-1245(secure cluster)
2. Upgrade Ambari to 2.5.0.0
3. Regenerate keytabs for missing hosts and let it complete
4. Go to YARN - configs

Result
Observed that yarn.nodemanager.linux-container-executor.cgroups.mount-path got added as a property with blank value
",upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-02-28 18:09:13,63
13046939,Pom.xml change in Workflow manager to fix the version of front end plugin to make it compatible with the maven version used in ambari,"The version for front end plugin has to be changed to make it compatible with the maven version used in Amabri
",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-28 11:11:24,45
13046623,Workflow manager is allowing to have empty action node,"Workflow manager is allowing to have empty action node.

Steps to reproduce :
1) Add a new node to the workflow
2) Delete node name and keep it empty.
3) Workflow manager is not complaining about empty node name.

",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-27 11:39:00,78
13046324,Ambari should install the unlimited key JCE policy based on service requirements even if cluster is not Kerberized,"Ambari should install the unlimited key JCE policy based on service requirements even if cluster is not Kerberized.  For example, if a service needs the unlimited key JCE policy for an encryption task not related to Kerberos.

On a similar note, the unlimited key JCE policy is not being distributed to the agents if Kerberos is enabled using the ""manual"" option - as opposed to integrating with an existing KDC. 

On a related note, Ambari Server host automatically unpacks JCE unlimited during ""ambari-server setup"" (unless custom JDK is used).
",encryption jce_policy kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2017-02-25 22:12:24,30
13045973,importing from workflow from the coordinator setup hangs,"Importing from workflow from the coordinator setup hangs
IMPACT: Users do not know what to do next.
STEPS TO REPRODUCE:
log into a WFM and create a workflow and then create coordinator
Add the path to the workflow for the coordinator, 
give it bad path and click on the import workflow
It should validate the path before it tries to load it.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-24 10:05:27,80
13045972,Workflow Manager. Nominal time should accept variables.,"Workflow Manager. Nominal time should accept variables.
currently nominal time accepts time only from date picker. it should allow user to enter variables as wel.
Reported by ArtemErvits",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-24 10:03:03,78
13045957,Kill a workflow jumps to another tab in WFM,"Steps:
1. log into ambari ui
2. go to wfm
3. select an existing workflow. e.g.Workflow10
4. choose edit workflow
5. choose the ""dashboard"" tab to go back to the workflow list
6. select the existing workflow and choose ""action->kill""",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-24 09:24:32,78
13045899,Workflow Manager-mapred action contains duplicates in editor if user saves and reopens editor,"Open a Map-Red editor.
Fill in details and Save.
Reopen the editor. you will see the configuration in both general and advanced section.
Preview xml looks fine though.
Expected:
the configuration should be not duplicated.
Reported by ArtemErvits",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-24 06:32:02,78
13045829,Ambari sends SNMP notifications in incorrect order,"Ambari sends notifications in wrong order. ideally first generated Alert should be notified first.
This issue is reproducible in straight steps. 

1. Have 4 to 5 alerts generated in short span of time.

2. when Ambari notifier thread queries Database with the query like ""select * from alert_notice where notify_state = 'PENDING'"" 

This query returns last one first.

Ex: 
2066	51	4876	PENDING	a11bffc9-3eda-42a7-b8ef-3f197cddfa26
2065	51	4874	PENDING	05e810cd-61a0-4d5c-9687-334aa71e0aaa
2064	51	4875	PENDING	b30999d3-0c78-46a3-aca4-f035743030b8
2063	51	4873	PENDING	9508afee-48dd-4599-a901-8d64ee07b909
2062	51	4872	PENDING	73d02181-138a-440e-8e8e-32f7ac5fbe2e
2061	51	4871	PENDING	42a134f0-f979-4be3-a51d-a5f804a11c89

So Notified threads loops that list and sends one by one.

This needs to be fixed. either have to read "" order by notification_id"" so that it gives correct order. ",patch,['ambari-server'],AMBARI,Bug,Minor,2017-02-23 23:37:31,14
13045681,"Restart Kafka broker failed after enabling kerberos with ""kinit(v5): No such file or directory while getting initial credentials""","Restart Kafka broker failed after enabling kerberos with ""kinit(v5): No such file or directory while getting initial credentials""

Note : hdfs.headless.keytab is missing on the Kafka broker host keytabs folder (/etc/security/keytabs)

Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/KAFKA/0.8.1/package/scripts/kafka_broker.py"", line 129, in <module>
    KafkaBroker().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 280, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 720, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/KAFKA/0.8.1/package/scripts/kafka_broker.py"", line 83, in start
    setup_ranger_kafka() #Ranger Kafka Plugin related call 
  File ""/var/lib/ambari-agent/cache/common-services/KAFKA/0.8.1/package/scripts/setup_ranger_kafka.py"", line 41, in setup_ranger_kafka
    recursive_chmod=True
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 459, in action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 456, in action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 240, in action_delayed
    main_resource.kinit()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 487, in kinit
    user=user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 273, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 70, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 92, in checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 140, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 293, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs@EXAMPLE.COM' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
kinit(v5): No such file or directory while getting initial credentials",system_test,['ambari-admin'],AMBARI,Bug,Blocker,2017-02-23 17:03:36,20
13045612,Job Link URL not working,Please give details and possible attach screen shots to clearly show what went wrong.,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-23 12:51:04,80
13045609,Command text lost when toggling from Command to Args on sqoop in Workflow manager.,Command text lost when toggling from Command to Args on sqoop action. Pressing cancel and opening the configure dialog again restores the settings but this is inconvenient if you had several changes or you haven’t saved the settings yet. ,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-23 12:29:14,78
13045589,Need to show data in sorted order in Hive view,"Need to show data in sorted order in Hive view.

Case 1: Tables list view in 'Query' Tab and 'Tables' tab
Tables should be listed in alphabetical order

Case 2: Jobs should be sorted based on Job Id in 'Jobs' tab. I guess descending order makes sense.

Case 3 :  'Saved Queries' and 'UDFs' tab
Result should be sorted based on user action time. Recent action should come first.

Case 4: 'Settings' tab :
Lexicographical order based on KEY.",HiveView2.0,['ambari-views'],AMBARI,Bug,Major,2017-02-23 11:21:11,45
13045505,All the workflow details are getting logged in UI console log,"All the workflow details are getting logged in UI console log.
Is this logging necessary ? Whether it will have performance implications in rendering the workflow ?

Steps to reproduce :
1) Create a workflow.
2) Refresh the browser.
3) While loading the workflow, all the details are being logged in the UI console log",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-23 04:48:16,79
13045504,Workflow details are lost upon refreshing the browser,"Workflow details are lost upon refreshing the browser.

Steps to reproduce :
1) Create a new workflow.
2) Now refresh the browser.
3) Upon refreshing, the previous workflow tab is not displayed and all the unsaved works are lost.
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-23 04:41:08,78
13045478,Add support for Spark2 upgrade from HDP-2.5,Support of Spark2 service upgrade (both EU and EU) from HDP-2.5.x to 2.6 need to be added,express_upgrade rolling_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-02-23 02:11:02,1
13045202,Not able to compute the table statistics for partitioned table,"Not able to compute the table statistics for partitioned table. API to compute the stat is failing with below error :

{code}
Error while compiling statement: FAILED: SemanticException [Error 10115]: Table is partitioned and partition specification is needed
{code}

Attaching the complete stack trace.
",HiveView2.0,['ambari-views'],AMBARI,Bug,Critical,2017-02-22 11:57:34,38
13045082,Action node shows unsupported properties even though there are none in workflow manager.,"Action node during edit shows unsupported properties. But in unsupported properties you see empty text.
Expected.
Unsupported properties should not be shown if there are no unsupported properties.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-22 05:56:42,78
13044820,Cannot edit join node name in Workflow Manager,"After create a fork node, the join node will generated automatically. But, I cannot edit the name of the join node. Only action I can take to that join node is delete.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-21 12:01:23,79
13044782,Table statistics is not getting computed when 'include columns' is checked,"Table statistics is not getting computed when 'include columns' is checked.

Steps to reproduce :
1) Create a new table and load data into the table .
2) Navigate to the table and recompute the statistics with 'include columns' being checked.
3) Table statistics is not computed by this action and 'Number of Rows' and 'Raw Data Size' still coming as 0.
If user runs the recompute without checking 'include columns' , then results are appearing as expected.",HiveView2.0,['ambari-views'],AMBARI,Bug,Critical,2017-02-21 10:00:54,38
13044777,"On submit of bundle, if any coordinator path contains variables, user should be notified about custom variables.","If there is a variable in file path, it becomes difficult to deduce variables as we don't know the actual workflow that the bundle or coordinator uses. The fix is to just prompt the user the variables cannot be auto detected as path is not known.
In bundle submit, we parse though all coordinator paths and their respective workflows to find the variables to be entered. But in case any of these paths contain ""variable"" section,WFD wont be able to auto prompt these variables . In this scenario, WFD has to notify the user to fill the variables manually using custom variables.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-21 09:45:40,78
13044597,Cut Node feature is broken in workflow manager,"Cut Node feature is broken in workflow manager. While cutting the node I am seeing below error in UI console.

{code}
TypeError: t is undefined
d<.deleteNode()
 oozie-designer-67b1e2c50d1397066ce78c6858be68c4.js:9
e.default<.deleteWorkflowNode()
 oozie-designer-67b1e2c50d1397066ce78c6858be68c4.js:3
e.default<.cutNode()
 oozie-designer-67b1e2c50d1397066ce78c6858be68c4.js:3
a<._addEvents/<()
 oozie-designer-67b1e2c50d1397066ce78c6858be68c4.js:8
J.event.dispatch()
 vendor-415a6fd6db11ecd27aa07f5115312174.js:4
J.event.add/g.handle()
{code}
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-20 16:30:23,78
13044530,WFM does not issue confirmation message when workflow is suspended or killed,"Steps:
1. log into ambari ui
2. go to wfm
3. upload workflow from file system:
4. submit the workflow
5. kill or suspend the workflow from WFM ui
Issue: 
workflow ui is not giving any confirmation message when wf is killed or suspended
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-20 11:33:24,78
13044525,Unable to filter by job id,"After attempting to resume a random SUSPENDED existing workflow 0000078-170215133231623-oozie-oozi-W (ran by the nightly system tests via hrt_qa user), the workflow went into RUNNING state. I wanted to see that workflow by itself on the dashboard, so I used filter criteria with the following syntax:
jobid=0000078-170215133231623-oozie-oozi-W
jobid:0000078-170215133231623-oozie-oozi-W
The first one returned:
Remote api failed
The second one showed the same error message but also a perptual animated busy loading icon.
I expected that if my syntax was incorrect, it would say so like ""jobid:..."" is not correct, do you mean ""Job Id="" or something to that effect. Also, for the second case, there should be a fast fail, rather than a perpetual hanging.
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-20 11:12:08,78
13044511,"When uploading a file using the WFM, need a message to say that either the file upload succeeded or failed","Steps to repo:
1. Create a WF
2. Add a java action
3. Go to Advanced Properties, and under File - click on Browse and use the File Browser to upload a file from local machine.
4. Need to receive a message whether the file upload is successful or failed.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-20 10:24:37,78
13044494,Server Error when trying to save and not overwrite,"When file path exists, typo error in response ""Worfklow path exists."". 
Expected is ""Workflow Path exists""",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-20 09:21:04,80
13044036,Finalize Operations stage fails when Enabling Kerberos using the manual option,"Finalize Operations stage fails when Enabling Kerberos using the manual option.

{noformat:title=Error Message}
Failed to process the identities, could not properly open the KDC operation handler: Must specify a principal but it is null or empty
{noformat}

",kerberos,['ambari-server'],AMBARI,Bug,Critical,2017-02-17 16:57:32,30
13043606,Custom job.properties are not retained in the workflow designer,"Custom job.properties are not retained in the workflow designer.

Steps to reproduce :
1) Submit a workflow with custom job properties say oozie.action.sharelib.for.sqoop=sqoop,hive.
2) From the dashboard, select this workflow.
3) When user tries to submit it again, previously included properties are not visible to the user.",wfd,['ambari-views'],AMBARI,Bug,Major,2017-02-16 11:31:14,45
13043597,Workflow Manager workflow rendering is broken in designer page,"Workflow rendering is broken in designer page.
Steps to reproduce :
1) Import the attached workflow and check that rendering of the workflow is broken.
",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-16 10:41:59,79
13043204,WFM Dashboard Actions menu items not working properly,"On the WFM Dashboard, some of the items in the Action menu in the jobs list don't seem to be working correctly.
I can kill a job with any status, including Succeeded. Doesn't seem you should be able to kill a succeeded job, since succeeded is a final state.
The Kill function works even if the Kill icon is grayed out, indicating it's inactive.
The second and third icons have no tool tips.
The third icon doesn't seem to do anything. The only time it's active is when a job has status of Killed. When I click the icon, nothing happens in the jobs table.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-15 09:45:19,78
13042922,Args value is not retained for ssh node,"Args/arg value is not retained for ssh node.

Steps to reproduce :
1) Create new workflow.
2) Add ssh node
3) Add args/arg value to ssh node and save the configuration
4) Now open the settings for ssh node. Previously inputted value is not visible in the UI.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-14 12:05:27,78
13042914,Credentials from one workflow are getting associated across other workflow,"Credentials from one workflow are getting associated across other workflow.

Steps to reproduce :
1) Create a workflow with credentials.
2) Now open a new workflow tab.
3) Add a new node to the workflow.
4) Preview the new workflow.
Credentials from older workflow are also part of this workflow.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-14 11:44:28,78
13042905,Not able to preview the workflow xml,"Preview xml is failing and I am seeing below error in console.log :

{code}
Uncaught TypeError: Cannot read property 'length' of undefined
    at n.validate (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-ef66fa01464545ddc3f5b0f360f1ea3c.js:7:13121)
    at n.visitNode (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-ef66fa01464545ddc3f5b0f360f1ea3c.js:9:21574)
    at http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-ef66fa01464545ddc3f5b0f360f1ea3c.js:9:22050
    at Array.forEach (native)
    at n.visitNode (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-ef66fa01464545ddc3f5b0f360f1ea3c.js:9:22028)
    at n.process (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-ef66fa01464545ddc3f5b0f360f1ea3c.js:9:19594)
    at n.previewWorkflow (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-ef66fa01464545ddc3f5b0f360f1ea3c.js:3:27474)
    at n.send (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-c52dd73472b01fcbec62872ba470c597.js:15:25559)
    at n [as send] (http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-c52dd73472b01fcbec62872ba470c597.js:11:18800)
    at http://172.27.44.87:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-c52dd73472b01fcbec62872ba470c597.js:13:10172
{code}

Steps to reproduce :
1) Import the attached workflow.
2) Try to preview the workflow.
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-14 11:15:20,78
13042880,Workflow manager- preview workflow has xml encodings,"Workflow manager, preview workflow has xml encoding. using decision editor and use characters like > < etc. these are encoded and sent to back end.
Expected 
conditional operators should not be encoded.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-14 09:29:54,78
13042870,Workflow Manager optimising finding nodes.,"Finding nodes during add node,delete node,add branch takes lot of time for very large workflows. Also serialising and serialising takes lot of time. These need to be optimised.",WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-14 09:05:26,79
13042623,Allow user to view Tez View after executing query,"
As per the parity with Hive View 1.5, allow user to traverse to tez view from the query page after executing a query.
",hive-view,['ambari-views'],AMBARI,Bug,Major,2017-02-13 17:56:16,45
13042563,Workflow Manager Styling changes,Workflow Manager Styling changes- Blue coloured fonts for action items.,WFD WFM,['ambari-views'],AMBARI,Bug,Major,2017-02-13 12:35:12,80
13042500,Issue while resetting the coordinator,"User is not able to reset the coordinator.
Steps to reproduce :
1) Import the attached coordinator.
2) Import the same coordinator in different window
3) Try to preview the xml or reset the coordinator. The UI is getting hung.
",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-13 07:25:51,45
13042497,Coordinator tag sequence is not maintained ,"Coordinator tag sequence is not maintained while creating coordinator.

Steps to reproduce : 
1) Import the attached coordinator.
2) In the preview xml, check the position of controls tag. Its coming in the end.
According to coordinator xsd, this tag should be in the beginning of the coordinator.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-13 07:23:52,78
13042021,Workflow Manager Flow Graph should show line transition to decision path and error node.,"Workflow Manager Flow Graph should show line transition to decision path and error node.
currently flow graph does not show in following scenarios.
-errorTo pointing to an Action node or join node on error Condition
-if actionNode errorTo points to kill node and that path is taken.

Expected.
The above scenarios should show positive flow line on the flog graph.",WFD WFM,['ambari-views'],AMBARI,Bug,Critical,2017-02-10 10:01:17,80
13041983,Workflow designer is getting hung while importing the workflow,"Workflow designer is getting hung while importing the workflow. Workflow is taking time to get loaded.
Attaching the workflow",WFD,['ambari-views'],AMBARI,Bug,Blocker,2017-02-10 07:10:27,78
13041631,User should not be allowed to Execute or visualize empty query,"User should not be allowed to Execute or visualize empty query. As of now when user performs these actions with empty query, UI keeps on loading in that state without coming out of it.
Steps to reproduce :
1) Navigate to Hive 2.0 query editor.
2) Click on Execute/Visual Explain with empty query.
3) UI keeps on loading.",hive-view,['ambari-views'],AMBARI,Bug,Major,2017-02-09 09:01:43,45
13041361,Workfow Manger- Issue in flow graph active transitions in case of decision and fork.,Workflow Manger- There is issue in flow graph active transitions in case of decision and fork. Submit a workflow containing decision or fork. The flow graph should mark green only the active path of execution. Currently it shows all paths .,WFD,['ambari-views'],AMBARI,Bug,Major,2017-02-08 12:06:24,80
13041356,Reset Workflow on newly created workflow blocks UI,"Reset Workflow on newly created workflow blocks UI.
Steps.
create new worklow. 
create a node.
click reset workflow.
Observe that UI is blocked by some modal.

Expected behaviour-: the workflow should become empty workflow with only  start and end node.
",wfd,['ambari-views'],AMBARI,Bug,Major,2017-02-08 11:22:19,45
13041355,Error to tag is not coming for the action node upon copying,"Error to tag is not coming for the action node upon copying
Steps to reproduce :
1) Add an action node to the workflow.
2) Copy the node and create duplicate node in the workflow.
3) Preview the xml. 'error to' tag is not present in the workflow.
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-02-08 11:19:50,78
13040949,Workflow Manager Should be able to handle fork with single path.,Workflow Manager Should be able to handle fork with single path while importing,WFD,['ambari-views'],AMBARI,Bug,Major,2017-02-07 11:36:58,79
13039778,Publish asset has issues when different users logins,"Login to user1.
Publish asset.
logout
login to user2.
Not able to publish asset
Expected.
Different users should be to publish assets.
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-02-02 09:17:13,80
13039756,Custom Action should be created without prompting for the action type,User is prompted to enter an action type before creating a custom action. It restricts user from changing the custom action type while creating the action. The user should be provided with a flexibility to create a custom action of any type while creating the action.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-02-02 06:40:44,78
13039463,Styling changes and spelling fixes ,Spelling fixes given by Beverley and minor coolor changes based on inputs from Priyanka,WFD,['ambari-views'],AMBARI,Bug,Major,2017-02-01 13:35:49,80
13039413,'num_llap_nodes' should show up as non-editable property when non-ambari managed queue is selected,"
- 'num_llap_nodes' as of now is shown as a slider and comes only when we have selected 'llap' named queue and is a child of root level. It is shown because 'num_llap_nodes' visible attribute is set as True by SA.
- For any other queue, 'num_llap_nodes' is set as visible=false by SA call and is not shown on UI.

*Expected:*
A way to show 'num_llap_nodes' node on UI for user, irrespective of the queue selected.",llap,['ambari-web'],AMBARI,Bug,Critical,2017-02-01 09:42:37,84
13038828,Job XML  tag is not coming as part of FS action node,"Job XML tag is not coming as part of FS action node.
Steps to reproduce :
1) Create a workflow.
2) Add FS action node
3) Add the settings for FS action node
4) Preview the xml.
Job xml is not coming as part of the preview xml",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-30 07:07:50,80
13038652,Inconsistent auth-to-local rules processing during Kerberos authentication,"Facing issue with local to auth rules. 
ambari-qa-cl1@EXAMPLE.COM is getting converted to ambari-qa-cl1 as well as ambari-qa with same ambari configuration ie authentication.kerberos.auth_to_local.rules=DEFAULT.

1st translation : 
{code}
28 Jan 2017 11:44:45,529  INFO [ambari-client-thread-3298] AmbariAuthToLocalUserDetailsService:102 - Translated ambari-qa-cl1@EXAMPLE.COM to ambari-qa-cl1 using auth-to-local rules during Kerberos authentication.
{code}

2nd translation :
{code}
28 Jan 2017 11:47:36,425  INFO [ambari-client-thread-3172] AmbariAuthToLocalUserDetailsService:102 - Translated ambari-qa-cl1@EXAMPLE.COM to ambari-qa using auth-to-local rules during Kerberos authentication.
28 Jan 2017 11:47:36,428  WARN [ambari-client-thread-3172] AmbariAuthToLocalUserDetailsService:136 - Failed find user account for user with username of ambari-qa during Kerberos authentication.
28
{code}

Since authentication.kerberos.auth_to_local.rules=DEFAULT ,  'ambari-qa-cl1@EXAMPLE.COM' should have been translated to 'ambari-qa-cl1'.
",authentication kerberos,['ambari-server'],AMBARI,Bug,Major,2017-01-28 16:10:46,30
13038641,Not able to set the sla for an action node,"User is not able to set the sla for an action node.
When user fills the sla info and tries to save, error message 'This field can't be blank' is shown corresponding to 'should end' field. This happen even when user has entered the value.
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-28 13:07:29,78
13038424,User preference like opened tabs/work in progress should not be available to a different user,"Login to user1.
create workspace by creating a workflow.
logout
login to user2.
the workspace of user1 is restored.

Expected.
each user should maintain their own preference while working in same browser.",wfd,['ambari-views'],AMBARI,Bug,Major,2017-01-27 12:08:59,45
13037670,XML generated for bundle's is not as per oozie xsd,"XML generated for bundle's is not as per oozie xsd. 

The tag to provide kick off time is not proper.
<control>
		<kick-off-time>2017-01-25T17:16Z</kick-off-time>
	</control>
As per XSD it should be 'controls'.



",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-25 11:52:42,78
13037021,Asset Manager import and validation issues,"Asset is not able to import properly from HDFS.
Asset does not check for duplicate name check.
Minor UX issues like slow loading exist.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-23 11:47:10,80
13036997,Trailing slash (/) on cluster resource causes incorrect authorization logic flow,"Trailing slash (/) on cluster resource causes incorrect authorization logic flow. It is debatable whether Ambari should allow this, but since it seems to in other cases - like if the user was an Ambari Administrator - this should be fixed. 

The problem occurs in the {{org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter}} where the filter attempts to figure out what the user is trying to get access to.  Since the regular expression for Cluster resources does acknowledge that a trailing ""/"" after the cluster name indicates a cluster, the request does not fall through to the Cluster resource handler ({{org.apache.ambari.server.controller.internal.ClusterResourceProvider}}) for authorization checks.  It uses the legacy logic, which is a little flawed as well.

The fix for this is to allow the trailing ""/"" in the regular expression representing Cluster requests:
{code:title=From org/apache/ambari/server/security/authorization/AmbariAuthorizationFilter.java:70}
  private static final String API_CLUSTERS_PATTERN = API_VERSION_PREFIX + ""/clusters/(\\w+)?"";
{code}

{code:title=To org/apache/ambari/server/security/authorization/AmbariAuthorizationFilter.java:70}
  private static final String API_CLUSTERS_PATTERN = API_VERSION_PREFIX + ""/clusters/(\\w+/?)?"";
{code}
",rbac,['ambari-server'],AMBARI,Bug,Major,2017-01-23 10:10:00,30
13036981,Not able to add the settings for action nodes in firefox browser,"User is not able to add the settings for action nodes  in firefox browser as action is failing with below javascript error :

{code}
TypeError: Object.values is not a function
{code} ",WFD,['ambari-views'],AMBARI,Bug,Critical,2017-01-23 09:13:08,78
13036531,Issue while submitting workflow as its not able to register as a project,Issue while submitting workflow as its not able to register as a project.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-20 11:43:58,80
13036321,Ldap sync fails when there are special characters in distinguished names,"Ldap sync fails when there are special characters in distinguished names. 

For example if there was a user with the distinguished name of {{OU=test/test,OU=users,DC=EXAMPLE,DC=COM}} and that user was a member of a synced group, then the lookup of the user using the membership attribute in the group would fail due to the special character.  

The error would look something like
{noformat}
REASON: Caught exception running LDAP sync. Uncategorized exception occured during LDAP processing; nested exception is javax.naming.NamingException: [LDAP: error code 1 - 000020D6: SvcErr: DSID-031007DB, problem 5012 (DIR_ERROR), data 0
]; remaining name 'OU=test/test,OU=users,DC=EXAMPLE,DC=COM'
{noformat}

*Solution*
Update the library versionf for Spring LDAP 
* {{org.springframework.security/spring-security-ldap}} to {{4.0.4.RELEASE}}
* {{org.springframework.ldap/spring-ldap-core}} to {{2.0.4.RELEASE}}

Then use {{LdapUtils.newLdapName}} to convert a String representing a DN into a {{javax.naming.ldap.LdapName}} and use that object in the search facility executed in {{org.apache.ambari.server.security.ldap.AmbariLdapDataPopulator#getFilteredLdapUsers(java.lang.String, org.springframework.ldap.filter.Filter)}}. 
",ldap,['ambari-server'],AMBARI,Bug,Critical,2017-01-19 20:03:18,30
13036150,Missing fields for workflow SLA,notification-msg and upstream-apps fields are missing for sla entity. Need to add this for workflow and coordinator.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-19 09:22:08,78
13036106,Need abilities to add a custom action node and import a workflow xml with custom action,"Oozie can be extended to support additional action types by writing a custom Action Node. Need ability to add a custom action node through the Workflow designer. Also, designer should allow users to import a workflow xml with custom actions.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-19 06:25:10,78
13035847,User is not able to import the draft version for a coordinator,"User is not able to import the draft version for a coordinator. 

Steps to reproduce :
1) Create a coordinator and save it.
2) Now in a new window, try to import the draft version of the save coordinator.
3) Action is failing with below error :
{code}
{""message"":""Access Error to file due to access control"",""status"":""error.file.access.control"",""stackTrace"":""org.apache.hadoop.security.AccessControlException: Permission denied: user\u003dadmin, access\u003dEXECUTE, inode\u003d\""/tmp/coordinator.wfdraft/coordinator.xml\"":admin:hdfs:-rw-r--r--\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:390)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$600(WebHdfsFileSystem.java:90)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.shouldRetry(WebHdfsFileSystem.java:661)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:627)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:463)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:492)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:488)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener.connect(WebHdfsFileSystem.java:1217)\n\tat org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:121)\n\tat org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:105)\n\tat org.apache.hadoop.hdfs.web.ByteRangeInputStream.\u003cinit\u003e(ByteRangeInputStream.java:90)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream.\u003cinit\u003e(WebHdfsFileSystem.java:1274)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.open(WebHdfsFileSystem.java:1188)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.ambari.view.utils.hdfs.HdfsApi$14.run(HdfsApi.java:324)\n\tat org.apache.ambari.view.utils.hdfs.HdfsApi$14.run(HdfsApi.java:322)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.ambari.view.utils.hdfs.HdfsApi.execute(HdfsApi.java:397)\n\tat org.apache.ambari.view.utils.hdfs.HdfsApi.open(HdfsApi.java:322)\n\tat org.apache.oozie.ambari.view.HDFSFileUtils.read(HDFSFileUtils.java:57)\n\tat org.apache.oozie.ambari.view.WorkflowFilesService.readWorkflowXml(WorkflowFilesService.java:54)\n\tat org.apache.oozie.ambari.view.OozieProxyImpersonator.readWorkflowXxml(OozieProxyImpersonator.java:393)\n\tat sun.reflect.GeneratedMethodAccessor536.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:287)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)\n\tat org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilter(AmbariBasicAuthenticationFilter.java:129)\n\tat org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:120)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.apache.ambari.server.view.AmbariViewsMDCLoggingFilter.doFilter(AmbariViewsMDCLoggingFilter.java:54)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.apache.ambari.server.view.ViewThrottleFilter.doFilter(ViewThrottleFilter.java:161)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:150)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user\u003dadmin, access\u003dEXECUTE, inode\u003d\""/tmp/coordinator.wfdraft/coordinator.xml\"":admin:hdfs:-rw-r--r--\n\tat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:112)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:358)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:534)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:610)\n\t... 121 more\n""}
{code}",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-18 10:37:47,78
13035830,User is not able to set the start and end date for a coordinator,"User is not able to set the start and end date for a coordinator.
Steps to reproduce :
1) Open the coordinator.
2) Switch the start date type to expression.
3) Switch back start date type to Date.
4) Select the date using date picker. UI is throwing the error ""This field must be in the format of MM/DD/YYYY hh:mm A""
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-18 09:52:25,78
13035797,Search button is not functional in the dashboard,"'Search button' is not functional in the dashboard.
Steps to reproduce :
1) Go to dashboard.
2) Add search filter (Job id, name etc)
3) Click on search icon. Action is not filtering the results.
Instead if user directly hits enter, then result gets filtered",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-18 07:28:18,78
13035795,Need to perform sanity checks before starting the oozie view,"Need to implement sanity checks before starting the oozie view.
As of I can think of below checks. 
1)HDFS service check
2) Oozie service check
3) User home directory check. ie /user/<job submitter> folder should be created and have the right permission on that folder",WFD,['ambari-views'],AMBARI,Improvement,Major,2017-01-18 07:25:01,78
13035794,Issue with delete action node operation,"Deleting decision node is deleting every other node proceeding it.
Steps to reproduce :
1) Import the the workflow with decision node
2) Delete the decision node.
3) See that even the end node is getting deleted.
Options",WFD,['ambari-views'],AMBARI,Bug,Critical,2017-01-18 07:23:42,79
13035790,Input logic tags are missing in the coordinator xml,"'Input-logic' tags are missing in the coordinator xml.

Steps to reproduce :
1) Import the attached xml.
2) Preview the xml.
3) Check that 'input-logic' tags which are available in original xml are missing in the preview xml.
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-18 07:05:18,78
13035520,Issue with asset manager modal window,"Asset manager modal window gets closed immediately once user deletes an asset.
Steps :
1) Click on asset manager.
2) Delete an asset.
3) See that asset manager window is closed.
Ideally its better to ask for confirmation from user before actually deleting the asset and once deletion is successful, display the confirmation message.
And close the asset manager window only when user clicks on 'cancel' or close button.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-17 12:24:30,80
13035502,Need to show proper error message in scenarios of failures with asset import,"Need to show proper error message in scenarios of failures with asset import.
While importing the asset for a given action node, its better to show only assets belonging to that particular type.
Steps to reproduce :
1) Save hive node settings as an asset.
2) Now try to import that setting for hive2 or node other than hive.
3) user is shown with the message 'Invalid asset settings'. 
Either user should be shown only the assets of that particular type or appropriate error message should be displayed as why import setting has failed.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-17 11:45:07,80
13035463,Organize tabs in Job details page,The tabs should be reordered in job details page in such a way that flow grap come first and static values like definition and configuration goes to the end.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-17 10:38:59,78
13035178,Dashboard-Show YARN RM URL in actions and flow graph and organize tabs,"Console Url is available which takes to RM UI. But not very intuitive. 
-We can add link in action column in actions row.
-Also flow graph can be moved after actions column 
-Also in flow graph, show RM url",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-16 10:23:40,78
13035168,Save is not present for coordinator and bundle.,"Save should be available for coordinator and bundle.
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-16 09:36:08,78
13034078,Need to handle unsupported configurations gracefully in workflow actions.,"Scenario - During import Workflow.
If any of the action contains configuration that the workflow designer does not understand(eg. streaming in map-reduce), the action editor should warn the users and show a text area for manually editing the configuration
The configuration should be taken into consideration while the final workflow xml is generated.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-12 05:55:40,78
13034051,Disable the Livy for the spark2 over the HDP-2.5 from [AMBARI-19248]  ,"Spark2 Client Install fails on HDP-2.5:
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/spark_client.py"", line 59, in <module>
    SparkClient().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 288, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/spark_client.py"", line 35, in install
    self.configure(env)
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/spark_client.py"", line 38, in configure
    import params
  File ""/var/lib/ambari-agent/cache/common-services/SPARK2/2.0.0/package/scripts/params.py"", line 200, in <module>
    livy2_pid_dir = status_params.livy2_pid_dir
AttributeError: 'module' object has no attribute 'livy2_pid_dir'",patch,['ambari-server'],AMBARI,Bug,Blocker,2017-01-12 01:45:22,71
13033609,Label changes and missing fields for action nodes,"Need below label changes for below action nodes :
1) Spark :
a) 'Job Tracker' should be changed to 'Resource Manager'
b) Change the 'Name' to 'Application Name'
c) 'Mode' field which is part of the xsd is missing.
2) Java :
a) Java opts to 'java Options'
b) Provide hidden label for 'Java opt' input field. (Required for automation)
3) Email :
a) 'Content Type' and 'Attachment' fields missing in email node
4) SSH
a) Provide label for 'args' radio group.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-11 06:36:18,80
13033605,Dashboard should display appropriate message if workflow data fetching api fails,Dashboard should display appropriate message if workflow data fetching api fails. Currently dashboard shows empty table without any error message. This provides the user wrong impression that no workflow is being run. Instead its better to show error message explicitly.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-11 06:16:01,80
13033329,Need ability to upload and download action asset from HDFS,"Need ability to upload asset to a HDFS location and download later. 
a) This helps in using assets across views. 
b) Also users may upload the saved asset into git for others to use.
This is based on inputs from Artem Ervits.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-10 10:43:52,80
13033328,Warning message when user tries to close a designer tab.,"User should be warned when designer tab is closed accidentally.
",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-10 10:43:16,78
13033286,Need ability to upload a file in HDFS browser.,Need ability to upload a file in HDFS browser.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-10 06:41:34,78
13033176,Use common property for principal name prefix to help with customization of unique principal names,"Use common property for principal name prefix to help with customization of unique principal names.  

All _headless_ Kerberos identities have a non-unique principal name (across clusters). To help this issue, the cluster name is appended to these principal names by adding ""-$\{cluster-name|toLower()\}"" after the principal name component. If the user wants to change this convention, they will need to find all _headless_ principals and make the change. On top of that, when adding new components, they will need to remember to make the change to new _headless_ principal names. 

A better solution is to provide a _global_ property named ""principal_suffix"" and use that in each _headless_ principal name. By default the value for this property will be

{code}
principal_suffix=""-${cluster_name|toLower()}""
{code}

If the user would like not use a prefix (in the event there is only a single cluster connecting to the KDC), the value can be changed to

{code}
principal_suffix=""""
{code}

Finally if the user would like to use some other randomizer, they can set the value to something else. For example

{code}
principal_suffix=""_12345""
{code}

The property is set in the Kerberos descriptor's ""properties"" block.   For example:

{code}
{
  ""properties"": {
    ""realm"": ""${kerberos-env/realm}"",
    ...,
    ""principal_suffix"": ""${cluster_name|toLower()}""
  },
  ""identities"": [
    ..., 
    {
      ""name"": ""smokeuser"",
      ""principal"": {
        ""value"": ""${cluster-env/smokeuser}-${principal_suffix}@${realm}"",
        ""type"": ""user"",
        ""configuration"": ""cluster-env/smokeuser_principal_name"",
        ""local_username"": ""${cluster-env/smokeuser}""
      },
      ...
    }
  ],
  ""services"": [
    {
{code} ",kerberos kerberos_descriptor,[],AMBARI,Bug,Major,2017-01-09 20:57:38,30
13033009,Import from local file system doesn't work if the same file is selected again.,"Import a workflow by using local file system. Do a reset layout. Import again the same file. No effect is seen.
Expected:
The file should be able to import again.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-09 09:00:29,78
13032744,Fix typos in the code,Fix some typos in the code while exploring the option of using git pull as the review mechanism.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-01-07 01:11:19,48
13032482,Namenode not resolved while submitting workflow in certain environments,"Namenode is not resovled while creating configuration in job.properties while submitting workflow in certain environments.
Step.
if the hdfs-site does not have dfs.namenode.rpc-address property workflow submission fails.
Expected.
certain environments may not have dfs.namenode.rpc-address set, and in those environments workflow should be submitted without error",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-06 07:13:10,78
13032299,Authentication negotiation HTTP response should be sent when Kerberos authentication is enabled,"Authentication negotiation HTTP response should be automatically sent when needed when Kerberos authentication is enabled.  

The expected HTTP response during authentication failure when Kerberos authentication into Ambari is enabled is as follows:
{noformat}
HTTP/1.1 401 Authentication requested
WWW-Authenticate: Negotiate
{noformat}

When Kerberos authentication into Ambari is not enabled the expected HTTP response for an authentication failure is:
{noformat}
HTTP/1.1 403 Missing authentication token
{noformat}

",authentication kerberos security,['ambari-server'],AMBARI,Bug,Major,2017-01-05 16:37:08,30
13032234,Resolve variables in workflow if workflow path in coordinator contains variables.,"Resolve variables in workflow ,if workflow path in coordinator contains variables..
Steps:
while creating coordinator use variables while configuring the workflow path. User is not prompted for the variables in workflow.

Expected.
the variables in workflow should be prompted for user to submit.

Also need to add custom properties that the user can configure while submitting workflow/coordinator/bundle.


",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-05 12:01:14,78
13032210,Workflow Draft Save-Not working,"Workflow should be able to save as draft even if it has errors. The configuration should be stored as .draft.json in the proper location. 
On retrieving, user should be able to load the draft on choosing the draft.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-05 09:40:41,80
13032033,Not able to save FS action node after populating job xml value,"Not able to save FS action node after populating job xml value. When user tries to save the action node settings, UI is throwing below error :

{code}
oozie-designer-b34005a….js:2 Uncaught TypeError: Cannot read property 'pushObject' of undefined
    at n.addFile (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-b34005a588d596541889745a2e518025.js:2:24866)
    at n.bindInputPlaceholder (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-b34005a588d596541889745a2e518025.js:2:24794)
    at n.trigger (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-5ac0512ea25105f32859ade26812f863.js:16:24070)
    at n [as trigger] (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-5ac0512ea25105f32859ade26812f863.js:11:18800)
    at http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-b34005a588d596541889745a2e518025.js:6:727
    at Map.forEach (native)
    at n.processMultivaluedComponents (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-b34005a588d596541889745a2e518025.js:6:683)
    at n.save (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/oozie-designer-b34005a588d596541889745a2e518025.js:6:1177)
    at n.send (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-5ac0512ea25105f32859ade26812f863.js:15:25559)
    at n [as send] (http://172.27.21.64:8080/views/WORKFLOW_MANAGER/1.0.0/WFD/assets/vendor-5ac0512ea25105f32859ade26812f863.js:11:18800)
{code}",WFD,['ambari-views'],AMBARI,Bug,Blocker,2017-01-04 17:38:58,45
13032028,Executing ambari-server unit tests with JDK 1.8 results in unit test failures,"Following unit tests fails when executed with JDK 1.8
{code}
mvn test -Dtest=DataStoreImplTest
mvn clean test -Dtest=UpgradeCatalog222Test
mvn test -Dtest=KerberosCheckerTest,AmbariBasicAuthenticationFilterTest
{code}
Same unit tests passes if the environment is switched to use JDK 1.7
",jdk1_8 unit-test,['ambari-server'],AMBARI,Bug,Critical,2017-01-04 17:33:32,30
13031883,Invalid message is shown after saving the workflow,"Invalid message is shown after saving the workflow. The message ""Transition deleted Undo"" is shown even when no changes are made to the workflow.
Steps to reproduce :
1) Import a workflow.
2) Click on one of the action nodes and open settings.
3) Without making any change, click on 'Save'
After saving, message ""Transition deleted Undo"" is shown to the user.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-04 07:20:31,78
13031881,Designer should always pick latest version for an action node,"Designer should always pick latest version for an action node and the actin node settings should be specific to each workflow tab.
But as of now, designer is picking up old settings even for a new tab.
Steps to reproduce:
1) Import a workflow with hive node and hive node version as 0.2.
2) Open a new tab and add a hive node to the new workflow.
3) Preview the xml for new workflow. Even the new workflow has hive version as 0.2. Ideally it should be the latest version 0.6.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-04 07:07:10,78
13031719,HOU Fails To Restart NameNode in non-HA Cluster,"*Steps*
# Deploy HDP-2.5.0.0 with Ambari-2.5.0.0-547 build (non-HA cluster)
# Start Host Ordered Upgrade to HDP-2.5.3
 
*Result:* Error at pre Upgrade HDFS step
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/ru_execute_tasks.py"", line 156, in <module>
    ExecuteUpgradeTasks().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 287, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/ru_execute_tasks.py"", line 153, in actionexecute
    shell.checked_call(task.command, logoutput=True, quiet=True)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'source /var/lib/ambari-agent/ambari-env.sh ; /usr/bin/ambari-python-wrap /var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py prepare_rolling_upgrade /var/lib/ambari-agent/data/command-111.json /var/lib/ambari-agent/cache/custom_actions /var/lib/ambari-agent/data/structured-out-111.json INFO /var/lib/ambari-agent/tmp' returned 1. 2017-01-03 17:07:47,095 - In the middle of a stack upgrade/downgrade for Stack HDP and destination version 2.5.3.0-37, determining which hadoop conf dir to use.
2017-01-03 17:07:47,096 - Hadoop conf dir: /usr/hdp/2.5.3.0-37/hadoop/conf
2017-01-03 17:07:47,096 - The hadoop conf dir /usr/hdp/2.5.3.0-37/hadoop/conf exists, will call conf-select on it for version 2.5.3.0-37
2017-01-03 17:07:47,096 - Checking if need to create versioned conf dir /etc/hadoop/2.5.3.0-37/0
2017-01-03 17:07:47,097 - call[('ambari-python-wrap', '/usr/bin/conf-select', 'create-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.3.0-37', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}
2017-01-03 17:07:47,141 - call returned (1, '/etc/hadoop/2.5.3.0-37/0 exist already', '')
2017-01-03 17:07:47,141 - checked_call[('ambari-python-wrap', '/usr/bin/conf-select', 'set-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.3.0-37', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}
2017-01-03 17:07:47,179 - checked_call returned (0, '')
2017-01-03 17:07:47,180 - Ensuring that hadoop has the correct symlink structure
2017-01-03 17:07:47,180 - Using hadoop conf dir: /usr/hdp/2.5.3.0-37/hadoop/conf
2017-01-03 17:07:47,182 - Stack Feature Version Info: stack_version=2.5, version=2.5.3.0-37, current_cluster_version=2.5.0.0-1245, upgrade_direction=upgrade -> 2.5.3.0-37
2017-01-03 17:07:47,185 - In the middle of a stack upgrade/downgrade for Stack HDP and destination version 2.5.3.0-37, determining which hadoop conf dir to use.
2017-01-03 17:07:47,185 - Hadoop conf dir: /usr/hdp/2.5.3.0-37/hadoop/conf
2017-01-03 17:07:47,186 - The hadoop conf dir /usr/hdp/2.5.3.0-37/hadoop/conf exists, will call conf-select on it for version 2.5.3.0-37
2017-01-03 17:07:47,186 - Checking if need to create versioned conf dir /etc/hadoop/2.5.3.0-37/0
2017-01-03 17:07:47,187 - call[('ambari-python-wrap', '/usr/bin/conf-select', 'create-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.3.0-37', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}
2017-01-03 17:07:47,224 - call returned (1, '/etc/hadoop/2.5.3.0-37/0 exist already', '')
2017-01-03 17:07:47,225 - checked_call[('ambari-python-wrap', '/usr/bin/conf-select', 'set-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.3.0-37', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}
2017-01-03 17:07:47,262 - checked_call returned (0, '')
2017-01-03 17:07:47,263 - Ensuring that hadoop has the correct symlink structure
2017-01-03 17:07:47,263 - Using hadoop conf dir: /usr/hdp/2.5.3.0-37/hadoop/conf
2017-01-03 17:07:47,271 - checked_call['rpm -q --queryformat '%{version}-%{release}' hdp-select | sed -e 's/\.el[0-9]//g''] {'stderr': -1}
2017-01-03 17:07:47,382 - checked_call returned (0, '2.5.3.0-37', '')
2017-01-03 17:07:47,385 - Performing a(n) upgrade of HDFS
2017-01-03 17:07:47,385 - Execute['/usr/hdp/current/hadoop-hdfs-namenode/bin/hdfs dfsadmin -fs hdfs://vs-hucluster2-5.openstacklocal:8020 -rollingUpgrade prepare'] {'logoutput': True, 'user': 'cstm-hdfs'}
######## Hortonworks #############
This is MOTD message, added for testing in qe infra
PREPARE rolling upgrade ...
17/01/03 17:07:51 WARN retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.rollingUpgrade over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): Safe mode should be turned ON in order to create namespace image.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:7839)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:7799)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1273)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:808)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)
	at org.apache.hadoop.ipc.Client.call(Client.java:1496)
	at org.apache.hadoop.ipc.Client.call(Client.java:1396)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy16.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rollingUpgrade(ClientNamenodeProtocolTranslatorPB.java:773)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)
	at com.sun.proxy.$Proxy17.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.rollingUpgrade(DFSClient.java:2999)
	at org.apache.hadoop.hdfs.DistributedFileSystem.rollingUpgrade(DistributedFileSystem.java:1394)
	at org.apache.hadoop.hdfs.tools.DFSAdmin$RollingUpgradeCommand.run(DFSAdmin.java:375)
	at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:1932)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2107)
rollingUpgrade: Safe mode should be turned ON in order to create namespace image.
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 420, in <module>
    NameNode().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 287, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 180, in prepare_rolling_upgrade
    namenode_upgrade.prepare_rolling_upgrade(hfds_binary)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode_upgrade.py"", line 244, in prepare_rolling_upgrade
    logoutput=True)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/hdp/current/hadoop-hdfs-namenode/bin/hdfs dfsadmin -fs hdfs://vs-hucluster2-5.openstacklocal:8020 -rollingUpgrade prepare' returned 255. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
PREPARE rolling upgrade ...
17/01/03 17:07:51 WARN retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.rollingUpgrade over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): Safe mode should be turned ON in order to create namespace image.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:7839)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:7799)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1273)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:808)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)
	at org.apache.hadoop.ipc.Client.call(Client.java:1496)
	at org.apache.hadoop.ipc.Client.call(Client.java:1396)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy16.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rollingUpgrade(ClientNamenodeProtocolTranslatorPB.java:773)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)
	at com.sun.proxy.$Proxy17.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.rollingUpgrade(DFSClient.java:2999)
	at org.apache.hadoop.hdfs.DistributedFileSystem.rollingUpgrade(DistributedFileSystem.java:1394)
	at org.apache.hadoop.hdfs.tools.DFSAdmin$RollingUpgradeCommand.run(DFSAdmin.java:375)
	at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:1932)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2107)
rollingUpgrade: Safe mode should be turned ON in order to create namespace image.
{code}",host_ordered_upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2017-01-03 18:21:00,31
13031603,YARN proxyusers properties has spelling mistake,"- We see that there are quite a lot of files in Ambari installation which uses YARN proxyuser properties with spelling mistake.

*Example:*
{code}
            ""yarn.timeline-service.http-authentication.proxyusers.*.hosts"": """"
            ""yarn.timeline-service.http-authentication.proxyusers.*.users"": """"
            ""yarn.timeline-service.http-authentication.proxyusers.*.groups"": """"
            ""yarn.resourcemanager.proxyusers.*.groups"": """"
            ""yarn.resourcemanager.proxyusers.*.hosts"": """"
            ""yarn.resourcemanager.proxyusers.*.users"": """"
{code}

- Ideally all the *""proxyusers.""* should be replaced with *""proxyuser.""*, Not sure why it did not affect so far as these changes are there for quite long time.   Even though those might be ignored from YARN side.

- Some old ambari docs shows the right property:   https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_ambari_views_guide/content/section_kerberos_setup_tez_view.html
",patch-available,['ambari-server'],AMBARI,Bug,Major,2017-01-03 08:18:50,22
13031589,User should be prompted to enter valid workflow xml file name while Submit/Validate,"User should be prompted to enter valid workflow xml file name while Submit/Validate

As of now user can provide any file name. Also user is not restricted even if he provides a directory structure.

For example if user provides xml file as /tmp/testPath, a file 'testPath' is getting created under directory /tmp/
This file has permission : '-rw-r--r--'

Due to this restricted permission, below issues are faced :
Issue 1 : User is not able to import the saved workflow.
Issue 2 : From the dashboard, if user selects any instance of the workflow and decides to edit the workflow, user is shown with below error :
{code}
There is some problem while importing.Please try again. Hide Log
org.apache.hadoop.security.AccessControlException: Permission denied: user=admin, access=EXECUTE, inode=""/oozieex/mr-ex1/workflow.xml"":admin:hadoop:-rw-r--r--

    at sun.reflect.GeneratedConstructorAccessor263.newInstance(Unknown Source)

    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)

    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)

    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)

    at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:390)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$600(WebHdfsFileSystem.java:90)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.shouldRetry(WebHdfsFileSystem.java:661)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:627)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:463)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:492)

    at java.security.AccessController.doPrivileged(Native Method)

    at javax.security.auth.Subject.doAs(Subject.java:422)

    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:488)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener.connect(WebHdfsFileSystem.java:1217)

    at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:121)

    at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:105)

    at org.apache.hadoop.hdfs.web.ByteRangeInputStream.(ByteRangeInputStream.java:90)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream.(WebHdfsFileSystem.java:1274)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.open(WebHdfsFileSystem.java:1188)

    at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)

    at org.apache.ambari.view.utils.hdfs.HdfsApi$14.run(HdfsApi.java:324)

    at org.apache.ambari.view.utils.hdfs.HdfsApi$14.run(HdfsApi.java:322)

    at java.security.AccessController.doPrivileged(Native Method)

    at javax.security.auth.Subject.doAs(Subject.java:422)

    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

    at org.apache.ambari.view.utils.hdfs.HdfsApi.execute(HdfsApi.java:397)

    at org.apache.ambari.view.utils.hdfs.HdfsApi.open(HdfsApi.java:322)

    at org.apache.oozie.ambari.view.HDFSFileUtils.read(HDFSFileUtils.java:57)

    at org.apache.oozie.ambari.view.WorkflowFilesService.readWorkflowXml(WorkflowFilesService.java:54)

    at org.apache.oozie.ambari.view.OozieProxyImpersonator.readWorkflowXxml(OozieProxyImpersonator.java:400)

    at sun.reflect.GeneratedMethodAccessor373.invoke(Unknown Source)

    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

    at java.lang.reflect.Method.invoke(Method.java:498)

    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)

    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)

    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)

    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)

    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)

    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)

    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)

    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)

    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)

    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)

    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)

    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)

    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)

    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)

    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:286)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:132)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)

    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)

    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)

    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)

    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.apache.ambari.server.view.AmbariViewsMDCLoggingFilter.doFilter(AmbariViewsMDCLoggingFilter.java:54)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.apache.ambari.server.view.ViewThrottleFilter.doFilter(ViewThrottleFilter.java:161)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)

    at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)

    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)

    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)

    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)

    at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)

    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)

    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)

    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)

    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)

    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)

    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)

    at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)

    at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)

    at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)

    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)

    at org.eclipse.jetty.server.Server.handle(Server.java:370)

    at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)

    at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)

    at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)

    at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)

    at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)

    at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)

    at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)

    at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)

    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)

    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)

    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=EXECUTE, inode=""/oozieex/mr-ex1/workflow.xml"":admin:hadoop:-rw-r--r--

    at org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:112)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:358)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:534)

    at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:610)
	... 119 more

{code}
 

",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-03 06:52:40,80
13031507,Need to support permission in string format for FS action node,"Need to add support for FS action nodes for identifying permission in string format. Currently only number format is supported.

If user imports a workflow with action node where permission is in string format, then permission is not set in the workflow designer view.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-02 14:09:34,45
13031506,Need to highlight XML tags in preview xml viewer,Need to highlight XML tags in preview xml viewer. As of now only xml structure is maintained. Highlighting individual tags would provide better insight for the preview xml.,WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-02 13:58:17,45
13031488,Issue with FS action node in oozie workflow designer,"Issue 1: Not able to save the settings for FS action node
Steps to reproduce:
1) Create a new workflow.
2) Add FS action node.
3) User is not able to edit the settings for the node
Issue 2: UI is hung after importing the workflow and while trying to edit FS action node
Attaching the workflow.
Steps to reproduce:
1) Import the attached workflow.
2) Try to edit the settings for FS node. 'clean up' node in the workflow.",WFD,['ambari-views'],AMBARI,Bug,Major,2017-01-02 11:42:08,45
13031100,Zeppelin stop failed during EU while deleting pid file with customized service user,"*Steps*
# Deploy HDP-2.5.0.0 cluster with Ambari-2.5.0.0 (customized service users enabled for all services including Zeppelin)
# Start Express Upgrade to 2.6.0.0-267

*Result:*
Observed error while stopping Zeppelin notebook
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/ZEPPELIN/0.6.0.2.5/package/scripts/master.py"", line 361, in <module>
    Master().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 287, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/ZEPPELIN/0.6.0.2.5/package/scripts/master.py"", line 159, in stop
    user=params.zeppelin_user)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/hdp/current/zeppelin-server/bin/zeppelin-daemon.sh stop >> /grid/0/log/zeppelin/zeppelin-setup.log' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
rm: cannot remove ‘/var/run/zeppelin/zeppelin-cstm-zeppelin-ctr-e85-1482808692054-0723-01-000003.hwx.site.pid’: Permission denied
rm: cannot remove ‘/var/run/zeppelin/zeppelin-cstm-zeppelin-ctr-e85-1482808692054-0723-01-000003.hwx.site.pid’: Permission denied
{code}
",express_upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-12-29 14:05:26,85
13030825,User is not able to validate/submit the workflow after browsing workflow path,"User is not able to validate/submit the workflow after browsing workflow path.
Steps to reproduce :
1) Import the workflow. (attaching the workflow)
2) Click on submit.
3) Browse the 'workflow path' using file browser.
4) After selecting the workflow path, UI scrolling is not functional and footer elements become non accessible.",WFD,['ambari-views'],AMBARI,Bug,Critical,2016-12-28 05:53:37,80
13030362,Asset Manager/Asset Upload/Asset import from DB,"Manage Assets.
-User should be able to list his assets.
-users should be able to delete his asset.
Export Asset.
-user should be able to select action node and export by giving name and description.
Import Asset
-User should be able to browse all assets and select one to be imported either on transition or on the current action node.
",WFD,['ambari-views'],AMBARI,Bug,Major,2016-12-23 11:39:49,80
13030353,User should be shown proper error message in file browser page,"User should be shown proper error message when user tries to navigate to a hdfs folder and he doesn't have access on that folder.
Steps to reproduce :
1) Try to import workflow from HDFS .
2) In the file browser, click on a folder on which user doesn't have permission
3) Though api /api/v1/views/Workflow%20Manager/versions/1.0.0/instances/WFD/resources/proxy/fileServices/fileops/listdir?path=%2Ftmp%2Fhive fails with appropriate error, UI keeps on loading.
UI should handle this error and show appropriate message to the user.",WFD,['ambari-views'],AMBARI,Bug,Major,2016-12-23 10:45:24,80
13029445,Asset support Rest API,Add rest API support for storing and retrieving assets from ambari view db.,WFD,['ambari-views'],AMBARI,Bug,Major,2016-12-20 13:13:09,79
13029425,Prompting for underlying workflow job properties in a coordinator and coordinator job properties in a bundle during job submission ,"The job properties of underlying workflow should be submitted when a coordinator is submitted. Similiarly, the job properties of underlying coordinators should be submitted while submitting a bundle job.",WFD,['ambari-views'],AMBARI,Bug,Major,2016-12-20 12:04:56,78
13029116,ATS reports as down in Ambari UI after upgrade,"Steps
1. Deploy HDP-2.5.0 cluster with Ambari-2.4.1.0
2. Upgrade Ambari to 2.5.0.0-419
3. Restart ATS and observe the status of ATS after sometime in Ambari UI

Result:
ATS shows down. Ambari-agent log shows below:
{code}
INFO 2016-12-08 16:14:44,419 ActionQueue.py:105 - Adding STATUS_COMMAND for component APP_TIMELINE_SERVER of service YARN of cluster cl1 to the queue.
INFO 2016-12-08 16:14:44,621 ActionQueue.py:105 - Adding STATUS_COMMAND for component NODEMANAGER of service YARN of cluster cl1 to the queue.
INFO 2016-12-08 16:14:46,429 PythonReflectiveExecutor.py:65 - Reflective command failed with exception:
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ambari_agent/PythonReflectiveExecutor.py"", line 57, in run_file
    imp.load_source('__main__', script)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 155, in <module>
    ApplicationTimelineServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 82, in status
    only_if = format(""test -e {yarn_historyserver_pid_file_old}"", user=status_params.yarn_user))
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 143, in run
    Logger.info_resource(resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 87, in info_resource
    Logger.info(Logger.filter_text(Logger._get_resource_repr(resource)))
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 110, in _get_resource_repr
    return Logger.get_function_repr(repr(resource), resource.arguments, resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 180, in get_function_repr
    return unicode(""{0} {{{1}}}"", 'UTF-8').format(name, arguments_str)
  File ""/usr/lib64/python2.6/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
AttributeError: 'NoneType' object has no attribute 'utf_8_decode'
INFO 2016-12-08 16:14:53,190 Controller.py:283 - Heartbeat (response id = 15365) with server is running...
INFO 2016-12-08 16:14:55,083 Heartbeat.py:90 - Adding host info/state to heartbeat message.
{code}
",upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-12-19 11:42:28,5
13028641,Preview xml feature is not functional across different tabs,"Preview xml feature is not functional across different tabs.UI hangs when user tries to view preview workflow from two different tabs.
Steps to reproduce :
1) Create a workflow and check preview is happening correctly.
2) Open new tab and create another workflow
3) UI hangs when user tries to preview.",WFD,['ambari-views'],AMBARI,Bug,Major,2016-12-16 10:25:44,78
13027933,Add permission for Service Auto Start,"Add permission to give a role the ability to set services to auto-start. This permission should be allowed at the cluster-level to toggle the feature for the cluster (Manage Service Auto Start Configuration) and at the service-level to toggle the feature for specific services (Manage Service Auto Start). However the service-level feature may not be available via all interfaces. 

The following roles should be able to toggle auto-start at the cluster level:
* Ambari Administrator
* Cluster Administrator
* Cluster Operator

The following roles should be able to toggle auto-start at the service level:
* Ambari Administrator
* Cluster Administrator
* Cluster Operator
* Service Administrator",rbac,['ambari-server'],AMBARI,Task,Major,2016-12-13 22:59:48,30
13027431,'Backup Oozie DB' message is masked when FixOozieAdminUsers code is invoked in EU wizard,"EU path from HDP-2.3.6.0 to 2.6.0.0 with Ambari-2.5.0.0

In upgrade pack:

There are two tasks:
a. Fix oozie admin users
b. Manual prompt to 'Backup Oozie Database'
When a.) gets invoked then the manual prompt message to backup Oozie DB gets suppressed. As a result user is not informed to backup the Oozie DB before the actual upgrade
Please see attached screenshots",express_upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-12-12 12:07:46,1
13027429,Configuring YARN with custom queues leads to misleading errors in Stack Advisor,"Configure custom (non-default) YARN queues as per this link (http://hortonworks.com/hadoop-tutorial/configuring-yarn-capacity-scheduler-ambari/)

Result: Stack Advisor gives errors related to deletion of 'default' queue (check screenshots - yarn1, yarn2) for various service configs like YARN, Hive, Tez, MR, Spark
",ui,['ambari-web'],AMBARI,Bug,Critical,2016-12-12 12:00:47,1
13027427,NPE during Ambari server schema upgrade,"Ambari server upgrade path: 2.2.2.0 to 2.5.0.0-453
Error seen while running 'ambari-server upgrade' command""
{code}
Using python  /usr/bin/python
Upgrading ambari-server
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Upgrade Ambari Server
INFO: Updating Ambari Server properties in ambari.properties ...
INFO: Updating Ambari Server properties in ambari-env.sh ...
WARNING: Original file ambari-env.sh kept
WARNING: Original file krb5JAASLogin.conf kept
INFO: File krb5JAASLogin.conf updated.
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: No mpack replay logs found. Skipping replaying mpack commands
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Fixing database objects owner
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Ambari Server configured for Oracle. Confirm you have made a backup of the Ambari Server database [y/n] (y)? INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: Upgrading database schema
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: AMBARI_SERVER_LIB is not set, using default /usr/lib/ambari-server
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: about to run command: /usr/jdk64/jdk1.7.0_67/bin/java -cp '/etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/share/java/ojdbc6.jar' org.apache.ambari.server.upgrade.SchemaUpgradeHelper > /var/log/ambari-server/ambari-server.out 2>&1
INFO: Return code from schema upgrade command, retcode = 1
ERROR: Error executing schema upgrade, please check the server logs.
ERROR: Error output from schema upgrade command:
ERROR: Exception in thread ""main"" org.apache.ambari.server.AmbariException
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:209)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:424)
Caused by: java.lang.NullPointerException
	at com.google.inject.persist.jpa.JpaPersistService.begin(JpaPersistService.java:70)
	at com.google.inject.persist.jpa.AmbariJpaPersistService.begin(AmbariJpaPersistService.java:29)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:106)
	at org.apache.ambari.server.upgrade.UpgradeCatalog240.executeDDLUpdates(UpgradeCatalog240.java:294)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:938)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:206)
	... 1 more


ERROR: Ambari server upgrade failed. Please look at /var/log/ambari-server/ambari-server.log, for more details.
ERROR: Exiting with exit code 11. 
REASON: Schema upgrade failed.
{code}",upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-12-12 11:56:58,21
13026451,hadoop.proxyuser.HTTP.hosts should not be updated when Hive is installed unless WebHcat is installed,"{{hadoop.proxyuser.HTTP.hosts}} should not be updated when Hive is installed unless WebHcat is installed.

This is happening because the following block in the Kerberos descriptor is at the HIVE service level rather than the WEBHCAT_SERVER component level.

{code}
        {
          ""core-site"": {
            ""hadoop.proxyuser.HTTP.hosts"": ""${clusterHostInfo/webhcat_server_host}""
          }
        },
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2016-12-07 19:46:45,30
13026442,"When disabling Kerberos, rm should be used when deleting the Ambari Server keytab file(s)","When disabling Kerberos, {{rm}} should be used when deleting the Ambari Server keytab file(s) rather than the Java {{java.io.File#delete}} method. This is to allow for the file to be removed properly when Ambari is not executed as _root_.  

Currently the Ambari server keytab files may not be removed due to permission issues. 
",kerberos sudo,['ambari-server'],AMBARI,Bug,Major,2016-12-07 19:10:14,21
13024634,Fix NPE in UpgradeCatalog250Test.testExecuteDMLUpdates,"Fix NPE in UpgradeCatalog250Test.testExecuteDMLUpdates

{noformat}
Running org.apache.ambari.server.upgrade.UpgradeCatalog250Test
Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.14 sec <<< FAILURE! - in org.apache.ambari.server.upgrade.UpgradeCatalog250Test
testExecuteDMLUpdates(org.apache.ambari.server.upgrade.UpgradeCatalog250Test)  Time elapsed: 0.272 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog250Test.testExecuteDMLUpdates(UpgradeCatalog250Test.java:234)
{noformat}
",unit-test,['ambari-server'],AMBARI,Bug,Blocker,2016-12-01 01:23:02,30
13024410,Atlas web UI inaccessible alert after enabling Namenode-HA,"After enabling Namenode-HA, atlas had a critical alert that ""This host-level alert is triggered if the Metadata Server Web UI is unreachable.""

From atlas logs I can see : 
{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.atlas.ApplicationProperties.getClass(ApplicationProperties.java:115)
        at org.apache.atlas.util.AtlasRepositoryConfiguration.getDeleteHandlerImpl(AtlasRepositoryConfiguration.java:76)
        at org.apache.atlas.RepositoryMetadataModule.configure(RepositoryMetadataModule.java:104)
        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)
        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)
        at com.google.inject.spi.Elements.getElements(Elements.java:110)
        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)
        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)
        at com.google.inject.Guice.createInjector(Guice.java:99)
        at com.google.inject.Guice.createInjector(Guice.java:84)
        at org.apache.atlas.web.listeners.GuiceServletConfig.getInjector(GuiceServletConfig.java:77)
        at com.google.inject.servlet.GuiceServletContextListener.contextInitialized(GuiceServletContextListener.java:47)
        at org.apache.atlas.web.listeners.GuiceServletConfig.contextInitialized(GuiceServletConfig.java:141)
        at org.eclipse.jetty.server.handler.ContextHandler.callContextInitialized(ContextHandler.java:800)
        at org.eclipse.jetty.servlet.ServletContextHandler.callContextInitialized(ServletContextHandler.java:444)
        at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:791)
        at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:294)
        at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1349)
        at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1342)
        at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:741)
        at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:505)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)
        at org.eclipse.jetty.server.Server.start(Server.java:387)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)
        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
        at org.eclipse.jetty.server.Server.doStart(Server.java:354)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.apache.atlas.web.service.EmbeddedServer.start(EmbeddedServer.java:92)
        at org.apache.atlas.Atlas.main(Atlas.java:118)
{code}",system_test,['ambari-server'],AMBARI,Bug,Critical,2016-11-30 12:10:08,35
13024081,Spark service check fails during EU due to Livy server not running,"Steps
1. Install HDP-2.5.0.0 with Ambari-2.4.1.0
2. Upgrade Ambari to 2.5.0.0
3. Register HDP-2.6.0.0 and install the bits
4. Start Express Upgrade

Result:
Error during Spark service as follow:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/SPARK/1.2.1/package/scripts/service_check.py"", line 62, in <module>
    SparkServiceCheck().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/SPARK/1.2.1/package/scripts/service_check.py"", line 59, in service_check
    raise Fail(format(""Connection to all Livy servers failed""))
resource_management.core.exceptions.Fail: Connection to all Livy servers failed
{code}",express_upgrade system_test,['ambari-upgrade'],AMBARI,Bug,Blocker,2016-11-29 13:53:35,86
13024078,RegionServer Restart During Upgrade Fails Because of Missing Import When Formatting PID,"Steps
1. Install HDP-2.5.0.0 with Ambari-2.4.1.0
2. Upgrade Ambari to 2.5.0.0
3. Register HDP-2.6.0.0 and install the bits
4. Start Express Upgrade

Result:
Observed error at 'Restart HBase Region Servers' task:
Traceback (most recent call last):
{code}
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 166, in <module>
    HbaseRegionServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 728, in restart
    self.post_upgrade_restart(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 84, in post_upgrade_restart
    upgrade.post_regionserver(env)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/upgrade.py"", line 48, in post_regionserver
    is_regionserver_registered(exec_cmd, params.hbase_user, params.hostname, re.IGNORECASE)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/decorator.py"", line 55, in wrapper
    return function(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/upgrade.py"", line 77, in is_regionserver_registered
    raise Fail(""RegionServer process is not running"")
resource_management.core.exceptions.Fail: RegionServer process is not running
{code}",express_upgrade system_test,['ambari-upgrade'],AMBARI,Bug,Blocker,2016-11-29 13:42:27,31
13021846,NPE when authenticating via a Centrify LDAP proxy,"When authenticating using LDAP where the LDAP server is a Centrify LDAP proxy, a {{NullPointerException}} is being thrown due to unexpected missing LDAP user object attributes. 

{noformat}
10 Nov 2016 08:23:38,789 ERROR [ambari-client-thread-25] AmbariLdapBindAuthenticator:95 - Caught exception
java.lang.NullPointerException
	at org.apache.ambari.server.security.authorization.AmbariLdapBindAuthenticator.authenticate(AmbariLdapBindAuthenticator.java:83)
	at org.springframework.security.ldap.authentication.LdapAuthenticationProvider.doAuthentication(LdapAuthenticationProvider.java:178)
	at org.springframework.security.ldap.authentication.AbstractLdapAuthenticationProvider.authenticate(AbstractLdapAuthenticationProvider.java:61)
	at org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProvider.authenticate(AmbariLdapAuthenticationProvider.java:73)
	at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:156)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:168)
	at org.apache.ambari.server.security.authentication.AmbariAuthenticationFilter.doFilter(AmbariAuthenticationFilter.java:88)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

*Cause*
The cause for this {{NPE}} is related to missing data from the attribute search request made to a Centrify LDAP proxy after a bind has occurred.  Since the query filter at this point is ""{(objectClass=*}}"", the Centrify LDAP proxy does not have enough data to determine what information to return to the caller.  However, the filter was something like ""{(objectClass=posixAccount}}"", it will be able to build a set of attributes to return to the caller since it would determine that the call wants data for a specific _profile_.

This can be seen by the following {{tcpdump}} entry:
{code}
LDAPMessage searchRequest(2) ""uid=user1,ou=people,ou=dev,dc=apache,dc=org"" baseObject
    messageID: 2
    protocolOp: searchRequest (3)
        searchRequest
            baseObject: uid=user1,ou=people,ou=dev,dc=apache,dc=org
            scope: baseObject (0)
            derefAliases: derefAlways (3)
            sizeLimit: 0
            timeLimit: 0
            typesOnly: False
            Filter: (objectClass=*)
            attributes: 0 items
    [Response In: 2]
    controls: 1 item
{code}

Note the filter line above: *{{Filter: (objectClass=*)}}*

From the Centrify LDAP proxy logs, the following lines can be seen showing that no mapping is avaialbe:

{noformat}
Nov  8 12:13:45 host1 slapd: cdc search start with filterstr: (objectClass=*)
Nov  8 12:13:45 host1 slapd: cdc search: objectType =  ( is mapped to NONE)
Nov  8 12:13:45 host1 slapd: cdc search after translation filter = (objectClass=*)
{noformat}

This search filter is hardcoded in {{com.sun.jndi.ldap.LdapCtx}} as seen in http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/com/sun/jndi/ldap/LdapCtx.java#1308.

This code is invoked from the Spring LDAP library after attempting to authenticate using the Spring org.springframework.security.ldap.authentication.BindAuthenticator class. 

*Solution*
To solve this, Ambari should avoid using {{org.springframework.security.ldap.authentication.BindAuthenticator}} to authenticate users via LDAP so that more control can be had over when and how user attributes are obtained. 
",ldap,['ambari-server'],AMBARI,Bug,Major,2016-11-18 20:38:33,30
13021401,Zeppelin view doesn't work with JDK 1.8_91+,"At ambari main page, click on the box icon next to the Admin tab on top right corner. Then click Zeppelin View. It will give the following error.


{code}
HTTP ERROR 500
Problem accessing /views/ZEPPELIN/1.0.0/AUTO_ZEPPELIN_INSTANCE/. Reason: 
Server Error
Caused by:
org.apache.jasper.JasperException: PWC6033: Unable to compile class for JSP
PWC6199: Generated servlet error:
The type java.io.ObjectInputStream cannot be resolved. It is indirectly referenced from required .class files
at org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:123)
at org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:296)
at org.apache.jasper.compiler.Compiler.generateClass(Compiler.java:376)
at org.apache.jasper.compiler.Compiler.compile(Compiler.java:437)
at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:608)
at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:360)
at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:486)
at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:380)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:575)
at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
at org.eclipse.jetty.server.Dispatcher.forward(Dispatcher.java:276)
at org.eclipse.jetty.server.Dispatcher.forward(Dispatcher.java:103)
at org.apache.ambari.view.zeppelin.ZeppelinServlet.doGet(ZeppelinServlet.java:68)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:286)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)
at org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilter(AmbariBasicAuthenticationFilter.java:129)
at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:120)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:200)
at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
at org.eclipse.jetty.server.Server.handle(Server.java:370)
at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)
at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)
at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)
at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)
at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
at java.lang.Thread.run(Thread.java:745)
Powered by Jetty://

{code}",ambari-views,['ambari-views'],AMBARI,Bug,Major,2016-11-17 09:23:00,66
13018986,Optionally force username from LDAP authentication data to be lowercase in Ambari,"Optionally force username from LDAP authentication data to be lowercase in Ambari based on LDAP import configuration.

In some cases the username declared in the relevant LDAP object is in all uppercase characters when the local Hadoop cluster expects the username to be all lowercase. As of Ambari 2.4.0, the username specified from the LDAP data is used to override the username known to Ambari.  This overwritten data may be in all uppercase characters, potentially breaking local username conventions. 

To help this scenario, provide a configuration option to force the username obtained from the LDAP object to be converted to all lowercase character. 

For example {{authentication.ldap.username.forceLowercase}}.

This optional configuration value is to default to false to maintain current functionality. 

",ldap,['ambari-server'],AMBARI,Bug,Major,2016-11-07 20:45:18,30
13014073,Metric level hadoop metrics2 filter is incompatible with AMS HadoopTimelineMetricsSink.,"{code}
2016-10-19 14:46:41,647 INFO  [HBase-Metrics2-1] timeline.HadoopTimelineMetricsSink: Collector Uri: http://apappu-hdp-3.hdp.com:6188/ws/v1/timeline/metrics
2016-10-19 14:46:41,653 INFO  [HBase-Metrics2-1] impl.MetricsSinkAdapter: Sink timeline started
2016-10-19 14:46:41,655 INFO  [HBase-Metrics2-1] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-10-19 14:46:41,655 INFO  [HBase-Metrics2-1] impl.MetricsSystemImpl: HBase metrics system started
java.lang.ClassCastException: org.apache.hadoop.metrics2.impl.MetricsRecordFiltered$1 cannot be cast to java.util.Collection
        at org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink.putMetrics(HadoopTimelineMetricsSink.java:258)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:186)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)
        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:134)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)
2016-10-18 13:39:42,896 ERROR [timeline] impl.MetricsSinkAdapter: Got sink exception and over retry limit, suppressing further error messages
java.lang.ClassCastException: org.apache.hadoop.metrics2.impl.MetricsRecordFiltered$1 cannot be cast to java.util.Collection
        at org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink.putMetrics(HadoopTimelineMetricsSink.java:258)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:186)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)
        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:134)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)
Tue Oct 18 13:41:53 PDT 2016 Terminating regionserver
{code}",ambari-metrics,['ambari-metrics'],AMBARI,Bug,Critical,2016-10-20 22:04:01,39
13011057,Ambari should be able to create arbitrary Kerberos identities for itself as declared in the Kerberos Descriptor,"Ambari should be able to create arbitrary Kerberos identities for itself as declared in the Kerberos Descriptor.

Currently, Ambari is hard-coded to create identities for itself and SPNEGO, but that may not be good enough for all scenarios. Therefore, there needs to be an {{AMBARI}} service block in the Kerberos descriptor to allow for arbitrary identities to be defined for the Ambari server - similar to how any other service  is defined in the Kerberos descriptor. 

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2016-10-10 14:36:02,30
13010152,Kerberos server actions should not timeout in minutes as specified in configuration,"Kerberos related server-side actions should not time out in minutes as specified in configuration.  Some Kerberos-related task can potentially take a much longer time based on number of hosts and components installed in the cluster. 

The {{Create Principals}} and {{Create Keytab Files}} stages need to be set to a rather large timeout value such that

{noformat}
if  server.task.timeout < X 
  timeout = X;
else 
  timeout =  server.task.timeout

Where X is set to something like 10 hours. 
{noformat}

10 hours seems to be a reasonable timeout value since it is not possible to specified an unlimited amount of time give Ambari's current task processing infrastructure. 

",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-10-06 13:29:01,30
13008777,Logging of ambari agent scheduling activities in debug mode,"We find lots of stale alerts in Ambari. On digging deeper from the Ambari-agent logs it is found that there were many services that missed the triggering window, but no insight as to why it is missed. The aim of this is to add some debug logs on two steps primarily:
1)When the scheduler iterates through the jobstore and decides it is time to schedule the job.
2)The threadpool, being forked and executing the callback function",ambari-agent,['ambari-agent'],AMBARI,Bug,Minor,2016-09-30 01:01:04,87
13008447,configs.sh spelling mistake fix,"The ""/var/lib/ambari-server/resources/scripts/configs.sh"" shows incorrect spelling of ""PUTting""  instead of ""putting""

Example:
{code}
# /var/lib/ambari-server/resources/scripts/configs.sh set erie1.example.com ErieCluster cluster-env fetch_nonlocal_groups false 
########## Performing 'set' fetch_nonlocal_groups:false on (Site:cluster-env, Tag:TOPOLOGY_RESOLVED)
########## Config found. Skipping origin value
########## PUTting json into: doSet_version1475127716251871184.json
########## NEW Site:cluster-env, Tag:version1475127716251871184
{code}",patch-available,['ambari-server'],AMBARI,Bug,Major,2016-09-29 05:54:28,22
13007441,Regression: krb5JAASLogin.conf is not updated during secure BP install,"When installing a secure cluster using Blueprints, Ambari's {{/etc/ambari-server/conf/krb5JAASLogin.conf}} is not updated to reflect the details of the Ambari Kerberos identity.

This was introduced by the patch for AMBARI-18406.
",blueprints kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-09-25 13:32:13,30
13007136,Refactor Unnecessary In-Memory Locks Around Business Objects,"The top 4 business objects in Ambari:
- ClusterImpl
- ServiceImpl
- ServiceComponentImpl
- ServiceComponentHostImpl

All use {{ReadWriteLock}} implementations to prevent dirty reads and concurrent writes. However, {{ClusterImpl}} exposes a ""global"" {{ReadWriteLock}} which the other business objects share. This causes tremendous problems with deadlocks, especially on slow databases.

Consider the case where you have 3 threads:

# thread-1 acquires {{ClusterReadLock}}
# thread-2 acquires {{ServiceComponenWriteLock}}
# thread-3 tries to get {{ClusterWriteLock}} and is blocked by {{thread-1}}
# thread-2 tries to get {{ClusterReadLock}} and is blocked by {{thread-3}}
# thread-1 tries to get {{ServiceComponentReadLock}} and is blocked by {{thread-2}}

Essentially, the exposure of the ""cluster global lock"" causes problems since multiple threads can acquire other internal locks and be blocked waiting on the global lock.

In general, I don't believe that the read locks help at all. Ambari usually encounters these locks while try to display web page information. Once displayed, the locks are removed and the information is already stale if there were write threads waiting.

These locks should be investigated and, for the most part, except in some cases involving concurrent writes, removed.

Part of the problem revolves around our assumption about how the ReadWriteLock works. The issue in the above scenario is that the clusterWriteLock request is pending. This actually blocks all subsequent readers even though the lock is not fair.

FYI, this code shows that a reader, in unfair mode, will wait when there is a waiting writer:

{noformat:title=Output}
Waiting for a read lock...
Read lock acquired!
Waiting for a write lock...
Trying to acquire a second read lock...
{noformat}

{code}
import java.util.concurrent.locks.ReentrantReadWriteLock;

public class Test {
  private static ReentrantReadWriteLock lock = new ReentrantReadWriteLock(false);

  public static void main(String[] args) throws InterruptedException {
    // A reader which takes too long to finish
    new Thread() {
      @Override
      public void run() {
        System.out.println(""Waiting for a read lock..."");
        lock.readLock().lock();
        System.out.println(""Read lock acquired!"");
        try {
          try {
            Thread.sleep(1000 * 60 * 60);
          } catch (InterruptedException e) {
          }
        } finally {
          lock.readLock().unlock();
        }
      }
    }.start();

    Thread.sleep(3000);

    // A writer which will be waiting
    new Thread() {
      @Override
      public void run() {
        System.out.println(""Waiting for a write lock..."");
        lock.writeLock().lock();
        System.out.println(""Write lock acquired!"");
        lock.writeLock().unlock();
      }
    }.start();

    Thread.sleep(3000);

    // Another reader
    new Thread() {
      @Override
      public void run() {
        System.out.println(""Trying to acquire a second read lock..."");
        lock.readLock().lock();
        try {
          System.out.println(""Second read lock acquired successfully!!"");
        } finally {
          lock.readLock().unlock();
        }
      }
    }.start();
  }
}
{code}",branch-feature-AMBARI-18456,['ambari-server'],AMBARI,Epic,Major,2016-09-23 12:17:55,31
13006812,upgrade ambari to 2.1.1 error,"ambari upgrade to 2.1.1, if the first time failed , excute ambari-server upgrade again  will error duplicate key value violates unique constraint ""pk_hostcomponentstate""
",ambari-server,['ambari-server'],AMBARI,Bug,Major,2016-09-22 10:46:19,70
13006547,Enforce granular role-based access control for custom actions,"Enforce granular role-based access control for custom actions.  Such actions are specified in {{/var/lib/ambari-server/resources/custom_action_definitions/system_action_definitions.xml}} 

For example:

{code}
  <actionDefinition>
    <actionName>check_host</actionName>
    <actionType>SYSTEM</actionType>
    <inputs/>
    <targetService/>
    <targetComponent/>
    <defaultTimeout>60</defaultTimeout>
    <description>General check for host</description>
    <targetType>ANY</targetType>
    <permissions>HOST.ADD_DELETE_HOSTS</permissions>
  </actionDefinition>
{code}

The ""permissions"" element that declare the permissions required to run the action.  These permissions must be used to authorize a user to perform the operation.  A user needs to have one of the listed permissions in order to be authorized. 

The relevant API entry points are:
* {{/api/v1/requests}}
* {{/api/v1/requests/clusters/:CLUSTER_NAME/request}}

Example:  The user executing the following REST API call must be assigned a role that has the {{HOST.ADD_DELETE_HOSTS}} authorization for the relevant cluster

{noformat}
POST /api/v1/requests
{
  ""RequestInfo"": {
    ""action"": ""check_host"",
    ""log_output"": ""false"",
    ""context"": ""Check host"",
    ""parameters"": {
      ""check_execute_list"": ""last_agent_env_check,installed_packages,existing_repos,transparentHugePage"",
      ""jdk_location"": ""http://host1.example.com:8080/resources/"",
      ""threshold"": ""20""
    }
  },
  ""Requests/resource_filters"": [
    {
      ""hosts"": ""host1.example.com""
    }
  ]
}
{noformat}

",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-09-21 15:51:13,30
13006337,Support PAM as an authentication option for Ranger in Ambari,"Ranger-842 has added PAM support for ranger, we need to add this part to ambari, to do automatic setup for ranger to use PAM authentication.",security,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2016-09-20 23:49:15,88
13005232,Create authentication filter to perform Kerberos authentication for Ambari,"Users should be able to authenticate to use Ambari by providing a Kerberos token using SPNEGO - Simple and Protected GSSAPI Negotiation Mechanism.  This includes access to the Ambari REST API as well as the Ambari web-based UI. 

The implementation should support the ability to perform the full SPNEGO handshake as well as access requests directly providing the appropriate HTTP header containing the Kerberos token. For example:

{noformat}
Authorization: Negotiate YIICcgY...r/vJcLO
{noformat}

In the full handshake model
# The client requests access to a web resource
# The server responds with an HTTP 401 status ({{Unauthorized}}), including the header {{WWW-Authenticate: Negotiate}}
# The client generates the Kerberos data and creates a new request containing the authentication header - {{Authorization: Negotiate YIICcgY...r/vJcLO}}

Since Ambari needs to generally return a HTTP status of 403 ({{Forbidden}}) when authentication is needed, a _hint_ must be sent along with the request indicate to Ambari that Kerberos authentication is desired.  If this _hint_ is received, then Ambari will respond with the appropriate status and header to initiate SPNEGO with the client. This _hint_ is an Ambari-specific header named ""X-Negotiate-Authentication"" with the value of ""true"":

{noformat}
X-Negotiate-Authentication: true
{noformat}

No matter what the handshake mechanism is (or lack of), once the Kerberos token is received by Ambari, Ambari is to parse and validate the token.  If a failure occurs, Ambari is to respond with the appropriate HTTP status and related header(s).  Upon success, the user's principal name is retrieved and converted into a _local_ user name.  The use of an auth-to-local rule set processor may be needed to perform this translation.  Using this _local_ username, an appropriate Ambari user account is located and used as the authenticated users identity - details, privileges, etc.... Failure to find an appropriate Ambari user account is to result in an authentication failure response.

*To enable this feature*, the following properties may be set in {{ambari.properties}}:

{{authentication.kerberos.enabled}} - Determines whether to use Kerberos (SPNEGO) authentication when connecting Ambari.
* {{true}} - enables this feature
* {{false}} - disables this feature (default)

{{authentication.kerberos.spnego.principal}} - The Kerberos principal name to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO.
* HTTP/_HOST  (default)

authentication.kerberos.spnego.keytab.file - The Kerberos keytab file to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO.
*  /etc/security/keytabs/spnego.service.keytab (default)

{{authentication.kerberos.user.types}} - A comma-delimited (ordered) list of preferred user types to use when finding the Ambari user account for the user-supplied Kerberos identity during authentication via SPNEGO.
* LDAP (default)

{{authentication.kerberos.auth_to_local.rules}} - The auth-to-local rules set to use when translating a user's principal name to a local user name during authentication via SPNEGO.
* DEFAULT (default)
* Rules are to be separated by ""/n"":
{noformat}
authentication.kerberos.auth_to_local.rules=RULE:[1:$1@$0](ambari-server-c1@EXAMPLE.COM)s/.*/admin/\nDEFAULT
{noformat}



",authentication kerberos security,['ambari-server'],AMBARI,Task,Major,2016-09-15 18:11:15,30
13004653,View instance cloning functionality,"This task is for providing an option to clone / copy a view instance. 

A new ""copy"" icon will appear next to a view instance. Clicking on this opens up the existing view with instance name and display name of the view appended with _Copy.
This will be useful when creating multiple view instances with mild configuration differences between them.",patch,"['ambari-admin', 'ambari-views']",AMBARI,Improvement,Minor,2016-09-13 16:34:42,75
13004572,rolling restart datanode cluster name show  null,"when rolling restart services ( eg:DataNode ) ,cluster name in audit log show null

2016-09-13T17:29:22.033+0800, User(admin), RemoteIp(127.0.0.1), Operation(Request from server), RequestType(POST), url(http://localhost:8080/api/v1/clusters/amabri/requests), ResultStatus(202 Accepted), Command(RESTART), Cluster name(null)

2016-09-13T17:29:22.042+0800, User(admin), Operation(_PARSE_.ROLLING-RESTART.DATANODE.1.1), Status(IN_PROGRESS), RequestId(15)",ambari-server,['ambari-server'],AMBARI,Improvement,Major,2016-09-13 12:09:50,70
13004430,Create authentication filter to encapsulate the various Ambari authentication methods,"Create a Spring authentication filter to encapsulate the various Ambari authentication methods since the Spring filter chain allows for a single authentication filter and Ambari needs to allow for multiple, optional, authentication filters to handle one of (but not limited to) the following authentication methods:

* Basic Auth 
* SSO (JWT)
* Kerberos token

",authentication kerberos security,['ambari-server'],AMBARI,Task,Major,2016-09-12 21:35:02,30
13004382,Add Ambari configuration options to support Kerberos token authentication,"Add the followng Ambari configuration options to support Kerberos token authentication

* {{authentication.kerberos.enabled}}
** Determines whether to use Kerberos (SPNEGO) authentication when connecting Ambari:  {{true}} to enable this feature; {{false}}, otherwise
* {{authentication.kerberos.spnego.principal}}
** The Kerberos principal name to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO
* {{authentication.kerberos.spnego.keytab.file}}
** The Kerberos keytab file to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO
* {{authentication.kerberos.user.types}}
** A comma-delimited (ordered) list of preferred user types to use when finding the Ambari user account for the user-supplied Kerberos identity during authentication via SPNEGO
* {{authentication.kerberos.auth_to_local.rules}}
** The auth-to-local rules set to use when translating a user's principal name to a local user name during authentication via SPNEGO.

NOTE: These properties are in the {{ambari.properties}} file since this feature may be enabled whether the rest of the cluster has Kerberos enabled or not. 
",authentication kerberos security,['ambari-server'],AMBARI,Task,Major,2016-09-12 18:29:20,30
13004380,Ambari authentication with Kerberos token,"Users should be able to authenticate to use Ambari by providing a Kerberos token using SPNEGO - Simple and Protected GSSAPI Negotiation Mechanism.  This includes access to the Ambari REST API as well as the Ambari web-based UI. 

The implementation should support the ability to perform the full SPNEGO handshake as well as access requests directly providing the appropriate HTTP header containing the Kerberos token. For example:

{noformat}
Authorization: Negotiate YIICcgY...r/vJcLO
{noformat}

In the full handshake model
# The client requests access to a web resource
# The server responds with an HTTP 401 status ({{Unauthorized}}), including the header {{WWW-Authenticate: Negotiate}}
# The client generates the Kerberos data and creates a new request containing the authentication header - {{Authorization: Negotiate YIICcgY...r/vJcLO}}

Since Ambari needs to generally return a HTTP status of 403 ({{Forbidden}}) when authentication is needed, a _hint_ must be sent along with the request indicate to Ambari that Kerberos authentication is desired.  If this _hint_ is received, then Ambari will respond with the appropriate status and header to initiate SPNEGO with the client. This _hint_ is an Ambari-specific header named ""X-Negotiate-Authentication"" with the value of ""true"":

{noformat}
X-Negotiate-Authentication: true
{noformat}

No matter what the handshake mechanism is (or lack of), once the Kerberos token is received by Ambari, Ambari is to parse and validate the token.  If a failure occurs, Ambari is to respond with the appropriate HTTP status and related header(s).  Upon success, the user's principal name is retrieved and converted into a _local_ user name.  The use of an auth-to-local rule set processor may be needed to perform this translation.  Using this _local_ username, an appropriate Ambari user account is located and used as the authenticated users identity - details, privileges, etc.... Failure to find an appropriate Ambari user account is to result in an authentication failure response.




",authentication kerberos security,['ambari-server'],AMBARI,Epic,Major,2016-09-12 18:27:20,30
13004379,Ambari authentication with Kerberos token,"Users should be able to authenticate to use Ambari by providing a Kerberos token using SPNEGO - Simple and Protected GSSAPI Negotiation Mechanism.  This includes access to the Ambari REST API as well as the Ambari web-based UI. 

The implementation should support the ability to perform the full SPNEGO handshake as well as access requests directly providing the appropriate HTTP header containing the Kerberos token. For example:

{noformat}
Authorization: Negotiate YIICcgY...r/vJcLO
{noformat}

In the full handshake model
# The client requests access to a web resource
# The server responds with an HTTP 401 status ({{Unauthorized}}), including the header {{WWW-Authenticate: Negotiate}}
# The client generates the Kerberos data and creates a new request containing the authentication header - {{Authorization: Negotiate YIICcgY...r/vJcLO}}

Since Ambari needs to generally return a HTTP status of 403 ({{Forbidden}}) when authentication is needed, a _hint_ must be sent along with the request indicate to Ambari that Kerberos authentication is desired.  If this _hint_ is received, then Ambari will respond with the appropriate status and header to initiate SPNEGO with the client. This _hint_ is an Ambari-specific header named ""X-Authentication-Type"" with the value of ""kerberos"":

{noformat}
X-Authentication-Type: kerberos
{noformat}

No matter what the handshake mechanism is (or lack of), once the Kerberos token is received by Ambari, Ambari is to parse and validate the token.  If a failure occurs, Ambari is to respond with the appropriate HTTP status and related header(s).  Upon success, the user's principal name is retrieved and converted into a _local_ user name.  The use of an auth-to-local rule set processor may be needed to perform this translation.  Using this _local_ username, an appropriate Ambari user account is located and used as the authenticated users identity - details, privileges, etc.... Failure to find an appropriate Ambari user account is to result in an authentication failure response.




",authentication kerberos security,['ambari-server'],AMBARI,Epic,Major,2016-09-12 18:27:10,30
13004321,"All classes recompiled due to Maven bug, even if none changed","maven-compiler-plugin version 3.0 has a [bug|https://issues.apache.org/jira/browse/MCOMPILER-187] that causes all classes to be recompiled even if no classes have changed.

{noformat}
[INFO] --- maven-compiler-plugin:3.0:compile (default-compile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1720 source files ...
...
[INFO] --- maven-compiler-plugin:3.0:testCompile (default-testCompile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 760 source files ...
{noformat}

version 3.1+ has [another, related bug|https://issues.apache.org/jira/browse/MCOMPILER-209] that causes all classes to be compiled even if only one has changed, although it seems to correctly detect the ""no change"" case.

{noformat}
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1720 source files ...
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2016-09-12 15:27:33,21
13003348,After upgrading cluster from HDP-2.4.x to HDP-2.5.x and added atlas service - missing kafka security properties,"Steps to repro:
* Install Ambari 2.2.2
* Install HDP-2.4.x cluster with Atlas
* Stop Atlas
* Upgrade Ambari to 2.4
* Delete Atlas service
* Upgrade the cluster to HDP-2.5.x cluster
* Add Atlas service.

*Below config properties are missing from atlas-applicataion.properties file for Atlas, Storm, Falcon, Hive services.*
#atlas.jaas.KafkaClient.option.keyTab = /etc/security/keytabs/atlas.service.keytab
#atlas.jaas.KafkaClient.option.principal = atlas/_HOST@EXAMPLE.COM

From HDP 2.4 to 2.5, the kerberos.json file for Atlas changed.
",kerberos_descriptor upgrade,['ambari-server'],AMBARI,Bug,Blocker,2016-09-07 18:34:31,30
13001559,Ambari PigView stores the script in HDFS in non UTF-8 format,"- In Ambari PigView if we have a script like following containing some special characters like 'á'

{code}
STOCK_A = LOAD '/user/admin/dummy.csv' USING PigStorage('á'); 
DESCRIBE STOCK_A;
{code}

Then after executing the script we see that in the HDFS it is stored as following:
{code}
hdfs dfs -cat /user/admin/pig/scripts/testlatin-2016-08-30_12-15.pig
STOCK_A = LOAD '/user/admin/dummy.csv' USING PigStorage('?'); 
DESCRIBE STOCK_A;
{code}

*Notice:*  the 'á' is converted to '?'",patch-available,['ambari-views'],AMBARI,Bug,Major,2016-08-31 04:48:51,22
12999545,Pig View Caching issue causes File does not exist: /user//pig/jobs/job_id/stdout and stderr,"Sometimes intermittently while runnign a script (like   pwd; ) from Pig View it fails with the following error. Even though the script gets executed successfully and Even if that file exist in the HDFS.

{code}
ERROR qtp-ambari-client-27 ServiceFormattedException:92 - File /user/admin/pig/jobs/test_31-07-2016-23-18-52/stderr not found.
ERROR qtp-ambari-client-27 ServiceFormattedException:95 - java.io.FileNotFoundException: File /user/admin/pig/jobs/test_31-07-2016-23-18-52/stderr not found. 
{code}

After disabling the browser cache the issue did not occur. ",patch,['ambari-views'],AMBARI,Bug,Major,2016-08-24 08:06:51,22
12998712,Oozie server start fails while enabling wire encryption with Atlas,"Oozie start fails with the following after wire encryption is enabled.
{code}
2016-08-20 17:16:11,341 - Execute['cd /var/tmp/oozie && /usr/hdp/current/oozie-server/bin/oozie-start.sh'] {'environment': {'OOZIE_CONFIG': '/usr/hdp/current/oozie-server/conf'}, 'not_if': ""ambari-sudo.sh su oozie -l -s /bin/bash -c 'ls /var/run/oozie/oozie.pid >/dev/null 2>&1 && ps -p `cat /var/run/oozie/oozie.pid` >/dev/null 2>&1'"", 'user': 'oozie'}
2016-08-20 17:16:15,494 - Found 3 files/directories inside Atlas Hive hook directory /usr/hdp/2.5.0.0-1234/atlas/hook/hive/
2016-08-20 17:16:15,701 - call['source /usr/hdp/current/oozie-server/conf/oozie-env.sh ; oozie admin -shareliblist hive | grep ""\[Available ShareLib\]"" -A 5'] {'logoutput': True, 'tries': 10, 'user': 'oozie', 'try_sleep': 5}
Error: IO_ERROR : java.io.IOException: Error while connecting Oozie server. No of retries = 1. Exception = Could not authenticate, Authentication failed, URL: http://nat-s11-4-bjps-stackdeploy-4.openstacklocal:11000/oozie/versions?user.name=oozie, status: 302, message: Found
2016-08-20 17:16:34,257 - Retrying after 5 seconds. Reason: Execution of 'source /usr/hdp/current/oozie-server/conf/oozie-env.sh ; oozie admin -shareliblist hive | grep ""\[Available ShareLib\]"" -A 5' returned 1. Error: IO_ERROR : java.io.IOException: Error while connecting Oozie server. No of retries = 1. Exception = Could not authenticate, Authentication failed, URL: http://nat-s11-4-bjps-stackdeploy-4.openstacklocal:11000/oozie/versions?user.name=oozie, status: 302, message: Found
{code}

Looks like the oozie URL used is still pointing to the http instead of https.

This is result of call {{oozie admin -shareliblist hive}} where it defaults to http url. So the calls need to be modified to include {{-oozie oozie_url}}.",240RMApproved,['stacks'],AMBARI,Bug,Blocker,2016-08-20 23:20:41,89
12998695,Namenode start failed on moving namenode on a HA cluster ,"*STR:*
1. Enable NameNode HA
2. Launch ""move namendode"" wizard

*Actual Result:* On the page ""Configure Component"" at the step to start namenode, both namenodes are started. This leads to failure of the request intermittently
*Expected Result:* Only one namenode ""the one which is not being moved"" should be started",240RMApproved,['ambari-web'],AMBARI,Bug,Critical,2016-08-20 18:48:05,48
12998694,Ambari should use oozied.sh for stopping oozie so that optional catalina args can be provided,"In some scenarios, the oozie stop can take longer and if a oozie start is attempted it can fail with address already in use",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-20 18:29:59,90
12998657,File View throws IllegalArguementException post Ambari 2.4.0.0 upgrade,"The fileView throws below exception after upgrade

{code}
 Service 'hdfs' check failed:
java.lang.IllegalArgumentException: The value of property dfs.namenode.https-address.stanleyhotel.nn1 must not be null
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:125)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1134)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1115)
	at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.copyClusterProperty(ConfigurationBuilder.java:227)
	at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.copyHAProperties(ConfigurationBuilder.java:210)
	at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.parseProperties(ConfigurationBuilder.java:102)
	at org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.buildConfig(ConfigurationBuilder.java:278)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.<init>(HdfsApi.java:65)
	at org.apache.ambari.view.utils.hdfs.HdfsUtil.connectToHDFSApi(HdfsUtil.java:127)
	at org.apache.ambari.view.commons.hdfs.HdfsService.hdfsSmokeTest(HdfsService.java:136)
	at org.apache.ambari.view.filebrowser.HelpService.hdfsStatus(HelpService.java:86)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:257)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.jwt.JwtAuthenticationFilter.doFilter(JwtAuthenticationFilter.java:96)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
	at org.apache.ambari.server.security.authentication.AmbariAuthenticationFilter.doFilter(AmbariAuthenticationFilter.java:88)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.view.AmbariViewsMDCLoggingFilter.doFilter(AmbariViewsMDCLoggingFilter.java:54)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.view.ViewThrottleFilter.doFilter(ViewThrottleFilter.java:161)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:150)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

{code}

",240RMApproved,['ambari-views'],AMBARI,Bug,Blocker,2016-08-20 06:28:00,91
12998620,Zeppelin service check fails after enabling SSL for Zeppelin,"Zeppelin service is running fine after enabling Zeppelin SSL. Howerver, service check fails with below error.

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/ZEPPELIN/0.6.0.2.5/package/scripts/service_check.py"", line 39, in <module>
    ZeppelinServiceCheck().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 280, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/ZEPPELIN/0.6.0.2.5/package/scripts/service_check.py"", line 36, in service_check
    logoutput=True)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 273, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 71, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 93, in checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 141, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 294, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of 'curl -s -o /dev/null -w'%{http_code}' --negotiate -u: -k xxx:9995 | grep 200' returned 1.{code}",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-19 23:19:53,85
12998505,RU: Storm components were stopped during RU and can not be started,"
STR:
# Install cluster 2.4.2.0-258 on Ambari 2.2.2.0
# Enable HA
# Enable security
# Upgrade ambari to 240
# Perform RU to 2.5.0.0-1208

Deeper study shows that kerberos descriptor json in database (""artifact"" table) still contains values and properties that are actual for 2.4 stack.
So the issue workflow should look like:
- Old stack version is installed
- Kerberos descriptor gets saved to database
- Security is enabled
- Stack upgrade is performed
- Keytab regeneration is performed, and it populates service config with obsolete property values

The issue happens on ""Stack upgrade is performed"" step. We never update kerberos descriptor json in database to correspond to a new stack.

From nimbus.out
{code}Exception in thread ""main"" java.lang.ExceptionInInitializerError
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:270)
at clojure.lang.RT.classForName(RT.java:2154)
at clojure.lang.RT.classForName(RT.java:2163)
at clojure.lang.RT.loadClassForName(RT.java:2182)
at clojure.lang.RT.load(RT.java:436)
at clojure.lang.RT.load(RT.java:412)
at clojure.core$load$fn__5448.invoke(core.clj:5866)
at clojure.core$load.doInvoke(core.clj:5865)
at clojure.lang.RestFn.invoke(RestFn.java:408)
at clojure.core$load_one.invoke(core.clj:5671)
at clojure.core$load_lib$fn__5397.invoke(core.clj:5711)
at clojure.core$load_lib.doInvoke(core.clj:5710)
at clojure.lang.RestFn.applyTo(RestFn.java:142)
at clojure.core$apply.invoke(core.clj:632)
at clojure.core$load_libs.doInvoke(core.clj:5749)
at clojure.lang.RestFn.applyTo(RestFn.java:137)
at clojure.core$apply.invoke(core.clj:632)
at clojure.core$require.doInvoke(core.clj:5832)
at clojure.lang.RestFn.invoke(RestFn.java:408)
at org.apache.storm.daemon.nimbus$loading__5340__auto____8560.invoke(nimbus.clj:16)
at org.apache.storm.daemon.nimbus__init.load(Unknown Source)
at org.apache.storm.daemon.nimbus__init.<clinit>(Unknown Source)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:270)
at clojure.lang.RT.classForName(RT.java:2154)
at clojure.lang.RT.classForName(RT.java:2163)
at clojure.lang.RT.loadClassForName(RT.java:2182)
at clojure.lang.RT.load(RT.java:436)
at clojure.lang.RT.load(RT.java:412)
at clojure.core$load$fn__5448.invoke(core.clj:5866)
at clojure.core$load.doInvoke(core.clj:5865)
at clojure.lang.RestFn.invoke(RestFn.java:408)
at clojure.lang.Var.invoke(Var.java:379)
at org.apache.storm.daemon.nimbus.<clinit>(Unknown Source)
Caused by: java.lang.ClassNotFoundException: backtype.storm.security.auth.authorizer.SimpleACLAuthorizer
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:190)
at org.apache.storm.daemon.common$mk_authorization_handler.invoke(common.clj:412)
at org.apache.storm.ui.core__init.load(Unknown Source)
at org.apache.storm.ui.core__init.<clinit>(Unknown Source)
... 35 more
{code}

",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-19 14:41:59,13
12998465,Bug Fixing in HueMigration View,"1. Hive History Query Insertion in hive 1.5 and hive 1.0 (Due to DB schema changed) 
2. Not able to migrate with kerberos enabled
3. UI Validation bugs
4. Mysql and Postgres Hue error.",None,['ambari-views'],AMBARI,Bug,Blocker,2016-08-19 11:34:43,91
12998248,"On UI Sometimes: after clicking delete property button, property does not deleted instead Configuration Group window is opened",Configuration Group window is opened instead deleting,240RMApproved,['ambari-web'],AMBARI,Bug,Blocker,2016-08-18 16:26:22,7
12998005,All storm commands failed,"Cluster is running with AD + MIT setup. looks to be an issue with the AD kerberos as we are getting 
{code}
Caused by: sun.security.krb5.KrbException: Clock skew too great (37)
        at sun.security.krb5.KrbAsRep.<init>(KrbAsRep.java:76) ~[?:1.7.0_101]
        at sun.security.krb5.KrbAsReqBuilder.send(KrbAsReqBuilder.java:316) ~[?:1.7.0_101]
        at sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:361) ~[?:1.7.0_101]
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:735) ~[?:1.7.0_101]
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584) ~[?:1.7.0_101]
{code}",240RMApproved,[],AMBARI,Bug,Blocker,2016-08-17 19:46:22,39
12997926,Hive Metastore restart failed during EU with 'Internal credentials cache error' while running kinit,"ambari-server --hash
8250e90dc9ebcf1bd3dbac9b9eca8a6e21e073c9
ambari-server-2.4.0.0-1127.x86_64

Observed this issue in one EU run with below steps:

# Install HDP-2.4.0.0 cluster with Ambari 2.2.1.1 (secure, HA cluster)
# Upgrade Ambari to 2.4.0.0
# Perform EU to 2.4.2.0 and let it complete
# Start EU to 2.5.0.0

Observed below error during Hive Metastore restart
{code}
Traceback (most recent call last):
  File \""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py\"", line 254, in <module>
    HiveMetastore().execute()
  File \""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\"", line 280, in execute
    method(env)
  File \""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\"", line 696, in restart
    self.pre_upgrade_restart(env, upgrade_type=upgrade_type)
  File \""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py\"", line 114, in pre_upgrade_restart
    self.upgrade_schema(env)
  File \""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py\"", line 193, in upgrade_schema
    Execute(kinit_command,user=params.smokeuser)
  File \""/usr/lib/python2.6/site-packages/resource_management/core/base.py\"", line 155, in __init__
    self.env.run()
  File \""/usr/lib/python2.6/site-packages/resource_management/core/environment.py\"", line 160, in run
    self.run_action(resource, action)
  File \""/usr/lib/python2.6/site-packages/resource_management/core/environment.py\"", line 124, in run_action
    provider_action()
  File \""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\"", line 273, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 71, in inner
    result = function(command, **kwargs)
  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 93, in checked_call
    tries=tries, try_sleep=try_sleep)
  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 141, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 294, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ' returned 1. kinit: Internal credentials cache error while storing credentials while getting initial credentials""
{code}

*A retry of the above failed task was successful and then EU proceeded to completion*",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-17 14:49:51,31
12997884,"After the upgrade of jetty, zeppelin-view fails to load","Observed this issue after upgrade of Ambari server from 2.2.2.0 to 2.4.0.0 where after running ""ambari-server start"" command, the logs throw below errors:

{code}
HTTP ERROR 500

Problem accessing /views/ZEPPELIN/1.0.0/Zepplin/. Reason:

    Server Error
Caused by:

java.lang.ClassCastException: org.apache.jasper.runtime.ELContextImpl cannot be cast to org.apache.jasper.runtime.ELContextImpl
  at org.apache.jasper.runtime.PageContextImpl.evaluateExpression(PageContextImpl.java:994)
  at org.apache.jsp.WEB_002dINF.index_jsp._jspService(org.apache.jsp.WEB_002dINF.index_jsp:72)
  at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:109)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
  at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:389)
  at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:486)
  at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:380)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:575)
  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
  at org.eclipse.jetty.server.Dispatcher.forward(Dispatcher.java:276)
  at org.eclipse.jetty.server.Dispatcher.forward(Dispatcher.java:103)
  at org.apache.ambari.view.zeppelin.ZeppelinServlet.doGet(ZeppelinServlet.java:55)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1507)
  at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.jav
{code}

[~jhurley] helped to look at the environment and mentioned that this is due to https://github.com/apache/ambari/commit/c03b6d4b01fbc336c296c9a1a92ca1308cba6ffc
",240RMApproved,['ambari-views'],AMBARI,Bug,Blocker,2016-08-17 11:30:29,66
12997840,yarn capacity scheduler queue issue,"Configure yarn queue using the yarn queue manager view. ( Below configuration hints at how the queue was configured using UI and this was not set using the configs under ""YARN"" service )
{code}
yarn.scheduler.capacity.root.hive.capacity=80
yarn.scheduler.capacity.root.hive.maximum-capacity=80
yarn.scheduler.capacity.root.queues=default,hive
yarn.scheduler.capacity.root.hive.queues=microstrategy,tpcds,tpch
yarn.scheduler.capacity.root.default.capacity=20
yarn.scheduler.capacity.root.hive.tpcds.capacity=25
yarn.scheduler.capacity.root.hive.tpcds.maximum-capacity=25
yarn.scheduler.capacity.root.hive.tpch.capacity=25
yarn.scheduler.capacity.root.hive.tpch.maximum-capacity=25
yarn.scheduler.capacity.root.hive.microstrategy.capacity=50
yarn.scheduler.capacity.root.hive.microstrategy.maximum-capacity=50
{code}
Below error is thrown while refreshing the queues.

{code}

esource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/rm.service.keytab rm/ctr-e25-1471039652053-0001-01-000002.hwx.site@HWX.VIBGYOR.COM; export HADOOP_LIBEXEC_DIR=/usr/hdp/current/hadoop-client/libexec && /usr/hdp/current/hadoop-yarn-resourcemanager/bin/yarn rmadmin -refreshQueues' returned 255. 16/08/16 05:01:39 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2
16/08/16 05:01:39 WARN retry.RetryInvocationHandler: Exception while invoking ResourceManagerAdministrationProtocolPBClientImpl.refreshQueues over rm2. Not retrying because try once and fail.
org.apache.hadoop.yarn.exceptions.YarnException: java.io.IOException: Failed to re-init queues
	at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.logAndWrapException(AdminService.java:762)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:398)
	at org.apache.hadoop.yarn.server.api.impl.pb.service.ResourceManagerAdministrationProtocolPBServiceImpl.refreshQueues(ResourceManagerAdministrationProtocolPBServiceImpl.java:102)
	at org.apache.hadoop.yarn.proto.ResourceManagerAdministrationProtocol$ResourceManagerAdministrationProtocolService$2.callBlockingMethod(ResourceManagerAdministrationProtocol.java:239)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)
Caused by: java.io.IOException: Failed to re-init queues
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:364)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:388)
	... 10 more
Caused by: java.lang.NumberFormatException: For input string: ""10000.0""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1258)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getMaximumSystemApplications(CapacitySchedulerConfiguration.java:289)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs(LeafQueue.java:166)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:139)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:629)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:649)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:513)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:360)
	... 11 more

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceManagerAdministrationProtocolPBClientImpl.refreshQueues(ResourceManagerAdministrationProtocolPBClientImpl.java:123)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)
	at com.sun.proxy.$Proxy8.refreshQueues(Unknown Source)
	at org.apache.hadoop.yarn.client.cli.RMAdminCLI.refreshQueues(RMAdminCLI.java:290)
	at org.apache.hadoop.yarn.client.cli.RMAdminCLI.run(RMAdminCLI.java:648)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.yarn.client.cli.RMAdminCLI.main(RMAdminCLI.java:793)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.YarnException): java.io.IOException: Failed to re-init queues
	at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.logAndWrapException(AdminService.java:762)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:398)
	at org.apache.hadoop.yarn.server.api.impl.pb.service.ResourceManagerAdministrationProtocolPBServiceImpl.refreshQueues(ResourceManagerAdministrationProtocolPBServiceImpl.java:102)
	at org.apache.hadoop.yarn.proto.ResourceManagerAdministrationProtocol$ResourceManagerAdministrationProtocolService$2.callBlockingMethod(ResourceManagerAdministrationProtocol.java:239)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)
Caused by: java.io.IOException: Failed to re-init queues
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:364)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:388)
	... 10 more
Caused by: java.lang.NumberFormatException: For input string: ""10000.0""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1258)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getMaximumSystemApplications(CapacitySchedulerConfiguration.java:289)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs(LeafQueue.java:166)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:139)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:629)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:649)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:513)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:360)
	... 11 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)
	at org.apache.hadoop.ipc.Client.call(Client.java:1496)
	at org.apache.hadoop.ipc.Client.call(Client.java:1396)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy7.refreshQueues(Unknown Source)
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceManagerAdministrationProtocolPBClientImpl.refreshQueues(ResourceManagerAdministrationProtocolPBClientImpl.java:120)
	... 13 more
refreshQueues: java.io.IOException: Failed to re-init queues
	at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.logAndWrapException(AdminService.java:762)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:398)
	at org.apache.hadoop.yarn.server.api.impl.pb.service.ResourceManagerAdministrationProtocolPBServiceImpl.refreshQueues(ResourceManagerAdministrationProtocolPBServiceImpl.java:102)
	at org.apache.hadoop.yarn.proto.ResourceManagerAdministrationProtocol$ResourceManagerAdministrationProtocolService$2.callBlockingMethod(ResourceManagerAdministrationProtocol.java:239)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)
Caused by: java.io.IOException: Failed to re-init queues
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:364)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:388)
	... 10 more
Caused by: java.lang.NumberFormatException: For input string: ""10000.0""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1258)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getMaximumSystemApplications(CapacitySchedulerConfiguration.java:289)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs(LeafQueue.java:166)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:139)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:629)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:649)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:513)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:360)


{code}
",240RMApproved,['ambari-views'],AMBARI,Bug,Blocker,2016-08-17 08:13:54,77
12997757,Hive Service check is failing after moving webhcat server,Moving of webhcat server should change proxyuser configs for hosts in core-site for webhcat user in non-kerberozed environment. Not doing so is causing this issue.,240RMApproved,['ambari-web'],AMBARI,Bug,Blocker,2016-08-17 00:21:48,92
12997676,Create Atlas log4j changes for failed notifications,"Create Atlas log4j in ambari-server/src/main/resources/common-services/ATLAS/0.7.0.2.5/configuration/atlas-log4j.xml which is based on the earlier version, but makes the changes specified in
https://issues.apache.org/jira/browse/ATLAS-1111
No need to change EU/RU config packs since HDP 2.3/2.4 -> HDP 2.5 requires removing Atlas service and all of its configs.",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-16 18:48:14,93
12997673,RU: Kafka brokers restart was stopped during downgrade cluster,"Kafka broker restart failed due to below error:
{noformat}
raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'xasecure.audit.destination.db' was not found in configurations dictionary!
{noformat}


Solution:
During RU downgrade, <pre-upgrade/> runs on downgrade as well, which deleted {{xasecure.audit.destination.db}} config property. 

{noformat}
<pre-upgrade>
          <task xsi:type=""configure"" id=""hdp_2_5_0_0_remove_ranger_kafka_audit_db"" />
</pre-upgrade>
{noformat}

Need to override it with a blank <pre-downgrade/> element.",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-16 18:27:59,25
12997653,Alert on Atlas after adding it to a secure cluster as HBase table initialization fails,"{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 198, in <module>
    HbaseRegionServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 280, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 720, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 124, in start
    self.post_start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 89, in post_start
    self.apply_atlas_acl(params.hbase_user)
  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 114, in apply_atlas_acl
    shell.checked_call(format(""{kinit_cmd}; {perm_cmd}""), user=params.hbase_user, tries=10, try_sleep=10)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 71, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 93, in checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 141, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 294, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/hbase.service.keytab hbase/nat-r6-gjss-ambari-blueprints-4re-1-1.openstacklocal@EXAMPLE.COM; echo ""grant 'atlas', 'RWXCA', 'atlas_titan'"" | hbase shell -n' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
ERROR ArgumentError: Can't find a table: atlas_titan
{code}",240RMApproved,['ambari-server'],AMBARI,Bug,Blocker,2016-08-16 17:26:08,24
12997534,Hive service check is failing after cluster Kerberization,"{code}
resource_management.core.exceptions.Fail: Execution of '/var/lib/ambari-agent/tmp/templetonSmoke.sh $HOSTNAME 50111 idtest.ambari-qa.1471340206.23.pig /etc/security/keytabs/smokeuser.headless.keytab true /usr/bin/kinit ambari-qa@REALM_NAME /var/lib/ambari-agent/tmp' returned 1. Templeton Smoke Test (ddl cmd): Failed. : {""error"":""Unauthorized connection for super-user: HTTP/HOST_NAME@REALM_NAME from IP HOST_IP""}http_code <500>
{code}",240RMApproved system_test,['ambari-server'],AMBARI,Bug,Blocker,2016-08-16 09:41:25,41
12997419,Oozie Hive actions fail when Atlas is installed since Atlas Hive Hooks need to be copied to Oozie Share Lib in HDFS,"After the Falcon-Atlas hook has been enabled, the following properties are added.

startup.properties
{noformat}
*.application.services=org.apache.falcon.security.AuthenticationInitializationService,\
      org.apache.falcon.workflow.WorkflowJobEndNotificationService, \
      org.apache.falcon.service.ProcessSubscriberService,\
      org.apache.falcon.extensions.ExtensionService,\
      org.apache.falcon.service.LifecyclePolicyMap,\
      org.apache.falcon.entity.store.ConfigurationStore,\
      org.apache.falcon.rerun.service.RetryService,\
      org.apache.falcon.rerun.service.LateRunService,\
      org.apache.falcon.service.LogCleanupService,\
      org.apache.falcon.metadata.MetadataMappingService,\
org.apache.atlas.falcon.service.AtlasService
{noformat}

falcon-env.sh
{noformat}
# Add the Atlas Falcon hook to the Falcon classpath
export FALCON_EXTRA_CLASS_PATH=/usr/hdp/current/atlas-client/hook/falcon/*:${FALCON_EXTRA_CLASS_PATH}
{noformat} 

Whenever Oozie submits Hive actions, they fail and the application logs show
{noformat}
hive.exec.post.hooks Class not found:org.apache.atlas.hive.hook.HiveHook
FAILED: Hive Internal Error: java.lang.ClassNotFoundException(org.apache.atlas.hive.hook.HiveHook)
java.lang.ClassNotFoundException: org.apache.atlas.hive.hook.HiveHook
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.hive.ql.hooks.HookUtils.getHooks(HookUtils.java:60)
	at org.apache.hadoop.hive.ql.Driver.getHooks(Driver.java:1384)
	at org.apache.hadoop.hive.ql.Driver.getHooks(Driver.java:1368)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1595)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1289)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1156)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1146)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:314)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:412)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:428)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:717)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
	at org.apache.oozie.action.hadoop.HiveMain.runHive(HiveMain.java:335)
	at org.apache.oozie.action.hadoop.HiveMain.run(HiveMain.java:312)
	at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:51)
	at org.apache.oozie.action.hadoop.HiveMain.main(HiveMain.java:69)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:242)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
{noformat}


This is because whenever Atlas is installed, the Hive Hook (which is installed via the Oozie RPM too, in  /usr/$stack/$version/atlas/hook/hive/) needs to be copied to the Oozie Share lib in HDFS (e.g., hdfs:///user/oozie/share/lib/lib_$timestamp/hive)

Further, need to save atlas-application.properties used for Hive in /etc/oozie/conf/action-conf/hive

Lastly, remove the atlas.rest.address property from hive-site.xml",240RMApproved,['stacks'],AMBARI,Bug,Blocker,2016-08-15 23:04:37,69
12997093,Define keytab/principal for Spark Thrift Server,"PROBLEM: As of now spark thrift server seems to be picking up the tgt from cache upon start up and will not renew the ticket when it expires. This causes the spark thrift server processes only valid for 7 days. 

Users need to created a script to manually renew the ticket , but this is causing inconvenience. ",240RMApproved,[],AMBARI,Bug,Critical,2016-08-12 23:16:37,86
12996898,Enable Namenode HA failing at install journal nodes with cluster operator user,"Enabling name node HA is failing at journal node install for cluster operator user. Looking at network from the chrome browser, looks like there is a 403.",240RMApproved rbac,['ambari-server'],AMBARI,Bug,Blocker,2016-08-12 09:51:20,6
12996288,Add Kerberos Automation documentation to Ambari source tree so it may be versioned ,"Add Kerberos Automation documentation to Ambari source tree so it may be versioned.  This documentation should be added as MD (markdown) files to {{.../ambari-server/docs/security/kerberos}}.
",documentation kerberos kerberos_descriptor,['ambari-server'],AMBARI,Documentation,Minor,2016-08-10 14:17:32,30
12995704,Hive service check is failing after adding additional webHCAT,"STR :
1. Install cluster
2. Install WebHCat server  (during install)
3. Install additional WebHCat server on a different host",system_test,['ambari-server'],AMBARI,Bug,Critical,2016-08-08 17:12:50,43
12995679,Decrease the number of retry count for check_ranger_login_urllib2,"If the Ranger Admin is down then while starting any service from Ambari it keeps retrying  75 times in the interval of 8 seconds (total 600 Seconds , Means 10 minutes) and then it finally starts the service like Kafka Broker service.

Following kind of logging we can see in the ambari console when the Ranger Admin is Down and when the kafka broker start request is triggered (Attaching the ""/var/lib/ambari-agent/data/output-297.txt"" log):

Snippet of the retry attempts:
{code}
2016-08-08 13:45:27,802 - HdfsResource[None] {'security_enabled': False, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': [EMPTY], 'default_fs': 'hdfs://jss1.example.com:8020', 'hdfs_resource_ignore_file': '/var/lib/ambari-agent/data/.hdfs_resource_ignore', 'hdfs_site': ..., 'kinit_path_local': 'kinit', 'principal_name': [EMPTY], 'user': 'hdfs', 'action': ['execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf', 'immutable_paths': [u'/apps/hive/warehouse', u'/mr-history/done', u'/app-logs', u'/tmp']}
2016-08-08 13:45:27,853 - RangeradminV2: Skip ranger admin if it's down !
2016-08-08 13:45:27,858 - Will retry 74 time(s), caught exception: Connection failed to Ranger Admin. Reason - [Errno 111] Connection refused.. Sleeping for 8 sec(s)
2016-08-08 13:45:35,869 - Will retry 73 time(s), caught exception: Connection failed to Ranger Admin. Reason - [Errno 111] Connection refused.. Sleeping for 8 sec(s)
.
.
.
2016-08-08 13:55:04,653 - Will retry 2 time(s), caught exception: Connection failed to Ranger Admin. Reason - [Errno 111] Connection refused.. Sleeping for 8 sec(s)
2016-08-08 13:55:12,665 - Will retry 1 time(s), caught exception: Connection failed to Ranger Admin. Reason - [Errno 111] Connection refused.. Sleeping for 8 sec(s)
2016-08-08 13:55:20,676 - Connection failed to Ranger Admin. Reason - [Errno 111] Connection refused.
2016-08-08 13:55:20,683 - File['/usr/hdp/current/kafka-broker/config/ranger-security.xml'] {'content': InlineTemplate(...), 'owner': 'kafka', 'group': 'hadoop', 'mode': 0644}
{code}


*What is Needed?*
Here we see that it is not worth to wait for 600 Seconds (10 Minutes) to retry and then start the service (kafka broker Or any other component).  Instead it can be reduced retry attempts to 15 times instead of trying 75 times.

*What was previous behavior?*
Before the [AMBARI-14710|https://issues.apache.org/jira/browse/AMBARI-14710] the retry attempt was set to 15 times which was more accurate.
",patch-available,['ambari-agent'],AMBARI,Bug,Major,2016-08-08 15:28:26,22
12995154,HBase start failing with API,"UrlPath : /api/v1/clusters/cl1
Using username : admin
Request body  : {""RequestInfo"":{""command"":""RESTART"",""context"":""Restart all required services"",""operation_level"":""host_component""},""Requests/resource_filters"":[{""hosts_predicate"":""HostRoles/stale_configs=true""}]}

Failed task :
{code}
""Traceback (most recent call last):\n  File \""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py\"", line 194, in <module>\n    HbaseRegionServer().execute()\n  File \""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\"", line 280, in execute\n    method(env)\n  File \""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\"", line 720, in restart\n    self.start(env, upgrade_type=upgrade_type)\n  File \""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py\"", line 120, in start\n    self.post_start(env, upgrade_type=upgrade_type)\n  File \""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py\"", line 89, in post_start\n    self.apply_atlas_acl(params.hbase_user)\n  File \""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py\"", line 110, in apply_atlas_acl\n    shell.checked_call(format(\""{kinit_cmd}; {permissions_cmd}\""), tries=10, try_sleep=10)\n  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 71, in inner\n    result = function(command, **kwargs)\n  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 93, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 141, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \""/usr/lib/python2.6/site-packages/resource_management/core/shell.py\"", line 294, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/hbase.service.keytab hbase/nat-r6-ohis-ambari-api-rbac-6-2.openstacklocal@EXAMPLE.COM; echo \""grant 'atlas', 'RWXCA'\"" | hbase shell -n' returned 1. kinit: Permission denied while getting initial credentials\n2016-08-05 04:29:14,573 FATAL [main] conf.Configuration: error parsing conf core-site.xml\njava.io.FileNotFoundException: /etc/hbase/2.5.0.0-1145/0/core-site.xml (Permission denied)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:93)\n\tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\n\tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)\n\tat java.net.URL.openStream(URL.java:1045)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2502)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2573)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2526)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2418)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1143)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1115)\n\tat org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1451)\n\tat org.apache.hadoop.hbase.io.compress.Compression$Algorithm.<init>(Compression.java:249)\n\tat org.apache.hadoop.hbase.io.compress.Compression$Algorithm.<init>(Compression.java:105)\n\tat org.apache.hadoop.hbase.io.compress.Compression$Algorithm$1.<init>(Compression.java:106)\n\tat org.apache.hadoop.hbase.io.compress.Compression$Algorithm.<clinit>(Compression.java:106)\n\tat org.apache.hadoop.hbase.HColumnDescriptor.<clinit>(HColumnDescriptor.java:135)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.jruby.javasupport.JavaSupport.loadJavaClass(JavaSupport.java:136)\n\tat org.jruby.javasupport.JavaSupport.loadJavaClassVerbose(JavaSupport.java:145)\n\tat org.jruby.javasupport.JavaClass.forNameVerbose(JavaClass.java:1193)\n\tat org.jruby.javasupport.Java.getProxyOrPackageUnderPackage(Java.java:914)\n\tat org.jruby.javasupport.Java.get_proxy_or_package_under_package(Java.java:947)\n\tat org.jruby.javasupport.JavaUtilities.get_proxy_or_package_under_package(JavaUtilities.java:54)\n\tat org.jruby.javasupport.JavaUtilities$s$2$0$get_proxy_or_package_under_package.call(JavaUtilities$s$2$0$get_proxy_or_package_under_package.gen:65535)\n\tat org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:201)\n\tat org.jruby.ast.CallTwoArgNode.interpret(CallTwoArgNode.java:59)\n\tat org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n\tat org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n\tat org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)\n\tat org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:120)\n\tat org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:165)\n\tat org.jruby.javasupport.util.RuntimeHelpers$MethodMissingMethod.call(RuntimeHelpers.java:497)\n\tat org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:203)\n\tat org.jruby.runtime.callsite.CachingCallSite.callMethodMissing(CachingCallSite.java:379)\n\tat org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:289)\n\tat org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:135)\n\tat org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:63)\n\tat org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:63)\n\tat org.jruby.ast.FCallOneArgNode.interpret(FCallOneArgNode.java:36)\n\tat org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n\tat org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n\tat org.jruby.evaluator.ASTInterpreter.INTERPRET_CLASS(ASTInterpreter.java:103)\n\tat org.jruby.evaluator.ASTInterpreter.evalClassDefinitionBody(ASTInterpreter.java:260)\n\tat org.jruby.ast.ModuleNode.interpret(ModuleNode.java:120)\n\tat org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n\tat org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n\tat org.jruby.ast.RootNode.interpret(RootNode.java:129)\n\tat org.jruby.evaluator.ASTInterpreter.INTERPRET_ROOT(ASTInterpreter.java:119)\n\tat org.jruby.Ruby.runInterpreter(Ruby.java:724)\n\tat org.jruby.Ruby.loadFile(Ruby.java:2489)\n\tat org.jruby.runtime.load.ExternalScript.load(ExternalScript.java:66)\n\tat org.jruby.runtime.load.LoadService.tryLoadingLibraryOrScript(LoadService.java:751)\n\tat org.jruby.runtime.load.LoadService.smartLoad(LoadService.java:332)\n\tat org.jruby.runtime.load.LoadService.require(LoadService.java:381)\n\tat org.jruby.runtime.load.LoadService.lockAndRequire(LoadService.java:304)\n\tat org.jruby.RubyKernel.requireCommon(RubyKernel.java:1079)\n\tat org.jruby.RubyKernel.require(RubyKernel.java:1062)\n\tat org.jruby.RubyKernel$s$1$0$require.call(RubyKernel$s$1$0$require.gen:65535)\n\tat org.jruby.internal.runtime.methods.JavaMethod$JavaMethodOneOrNBlock.call(JavaMethod.java:319)\n\tat org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:312)\n\tat org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:169)\n\tat usr.hdp.$2_dot_5_dot_0_dot_0_minus_1145.hbase.bin.hirb.__file__(/usr/hdp/2.5.0.0-1145/hbase/bin/hirb.rb:118)\n\tat usr.hdp.$2_dot_5_dot_0_dot_0_minus_1145.hbase.bin.hirb.load(/usr/hdp/2.5.0.0-1145/hbase/bin/hirb.rb)\n\tat org.jruby.Ruby.runScript(Ruby.java:697)\n\tat org.jruby.Ruby.runScript(Ruby.java:690)\n\tat org.jruby.Ruby.runNormally(Ruby.java:597)\n\tat org.jruby.Ruby.runFromMain(Ruby.java:446)\n\tat org.jruby.Main.doRunFromMain(Main.java:369)\n\tat org.jruby.Main.internalRun(Main.java:258)\n\tat org.jruby.Main.run(Main.java:224)\n\tat org.jruby.Main.run(Main.java:208)\n\tat org.jruby.Main.main(Main.java:188)\nNameError: cannot initialize Java class org.apache.hadoop.hbase.HColumnDescriptor\n  get_proxy_or_package_under_package at org/jruby/javasupport/JavaUtilities.java:54\n                      method_missing at file:/grid/0/hdp/2.5.0.0-1145/hbase/lib/jruby-complete-1.6.8.jar!/builtin/javasupport/java.rb:51\n                      HBaseConstants at /usr/hdp/2.5.0.0-1145/hbase/lib/ruby/hbase.rb:91\n                              (root) at /usr/hdp/2.5.0.0-1145/hbase/lib/ruby/hbase.rb:34\n                             require at org/jruby/RubyKernel.java:1062\n                              (root) at /usr/hdp/2.5.0.0-1145/hbase/bin/hirb.rb:118\n2016-08-05 04:29:14,697 FATAL [Thread-1] conf.Configuration: error parsing conf core-site.xml\njava.io.FileNotFoundException: /etc/hbase/2.5.0.0-1145/0/core-site.xml (Permission denied)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:93)\n\tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\n\tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)\n\tat java.net.URL.openStream(URL.java:1045)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2502)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2573)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2526)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2418)\n\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:981)\n\tat org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1031)\n\tat org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1432)\n\tat org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:68)\n\tat org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:82)\n\tat org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:97)\n\tat org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl$1.call(ConfigurationFactory.java:49)\n\tat org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl$1.call(ConfigurationFactory.java:46)\n\tat org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:78)\n\tat org.apache.phoenix.util.PhoenixContextExecutor.callWithoutPropagation(PhoenixContextExecutor.java:93)\n\tat org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl.getConfiguration(ConfigurationFactory.java:46)\n\tat org.apache.phoenix.jdbc.PhoenixDriver$1.run(PhoenixDriver.java:84)\nException in thread \""Thread-1\"" java.lang.RuntimeException: java.io.FileNotFoundException: /etc/hbase/2.5.0.0-1145/0/core-site.xml (Permission denied)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2673)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2526)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2418)\n\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:981)\n\tat org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1031)\n\tat org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1432)\n\tat org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:68)\n\tat org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:82)\n\tat org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:97)\n\tat org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl$1.call(ConfigurationFactory.java:49)\n\tat org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl$1.call(ConfigurationFactory.java:46)\n\tat org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:78)\n\tat org.apache.phoenix.util.PhoenixContextExecutor.callWithoutPropagation(PhoenixContextExecutor.java:93)\n\tat org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl.getConfiguration(ConfigurationFactory.java:46)\n\tat org.apache.phoenix.jdbc.PhoenixDriver$1.run(PhoenixDriver.java:84)\nCaused by: java.io.FileNotFoundException: /etc/hbase/2.5.0.0-1145/0/core-site.xml (Permission denied)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:93)\n\tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\n\tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)\n\tat java.net.URL.openStream(URL.java:1045)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2502)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2573)\n\t... 14 more""
{code}",system_test,['ambari-server'],AMBARI,Bug,Critical,2016-08-05 09:12:01,13
12994916,Pig view not loading After opening huetoambari view.,"PROBLEM: Pig view not loading

IMPACT: can't use it

STEPS TO REPRODUCE:

1. create instance of hue to view migration tool
2. create instance of pig view
3. open pig view",None,['ambari-views'],AMBARI,Bug,Major,2016-08-04 14:51:59,91
12994888,Place holder of Ambari Server hostname is incorrectly labelled ,"
PROBLEM: Incorrectly labeled has hue server hostname

this is for the hue to view migration tool view

IMPACT: Gives false information to customer


",None,['ambari-views'],AMBARI,Bug,Major,2016-08-04 12:51:16,94
12994591,Clean cached resources on host removal,"When a host is removed from the cluster and later from ambari there's a chance the agent registers back to the ambari server before the agent is stopped.

Stopping the machine running the agent without the host being deleted again leads to an inconsistent state in the ambari-server due to cached state.

Resolution:
The cached resources get cleared on host delete event. ",ambari-server,['ambari-server'],AMBARI,Bug,Major,2016-08-03 13:38:01,95
12994361,Kerberos identity definitions in Kerberos descriptors should explicitly declare a reference,"Kerberos identity definitions in Kerberos descriptors should explicitly declare a reference rather than rely on the identity's _name_ attribute. 

Currently, the set of Kerberos identities declared at a service-level or a component-level can contain identities with unique names.  For example using:

{code}
  ""identities"": [
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property1.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property1.keytab"",
        ...
      }
    },
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property2.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property2.keytab"",
        ...
      }
    }
  ]
{code}

Only the first ""identity"" principal is realized and the additional one is ignored, leaving the configurations {{service-site/property2.principal}} and {{service-site/property2.keytab}} untouched when Kerberos is enabled for the service. 

To help this, the 2nd instance can be converted to a reference, overriding only the attributes the need to be changed - like the configurations. 

{code}
  ""identities"": [
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property1.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property1.keytab"",
        ...
      }
    },
    {
      ""name"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property2.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property2.keytab""
      }
    }
  ]
{code}

This allows for both identity declarations to be realized, however this is limited to only the 2 instances. If a 3rd instance is needed (to set an additional configuration), it must look be:

{code}
    {
      ""name"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property3.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property3.keytab""
      }
    }
{code}

However since it's name is the same as the 2nd instance, it will be ignored. 

If explicit references are specified, then multiple uniquely-named identity blocks will be allowed to reference the same base identity, effectively enabling the ability to declare unlimited configurations for the same identity definition:

{code}
  ""identities"": [
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property1.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property1.keytab"",
        ...
      }
    },
    {
      ""name"": ""identitiy_reference1"",
      ""reference"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property2.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property2.keytab""
      }
    },
    {
      ""name"": ""identitiy_reference2"",
      ""reference"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property3.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property3.keytab""
      }
    }
  ]
{code}

NOTE: Backwards compatibility must be maintained when implementing this as to not break existing Kerberos descriptors. So identity block names the look like paths are to continue to be treated as references. 
",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-08-02 18:49:47,30
12994251,Allow multiple configurations for a Kerberos identity principal and keytab definition,"Allow multiple configurations for a Kerberos identity principal and keytab definition.

Currently we allow:

{code}
{
              ""name"": ""some_name"",
              ""principal"": {
                ""value"": ""foobar/_HOST@${realm}"",
                ""type"" : ""service"",
                ""configuration"": ""config1/property1"",
                ""local_username"" : ""${hadoop-env/hdfs_user}""
              },
              ""keytab"": {
                ""file"": ""${keytab_dir}/foobar.service.keytab"",
                ""owner"": {
                  ""name"": ""${config-env/foobar_user}"",
                  ""access"": ""r""
                },
                ""group"": {
                  ""name"": ""${cluster-env/user_group}"",
                  ""access"": """"
                },
                ""configuration"": ""config1/property2""
              }
            },
{code}

but we should allow for 

{code}
{
              ""name"": ""some_name"",
              ""principal"": {
                ""value"": ""foobar/_HOST@${realm}"",
                ""type"" : ""service"",
                ""configurations"": [""config1/property1"", ""config2/propertyA""],
                ""local_username"" : ""${hadoop-env/hdfs_user}""
              },
              ""keytab"": {
                ""file"": ""${keytab_dir}/foobar.service.keytab"",
                ""owner"": {
                  ""name"": ""${config-env/foobar_user}"",
                  ""access"": ""r""
                },
                ""group"": {
                  ""name"": ""${cluster-env/user_group}"",
                  ""access"": """"
                },
                ""configurations"":[ ""config1/property2"",  ""config2/propertyB""]
              }
            },
{code}",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-08-02 11:22:05,30
12994023,Integrate Druid With Ambari,"This task includes adding support for druid cluster provisioning via Ambari.
Details about Druid cluster design and different node types are present here - 
http://druid.io/docs/latest/design/design.html

In general, Druid can be defined as a service in HDP which has following components - 
1) Coordinator 
2) Overlord 
3) Historical 
4) Broker 
5) Middlemanager 
 
Druid also has external dependencies on following 
1) Zookeeper - Ambari should be able to pass in zk configs to druid cluster 
2) Deep storage - A distributed FS, can be one of HDFS/S3 or any other NFS.
3) Metadata Store - Mysql/Postgres. can be either provided by the user or a mysql instance provisioned by ambari itself.
",features,['ambari-server'],AMBARI,Task,Major,2016-08-01 16:48:59,68
12993593,Coverity Scan Security Vulnerability - SQL injection,"The Ambari coverity scan found two ""High impact security"" issues, both SQL Injections.  They are both the same coding issue, but one is in OracleConnector.java, and one is in the analogous method in PostgresConnector.java.

This is the key description:
{quote}
 CID 167755 (#1 of 1): SQL injection (SQLI)9. sql_taint: Insecure concatenation of a SQL statement. The value searchClause is tainted.

Perform one of the following to guard against SQL injection attacks.
* Parameterize the SQL statement using ? positional characters. Bind the tainted values to the ? positional parameters using one of the PreparedStatement.set* methods.
* Validate user-supplied values against predefined constant values. Concatenate these constant values into the SQL statement.
* Cast tainted values to safe types such as integers. Concatenate these type safe values into the statement.

[More Information|https://scan3.coverity.com/doc/en/cov_checker_ref.html#id_sql_generic]
{quote}

This is the one in OracleConnector.java, lines 32 -55:

{code}
32  @Override
  8. taint_path_param: Parameter searchClause receives the tainted data.
33  protected PreparedStatement getQualifiedPS(Statements statement, String searchClause, Workflows.WorkflowDBEntry.WorkflowFields field, boolean sortAscending, int offset, int limit) throws IOException {
34    if (db == null)
35      throw new IOException(""db not initialized"");
36
37    String order = "" ORDER BY "" + field.toString() + "" "" + (sortAscending ? SORT_ASC : SORT_DESC);
38
39    String query = ""select * \n"" +
40        ""  from ( select "" +
41//        ""/*+ FIRST_ROWS(n) */ \n"" +
42        ""  a.*, ROWNUM rnum \n"" +
43        ""      from (""
  CID 167755 (#1 of 1): SQL injection (SQLI)9. sql_taint: Insecure concatenation of a SQL statement. The value searchClause is tainted.
  Perform one of the following to guard against SQL injection attacks.

    Parameterize the SQL statement using ? positional characters. Bind the tainted values to the ? positional parameters using one of the PreparedStatement.set* methods.
    Validate user-supplied values against predefined constant values. Concatenate these constant values into the SQL statement.
    Cast tainted values to safe types such as integers. Concatenate these type safe values into the statement.

More Information
44        + statement.getStatementString() + searchClause + order +
45        "") a \n"" +
46        ""      where ROWNUM <= "" + (offset + limit) + "") \n"" +
47        ""where rnum  >= "" + offset;
48
49    try {
  10. sql_sink: Passing the tainted value query to the SQL API java.sql.Connection.prepareStatement(java.lang.String) may allow an attacker to inject SQL.
50      return db.prepareStatement(query);
51    } catch (SQLException e) {
52      throw new IOException(e);
53    }
54
55  }
{code}

This is the one in PostgresConnector.java, lines 495-504:

{code}
 
   8. taint_path_param: Parameter searchClause receives the tainted data.
495  protected PreparedStatement getQualifiedPS(Statements statement, String searchClause) throws IOException {
496    if (db == null)
497      throw new IOException(""postgres db not initialized"");
498    try {
499      // LOG.debug(""preparing "" + statement.getStatementString() + searchClause);
   CID 167743 (#1 of 1): SQL injection (SQLI)9. sql_taint: Insecure concatenation of a SQL statement. The value searchClause is tainted. Passing the tainted command to the SQL API java.sql.Connection.prepareStatement(java.lang.String) may allow an attacker to inject SQL.
   Perform one of the following to guard against SQL injection attacks.

    Parameterize the SQL statement using ? positional characters. Bind the tainted values to the ? positional parameters using one of the PreparedStatement.set* methods.
    Validate user-supplied values against predefined constant values. Concatenate these constant values into the SQL statement.
    Cast tainted values to safe types such as integers. Concatenate these type safe values into the statement.

More Information
500      return db.prepareStatement(statement.getStatementString() + searchClause);
501    } catch (SQLException e) {
502      throw new IOException(e);
503    }
504  }
{code}

*Solution*
Remove code supporting an unsupported REST API call to obtain jobtracker information.  his entry point is handled by {{org.apache.ambari.eventdb.webservice.WorkflowJsonService}}.  By removing this class and cleaning up orphaned code, the SQL injection issue list above will be solved. 

",coverity security,['ambari-server'],AMBARI,Bug,Critical,2016-07-29 17:13:13,30
12993105,Ambari should not recursively chown for HAWQ hdfs upon every start,"This results in changing of owner even if the owner value is same. The operation is very costly if there are a lot of subdirectories.

The owner value only changes when you switch from regular mode to secure mode and vice-versa.",performance,['ambari-server'],AMBARI,Bug,Major,2016-07-27 23:36:08,96
12993066,Fix SQOOP Kerberos descriptor to contain a 'component' block,"Fix SQOOP Kerberos descriptor to contain a 'component' block. Without this block, a NPE is being thrown.  A {{null}} check should also be performed to avoid future issues. 
",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2016-07-27 20:54:06,30
12992579,Login flow break when only ldap auth is enabled,"*Actual :* 
When property logsearch.auth.ldap.enable=true and other authentication methods are disabled then user can login with any username and password.


*Excepted*
User should login only for valid ldap users. 

#set properties value to reproduce this issue
logsearch.auth.file.enable=false
logsearch.auth.ldap.enable=true
logsearch.auth.simple.enable=false
logsearch.auth.external_auth.enable=false",patch,['logsearch'],AMBARI,Bug,Major,2016-07-26 13:33:24,97
12992357,"Stack advisor property removal recommendations are ignored when regenerating keytabs, leaving properties with empty values","Stack advisor property removal recommendations are ignored when regenerating keytabs, leaving properties with empty values.  

This causes (validation) issues when properties expected to be removed exist but have empty values. ",kerberos stack_advisor,['ambari-server'],AMBARI,Bug,Major,2016-07-25 17:53:55,30
12992105,Cluster Administrator role is unable to perform 'Install Packages' operation,"*Steps*
# Create a user and assign ""Cluster Administrator"" role
# As an admin, Register a new version
# Logout and login as the user created in step 1
# Try to perform ""Install packages"" operation from UI

*Result*
""status"" : 403,
""The authenticated user does not have the appropriate authorizations to create the requested resource(s)""
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-07-24 17:33:56,30
12991103,Kerberos Client fails to install,"Log
{noformat}
Traceback (most recent call last):
    File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 80, in <module>
      KerberosClient().execute()
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 280, in execute
      method(env)
    File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 28, in install
      self.install_packages(env)
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 545, in install_packages
      if Script.check_package_condition(package):
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 583, in check_package_condition
      return chooser_method()
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/package_conditions.py"", line 93, in should_install_kerberos_server
      return 'role' in config and not _has_applicable_local_component(""KERBEROS_CLIENT"")
  TypeError: _has_applicable_local_component() takes exactly 2 arguments (1 given)
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-07-20 12:45:08,30
12990765,Adding new components for existing services should process Kerberos-related configuration changes,"When adding new components for existing services, Kerberos-related configuration changes should be processed.  This should happen when an new component, not a new instance of an existing component is installed. 

Care must be taken to not reintroduce the issue described in AMBARI-14506 or if unavoidable, proper documentation should be added 
",kerbeos,['ambari-server'],AMBARI,Bug,Major,2016-07-19 13:42:06,30
12990485,Kerberos-related configs are not applied before INSTALL command is built on add service,"Kerberos-related configs are not applied before INSTALL command is built on add service.  

This occurs when new services and components are added to an existing cluster where Kerberos is enabled. Due to the order of when command detail (JSON) structures are built and when service configurations are set, Kerberos-related configurations will not be available for new services.  This is a regression created by the patch for AMBARI-17629.

*Solution*
Ensure that Kerberos-related configuration updates are applied before the INSTALL command details are created. 

",kerberos regression,['ambari-server'],AMBARI,Bug,Critical,2016-07-18 17:42:31,30
12990346,Ranger UserSync stopped after EU from 2.2.9 to 2.4.2 with SSLHandshakeException,"Steps
- Deploy 2.2.9 cluster with Ambari 2.2.1.1
- Upgrade Ambari to 2.4.0.0
- Perform Express Upgrade (EU) to 2.4.2.0 and let it complete
- Observed that after EU, Ranger Usersync reports as down",express_upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-07-18 11:23:42,25
12989852,Cluster user role is permitted to install packages using API,"With ""Cluster User"" role, submitting ""install packages"" API call goes through, even though it should be blocked

{code}
#curl -u cu:1234 -H ""X-Requested-By: ambari"" -i -X  POST http://ambari-server:8080/api/v1/clusters/cl1/stack_versions -d '{""ClusterStackVersions"":{""stack"":""HDP"",""version"":""2.3"",""repository_version"":""2.3.0.0""}}'
HTTP/1.1 202 Accepted
Date: Wed, 29 Jun 2016 05:55:16 GMT
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Set-Cookie: AMBARISESSIONID=11njwu8py6m511511liub068vj;Path=/;HttpOnly
Expires: Thu, 01 Jan 1970 00:00:00 GMT
User: cu
Content-Type: text/plain
Vary: Accept-Encoding, User-Agent
Content-Length: 136
Server: Jetty(9.2.11.v20150529)

{
  ""href"" : ""http://ambari-server:8080/api/v1/clusters/cl1/requests/36"",
  ""Requests"" : {
    ""id"" : 36,
    ""status"" : ""Accepted""
  }
}
{code}

Role of the user ""cu""
{code}
{
  ""href"" : ""http://ambari-server:8080/api/v1/users/cu/privileges/7"",
  ""PrivilegeInfo"" : {
    ""cluster_name"" : ""cl1"",
    ""permission_label"" : ""Cluster User"",
    ""permission_name"" : ""CLUSTER.USER"",
    ""principal_name"" : ""cu"",
    ""principal_type"" : ""USER"",
    ""privilege_id"" : 7,
    ""type"" : ""CLUSTER"",
    ""user_name"" : ""cu""
  }
}
{code}
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-07-15 16:16:13,30
12989435,Support PAM Authentication,"LDAP is complicated and needs careful configuration especially if synchronizing with a local users repository. It can even get more complex, when trying to support users from multiple domains, which is not supported by Ambari right now.

Tools like SSSD, Winbind, Quest, Centrify, ... do a good job of integrating complex LDAP/AD environments to Unix/Linux based systems using PAM.

Using PAM in Ambari could potentials simplify user authentication a lot.

As users synchronization would not be required anymore, users would need to be created at first log in. This can be borrowed from the newly implemented JWT authentication.

Other projects using PAM authentication:
(In Hadoop Knox) https://issues.apache.org/jira/browse/KNOX-537
(With Spring Auth) https://github.com/ImmobilienScout24/yum-repo-server/blob/master/src/main/java/de/is24/infrastructure/gridfs/http/security/PamAuthenticationProvider.java",security,['ambari-server'],AMBARI,New Feature,Major,2016-07-14 12:21:42,98
12989211,AMS: Split point calculation not optimal for large clusters.,"Use hbase.regionserver.global.memstore.upperLimit instead of lowerLimit for available memory for split points calculation. Change default region counts for METRIC_RECORD and METRIC_AGGREGATE to 2. 
",ambari-metrics,['ambari-metrics'],AMBARI,Bug,Major,2016-07-13 18:17:59,39
12989095,[RU/Downgrade] ranger service goes down in between of RU downgrade,"Scenario:
1. install ranger with stack 2.4 and ambari-2.4.0
2. register the version of stack 2.5
3. perform the upgrade
at the step where it where zookeepers are upgraded , just downgraded.
4. downgrade was successful 
5. check ranger service status , it was down
try to restart the ranger but ranger startup is failing ",upgrade,['ambari-upgrade'],AMBARI,Bug,Blocker,2016-07-13 12:06:32,25
12988488,Ambari agent can't start when TLSv1 is disabled in Java security,"Currently, the commit for https://issues.apache.org/jira/browse/AMBARI-14236 explicit force the SSL protocol to TLSv1 in  ambari-agent/src/main/python/ambari_agent/alerts/web_alert.py.  Unfortunate, this setting in effect whenever web_alert pacackged is loaded (ambari-agent/src/main/python/ambari_agent/AlertSchedulerHandler.py) regardless whether ssl is used or not. 

As a result, disabling TLSv1 in Ambari server will cause the agent to fail to start.

Recreate:

In Ambari's acitve JDK on Ambari server node, in java.security file, set jdk.tls.disabledAlgorithms=MD5, SSLv2, SSLv3, TLSv1, DSA, RC4, RSA keySize < 2048
restart ambari-server, and you will see errors in ambari agent logs:

ERROR 2016-07-11 15:11:15,269 NetUtil.py:84 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol
ERROR 2016-07-11 15:11:15,269 NetUtil.py:85 - SSLError: Failed to connect. Please check openssl library versions.
Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.",security,['ambari-agent'],AMBARI,Bug,Major,2016-07-12 00:06:33,13
12988168,Update JDK version of Dockerfile in dev-support,The Dockerfile in dev-support uses Java 7 while its support is finished. It would be nice it is Java 8 for developing purpose.,docker,[],AMBARI,Improvement,Major,2016-07-11 01:40:14,99
12988072,Changes to stack advisor framework to help with service advisors,"The following changes are needed in the stack advisor framework to help with service advisors:

* Add additional logging to show why a service advisor implementation was not loaded
* Move {{isSecurityEnabled}} from {{stacks/HDP/2.0.6/services/stack_advisor.py}} to a class member of {{DefaultStackAdvisor}} in {{stacks/stack_advisor.py}} so that all stack and service advisors may be able to use it
",service_advisor stack_advisor,['ambari-server'],AMBARI,Bug,Major,2016-07-09 20:59:49,30
12987891,AUTH_TO_LOCAL rules are not updated when adding services to a Blueprint-installed cluster,"When adding new services and components to a cluster that was initially created via Blueprints (rather than via the Ambari UI), auth-to-local rules that are expected to be created as indicated by the Kerberos descriptor are not.

It occurs because the components being installed are in the {{INIT}} state where the logic to determine whether to include the new auth-to-local rules or not expects the components to be in either the {{INSTALLED}} or {{STARTED}} states. This is due to logic added when resolving AMBARI-14232.

*Solution*:
Allow for auth-to-local rules for new services and components to be added when the state of the components are  {{INIT}} as well as {{INSTALLED}} and {{STARTED}}, when the cluster was installed via Blueprints. 
",auth_to_local kerberos,['ambari-server'],AMBARI,Bug,Critical,2016-07-08 18:48:35,30
12987471,Add localjceks support in ambari for Ranger and Ranger KMS services,"Currently, localjceks scheme is not supported for ranger installation through Ambari. Ambari should use localjecks scheme to store, retrieve and list its alias and values while installing ranger through Ambari.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2016-07-07 12:56:20,25
12987076,Ambari-web unable to automatically add MASTER component with cardinality ALL on all nodes,"While trying to add a MASTER component with cardinality ALL:

org.apache.ambari.server.controller.spi.ResourceAlreadyExistsException: Attempted to create a host_component which already exists: [clusterName=HDP, hostName=node1.mydomain.com, componentName=SERVICE_MASTER]


",patch,['ambari-web'],AMBARI,Bug,Major,2016-07-06 07:49:58,100
12985006,authorizer.class.name not being set on secure kafka clusters,"The {{kafka-broker/authorizer.class.name}} property is not being set properly when Kerberos is enabled.

The following logic should be followed:
{noformat}
if Kerberos is enabled
  if ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled == yes
    set authorizer.class.name to ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer""
  else
    set authorizer.class.name to ""kafka.security.auth.SimpleAclAuthorizer""
else
  remove authorizer.class.name
{noformat}

This should be updated in the stack advisor code. 

While at it, configurations from Kafka's {{kerberos.json}} file should be moved to the stack advisor to help ensure properties are set in the the same place to help with code maintenance and consistency.
",kerberos kerberos_descriptor stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2016-06-29 13:37:39,30
12985004,authorizer.class.name not being set on secure kafka clusters,"The {{kafka-broker/authorizer.class.name}} property is not being set properly when Kerberos is enabled.

The following logic should be followed:
{noformat}
if Kerberos is enabled
  if ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled == yes
    set authorizer.class.name to ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer""
  else
    set authorizer.class.name to ""kafka.security.auth.SimpleAclAuthorizer""
else
  remove authorizer.class.name
{noformat}

This should be updated in the stack advisor code. 

While at it, configurations from Kafka's {{kerberos.json}} file should be moved to the stack advisor to help ensure properties are set in the the same place to help with code maintenance and consistency.
",kerberos kerberos_descriptor stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2016-06-29 13:36:29,30
12985003,authorizer.class.name not being set on secure kafka clusters,"The {{kafka-broker/authorizer.class.name}} property is not being set properly when Kerberos is enabled.

The following logic should be followed:
{noformat}
if Kerberos is enabled
  if ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled == yes
    set authorizer.class.name to ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer""
  else
    set authorizer.class.name to ""kafka.security.auth.SimpleAclAuthorizer""
else
  remove authorizer.class.name
{noformat}

This should be updated in the stack advisor code. 

While at it, configurations from Kafka's {{kerberos.json}} file should be moved to the stack advisor to help ensure properties are set in the the same place to help with code maintenance and consistency.
",kerberos kerberos_descriptor stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2016-06-29 13:34:48,30
12982096,Migrate AMS queries to use ROW_TIMESTAMP instead of native timerange hint,"With PHOENIX-914, there is a change in implementation , As earlier, timestamp range was passed as a hint to the query to get advantage of native timerange optimization in hbase but with new implementation we can mark the timestamp column in the schema as a ROW_TIMESTAMP and pass timestamp range with “where” clause only to achieve equivalent performance and better accuracy.

For eq:-
With earlier implementation , AMS forms query like this:-

SELECT /*+ NATIVE_TIME_RANGE(1448029523000) */ METRIC_NAME, APP_ID, INSTANCE_ID, SERVER_TIME, UNITS, METRIC_SUM, HOSTS_COUNT, METRIC_MAX, METRIC_MIN FROM METRIC_AGGREGATE WHERE (METRIC_NAME IN ('regionserver.Server.totalRequestCount', 'regionserver.Server.blockCacheCountHitPercent', 'regionserver.Server.regionCount', 'regionserver.Server.compactionQueueLength', 'regionserver.Server.storeFileCount', 'master.Server.averageLoad')) AND APP_ID = 'ams-hbase' AND SERVER_TIME >= 1448029643000 AND SERVER_TIME < 1448033243 ORDER BY METRIC_NAME, SERVER_TIME LIMIT 11520

But with PHOENIX-914 :-

Declare SERVER_TIME as ROW_TIMESTAMP in schema:-
CREATE TABLE DESTINATION_METRICS_TABLE (SERVER_TIME DATE not null, METRIC_ID  CHAR(15) not null, METRIC_VALUE bigint … CONSTRAINT PK PRIMARY KEY(CREATED_DATE ROW_TIMESTAMP, METRIC_ID…)) …;

And remove hint from the query:-
SELECT  METRIC_NAME, APP_ID, INSTANCE_ID, SERVER_TIME, UNITS, METRIC_SUM, HOSTS_COUNT, METRIC_MAX, METRIC_MIN FROM METRIC_AGGREGATE WHERE (METRIC_NAME IN ('regionserver.Server.totalRequestCount', 'regionserver.Server.blockCacheCountHitPercent', 'regionserver.Server.regionCount', 'regionserver.Server.compactionQueueLength', 'regionserver.Server.storeFileCount', 'master.Server.averageLoad')) AND APP_ID = 'ams-hbase' AND SERVER_TIME >= 1448029643000 AND SERVER_TIME < 1448033243 ORDER BY METRIC_NAME, SERVER_TIME LIMIT 11520

",ambari-metrics,[],AMBARI,Bug,Critical,2016-06-22 21:10:41,39
12981050,"AmbariServer looks for ldap_url, container_dn in blueprint even for MIT security type","{{ldap_url}} and {{container_dn}} are expected for MIT security type.  They should only be required for AD integration. 

{code:title=Example BP}
  {
    ""configurations"": [
      {
        ""cluster-env"": {
          ""properties"": {
            ""command_retry_max_time_in_sec"": ""1200""
          }
        }
      },
      {
        ""kerberos-env"": {
          ""properties"": {
            ""realm"": ""EXAMPLE.COM"",
            ""kdc_type"": ""mit-kdc"",
            ""kdc_hosts"": ""kdc.example.com"",
            ""admin_server_host"": ""kdc.example.com"",
            ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
            ""service_check_principal_name"": ""cl1-QutreRP8p3""
          }
        }
      },
      {
        ""krb5-conf"": {
          ""properties"": {
            ""domains"": """",
            ""manage_krb5_conf"": ""true""
          }
        }
      }
    ],
    ""host_groups"": [
      {
        ""name"": ""host1"",
        ""cardinality"": ""1"",
        ""components"": [
          {
            ""name"": ""DATANODE""
          },
          {
            ""name"": ""NFS_GATEWAY""
          },
          {
            ""name"": ""HDFS_CLIENT""
          },
          {
            ""name"": ""NODEMANAGER""
          },
          {
            ""name"": ""YARN_CLIENT""
          },
          {
            ""name"": ""MAPREDUCE2_CLIENT""
          },
          {
            ""name"": ""HBASE_REGIONSERVER""
          },
          {
            ""name"": ""HBASE_CLIENT""
          },
          {
            ""name"": ""PHOENIX_QUERY_SERVER""
          },
          {
            ""name"": ""HIVE_CLIENT""
          },
          {
            ""name"": ""HCAT""
          },
          {
            ""name"": ""OOZIE_CLIENT""
          },
          {
            ""name"": ""ZOOKEEPER_CLIENT""
          },
          {
            ""name"": ""SUPERVISOR""
          },
          {
            ""name"": ""FALCON_CLIENT""
          },
          {
            ""name"": ""FLUME_HANDLER""
          },
          {
            ""name"": ""METRICS_MONITOR""
          },
          {
            ""name"": ""RANGER_TAGSYNC""
          },
          {
            ""name"": ""TEZ_CLIENT""
          },
          {
            ""name"": ""PIG""
          },
          {
            ""name"": ""SQOOP""
          },
          {
            ""name"": ""SLIDER""
          },
          {
            ""name"": ""KERBEROS_CLIENT""
          },
          {
            ""name"": ""MAHOUT""
          },
          {
            ""name"": ""HST_AGENT""
          },
          {
            ""name"": ""LOGSEARCH_LOGFEEDER""
          },
          {
            ""name"": ""LOGSEARCH_SOLR_CLIENT""
          }
        ]
      }
    ],
    ""Blueprints"": {
      ""blueprint_name"": ""bp1"",
      ""stack_name"": ""HDP"",
      ""stack_version"": ""2.5""
    }
  }
{code}

{noformat}
curl -H ""X-Requested-By:ambari"" -u admin:admin -i -X  POST -d @./bp1.json http://localhost:8080/api/v1/blueprints/bp1
HTTP/1.1 100 Continue

HTTP/1.1 400 Bad Request
Date: Mon, 20 Jun 2016 19:02:27 GMT
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Set-Cookie: AMBARISESSIONID=1a4dqzhedwoog4xg8jbu36e2q;Path=/;HttpOnly
Expires: Thu, 01 Jan 1970 00:00:00 GMT
User: admin
Content-Type: text/plain
Content-Length: 227
Server: Jetty(9.2.11.v20150529)

{
  ""status"" : 400,
  ""message"" : ""Blueprint configuration validation failed: Missing required properties.  Specify a value for these properties in the blueprint configuration. {host1={kerberos-env=[ldap_url, container_dn]}}""
}
{noformat}
",kerberos,[],AMBARI,Bug,Major,2016-06-20 19:03:52,30
12977956,App timeline Server start fails on enabling HA because namenode is in safemode,"On the last step ""Start all"" on enabling HA below happens:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 147, in <module>
    ApplicationTimelineServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 219, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 43, in start
    self.configure(env) # FOR SECURITY
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 54, in configure
    yarn(name='apptimelineserver')
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/yarn.py"", line 276, in yarn
    mode=0755
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 154, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 463, in action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 460, in action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 259, in action_delayed
    self._set_mode(self.target_status)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 366, in _set_mode
    self.util.run_command(self.main_resource.resource.target, 'SETPERMISSION', method='PUT', permission=self.mode, assertable_result=False)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 195, in run_command
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of 'curl -sS -L -w '%{http_code}' -X PUT 'http://os-s11-3-iavzl-nat-s-ru242to25susesecha-12.openstacklocal:50070/webhdfs/v1/ats/done?op=SETPERMISSION&user.name=hdfs&permission=755'' returned status_code=403. 
{
  ""RemoteException"": {
    ""exception"": ""RetriableException"", 
    ""javaClassName"": ""org.apache.hadoop.ipc.RetriableException"", 
    ""message"": ""org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot set permission for /ats/done. Name node is in safe mode.\nThe reported blocks 675 needs additional 16 blocks to reach the threshold 0.9900 of total blocks 697.\nThe number of live datanodes 20 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.""
  }
}
{code}

This happens because NN is not yet out of safemode at the moment of ats start, because DNs just started.

To fix this ""stop namenodes"" has to be triggered before ""start all"".

If this is done, on ""Start all"" it will be ensured that datanodes start prior to NN, and that NN are out of safemode before ATS start.",ha namenode,[],AMBARI,Bug,Critical,2016-06-12 11:53:54,101
12977848,Add SERVICE.VIEW_OPERATIONAL_LOGS authorization to SERVICE.ADMINISTRATOR role and above,"Add SERVICE.VIEW_OPERATIONAL_LOGS authorization to the following roles:

* AMBARI.ADMINISTRATOR 
* CLUSTER.ADMINISTRATOR 
* CLUSTER.OPERATOR 
* SERVICE.ADMINISTRATOR

This is a DB change adding an authorization record to the {{roleauthorization}} table and relevant records for the different roles to the {{permission_roleauthorization}} table. 

The description/name of the {{SERVICE.VIEW_OPERATIONAL_LOGS}} authorization should be
{noformat}
View service operational logs
{noformat}
",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-06-11 10:29:17,30
12976678,Conflicts supported OS version,"[Apache Ambari Website|https://ambari.apache.org/] says Ambari Apache supports RHEL 5,6, CentOS 5,6, and OEL 5,6.

But [docs at Hortonworks webpage|http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.2.0/bk_Installing_HDP_AMB/content/_operating_systems_requirements.html] says  RHEL 6,7, CentOS 6,7, and OEL 6,7.

We should fix this conflicts.",easyfix,['documentation'],AMBARI,Bug,Trivial,2016-06-08 00:43:22,99
12975797,Firewall check returns WARNING even if iptables and firewalld are stopped on CentOS7,"In firewall.py, {{""systemctl is-active iptables || systemctl is-active firewalld""}} is passed to {{run_in_shell}} function, which splits cmd string by using {{shlex.split}}.

{{run_in_shell}} function finally calls {{subprocess.Popen}} with {{shell=True}}, so the cmd string is evaluated like {{Popen(['/bin/sh', '-c', 'systemctl', 'is-active', 'iptables', '||', 'systemctl', 'is-active', 'firewalld'])}}. This doesn't returns values as expected, because after args[1] (in this case, after the first {{is-active}}) are evaluated as sh arguements.

{{systemctl is-active}} can take multiple arugments, so we can use it.",patch,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2016-06-05 11:02:11,99
12975730,Fix description of SERVICE.ADD_DELETE_SERVICES permission,"The description of the SERVICE.ADD_DELETE_SERVICES permission currently reads

{quote}
Add Service to cluster
{quote}

This should be changed to

{quote}
Add/delete services
{quote}
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-06-04 10:55:45,30
12975482,Cluster operator and ServiceAdministrator not allowed to create config group,"Cluster operator and ServiceAdministrator are not allowed to create a config group even though it should be.

It is visible in the UI but creating it gives a 403 error in the backend and the UI goes for a toss.",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-06-03 13:29:20,30
12975326,VDF: Register Version UI inconsistencies + bug,"Found a couple inconsistency issues, plus a bug.",vdf,['ambari-web'],AMBARI,Bug,Critical,2016-06-02 22:34:36,102
12975298,Provide ability in AMS to specify minimum split points for high volume tables,"Configs introduced

METRIC_RECORD
ams-site : timeline.metrics.precision.table.min.regions

METRIC_AGGREGATE
ams-site : timeline.metrics.aggregate.table.min.regions

Default value for the configs is 1. ",ambari-metrics,[],AMBARI,Task,Critical,2016-06-02 21:07:12,39
12975218,Service admin and cluster operator can't modify service configs through API,"Using a serviceoperator user, trying to modify config using the 2 step modification process :

Request type : POST
Request URL : /api/v1/clusters/cl1/configurations
Auth : serviceadminuser/password
Request Body :
{code}
{""type"":""ams-env"",""tag"":""version146474298002"",""properties"":{""ambari_metrics_user"":""ams"",""metrics_monitor_log_dir"":""/grid/0/log/metric_monitor_updated"",""metrics_collector_heapsize"":""512"",""metrics_collector_pid_dir"":""/var/run/ambari-metrics-collector"",""metrics_collector_log_dir"":""/grid/0/log/metric_collector"",""metrics_monitor_pid_dir"":""/var/run/ambari-metrics-monitor"",""content"":""\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME\u003d{{java64_home}}\n\n# Collector Log directory for log4j\nexport AMS_COLLECTOR_LOG_DIR\u003d{{ams_collector_log_dir}}\n\n# Monitor Log directory for outfile\nexport AMS_MONITOR_LOG_DIR\u003d{{ams_monitor_log_dir}}\n\n# Collector pid directory\nexport AMS_COLLECTOR_PID_DIR\u003d{{ams_collector_pid_dir}}\n\n# Monitor pid directory\nexport AMS_MONITOR_PID_DIR\u003d{{ams_monitor_pid_dir}}\n\n# AMS HBase pid directory\nexport AMS_HBASE_PID_DIR\u003d{{hbase_pid_dir}}\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE\u003d{{metrics_collector_heapsize}}\n\n# HBase normalizer enabled\nexport AMS_HBASE_NORMALIZER_ENABLED\u003d{{ams_hbase_normalizer_enabled}}\n\n# HBase compaction policy enabled\nexport AMS_HBASE_FIFO_COMPACTION_ENABLED\u003d{{ams_hbase_fifo_compaction_enabled}}\n\n# HBase Tables Initialization check enabled\nexport AMS_HBASE_INIT_CHECK_ENABLED\u003d{{ams_hbase_init_check_enabled}}\n\n# AMS Collector options\nexport AMS_COLLECTOR_OPTS\u003d\""-Djava.library.path\u003d/usr/lib/ams-hbase/lib/hadoop-native\""\n{% if security_enabled %}\nexport AMS_COLLECTOR_OPTS\u003d\""$AMS_COLLECTOR_OPTS -Djava.security.auth.login.config\u003d{{ams_collector_jaas_config_file}}\""\n{% endif %}\n\n# AMS Collector GC options\nexport AMS_COLLECTOR_GC_OPTS\u003d\""-XX:+UseConcMarkSweepGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{ams_collector_log_dir}}/collector-gc.log-`date +\u0027%Y%m%d%H%M\u0027`\""\nexport AMS_COLLECTOR_OPTS\u003d\""$AMS_COLLECTOR_OPTS $AMS_COLLECTOR_GC_OPTS\""\n\n    ""}}
{code}

Request response : 
{code}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2016-06-02 16:28:01,30
12973958,Provide context  for hdp-select failures during ambari component install,"In case the *hdp-select* command fails during a service / component installation there's no contextual information about the cause of the failure.

This issue is for logging information about the machine on which the hdp-select command fails.

This solution wraps hdp-select command calls in a try/catch block and logs failure / hdp installationrelated information.   ",ambari,['ambari-server'],AMBARI,Bug,Major,2016-05-30 12:19:42,95
12973159,Different repositories show same versions of services,"Different repositories show same versions of services. The versions are from the last installed repo.
The order of repo is incorrect in some scenarios.
",vdf,['ambari-web'],AMBARI,Bug,Major,2016-05-26 18:28:52,102
12972445,Cluster operator and cluster admin not allowed to install ambari agent,"Cluster operator and the cluster admin must be allowed to add/delete hosts but install of agents using /bootstrap fails with 403
",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-05-24 17:00:55,30
12972325,Any start command fails if AMS is installed.,"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 39, in <module>
        BeforeStartHook().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 254, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 28, in hook
        import params
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/params.py"", line 145, in <module>
        metric_collector_hosts += host + ':' + metric_collector_port + ','
    TypeError: unsupported operand type(s) for +=: 'NoneType' and 'str'
    Error: Error: Unable to run the custom hook script ['/usr/bin/python', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py', 'START', '/var/lib/ambari-agent/data/command-9.json', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START', '/var/lib/ambari-agent/data/structured-out-9.json', 'INFO', '/var/lib/ambari-agent/tmp']
    

",ambari-metrics ambari-server,[],AMBARI,Bug,Major,2016-05-24 09:08:15,5
12971378,'Configure Ambari Identity' fails when enabling Kerberos on non-root Ambari server,"Configure Ambari Identity phase of Enable Kerberos Wizard fails since Ambari does not have permission perform some root-level file system tasks like chown. 

STR:
# Install Ambari, executing as non-root
# Create cluster (Zookeeper-only is fine)
# Enable Kerberos (any KDC is fine)

",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-05-20 02:16:09,30
12970765,Ambari agent cancel logic does not use correct name to access task id,"one very careless mistake for ambari agent cancel logic 
task_id -> taskId",patch,['ambari-agent'],AMBARI,Bug,Major,2016-05-18 13:33:40,103
12970759,Should close file object in security.py,"{{reqSignCrt}} method opens CSR file with {{open}} function but that doesn't close it.
And, {{loadSrvrCrt}} method opens a file but this is unclosed too.
They waste resources and may cause a problem.
",patch-available,['ambari-agent'],AMBARI,Bug,Minor,2016-05-18 13:16:53,99
12969507,yarn.timeline-service.enabled is set to false in secure cluster ,"yarn.timeline-service.enabled  property is set to false when Kerberos is enabled. It should set set to true. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-05-13 10:23:36,30
12967386,"RBAC: Change permission_label for role AMBARI.ADMINISTRATOR to ""Ambari Admin""","Change permission_label for role AMBARI.ADMINISTRATOR from ""Administrator"" to ""Ambari Admin""
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-05-11 12:11:22,30
12967161,Add conditional constraints for Kerberos identities to control when they are created,"Add conditional constraints for Kerberos identities to control when they are created. For example if Kerberos Identity should only be created (and distributed) for a component when some other component or service is installed. 

An example of this would be
{code}
{
  ""name"": ""/HIVE/HIVE_SERVER/hive_server_hive"",
  ""principal"": {
    ""configuration"": ""hive-interactive-site/hive.llap.daemon.service.principal""
  },
  ""keytab"": {
    ""configuration"": ""hive-interactive-site/hive.llap.daemon.keytab.file""
  },
  ""when"" : {
      ""contains"" : [""services"", ""HIVE""]
  }
}
{code}

Note the ""{{when}}"" clause. This indicates that this identity should only be processed when the set of services contains ""HIVE"".  An alternative to this would be to test the set of components for a certain component. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Critical,2016-05-10 18:42:39,30
12966495,View display name spills to next line for HUETOAMBARI_MIGRATION,"With the latest trunk code, the HUETOAMBARI_MIGRATION view name  spills over to the next line.",Patch,['ambari-admin'],AMBARI,Bug,Minor,2016-05-09 22:47:32,75
12966269,The 'krb5-conf' configuration is not available,"Configuration is loaded:
{code}
06 May 2016 10:52:11,998  INFO [qtp-ambari-client-26] ClusterImpl:346 - Service config types loaded: {KAFKA=[ranger-kafka-policymgr-ssl, kafka-log4j, kafka-env, kafka-broker, ranger-kafka-security, ranger-kafka-plugin-properties, ranger-kafka-audit], PIG=[pig-properties, pig-env, pig-log4j], ZEPPELIN=[zeppelin-env, zeppelin-config], LOGSEARCH=[logsearch-service_logs-solrconfig, logsearch-admin-json, logfeeder-log4j, logsearch-env, logsearch-solr-log4j, logfeeder-env, logsearch-audit_logs-solrconfig, logsearch-solr-env, logfeeder-properties, logsearch-properties, logsearch-log4j, logsearch-solr-client-log4j, logsearch-solr-xml], RANGER_KMS=[kms-properties, ranger-kms-security, ranger-kms-site, kms-site, kms-env, dbks-site, ranger-kms-audit, ranger-kms-policymgr-ssl, kms-log4j], MAPREDUCE2=[mapred-site, mapred-env], SLIDER=[slider-log4j, slider-env, slider-client], HIVE=[llap-cli-log4j2, hive-interactive-site, hive-exec-log4j, hive-env, ranger-hive-policymgr-ssl, tez-interactive-site, hive-site, hivemetastore-site, hive-interactive-env, webhcat-env, ranger-hive-plugin-properties, webhcat-site, hive-log4j, ranger-hive-audit, webhcat-log4j, hiveserver2-site, hcat-env, llap-daemon-log4j, ranger-hive-security], TEZ=[tez-env, tez-site], HBASE=[ranger-hbase-security, hbase-env, hbase-policy, hbase-log4j, hbase-site, ranger-hbase-policymgr-ssl, ranger-hbase-audit, ranger-hbase-plugin-properties], RANGER=[admin-properties, tagsync-log4j, ranger-site, ranger-ugsync-site, ranger-admin-site, ranger-tagsync-site, usersync-log4j, tagsync-application-properties, usersync-properties, admin-log4j, ranger-env], OOZIE=[oozie-log4j, oozie-env, oozie-site], FLUME=[flume-env, flume-conf], MAHOUT=[mahout-log4j, mahout-env], HDFS=[ssl-server, hdfs-log4j, ranger-hdfs-audit, ranger-hdfs-plugin-properties, ssl-client, hdfs-site, ranger-hdfs-policymgr-ssl, ranger-hdfs-security, hadoop-policy, hadoop-env, core-site], AMBARI_METRICS=[ams-ssl-client, ams-ssl-server, ams-hbase-log4j, ams-grafana-env, ams-hbase-policy, ams-hbase-security-site, ams-hbase-env, ams-env, ams-grafana-ini, ams-log4j, ams-site, ams-hbase-site], SPARK=[spark-thrift-fairscheduler, spark-thrift-sparkconf, spark-log4j-properties, spark-defaults, spark-metrics-properties, spark-hive-site-override, spark-env], SMARTSENSE=[hst-log4j, hst-server-conf, hst-common-conf, capture-levels, hst-agent-conf, anonymization-rules], YARN=[ranger-yarn-policymgr-ssl, yarn-site, ranger-yarn-audit, ranger-yarn-security, ranger-yarn-plugin-properties, yarn-env, capacity-scheduler, yarn-log4j], FALCON=[falcon-startup.properties, falcon-runtime.properties, falcon-env], SQOOP=[sqoop-site, sqoop-env], ZOOKEEPER=[zoo.cfg, zookeeper-env, zookeeper-log4j], STORM=[ranger-storm-plugin-properties, storm-site, ranger-storm-audit, storm-cluster-log4j, storm-worker-log4j, ranger-storm-policymgr-ssl, ranger-storm-security, storm-env], ATLAS=[atlas-hbase-site, atlas-log4j, atlas-env, application-properties], GANGLIA=[ganglia-env], KNOX=[knoxsso-topology, ranger-knox-security, users-ldif, knox-env, ranger-knox-plugin-properties, gateway-site, gateway-log4j, ranger-knox-policymgr-ssl, ranger-knox-audit, topology, admin-topology, ldap-log4j], KERBEROS=[kerberos-env, krb5-conf], ACCUMULO=[accumulo-log4j, accumulo-env, client, accumulo-site]}
{code}

But: 
{noformat}
06 May 2016 12:43:46,050 ERROR [qtp-ambari-client-171] AbstractResourceProvider:314 - Caught AmbariException when getting a resource
org.apache.ambari.server.AmbariException: The 'krb5-conf' configuration is not available
	at org.apache.ambari.server.controller.KerberosHelperImpl.getKerberosDetails(KerberosHelperImpl.java:1903)
	at org.apache.ambari.server.controller.KerberosHelperImpl.addAmbariServerIdentity(KerberosHelperImpl.java:1364)
	at org.apache.ambari.server.controller.KerberosHelperImpl.getActiveIdentities(KerberosHelperImpl.java:1283)
	at org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider$GetResourcesCommand.invoke(HostKerberosIdentityResourceProvider.java:163)
	at org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider$GetResourcesCommand.invoke(HostKerberosIdentityResourceProvider.java:145)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.getResources(AbstractResourceProvider.java:307)
	at org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider.getResources(HostKerberosIdentityResourceProvider.java:134)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:966)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:141)
	at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:512)
	at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:464)
	at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:437)
	at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:217)
	at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:69)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:145)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:90)
	at org.apache.ambari.server.api.services.HostService.getHost(HostService.java:80)
	at sun.reflect.GeneratedMethodAccessor205.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
{noformat}

*Cause*
This is caused in the {{org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider}} when the relevant host is the host where the Ambari server is installed and Kerberos is *_not_* enabled.  

When querying information about a host via {{GET /api/v1/clusters/CLUSTERNAME/hosts/HOSTNAME}}, the relevant Kerberos identities for that host are generated.  This happens whether Kerberos is enabled or not.  If the host is the host where the Ambari server is installed, than code is invoked to calculate the Ambari server's Kerberos identity.  In this code, the Kerberos-specific configurations are retrieved. If Kerberos is not enabled, these configurations will not be available and thus the error, ""The 'krb5-conf' configuration is not available"", is encountered. 

*Solution*
# Stop calculating the Kerberos identities when Kerberos is not enabled
# Protect access to the Kerberos configurations and set default values for needed configuration properties",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-05-09 19:01:23,30
12966012,"""Hadoop Root Logger"" does not take effect when changing the value","From ambari -> HDFS -> Configs  -> Advanced hadoop-env

modify

Hadoop Root Logger

from INFO,RFA to DEBUG,RFA and restart HDFS, it does not take effect.
See the pic in the attachment.",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Bug,Critical,2016-05-09 09:38:46,99
12964955,Authorizations given to role-based principals must be dereferenced upon user login,"Authorizations given to role-based principals must be dereferenced upon user login.  These authorizations are dynamically determined based on the user's set of roles.  

In {{org.apache.ambari.server.security.authorization.AmbariLocalUserDetailsService#loadUserByUsername}}, the set of {{GrantedAuthorities}} the authenticated user is calculated.  During this process, using the set of {{cluster-level roles}} a user is granted, any permissions given to matching role-based principals should be given to the user. 

This essentially work like giving privileges to a group of users calculated at runtime. 

A use-case to support the need for this is to assign access to a view to all users with some specific role. Currently we can assign access to a view to a specific user or group by assigning that user or group the {{VIEW.USER}} role applied to the specific view.  To assign access a view to users who have a specific role, a {{role}} will need to behave like a {{principal}}.
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-05-04 12:32:17,30
12964952,Allow roles to be treated like principals in Ambari DB,"To support assigning privileges to users based on their roles provide support in the Ambari database to allow a {{role}} to be referenced as a {{principal}} similar in the way a {{user}} and a {{group}} a referenced as a {principal}}.

A use-case to support the need for this is to assign access to a view to all users with some specific role. Currently we can assign access to a view to a specific user or group by assigning that user or group the {{VIEW.USER}} role applied to the specific view.  To assign access a view to users who have a specific role, a {{role}} will need to behave like a {{principal}}.

The following changes need to be made to the database:

* Add {{principal_id}} column to the {{adminpermission}} table
* Create a {{principaltype}} record where the {{principal_type_name}} is '{{ROLE}}'
* Add records to the {{adminprincpal}} table to represent each role in {{adminpermission}}
* Update {{adminpermission.principal_id}} to match the relevant records from {{adminprincipal}}

After this is complete, {{adminprivilege}} records can be created using roles as principals. 

NOTE: special handling will need to be done in the authorization logic to dereference the role associations with the authenticated user, similar in the way this is done for groups. 

",rbac,['ambari-server'],AMBARI,Bug,Major,2016-05-04 12:18:34,30
12964753,mapreduce.jobhistory.http.policy & mapreduce.jobhistory.webapp.https.address properties are not respected,"Ambari 2.2:
Configure SSL for MapReduce by setting these properties in mapred-site.xml
mapreduce.jobhistory.http.policy=HTTPS_ONLY
mapreduce.jobhistory.webapp.https.address=host:https_port

These properties are not respected and as a result JMX uses http protocol instead of https.",patch,['ambari-server'],AMBARI,Bug,Major,2016-05-03 19:11:31,104
12964660,RBAC: Clean up roles after ambari upgrade,"After AMBARI-16102, we no longer support more than 1 role per user. So we need a clean up when user upgrade to Ambari 2.4. If user has more than 1 role, we should keep the most ""powerful"" role and delete the rest. ",rbac upgrade,['ambari-server'],AMBARI,Bug,Major,2016-05-03 13:10:02,30
12963443,Use Ambari Server as credential server,"Add one more authentication provider in LogSearch using credential server of Ambari Server.
Ambari Authentication ref : https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md#authentication",feature,['ambari-server'],AMBARI,New Feature,Critical,2016-04-28 10:41:45,105
12962492,Add ambari-metrics-common dependency instead of hard-coded metrics code in Logfeeder,"Add dependency in Logfeeder and remove org.apache.hadoop.metrics2.sink package 
<dependency>
      <groupId>org.apache.ambari</groupId>
      <artifactId>ambari-metrics-common</artifactId>
      <version>${project.version}</version>
    </dependency>
",improvement,['ambari-server'],AMBARI,Improvement,Critical,2016-04-26 06:31:45,29
12961844,Stack Advisor issue when adding service to Kerberized cluster,"When adding a service to a Kerberized cluster and click install nothing happens on the UI and i see the following error in the ambari server logs

{code}
20 Apr 2016 16:03:56,818  INFO [qtp-ambari-client-2764] KerberosHelperImpl:735 - Adding identity for JOURNALNODE to auth to local mapping
20 Apr 2016 16:03:56,818  INFO [qtp-ambari-client-2764] KerberosHelperImpl:735 - Adding identity for METRICS_COLLECTOR to auth to local mapping
20 Apr 2016 16:03:56,857  INFO [qtp-ambari-client-2764] StackAdvisorRunner:47 - Script=/var/lib/ambari-server/resources/scripts/stack_advisor.py, actionDirectory=/var/run/ambari-server/stack-recommendations/323, command=recommend-configurations
20 Apr 2016 16:03:56,860  INFO [qtp-ambari-client-2764] StackAdvisorRunner:61 - Stack-advisor output=/var/run/ambari-server/stack-recommendations/323/stackadvisor.out, error=/var/run/ambari-server/stack-recommendations/323/stackadvisor.err
20 Apr 2016 16:03:56,917  INFO [qtp-ambari-client-2764] StackAdvisorRunner:69 - Stack advisor output files
20 Apr 2016 16:03:56,917  INFO [qtp-ambari-client-2764] StackAdvisorRunner:70 -     advisor script stdout: StackAdvisor implementation for stack HDP, version 2.0.6 was loaded
StackAdvisor implementation for stack HDP, version 2.1 was loaded
StackAdvisor implementation for stack HDP, version 2.2 was loaded
StackAdvisor implementation for stack HDP, version 2.3 was loaded
StackAdvisor implementation for stack HDP, version 2.4 was loaded
Returning HDP24StackAdvisor implementation
Error occured in stack advisor.
Error details: 'NoneType' object is not iterable
20 Apr 2016 16:03:56,917  INFO [qtp-ambari-client-2764] StackAdvisorRunner:71 -     advisor script stderr: Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 158, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 109, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 570, in recommendConfigurations
    calculation(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/./../stacks/HDP/2.0.6/services/stack_advisor.py"", line 627, in recommendAmsConfigurations
    if set(amsCollectorHosts).intersection(dn_hosts):
TypeError: 'NoneType' object is not iterable
20 Apr 2016 16:03:56,918  INFO [qtp-ambari-client-2764] AbstractResourceProvider:802 - Caught an exception while updating host components, retrying : org.apache.ambari.server.AmbariException: Stack Advisor reported an error: TypeError: 'NoneType' object is not iterable
StdOut file: /var/run/ambari-server/stack-recommendations/323/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/323/stackadvisor.err
{code}

*Solution*
Pass to the stack advisor information about all installed services where each component is installed (component host map)

 ",kerberos,['ambari-server'],AMBARI,Bug,Critical,2016-04-22 19:03:13,30
12961771,Ambari support for bucket caching parameter - hbase.bucketcache.ioengine,"In order to utilize SSD as a L2 (bucket) cache, we specify a file location to hbase.bucketcache.ioengine parameter.
hbase.bucketcache.ioengine=file:<filepath>
The directory location specified to the above parameter could be non-existent or non-accessible.
Since ambari-agent runs with root permissions, the proposed change here is to read the 'hbase.bucketcache.ioengine' parameter from hbase configuration and see if it points to a file location (by looking at file: prefix). 
In case ioengine points to a file, ambari-agent should create the underlying directory and assign appropriate permissions to hbase:hadoop user.
Activity",newbie patch,['stacks'],AMBARI,Bug,Major,2016-04-22 17:59:31,106
12960863,Auth-to-local rule generation duplicates default rules when adding case-insensitive default rules,"When re-generating auth-to-local rules where existing rules are already set, the default (or fallback) rule for the default and additional realms is duplicated but the extra instance(s) have the case-insensitive flag:

Example:
{noformat:title=Was}
...
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
...
{noformat}
{noformat:title=Becomes}
...
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*///L
...
{noformat}

*Steps to Reproduce*
# Create cluster with (at least) HDFS
# Enable Kerberos (do not check the box next to ""Enable case insensitive username rules""; kerberos-env/case_insensitive_username_rules should be false
# Edit Kerberos configuration and check ""Enable case insensitive username rules"" to set kerberos-env/case_insensitive_username_rules to true
# Regenerate Keytabs
# See duplicate entry in HDFS configs (core-site/hadoop.security.auth_to_local)

",auth_to_local kerberos,['ambari-server'],AMBARI,Bug,Critical,2016-04-21 18:28:54,30
12960822,Update Moment.js to latest stable version 2.13.0,"The latest stable version of Moment.js is 2.13.0.
This task is for updating the version of Moment.js used in

contrib/views/slider/src/main/resources/ui/vendor/scripts/common/
ambari-web/vendor/scripts",patch,"['ambari-views', 'ambari-web']",AMBARI,Task,Major,2016-04-21 16:52:49,75
12960745,Regenerating keytabs on re-imaged hosts results in error during 'Creating Principals',"We had a 1600 unsecured cluster initially, from which 700 nodes were destroyed. Though Ambari-server knew of 1600 hosts, only 900 were heartbeating. At this point we secured the cluster and everything was good. Then we brought back the 700 hosts, which started heartbeating with ambari-server. 

At this point we did 'Regenerate Keytabs' which failed at the 'Create Principals' step (image attached), as it was trying to re-create principal which is already existing with kadmin, and with ambari-server.

*Create Principals*
Stderr:
{noformat}
2016-04-21 01:28:52,985 - Failed to create or update principal, HTTP/host1.example.com@EXAMPLE.COM - Failed to create service principal for HTTP/host1.example.com@EXAMPLE.COM
STDOUT: Authenticating as principal admin/admin with password.

STDERR: WARNING: no policy specified for HTTP/host1.example.com@EXAMPLE.COM; defaulting to no policy
add_principal: Principal or policy already exists while creating ""HTTP/host1.example.com@EXAMPLE.COM"".
{noformat}

Stdout:
{noformat}
2016-04-21 01:27:32,400 - Processing identities...
2016-04-21 01:28:29,874 - Processing principal, HTTP/host1.example.com@EXAMPLE.COM
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-04-21 13:15:05,30
12959876,Kerberos wizard stuck trying to schedule service check operation,"Attached jstack after firing up the curl call.

{code}
curl -u admin:admin -H ""X-Requested-By:ambari"" -i -X POST -d'{""RequestInfo"":{""context"":""Kerberos Service Check"",""command"":""KERBEROS_SERVICE_CHECK"",""operation_level"":{""level"":""CLUSTER"",""cluster_name"":""c1""}},""Requests/resource_filters"":[{""service_name"":""KERBEROS""}]}' http://104.196.89.51:8080/api/v1/clusters/c1/requests
{code}

Behavior:
- Call timedout on the UI and wizard cannot proceed further.
- Exception in the server log after long wait:
{code}
12 Apr 2016 23:01:45,231  INFO [qtp-ambari-client-818] AmbariManagementControllerImpl:3376 - Received action execution request, clusterName=c1, request=isCommand :true, action :null, command :KERBEROS_SERVICE_CHECK, inputs :{}, resourceFilters: [RequestResourceFilter{serviceName='KERBEROS', componentName='null', hostNames=[]}], exclusive: false, clusterName :c1
12 Apr 2016 23:01:45,409  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HIVE
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: AMBARI_METRICS
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HDFS
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: MAPREDUCE2
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: OOZIE
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: TEZ
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HBASE
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: ZOOKEEPER
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: KERBEROS
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: YARN
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: PIG
12 Apr 2016 23:03:49,984  WARN [qtp-ambari-client-818] MITKerberosOperationHandler:434 - Failed to execute kadmin:
        Command: /usr/bin/kadmin -s perf-b-3.c.pramod-thangali.internal -p admin -w ******** -r EXAMPLE.COM -q ""get_principal admin""
        ExitCode: 1
        STDOUT: Authenticating as principal admin with password.

        STDERR: kadmin: Client not found in Kerberos database while initializing kadmin interface
{code}",kerberos kerberos-wizard kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-04-19 03:00:29,30
12959354,Views work for Hue to Views Migration Tool,"Problem: Hue is planned to be deprecated, hence need a way for customers to migrate over artifacts from Hue to the equivalent Views.
Link to the PRD Doc https://docs.google.com/a/hortonworks.com/document/d/13m-Ioxamq4oF3yYHASkVjJ3mhpRlKuM12BCgCAJaap4/edit?usp=sharing
This RMP is a clone specifically to cover the work on the Views Framework side.",ambari-views dev-test scoping,['ambari-views'],AMBARI,New Feature,Critical,2016-04-16 06:10:59,94
12958659,Ambari-Server Unit Test failures in trunk,"Unit test failure in ambari-server in trunk

https://builds.apache.org/job/Ambari-trunk-Commit/4650/console

Results :

Failed tests: 
  UpgradeCatalog240Test.testExecuteDDLUpdates:227 
  Unexpected method call DBAccessor.addColumn(""viewinstanceentity"", org.apache.ambari.server.orm.DBAccessor$DBColumnInfo@1a3492):
    DBAccessor.addColumn(""host_role_command"", capture(org.apache.ambari.server.orm.DBAccessor$DBColumnInfo@1fbce70)): expected: 1, actual: 1

Tests run: 4225, Failures: 1, Errors: 0, Skipped: 32",revert,['ambari-server'],AMBARI,Bug,Blocker,2016-04-13 23:23:42,41
12957737,Provide backward compatibility for kdc_host in krb5-conf for krb5.conf templates,"Provide backward compatibility for {{kdc_host}} in {{krb5-conf}} for {{krb5.conf}} templates.

This is necessary in the event the {{krb5.conf}} template is not updated during the Ambari 2.4.0 upgrade while the {{kdc_host}} property is renamed to {{kdc-hosts}}. 
",kerberos,['ambari-server'],AMBARI,Task,Major,2016-04-11 15:42:52,30
12957505,HIVE service_check doesn't work properly,"In templetonSmoke.sh, there are
1) unnecessary `exit 0`
2) lack of redirect `>` command
3) unassigned variable

we should correct them all to check the HIVE service properly.",patch-available,['ambari-server'],AMBARI,Bug,Major,2016-04-10 04:15:48,5
12957369,Assign Slaves and Clients Page: Incorrect validation error shown,"In certain combination of number of hosts and master component selection, incorrect validation error is shown on ui.
Following STR is one of the way to reproduce the issue:

*STR:*
# On 2 node cluster, Change Metrics Collector from the default host to another host.
# Click on next
# Assign Slaves and Clients page has validation error saying {color:red}Metrics Monitor component should be installed on all hosts in cluster.{color}

This happens because recommendation API called by ambari-web when user changed the host for Metrics collector does not have ""Metrics Monitor"" recommended for all hosts. 
*Expected behavior:* A component with cardinality ""ALL"" should be recommended on all hosts by component recommendation API irrespective of the data posted with the API.",regresion,['ambari-server'],AMBARI,Bug,Critical,2016-04-08 22:51:01,107
12956830,"configs.sh expands ***** in config values to a local file list, causing broken config files","When you try to get the value of, say, pig-log4j like this, it outputs correctly
{code}
curl -k -s -u $AMBARI_USER:$AMBARI_PASSWORD ""$AMBARI_HOST:$AMBARI_PORT/api/v1/clusters/$CLUSTER_NAME/configurations?type=pig-log4j&tag=version1""
{code}

If you use configs.sh to do the same
{code}
configs.sh -u $AMBARI_USER -p $AMBARI_PASSWORD -port $AMBARI_PORT get $AMBARI_HOST $CLUSTER_NAME pig-log4j
{code}

it will have replaced the ***** with the file list in the working directory

So,
{code}
 ""content"" : ""\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \""License\""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n# ***** Set root logger level to DEBUG and its only appender to A.\nlog4j.logger.org.apache.pig=info, A\n\n# ***** A is set to be a ConsoleAppender.\nlog4j.appender.A=org.apache.log4j.ConsoleAppender\n# ***** A uses PatternLayout.\nlog4j.appender.A.layout=org.apache.log4j.PatternLayout\nlog4j.appender.A.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n""
{code}

will be output as 
{code}
""content"" : ""\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements. See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership. The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \""License\""); you may not use this file except in compliance\n# with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied. See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n# bigr configs.sh gpfs mmls ssh symphony Set root logger level to DEBUG and its only appender to A.\nlog4j.logger.org.apache.pig=info, A\n\n# bigr configs.sh gpfs mmls ssh symphony A is set to be a ConsoleAppender.\nlog4j.appender.A=org.apache.log4j.ConsoleAppender\n# bigr configs.sh gpfs mmls ssh symphony A uses PatternLayout.\nlog4j.appender.A.layout=org.apache.log4j.PatternLayout\nlog4j.appender.A.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n""
{code}
As you can see, there are no ***** string in the output any longer, but instead it has been replaced with ""bigr configs.sh gpfs mmls ssh symphony"" which happened to be the local files in my working dir. 

It all comes from this line in configs.sh, line 241-247
{code}
    if [ ""$propertiesStarted"" -gt ""0"" ]; then
      if [ -z $FILENAME ]; then
        echo $line
      else
        echo $line >> $FILENAME
      fi
    fi
{code}
where the echo do not quote the $line to output",easyfix patch,['ambari-server'],AMBARI,Bug,Major,2016-04-07 10:51:04,108
12956188,Kerberos: Allow multiple KDC hosts to be set while enabling Kerberos,"Because multiple KDCs may exist for an installation (failover, high availability, etc...), Ambari should allow a user to specify multiple KDC hosts to be set while enabling Kerberos and updating the Kerberos service's configuration.

This should be done by allowing {{kerberos-env/kdc_host}} to accept a (comma-)delimited list of hosts and then parsing that list properly when building the krb5.conf file where each {{kdc_host}} item generates an entry in the relevant realm block.  For example:

{noformat:title=kerberos-env}
{
  ...
 ""kdc_hosts"" : ""kdc1.example.com, kdc2.example.com""
  ...
}
{noformat}

{noformat:title=krb5.conf}
[realms]
  EXAMPLE.COM = {
    ...
    kdc = kdc1.example.com
    kdc = kdc2.example.com
    ...
  }
{noformat}",kdc kerberos,['ambari-server'],AMBARI,Bug,Major,2016-04-05 15:47:54,30
12955569,YARN service_check doesn't fail when application status is not reasonable,"If yarn app state is not state or yarn app finalStatus is not succeeded, YARN service check should fail.

But in the YARN [service_check.py|https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/package/scripts/service_check.py#L130], it doesn't fail because raise statement is in try block and there is only `pass` in except block.",easyfix patch-available,['ambari-server'],AMBARI,Bug,Major,2016-04-03 08:36:30,41
12954050,hive-interactive-env: avoiding including copies of Hive-1.x classes,Spark-assembly jars contain Hive-1.x classes which break both rolling upgrades and  hive-2.0 installations.,Hive,['stacks'],AMBARI,Bug,Major,2016-03-28 18:50:30,109
12953637,"Use "">>"" instead of "">"" to write ambari-metrics-monitor.out","If we want to rotate ambari-metrics-monitor.out by logrotate (using copytruncate option), we need to use "">>"" instead of "">"" not to keep the file offset to write. If the offset is kept, null characters are padded in the new file.
{code:title=ambari-metrics-monitor}
    nohup ${PYTHON} ${METRIC_MONITOR_PY_SCRIPT} ""$@"" > ${OUTFILE} 2>&1 &
{code}
should be 
{code}
    nohup ${PYTHON} ${METRIC_MONITOR_PY_SCRIPT} ""$@"" >> ${OUTFILE} 2>&1 &
{code}",newbie,['ambari-metrics'],AMBARI,Improvement,Major,2016-03-25 18:24:04,67
12952995,RBAC based user access to view instances are not honoured,"Problem:
1. Create a cluster 
2. Create some view instances in amber
3. Create a local non-admin ambari user
4. Grant the newly created user access to one of the view instances

Log-in with the non-admin user. The user should only see the view instances it has permission instead of all view instances.

This seems to have been introduced by https://issues.apache.org/jira/browse/AMBARI-14194",rbac security,['ambari-server'],AMBARI,Bug,Critical,2016-03-23 21:49:39,30
12952185,/var/lib/ambari-agent/cache/cluster_configuration/configurations.json file contains various passwords in plain text in world readable file,"Various passwords are in plain text in world readable configurations.json file

$ ls -altr /var/lib/ambari-agent/cache/cluster_configuration/configurations.json

-rw-r--r-- 1 root root 176342 Mar  4 08:55 /var/lib/ambari-agent/cache/cluster_configuration/configurations.json

",security,['ambari-agent'],AMBARI,Bug,Major,2016-03-21 20:53:28,100
12951353,Auto-start services: Listen for changes to auto-start values and send them to the agent during heartbeat,"In HeartbeatHandler::handleHeartBeat(), send the recovery configuration to the agent. Instead of pulling the recovery configuration from the application cache, implement change events and get notified whenever there are changes to the recovery configuration.",ambari-server,['ambari-server'],AMBARI,Task,Major,2016-03-17 21:53:24,93
12951043,Change timeouts for Hbase and Phoenix,"Change a few timeouts in AMS Hbase and Phoenix to 5 minutes in Hbase-site.xml

hbase.client.scanner.timeout.period = 300000 
hbase.rpc.timeout = 300000
phoenix.query.timeoutMs = 300000
phoenix.query.keepAliveMs = 300000",ambari-metrics,['ambari-metrics'],AMBARI,Task,Major,2016-03-17 00:14:14,39
12950004,Fix ArtifactResourceProviderTest to avoid set ordering issues,"Due to the inconsistent ordering of resources in the set of Resources created in {{org.apache.ambari.server.controller.internal.ArtifactResourceProvider#getResources}}, {{org.apache.ambari.server.controller.internal.ArtifactResourceProviderTest#testDeleteResources}} fails since the test case expects a certain order in which the Resource are processed. 

Since it doesn't really make a difference what order the resources are processed when deleting, the test case should be fixed to not care about that. 

The follow test failure has been encountered due to this:

{noformat}
java.lang.AssertionError: 
  Unexpected method call ArtifactDAO.findByNameAndForeignKeys(""test-artifact2"", {cluster=500}):
    ArtifactDAO.findByForeignKeys({cluster=500}): expected: 1, actual: 1
    ArtifactDAO.findByNameAndForeignKeys(""test-artifact"", {cluster=500}): expected: 1, actual: 0
	at org.easymock.internal.MockInvocationHandler.invoke(MockInvocationHandler.java:44)
	at org.easymock.internal.ObjectMethodsFilter.invoke(ObjectMethodsFilter.java:94)
	at org.easymock.internal.ClassProxyFactory$MockMethodInterceptor.intercept(ClassProxyFactory.java:97)
	at org.apache.ambari.server.orm.dao.ArtifactDAO$$EnhancerByCGLIB$$40076a34.findByNameAndForeignKeys(<generated>)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProvider$4.invoke(ArtifactResourceProvider.java:377)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProvider$4.invoke(ArtifactResourceProvider.java:365)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:451)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:332)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProvider.deleteResources(ArtifactResourceProvider.java:245)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProviderTest.testDeleteResources(ArtifactResourceProviderTest.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
{noformat}
",unit-test,['ambari-server'],AMBARI,Bug,Major,2016-03-14 18:41:02,30
12948831,Provide composite Kerberos Descriptor via the REST API,"Provide an entry point in the REST API to retrieve the _composite_ Kerberos descriptor. This includes the default Kerberos descriptor built from the stack definitions overlaid with the (potentially sparse) Kerberos descriptor stored as an artifact of the cluster. 

The entry point should be a read-only sub-resource of ""clusters"", and should only be made available if asked for explicitly due to the size of the data that will be returned.

{noformat:title=Example API call}
GET /api/v1/clusters/:cluster_name/kerberos_descriptors/COMPOSITE
{noformat}

Note: Kerberos Descriptors available via this interface are:
* STACK - the default Kerberos Descriptor from the relevant stack definition
* USER - the user-suppled updates to the stack default Kerberos Descriptor
* COMPOSITE - the stack default Kerberos Descriptor with the user-suppled updates applied (this is the Kerberos Descriptor used when performing Kerberos-related operations)
",kerberos_descriptor rest_api,['ambari-server'],AMBARI,Task,Major,2016-03-10 15:50:19,30
12947689,Kerberos: Provide SHA256 or SHA512 options for template principal digest,"When generating accounts in an Active Directory, it may be useful to add a unique value to CN's.  In the past generating this value was done by taking the SHA1 hash of the relevant normalized principal name. For example {{ambari-qa-c1@EXAMPLE.COM}} yields {{d9b48cb1c075d3da9fab4855a4031266bab8fb6a}}.  

Because using SHA1 at all may not be desirable, Ambari should provide options to use the following digest algorithms in the Active Directory account creation attribute template ({{kerberos-env/ad_create_attributes_template}}:

||Attribute Variables||Example||
|$principal_digest|SHA1 hash of the $normalized_principal|
|$principal_digest_256|SHA256 hash of the $normalized_principal|
|$principal_digest_512|SHA512 hash of the $normalized_principal|

   ",active-directory active_directory kerberos,['ambari-server'],AMBARI,Bug,Major,2016-03-07 13:33:01,30
12947405,Host Service Summary Page does not display long service names well,"In Ambari UI, navigate to Hosts and click on the hostname where the service is installed. The summary page displays the list of services installed.
Observe that longer service names does not display well. The refresh icon etc. spills to the next line (see attachment ""Original spacing with long service name"").
Also, note the indentation of the last button for Clients does not align well with the buttons above it.",ambari-web,['ambari-web'],AMBARI,Bug,Minor,2016-03-05 02:24:52,110
12944719,Auto-start services: Create default settings for auto start,"Default settings for auto start should be made available via a configurable source like metainfo.xml.

*Stack definition file* _/var/lib/ambari-server/resources/common-services/<service_name>/<version>/metainfo.xml_ specifies whether a component  is enabled for auto start. Add a flag to each component - *<recovery_enabled>true</recovery_enabled>* - when it is auto start enabled. 

If recovery_enabled tag is not specified, the default value is true for slave components and false for master components.",ambari-server,['ambari-server'],AMBARI,Task,Major,2016-02-26 00:07:32,93
12944649,Authorization for Auto start services,"We need to add authorization for Auto start services.

*Solution*
* Users with the ability to modify cluster configurations ({{CLUSTER.MODIFY_CONFIGS}}) should be able to toggle the cluster-level auto start value.
* Users with the ability to start and stop services ({{SERVICE.START_STOP}}) should be able to toggle the service-level auto start value.

",rbac,['ambari-server'],AMBARI,Bug,Major,2016-02-25 20:04:36,30
12942990,UpgradeCatalog230 is not idempotent,"If ambari-server upgrade is run again, the following error is encountered. One easy way to test it is to set the version in the DB to an older version and then call ambari-server upgrade.

e.g. {{update metainfo set metainfo_value = '2.2.0' where neatinfo_key = 'version';}}
 
{code}
Error output from schema upgrade command:
Exception in thread ""main"" org.apache.ambari.server.AmbariException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""adminpermission_pkey""
Error Code: 0
Call: INSERT INTO adminpermission (permission_id, permission_label, permission_name, sort_order, resource_type_id) VALUES (?, ?, ?, ?, ?)
	bind => [5 parameters bound]
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:233)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:307)
Caused by: javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""adminpermission_pkey""
Error Code: 0
Call: INSERT INTO adminpermission (permission_id, permission_label, permission_name, sort_order, resource_type_id) VALUES (?, ?, ?, ?, ?)
	bind => [5 parameters bound]
	at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:157)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
	at org.apache.ambari.server.upgrade.UpgradeCatalog230.addNewPermissions(UpgradeCatalog230.java:144)
	at org.apache.ambari.server.upgrade.UpgradeCatalog230.executeDMLUpdates(UpgradeCatalog230.java:127)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:659)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:230)
	... 1 more
Caused by: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""adminpermission_pkey""
Error Code: 0
Call: INSERT INTO adminpermission (permission_id, permission_label, permission_name, sort_order, resource_type_id) VALUES (?, ?, ?, ?, ?)
	bind => [5 parameters bound]
	at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1611)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:898)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
	at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
{code}",upgrade,['ambari-server'],AMBARI,Bug,Critical,2016-02-24 15:55:04,30
12941304,Slider view: does not show the most recently executed app if there are many applications having same app name,"slider-view should show the most recently executed app if there are many applications having same appname.
but, application's id is compared by String compareTo (lexicographical order).
for example, there are two applications having same appname and these appids are “1453775865125_9"", “1453775865125_10”.
the application that appid is “1453775865125_9"" will be shown by lexicographical order .
that's not we expected.
",contrib,[],AMBARI,Bug,Minor,2016-02-23 06:59:34,111
12941099,Functionality to purge operational logs from the ambari database,"Ambari operational data may significantly grow over time and as a result the database operations may degrade.

This feature is about allowing operators to purge related tables based on configurable cleaning policies. (As a first step only time based cleanup is to be supported)",features,['ambari-server'],AMBARI,New Feature,Major,2016-02-22 16:42:35,95
12940989,Move NameNode wizard in HA environment fails to update configs correctly when namenode is not running on default port,"Condition:
1. hdfs deploy ha
2. ""dfs.namenode.http-address"" is not set to the default port 50070

Result:
1. Ambari will always substitute configures relative to ""nn2""
2. The active ""Move Namenode"" failed and make configure inconsistent, the cluster crashes

Code location:
{code:title=ambari-web/app/controllers/main/service/reassign/step4_controller.js|borderStyle=solid}

if (App.get('isHaEnabled')) {
      var nameServices = configs['hdfs-site']['dfs.nameservices'];
      var suffix = (configs['hdfs-site']['dfs.namenode.http-address.' + nameServices + '.nn1'] === sourceHostName + ':50070') ? '.nn1' : '.nn2';
      configs['hdfs-site']['dfs.namenode.http-address.' + nameServices + suffix] = targetHostName + ':50070';
      ...
    }
{code}",namenode,['ambari-web'],AMBARI,Bug,Critical,2016-02-22 09:10:04,112
12940764,Can't install ambari-server in Docker environment,"I tried to install and run ambari-server with Docker referring to [this|https://cwiki.apache.org/confluence/display/AMBARI/Development+in+Docker#DevelopmentinDocker-CreateDockerImage] then I got an error.

{code:none}
$ sudo docker rm ambari1
$ sudo docker run --privileged -t -p 80:80 -p 5005:5005 -p 8080:8080 -h node1.mydomain.com --name ambari1 -v $(pwd):/tmp/ambari ambari/build /tmp/ambari-build-docker/bin/ambaribuild.py server -b

...(snip)...

Traceback (most recent call last):
  File ""/tmp/ambari-build-docker/bin/ambaribuild.py"", line 239, in <module>
    retcode = install_ambari_server()
  File ""/tmp/ambari-build-docker/bin/ambaribuild.py"", line 52, in install_ambari_server
    cwd=""/tmp/ambari/ambari-server/target/rpm/ambari-server/RPMS/noarch"")
  File ""/usr/lib64/python2.6/subprocess.py"", line 642, in __init__
    errread, errwrite)
  File ""/usr/lib64/python2.6/subprocess.py"", line 1238, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory: '/tmp/ambari/ambari-server/target/rpm/ambari-server/RPMS/noarch'
{code}

I think ambari-server-*.rpm is x86_64, and is in ambari-server/target/rpm/ambari-server/RPMS/x86_64",easyfix patch,[],AMBARI,Bug,Minor,2016-02-20 09:25:11,99
12940709,Auto-start services: Add upgrade support for changes to servicecomponentdesiredstate table,New column recovery_enabled was added to servicecomponentdesiredstate. Upgrade support has to be enabled so the column is added to the table when upgrading to 2.4.0.,ambari-server,['ambari-server'],AMBARI,Task,Major,2016-02-19 23:57:18,93
12939911,Auto-start services: Backend API and DB changes for component auto start,"Add a new field *recovery_enabled* in *servicecomponentdesiredstate* and provide API support for PUT and GET.

*Type: PUT*
Request 1: api/v1/clusters/<cluster_name>/components?ServiceComponentInfo/component_name.in(<enabled_component_names>)
Request Params: application/json
{
	ServiceComponentInfo: {
		“recovery_enabled”: “true”
	}
}

Request 2:
api/v1/clusters/<cluster_name>/components?ServiceComponentInfo/component_name.in(<disabled_component_names>)
Request Params: application/json
{
	ServiceComponentInfo: {
		recovery_enabled: “false”
	}
}

Request 3:
api/v1/clusters/testcluster/components/<component_name> 
Request Params: application/json
'{""ServiceComponentInfo"" : {""recovery_enabled"":""false""}}'

*Type:GET*
Request: api/v1/clusters/<cluster_name>/components?fields=ServiceComponentInfo/component_name,ServiceComponentInfo/service_name,ServiceComponentInfo/category,ServiceComponentInfo/recovery_enabled
",ambari-server,['ambari-server'],AMBARI,Task,Major,2016-02-17 19:54:57,93
12938992, Improve Ambari user management for supporting large enterprise ,"Currently, Ambari user authentication is done via 2 modes:

1.  Ambari defined users (not necessarily local OS users) 
2.  LDAP users whose group and users have to be imported into Ambari

In both cases,  Ambari predefines the ""admin"" user that has admin role which is used for managing Ambari cluster and Ambari users.  Furthermore, Ambari maintains a separate user database independent of any other user directory such as the /etc/passwd file.  Even with LDAP integration, Ambari requires synching with the LDAP server users into Ambari's database.    Ambari's maintenance of this private user database is problematic especially  in a large enterprise environment where user management is often done thru group membership as employees change roles frequently. 

In this JIRA, we propose a two-pronged approach to simplify and enable enterprise class authentication support in Ambari.   In this proposal,  Ambari will provide support for PAM authentication, and in this PAM mode, it will no longer track individual Ambari users in its own database.  Ambari will only track groups and manage access control by granting access to groups.  When a user attemp to log in,  Ambari will authenticate the user via PAM.  Once authenticated, it will determine the group(s) that the user belong thru.   It then grants user permission based on the group information retrieved from PAM.

With PAM, LDAP can also be enabled via PAM-LDAP and  customer will no longer need to perform any synching action.
",security-groups,['ambari-server'],AMBARI,Epic,Major,2016-02-13 02:08:56,113
12938906,Return privilege information with results from GroupResourceProvider,"Return privilege information with results from GroupResourceProvider, which is invoked when issuing the following REST API call:
{noformat}
GET /api/v1/groups
{noformat}

The result set should looks something like:
{code}
{
  ""href"" : ""http://ambari-server:8080/api/v1/groups?fields=privileges/*"",
  ""items"" : [
    {
      ""href"" : ""http:///ambari-server:8080/api/v1/groups/group1"",
      ""Groups"" : {
        ""group_name"" : ""group1""
      },
      ""privileges"" : [
        {
          ""href"" : ""http://ambari-server:8080/api/v1/groups/group1/privileges/1"",
          ""PrivilegeInfo"" : {
            ""cluster_name"" : ""c1"",
            ""permission_label"" : ""Cluster User"",
            ""permission_name"" : ""CLUSTER.USER"",
            ""principal_name"" : ""group1"",
            ""principal_type"" : ""GROUP"",
            ""privilege_id"" : 3,
            ""type"" : ""CLUSTER"",
            ""group_name"" : ""group1""
          }
        }
      ]
    }
  ]
}
{code}",rbac,['ambari-server'],AMBARI,Task,Major,2016-02-12 20:12:14,30
12938869,KerberosDescriptorTest failed due to moved/missing test directory,"Unit tests in {{org.apache.ambari.server.stack.KerberosDescriptorTest}} fail due to moved/missing directory - {{.../ambari/ambari-server/target/classes/stacks}}.  

{{org.apache.ambari.server.stack.KerberosDescriptorTest}} should use {{{.../ambari/ambari-server/src/main/resources/stacks}} instead. ",unit-test,['ambari-server'],AMBARI,Bug,Major,2016-02-12 18:20:03,30
12938079,DB consistency: Add a consistency check on clusterconfigmapping in ClusterImpl::getDesiredConfigs() for NPE.,"In ClusterImpl::getDesiredConfigs(), log and skip the following scenario:

*Unmapped version_tag entries:* clusterconfigmapping -> clusterconfig cause NullPointerException in ClusterImpl.java ClusterImpl::getDesiredConfigs(boolean allVersions):
    c.setVersion(allConfigs.get(e.getType()).get(e.getTag()).getVersion());
",ambari-server,['ambari-server'],AMBARI,Bug,Critical,2016-02-10 04:19:32,93
12937979,Provide explicit ordering for roles,"Since it may be desired to order roles in ways other than alphabetically, each role should have an explicit numerical order that may be used by UI's. 

Roles should be explicitly ordered by the amount of access privileges they have. 


||Role Name||Explicit Order Value||
|Ambari Administrator|1|
|Cluster Administrator|2|
|Cluster Operator|3|
|Service Administrator|4|
|Service Operator|5|
|Cluster User|6|
|View User|7|


",rbac,['ambari-server'],AMBARI,Task,Major,2016-02-09 19:30:14,30
12937088,Some options have not been applied on Ambari Metrics Collector,"Ambari Metrics Collector's jinfo is as below:

{noformat}
[root@c6403 ~]# sudo -u ams /usr/jdk64/jdk1.8.0_60/bin/jinfo 18535
Attaching to process ID 18535, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.60-b23
Java System Properties:

proc_timelineserver =
java.runtime.name = Java(TM) SE Runtime Environment
java.vm.version = 25.60-b23
sun.boot.library.path = /usr/jdk64/jdk1.8.0_60/jre/lib/amd64
java.vendor.url = http://java.oracle.com/
java.vm.vendor = Oracle Corporation
path.separator = :
file.encoding.pkg = sun.io
java.vm.name = Java HotSpot(TM) 64-Bit Server VM
sun.os.patch.level = unknown
sun.java.launcher = SUN_STANDARD
user.country = US
user.dir = /home/ams
java.vm.specification.name = Java Virtual Machine Specification
java.runtime.version = 1.8.0_60-b27
java.awt.graphicsenv = sun.awt.X11GraphicsEnvironment
os.arch = amd64
java.endorsed.dirs = /usr/jdk64/jdk1.8.0_60/jre/lib/endorsed
line.separator =

java.io.tmpdir = /tmp
java.vm.specification.vendor = Oracle Corporation
os.name = Linux
sun.jnu.encoding = UTF-8
java.library.path = /usr/lib/ams-hbase/lib/hadoop-native -XX:+UseConcMarkSweepGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/ambari-metrics-collector/collector-gc.log-201602051642
java.specification.name = Java Platform API Specification
java.class.version = 52.0
java.net.preferIPv4Stack = true
sun.management.compiler = HotSpot 64-Bit Tiered Compilers
os.version = 2.6.32-358.el6.x86_64
...
{noformat}

It looks like {{AMS_COLLECTOR_GC_OPTS}} ( which is defined in ams-env.xml ) is interpreted as {{java.library.path}}.

This is caused by an initialization script {{/usr/sbin/ambari-metrics-collector}}.

{{AMS_COLLECTOR_OPTS}} should not be double-quoted.",patch,['ambari-metrics'],AMBARI,Bug,Major,2016-02-05 17:30:03,99
12937032,Some user-specified auth-to-local rules fail to render when auto generating auth-to-local rules,"When processing the user-specified auth-to-local rules while automatically generating auth-to-local rule sets using {{org.apache.ambari.server.controller.AuthToLocalBuilder}}, some rules fail to render.

This occurs when escape characters are used in the replacement pattern or replacement string of the rule, where a role looks like:

{code}
RULE: [n:string](regexp)s/pattern/replacement/[g]
{code}

To fix this issue, the regular expression used to parse the rules need to be augmented to allow for those characters.
",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-02-05 13:13:05,30
12936453,"Metrics not displayed for ""Last 1 year"" period.","PROBLEM
Metrics not displayed for Last 1 year period interval on Ambari.

BUG
The daily aggregator discards the last timestamp because it is fractionally more in the past than the cutOff time. We use 1*DAY as current cutoff. This can cause checkpoints to be dropped intermittently.

FIX
Change cutOff multiplier for daily aggregators from 1 to 2.",ambari-metrics,['ambari-metrics'],AMBARI,Bug,Major,2016-02-03 19:50:28,39
12936402,Add API directive to force toggling Kerberos if the cluster's security type has not changed,"Add an API directive to force toggling Kerberos if the cluster's security type has not changed.  

This is useful for _retry_ attempts to enable or disable Kerberos when the workflow progressed far enough to store the _new_ security type.  

Trying to enable Kerberos when the cluster's security type is already set to {{KERBEROS}} will result in a successful response from the Ambari server but no actions will be performed.  Same for attempting to disable Kerberos when the cluster's security type is already set to {{NONE}}.  

By forcing the operation using the {{force_toggle_kerberos=true}} directive, the security type check is avoided, thus allowing the _retry_ operation to proceed. 

Example: 
{code}
PUT /api/v1/clusters/CLUSTER_NAME?force_toggle_kerberos=true
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
    }
}
{code}",kerberos rest_api,['ambari-server'],AMBARI,Task,Major,2016-02-03 17:35:34,30
12935664,Increase timeout for server-side tasks,"Increase timeout for server-side tasks, or make the timeout configurable.

This is to help in the case where environmental issues cause Kerberos-related commands to take longer than usual, thus increasing the time it takes to process Kerberos server-side actions. If the Kerberos-server side actions timeout, Ambari fails the task and the user is stuck not able to perform the desired Kerberos-related action.",kerberos server-side,['ambari-server'],AMBARI,Bug,Critical,2016-02-01 16:36:37,30
12935652,Auto start: Maintenance mode of components should be respected when handling agent registration.,"Even if a service or component is set to auto start but is in maintenance mode, it should not auto start. Components in maintenance mode (because host/service is in maintenance mode) should not be enabled for auto start.

Current implementation:
In the current implementation we have both enabledComponents and disabledComponents. To determine if a specified component is enabled for recovery, an inclusive and exclusive test against enabledComponents and disabledComponents is used. There is no support for excluding a component in maintenance since that state is not available to the agent.

To determine if a component marked for auto start is in maintenance state, it's maintenance state has to be queried from the server, which makes it cumbersome and agent managed. 

New design:
1. Components to be enabled for auto start must be explicitly specified as comma separated values in the enabledComponents section of the recovery configuration. 
2. During agent registration, the server will determine if each of the components marked for auto start is in maintenance mode or not and filter the list before sending the list (enabledComponents) in the recovery configuration to the agent.",ambari-agent ambari-server,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2016-02-01 15:49:57,93
12934009,Register Version: add ability to load new version definition file,"This one belongs to UI work of Patch Upgrade ( Provide ability to apply single patches on top of RU release), and is the first part to implement"" Manage Ambari > Register Version"" page.
Should generate a new ""Register Version"" page, and add the ability to load new version definition file through two options:
1. Browse local files and then Upload file,
2. Enter the Url then click on ""Read Version File"" button,
After either option, the Version File (a .xml file) should be POST to server, then loaded (GET call) to the page content.

Be sure to load the mock data, will integrate API later.",Patch-Upgrade,['ambari-web'],AMBARI,Task,Critical,2016-01-26 01:43:35,102
12932381,Hive View: Hisotry filter: UI issues related to date range overlap and duration values,"Hive View: Issues from User interface perspective
1) History Tab On resizing windows Tab From and ToDate gets overlapped one above the other.
attached screen shot (HistoryTabOverlap)
2) History Tab Time(exec time) slider bar shows 0sec on both ends screenshot attached(HistoryTabTimeslider)",hive-view,['ambari-views'],AMBARI,Bug,Major,2016-01-19 11:03:01,114
12932343,TimelineServer configuration is missing in yarn-env.xml.,"In yarn-env.xml, {{YARN_HISTORYSERVER_OPTS}} and {{YARN_HISTORYSERVER_HEAPSIZE}} are not valid for TimelineServer.
{noformat:title=ambari-server/src/main/resources/stacks/HDP/2.3/services/YARN/configuration/yarn-env.xml}
      # Specify the max Heapsize for the HistoryManager using a numerical value
      # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set
      # the value to 1024.
      # This value will be overridden by an Xmx setting specified in either YARN_OPTS
      # and/or YARN_HISTORYSERVER_OPTS.
      # If not specified, the default value will be picked from either YARN_HEAPMAX
      # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.
      export YARN_HISTORYSERVER_HEAPSIZE={{apptimelineserver_heapsize}}
{noformat}
{{HISTORYSERVER}} should be {{TIMELINESERVER}}.",newbie,['stacks'],AMBARI,Bug,Major,2016-01-19 08:03:12,67
12932297,[Umbrella] Multi Everything Architecture,"*Multi Stack Services* (AMBARI-19621)
_Scenario: Deploy HDP & HDF services in same cluster_
- Deploy HDFS from HDP and Kafka, Storm, NiFI from HDF in the same cluster.

_Scenario: Deploy HDP services from different stack versions in same cluster_
- Deploy ZK, HDFS, YARN from HDP-2.5, but deploy latest SPARK from HDP-2.6.


*Multiple Service Instances* (AMBARI-20463)
_Scenario: Multi service instances on same version_
- Cluster includes instance of ZooKeeper vX which is being used by HDFS, YARN.
- User wants to add instance of ZooKeeper vX which is being used by STORM and KAFKA

_Scenario: Multi service instances on different versions_
- Cluster includes instance of SPARK vX.
- User wants to add additional instance of SPARK vY.

*Multi Host Component Instances* (AMBARI-20465)
_Scenario: Multi component instances from a service instance on the same host_
- Single host with 128GB RAM, 16 (actual) Cores, 12*4TB disks.
- Single instance of KAFKA broker is unable to utilize all the resources on the host. 
- User wants to scale up performance by deploying multiple instances of the KAFKA brokers/host.

*Multi Cluster* (AMBARI-20466)
_Scenario: Manage multiple Hadoop clusters under single Ambari Server_
- Customer has multiple small Hadoop clusters and would like to manage and monitor them with a single Ambari Server instance.

*Multi Yarn Hosted Services* (AMBARI-17353)
_Scenario: HBase on YARN_
- Deploy second instance of HBase as a long running YARN service.
- Manage YARN hosted service similar to traditional hosted services. 
- First class support for Yarn hosted services.

_Scenario: Credit Fraud Detection YARN Assembly_
- YARN Assembly can have its own ZK, KAFKA etc.
- Manage YARN Assemblies as first-class citizen.

*Multi Everything Aware APIs* (AMBARI-19620)
- V2 Ambari Rest API to support Multi Everything Architecture


",pull-request-available,"['ambari-agent', 'ambari-server', 'ambari-upgrade', 'ambari-web', 'stacks']",AMBARI,Epic,Critical,2016-01-19 02:55:38,41
12931653,disabling kerberos does not remove auth to local rules,"After disabling Kerberos to fix a user generated issue with a principal name pattern, the auth-to-local mapping(s) were not removed and thus not _fixing_ the issues that were caused: 

{noformat:title=Invalid hadoop.security.auth_to_local value}
 <property>
       <name>hadoop.security.auth_to_local</name>
       <value>RULE:[1:$1@$0](${hbase_user}@EXAMPLE.COM)s/.*/hbase/
 RULE:[1:$1@$0](${hdfs_user}@EXAMPLE.COM)s/.*/hdfs/
 RULE:[1:$1@$0](${smokeuser}@EXAMPLE.COM)s/.*/ambari-qa/
 RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
 RULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/ams/
 RULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/ams/
 RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/
 RULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/hbase/
 RULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/hive/
 RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/
 RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/
 RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/
 RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/
 RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/
 RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/
 RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/
 DEFAULT</value>
     </property>
{noformat}

{noformat:title=Errors in log}
2016-01-13 21:51:17,825 FATAL datanode.DataNode (DataNode.java:secureMain(2429)) - Exception in secureMain
java.util.regex.PatternSyntaxException: Illegal repetition near index 0
${hbase_user}@EXAMPLE.COM
^
        at java.util.regex.Pattern.error(Pattern.java:1924)
        at java.util.regex.Pattern.closure(Pattern.java:3104)
        at java.util.regex.Pattern.sequence(Pattern.java:2101)
        at java.util.regex.Pattern.expr(Pattern.java:1964)
        at java.util.regex.Pattern.compile(Pattern.java:1665)
        at java.util.regex.Pattern.<init>(Pattern.java:1337)
        at java.util.regex.Pattern.compile(Pattern.java:1022)
        at org.apache.hadoop.security.authentication.util.KerberosName$Rule.<init>(KerberosName.java:193)
        at org.apache.hadoop.security.authentication.util.KerberosName.parseRules(KerberosName.java:336)
        at org.apache.hadoop.security.authentication.util.KerberosName.setRules(KerberosName.java:397)
        at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:75)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:275)
        at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:311)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2192)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2242)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2422)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2446)
2016-01-13 21:51:17,830 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1
2016-01-13 21:51:17,832 INFO  datanode.DataNode (LogAdapter.java:info(45)) - SHUTDOWN_MSG:
/************************************************************
{noformat}

The auth-to-local mappings should be removed when Kerberos is disabled.",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-01-16 11:40:13,30
12931590,Kerberos automation logic should use stack advisor when determining configuration updates,"Kerberos automation logic should use the stack advisor when determining configuration updates.  This will ensure that property updates are valid given the cluster's configuration and whether the cluster was created using the API, UI, or BluePrints.
",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-01-15 22:29:43,30
12930231,Replace Python client with a working/useful one,"The Python client has bitrotted pretty seriously and seems to be abandoned.  I have created one on Github that has kept up pretty well and does nearly everything the API offers (there might be some newer features yet unexplored).  I talked with Alejandro about contributing it and he was in favor, so I'm doing that now.",ambari-client python,[],AMBARI,Story,Major,2016-01-14 19:05:56,115
12928737,ambari-funtest: Windows profile required in pom.xml,ambari-funtest project fails to build on Windows because the pom.xml does not contain a profile for Windows.,patch,['ambari-server'],AMBARI,Bug,Major,2016-01-08 21:19:44,93
12924942,CentOS 7.2/RHEL 7.2 ambari-server systemd script is broken,"systemd was updated in RHEL 7.2 and thus also in CentOS 7.2.  This update breaks compatibility with how ambari-server starts up.  There is a simple fix, just create the file /usr/lib/systemd/system/ambari-server.service with these contents:

{noformat}
[Unit]
Description=ambari-server service
After=xe-linux-distribution.service

[Service]
Type=forking
ExecStart=/usr/sbin/ambari-server start
ExecStop=/usr/sbin/ambari-server stop

[Install]
WantedBy=multi-user.target
{noformat}

There's probably a better way to work with systemd in a more systemd-ish way, but this restores compatibility for now with minimal effort.  We should update the RPM to install this file if the system is managed by systemd.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2015-12-30 20:02:30,98
12924678,Failed to deploy Kerberized cluster via blueprint with custom principal name,"Failed to deploy Kerberized cluster via Blueprint with custom principal name set in Kerberos descriptor declared in _Cluster Creation Template_ like 

{code}
...
  ""security"": {
    ""type"": ""KERBEROS"",
    ""kerberos_descriptor"": {
      ""identities"": [
        {
          ""name"": ""smokeuser"",
          ""principal"": {
            ""value"": ""smokeuser9jJevBQAYGQWnRkuapSEp@${realm}""			
          }
        }
      ]
    }
  },
...
{code}

The following error (shown in ambari-server.log) was encountered while the cluster was being built: 
{noformat}
Failed to create keytab for smokeuser9jJevBQAYGQWnRkuapSEp@EXAMPLE.COM, missing cached file
{noformat}

h3. Cause
This was caused because the Kerberos descriptor in the _Blueprint_ or _Cluster Creation Template_ did not declare the type property of the principal being updated.  This caused the logic in Ambari to assume the principal was a _service_ principal rather than a _user_ (or headless) principal.  Because of this, when merging the updates to the default Kerberos descriptor (from the stack), the {{smokeuser}} principal type was changed _user_ to _service_.  Thus it was skipped over when the _Blueprints_ process executed the phase to ensure (and cache) headless identities.  

The bug is in the logic parsing the Kerberos descriptor.  By not specifying a principal type, the logic assumes the principal type is _service_; however in this case, the principal type needs to be {{null}}, so that when the user-specified Kerberos descriptor is merged with the default Kerberos descriptor, the default principal type is not changed. 

*NOTE:* This is not limited to _Blueprints_, the issue will cause issues if the specified Kerberos descriptor artifact is missing {{principal/type}} properties as well.  See [Set the Kerberos Descriptor|https://cwiki.apache.org/confluence/display/AMBARI/Automated+Kerberizaton#AutomatedKerberizaton-SettheKerberosDescriptor]

h3. Solution
Change the logic it the Kerberos descriptor parser to allow for the principal type to be {{null}}. Handle this value being {{null}} by consumers of this data such that {{null}} indicates the default value of {{service}}. This will keep the current behavior consistent, and also allow for the merging facility to properly merge principal updates - like changing the principal name pattern without needing to specify the principal type as well. 

h3. Workaround
Explicitly set the {{type}} property of _user_ (or headless) principals in the Kerberos descriptor:
{code}
...
  ""security"": {
    ""type"": ""KERBEROS"",
    ""kerberos_descriptor"": {
      ""identities"": [
        {
          ""name"": ""smokeuser"",
          ""principal"": {
            ""type"" : ""USER"",
            ""value"": ""smokeuser9jJevBQAYGQWnRkuapSEp@${realm}""			
          }
        }
      ]
    }
  },
...
{code}

",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-12-29 12:52:27,30
12923643,Make the hdfs replication as 1 when it is single node cluster,By default the replication is 3 which doesn't work when it is single node cluster. I meet this issue when I install hdp in one single node.  ,ambari-server newbie stack,[],AMBARI,Improvement,Major,2015-12-22 02:26:06,107
12923615,SERVICE.MANAGE_CONFIG_GROUPS missing from CLUSTER.ADMINISTRATOR and AMBARI.ADMINISTRATOR roles in MySQL create script,"SERVICE.MANAGE_CONFIG_GROUPS missing from CLUSTER.ADMINISTRATOR and AMBARI.ADMINISTRATOR roles in MySQL create script.
",rbac,[],AMBARI,Bug,Blocker,2015-12-22 00:12:33,30
12921052,Enforce granular role-based access control for ldap-sync functions,"Enforce granular role-based access control for ldap sync functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Sync LDAP users                   |   |   |   |   |   | (+)| 
|Sync LDAP groups                   |   |   |   |   |   |(+)| 

Entry points affected:
GET  /api/v1/ldap_sync_events
POST  /api/v1/ldap_sync_events
DELETE  /api/v1/ldap_sync_events/:event_id
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-11 13:01:04,6
12920870,Ambari local admin endpoint doesn’t work when sso is enabled.," Ambari local admin endpoint doesn’t work when sso is enabled.

",test,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2015-12-10 20:02:05,7
12920800,Enforce granular role-based access control for configuration functions,"Enforce granular role-based access control for configuration functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View service configurations           |(+)|(+)|(+)|(+)|(+)|(+)|
|Compare service configurations        |(+)|(+)|(+)|(+)|(+)|(+)|
|Manage configuration groups   |   |   |(+)|(+)|(+)|(+)|
|View host configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View cluster configuration            |(+)|(+)|(+)|(+)|(+)|(+)|

Entry points affected:
- GET /api/v1/clusters/:clusterId/configurations
- POST /api/v1/clusters/:clusterId/configurations
- GET /api/v1/clusters/:clusterId/configurations/service_config_versions
- GET /api/v1/clusters/:clusterId/config_groups
- GET /api/v1/clusters/:clusterId/config_groups/:groupId
- POST /api/v1/clusters/:clusterId/config_groups
- PUT /api/v1/clusters/:clusterId/config_groups/:groupId
- DELETE /api/v1/clusters/:clusterId/config_groups/:groupId",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-10 15:59:56,6
12920492,Change authorization resource Id to be adminresource id,The RBAC changes attempt to perform authorization checks on resource-specific identifiers. This is not the intended use of the authorization (admin*/auth*) tables as the resource's adminresource id is to be looked up and an authorization check is to be performed on that. ,rbac,['ambari-server'],AMBARI,Bug,Critical,2015-12-09 17:21:42,30
12919716,"Entity removal done trough the EntityManager (AlertHistory, AlertNotice)",Removal of entities need to be done through the EntityManager in order for the shared cache to remain consistent.,patch,['ambari-server'],AMBARI,Bug,Critical,2015-12-07 14:42:33,95
12918951,User with SERVICE.MODIFY_CONFIGS authorization fails with 403 status updating configs,"User with SERVICE.MODIFY_CONFIGS authorization fails with 403 status updating configs

{noformat:title=PUT /api/v1/clusters/:cluster_name}
[
  {
    ""Clusters"": {
      ""desired_config"": [
        {
          ""type"": ""zoo.cfg"",
          ""tag"": ""version1449226525346"",
          ""properties"": {
            ""autopurge.purgeInterval"": ""24"",
            ""autopurge.snapRetainCount"": ""30"",
            ""dataDir"": ""/hadoop/zookeeper"",
            ""tickTime"": ""2000"",
            ""initLimit"": ""11"",
            ""syncLimit"": ""5"",
            ""clientPort"": ""2181""
          },
          ""service_config_version_note"": """"
        }
      ]
    }
  }
]
{noformat}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2015-12-04 11:15:51,30
12918102,Enforce granular role-based access control for ldap-sync functions,"Enforce granular role-based access control for ldap sync functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Sync LDAP users                   |   |   |   |   |   | (+)| 
|Sync LDAP groups                   |   |   |   |   |   |(+)| 

Entry points affected:
* TBD
",ldap rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:26:20,6
12918099,Enforce granular role-based access control for Views functions,"Enforce granular role-based access control for Views functions:

|| || View\\User || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Manage Ambari Views           |   |   |   |   |   |   |(+)|
|Use  Ambari View           |(+) |   |   |   |   |   |   |

Entry points affected:
* TBD
",rbac views,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:22:00,30
12918096,Enforce granular role-based access control for configuration functions,"Enforce granular role-based access control for configuration functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View service configurations           |(+)|(+)|(+)|(+)|(+)|(+)|
|Compare service configurations        |(+)|(+)|(+)|(+)|(+)|(+)|
|Manage configuration groups   |   |   |(+)|(+)|(+)|(+)|
|View host configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View cluster configuration            |(+)|(+)|(+)|(+)|(+)|(+)|

Entry points affected:
* TBD
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:16:10,6
12918094,Enforce granular role-based access control for metrics functions,"Enforce granular role-based access control for metrics functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View service metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View host metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View cluster metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|

Entry points affected:
* TBD
",rbac1,['ambari-web'],AMBARI,Task,Major,2015-12-03 19:10:11,43
12918092,Enforce granular role-based access control for host functions,"Enforce granular role-based access control for host functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Turn on/off maintenance mode  |   |   |   |(+)|(+)|(+)|
|Add/Delete hosts              |   |   |   |(+)|(+)|(+)|
|View host details              |(+)|(+)| (+)|(+)|(+)|(+)|

Entry points affected:
* GET api/vi/hosts
* GET api/vi/hosts/:host_name
* POST api/vi/hosts/:host_name
* PUT api/vi/hosts/:host_name
* DELETE api/vi/hosts/:host_name
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:05:13,30
12918091,Enforce granular role-based access control for service functions,"Enforce granular role-based access control for service functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Start/Stop/Restart Service    |   |(+)|(+)|(+)|(+)|(+)|
|Decommission/recommission     |   |(+)|(+)|(+)|(+)|(+)|
|Run service checks            |   |(+)|(+)|(+)|(+)|(+)|
|Turn on/off maintenance mode  |   |(+)|(+)|(+)|(+)|(+)|
|Perform service-specific tasks|   |(+)|(+)|(+)|(+)|(+)|
|Move to another host          |   |   |(+)|(+)|(+)|(+)|
|Enable HA                     |   |   |(+)|(+)|(+)|(+)|
|Add Service to cluster        |   |   |   |   |(+)|(+)|
|Install components            |   |   |   |(+)|(+)|(+)|
|Modify service configurations         |   |   |(+)|(+)|(+)|(+)|
|Set service users and groups  |   |   |   |   |   |(+)|

Entry points affected:
* GET /api/v1/clusters/:cluster_name/services
* GET /api/v1/clusters/:cluster_name/services/:service_name
* POST /api/v1/clusters/:cluster_name/services/:service_name
* PUT /api/v1/clusters/:cluster_name/services/:service_name
* DELETE /api/v1/clusters/:cluster_name/services/:service_name
* GET /api/v1/clusters/:cluster_name/services/:service_name/components
* GET /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* POST /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* PUT /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* DELETE /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* POST /api/v1/clusters/:cluster_name/hosts
* POST /api/v1/clusters/:cluster_name/requests",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:01:58,30
12917646,Enforce granular role-based access control for credential functions,"Enforce granular role-based access control for credential functions

Entry points affected:
* GET /clusters/:cluster_name/credentials
* GET /clusters/:cluster_name/credentials/:alias
* POST /clusters/:cluster_name/credentials/:alias
* PUT /clusters/:cluster_name/credentials/:alias
* DELETE /clusters/:cluster_name/credentials/:alias
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-02 16:58:55,30
12917364,FK_permission_roleauthorization_permission_id is too long for a constraint identifier,"{noformat}
ALTER TABLE permission_roleauthorization ADD CONSTRAINT FK_permission_roleauthorization_permission_id FOREIGN KEY (permission_id) REFERENCES adminpermission(permission_id)
Error report -
SQL Error: ORA-00972: identifier is too long
00972. 00000 -  ""identifier is too long""
*Cause:    An identifier with more than 30 characters was specified.
*Action:   Specify at most 30 characters.
{noformat}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2015-12-01 20:05:51,30
12917354,Enforce granular role-based access control for group functions,"Enforce granular role-based access control for alert functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Manage groups                 |   |   |   |   |   |(+)|

Entry points affected: 
* GET /api/v1/groups
* GET /api/v1/groups/:group_name
* PUT /api/v1/groups/:group_name
* POST /api/v1/groups/:group_name
* DELETE /api/v1/groups/:group_name
* GET /api/v1/groups/:group_name/members
* PUT /api/v1/groups/:group_name/members
* POST /api/v1/groups/:group_name/members
* DELETE /api/v1/groups/:group_name/members",rbac,['ambari-server'],AMBARI,Task,Major,2015-12-01 19:28:16,30
12917350,Enforce granular role-based access control for alert functions,"Enforce granular role-based access control for alert functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View alerts (service)        |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable alerts (service)          |   |   |(+)|(+)|(+)|(+)|
|View alerts (cluster)                   |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable alerts (cluster)         |   |   |   |   |(+)|(+)|

Entry points affected: {color:red}TBD{color}
",rbac,['ambari-server'],AMBARI,Task,Major,2015-12-01 19:15:15,30
12916980,Enforce granular role-based access control for stack version functions,"Enforce granular role-based access control for stack version functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View stack version details    |(+)|(+)|(+)|(+)|(+)|(+)|
|Manage stack versions         |   |   |   |   |   |(+)|
|Edit stack repository URLs    |   |   |   |   |   |(+)|

Entry points affected:
* GET /api/v1/stacks/:stack_name/versions/:version_id
* GET /api/v1/stacks/:stack_name/versions/:version_id
* PUT /api/v1/stacks/:stack_name/versions/:version_id
* POST /api/v1/stacks/:stack_name/versions/:version_id
* DELETE /api/v1/stacks/:stack_name/versions/:version_id
",rbac security,['ambari-server'],AMBARI,Task,Major,2015-11-30 15:15:38,30
12916193,Enforce granular role-based access control for cluster functions,"Enforce granular role-based access control for cluster functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View stack version details    |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable Kerberos       |   |   |   |   |(+)|(+)|
|Upgrade/downgrade stack       |   |   |   |   |(+)|(+)|
|Create new clusters           |   |   |   |   |   |(+)|
|Rename clusters               |   |   |   |   |   |(+)|

Entry points affected:
* PUT /api/v1/clusters/:cluster_name 
* POST /api/v1/clusters/:cluster_name 

*Note: Read-only requests (GET) are not protected so that the front end is not broken.*
",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-25 16:10:32,30
12915829,Change Anonymous API Authentication To A Declared User,"When using {{api.authenticate=false}}, REST requests to the Ambari APIs don't need to contain any user information. As a result, new code being placed which assumes an authenticated user will throw NPE exceptions:

{code}
      // Ensure that the authenticated user has authorization to get this information
      if (!isUserAdministrator && !AuthorizationHelper.getAuthenticatedName().equalsIgnoreCase(userName)) {
        throw new AuthorizationException();
      }
{code}

{code}
java.lang.NullPointerException
	at org.apache.ambari.server.controller.internal.ActiveWidgetLayoutResourceProvider.getResources(ActiveWidgetLayoutResourceProvider.java:156)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:946)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:132)
	at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:512)
	at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:381)
	at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:217)
{code}

Recommend changing this option to something like

{code}
api.authenticated.user=admin
{code}

This will preserve the existing functionality while allowing the new code to continue to assume authenticated users.",api authentication rbac,['ambari-server'],AMBARI,Bug,Major,2015-11-24 15:51:26,30
12915246,Express Upgrade - SQL Exception while trying to DeregisterVersion (after install packages step failed due to invalid repo),"Problem :

*+Steps :+*
* Through Manage Versions add a new HDP repo e.g. http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/2.x/updates/2.3.4.0
* Since i was sure this repo will exist so skipped repository check
* executed install packages.
* After it was completed, Installed count was 0 and Not Installed Count was 5
* Went to the *Manage Version* to modify the repo location. http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/2.x/BUILDS/2.3.4.0-3305
*   Since the save action failed with exception ""Version update error""
* Pressed on cancel and then DeRegister the Version, which failed with error

*+ambari-server log snapshot+*

look at below logs for sqlException
{code}
21 Nov 2015 22:57:30,978 ERROR [qtp-client-1042] AmbariManagementControllerImpl:3687 - Could not access base url . http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/2.x/updates/2.3.4.0 . Server returned HTTP response code: 403 for URL: http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml
21 Nov 2015 22:57:59,348  WARN [pool-4-thread-1] DistributeRepositoriesActionListener:106 - Distribute repositories did not complete, will set all INSTALLING versions for host pankaj-upgrade-eu-4.novalocal to INSTALL_FAILED.
21 Nov 2015 22:58:06,298  WARN [pool-4-thread-1] DistributeRepositoriesActionListener:106 - Distribute repositories did not complete, will set all INSTALLING versions for host pankaj-upgrade-eu-3.novalocal to INSTALL_FAILED.
21 Nov 2015 22:58:07,538  WARN [pool-4-thread-1] DistributeRepositoriesActionListener:106 - Distribute repositories did not complete, will set all INSTALLING versions for host pankaj-upgrade-eu-1.novalocal to INSTALL_FAILED.
21 Nov 2015 22:58:09,346  WARN [pool-4-thread-1] DistributeRepositoriesActionListener:106 - Distribute repositories did not complete, will set all INSTALLING versions for host pankaj-upgrade-eu-5.novalocal to INSTALL_FAILED.
21 Nov 2015 22:58:10,256  WARN [pool-4-thread-1] DistributeRepositoriesActionListener:106 - Distribute repositories did not complete, will set all INSTALLING versions for host pankaj-upgrade-eu-2.novalocal to INSTALL_FAILED.
21 Nov 2015 22:58:54,618 ERROR [qtp-client-25] AbstractResourceProvider:338 - Caught AmbariException when modifying a resource
org.apache.ambari.server.AmbariException: Upgrade pack can't be changed for repository version which has a state of INSTALL_FAILED on cluster cl1
	at org.apache.ambari.server.controller.internal.RepositoryVersionResourceProvider$5.invoke(RepositoryVersionResourceProvider.java:261)
	at org.apache.ambari.server.controller.internal.RepositoryVersionResourceProvider$5.invoke(RepositoryVersionResourceProvider.java:235)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:450)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:331)
	at org.apache.ambari.server.controller.internal.RepositoryVersionResourceProvider.updateResources(RepositoryVersionResourceProvider.java:235)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:310)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.update(PersistenceManagerImpl.java:104)
	at org.apache.ambari.server.api.handlers.UpdateHandler.persist(UpdateHandler.java:42)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:106)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:75)
	at org.apache.ambari.server.api.services.RepositoryVersionService.updateRepositoryVersion(RepositoryVersionService.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	……
21 Nov 2015 22:59:53,301 ERROR [qtp-client-25] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:
Local Exception Stack:
Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_CLUSTER_VERSION_REPOVERS_ID) violated - child record found

Error Code: 2292
Call: DELETE FROM repo_version WHERE (repo_version_id = ?)
	bind => [1 parameter bound]
	at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:331)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:900)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
	at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
	at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
	at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
	at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72)
	at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52)
	at org.apache.ambari.server.orm.dao.RepositoryVersionDAO$$EnhancerByGuice$$18a36950.remove(<generated>)
	at org.apache.ambari.server.controller.internal.RepositoryVersionResourceProvider.deleteResources(RepositoryVersionResourceProvider.java:333)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.deleteResources(ClusterControllerImpl.java:330)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.delete(PersistenceManagerImpl.java:111)
	at org.apache.ambari.server.api.handlers.DeleteHandler.persist(DeleteHandler.java:44)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:106)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:75)
	at org.apache.ambari.server.api.services.RepositoryVersionService.deleteRepositoryVersion(RepositoryVersionService.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	…..
Caused by: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_CLUSTER_VERSION_REPOVERS_ID) violated - child record found

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
	at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
	at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
	... 108 more
21 Nov 2015 22:59:53,304 ERROR [qtp-client-25] AmbariJpaLocalTxnInterceptor:122 - [DETAILED ERROR] Internal exception (1) :
java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_CLUSTER_VERSION_REPOVERS_ID) violated - child record found

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
	at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
	at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
	at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
	at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
	at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
	at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72)
	at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52)
	at org.apache.ambari.server.orm.dao.RepositoryVersionDAO$$EnhancerByGuice$$18a36950.remove(<generated>)
	at org.apache.ambari.server.controller.internal.RepositoryVersionResourceProvider.deleteResources(RepositoryVersionResourceProvider.java:333)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.deleteResources(ClusterControllerImpl.java:330)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.delete(PersistenceManagerImpl.java:111)
	at org.apache.ambari.server.api.handlers.DeleteHandler.persist(DeleteHandler.java:44)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:106)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:75)
	
	
21 Nov 2015 22:59:53,309  WARN [qtp-client-25] ServletHandler:563 - /api/v1/stacks/HDP/versions/2.3/repository_versions/52
javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_CLUSTER_VERSION_REPOVERS_ID) violated - child record found

Error Code: 2292
Call: DELETE FROM repo_version WHERE (repo_version_id = ?)
	bind => [1 parameter bound]
	at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:157)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
	at org.apache.ambari.server.controller.internal.RepositoryVersionResourceProvider.deleteResources(RepositoryVersionResourceProvider.java:333)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.deleteResources(ClusterControllerImpl.java:330)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.delete(PersistenceManagerImpl.java:111)
	at org.apache.ambari.server.api.handlers.DeleteHandler.persist(DeleteHandler.java:44)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:106)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:75)
	at org.apache.ambari.server.api.services.RepositoryVersionService.deleteRepositoryVersion(RepositoryVersionService.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	
Caused by: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_CLUSTER_VERSION_REPOVERS_ID) violated - child record found

Error Code: 2292
Call: DELETE FROM repo_version WHERE (repo_version_id = ?)
	bind => [1 parameter bound]
	at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:331)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:900)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
	at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
	at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
	at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
	at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
	... 93 more
Caused by: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_CLUSTER_VERSION_REPOVERS_ID) violated - child record found

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
	at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
	at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
	at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
	... 105 more
{code}",upgrade,['ambari-server'],AMBARI,Bug,Critical,2015-11-23 07:04:50,13
12915084,Fix translation of different view resource type entities to all be of type VIEW,"Fix translation of different view resource type entities to all be of type VIEW. 

When an Ambari view is added to Ambari, a new resource type ({{adminresourcetype}}) is added.  Each of these types need to resolve to represent a _view_  resource ({{org.apache.ambari.server.security.authorization.ResourceType#VIEW}} so that authorization checks can be performed properly. 

For example:
{noformat}
 resource_type_id |    resource_type_name
------------------+---------------------------
                1 | AMBARI
                2 | CLUSTER
                3 | VIEW
                5 | ADMIN_VIEW{2.1.2}
                6 | FILES{1.0.0}
                7 | PIG{1.0.0}
                8 | CAPACITY-SCHEDULER{1.0.0}
                9 | TEZ{0.7.0.2.3.2.0-377}
               10 | SLIDER{2.0.0}
               11 | HIVE{1.0.0}
               55 | ADMIN_VIEW{2.2.0.0}
               56 | TEZ{0.7.0.2.3.2.0-3539}
{noformat}

The translation needs to be be:

{noformat}
AMBARI                    | ResourceType.AMBARI
CLUSTER                   | ResourceType.CLUSTER
VIEW                      | ResourceType.VIEW
ADMIN_VIEW{2.1.2}         | ResourceType.VIEW
FILES{1.0.0}              | ResourceType.VIEW
PIG{1.0.0}                | ResourceType.VIEW
CAPACITY-SCHEDULER{1.0.0} | ResourceType.VIEW
TEZ{0.7.0.2.3.2.0-377}    | ResourceType.VIEW
SLIDER{2.0.0}             | ResourceType.VIEW
HIVE{1.0.0}               | ResourceType.VIEW
ADMIN_VIEW{2.2.0.0}       | ResourceType.VIEW
TEZ{0.7.0.2.3.2.0-3539}   | ResourceType.VIEW
{noformat}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2015-11-21 12:22:21,30
12915078,Return HTTP 403 on REST API authorization failures,"Return HTTP 403 on REST API authorization failures.

{code:title=403 Forbidden} 
{
  ""status"" : 403,
  ""message"" : ""The authenticated user is not authorized to perform the requested operation""
}
{code}
",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-21 10:52:45,30
12914689,Enforce granular role-based access control for user functions,"Enforce granular role-based access control for user functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Create new clusters           |   |   |   |   |   |(+)|
|Manage users                  |   |   |   |   |   |(+)|
|Assign permissions/roles      |   |   |   |   |   |(+)|

Entry points affected:
* GET /api/v1/users/:username
* GET /api/v1/users/:username/widget_layouts
* GET /api/v1/users/:username/privileges
* POST /api/v1/users/:username
* DELETE /api/v1/users/:username
* PUT /api/v1/users/:username
* GET /api/v1/priviliges
* POST /api/v1/priviliges
* GET /api/v1/priviliges/:privilege_id
* DELETE /api/v1/priviliges/:privilege_id
* PUT /api/v1/priviliges/:privilege_id
* GET /api/v1/clusters/:cluster_name/priviliges
* GET /api/v1/clusters/:cluster_name/priviliges/:privilege_id
* POST /api/v1/clusters/:cluster_name/priviliges
* DELETE /api/v1/clusters/:cluster_name/priviliges/:privilege_id
* PUT /api/v1/clusters/:cluster_name/priviliges/:privilege_id
",rbac security,['ambari-server'],AMBARI,Task,Major,2015-11-20 00:32:18,30
12914157,Create base infrastructure to allow for granular role based access control,"Create base infrastructure to allow for granular role based access control. 

This entails creating a base class to help with authorization checks.  The base class is to contain a default authorization check implementation but allow derived classes to override the logic to implement more sophisticated checks. 
",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-18 16:30:09,30
12913466,Create API entry points for getting authorization information,"Create API entry points for getting authorization information.  

The following entry points need to be created:
{noformat}
GET /api/v1/authorizations
GET /api/v1/permissions/:PERMISSION_ID/authorizations
GET /api/v1/users/:USERNAME/authorizations
{noformat}

The following entry points need to be updated to supply more information:
{noformat}
GET /api/v1/permissions/:PERMISSION_ID
{noformat}

*Note: The data layer is stubbed out and left for implementation in AMBARI-13865*",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-16 21:42:13,30
12912644,Not possible to pause  upgrade if  Finalize Upgrade Pre-Check failed due MM host,"*STR*
# deploy cluster with host witout masters
# turn Maintains mod on host
# perform upgrade to finalize 
*RESULT*
Finalize Upgrade Pre-Check failed due to MM host not updated
there are no possibility to pause  upgrade
as result not possible to delete MM host",fpc system_test,['ambari-web'],AMBARI,Bug,Blocker,2015-11-12 22:41:46,102
12912592,Add authorizations to permissions so that the definition of a permission (or role) is explicit,"Add authorizations to permissions so that the definition of a permission (or role) is explicit.

A new table needs to be created to store the _authorizations_:
{code}
TABLE roleauthorization (
  authorization_id VARCHAR(100) NOT NULL,
  authorization_name VARCHAR(255) NOT NULL,
  resource_type_id INTEGER NOT NULL,
  PRIMARY KEY(authorization_id)
)
{code}

A new table needs to be added to map _authorizations_ to _permissions_
{code}
TABLE permission_roleauthorization (
  permission_id BIGINT NOT NULL,
  authorization_id VARCHAR(100) NOT NULL,
  PRIMARY KEY(permission_id, authorization_id)
);
{code}

A new Entity needs to be created to hold the authorization record data ({{org.apache.ambari.server.orm.entities.AuthorizationEntity}}).

The existing PermissionEntity {{org.apache.ambari.server.orm.entities.PermissionEntity}} needs to be updated to include AuthorizationEntities.
",rbac,[],AMBARI,Task,Major,2015-11-12 20:11:18,30
12912338,Need IDs or classes for messages in Rolling Upgrade window,"STR: 
1) Start Rolling Upgrade with Skip all Service Check failures and Skip all Slave Component failures
2) Break some Core Slaves, for example, DataNode
3) Waiting for the message (screen added)

I need at least some distinctive features for these messages to write xpathes ",fpc system-tests,['ambari-web'],AMBARI,Bug,Blocker,2015-11-11 23:27:20,102
12912237,Support WebHDFS over SSL in Ambari Views,Currently Ambari Views do not support file system schema for swebhdfs - webhdfs over SSL.,files filesystem,['ambari-views'],AMBARI,Bug,Major,2015-11-11 18:12:38,91
12911874,Add (descriptive) label to permission resource and database schema,"Add (descriptive) label to permission resource and database schema to avoid having to hardcode a descriptive name for a permission. For example:


||Permission Name||Permission Label||
|VIEW.USER|View User|
|CLUSTER.USER|Cluster User|
|SERVICE.OPERATOR|Service Operator|
|SERVICE.ADMINISTRATOR|Service Administrator|
|CLUSTER.OPERATOR|Cluster Operator|
|CLUSTER.ADMINISTRATOR|Cluster Administrator|

This descriptive label can be used in user interfaces so that a descriptive (or friendly) name does not have to be hardcoded. 

{noformat:title=API Call Example}
GET api/v1/permissions/1

200 OK
{
  ""href"" : ""http://your.ambari.server/api/v1/permissions/1"",
  ""PermissionInfo"" : {
    ""permission_id"" : 1,
    ""permission_name"" : ""AMBARI.ADMIN"",
    ""permission_label"" : ""Administrator"",
    ""resource_name"" : ""AMBARI""
  }
}
{noformat}",rbac,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2015-11-10 15:08:38,30
12911539,Rename existing permissions to prepare for new roles,"Rename existing permissions to prepare for the new role base access control naming conventions:

* View User:  {{VIEW.USE}} --> {{VIEW.USER}}
* Cluster User:  {{CLUSTER.READ}} --> {{CLUSTER.USER}}
* Cluster Administrator:  {{CLUSTER.OPERATE}} --> {{CLUSTER.ADMINISTRATOR}}
* Administrator: {{AMBARI.ADMIN}} --> {{AMBARI.ADMINISTRATOR}}
",permissions rbac roles,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2015-11-09 14:28:09,30
12911429,Fix typo in warn message when restarting DataNodes,"{code:title=ambari-web/app/messages.js}
  'rollingrestart.dialog.warn.datanode.batch.size': 'Restarting more than one DataNode at a time is not recommended. Doing so can lead to data unavailability and/or possible loss of data being actively written to HFDS.',
{code}
HFDS should be HDFS.",newbie,['ambari-web'],AMBARI,Bug,Trivial,2015-11-09 04:36:32,67
12910002,Minimize HDFS and other headless keytab distribution (security concerns),"Currently, we distribute the *hdfs* headless principal to pretty much every single host in the cluster.  
Since *hdfs* is a super user in HDFS, if any one of the hdfs keytabs are compromised on any host, the user can do anything on HDFS.
We need to revisit and see if we can restrict the number of hosts to which we distribute the hdfs headless keytab.
For example, we can perform necessary HDFS operations on one of the master hosts available, rather than picking an arbitrary client / slave hosts as we do today.
Also, we should look into not only hdfs headless keytabs but all other headless ones like hbase, storm, etc.",hdfs keytabs security,['ambari-server'],AMBARI,Bug,Critical,2015-11-03 14:16:04,30
12909705,UpgradeCatalog212 not idempotent,"The UpgradeCatalog212 logic is not idempotent:
- the executePreDMLUpdate logic relies on a column that could potentially be deleted by a previously ran upgrade",patch,['ambari-server'],AMBARI,Bug,Major,2015-11-02 15:54:53,95
12907520,"When adding components to a Kerberized cluster, the set of hosts to create principals for should be limited to only the relevant set","When adding components to a Kerberized cluster, the set of hosts to create principals for should be limited to only the relevant set.  This is because if a component for an existing service is added to an existing host, principals and keytab files for the existing components will get unnecessarily updated. 
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-10-23 18:55:36,30
12907518,ActionScheduler#filterParallelPerHostStages should not filter out stages with server-side actions,"org.apache.ambari.server.actionmanager.ActionScheduler#filterParallelPerHostStages should not filter out stages that contain server-side actions (to be executed on the Ambari server host). This is because tasks in these stages are typically required to complete before other stages on other hosts are executed. 

For example while enabling Kerberos for an added service, principals and keytab files need to be created before the stage to distribute the key tab files. The principal and keytab file creation happens on the Ambari server (as server-side actions) and the distribution tasks happen on the relevant hosts. If the server-side stages are filtered out (in the event multiple stages are pending for the Ambari server host), then one or more might be skipped and the distribution task is queued.  In this scenario, the distribution stage will fail since the required keytab files will not have been created. 
",actions kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-10-23 18:47:24,30
12905601,Expose in the API whether downgrade is supported or not,"Need API to expose whether a downgrade is supported or not, so that the UI can display the downgrade button accordingly.",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Blocker,2015-10-16 17:50:04,13
12905481,Bootstrap Cluster via different SSH Port Number,"For the time being, bootstrapping cluster is done via default port number 22 for SSH and SCP protocols without an option to specify the port number. 

There was a need to bootstrap cluster via different port number at the laboratory which I work at but Apache Ambari didn't have that option so we add that functionality. It still uses the default port number 22 unless you change it at Installer Wizard Step 2. 

A Textfieldview (default written value '22' SSH Port number ) added to installer wizard step 2, same as specifying ssh user field which is default to 'root' user, to take the SSH port input from user. After that, Port number is transferred to bootstrapping functions along with sshUser value.",features patch,"['ambari-server', 'ambari-web']",AMBARI,Improvement,Major,2015-10-16 10:29:24,116
12905052,"Ambari 1.7 to 2.1.x upgrade with existing Kerberos, keytab files fail to be distributed to some hosts","When upgrading from Ambari 1.7 to 2.1.x, where the source cluster has been Kerberized, keytab files fail to distribute to some hosts.  

*Steps to reproduce*
# Create cluster with Ambari 1.7.0 (multiple nodes, required)
# Enable Kerberos (manually)
# Upgrade to Ambari 2.1.2
# Enable Kerberos (automated)
# Not all hosts will receive keytab files

*Cause*
This is due to the way Ambari keeps track of which components (on each host) are properly Kerberized.  So if Kerberos was enabled before the upgrade to 2.x, at some point the agents will report back to Ambari that its components are secured with Kerberos.  Ambari will then use this data to determine which hosts need to be processed when enabling Kerberos. If some of the hosts have reported back before the determination of which hosts need to be process, those hosts will most-likely be skipped and thus, the new keytab files will not be distributed to them.

*Solution*
The solution is to remove outdated code previously used to determine how to handle scenarios where new services are added to a Kerberized cluster.  This code compared the existing services' security state with some desired security state.  If they matched, then no work needed to be done.  The state in particular is {{SECURED_KERBEROS}} and can be seen in the {{security_state}} column of the {{hostcomponentstate}} table.  

The code in question is outdated since new services being added to a Kerberized cluster no longer  requires the invocation of the {{KerberosHelper.toggleKerberos}} method.  The current implementing invokes more granular code to  configure the relevant service(s) and generate the Kerberos identities in a more granular fashion during state changes rather then after the fact. 
 ",kerberos upgrade,['ambari-server'],AMBARI,Bug,Major,2015-10-15 00:20:07,30
12904491,Blueprints: Implement REST resource for storing Kerberos descriptors,"h2. Support for saving a Kerberos descriptor to the REST API in Ambari

In order to support referring to a Kerberos descriptor by name in the Cluster Creation Template (POST-ing a Kerberos descriptor to a REST resource, then referring to it during Blueprint deployment), we’ll need a new REST resource defined in the Ambari REST API.  

We’ll need the ability to POST a kerberos.json descriptor document to:

http://ambari-host:ambari-port/api/v1/kerberos_descriptors/kerberos_descriptor_name

Where {code}kerberos_descriptors{code} is the collection resource for all Kerberos descriptors to be saved to the back end.  This can be any number of descriptors that are POST-ed, and the fact that a descriptor is posted here is not necessarily an indicator that it is used in a cluster deployment.  This is similar to how the {code}blueprints{code} resource works, in that the documents are saved in the Ambari DB to be referenced later by deployments (Blueprint deployments in our case). 

The {code}kerberos_descriptor_name{code} can be any arbitrary name given to the customer’s customized kerberos descriptor.  This can be a partial document, listing only the overrides necessary, as the default Kerberos descriptors will be merged by the Kerberos framework when the Kerberization is setup.  

For most Blueprint deployments that require Kerberos, the Blueprint or Cluster creation template will reference this “kerberos_descriptor_name” when indicating the kerberos descriptor used to configure Kerberos for this cluster. 

There needs to be support for:

1. POST-ing a Kerberos descriptor to this REST endpoint
2. Obtaining the list of Kerberos descriptors by making a GET call on the following URL:

http://ambari-host:ambari-port/api/v1/kerberos_descriptors

3. Obtaining a named Kerberos descriptor by making a GET call on the following URL:

http://ambari-host:ambari-port/api/v1/kerberos_descriptors/kerberos_descriptor_name

This portion of the Blueprints Kerberos support is not necessarily Blueprint-specific, and should be implemented without any direct references to Blueprint code or constructs.  


h2. Ambari Database Upgrade issues 

The addition of this new resource type will require the creation of one more more database tables in order to store the Kerberos descriptors in the Ambari back-end.  

This current task will encompass any Database table changes needed to make these additions, and will also likely require some ambari-server Upgrade handling. This will involve using the existing Ambari Upgrade utilities to support moving from older Ambari installs to Ambari 2.2. The main work here will be updating existing database tables to support the new structure. For this particular task, this will likely mean that an upgrade will need to add the new tables to the Ambari Database.  

h2. Existing Example of this type of resource
The Ambari REST API includes many services and ResourceProvider implementations that could serve as a model for this implementation.  The ""blueprints"" resource is probably the most straightforward in this case.  

Example of a Service interface that must be defined for each resource:

{code}org.apache.ambari.server.api.services.BlueprintService{code}

Example of a ResourceProvider implementation that must be defined for each resource:

{code}org.apache.ambari.server.controller.internal.BlueprintResourceProvider{code}

Example of a resource definition that must be defined for a new resource:

{code}org.apache.ambari.server.api.resources.BlueprintResourceDefinition{code}

There may be other classes required for this implementation, but the Blueprint resource pattern is probably the best starting point for this current task.  

h2. Ambari REST API Documentation

The following link shows the Ambari REST API Documentation, which will likely be useful during this task:

https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md


h2. Examples of Kerberos Descriptors

The default Kerberos descriptors are defined in the stacks, and can be found at the service-level and stack-level.  

In the trunk repo, the stack definitions for the ""HDP"" stack can be found in:

{code}ambari/ambari-server/src/main/resources/stacks/HDP{code}

All of the Kerberos Descriptors in the stacks are named ""kerberos.json"".  

From the directory mentioned above in the stack definitions, the following shows the default kerberos descriptors for the ""HDP"" stacks:

{code}
find . -name ""kerberos.json""
./2.0.6/kerberos.json
./2.2/services/YARN/kerberos.json
./2.3/services/ACCUMULO/kerberos.json
./2.3/services/KAFKA/kerberos.json
./2.3/services/TEZ/kerberos.json
./2.3/services/YARN/kerberos.json
./2.3.GlusterFS/services/ACCUMULO/kerberos.json
./2.3.GlusterFS/services/TEZ/kerberos.json
{code}",patch,['ambari-server'],AMBARI,Bug,Major,2015-10-13 10:20:58,95
12903597, Stack featurization: Enhance Ambari framework to facilitate alternative stack definitions,"Currently, Ambari common services (git/ambari/ambari-server/src/main/resources/common-services/*) codes contain specific logic about HDP stack including stack name (HDP), version (2.0, 2.1, 2.2...), and install location (/usr/hdp).

The refactor of services from the stack into a common area is a first important steps.  However, we need refactor stack specific information out these services so that they can be reused by other distribution and stack definition as part of the ODP initiative.

We hope to use this feature JIRA and its subtask to deliver a setup of changes that will make the common services codes fully parameterized and can be reused easily by any other distribution of Ambari.     ",common-services parametize,['ambari-server'],AMBARI,Epic,Major,2015-10-09 01:31:02,117
12903233,Security-related HTTP headers should be set separately for Ambari Views then for Ambari server UI,"The security-related HTTP headers should be set separately for the Ambari Views then for the Ambari server UI. This is because they have different requirements.  For example the Ambari server UI should not be allowed to execute in an iframe (by default) where Ambari View must be able to execute in an iframe invoked from the same origin.

The relevant headers are:
* Strict-Transport-Security
* X-Frame-Options
* X-XSS-Protection

These headers should be configurable via the ambari.properties such that they may be turned on or off - and set to some custom value.

The default value for this headers should be as follows:
* Strict-Transport-Security: max-age=31536000
* X-Frame-Options: SAMEORIGIN
* X-XSS-Protection: 1; mode=block

Strict-Transport-Security should only be turned on if SSL is enabled.

The relevant Ambari properties should be:
* Strict-Transport-Security: views.http.strict-transport-security
* X-Frame-Options: views.http.x-frame-options
* X-XSS-Protection: views.http.x-xss-protection

By setting any of these to be empty, the header is to be turned off (or not set).

For example:
{code:title=Sets Strict-Transport-Security to a custom value}
views.http.strict-transport-security=max-age=31536000; includeSubDomains
{code}

{code:title=Turns Strict-Transport-Security off}
views.http.strict-transport-security=
{code}
",security,['ambari-server'],AMBARI,Bug,Major,2015-10-08 04:13:43,30
12901986,Kerberos: Retain KDC admin credentials,"Enhance the Kerberos backend to allow for the retention of KDC administrative credentials.  Once securely stored, users may opt to remove the stored credentials.  

See AMBARI-13214 for information on the relevant API calls. 

The alias name for the KDC administrator credential should be *kdc.admin.credential*

For example:

*Create Credential Resource*
{code}
POST /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{
  ""Credential"" : {
    ""principal"" : ""admin/admin@EXAMPLE.COM"",
    ""key"" : ""h4d00p&!"",
    ""type"" : ""persisted""
  }
}
{code}

*Update Credential Resource*
{code}
PUT /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{
  ""Credential"" : {
    ""key"" : ""newpassword"",
    ""type"" : ""temporary""
  }
}
{code}

*Get Credential Resource*
{code}
GET /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{code}

*Delete Credential Resource*
{code}
DELETE /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-10-01 20:32:25,30
12896371,Kerberos: Allow multiple KDC hosts to be set while enabling Kerberos,"Because multiple KDCs may exist for an installation (failover, high availability, etc...), Ambari should allow a user to specify multiple KDC hosts to be set while enabling Kerberos and updating the Kerberos service's configuration.

This should be done by allowing {{kerberos-env/kdc_host}} to accept a (comma-)delimited list of hosts and then parsing that list properly when building the krb5.conf file where each {{kdc_host}} item generates an entry in the relevant realm block.  For example:

{noformat:title=kerberos-env}
{
  ...
 ""kdc_hosts"" : ""kdc1.example.com, kdc2.example.com""
  ...
}
{noformat}

{noformat:title=krb5.conf}
[realms]
  EXAMPLE.COM = {
    ...
    kdc = kdc1.example.com
    kdc = kdc2.example.com
    ...
  }
{noformat}",kerberos,"['ambari-agent', 'ambari-server', 'ambari-web']",AMBARI,Bug,Minor,2015-09-25 10:31:33,30
12896073,kdc_type lost when updating kerberos-env via Kerberos service configuration page,"After editing the kerberos-env configuration using Ambari's Kerberos service configuration page and saving the new configuration, the {{kdc_type}} property is lost and not saved with the new configuration.

*By loosing this value, any future Kerberos-related operations will fail with errors since the mandatory kerberos-env/kdc_type property will be missing.*

{code:title=kerberos-env before update}
{
  ""kdc_type"": ""mit-kdc"",
  ""password_min_uppercase_letters"": ""1"",
  ""password_min_whitespace"": ""0"",
  ""password_min_punctuation"": ""1"",
  ""password_min_digits"": ""1"",
  ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
  ""kdc_create_attributes"": """",
  ""admin_server_host"": ""host1"",
  ""password_min_lowercase_letters"": ""1"",
  ""container_dn"": """",
  ""password_length"": ""20"",
  ""case_insensitive_username_rules"": ""false"",
  ""manage_identities"": ""true"",
  ""service_check_principal_name"": ""${cluster_name}-${short_date}"",
  ""kdc_host"": ""host1"",
  ""ad_create_attributes_template"": ""\n{\n  \""objectClass\"": [\""top\"", \""person\"", \""organizationalPerson\"", \""user\""],\n  \""cn\"": \""$principal_name\"",\n  #if( $is_service )\n  \""servicePrincipalName\"": \""$principal_name\"",\n  #end\n  \""userPrincipalName\"": \""$normalized_principal\"",\n  \""unicodePwd\"": \""$password\"",\n  \""accountExpires\"": \""0\"",\n  \""userAccountControl\"": \""66048\""\n}"",
  ""install_packages"": ""true"",
  ""realm"": ""EXAMPLE.COM"",
  ""ldap_url"": """",
  ""executable_search_paths"": ""/usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin""
}
{code}

{code:title=kerberos-env after update}
{
  ""password_min_uppercase_letters"": ""1"",
  ""password_min_whitespace"": ""0"",
  ""password_min_punctuation"": ""1"",
  ""password_min_digits"": ""1"",
  ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
  ""kdc_create_attributes"": """",
  ""admin_server_host"": ""hist1:88"",
  ""password_min_lowercase_letters"": ""1"",
  ""container_dn"": """",
  ""password_length"": ""20"",
  ""case_insensitive_username_rules"": ""false"",
  ""manage_identities"": ""true"",
  ""service_check_principal_name"": ""${cluster_name}-${short_date}"",
  ""kdc_host"": ""host1:88"",
  ""ad_create_attributes_template"": ""\n{\n  \""objectClass\"": [\""top\"", \""person\"", \""organizationalPerson\"", \""user\""],\n  \""cn\"": \""$principal_name\"",\n  #if( $is_service )\n  \""servicePrincipalName\"": \""$principal_name\"",\n  #end\n  \""userPrincipalName\"": \""$normalized_principal\"",\n  \""unicodePwd\"": \""$password\"",\n  \""accountExpires\"": \""0\"",\n  \""userAccountControl\"": \""66048\""\n}"",
  ""install_packages"": ""true"",
  ""realm"": ""EXAMPLE.COM"",
  ""ldap_url"": """",
  ""executable_search_paths"": ""/usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin""
}
{code}

{code:title=Javascript Error}
Uncaught TypeError: Cannot read property 'get' of undefined
App.MainServiceInfoConfigsController.Em.Controller.extend.prepareConfigObjects @ app.js:22525
App.MainServiceInfoConfigsController.Em.Controller.extend.parseConfigData @ app.js:22490
App.ConfigsLoader.Em.Mixin.create.loadCurrentVersionsSuccess @ app.js:61506
Em.Object.extend.send.opt.success @ app.js:154010
f.Callbacks.o @ vendor.js:125
f.Callbacks.p.fireWith @ vendor.js:125
w @ vendor.js:127
f.support.ajax.f.ajaxTransport.c.send.d @ vendor.js:127
app.js:55160 App.componentConfigMapper execution time: 1.048ms
{code}

*Steps to reproduce*
# Create cluster (Zookeeper-only is fine)
# Enable Kerberos (any KDC, MIT KDC is fine)
# Browse to Kerberos service configuration page
# Change a value (maybe add or remove the port for the KDC server value)
# Save the configuration
# After view refreshes, the waiting icon appears and does not go away

*Workaround*
Manually add the {{kerberos-env/kdc_type}} property back to the current kerberos-env configuration.  The value must be either ""mit-kdc"" or ""active-directory"" and must be the correct one for the configuration.  Once this is done, Ambari should be restarted so that any cached configuration data is refreshed. 

This can also be fixed using {{/var/lib/ambari-server/resources/scripts/configs.sh}}.",regression,['ambari-web'],AMBARI,Bug,Critical,2015-09-24 10:56:15,30
12895901,"Create a credentials resource used to securely set, update, and remove credentials used by Ambari","Storage of the credentials is to be done using the existing _secure_ credentials provider API which already exits within Ambari.  See {{org.apache.ambari.server.security.encryption.CredentialStoreService}} and {{org.apache.ambari.server.security.encryption.CredentialStoreServiceImpl}}.

Credential may be stored in either Ambari's persistent or temporary secure storage facilities. 

*Test capabilities*
* Request 
{noformat}GET api/v1/clusters/{CLUSTER_NAME}{noformat}
* Responses
{code:title=200 OK}
{
  ...
  ""credential_store_properties"" : {
    ""storage.persistent"" : ""true"",
    ""storage.temporary"" : ""true""
  },
  ...
}
{code}

*Setting the credentials*
* Request 
{noformat}POST /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
{code:title=payload}
{
  ""Credential"" : {
    ""principal"" : ""USERNAME"",
    ""key"" : ""SECRET"",
    ""type"" : ""persisted""
  }
}
{code}
Where:
** principal:  the principal (or username) part of the credential to store
** key: the secret key part of the credential to store
** type:  declares the storage facility type: persisted or temporary
* Responses
{code:title=200 OK}
<empty>
{code}
{code:title=400 Bad Request}
{
  ""status"": 400,
  ""message"": ""Cannot persist credential in Ambari's secure credential store since secure storage has not yet be configured.  Use ambari-server setup-security to enable this feature.""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Updating the credentials*
* Request
{noformat}PUT /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
{code:title=payload}
{
  ""Credential"" : {
    ""principal"" : ""USERNAME"",
    ""key"" : ""SECRET1"",
    ""type"" : ""persisted""
  }
}
{code}
Where:
** principal:  the principal (or username) part of the credential to store
** key: the secret key part of the credential to store
** type:  declares the storage facility type: persisted or temporary
* Responses
{code:title=200 OK}
<empty>
{code}
{code:title=400 Bad Request}
{
  ""status"": 400,
  ""message"": ""Cannot persist credential in Ambari's secure credential store since secure storage has not yet be configured.  Use ambari-server setup-security to enable this feature.""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Removing the credentials*
* Request
{noformat}DELETE /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
* Responses
{code:title=200 OK}
<empty>
{code}
{code:title=404 Not Found}
{
  ""status"": 404,
  ""message"": ""Not Found""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Listing credentials*
* Request
{noformat}GET /api/v1/clusters/{CLUSTER_NAME}/credentials{noformat}
* Responses 
{code:title=200 OK}
{
  ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials"",
  ""items"" : [
    {
      ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials/kdc.admin.credentials"",
      ""Credential"" : {
        ""alias"" : ""kdc.admin.credentials"",
        ""cluster_name"" : ""c1""
      }
    },
    {
      ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials/service.admin.credentials"",
      ""Credential"" : {
        ""alias"" : ""service.admin.credentials"",
        ""cluster_name"" : ""c1""
      }
    }
  ]
}
{code}
{code:title=404 Not Found}
{
  ""status"": 404,
  ""message"": ""Not Found""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Retrieving credentials*
* Request
{noformat}GET /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
* Responses 
{code:title=200 OK}
{
  ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials/kdc.admin.credentials"",
  ""Credential"" : {
    ""alias"" : ""kdc.admin.credentials"",
    ""cluster_name"" : ""c1"",
    ""type"" : ""persisted""
  }
}
{code}
{code:title=404 Not Found}
{
  ""status"": 404,
  ""message"": ""Not Found""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}",security,['ambari-server'],AMBARI,Bug,Critical,2015-09-23 20:45:43,30
12895269,METRICS_COLLECTOR is not live messages are flooding the log files,"It seems that the any time MetricsPropertyProvider floods the log files for no good reason. This should be a DEBUG log instead of an INFO or it should be logged once when the METRICS_COLLECTOR gets enabled or disabled.

{code}
21 Sep 2015 21:15:34,697  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,697  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,697  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,698  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,698  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,699  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,699  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,699  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,700  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,700  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,700  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,701  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,701  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,702  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,702  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,702  INFO [qtp-client-551] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
21 Sep 2015 21:15:34,735  INFO [qtp-client-559] MetricsPropertyProvider:525 - METRICS_COLLECTOR is not live. Skip populating resources with metrics.
{code}",log,['ambari-web'],AMBARI,Bug,Critical,2015-09-21 22:10:17,42
12864063,Add Spark Thrift Ambari Service,"New feature to add spark thrift server support on Ambari. 

Design specification attached. 

Instruction to add thrift server to an existing cluster:
1) If running on HDP distro, Update metainfo.xml @ /var/lib/ambari-server/resources/stacks/HDP/2.3/services/SPARK/metainfo.xml. Change <deleted>true</deleted> to <deleted>false</deleted>. 
2) Use Ambari UI if spark has not been installed before. It will show up as a installable service alongside with spark history server. 
3) If spark component has been installed already, use API to add thrift-server as a new service. Service name: SPARK_THRIFTSERVER


",patch,[],AMBARI,New Feature,Major,2015-09-14 23:25:55,118
12863261,Web Client Should Display Skipped Tasks Differently In Upgrade Wizard,"AMBARI-13032 will introduce a new concept of skipped tasks which are commands that failed and were automatically skipped. This is used currently in the stack upgrade as an optional way to prevent slave/client failures from stopping the upgrade. 

Tasks which are skipped are now marked with the {{SKIPPED_FAILED}} status, however their encompassing stage is still COMPLETED. This is basically the same behavior if a {{HOLDING_FAILED}} tasks was ignored during the upgrade.",design-reviewed ru-ui-review,['ambari-web'],AMBARI,Bug,Major,2015-09-11 00:56:47,102
12863095,Kerberos: Allow user to specify additional realms for auth-to-local rules,"Allow user to specify additional realms for auth-to-local rules. This will add _default_ rules for the specified realm(s) to the generated auth-to-local rule sets. For example:

{noformat}
RULE:[1:$1@$0](.*@USER_REALM.COM)s/@.*//
{noformat}

The value should be a (comma) delimited list of realm names set in set of global properties in the Kerberos Descriptor.",kerberos kerberos-wizard,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2015-09-10 15:46:40,30
12861664,Turning on the Maintenance mode wouldn't let you install the additional components  properly,"Scenario:
1. Turn on the maintenance mode on HDFS service to suppress alerts
2. Add Name Node HA
3. During the adding addional name nodes and journal nodes, the amabri server supposed to stop the name node service but it wouldn't stop. 
4. As a result there is a lock on the HDFS and nameNode initializeEdits steps fail

Expected Result:
Even when in maintenance mode, Amabri should be able to add services, start and stop the services as required.",patch,"['ambari-admin', 'ambari-server']",AMBARI,Bug,Major,2015-09-03 20:23:04,7
12859462,Ambari Server and Agent should be more resilent when disk full,"When /var/lib/ambari-agent/data/structured-out-status.json get corrupted (probably when disk gets full), ambari-agent fails to report component status to the server; UI dashboard shows ""n/a"" for all components.",ambari-agent,['ambari-agent'],AMBARI,Bug,Critical,2015-08-26 22:39:35,93
12858893,File Browser view - Downloading a directory always creates a zip with name hdfs.zip,"Create a directory using FIle browser view. Try to download it , it always gets downloaded with hdfs.zip -- instead of the file name. 

It is much better if the zip is named after the directory that gets zipped instead of the user how owns it.

Repro:

1) create test1 and test2 directories
2) click on download icon
3) both directories will be downloaded as hdfs.zip

Resolution : 
If multiple files/folders are downloaded then the zip file name should be hdfs.zip . But if single file or folder is downloaded then it should be named after the downloaded file.
",patch,['ambari-views'],AMBARI,Bug,Major,2015-08-26 07:23:19,38
12857016,Disable old APIs to manage baseurl values,"Starting Ambari-2.0 and HDP-2.2, the baseurl for HDP/HDP-UTILS are being tracked in the repo_version table. However, the API to manage baseurls through metainfo are still working and it causes confusion for people who have been using these APIs.

The following API should throw an error if HDP version is 2.2 or higher. Note that this is an error only if HDP version is 2.2. or higher and the property being modified is ""base_url"".

{code}
curl -u admin:admin -H ""X-Requested-By: ambari"" -X PUT -d '{""Repositories"":{""base_url"":""http://public-repo-1.hortonworks.com/HDP/centos6/2.x/GA/2.2.0.0"",""verify_base_url"":true}}' http://localhost:8080/api/v1/stacks/HDP/versions/2.2/operating_systems/redhat6/repositories/HDP-2.2
{code}",ambari-server newbie,['ambari-server'],AMBARI,Bug,Critical,2015-08-18 19:35:00,89
12856172,Adding host via blueprint fails on secure cluster,"*STR*
Install cluster via blueprints
Enable Kerberos security
Add host via blueprints

*Result*
Adding hosts freeze forever
In ambari-server.log:
{code}
The KDC administrator credentials must be set in session by updating the relevant Cluster resource.This may be done by issuing a PUT to the api/v1/clusters/(cluster name) API entry point with the following payload:
{
  ""session_attributes"" : {
    ""kerberos_admin"" : {""principal"" : ""(PRINCIPAL)"", ""password"" : ""(PASSWORD)""}
  }
{code}

*Cause*
This is caused because the KDC administrative credentials are not available when needed during the add host process.  If set in the HTTP session, the credentials are not accessible since the Kerberos logic is executed outside the scope of that HTTP session.  

*Solution*
Store the KDC credentials to a _more secure_ global credential store that is accessible no matter what the context is.  This storage facility is in-memory and has a retention period of 90 minutes.  This solution refactors the current CredentialStoreService and MasterKeyService classes to allow for file-based and in-memory implementations. It also paves the way for future changes to allow for the KDC administrative credentials to be persisted indefinitely.",blueprints kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-08-14 14:23:35,30
12856148,Adding a CustomCommands causes Components are not being displayed in Host view,"We are trying to add a CustomCommands to our service in HDP2.3 stack, but once  CustomCommands added and restart Ambari in host view we can't see any components. This was working fine in HDP2.2(Ambari-2.0.1).
To reproduce follow below steps:
   1. As per the Amabri desgin document https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=38571133, we tried to add a TESTSERVICE service like below which contains customCommands in metainfo.xml:
<?xml version=""1.0""?>
<metainfo>
  <schemaVersion>2.0</schemaVersion>
  <services>
    <service>
      <name>TESTSERVICE</name>
      <displayName>Test Service</displayName>      
      <version>1.0.0.0</version>
      <components>
       <component>
          <name>TEST_COMPONENT</name>
          <displayName>Analytic Metastore</displayName>
          <category>MASTER</category>
          <cardinality>1</cardinality>
          <clientsToUpdateConfigs></clientsToUpdateConfigs>
          
          <commandScript>
            <script>scripts/script.py</script>
            <scriptType>PYTHON</scriptType>
          </commandScript>
          <customCommands>
            <customCommand>
              <name>CustonAction</name>
              <commandScript>
                <script>scripts/script.py</script>
                <scriptType>PYTHON</scriptType>
                <timeout>600</timeout>
              </commandScript>
            </customCommand>
          </customCommands>
        </component>        
      </components>
    </service>
  </services>
</metainfo>

 2. restart Ambari server
 3. Add service TESTSERVICE to any host
 4. goto host view and see all the components not showed.
 5. Also Service actions popup menu doesn't show this CustomAction.
",feature_custom_service,['ambari-web'],AMBARI,Bug,Major,2015-08-14 12:23:35,100
12855729,Stop-and-Start Upgrade: UpgradeHelper to support nonrolling upgrade pack,"Initial commit to introduce a ""nonrolling"" type of Upgrade Pack, so that UpgradeHelper can orchestrate it correctly during an upgrade.

For starters, this can create a nonrolling upgrade pack for HDP 2.2.x -> 2.2.y",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Critical,2015-08-13 00:05:17,69
12855577,Kerberos: Test KDC Connection succeeds but prints ERROR to ambari server log,"When enabling Kerberos, click Test KDC Connection, things succeed but there is an error printed to ambari-server log. This is happening when the KDC listens only on UDP and the TCP check fails.

{code}
06 Aug 2015 21:20:16,179 ERROR [qtp-client-29] KdcServerConnectionVerification:133 - Unable to connect to Kerberos Server
06 Aug 2015 21:20:26,111 ERROR [qtp-client-25] KdcServerConnectionVerification:133 - Unable to connect to Kerberos Server
{code}

*Solution*
Fix logging to be more explicit to avoid confusion
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-08-12 15:12:46,30
12854150,AMS is doing a (Select *) for certain point in time metrics from Dashboard page,"API calls for certain point in time metrics are fetching data for 1 day.

API:
{code}
http://hostname:6188/ws/v1/timeline/metrics?metricNames=dfs.FSNamesystem.MissingReplOneBlocks%2Cdfs.FSNamesystem.TransactionsSinceLastCheckpoint%2Cdfs.FSNamesystem.MillisSinceLastLoadedEdits%2Cdfs.FSNamesystem.SnapshottableDi rectories%2Cdfs.FSNamesystem.LastCheckpointTime%2Cdfs.FSNamesystem.TotalFiles%2Cmaster.Master.QueueCallTime_median%2Cdfs.FSNamesystem.ExpiredHeartbeats%2Cdfs.FSNamesystem.PostponedMisreplicatedBlocks%2Cdfs.FSNamesystem.LastWrittenTransactionId%2Cdfs.FSNamesystem.Snapshots%2Cjvm.JvmMetrics.MemHeapCommittedM%2Cdfs.FSNamesystem.TransactionsSinceLastLog Roll%2Cmaster.Server.averageLoad%2Cjvm.JvmMetrics.MemHeapUsedM%2Cdfs.FSNamesystem.PendingDataNodeMessageCount%2Cmaster.AssignmentManger.ritCount%2Cdfs.FSNamesystem.StaleDataNodes&hostname=namenode_hostnamel&appId=NAMENODE
{code}

Returned result on a cluster > 1 day old takes very long time.
The call should return 1 latest value for each metric vs all values.",ambari-metrics,[],AMBARI,Bug,Blocker,2015-08-11 13:19:14,28
12853965,Stop-and-Start Upgrade: Handle Downgrade path,"A downgrade is the exact same steps repeated so far, starting from the beginning. Instead of tacking backups, will have to restore DBs.",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:48:45,69
12853964,Stop-and-Start Upgrade: Ability to jump multiple major versions,Allow upgrading from HDP 2.x to HDP 2.y where y-x >= 2,branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:48:03,69
12853962,Stop-and-Start Upgrade: Revise PreChecks,"The PreChecks need to be specific to the Upgrade Pack.
This will allow relaxing them; for instance, NameNode HA is not required in Stop-and-Start Upgrade, but is during Rolling Upgrade.",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:45:31,13
12853961,Express Upgrade: Parallelize restarts,"Parallelize the start of certain components like Core Slaves, like DataNode, NodeManager, and RegionServer.
",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:44:31,41
12853960,Stop-and-Start Upgrade: Handle Core Services,"Stop-and-Start Upgrade of the Stack to handle core services: HDFS, YARN, MR, ZK",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:43:36,69
12853959,Stop-and-Start Upgrade: Move Configs out of Upgrade Pack,"The configs need to move out of the Upgrade Packs and into their own file.
This will make it easier to maintain, and clearer since there will not be any dups.",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:42:52,13
12853958,Stop-and-Start Upgrade: DB Schema Changes,Make required database schema changes such as moving the upgrade_pack column from the repo_version to the upgrade table.,branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Story,Major,2015-08-10 20:41:40,69
12853954,Update the stack by stopping and starting services in an orchestrated fashion,"Ambari needs to provide an option to upgrade the stack that is much faster, by taking downtime. Instead of rolling restarts, this will stop all services, and start them one at a time, with all hosts starting in parallel/batches.

Benefits:
* Relaxes requirements since NameNode HA is not needed
* Faster since,
** Can upgrade an entire component by parallelizing across hosts. E.g., all ZK servers at once
** Parallelize across components. E.g., Falcon Server and Storm Nimbus servers in parallel
",branch:branch-dev-stop-all-upgrade,['ambari-server'],AMBARI,Epic,Critical,2015-08-10 20:31:26,69
12853766,[RM HA] YARN service shows RED when one of RM goes down but the active RM works fine,"In a RM HA setup, YARN could work ever there is only 1 RM available, I would expect the service status for YARN is yellow or green(with alerts) from UI instead of red.",patch,['ambari-web'],AMBARI,Bug,Critical,2015-08-10 08:15:29,104
12853118,Ambari repo file (e.g. ambari.repo) should not be required,"While it is common to have the repo file be available and even required if Ambari is to be used to add new hosts, for hosted deployments (where users have pre-prepared VM/docker images) the repo file is not necessary. This is because the image usually has ambari components deployed. Add host translates to bringing up a docker/VM instance and starting an agent instance. ",ambari-server newbie,['ambari-server'],AMBARI,Bug,Critical,2015-08-08 15:15:31,93
12852341,Deleting a service fails if service's desired state is STARTED,"*STR:*
#  Stop all components of a service separately
#  Do a delete call  for that service via API

*Expected Result:* Service should be deleted as all components of the service are in installed state (no daemon of a service is running)
*Actual Result:* API fails with an exception {code} org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Cannot remove AMBARI_METRICS. Desired state STARTED is not removable.  Service must be stopped or disabled.{code}",newbie patch,['ambari-server'],AMBARI,Bug,Major,2015-08-05 18:46:51,93
12851489,Kerberos: fails check during enable Kerb with SLES,"When executing the Kerberos service check, the following error occurs:
{code}
stderr:   /var/lib/ambari-agent/data/errors-24.txt

Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py"", line 81, in <module>
    KerberosServiceCheck().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 218, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py"", line 64, in service_check
    user=params.smoke_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 258, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 70, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 92, in checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 140, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 291, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -c /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c -kt /etc/security/keytabs/kerberos.service_check.080315.keytab MyCluster-080315@EXAMPLE.COM' returned 1. kinit(v5): Credentials cache permissions incorrect when initializing cache /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c

stdout:   /var/lib/ambari-agent/data/output-24.txt

Performing kinit using MyCluster-080315@EXAMPLE.COM
2015-08-03 19:11:57,085 - Execute['/usr/bin/kinit -c /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c -kt /etc/security/keytabs/kerberos.service_check.080315.keytab MyCluster-080315@EXAMPLE.COM'] {'user': 'jambari-qa'}
2015-08-03 19:11:57,179 - File['/var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c'] {'action': ['delete']}
{code}

This error happens only on SLES, however the cause exists on all platforms.  The other platforms silently ignore the condition; which, however, does not have any bearing on the results of the _kinit_ test. 

*Cause*
The ""Credentials cache permissions incorrect when initializing cache"" issue is caused by the inability to write the Kerberos ticket cache file to the specified location. In the case it is /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c.  The reason for the write failure is that /var/lib/ambari-agent/data/tmp is not writable by the user executing the _kinit_ call - which is the Ambari smoke test user (typically ambari-qa).  The directory's permissions are
{noformat}
drwxr-xr-x. 4 root root 4096 Aug  3 22:20 /var/lib/ambari-agent/data/tmp/
{noformat}

*Solution*
In order for the ambari smoke test user to be able to write to the relevant directory (/var/lib/ambari-agent/data/tmp), the permissions must be set at least as follows
{noformat}
drwxrwxr-x. 4 root hadoop 4096 Aug  3 22:20 /var/lib/ambari-agent/data/tmp/
{noformat}
However, at the time this directory is created, it is not known what the name of the _hadoop_ group is, so the next best solution is to set the permissions as 
{noformat}
drwxrwxrwx. 4 root root 4096 Aug  3 22:20 /var/lib/ambari-agent/data/tmp/
{noformat}

If the ambari-agent is installed manually via the relevant package manager, the directory is created with the open permissions (777,  drwxrwxrwx) via the packages install_helper.sh post install script.  However if Ambari installs the agent via SSH, the directory is created with the more restrictive permissions (755, drwxr-xr-x) via the agent bootstrap.py script. 

To make these consistent, the following needs to be changed
{code:title=bootstrap.py:650}
   command = ""sudo mkdir -p {0} ; sudo chown -R {1} {0} ; sudo chmod 755 {3} ; sudo chmod 755 {2} ; sudo chmod 755 {0}"".format(
      self.TEMP_FOLDER, quote_bash_args(params.user), DEFAULT_AGENT_DATA_FOLDER, DEFAULT_AGENT_LIB_FOLDER)
{code}
to
{code:title=bootstrap.py (change)}
   command = ""sudo mkdir -p {0} ; sudo chown -R {1} {0} ; sudo chmod 755 {3} ; sudo chmod 755 {2} ; sudo chmod 777 {0}"".format(
      self.TEMP_FOLDER, quote_bash_args(params.user), DEFAULT_AGENT_DATA_FOLDER, DEFAULT_AGENT_LIB_FOLDER)
{code}

*Note:* self.TEMP_FOLDER contains the path to the Ambari agent temp folder (typically, /var/lib/ambari-agent/data/tmp).






",directory-permissions install,['ambari-server'],AMBARI,Bug,Critical,2015-08-04 20:06:14,30
12850633,AMS: hbase_regionserver_heapsize validation error should not apply when HBase is used in embedded mode,"AMS shows a validation error that hbase_regionserver_heapsize should not be below 12GB. Though this is correct, it does not apply when HBase is used in embedded mode - so this validation error should not show up.",ambari-metrics,['stacks'],AMBARI,Bug,Critical,2015-07-31 15:56:11,28
12850567,"When regenerating Kerberos principals, ensure Kerberos Descriptor changes are applied to services","When regenerating Kerberos principals, ensure Kerberos Descriptor changes are applied to services.  

It is possible for changes to be made to the Kerberos Descriptor.  One way to propagate these changes into the service configurations is to process the Kerberos Descriptor and update service configurations while performing tasks for the ""Regenerate Keytabs"" operation. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-07-31 10:23:19,30
12850008,When ambari.repo has insufficient permissions for non-root ambari-server to read failure messages are confusing,"*STR*
# run ""ambari-server setup"" and configure ambari to use not default user
{code}Customize user account for ambari-server daemon [y/n] (n)? y
Enter user account for ambari-server daemon (root):ambariuser{code}
# run ""ambari-server start""
# navigate through install wizard to ""Confirm Hosts"" step
*ER* hosts confirmation succeed 
*AR* Hosts registration failed
{code}==========================
Creating target directory...
==========================

Command start time 2015-06-18 13:45:28

Connection to c6401.ambari.apache.org closed.
SSH command execution finished
host=c6401.ambari.apache.org, exitcode=0
Command end time 2015-06-18 13:45:29

==========================
Copying common functions script...
==========================

Command start time 2015-06-18 13:45:29

scp /usr/lib/python2.6/site-packages/ambari_commons
host=c6401.ambari.apache.org, exitcode=0
Command end time 2015-06-18 13:45:29

==========================
Copying OS type check script...
==========================

Command start time 2015-06-18 13:45:29

scp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.py
host=c6401.ambari.apache.org, exitcode=0
Command end time 2015-06-18 13:45:29

==========================
Running OS type check...
==========================

Command start time 2015-06-18 13:45:29
Cluster primary/cluster OS family is redhat6 and local/current OS family is redhat6

Connection to c6401.ambari.apache.org closed.
SSH command execution finished
host=c6401.ambari.apache.org, exitcode=0
Command end time 2015-06-18 13:45:29

==========================
Checking 'sudo' package on remote host...
==========================

Command start time 2015-06-18 13:45:29
sudo-1.8.6p3-7.el6.x86_64

Connection to c6401.ambari.apache.org closed.
SSH command execution finished
host=c6401.ambari.apache.org, exitcode=0
Command end time 2015-06-18 13:45:29

==========================
Copying repo file to 'tmp' folder...
==========================

Command start time 2015-06-18 13:45:29

/etc/yum.repos.d/ambari.repo: Permission denied
scp /etc/yum.repos.d/ambari.repo
host=c6401.ambari.apache.org, exitcode=1
Command end time 2015-06-18 13:45:29

==========================
Moving file to repo dir...
==========================

Command start time 2015-06-18 13:45:29
mv: cannot stat `/var/lib/ambari-agent/data/tmp/ambari1434635129.repo': No such file or directory

Connection to c6401.ambari.apache.org closed.
SSH command execution finished
host=c6401.ambari.apache.org, exitcode=1
Command end time 2015-06-18 13:45:29

==========================
Copying setup script file...
==========================

Command start time 2015-06-18 13:45:29

scp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py
host=c6401.ambari.apache.org, exitcode=0
Command end time 2015-06-18 13:45:30

ERROR: Bootstrap of host c6401.ambari.apache.org fails because previous action finished with non-zero exit code (1)
ERROR MESSAGE: Execute of '<bound method BootstrapDefault.copyNeededFiles of <BootstrapDefault(Thread-1, started daemon 140695872956160)>>' failed
STDOUT: Try to execute '<bound method BootstrapDefault.copyNeededFiles of <BootstrapDefault(Thread-1, started daemon 140695872956160)>>'{code}",newbie patch,['ambari-server'],AMBARI,Bug,Major,2015-07-29 18:04:17,93
12849411,Provide ability to apply single patches on top of RU release,Documentation and updates to follow,pull-request-available,[],AMBARI,Epic,Critical,2015-07-27 19:45:46,33
12849361,"""kdc_create_attributes"" field should not be required","During security enabling there is present required property ""kdc_create_attributes"" on Configure Kerberos page. But this property should not be required (or should not be empty be default).",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Blocker,2015-07-27 16:13:47,30
12848876,Kerberos configuration properties should be final and uneditable outside the Kerberization wizard.,"Some Kerberos service properties should be final and not be editable outside the wizard.

*kerberos-env*
* Final and not be editable
** kdc_type
** manage_identities
** install_packages
** case_insensitive_username_rules
** realm
 
* Editable (changing them will require Keytabs to be regenerated and services to be restarted)
** ldap_url
** container_dn
** encryption_types
** kdc_host
** admin_server_host

* Editable  (changes will only be seen when [re]generating Kerberos identities and keytab files)
** password_length
** password_min_lowercase_letters
** password_min_uppercase_letters
** password_min_digits
** password_min_punctuation
** password_min_whitespace
** create_attributes_template

* Editable (changes will only be seen when certain operations are invoked)
** executable_search_paths
** service_check_principal_name

*krb5-conf*
* Final and not be editable
** domains
** manage_krb5_conf
** conf_dir
** content",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-07-24 12:25:28,30
12848441,ambari-server failed to properly detect of postgresql database status on RHEL 7.x ,"Ambari-server failed to initalize the postgre db when postgresql packages are installed as part of Ambari installation on RHEL 7.1, and 7.0 in some scenario.  

Ambari-server detects status of Postgre via the ""service postgresql status"" call, and perform regex on  the output for ""running"" or ""stopped"".   This works OK for RHEL 6, but not for RHEL 7 as its output may include ""running"" when postgres is not actually running.
",RHEL7.1,['ambari-server'],AMBARI,Bug,Major,2015-07-23 01:59:15,113
12846893,Kerberos: Allow setting/clearing attributes for MIT KDC identities,"Allow for attributes to be set (or unset) for identities created in an MIT (or similar) KDC. 

A user should be able to specify a list of attributes to be set or unset while creating identities using the MIT kadmin utility. For example:
{noformat}
-requires_preauth max_renew_life=7d
{noformat}

*Solution*
Add a property in {{kerberos-env}} or reuse {{kerberos-env/create_attributes_template}} to store the user-specified attributes.",kdc kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-07-22 13:30:17,30
12846712,RU - Use haadmin failover command instead of killing ZKFC during upgrade/downgrade,"Currently RU orchestration during upgrade/downgrade kills ZKFC on the active NameNode to initiate a failover to standby. We should instead use the failover command.
E.g.,

{code}
su hdfs -c 'hdfs haadmin -failover nn1 nn2'
{code}
Where nn1 is the current namenode if it if the active one, and nn2 is the remaining namenode.

This is safer than killing zkfc on the active namenode because this command first tries to gracefully transition a NameNode to the Standby state. If this fails, the fencing methods (as configured by dfs.ha.fencing.methods) will be attempted until one succeeds. After this process the second NameNode will be transitioned to the Active state. 

It reduces long waits between ZKFC kill, failure kicking-in after a timeout, and then NN becoming active.",rolling_upgrade,['ambari-server'],AMBARI,Story,Major,2015-07-21 23:01:15,69
12846017,"Metric Monitor Start Failed with error ""ambari-metrics-monitor already running""","Metrics monitor start script should not fail if daemon already running with correct PID

{code}
Checking for previously running Metric Monitor...
tput: No value for $TERM and no -T specified
ERROR: ambari-metrics-monitor already running
{code}",ambari-metrics,['ambari-metrics'],AMBARI,Bug,Critical,2015-07-17 23:02:25,42
12845909,Kerberos: ServiceResourceProvider queries for KDC connectivity when not needed,"When querying for information about services installed in a Kerberized cluster via the REST API, the ServiceResourceProvider always attempts to contact the KDC (or Active Directory) if the KERBEROS service is selected within the query. 

This can be seen about every 15 seconds,  when the UI queries for the state of the services in a Kerberized cluster using the following query:
{noformat}
GET  /api/v1/clusters/{cluster_name}/services?fields=ServiceInfo/state,ServiceInfo/maintenance_state&minimal_response=true
{noformat}

The result from this query does not contain the KDC connectivity attributes (which is expected), yet the detail are obtained.  

This issue causes excess overhead in Ambari as well as on the relevant KDC or Active Directory. Also the kdamin.log fills up with messages like:
{noformat:title=/var/log/kadmind.log}
Jun 29 14:31:42 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:31:42 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:31:42 some-host-1 kadmind[2383](info): closing down fd 29
Jun 29 14:32:49 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:32:49 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:32:49 some-host-1 kadmind[2383](info): closing down fd 29
Jun 29 14:34:35 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:34:35 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:34:35 some-host-1 kadmind[2383](info): closing down fd 29
Jun 29 14:35:28 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:35:28 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:35:28 some-host-1 kadmind[2383](info): closing down fd 29
{noformat}

*Solution*
Only query for the KDC attributes when explicitly or implicitly queried. This can be done by conditionally setting the relevant properties near {{org/apache/ambari/server/controller/internal/ServiceResourceProvider.java:1394}} by inspecting the request for relevant identifiers using something like the following:
{code}
requestedIds.contains(propertyId) || isPropertyCategoryRequested(propertyId, requestedIds);
{code}
",kerberos rest_api,['ambari-server'],AMBARI,Bug,Major,2015-07-17 15:33:49,30
12845005,Add Host and Add Service Wizards do not contain a Download CSV button when Kerberos is enabled,"Add Host and Add Service Wizards do not contain a Download CSV button when Kerberos is enabled.

!Ambari - Add Service Wizard - Review.png!
!Ambari - Add Host Wizard - Review.png!",kerberos,['ambari-web'],AMBARI,Bug,Major,2015-07-14 16:49:44,7
12843873,kinit of hdfs Kerberos identity fails when starting added service(s) after upgrade to Ambari 2.1.0,"STR:
1. Install old version of ambari (2.0.1)
2. Enable security
3. Do Ambari only upgrade to ambari2.1.0
4. Add some component - HiveServer2 or Ooozie server
5. Try to start added component

Actual result:
Start have been failed. 

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive_server.py"", line 182, in <module>
    HiveServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", 

line 216, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive_server.py"", line 83, in start
    self.configure(env) # FOR SECURITY
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive_server.py"", line 54, in configure
    hive(name='hiveserver2')
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in 

thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 127, in hive
    mode=params.webhcat_hdfs_user_mode
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in 

__init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 390, in 

action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 387, in 

action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 236, in 

action_delayed
    main_resource.kinit()
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 416, in kinit
    user=user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in 

__init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", 

line 254, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 70, in 

inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 92, in 

checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 140, in 

_call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 291, in 

_call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -kt 

/etc/security/keytabs/hdfs.headless.keytab hdfs@EXAMPLE.COM' returned 1. kinit: Keytab 

contains no suitable keys for hdfs@EXAMPLE.COM while getting initial credentials
{code}


Expected results:
Can start all added components.

*Cause*
The Kerberos Descriptor structure changed between Ambari 2.0 and Ambari 2.1.  This change moved the ""hdfs"" Kerberos identity descriptor from the _global_ scope to under the HDFS service. After upgrading from Ambari 2.0 to Ambari 2.1  an additional ""hdfs"" Kerberos identity descriptor was added with the new principal name pattern - $\{hadoop-env/hdfs_user\}-$\{cluster_name\}@$\{realm\}.  This occurred because the stored Kerberos Descriptor contained the _old_ structure, and when Ambari generated a composite Kerberos Descriptor made up of the Kerberos Descriptor compiled from the relevant stack definition with stored changes applied, that additional ""hdfs"" Kerberos identity descriptor was added.  Because if this, the Kerberos logic became _confused_ and overwrote the existing hdfs keytab file with one that contained the new principal name.

*Solution*
While migrating Ambari 2.0 to Ambari 2.1, fix the stored Kerberos Descriptor structure to match the new version's structure.
",kerberos upgrade,['ambari-server'],AMBARI,Bug,Major,2015-07-09 14:31:03,30
12843592,Kerberos: LDAP error updating and removing service principals in AD,"An LDAP error occurs while updating and removing service principals in clusters with Kerberos enabled using Active Directory.

The following exception is thrown when removing an account:
{noformat}
javax.naming.NamingException: [LDAP: error code 1 - 000020D6: SvcErr: DSID-0310081B, problem 5012 (DIR_ERROR), data 0
]
noformat}

The following exception is thrown when updating an account's password
{noformat}
javax.naming.NamingException: [LDAP: error code 1 - 000020D6: SvcErr: DSID-0310081B, problem 5012 (DIR_ERROR), data 0
^@]
{noformat}
",active_directory kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-07-08 18:10:46,30
12842403,Support PAM as authentication mechanism for accessing Ambari UI/REST,"Ambari GUI is using default ""admin"" user which is not a real user in operating system.  Some company has strict password policy which can not be enforced to Ambari.  It would be good to implement a Shiro PAM connector to authenticate user by Linux user credential.",security,"['ambari-server', 'ambari-web']",AMBARI,Story,Major,2015-07-02 18:39:57,104
12842349,hive-site/hive.metastore.sasl.enabled value incorrect when adding Hive to a Kerberized Cluster,"When *adding* Hive to an existing Kerberized cluster, the {{hive-site/hive.metastore.sasl.enabled}} value is set to {{false}} when it should be {{true}}.  If Hive was installed before enabling Kerberos,  
{{hive-site/hive.metastore.sasl.enabled}} is set to {{true}} after enabling Kerberos.

If {{hive-site/hive.metastore.sasl.enabled}} is {{false}} in a Kerberized cluster, the following error can be seen in the hiverserver2.log:

{noformat:title=/var/log/hive/hiveserver2.log}
2015-07-01 23:35:16,128 ERROR [HiveServer2-Handler-Pool: Thread-37]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type GSSAPI
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.thrift.transport.TTransportException: Unsupported mechanism type GSSAPI
        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)
        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:138)
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
        ... 4 more
{noformat}

*Cause*
It appears that the front end is updating the Kerberos Descriptor _artifact_ with _old_ data rather than the data the is specified on the stack's Kerberos Descriptor. This occurs during the transition between the ""Review"" and ""Install, Start, Test"" pages of the ""Add Service Wizard"".

*Solution*
Use the current Kerberos Descriptor's values as default value for the updated Kerberos Descriptor and update only what the user changes in the relevant fields.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-07-02 15:44:10,30
12842141,Exceptions encountered in Ambari Views are missing in ambari-server.log,"When an exception is encountered in the Ambari View (say Ambari Slider View), it shows up as a banner with red-text in the top. However there are instances when there is no corresponding log or stack trace dumped in /var/log/ambari-server/ambari-server.log. This happens even if the ambari-server log level is bumped up to DEBUG.

This behavior was primarily noticed in a secured cluster.

Please see attached screenshots as reference to the exception seen in the View UI.",views,[],AMBARI,Bug,Major,2015-07-01 23:02:56,81
12842106,HCat Service Check warns keytab contains no suitable keys when Kerberos is enabled,"HCat Service Check (part of the Hive Service Check) fails in cluster where Kerberos is enabled:

{noformat}
Test connectivity to hive server
Waiting for the Hive server to start...
2015-07-01 18:39:17,173 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-tezview@EXAMPLE.COM; '] {'user': 'ambari-qa'}
2015-07-01 18:39:17,321 - Execute['! beeline -u 'jdbc:hive2://c6502.ambari.apache.org:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM' -e '' 2>&1| awk '{print}'|grep -i -e 'Connection refused' -e 'Invalid URL''] {'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'], 'user': 'ambari-qa', 'timeout': 30}
Successfully connected to c6502.ambari.apache.org on port 10000
Successfully connected to Hive at c6502.ambari.apache.org on port 10000 after 6 seconds
2015-07-01 18:39:23,313 - File['/var/lib/ambari-agent/data/tmp/hcatSmoke.sh'] {'content': StaticFile('hcatSmoke.sh'), 'mode': 0755}
2015-07-01 18:39:23,314 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa; env JAVA_HOME=/usr/jdk64/jdk1.8.0_40 /var/lib/ambari-agent/data/tmp/hcatSmoke.sh hcatsmokeida8c06641_date390115 prepare'] {'logoutput': True, 'path': ['/usr/sbin', '/usr/local/bin', '/bin', '/usr/bin', '/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/var/lib/ambari-agent:/usr/hdp/current/hive-client/bin:/usr/hdp/current/hadoop-client/bin'], 'tries': 3, 'user': 'ambari-qa', 'try_sleep': 5}
kinit: Keytab contains no suitable keys for ambari-qa@EXAMPLE.COM while getting initial credentials
WARNING: Use ""yarn jar"" to launch YARN applications.
{noformat}

The issue appears to be the wrong principal name in the {{kinit}} command - note the missing cluster name and realm in the principal name value.
{noformat}
/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa
{noformat}

*Cause*
The error is caused by the use of the wrong variable when generating the kinit command at common-services/HIVE/0.12.0.2.0/package/scripts/hcat_service_check.py:44
{noformat}
        {kinit_path_local} -kt {smoke_user_keytab} {smokeuser}
{noformat}

*Solution*
At common-services/HIVE/0.12.0.2.0/package/scripts/hcat_service_check.py:44, change {{smokeuser}} to {{smokeuser_principal}}.",hive kerberos,['ambari-server'],AMBARI,Bug,Major,2015-07-01 21:00:30,30
12841036,Enabling Kerberos on cluster with AMS and no HDFS fails,"In a cluster where AMS is installed but HDFS is _not_ installed, enabling Kerberos fails due to the inability for the server-side Kerberos logic to replace ${hadoop-env/hdfs_user} when generating the metadata used to create principals and distribute keytab files.

This condition yields the following principal (when the cluster name is AMSNOHDFS and the realm is EXAMPLE.COM)
{noformat}
    $\{hadoop-env/hdfs_user\}-AMSNOHDFS@EXAMPLE.COM
{noformat}

This is successfully created in the (MIT) KDC. Also, the relative keytab file appears to have been successfully created as well.

However, when distributing the keytab file and setting the ownership attributes, the agent-side script fails with 
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 77, in <module>
    KerberosClient().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 216, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 67, in set_keytab
    self.write_keytab_file()
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_common.py"", line 397, in write_keytab_file
    group=group)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 108, in action_create
    self.resource.group, mode=self.resource.mode, cd_access=self.resource.cd_access)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 44, in _ensure_metadata
    _user_entity = pwd.getpwnam(user)
KeyError: 'getpwnam(): name not found: $\{hadoop-env/hdfs_user\}'
{code}

*NOTE: \ needed to be added to the hadoop-env/hdfs_user placeholder due to formatting issue*

*Solution:* 
Remove the HDFS identity reference in AMS and assume the hdfs keytab file will be on the appropriate host(s) when HDFS is installed
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-27 01:15:39,30
12840700,Kerberos: prompts for HDFS ambari principals w/o hdfs in cluster,"Installed HDP 2.2 cluster with only storm, zk, kafka, knox.

When enabling Kerberos, the _Configure Principals_ page of the _Kerberos Wizard_  prompts for the HDFS Ambari principal and keytab file although HDFS is not installed in cluster.

The HDFS Ambari principal and keytab file should also not display once enabled. (under Admin > Kerberos).

*Solution*
* Move the HDFS identity to the HDFS Kerberos Descriptor (from the top-level descriptor). 
* Change references to ""/hdfs"" to be ""/HDFS/hdfs""

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-26 00:14:05,30
12840352,Kerberos: storm headless princ + keytab seem incorrect,"* The storm user principals should be derived using the storm user + cluster name: 
{noformat}
storm-${cluster_name}@${realm}
{noformat}

* The keytab name should be consistent with other headless identities: 
{noformat}
storm.headless.keytab
{noformat}

*Solution*
Update Storm's Kerberos descriptor to fix the relevant identitiy ",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-24 21:54:40,30
12840257,Falcon Server Web UI alert fails when Kerberos is enabled,"Falcon Server Web UI alert fails when Kerberos is enabled.  The error message is 
{noformat}
agent/data/tmp/web_alert_cc_2144dc375b8556f5a0c2629adedd2a99 -kt {{falcon-startup.properties/*.falcon.http.authentication.kerberos.keytab}} {{falcon-startup.properties/*.falcon.http.authentication.kerberos.principal}} > /dev/null' returned 1. kinit: Client not found in Kerberos database while getting initial credentials)
{noformat}

*Cause*
This issue was introduced when the patch for AMBARI-11656 was applied.  

The issue is related to this line:
{code:title=ambari_agent/alerts/base_alert.py:217}
      replacement_match_regex = r""{{(%s)}}"" % placeholder_key
{code}

When the relative falcon properties are applied, the generated regular expression becomes 
{noformat}
{{(falcon-startup.properties/*.falcon.http.authentication.kerberos.keytab)}}
{noformat}

Which wants to match on values like:
* falcon-startup.properties.falcon.http.authentication.kerberos.keytab
* falcon-startup.properties/.falcon.http.authentication.kerberos.keytab
* falcon-startup.properties/////////////Rfalcon.http.authentication.kerberos.keytab

Not the one we really want - falcon-startup.properties/*.falcon.http.authentication.kerberos.keytab

Either the {{*}} needs to be escaped or the use of regular expressions needs to be changed.

*Solution*
Remove the regular expression replacement and use somple string replacement instead. ",alerts kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2015-06-24 17:02:13,30
12838805,Retry of Kerberize Cluster doesn't work,"*Steps to reproduce:*
# Launch Enable Kerberos wizard
# Proceed to Kerberize Cluster step
# Call to Kerberize Cluster failed
# Hit Retry

*Actual Result:*
Retry doesn't work
JS error: {{Uncaught TypeError: this.unkerberizeCluster is not a function}}",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-06-18 14:05:14,30
12838484,Display label for the custom action,"As currently implemented, the name of the function in the Python script is shown as it is in the Ambari web UI for the custom command:

{noformat}
<customCommand>
  <name>Remove_Sample_Data</name>
  <commandScript>
    <script>scripts/server.py</script>
    <scriptType>PYTHON</scriptType>
    <timeout>600</timeout>
  </commandScript>
</customCommand>
{noformat}

It would be more convenient to have a display label in addition  to the function name so that the menu item would look like ""Remove  Sample Data"" instead of ""Remove_Sample_Data"".",feature_custom_service,['ambari-server'],AMBARI,Improvement,Major,2015-06-17 14:09:45,60
12838173,Kerberos Wizard: Moving between steps 2 and 3 back/forward cause API error.,"STR:
* Install cluster with HDFS, Hive and dependcies.
* Start Kerberos Wizard
* Proceed to step 3 *Install and Test Kerberos Client*
* Go back to step2 *Configure Kerberos*
* Click on next button

AR: Popup with request error shown
{noformat}
500 status code recieved on POST method for API: /api/v1/clusters/c1/hosts

Error message: org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: ServiceComponent not found, clusterName=c1, serviceName=KERBEROS, serviceComponentName=KERBEROS_CLIENT
{noformat}
After popup confirmation click on Next button doesn't do anything.
ER: No errors shown, user can proceed to the next step.

Workaround: Refresh page on step2 and hit Next button.",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-06-16 14:06:33,30
12837801,Kerkeros Service Configs: KDC Type Value is corrupted,"*Steps to reproduce:*
1. Install Kerberos on the Cluster
2. Go to Kerberos Configs
3. Change any value in the configuration
4. Click Save and Confirm

*Expect Result:*
The KDC Type Value should remain unchanged.
The user should be able to make further modifications to the configs after this points.

*Actual Result:*
The Kerberos KDC Type Value is changed to the descriptive value and thus is unexpected.  Since this is a required field, but cannot be edited, the user can no longer save any changes after this point",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Major,2015-06-15 10:37:08,30
12837406,Kerberos: Oozie auth rules do not look correct,"Due to the fact that the oozie.authentication.kerberos.name.rules are  auto-generated the following rules should be removed from oozie-site.xml from all the stack configurations:

{panel:title=oozie-site.xml}
RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUSER/
RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
DEFAULT
{panel}

Note: this is an update for AMBARI-11179 to fix older stacks. ",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-06-12 13:25:54,30
12836748,Download Client Configs function results in an exception and error 500,"Since Ambari 2.0, Download Client Configs function for any Service results in an exception and error 500.

Clicking on any Service Action -> Download Client Configs results in an error instead of giving the confirmation prompt to save the client configs tar.gz file.

Example:
HDFS Client Configs
Generation of HDFS Client configurations file has failed with 500 error: 

org.apache.ambari.server.controller.spi.SystemException: Execution of ""ambari-python-wrap /var/lib/ambari-server/resources/stacks/stacks/BigInsights/4.0/services/HDFS/package/scripts/hdfs_client.py generate_configs /var/lib/ambari-server/tmp/HDFS_CLIENT-configuration.json /var/lib/ambari-server/resources/stacks/stacks/BigInsights/4.0/services/HDFS/package /var/lib/ambari-server/tmp/structured-out.json INFO /var/lib/ambari-server/tmp"" returned 2. java.lang.Throwable: /usr/bin/python2.6: can't open file '/var/lib/ambari-server/resources/stacks/stacks/BigInsights/4.0/services/HDFS/package/scripts/hdfs_client.py': [Errno 2] No such file or directory

Do you want to try again?

This bug will only manifest if the stack being used does not make use of common-services.",patch,['ambari-server'],AMBARI,Bug,Major,2015-06-10 06:42:57,100
12836734,Failure to add or install component defined with cardinality ALL,"A MASTER, SLAVE or CLIENT component defined with a cardinality of ALL results in a failure while adding the service, upon clicking Deploy, with an error like:

org.apache.ambari.server.controller.spi.ResourceAlreadyExistsException: Attempted to create host_component's which already exist: [clusterName=CLUSTER1, hostName=node1.domain.com, componentName=MYSERVICE_MASTER],[clusterName=CLUSTER1, hostName=node1.domain.com, componentName=MYSERVICE_MASTER]

or

org.apache.ambari.server.controller.spi.ResourceAlreadyExistsException: Attempted to create a host_component which already exists: [clusterName=CLUSTER2, hostName=node10.domain.com, componentName=MYSERVICE_SLAVE]


",patch,['ambari-server'],AMBARI,Bug,Major,2015-06-10 05:36:08,100
12836249,Service configs are not updated with enabling Kerberos unless the service has identities to process,"Service configs are not updated with enabling Kerberos unless the service has identities to process. This is an issue in the case a client component requires a configuration change when Kerberos is enabled and the Service has no Kerberos identities. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-06-08 19:23:56,30
12836110,Ambari creation of oozie/conf/adminusers.txt breaks oozie role seperation for Kerberos,"Oozie restart from Ambari rewrites oozie/conf/adminusers.txt

To support role separation for Kerberos, we need an additional line added to the end of this file.

The new line is:
{noformat}
oozie-admin
{noformat}

The new file should be generated as:
{code}
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Users should be set using following rules:
#
#     One user name per line
#     Empty lines and lines starting with '#' are ignored

oozie
oozie-admin
{code}

*Solution*
1. Replace
{code:title=common-services/OOZIE/4.0.0.2.0/package/templates/adminusers.txt.j2}
{{oozie_user}}
{code}
with 
{code:title=common-services/OOZIE/4.0.0.2.0/package/templates/adminusers.txt.j2}
{% if oozie_admin_users %}
{% for oozie_admin_user in oozie_admin_users.split(',') %}
  {{oozie_admin_user}}
{% endfor %}
{% endif %}
{code}

2. Add new property
{code:title=common-services/OOZIE/4.0.0.2.0/configuration/oozie-env.xml}
  <property>
    <name>oozie_admin_users</name>
    <value>oozie, oozie-admin</value>
    <description>Oozie admin users.</description>
  </property>
{code}

3. If the admin user list needs to change when enabling Kerberos, oozie-env/oozie_admin_users can be set in Oozie's Kerberos descriptor (kerberos.json).
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-06-08 10:31:33,30
12835919,Invalid property value set in core-site.xml when KNOX HA is enabled,"In KNOX-HA cluster, noticed that the “hadoop.proxyuser.knox.hosts"" property in /etc/hadoop/conf/core-site.xml
 refers to only one node and knox is running on multiple nodes.
{code}
 <property>
      <name>hadoop.proxyuser.knox.hosts</name>
      <value>host1</value>
    </property>
{code}
The value for this property should include all the knox hosts
{code}
 <property>
      <name>hadoop.proxyuser.knox.hosts</name>
      <value>host1,host4,etc...</value>
    </property>
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-06 11:40:08,30
12835914,Kerberos: adjust ambari headless principals for unique names,"1) Rollup all headless principal names up to Ambari Principals tab. Currently looks like Storm and Spark are on second tab, under their section, not under Ambari tab with ambari-qa, hdfs, hbase, etc. Also make sure the UI has user readable labels like the others for consistency (see the screen shot. spark.history.kerberos.principal should be ""Spark user principal"" for example).

2) By default, all of these to be cluster-name scoped by default. {code}-${cluster_name}{code} It does no harm for those that don't care... And for those that care about headless principal names to be unique, this ends up being done by default (and saves the user from having to remember to set it this way).

Ultimately when users want to add variables to their principal names they will be doing it across the board - whatever we can do to make it easier for users to do so, would be better.  If we had all principals in one pane they can quickly add all of them and visually validate.

*Solution*
Update the details for all _user_ ({{identities/type = user}}) Kerberos Identity entries in {{kerberos.json}} files to add the following to the principal name
{code}
-${cluster_name}
{code}

For example:
{code}
${hadoop-env/hdfs_user}@${realm}
{code}

to

{code}
${hadoop-env/hdfs_user}-${cluster_name}@${realm}
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-06 10:31:42,30
12834810,Falcon version command failed on secure runs,"After enabling Kerberos, the value for property *.falcon.http.authentication.kerberos.name.rules is set to 
{code}
RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/
RULE:[1:$1@$0](hdfs@EXAMPLE.COM)s/.*/hdfs/
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/ams/
RULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/ams/
RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](falcon@EXAMPLE.COM)s/.*/falcon/
RULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/hive/
RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/
RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/
RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/
DEFAULT
{code}

This is incorrect. The correct value should be 
{code}
RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/ \
RULE:[1:$1@$0](hdfs@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*// \
RULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/ams/ \
RULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/ams/ \
RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](falcon@EXAMPLE.COM)s/.*/falcon/ \
RULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/hive/ \
RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/ \
RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/ \
RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/ \
RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/ \
RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/ \
DEFAULT
{code}

Please notice the ""\"" at end of each RULE. This is needed because of the type of configuration file the data is in - a (Java) properties file, where properties values must be a single line or escaped if multi-lined. 

*Solution*
Convert the multi-line auth-to-local rule to meet the requirements of the configuration file type by allowing the _concatenation type_ to be specified in the Kerberos descriptor.  The following concatenation types are allowed:
* *new lines* - each rule is separated by a new line 
* *new_lines_escaped* - each rule is separated by an escaped new line
* *spaces* - each rule is separated by a whitespace charater



",kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2015-06-02 21:01:58,30
12834323,Kerberos: provide option to set test account name,"In many situations with large-scale Active Directory deployments, the krb5.conf is managed outside of Ambari.  This krb5.conf file is configured with all of the DC's in the AD domain, and the outbound requests to the KDC from clients are load balanced across those servers.  In many scenarios the user replication latency causes issues with users not found during the test process.  Due to the fact that we generate a new user every time we test, this can get users to a circular situation in which they can never leave this state because of multi-KDC's in their krb5.conf and delay associated with replication.

1) Expose the option to set the test kerberos client principal name (under Advanced kerberos-env)
2) Default the value to something unique, but less than 20 characters {code}
${cluster_name}-${short_date}
{code}",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-06-01 18:27:10,30
12832882,'Add Metric' menu cannot open after a dataset/expression got deleted,"*STR*:

1. Edit/Create a widget in type ""Template"" or ""Graph""
2. In step2, add a new ""Expression"" or ""Dataset"", then delete it.
3. Open ""Add metric"" menu, it cannot be open. 
4. Add another new ""Expression"" or ""Dataset"", 
5. Open ""Add metric"" menu, it can be open successfully. ",bb1,['ambari-web'],AMBARI,Bug,Major,2015-05-26 22:14:02,102
12832854,"Kerberos: UI shows Kerberize Cluster step as failed with a retry button, but the backend keeps moving forward to Kerberize the cluster","Precondition: On the cluster where this was observed, there were previous attempts to kerberize the cluster.

STR:
Go through the Enable Kerberos Wizard.
The only non-default option taken was to not manage krb5.conf (presented on the second page of the wizard).

Chrome developer tool shows that there was a POST on /api/v1/clusters/woah/artifacts/kerberos_descriptor failing with 409.  

{code}
{
""status"" : 409,
""message"" : ""Attempted to create an artifact which already exists,
artifact_name='kerberos_descriptor',
foreign_keys='{Artifacts/cluster_name=woah}'""
}
{code}

It doesn't seem like this is the cause of the issue (though we need to investigate).

The UI keeps showing a spinner for several minutes, then shows a failure.
This is because a call to PUT on the cluster resource to set security_type takes more than 3 minutes, and the browser aborts the request.
However, the backend kept moving forward to Kerberize the cluster (ambari-server.log was being tailed to check on progress).
After verifying that all principals and keytabs were generated/distributed, the wizard was closed (the last step of the wizard is to start all services and run service checks, but this was skipped because the previous step failed.)
The cluster was in fact successfully Kerberized.

*Note:* The condition is likely to have occurred due to a timeout related to the number of hosts and services in the cluster.  The preparation phase of enabling Kerberos is performed within the handler of the relavant API call. Most of this work should be moved out to a stage which is handled asynchronously.",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-26 20:25:19,30
12832853,"Kerberos: UI shows Kerberize Cluster step as failed with a retry button, but the backend keeps moving forward to Kerberize the cluster","Precondition: On the cluster where this was observed, there were previous attempts to kerberize the cluster.

STR:
Go through the Enable Kerberos Wizard.
The only non-default option taken was to not manage krb5.conf (presented on the second page of the wizard).

Chrome developer tool shows that there was a POST on /api/v1/clusters/woah/artifacts/kerberos_descriptor failing with 409.  

{code}
{
""status"" : 409,
""message"" : ""Attempted to create an artifact which already exists,
artifact_name='kerberos_descriptor',
foreign_keys='{Artifacts/cluster_name=woah}'""
}
{code}

It doesn't seem like this is the cause of the issue (though we need to investigate).

The UI keeps showing a spinner for several minutes, then shows a failure.
This is because a call to PUT on the cluster resource to set security_type takes more than 3 minutes, and the browser aborts the request.
However, the backend kept moving forward to Kerberize the cluster (ambari-server.log was being tailed to check on progress).
After verifying that all principals and keytabs were generated/distributed, the wizard was closed (the last step of the wizard is to start all services and run service checks, but this was skipped because the previous step failed.)
The cluster was in fact successfully Kerberized.

*Note:* The condition is likely to have occurred due to a timeout related to the number of hosts and services in the cluster.  The preparation phase of enabling Kerberos is performed within the handler of the relavant API call. Most of this work should be moved out to a stage which is handled asynchronously.",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-26 20:23:48,30
12832574,Kerberos: display warning that the user must create/distribute principals and keytabs,"In Ambari 1.7.0, we used to display a warning to the end user to create principals/keytabs when:
* a new service is added to the cluster (Review page)
* a new host is added to the cluster (Review page)
* a new component is added to an existing host (dialog popup)
We need to bring these back when and only when the cluster is already kerberized with ""Manual"" option.

*Note: This is a fix for an issue that AMBARI-11244 created where if security is disabled, the user is not able to add components on host details page.  Upon clicking on add component nothing happens.*
",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-25 21:21:14,30
12832381,Kerberos: Creating principals in AD when special characters are involved causes failures,"Creating principals in AD when special characters are involved causes failures.

The following characters in the CN need to be escaped:
{noformat}
/ , \ # + < > ; "" =
{noformat}

*Note:* javax.naming.ldap.Rdn will properly escape relative distinguished name parts.


The following characters in the sAMAccountName need to be removed or replaced:
{noformat}
[ ] : ; | = + * ? < > / \
{noformat}

*Note:* This needs to be done explicitly within the attributes set if a relevant entry exists.
{code}
// Replace the following _illegal_ characters: [ ] : ; | = + * ? < > / \
value = value.toString().replaceAll(""\\[|\\]|\\:|\\;|\\||\\=|\\+|\\*|\\?|\\<|\\>|\\/|\\\\"", ""_"");
{code}
",active-directory active_directory kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-23 21:31:34,30
12832341,"Kerberos FE: during disable, need option skip if unable to access KDC to remove principals","Attempted to disable kerb, fails on step to unkerberize because KDC admin is locked out.

Click retry, can't make it past that.

Need option to skip and finish ""disable kerberos"" even if Ambari cannot get the principals cleaned up (i.e. cannot access the KDC) Losing access to the KDC and attempting to disable where ambari can't clean-up the principals should be a skip'able step. User should still be able to get to a clean, not-enabled-kerberos-ambari-state w/o accessing the KDC.

*Solution*
Based on user input, execute API call to disable Kerberos with the *manage_kerberos_identities* _directive_ set to *false*.  Example:
{code:title=PUT /api/v1/clusters/c1?manage_kerberos_identities=false}
{
  ""Clusters"": {
    ""security_type"" : ""NONE""
  }
}
{code}
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-23 11:29:03,30
12832262,HdfsResource does not account for sticky bit,"Specifying a mode of 01777 will result in an exception like:

Exception in thread ""main"" java.lang.IllegalArgumentException: 1777
at org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60)
at org.apache.hadoop.fs.permission.UmaskParser.<init>(UmaskParser.java:42)
at org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:106)
at org.apache.ambari.fast_hdfs_resource.Resource.setMode(Resource.java:217)
at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:78)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


In fast_hdfs_resource, setMode is used to set the permissions on the Resource object using the specified mode. Passing in the mode as a string for FsPermission is what results in the problem because the constructor that is used with string as argument uses the UmaskParser. Umask values are slightly different from standard modes as they cannot specify t (sticky bit) or X.

Passing in the argument as a short is probably what is needed here to enable using the FsPermission code that accounts for the sticky bit.",patch,['contrib'],AMBARI,Bug,Major,2015-05-22 20:37:12,100
12832257,Finer-grained role AuthZ for Ambari Users,"Ambari currently integrates with external authentication systems and is able to authenticate users using enterprise-wide LDAP systems, such as Active Directory, OpenLDAP, and Apache Directory Service. However, more flexibility is now needed to allow for those authenticated users to be segmented into more granular roles.  These roles allow Ambari-level administrators to create different levels of cluster-level administrators to manage certain administrative operations that need to be performed on a cluster. This effectively spreads out the responsibilities of managing a cluster while not handing over total control of the Ambari management facility. 

Ambari to provide role-based access controls beyond today's Ambari Admin, Operator and Read-Only permissions.

|| Role || Description ||
| *Cluster User* (was Read-only) | This exists as of Ambari 1.7.0. Read-only view of cluster information, including configurations, service status and health alerts|
| *Service Operator* | Provides control of service lifecycle (start/stop/restart/decomm/recom) |
| *Service Administrator* | Service Operator + ability to re-configure (change/compare/revert), configure HA |
| *Cluster Operator* | Service Administrator + add/remove hosts and components (for existing services) |
| *Cluster Administrator* | Cluster Operator + enable/disable kerberos, modify alerts, add service, perform upgrade (renamed from Operator) |
| Administrator | This exists as of Ambari 1.7.0. Full cluster control + manage user, groups and views and this flag is applicable to any user regardless of Role |

Each role is to have permissions as shown below:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
||Service-level Permissions||
|View metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configurations           |(+)|(+)|(+)|(+)|(+)|(+)|
|Compare configurations        |(+)|(+)|(+)|(+)|(+)|(+)|
|View alerts        |(+)|(+)|(+)|(+)|(+)|(+)|
|Start/Stop/Restart Service    |   |(+)|(+)|(+)|(+)|(+)|
|Decommission/recommission     |   |(+)|(+)|(+)|(+)|(+)|
|Run service checks            |   |(+)|(+)|(+)|(+)|(+)|
|Turn on/off maintenance mode  |   |(+)|(+)|(+)|(+)|(+)|
|Perform service-specific tasks|   |(+)|(+)|(+)|(+)|(+)|
|Modify configurations         |   |   |(+)|(+)|(+)|(+)|
|Manage configuration groups   |   |   |(+)|(+)|(+)|(+)|
|Move to another host          |   |   |(+)|(+)|(+)|(+)|
|Enable/disable alerts          |   |   |(+)|(+)|(+)|(+)|
|Enable HA                     |   |   |(+)|(+)|(+)|(+)|
|Add Service to cluster        |   |   |   |   |(+)|(+)|
||*Host-level Permissions*||
|View metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|Turn on/off maintenance mode  |   |   |   |(+)|(+)|(+)|
|Install components            |   |   |   |(+)|(+)|(+)|
|Add/Delete hosts              |   |   |   |(+)|(+)|(+)|
||Cluster-level Permissions||
|View metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View stack version details    |(+)|(+)|(+)|(+)|(+)|(+)|
|View alerts                   |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable alerts         |   |   |   |   |(+)|(+)|
|Enable/disable Kerberos       |   |   |   |   |(+)|(+)|
|Upgrade/downgrade stack       |   |   |   |   |(+)|(+)|
||Ambari-level Permissions||
|Create new clusters           |   |   |   |   |   |(+)|
|Set service users and groups  |   |   |   |   |   |(+)|
|Rename clusters               |   |   |   |   |   |(+)|
|Manage users                  |   |   |   |   |   |(+)|
|Manage groups                 |   |   |   |   |   |(+)|
|Manage Ambari Views           |   |   |   |   |   |(+)|
|Assign permissions/roles      |   |   |   |   |   |(+)|
|Manage stack versions         |   |   |   |   |   |(+)|
|Edit stack repository URLs    |   |   |   |   |   |(+)|

*NOTE:  [^AmbariRole-basedAccessControl.pdf] claims the RBAC update is available in Ambari 2.2.0, however it was not implemented until Ambari 2.3.0 and further.*",permissions rbac roles,['ambari-server'],AMBARI,Epic,Major,2015-05-22 20:19:51,30
12832018,DDL errors seen on a cluster while enabling Kerberos,"{noformat}
1 May 2015 00:18:53,950  INFO [Server Action Executor Worker 256] KerberosServerAction:436 - Processing identities completed.
21 May 2015 00:18:56,754  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:18:57,658  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:18:57,926  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:19:00,677  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:19:12,502 ERROR [ambari-action-scheduler] ClusterImpl:2459 - ServiceComponentHost lookup exception
21 May 2015 00:19:21,850 ERROR [ambari-action-scheduler] ClusterImpl:2459 - ServiceComponentHost lookup exception
21 May 2015 00:19:21,870  INFO [Server Action Executor Worker 263] KerberosServerAction:332 - Processing identities...
21 May 2015 00:19:21,873  INFO [Server Action Executor Worker 263] DestroyPrincipalsServerAction:90 - Destroying identity, ambari-qa_ruocwwby@EXAMPLE.COM
21 May 2015 00:19:22,123 ERROR [Server Action Executor Worker 263] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:
Local Exception Stack:
Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_KRB_PR_HOST_PRINCIPALNAME) violated - child record found

Error Code: 2292
Call: DELETE FROM kerberos_principal WHERE (principal_name = ?)
        bind => [1 parameter bound]
        at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:331)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:900)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
        at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
        at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
        at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
        at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72)
        at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52)
        at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$9e172ec6.remove(<generated>)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:107)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processRecord(KerberosServerAction.java:504)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:393)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:64)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:504)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:441)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_KRB_PR_HOST_PRINCIPALNAME) violated - child record found

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
        at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
        at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
        at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
        ... 23 more
21 May 2015 00:19:22,126 ERROR [Server Action Executor Worker 263] AmbariJpaLocalTxnInterceptor:122 - [DETAILED ERROR] Internal exception (1) :
java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_KRB_PR_HOST_PRINCIPALNAME) violated - child record found

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
        at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
        at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
        at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
        at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
        at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
        at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
        at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72)
        at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52)
        at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$9e172ec6.remove(<generated>)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:107)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processRecord(KerberosServerAction.java:504)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:393)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:64)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:504)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:441)
        at java.lang.Thread.run(Thread.java:745)
21 May 2015 00:19:22,127  WARN [Server Action Executor Worker 263] DestroyPrincipalsServerAction:119 - Failed to remove identity for ambari-qa_ruocwwby@EXAMPLE.COM from the Ambari database - Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-22 01:16:00,30
12831969,Graph Zoom in tooltip should be consistent on different pages,"1. The ""Zoom in tooltip"" for graph are different on dashboard page and service page. see attached.

2. On Dashboard page, in Graph popup, the legend looks too close to the left. Should be centered. see attached. (This will be resolved after time range got enabled)

3. As the zoom in function is disabled in preview, the tooltip ""Click to zoom in"" should not show up in preview ",bb1,['ambari-web'],AMBARI,Task,Major,2015-05-21 21:52:53,102
12831956,In Graph widget Preview the second line title is hidden partially,"Create/Edit a graph widget, in step3 preview, the second line title is hidden, see attached.
Both wrong and correct images are attached.

",bb1,['ambari-web'],AMBARI,Bug,Critical,2015-05-21 21:12:59,102
12831747,HBase widget display typos,HBase RS widgets use Reqion instead of Region in many places,widget,['ambari-web'],AMBARI,Bug,Major,2015-05-21 14:38:29,119
12831386,Quick links for custom services,"The existing version of the Ambari (2.0) does not have a declarative way to add ""Quick Links"" for the custom services. Currently, to add a ""Quick Links"" dropdown one has to do the following:

1. Extract app.js file from /usr/lib/ambari-server/web/javascripts/app.js.gz to some directory.

2. Add  MY_CUSTOM_SERVICE entry to the quickLinks: 
{noformat}
 mapQuickLinks: function (finalJson, item){
    if(!(item && item.ServiceInfo)) return;
    var quickLinks = {
      OOZIE: [19],
      GANGLIA: [20],
      STORM: [31],
      FALCON: [32],
      RANGER: [33],
      SPARK: [34],
      MY_CUSTOM_SERVICE: [35]
    };
{noformat}
3. Add an entry to 
{noformat}
App.QuickLinks.FIXTURES = [


  {
    id: 35,
    label:'My Custom Service Admin UI',
    url:'%@://%@:8090/my_custom_service/admin',
    service_id: 'MY_CUSTOM_SERVICE',
    template:'%@://%@:8090/my_custom_service/admin'
  },
{noformat}
4. GZip the app.js and upload it to the Ambari server as  /usr/lib/ambari-server/web/javascripts/app.js.gz.


Having a declarative way of adding ""Quick Links"" on the custom service level will be much more convenient. 

",feature_custom_service,['ambari-server'],AMBARI,Improvement,Major,2015-05-20 14:14:31,60
12831109,Add Service Wizard requests KDC information when cluster is configured for Manual Kerberos,"Add Service Wizard requests KDC information when cluster is configured for Manual Kerberos. 

See screen shots:

!Ambari - ManKerb 2015-05-11 15-11-04.png!

When configured for Kerberos with the _Manual_ option, the _Add Host Wizard_ should not warn about or prompt for KDC details.  The _wizard_ should also allow the user to download a CSV file of expected (user-generated) principals and keytabs files from the Review (or conformation page).  
Additional working on the Review/Confirmation page may be needed to inform the user that they should create the principals and keytabs before continuing. 

To know whether the cluster is configured for _manual kerberos_:
* Cluster.security_type = ""KERBEROS""
* kerberos-env/manage_identities = ""false""",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-19 18:10:09,30
12831083,Kerberos: display warning that the user must create/distribute principals and keytabs,"In Ambari 1.7.0, we used to display a warning to the end user to create principals/keytabs when:
* a new service is added to the cluster (Review page)
* a new host is added to the cluster (Review page)
* a new component is added to an existing host (dialog popup)
We need to bring these back when and only when the cluster is already kerberized with ""Manual"" option.",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-19 17:11:16,30
12830542,templeton.hive.properties property value substitution should be done by ambari-server,"The webhcat-site/templeton.hive.properties property value substitution should be done by ambari-server.  This value is more complicated than others since the (embedded) hive.metastore.uris property is a list of thrift URIs generated using the set of hosts the Hive Metastore is installed on.  This list of hosts comes from the clusterHostInfo/hive_metastore_host value, which in the KerberosHelper (org.apache.ambari.server.controller.KerberosHelper) is available as a comma-delimited list of hosts.

{code}
clusterHostInfo/hive_metastore_host = ""host1,host2,host3""
{code}

To generate configuration values when enabling Kerberos, the KerberosHelper class uses org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor#replaceVariables to replace variables specified in the Kerberos Descriptor.  Currently this mechanism uses a simple replacement scheme, which is not sufficient to generate string using a delimited list of values. 

In order to solve this issue, ""functions"" need to be applied to replacement data before making the substitution.  In this case a ""function"" named ""each"" will be created that takes the following arguments:
* pattern with placeholders
* delimiter to use to concatenate values generated using the patter
* regex to use to split the original string

For example: 
{code:title=function declaration, commas are escaped when not intended to separate function args}
each(thrift://%s:9083, \\,, \s*\,\s*)
{code}

To indicate this function is to be used, the following Kerberos Descriptor variable replacement syntax is to be used:
{code}
${clusterHostInfo/hive_metastore_host|each(thrift://%s:9083, \\,, \s*\,\s*)}
{code}

Note: \ characters need to be escaped in JSON structure values.  For example: 
{code}
""some.property"" : ""${clusterHostInfo/hive_metastore_host|each(thrift://%s:9083, \\\\,, \\s*\\,\\s*)}""
{code}

If clusterHostInfo/hive_metastore_host = ""host1,host2,host3"", the result would be 
{code}
thrift://host1:9083\,thrift://host2:9083\,thrift://host3:9083
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-05-17 22:33:15,30
12830226,Kerberos: Oozie auth rules do not look correct,"0) create cluster, hDP 2.2, build 1203
1) Kerb cluster (hdfs, yarn,zk)
2) add ozzie
3) add hbase
4) everything seems ok.
5) I went and looked at oozie configs, oozie.authentication.kerberos.name.rules property looks like this...is this correct?

{code}
RULE:[1:$1@$0](ambari-qa-MyCluster@EXAMPLE.COM)s/.*/ambari-qa/
RULE:[1:$1@$0](hbase-MyCluster@EXAMPLE.COM)s/.*/hbase/
RULE:[1:$1@$0](hdfs-MyCluster@EXAMPLE.COM)s/.*/hdfs/
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[1:$1@$0](.*@.*TODO-KERBEROS-DOMAIN)s/@.*//
RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUSER/
RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/hbase/
RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/
RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/
RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/
DEFAULT
{code}


*Solution*
Remove the following values for oozie-site/oozie.authentication.kerberos.name.rules

{code:title=common-services/OOZIE/4.0.0.2.0/configuration/oozie-site.xml:145}
      RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUxSER/
      RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
      RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      DEFAULT
{code}

{code:title=common-services/OOZIE/5.0.0.2.3/configuration/oozie-site.xml:24}
      RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUxSER/
      RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
      RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      DEFAULT
{code}",keberos,['ambari-server'],AMBARI,Bug,Major,2015-05-15 16:52:08,30
12830177,Kerberos: Principals fail to be created if whitespace exists in generated passwords,"Principals fail to be created if whitespace exists in generated passwords.

{code:title=failure condition}
add_principal -pw 265 V$^+91axff4i56 ambari-qa@EXAMPLE.COM
{code}

To fix this, quotes are needed around the password:

{code:title=fix}
add_principal -pw ""265 V$^+91axff4i56"" ambari-qa@EXAMPLE.COM
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-15 14:23:15,30
12829980,Kerberos: service page incorrect with manual,"After enabling kerb manual, the configs tab under Kerberos is incorrect.

1) the type should indicate the accurate type (existing MIT, ad or manual), just like the wizard
2) the kadmin host is showing red but this is manual and it's not required
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-14 20:53:31,30
12829868,Add host kerberos client install does not have command text,"Kerb cluster, add host.
See screen shot

This issue appears to be a front end issue.  Here is the culprit: 

{code:title=app/templates/wizard/step9/step9HostTasksLogPopup.hbs:47}
{{taskInfo.role}} {{taskInfo.command}}
{code}

Rather then attempting to use the task's _command_detail_ value, the front end only displays the task's _role_ and _command_ values which are constrained by the code to be a certain set of values. The _command_detail_ field does not have that constraint.  

For the offending items in the relevant view,  the following values are set:
{code}
role: KERBEROS_CLIENT (translated somewhere to ""Kerberos Client"")
command: CUSTOM_COMMAND (translated somewhere to ""custom command"")
command_details: SET_KEYTAB KERBEROS/KERBEROS_CLIENT
{code}

On this note there are several items in that same display (for the Ambari server host) that read ""Ambari Server Action execute"". This is the same issue.

!add_service_log.png!

The background operations view displays these values properly:

!background_operations_log.png!
",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-14 14:43:22,30
12829865,"Kerberos: Selecting ""Manual"" option and then going back to ""Existing KDC"" option loses KDC host field and connection tester","*Steps to reproduce:*
# Create cluster
# Enable Kerberos
# Select Manual Option - See no KDC Host field and test button (correct)
# Select Back button
# Select MIT KDC Option - See no KDC Host field and test button (incorrect)


",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-14 14:29:33,30
12829863,Blueprint provisioned clusters don't have service states updated,"Clusters that are provisioned via the REST API don't have service states updated.  This doesn't affect the cluster starting and being operational but in cases where a component doesn't start, it isn't possible to start the component in the UI by starting the service.  Starting the individual host component will work successfully.

 ",blueprints,['blueprints'],AMBARI,Bug,Major,2015-05-14 14:21:45,120
12829317,Kerberos: missing identities for AMS in the CSV,"There are some naming consistency issues and there are missing entries in the CSV related to AMS.

*Naming inconsistencies*
* ams.zookeeper.principal
** Is: zookeeper/_HOST@$\{realm\}
** Should be: amszk@_HOST$\{realm\}
* ams.zookeeper.keytab 
** Is: $\{keytab_dir\}/zk.service.ams.keytab
** Should be: $\{keytab_dir\}/ams-zk.service.keytab

*Missing CSV entries*
* ams.collector.ketab
* ams-hbase.regionserver.keytab

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-05-12 20:18:30,30
12828832,falcon client not initalizing for secure clusters,"{code}
# falcon admin -version
ERROR: Unable to initialize Falcon Client object
{code}

It would also be immensely helpful if we print some logs some where for this. Or create an env var that when set allows us to see debug logs because i have no idea why this error came up and how to debug any further. Nothing ever made it to the server side logs.

*Solution*
The *.falcon.http.authentication.kerberos.name.rules need to be uodated such that they are the same as hadoop.security.auth_to_local. To do this, add the following under the relevant service item in the Falcon kerberos.json file:

{code}
      ""auth_to_local_properties"" : [
        ""falcon-startup.properties/*.falcon.http.authentication.kerberos.name.rules""
      ],
{code}

For example:

{code}
{
  ""services"": [
    {
      ""name"": ""FALCON"",
      ...
      ""auth_to_local_properties"" : [
        ""falcon-startup.properties/*.falcon.http.authentication.kerberos.name.rules""
      ],
      ...
   }
 ]
}
{code}


",falcon kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-05-11 14:40:22,30
12828388,Kerberos Wizard: Hide create_attributes_template when not configuring for Active Directory ,"Hide create_attributes_template when not configuring for Active Directory since it does not apply for MIT KDC configuration
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Major,2015-05-08 15:08:07,30
12828373,Kerberos: cleanup Manual kerb req text,"Update checkbox ""reqs"" on Page 1 of wizard with choice of Manual Kerb:

* Cluster hosts have network access to the KDC
* Kerberos client utilities (such as kinit) have been installed on every cluster host
* The Java Cryptography Extensions (JCE) have been setup on the Ambari Server host and all hosts in the cluster
* The Service and Ambari Principals will be manually created in the KDC before completing this wizard
* The keytabs for the Service and Ambari Principals will be manually created and distributed to cluster hosts before completing this wizard
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-08 14:44:32,30
12828368,Kerberos: manual kerb is installing kerb packages + overwriting krb5.conf,"enable kerb, select manual kerb option.

As first task inStop Services, it's performing kerb client install, which is installs 1) kerb packages + 2) overwriting my krb5.conf (shouldn't do either of these since this is a manual kerb).

{code}
2015-05-05 19:09:31,878 - Skipping installation of existing package krb5-workstation
2015-05-05 19:09:31,882 - Directory['/etc'] {'owner': 'root', 'group': 'root', 'mode': 0755, 'recursive': True}
2015-05-05 19:09:32,030 - File['/etc/krb5.conf'] {'owner': 'root', 'content': Template('krb5_conf.j2'), 'group': 'root', 'mode': 0644}
2015-05-05 19:09:32,206 - Writing File['/etc/krb5.conf'] because contents don't match
2015-05-05 19:09:32,491 - Execute['ambari-sudo.sh  -H -E touch /var/lib/ambari-agent/data/hdp-select-set-all.performed ; ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.2 | tail -1`'] {'not_if': 'test -f /var/lib/ambari-agent/data/hdp-select-set-all.performed', 'only_if': 'ls -d /usr/hdp/2.2*'}
2015-05-05 19:09:32,553 - Skipping Execute['ambari-sudo.sh  -H -E touch /var/lib/ambari-agent/data/hdp-select-set-all.performed ; ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.2 | tail -1`'] due to not_if
2
{code}

*Solution*
The wizard needs to set the following configurations when ""Manual"" is selected:

{{kerberos-env/manage_identities}} = ""false""
{{kerberos-env/install_packages}} = ""false""
{{krb5-conf/manage_krb5_conf}} = ""false""
{{kerberos-env/kdc-type}} = ""none""",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-08 13:52:21,30
12828358,Kerberos Wizard: Downloaded CSV file contains only default data,"The CSV file that is downloaded from the Kerberos Wizard does not contain the configured Kerberos identity details - it contains only default data. 

This is due to where in the workflow the Kerberos descriptor is posted to Ambari (as an artifact).  The data *is* posted during the transition between the _Stop Services_ page and the _Kerberize Cluster_ page. The data *should be* posted during the transition between the _Configure Identities_ page and the _Confirm Configuration_ page.",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-08 12:57:17,30
12828328,Kerberos: Keytab files are not distributed during add host if a retry is necessary during installation,"When adding a new host to a cluster where Kerberos is enabled and the installation of the new components fails, upon retry the keytabs are not distributed to the host after successfully installing the components.  _Note:  the new identities were not created either_.

*Workaround*
To recover from this, the missing keytabs can be regenerated using the _Regenerate Keytabs_ feature with the _missing only_ option specified. The component can then be started successfully.

*Steps to reproduce*
# Create cluster (can be small, one node with only HDFS and Zookeeper)
# Enable Kerberos
# Add new host with only DataNode (no clients, only to make the failure happen quicker)
# While the relevant hadoop packages are being installed, kill the package manger (i.e., yum, zypper, etc...)
# The installation of the component will fail and the retry button will be available
# Click the retry button and allow the installation to complete
# Startup of the Datanode component will fail due to missing keytab
{code}
2015-03-21 01:43:47,911 FATAL datanode.DataNode (DataNode.java:secureMain(2385)) - Exception in secureMain
java.io.IOException: Login failure for dn/c6504.ambari.apache.org@EXAMPLE.COM from keytab /etc/security/keytabs/dn.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user
{code}
_Note: Error indicates a keytab file was found but wrong password, this isn't the case since the keytab file was not on the host._
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-08 11:23:57,121
12828309,"Kerberos service check is named ""Create principals""","# Deploy cluster
# Enable security
# Run Kerberos service check

In background operations Kerberos service check is named as ""Create principals""",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-08 10:08:04,30
12828075,Ambari does not renew kerberos tickets,"If I go through setup of secure cluster, also execute ambari-server setup-security option #3, there are 2 problems:
# Ambari does not kinit using the specified principal and keytab
# If I manually kinit, all is well until the ticket expires - I would expect Ambari to renew ticket prior to expiry.
# The process uses the interactive user's ticket cache (separate Jira: AMBARI-11001)",JAAS,['ambari-server'],AMBARI,Bug,Critical,2015-05-07 18:25:55,30
12828073,Ambari uses users' interactive ticket cache,"It appears that it is necessary to kinit prior to starting ambari-server, even after ambari-server setup-security (#3). It seems that this should be automatically handled by Ambari. 

Ambari-server should NOT use the same ticket cache as the interactive user. 

STR:
1. kinit
2. ambari-server start
3. verify that ambari-server can authenticate with ticket specified in #1
4. kdestroy
5. try to authenticate through Ambari again (it will not work)

*Solution*
Ensure JAAS Login works properly such that the Kerberos tickets for the account that executes Ambari is not relevant.

",JAAS,['ambari-server'],AMBARI,Bug,Critical,2015-05-07 18:22:50,30
12827792,Kerberos: no need for Manage Identities checkbox,"See attached. On page 2 of the Kerb wizard, we have a checkbox for ""manage kerberos identities"". if the user chooses existing MIT or AD on page 1, this is a given. If they choose manage manually, this is not required.

Doesn't seems like there is a need for this checkbox on the wizard since page 1 answers the question.",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-06 20:10:51,30
12827786,"Kerberos: ""Attention: Some configurations need your attention"" msg in Enable Kerb Wizard hides fields","In the *Enable Kerberos Wizard*'s *Configure Kerberos* step when there are configurations that require attention the 'Attention: Some configurations need your attention before you can proceed.
+Show me properties with issues+' alert shows up.

When users click +Show me properties with issues+, and resolve their problems the alert disappears.  The issue is that there is no way to see all of the non-required properties without purposefully messing up one of the required ones so the alert comes back with the ""+Show all properties+"" link in it.

What should happen is as follows:
# If there are properties that require attention, a yellow attention div is presented with ""Attention: Some configurations need your attention before you can proceed.
+Show me properties with issues+""
# The user clicks on that link and start addressing the required configurations.
# When all of the configurations have been filled out that div is replaced with a green div with ""All configurations have been addressed. +Show all properties+""

See attached screenshots for example of issue in current implementation",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-06 19:38:51,30
12827440,Fix KERBEROS service reference to common services in HDP 2.3,"Fix KERBEROS service reference to common services in HDP 2.3

The metainfo.xml file should read
{code:title=stacks/HDP/2.3/services/KERBEROS/metainfo.xml}
<metainfo>
  <schemaVersion>2.0</schemaVersion>
  <services>
    <service>
      <name>KERBEROS</name>
      <extends>common-services/KERBEROS/1.10.3-10</extends>
    </service>
  </services>
</metainfo>
{code}",kerberos,['ambari-server'],AMBARI,Task,Critical,2015-05-05 21:38:35,30
12827290,Blueprints processor needs stronger validation of Blueprint JSON structure,"The Blueprints processor needs to have stronger validation during the process of handling a POST for a newly-submitted Blueprint from a REST client.
The Blueprints structure requires that the ""configurations"" element be an array of Maps, with each Map representing a given configuration type (""core-site"", ""storm-site"", etc).
Typically, the configurations structure should look like:
""""configurations"": [
{
""mapred-env"": {
""properties"":
{ ""mapreduce_log_dir_prefix"": ""/grid/0/log/hadoop-mapreduce"" }
}
},
{ ""mapred-site"": {
""properties"":
{ ""install-test-mapred-site"": ""install-test-mapred-site-VALUE"" }
}
},
{ ""yarn-env"": {
""properties"":
{ ""yarn_log_dir_prefix"": ""/grid/0/log/hadoop"" }
}
}
]""
The snippet above is just an example to illustrate the expected format.
Recent changes to the Blueprint's structure to handle ""properties_attributes"" (such as ""final"") have modified the structure of a Blueprint, so that configuration properties are now contained in a ""properties"" map, internal to each configuration type. Creating new Blueprints using this newer format (the older format is still acceptable to the processor) require that the configuration types are still enclosed in a map, even though an internal map is also present, which may be a source of confusion.
An example of an incorrect format would be:
""""configurations"": [
{
""mapred-env"": {
""properties"":
{ ""mapreduce_log_dir_prefix"": ""/grid/0/log/hadoop-mapreduce"" }
},
""mapred-site"": {
""properties"":
{ ""install-test-mapred-site"": ""install-test-mapred-site-VALUE"" }
},
""yarn-env"": {
""properties"":
{ ""yarn_log_dir_prefix"": ""/grid/0/log/hadoop"" }
}
]
""
Please note that in the example above a single map is created, with all configuration elements being added to that entry's configuration type. This causes a corrupted Blueprint to be stored and used for deployments, since all configuration elements added in the original Blueprint will be registered under one of the config types used in the original Blueprint. This will cause many types of cluster startup failures, as the configuration elements will be added to incorrect locations.

When the Blueprint processor receives a POST submission with a new Blueprint that includes this incorrect format for ""configurations"", the processor must reject this submission, and send back a relevant error code/message, so that the user can resolve this problem prior to attempting a cluster deployment.
This issue will not affect the older format for Blueprints, and so any backwards-compatible Blueprints used should work fine. This issue will only occur when new Blueprints are submitted that use the newer syntax/format for defining configuration properties (with support for properties_attributes) as well.
This JIRA is being created to track this issue, as it could potentially be a user experience issue going forward, and should probably be resolved in Ambari 2.1, if possible.",blueprints,['ambari-server'],AMBARI,Bug,Major,2015-05-05 13:59:33,121
12827273,"After disable Kerberos and change Realm, test principal with old realm is created when re-enabling Kerberos","After disable Kerberos and change Realm, test principal with old realm is created when re-enabling Kerberos
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-05 12:33:25,121
12826963,kerberos-env/kdc_type must not be mandatory if kerberos-env/manage_identities is false.,{{kerberos-env/kdc_type}} must not be mandatory if {{kerberos-env/manage_identities}} is _false_.,kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-04 16:59:12,30
12826445,Kerberos Identity data is empty when Cluster.security_type != KERBEROS,"Kerberos Identity data is empty when Cluster.security_type does not equal ""KERBEROS"".  

Technically this seems to make sense since there shouldn't be any relevant Kerberos identities if the cluster isn't configured for Kerberos; however a _chicken-and-the-egg_ condition is encountered when manually enabled Kerberos and a listing of the needed Kerberos identities is needed before Kerberos is to be fully enabled. 

*Solution*
Allow the keberos_identities API end points to generate data even if Cluster.security_type is not equal to ""KERBEROS"".",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-04-30 22:09:31,30
12826040,Kerberos: Password generator needs to generate passwords based on a pattern to satisfy password policy,"The password generator used to generate passwords for identities needs to generate passwords based on a pattern rather than just a random sequence of characters. 

Within the KDC, there may be a policy in place requiring a certain characteristics for the password. By creating a password consisting if 18 characters pulled randomly from {{abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890?.!$%^*()-_+=~}}, there is no guarantee that any specific policy will be met. 
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-29 18:19:32,30
12822869,"Slider view page throws ""Could not initialize class org.apache.hadoop.ipc.ProtobufRpcEngine""","Here is the exception stack from ambari logs -
{color:red}
{noformat}
21 Apr 2015 20:31:13,524 ERROR [qtp-client-1518] AbstractResourceProvider:181 - Caught exception getting view sub resources.
java.lang.RuntimeException: Could not initialize class org.apache.hadoop.ipc.ProtobufRpcEngine
        at org.apache.ambari.view.slider.SliderAppsViewControllerImpl.createSliderClient(SliderAppsViewControllerImpl.java:838)
        at org.apache.ambari.view.slider.SliderAppsViewControllerImpl$3.run(SliderAppsViewControllerImpl.java:510)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.ambari.view.slider.SliderAppsViewControllerImpl.invokeSliderClientRunnable(SliderAppsViewControllerImpl.java:507)
        at org.apache.ambari.view.slider.SliderAppsViewControllerImpl.getSliderApps(SliderAppsViewControllerImpl.java:899)
        at org.apache.ambari.view.slider.SliderAppsResourceProvider.getResources(SliderAppsResourceProvider.java:98)
        at org.apache.ambari.server.view.ViewSubResourceProvider.getResources(ViewSubResourceProvider.java:165)
        at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:945)
        at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:132)
        at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:475)
        at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:434)
        at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:407)
        at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:217)
        at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:68)
        at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:103)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:72)
        at org.apache.ambari.server.api.services.ViewInstanceService.getService(ViewInstanceService.java:96)
        at sun.reflect.GeneratedMethodAccessor152.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
/slider
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.apache.ambari.server.controller.FailsafeHandlerList.handleNonFailSafe(FailsafeHandlerList.java:148)
        at org.apache.ambari.server.controller.AmbariHandlerList.handleNonFailSafe(AmbariHandlerList.java:157)
        at org.apache.ambari.server.controller.FailsafeHandlerList.handle(FailsafeHandlerList.java:130)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:363)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.ipc.ProtobufRpcEngine
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:274)
        at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)
        at org.apache.hadoop.ipc.RPC.getProtocolEngine(RPC.java:204)
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:577)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:420)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:316)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:677)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:170)
        at org.apache.slider.common.tools.CoreFileSystem.<init>(CoreFileSystem.java:81)
        at org.apache.slider.common.tools.SliderFileSystem.<init>(SliderFileSystem.java:38)
        at org.apache.slider.client.SliderClient.initHadoopBinding(SliderClient.java:540)
        at org.apache.ambari.view.slider.SliderAppsViewControllerImpl$6.init(SliderAppsViewControllerImpl.java:817)
        at org.apache.ambari.view.slider.SliderAppsViewControllerImpl.createSliderClient(SliderAppsViewControllerImpl.java:831)
        ... 98 more
{noformat}
{color}",views,['ambari-views'],AMBARI,Bug,Major,2015-04-22 00:46:36,122
12822521,Use kerberos-env/executable_search_paths to indicate where to look for Kerberos utilities in HBASE agent-side scripts,"Use kerberos-env/executable_search_paths to indicate where to look for Kerberos utilities in HBASE agent-side scripts.

See solution for AMBARI-10452.",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-21 01:22:19,30
12821979,Add the ability to obtain details about required Kerberos identities,"Add the ability to obtain details about required Kerberos identities for the cluster.   These details should be obtained using a REST API call formatted as a JSON structure.  

Resulting JSON block per Kerberos identity:
{code}
    ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
    }
{code}

The data will be converted into CSV-formatted data similar to the file exported from Ambari 1.7 (as follows):

||host||description||principal||keytab file name||keytab file base path||keytab file owner||keytab file group||keytab file mode||
|host1|Ambari Smoke Test User|ambari-qa@EXAMPLE.COM|smokeuser.headless.keytab|/etc/security/keytabs|ambari-qa|hadoop|440|
|host1|HDFS User|hdfs@EXAMPLE.COM|hdfs.headless.keytab|/etc/security/keytabs|hdfs|hadoop|440|
|host1|HDFS SPNEGO User|HTTP/host1@EXAMPLE.COM|spnego.service.keytab|/etc/security/keytabs|root|hadoop|440|
|host1|HDFS SPNEGO User|HTTP/host1@EXAMPLE.COM|spnego.service.keytab|/etc/security/keytabs|root|hadoop|440|
|host1|DataNode|dn/host1@EXAMPLE.COM|dn.service.keytab|/etc/security/keytabs|hdfs|hadoop|400|
|host1|NameNode|nn/host1@EXAMPLE.COM|nn.service.keytab|/etc/security/keytabs|hdfs|hadoop|400|
|host1|ZooKeeper Server|zookeeper/host1@EXAMPLE.COM|zk.service.keytab|/etc/security/keytabs|zookeeper|hadoop|400|


*Solution*
The following API calls are to be used to obtain the data:

{code:title=GET /api/v1/clusters/c1/hosts?fields=kerberos_identities/*}
{
  ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts?fields=kerberos_identities/*"",
  ""items"" : [
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host1""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/HTTP%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/dn%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/nm%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/nn%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""namenode_nn"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""nn/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/zookeeper%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""zookeeper_zk"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""zookeeper"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""zookeeper/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host2""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/HTTP%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/dn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/jhs%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""history_server_jhs"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""mapred"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/jhs.service.keytab"",
            ""principal_local_username"" : ""mapred"",
            ""principal_name"" : ""jhs/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/nm%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/nn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""secondary_namenode_nn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""nn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/rm%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""resource_manager_rm"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/rm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""rm/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/yarn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""app_timeline_server_yarn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/yarn.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""yarn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/zookeeper%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""zookeeper_zk"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""zookeeper"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""zookeeper/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host3""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/HTTP%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/amshbase%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""ams_hbase_master_hbase"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""ams"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/ams-hbase.master.keytab"",
            ""principal_local_username"" : ""ams"",
            ""principal_name"" : ""amshbase/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/dn%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/nm%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/zookeeper%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""ams_zookeeper"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""ams"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.ams.keytab"",
            ""principal_local_username"" : ""ams"",
            ""principal_name"" : ""zookeeper/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    }
  ]
}
{code}

{code:title=GET /api/v1/clusters/c1/kerberos_identities?fields=*}
{
  ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities?fields=*"",
  ""items"" : [
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nn%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""namenode_nn"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""nn/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""zookeeper_zk"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""zookeeper"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""zookeeper/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/jhs%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""history_server_jhs"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""mapred"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/jhs.service.keytab"",
        ""principal_local_username"" : ""mapred"",
        ""principal_name"" : ""jhs/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""secondary_namenode_nn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""nn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/rm%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""resource_manager_rm"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/rm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""rm/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/yarn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""app_timeline_server_yarn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/yarn.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""yarn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""zookeeper_zk"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""zookeeper"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""zookeeper/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/amshbase%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""ams_hbase_master_hbase"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""ams"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/ams-hbase.master.keytab"",
        ""principal_local_username"" : ""ams"",
        ""principal_name"" : ""amshbase/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""ams_zookeeper"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""ams"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.ams.keytab"",
        ""principal_local_username"" : ""ams"",
        ""principal_name"" : ""zookeeper/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    }
  ]
}
{code}

{code:title=GET /api/v1/clusters/c1/kerberos_identities?fields=*&format=csv}
host,description,principal name,principal type,local username,keytab file path,keytab file owner,keytab file owner access,keytab file group,keytab file group access,keytab file mode,keytab file installed
host1,/spnego,HTTP/host1@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host1,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host1,datanode_dn,dn/host1@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host1,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host1,nodemanager_nm,nm/host1@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host1,namenode_nn,nn/host1@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/nn.service.keytab,hdfs,r,hadoop,,400,true
host1,zookeeper_zk,zookeeper/host1@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/zk.service.keytab,zookeeper,r,hadoop,,400,true
host2,/spnego,HTTP/host2@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host2,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host2,datanode_dn,dn/host2@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host2,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host2,history_server_jhs,jhs/host2@EXAMPLE.COM,SERVICE,mapred,/etc/security/keytabs/jhs.service.keytab,mapred,r,hadoop,,400,true
host2,nodemanager_nm,nm/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host2,secondary_namenode_nn,nn/host2@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/nn.service.keytab,hdfs,r,hadoop,,400,true
host2,resource_manager_rm,rm/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/rm.service.keytab,yarn,r,hadoop,,400,true
host2,app_timeline_server_yarn,yarn/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/yarn.service.keytab,yarn,r,hadoop,,400,true
host2,zookeeper_zk,zookeeper/host2@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/zk.service.keytab,zookeeper,r,hadoop,,400,true
host3,/spnego,HTTP/host3@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host3,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host3,ams_hbase_master_hbase,amshbase/host3@EXAMPLE.COM,SERVICE,ams,/etc/security/keytabs/ams-hbase.master.keytab,ams,r,hadoop,,400,true
host3,datanode_dn,dn/host3@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host3,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host3,nodemanager_nm,nm/host3@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host3,ams_zookeeper,zookeeper/host3@EXAMPLE.COM,SERVICE,ams,/etc/security/keytabs/zk.service.ams.keytab,ams,r,hadoop,,400,true
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-04-17 19:37:38,30
12821690,NameNode Restart fails after attempt to Kerberize Cluster,"When attempting to restart the HDFS NameNode after running the Kerberos wizard to enable Kerberos, the NameNode fails to startup.  

The underlying failure in the ambari-agent appears to be:

""Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 298, in <module>
    NameNode().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 214, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 72, in start
    namenode(action=""start"", rolling_restart=rolling_restart, env=env)
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py"", line 38, in namenode
    setup_ranger_hdfs()
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py"", line 66, in setup_ranger_hdfs
    hdfs_repo_data = hdfs_repo_properties()
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py"", line 194, in hdfs_repo_properties
    config_dict['dfs.datanode.kerberos.principal'] = params._dn_principal_name
AttributeError: 'module' object has no attribute '_dn_principal_name'""

This keeps the HDFS NameNode from starting up properly after Kerberos is Enabled, and this seems to keep the process of Enabling Kerberos from completing.  

The problem appears to be a Python coding issue where _private_ variables (declared with a leading underscore) are not imported from {{common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py}} into {{common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py}}.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-04-16 21:47:26,30
12821299,Local root user's group being assigned to hadoop,"When installing Ranger through Ambari, you have the option to specify DB user names. If you leave the Ranger DB root user configuration db_root_user as the default (which is 'root'), Ambari will erroneously attempt to create a local user named root, and assign it to the group 'hadoop'.  This results in local root users being reassigned to the group 'hadoop'.  In addition, both the db_user param and audit_db_name param are erroneously being used to create local users (granted, with less severe consequences).",patch security,"['ambari-server', 'security']",AMBARI,Bug,Critical,2015-04-15 20:55:32,123
12820969,Provide option to not install Kerberos Client packages,"In some environments, the Kerberos Client packages are already installed on the machines and the operator does not want to install the OS kerberos client packages.

Ambari should provide an option in the Enable Kerberos Wizard to not install Kerberos Clients on the machines.

Under the Advanced krb.conf section, at the bottom, expose this checkbox for this option:

{code}
[ ] Do not install Kerberos Client packages
{code}

After enabling Kerberos, this configuration option should be visible under the Services > Kerberos > Configs.

This would instruct Ambari to not install the Kerberos Client packages when installing the Kerberos Client component. We can still have Ambari install the Kerberos Client component, just circumvent the install packages.

*Note:* If selected, it is assumed that the user will install packages containing executables compatible with the MIT Kerberos 5 version 1.10.3",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-14 21:18:58,30
12820902,Add the ability to enable Kerberos and not manage identities,"Add the ability to enable Kerberos and not manage identities.  This should be done by allowing a user to specify whether all relevant Kerberos identities _should_ or _should not_ be managed by Ambari.  

A *kerberos-env* property named *manage_identities* is to be added where its value may be either _true_ or _false_.  By default the value is _true_ (or rather _not false_).  

If _not false_, Ambari will access the registered KDC to create, update, and delete Kerberos identities as needed.  Ambari will also create, distribute, and delete keytab files as needed. Because of this, the KDC administrator credentials are required. This is the current behavior of Ambari 2.0.0.

If _false_, Ambari will *not* access the registered KDC to create, update, or delete Kerberos identities.  It will also *not* create, distribute, or delete keytab files. Not KDC administrator credentials will be needed.

Note: a lot of this work has been done for AMBARI-10305.  A current known problem with the solution for AMBARI-10305 is that the Kerberos service check fails when kerberos-env/manage_identities is false due to missing data since the special smoke user was not created.",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-14 18:46:43,30
12820899,Manually enable Kerberos security,"Provide an option for users that want to enable Kerberos in the cluster via Ambari but do not want any automation. With this option, ambari will not require any access to the KDC, will not install kerberos clients, will not attempt to generate any principals or keytabs and will not distribute any keytabs. Keytab regeneration will not be available, and when there are changes to the cluster (add service, add/remove/change host), the user is responsible for creating principals and making sure the appropriate keytabs are in place on the host for proper cluster function (although Ambari should handle updating any configs).

Effectively, this above option provides a manual Kerberos option for users that are looking to have the similar ""hands-off"" ambari kerberos experience of 1.7.0 or earlier.

On the Kerberos Wizard, provide an option (below Existing MIT KDC and Existing Active Directory):

[ ] Manage Kerberos principals and keytabs manually

Which will send the wizard thru a path that does not prompt for KDC information, or attempt to install clients or create principals/keytabs. The user should have a chance to Configure Identities as part of the wizard and the wizard will push the configs, performs restarts, etc. Users should have an option to download a CSV of principals, keytabs, hosts, locations, permissions, ownership.",kerberos,"['alerts', 'ambari-agent', 'ambari-server']",AMBARI,Epic,Major,2015-04-14 18:41:23,30
12820294,Ambari unable to start services using non-default kinit_path_local,"PROBLEM: Ambari is unable to start services after running the Enable Security wizard on a cluster that uses a non-standard path to the Kerberos utilities, such as kinit.

STEPS TO REPRODUCE: 
1. Start with non-Kerberized cluster (2.2 Sandbox works fine)
2. Move Kerberos utilities from /usr/bin/ to a new location, example: /usr/myorg/bin/
3. Run 'Enable Security' wizard in Ambari, specify new path for kinit, Apply
4. Watch 'Start All Services' step fail
5. Attempt to 'Restart all components with Stale Configs for HDFS,' which fails with the following error:

Fail: Execution of ' -kt /etc/security/keytabs/hdfs.headless.keytab hdfs' returned 127. -bash: -kt: command not found

Due to the error that occurs with trying to manually restart the HDFS service, it seems like kinit_path_local is empty when the path to kinit is modified. It looks like each service uses the function from:
./ambari-common/src/main/python/resource_management/libraries/functions/get_kinit_path.py

But typically only these three hardcoded paths are passed to that function:
/usr/bin, /usr/kerberos/bin, /usr/sbin

The custom path defined in Ambari is never passed to that function, so the result is always empty.
",kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2015-04-11 11:07:32,30
12787208,"Kerberos: during disable, need option skip if unable to access KDC to remove principals","Attempted to disable kerb, fails on step to unkerberize because KDC admin is locked out.

Click retry, can't make it past that.

Need option to skip and finish ""disable kerberos"" even if Ambari cannot get the principals cleaned up (i.e. cannot access the KDC) Losing access to the KDC and attempting to disable where ambari can't clean-up the principals should be a skip'able step. User should still be able to get to a clean, not-enabled-kerberos-ambari-state w/o accessing the KDC.

*Solution*
Add a flag to the kerberos-env configuration to specify whether Kerberos identities should be managed by Ambari (true, default) or not (false).  This flag is to be overridable via a _directive_ like {{manage_identities=false}} when disabling Kerberos, which will skip over any KDC administrative processes. ",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-04-01 00:35:43,30
12786203,Principal and Keytab configuration specifications are ignored when disabling Kerberos,"Principal and Keytab configuration specifications are ignored when disabling Kerberos and thus they are not updated or removed as needed.
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-27 13:51:24,30
12784826,Storm service check failed after disabling security,"From nimbus.log
{code}
 java.lang.NullPointerException: null
	at backtype.storm.security.auth.authorizer.SimpleACLAuthorizer.permit(SimpleACLAuthorizer.java:93) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.5.1.jar:na]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:773) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:777) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.daemon.nimbus$fn__4017$exec_fn__1597__auto__$reify__4032.getClusterInfo(nimbus.clj:1215) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:1668) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:1656) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:32) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:34) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:156) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.server.THsHaServer$Invocation.run(THsHaServer.java:201) [storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_67]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_67]
{code}

*Solution*
The solution is to remove configuration properties that do not have default values when disabling Kerberos.

{code}
For each property updated per the Kerberos descriptor
    Look up the default value from the stack definition
    If a value is found, update the property to that ""default"" value
    If a value is not found, remove the value from the configuration
{code}

For Storm in particular, remove the following properties:
- storm-site/drpc.authorizer
- storm-site/nimbus.authorizer
- storm-site/storm.principal.tolocal
- storm-site/ui.filter",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-23 14:36:52,30
12782947,Wrong alert on added host on secure cluster,"*STR*
Install HDP
Enable security
Add host with datanode

*Result*
For added host there is an alert:
Connection failed to http://c6404.ambari.apache.org:1022
Manually this webpage is avaliable

*Expected*
No alert

*Cause*
The error that gets generated is 
{code}
Local variable 'error_msg' might be referenced before assignment
{code}

*Solution*
Fix Python code by initializing {{error_msg}} properly.,",alerts,['ambari-server'],AMBARI,Bug,Critical,2015-03-18 17:29:11,30
12782535,Hive alert on secured cluster,"When Kerberos is enabled, Hive components show alerts due to the following error:

{code}
WARNING 2015-03-16 06:01:08,253 base_alert.py:140 - [Alert][hive_metastore_process] Unable to execute alert. Execution of '/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa; ' returned 1. kinit: Keytab contains no suitable keys for ambari-qa@REALM while getting initial credentials
{code}

This occurs because the alert logic for Hive uses {{cluster-env/smokeuser}} rather than {{cluster-env/smokeuser_principal_name}} to get the principal name for the smoke test identity. 
",alerts kerbeos,['ambari-server'],AMBARI,Bug,Critical,2015-03-17 11:07:16,30
12782436,"RU - Unable to begin RU due to MySQL ""Error preallocating sequence numbers""","Tried to make an RU,

* Deployed clustered, enabled Namenode HA, distributed bits
* Attempt to begin an RU (through both UI and curl call)
{code}
curl -X POST -u admin:admin -H 'X-Requested-By:1' http://server:8080/api/v1/clusters/cl1/upgrades -d '{""Upgrade"": {""repository_version"": ""2.2.3.0-2610""}}'
{
  ""status"": 500,
  ""message"": ""\nException Description: Error preallocating sequence numbers.  The sequence table information is not complete.""
}
{code}

{code}
Exception Description: Error preallocating sequence numbers.  The sequence table information is not complete.
{code}

Saw this on two clusters (Ubuntu and SP3) with MySQL Ambari DB that I deployed to EC2.
Ambari version, 2.0.0-137
Database: MySQL

{code}
mysql> select * from ambari_sequences;
+--------------------------+----------------+
| sequence_name            | sequence_value |
+--------------------------+----------------+
| alert_current_id_seq     |            132 |
| alert_definition_id_seq  |             64 |
| alert_group_id_seq       |             20 |
| alert_history_id_seq     |            456 |
| alert_notice_id_seq      |              0 |
| alert_target_id_seq      |              0 |
| cluster_id_seq           |              2 |
| cluster_version_id_seq   |              2 |
| configgroup_id_seq       |              1 |
| config_id_seq            |             95 |
| group_id_seq             |              1 |
| host_role_command_id_seq |           1001 |
| host_version_id_seq      |             12 |
| member_id_seq            |              1 |
| operation_level_id_seq   |              5 |
| permission_id_seq        |              5 |
| principal_id_seq         |              2 |
| principal_type_id_seq    |              3 |
| privilege_id_seq         |              1 |
| repo_version_id_seq      |              2 |
| requestschedule_id_seq   |              1 |
| resourcefilter_id_seq    |             20 |
| resource_id_seq          |              4 |
| resource_type_id_seq     |              8 |
| service_config_id_seq    |             31 |
| upgrade_id_seq           |              2 |
| upgrade_item_id_seq      |              0 |
| user_id_seq              |              2 |
| viewentity_id_seq        |              0 |
| view_instance_id_seq     |              2 |
+--------------------------+----------------+
30 rows in set (0.00 sec)
{code}",rolling_upgrade,['ambari-server'],AMBARI,Bug,Major,2015-03-17 01:29:12,69
12781984,Kerberos service check doesn't work is new HTTP session is created,"# Create cluster
# Enable security
# Logout or restart Ambari server
# Go to Kerberos service page
# Click service actions
# Click Run Service Check
# Nothing happens",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-14 09:48:29,30
12781969,"RU - AMS service check may fail, need to exclude AMS from Service Check Group in Upgrade Pack","During a rolling upgrade, the AMS service check is failing.
Since AMS is not part of the upgrade pack, Ambari RU should be robust enough to proceed without it in case it does fail.",rolling_upgrade,['ambari-server'],AMBARI,Bug,Major,2015-03-14 02:06:58,69
12781670,RU failed on RESTART Oozie Server because hadoop-lzo should only be copied if lzo compression is enabled ,"STR:
Deploy a cluster with Oozie
Confirm that lzo compression is not enabled ( http://docs.hortonworks.com/HDPDocuments/Ambari-1.6.0.0/bk_ambari_reference/content/ambari-ref-lzo-configure.html)
Comment a Rolling Upgrade, which will fail in Oozie because it will not find the hadoop-lzo jar.

The jar should only be copied if lzo is enabled.

{code}
2015-03-11 21:32:49,019 - u""Execute['hdp-select set oozie-server 2.2.2.0-2591']"" {}
2015-03-11 21:32:49,064 - Restoring Oozie configuration directory after upgrade...
2015-03-11 21:32:49,065 - Extracting /tmp/oozie-upgrade-backup/oozie-conf-backup.tar to /etc/oozie/conf
2015-03-11 21:32:49,179 - Error while executing command 'restart':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 214, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 365, in restart
    self.pre_rolling_restart(env)
  File ""/var/lib/ambari-agent/cache/common-services/OOZIE/4.0.0.2.0/package/scripts/oozie_server.py"", line 165, in pre_rolling_restart
    oozie_server_upgrade.prepare_libext_directory()
  File ""/var/lib/ambari-agent/cache/common-services/OOZIE/4.0.0.2.0/package/scripts/oozie_server_upgrade.py"", line 141, in prepare_libext_directory
    hadoop_client_new_lib_dir, hadoop_lzo_pattern))
Fail: There are no files at /usr/hdp/2.2.2.0-2591/hadoop/lib matching hadoop-lzo*.jar
2015-03-11 21:32:49,235 - Command: /usr/bin/hdp-select status oozie-server > /tmp/tmpVfVrGL
Output: oozie-server - 2.2.2.0-2591
{code}",rolling_upgrade,['ambari-server'],AMBARI,Bug,Major,2015-03-13 00:32:29,69
12781640,"The path(s) to the Kerberos utilities (kadmin, klist, etc...) should be configurable","The path(s) to the Kerberos utilities (kadmin, klist, etc...) should be configurable so that the utilities can be found if using custom Kerberos packages. 

This should work for both the Ambari server and agent-side functions. 
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-12 22:44:36,30
12781567,Some of Hive principal fields doesn't have default value when enabling security,"1.Go to installed cluster
2.Open Enable Security wizard
3.Proceed to step Configure Identities
4.Go to advanced tab
some principal values on Hive panel are empty",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-03-12 17:51:20,30
12781564,500 error on installing kerberos clients,"On stage of installing kerberos clients appears error 500 with text:
{code:java}
An internal system exception occurred; Unexpected error condition executing kadmin command§
{code}
This is due to a bad search path used to find kadmin. It should be able to be set by a user.",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-12 17:35:52,30
12781524,Test Kerberos Client (KERBEROS_SERVICE_CHECK) is failed after Ambari only upgrade from 1.6.0 to 2.0.0 when enable securitty (Centos 5.9),"*Cluster*: http://172.18.145.129:8080
STR:
1)Deploy old version with all services
2)Make ambari only upgrade to 2.0.0 .
3)Enable security

Expected result:
Enable security is passed.

Actual result
KERBEROS_SERVICE_CHECK is failed after Ambari only upgrade from 1.6.0 to 2.0.0 when enable securitty
{code}
--------------------------------------------------------------------------------
{
  ""href"" : ""http://172.18.145.129:8080/api/v1/clusters/cl1/requests/8/tasks/258"",
  ""Tasks"" : {
    ""attempt_cnt"" : 1,
    ""cluster_name"" : ""cl1"",
    ""command"" : ""SERVICE_CHECK"",
    ""command_detail"" : ""SERVICE_CHECK KERBEROS"",
    ""end_time"" : 1426091879341,
    ""error_log"" : ""/var/lib/ambari-agent/data/errors-258.txt"",
    ""exit_code"" : 1,
    ""host_name"" : ""amb-upg160-rhel5oracle1426082627-9.cs1cloud.internal"",
    ""id"" : 258,
    ""output_log"" : ""/var/lib/ambari-agent/data/output-258.txt"",
    ""request_id"" : 8,
    ""role"" : ""KERBEROS_SERVICE_CHECK"",
    ""stage_id"" : 4,
    ""start_time"" : 1426091872413,
    ""status"" : ""FAILED"",
    ""stderr"" : ""Traceback (most recent call last):\n  File \""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py\"", line 61, in <module>\n    KerberosServiceCheck().execute()\n  File \""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\"", line 214, in execute\n    method(env)\n  File \""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py\"", line 55, in service_check\n    os.remove(ccache_file_path)\nOSError: [Errno 2] No such file or directory: '/var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_49ae9e4ea129d8216306a4a6e431c812'"",
    ""stdout"" : ""Performing kinit using ambari-qa_vvwbohue@EXAMPLE.COM\n2015-03-11 16:37:59,095 - u\""Execute['/usr/kerberos/bin/kinit -c /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_49ae9e4ea129d8216306a4a6e431c812 -kt /etc/security/keytabs/kerberos.service_check.vvwbohue.keytab ambari-qa_vvwbohue@EXAMPLE.COM']\"" {}"",
    ""structured_out"" : { }
  }
}
--------------------------------------------------------------------------------

{code}
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-12 15:30:14,30
12781015,Kerberos: Password generator needs to generate passwords based on rules to satisfy password policy,"The password generator used to generate passwords for identities needs to generate passwords based on a rule set rather than just a random sequence of characters. 

In a KDC (MIT or Active Directory), there may be a policy in place requiring a certain characteristics for the password. By creating a password consisting if 18 characters pulled randomly from {{abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890?.!$%^*()-_+=~}}, there is no guarantee that any specific policy will be met. 

The following rules should be settable:
* Length
* Minimum number of lowercase letters (a-z)
* Minimum number of uppercase letters (A-Z)
* Minimum number of digits (0-9)
* Minimum number of punctuation characters ({{?.!$%^*()-_+=~}})",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-10 22:39:09,30
12780985,Kerberos: Run ambari-server using non-root causes issues with AD velocity engine,"setup ambari-server to run with non-root daemon (ambari-server setup, select a non-root daemon account) and the following exception will be thrown when creating identities in an Active Directory when enabling Kerberos.

{code}
java.lang.RuntimeException: Velocity could not be initialized!
	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:307)
	at org.apache.velocity.runtime.RuntimeInstance.parse(RuntimeInstance.java:1196)
	at org.apache.velocity.runtime.RuntimeInstance.parse(RuntimeInstance.java:1181)
	at org.apache.velocity.runtime.RuntimeInstance.evaluate(RuntimeInstance.java:1297)
	at org.apache.velocity.runtime.RuntimeInstance.evaluate(RuntimeInstance.java:1265)
	at org.apache.velocity.app.Velocity.evaluate(Velocity.java:180)
	at org.apache.ambari.server.view.ViewContextImpl.parameterize(ViewContextImpl.java:381)
	at org.apache.ambari.server.view.ViewContextImpl.getProperties(ViewContextImpl.java:194)
	at org.apache.ambari.view.filebrowser.HdfsService.getApi(HdfsService.java:67)
	at org.apache.ambari.view.filebrowser.FileOperationService.listdir(FileOperationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:708)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1329)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:166)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.FailsafeHandlerList.handle(FailsafeHandlerList.java:132)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:363)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.velocity.exception.VelocityException: Error initializing log: Failed to initialize an instance of org.apache.velocity.runtime.log.Log4JLogChute with the current runtime configuration.
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:875)
	at org.apache.velocity.runtime.RuntimeInstance.init(RuntimeInstance.java:262)
	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:302)
	... 93 more
Caused by: org.apache.velocity.exception.VelocityException: Failed to initialize an instance of org.apache.velocity.runtime.log.Log4JLogChute with the current runtime configuration.
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:220)
	at org.apache.velocity.runtime.log.LogManager.updateLog(LogManager.java:269)
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:871)
	... 95 more
Caused by: java.lang.RuntimeException: Error configuring Log4JLogChute : 
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.velocity.util.ExceptionUtils.createWithCause(ExceptionUtils.java:67)
	at org.apache.velocity.util.ExceptionUtils.createRuntimeException(ExceptionUtils.java:45)
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:133)
	at org.apache.velocity.runtime.log.Log4JLogChute.init(Log4JLogChute.java:85)
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:157)
	... 97 more
Caused by: java.io.FileNotFoundException: velocity.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:142)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
	at org.apache.log4j.FileAppender.<init>(FileAppender.java:110)
	at org.apache.log4j.RollingFileAppender.<init>(RollingFileAppender.java:79)
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:118)
	... 99 more
{code}",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-10 20:45:17,30
12780561,Maintenance Mode Hosts do not get keytabs,"A downed host behaves a little differently then a host that is up and in a maintenance state.  

It appears that where a downed host is not included in steps to install the Kerberos client nor is it included when choosing a host to run the service check on. 

However a host that is up and in maintenance mode is not included in steps to install the Kerberos client, but may be selected as the host to run the service check on. Resulting in this error:

{code}
2015-03-04 20:58:03,164 - Error while executing command 'service_check':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 208, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py"", line 58, in service_check
    raise Fail(err_msg)
Fail: Failed to execute kinit test due to principal or keytab not found or available
{code}

The error is correct in that the keytab has not been distributed to it, this is because the Kerberos Client has not been installed on it.  
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-09 18:17:10,30
12780474,"Oozie connection refused: unable to initialize service, Cannot get password from user.","Oozie connection refused: unable to initialize service, Cannot get password from user.

{noformat}
org.apache.oozie.service.ServiceException: E0100: Could not initialize service [org.apache.oozie.service.HadoopAccessorService], Login failure for oozie/ip-172-31-34-48.ec2.internal@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:187)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:130)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:101)
	at org.apache.oozie.service.Services.setServiceInternal(Services.java:383)
	at org.apache.oozie.service.Services.setService(Services.java:369)
	at org.apache.oozie.service.Services.loadServices(Services.java:302)
	at org.apache.oozie.service.Services.init(Services.java:210)
	at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:45)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4210)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4709)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:799)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)
	at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:675)
	at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:601)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:502)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1317)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:324)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1065)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1057)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)
	at org.apache.catalina.core.StandardService.start(StandardService.java:525)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:754)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:595)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)
Caused by: java.io.IOException: Login failure for oozie/ip-172-31-34-48.ec2.internal@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)
	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:179)
	... 31 more
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:926)
	... 32 more
{noformat}

This issue is really related to unexpected data in the oozie-site.xml file. It is expected that the principals names contain {{_HOST}} rather than the resolved hostname. This appears to be a viable solution for Oozie version 4.1.0.2.2.2.0, but not for version 4.0.0.2.0.14.0.

",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-09 10:56:04,30
12780444,Ambari storm logviewer in secure mode doesn't work,"Storm logviewer uses the same UI.filter config thats being used for Storm UI.
In secure mode storm UI uses SPENGO to authenticate user to access the UI.
Similarly logviewer also does the same .
But in Ambari 1.7 we advise user to create HTTP/storm-ui@REALM and this gets added to storm.yaml.
As this is bound to a host storm logviewers which are running one per supervisor won't be able to use this key .

Solution:
There is a configuration problem in the {{/etc/storm/conf/storm.yaml}} file.  In particular the issue is here:
{code:title=/etc/storm/conf/storm.yaml:109}
ui.filter.params:
  ""type"": ""kerberos""
  ""kerberos.principal"": ""HTTP/host-2.internal@EXAMPLE.COM""
  ""kerberos.keytab"": ""/etc/security/keytabs/spnego.service.keytab""
  ""kerberos.name.rules"": ""DEFAULT""
{code}

The {{kerberos.principal}} value should be the SPNEGO principal for the localhost, not the host where the UI server is running.  In this example, the localhost is *host-4.internal*  so the {{kerberos.principal}} value should be *HTTP/host-4.internal@EXAMPLE.COM* not *HTTP/host-2.internal@EXAMPLE.COM*.  The Storm UI server is running on *host-2.internal*

The fix for this should be in the code around 

{code:title=common-services/STORM/0.9.1.2.1/package/scripts/params.py:103} 
    _storm_ui_jaas_principal_name = config['configurations']['storm-env']['storm_ui_principal_name']
    storm_ui_host = default(""/clusterHostInfo/storm_ui_server_hosts"", [])
    storm_ui_jaas_principal = _storm_ui_jaas_principal_name.replace('_HOST',storm_ui_host[0].lower())
{code}

{{storm_ui_jaas_principal}} is then used in the template to build the storm.yaml file.  
",kerberos,[],AMBARI,Bug,Major,2015-03-09 09:48:54,30
12780171,"On GlusterFS stack, Enable Security Wizard doesn't actually enable secure mode","When I try to enable Hadoop Secure Mode via Ambari ""Enable Security Wizard"" on
GlusterFS stack, the properties which controls security are not changed.
Moreover Ambari reports that Secure mode is enabled and didn't notice
the problem.

h3. Actual results

There are no {{hadoop.security}} properties defined anywhere:

{noformat}
# grep hadoop.security /etc/hadoop/conf/*.xml
#
{noformat}

This means that the security mode hasn't been actually enabled.

Note that keytab definitons are configured fine, see eg.:

{noformat}
# grep keytab /etc/hadoop/conf/*.xml
/etc/hadoop/conf/mapred-site.xml:      <name>mapreduce.jobhistory.webapp.spnego-keytab-file</name>
/etc/hadoop/conf/mapred-site.xml:      <value>/etc/security/keytabs/spnego.service.keytab</value>
/etc/hadoop/conf/mapred-site.xml:      <name>mapreduce.jobhistory.keytab.file</name>
/etc/hadoop/conf/mapred-site.xml:      <value>/etc/security/keytabs/jhs.service.keytab</value>
/etc/hadoop/conf/mapred-site.xml:      <name>mapreduce.jobhistory.keytab</name>
/etc/hadoop/conf/mapred-site.xml:      <value>/etc/security/keytabs/jhs.service.keytab</value>
/etc/hadoop/conf/yarn-site.xml:      <name>yarn.nodemanager.webapp.spnego-keytab-file</name>
/etc/hadoop/conf/yarn-site.xml:      <value>/etc/security/keytabs/spnego.service.keytab</value>
/etc/hadoop/conf/yarn-site.xml:      <name>yarn.nodemanager.keytab</name>
/etc/hadoop/conf/yarn-site.xml:      <value>/etc/security/keytabs/nm.service.keytab</value>
/etc/hadoop/conf/yarn-site.xml:      <name>yarn.resourcemanager.webapp.spnego-keytab-file</name>
/etc/hadoop/conf/yarn-site.xml:      <value>/etc/security/keytabs/spnego.service.keytab</value>
/etc/hadoop/conf/yarn-site.xml:      <name>yarn.resourcemanager.keytab</name>
/etc/hadoop/conf/yarn-site.xml:      <value>/etc/security/keytabs/rm.service.keytab</value>
{noformat}

h3. Expected results

Following configuration properties can be found in {{core-site}} conf file.

{code:xml}
<property>
   <name>hadoop.security.authentication</name>
   <value>kerberos</value>
</property>

<property>
  <name>hadoop.security.authorization</name>
  <value>true</value>
</property>

<property>
  <name>hadoop.security.auth_to_local</name>
  <value>RULE:[2:$1@$0](rm@.*RHSHADOOPQA.REDHAT.COM)s/.*/yarn/
RULE:[2:$1@$0](nm@.*RHSHADOOPQA.REDHAT.COM)s/.*/yarn/
RULE:[2:$1@$0](nn@.*RHSHADOOPQA.REDHAT.COM)s/.*/hdfs/
RULE:[2:$1@$0](dn@.*RHSHADOOPQA.REDHAT.COM)s/.*/hdfs/
RULE:[2:$1@$0](hbase@.*RHSHADOOPQA.REDHAT.COM)s/.*/hbase/
RULE:[2:$1@$0](hbase@.*RHSHADOOPQA.REDHAT.COM)s/.*/hbase/
RULE:[2:$1@$0](oozie@.*RHSHADOOPQA.REDHAT.COM)s/.*/oozie/
RULE:[2:$1@$0](jhs@.*RHSHADOOPQA.REDHAT.COM)s/.*/mapred/
RULE:[2:$1@$0](jn/_HOST@.*RHSHADOOPQA.REDHAT.COM)s/.*/hdfs/
RULE:[2:$1@$0](falcon@.*RHSHADOOPQA.REDHAT.COM)s/.*/falcon/
DEFAULT</value>
</property>
{code}

Expected configuration is based on result of ""Enable Security Wizard"" for
normal HDFS stack.
",glusterfs keberos secure,['stacks'],AMBARI,Bug,Major,2015-03-06 21:08:42,124
12779663,RU - Service Check group to include all services with a service_check script,"Installed a minimal 3-node cluster with HDFS, MR, YARN, Pig, Tez.
Performed an RU.

Expected result is for the last service check to be ran on all components. However, it skipped Pig Service Check.

http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8
{code}
{
  ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8"",
  ""UpgradeGroup"" : {
    ""completed_task_count"" : 4,
    ""group_id"" : 8,
    ""in_progress_task_count"" : 0,
    ""name"" : ""SERVICE_CHECK"",
    ""progress_percent"" : 100.0,
    ""request_id"" : 32,
    ""status"" : ""COMPLETED"",
    ""title"" : ""All Service Checks"",
    ""total_task_count"" : 4
  },
  ""upgrade_items"" : [
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/47"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 47
      }
    },
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/48"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 48
      }
    },
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/49"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 49
      }
    },
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/50"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 50
      }
    }
  ]
}
{code}

http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items?fields=UpgradeItem/text
{code}
{
  ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items?fields=UpgradeItem/text"",
  ""items"" : [
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/47"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 47,
        ""text"" : ""Service Check HDFS""
      }
    },
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/48"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 48,
        ""text"" : ""Service Check YARN""
      }
    },
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/49"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 49,
        ""text"" : ""Service Check ZooKeeper""
      }
    },
    {
      ""href"" : ""http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/upgrades/32/upgrade_groups/8/upgrade_items/50"",
      ""UpgradeItem"" : {
        ""cluster_name"" : ""c1"",
        ""group_id"" : 8,
        ""request_id"" : 32,
        ""stage_id"" : 50,
        ""text"" : ""Service Check MapReduce2""
      }
    }
  ]
}
{code}

The Upgrade Pack contains,
{code}
    <group name=""SERVICE_CHECK"" title=""All Service Checks"" xsi:type=""service-check"">
      <skippable>true</skippable>
      <direction>UPGRADE</direction>
      <priority>
        <service>HDFS</service>
        <service>YARN</service>
        <service>HBASE</service>
      </priority>
    </group>
{code}

Because the pig service check was not ran, the new tez tarball was not copied to HDFS.
The underlying issue is that a service is not added to the Service Check group if it is a clientOnly service. However, Pig is clientOnly but still have a service check python script.",rolling_upgrade,['ambari-server'],AMBARI,Bug,Major,2015-03-05 01:55:27,69
12779592,Ambari must support deployment on separate host,"It should be possible to deploy Ambari on a host that does not include any other services.

The primary issue is that Ambari needs to be able to distribute keytabs to other hosts even if Ambari-Server is not running on a host with services that would otherwise have forced the Ambari-Server to be included in the keytab distribution process.

To be clear, the following use case should be supported:

* The Ambari-Server is deployed on a host with no other services 
** Other services are deployed on hosts separate from Ambari-Server
* addHost should be possible
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-04 21:26:07,30
12779275,Kerberos: Add Host did not generate keytabs,"1) using build 440
2) three node cluster, hdfs, yarn, mr, tez, hive, zk, pig, ams
3) setup nnha, rmha
4) enabled kerb
5) all is good
6) added second hive metastore
7) added second hiveserver2
8) all is good
9) added host with DN and clients
10) keytabs are not created on the new host. i was not prompted for kdc creds. basically, i did 1-9 all in one shot, never logging out.

As a workaround #1:
- Attempted to regen keytabs, with ""missing only"" checkbox checked. it looks like it remade all principals and keytabs for the cluster but didn't distribute the keytabs. That is concerning that this might be an additional issue for another JIRA maybe. Anycase: didn't result in getting keytabs on my new host.

As a workaround #2:
- Attempted regen keytabs all. Made all princs and keytabs and distributed for cluster hosts except my new host. So no lock here either.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-04 02:37:36,30
12779272,Fix minor issues with test_security_status test cases,Fix minor issues with test_security_status test cases,kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-04 02:16:07,30
12779166,"Storm principal is marked as a service rather than a user principal, causes issue adding hosts with Supervisor",Storm principal is marked as a _service_ rather than a _user_ principal. This will cause issues when adding hosts with Supervisor on it due to Ambari's lack of caching of service principals.,kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-03-03 20:03:01,30
12779129,core-site.xml having wrong value for hadoop.proxyuser.HTTP.groups,"core-site.xml having wrong value for hadoop.proxyuser.HTTP.groups

{code}
<property>
<name>hadoop.proxyuser.HTTP.groups</name>
<value>${core-site/proxyuser_group}</value>
</property>
{code}

It should be 

{code}
<property>
<name>hadoop.proxyuser.HTTP.groups</name>
<value>users</value>
</property>
{code}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-03-03 17:55:56,30
12779076,security_status test cases sporadically fail due to import of status_params module,"security_status test cases sporadically fail due to import of status_params module. For example {{test_security_status}} in {{stacks/2.0.6/HDFS/test_hdfs_client.py}} imports {{status_params.py}} which is not in the test directory path. 

Instead of importing  {{status_params.py}}, use values from  {{self.config_dict}}

Example:

{code:title=Before}
    cached_kinit_executor_mock.called_with(status_params.kinit_path_local,
                                           status_params.hdfs_user,
                                           status_params.hdfs_user_keytab,
                                           status_params.hdfs_user_principal,
                                           status_params.hostname,
                                           status_params.tmp_dir)
{code}

{code:title=After}
    cached_kinit_executor_mock.called_with(
      '/usr/bin/kinit',
      self.config_dict['configurations']['hadoop-env']['hdfs_user'],
      self.config_dict['configurations']['hadoop-env']['hdfs_user_keytab'],
      self.config_dict['configurations']['hadoop-env']['hdfs_user_principal_name'],
      self.config_dict['hostname'],
      '/tmp'
    )
{code}",kerberos,['ambari-agent'],AMBARI,Bug,Major,2015-03-03 14:00:18,30
12778522,Kerberos: Kerberos Service Check needs to generate and destroy it's own unique identity for testing,"The Kerberos _service check_ needs to generate it's own unique identity to use for testing and then destroy it when complete.  This will ensure that any _known_ identities (such as the smokeuser, usually ambari-qa) does not accidentally get removed if shared between clusters or if the service check is run after Kerberos is enabled. 

The service check must perform the following steps:

# Create a unique principal in the relevant KDC (server)
# Test that the principal can be used to authenticate via kinit (agent)
# Destroy the principal (server)
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-02-28 22:07:38,30
12777750,RU - Bug fixes for Host Version transition and Finalize,"1. There is a bug in ServiceComponentHostSummary.java since its constructor should not be comparing against the string UKNOWN, but rather rely on ComponentInfo to determine which components need to advertise a version. Also, the function haveAllComponentsFinishedAdvertisingVersion should instead compare the size of all components that can advertise a version, and the subset of those that have done so already.

2. ClusterImpl.java has a bug in transitionHostVersionState() when setting the state to UPGRADING if the Host has exactly one Component with a version.

3. FinalizeUpgradeAction.java has redundant code now that AMBARI-9755 is in because the call to transitionClusterVersion() will also take care of transitioning HostVersions from INSTALLED->CURRENT for hosts that only have components that do not advertise a version.

It is of the utmost importance to test these changes in the following scenarios:
Host added during cluster install
Host added after cluster install
Host added after bits are distributed

A Host may be added with either 0 services, only services that do not advertise a version, or at least one service that advertises a version.",rolling_upgrade,['ambari-server'],AMBARI,Bug,Major,2015-02-26 02:09:17,69
12777738,Agent is spamming logs with exceptions ,"{code}
INFO 2015-01-19 14:00:32,786 PythonExecutor.py:114 - Result: {'structuredOut': {u'securityIssuesFound': u'Configuration file hbase-site did not pass the vali
dation. Reason: Property hbase.regionserver.keytab.file does not exist', u'processes': [], u'securityState': u'UNSECURED'}, 'stdout': '2015-01-19 10:10:12,89
4 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/scr
ipt/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 
90, in status\n    check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/ch
eck_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:10:50,579 - Error while exe
cuting command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", lin
e 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    
check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.
py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:12:46,728 - Error while executing command \'st
atus\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n
    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    check_process_statu
s(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in ch
eck_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:12:47,149 - Error while executing command \'status\':\nTraceback 
(most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  
File ""/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py"", line 86, in status\n    raise ComponentIsNotRunning()\nC
omponentIsNotRunning\n2015-01-19 10:13:46,533 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/sit
e-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/
0.96.0.2.0/package/scripts/hbase_master.py"", line 73, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_manage
ment/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:
13:47,003 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libra
ries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py
"", line 90, in status\n    check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/func
tions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:13:47,438 - Error w
hile executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.
py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line
 76, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py""
, line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:13:47,872 - Error while executing command \'statu
s\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n   
 method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py"", line 86, in status\n    raise ComponentI
sNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:39,410 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/l
ib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common
-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 110, in status\n    check_process_status(status_params.namenode_pid_file)\n  File ""/usr/lib/pytho
n2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nCom
ponentIsNotRunning\n2015-01-19 10:19:40,004 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-
packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.
96.0.2.0/package/scripts/hbase_master.py"", line 73, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_manageme
nt/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19
:40,420 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/librari
es/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"",
 line 110, in status\n    check_process_status(status_params.namenode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/funct
ions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:40,852 - Error wh
ile executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.p
y"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py"", line 73, in
 status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 
41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:41,278 - Error while executing command \'status\':\nT
raceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method
(env)\n  File ""/var/lib/ambari-agent/cache/common-services/KAFKA/0.8.1.2.2/package/scripts/kafka_broker.py"", line 70, in status\n    check_process_status(sta
tus_params.kafka_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_proc
ess_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:41,676 - Error while executing command \'status\':\nTraceback (most re
cent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/v
ar/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    check_process_status(status_params.datanode_pi
d_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    rai
se ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:42,112 - Error while executing command \'status\':\nTraceback (most recent call last):\n 
 File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent
{code}",kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2015-02-26 01:43:23,30
12777389,Local user mapping for hdfs headless principal not set in Kerberos descriptor,The local user mapping for the hdfs headless principal not set in Kerberos descriptor.  It should be set to {{hadoop-env/hdfs_user}},kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-02-25 01:46:18,30
12777381,"Root user has spnego (HTTP) kerberos ticket set after Kerberos is enabled, root should have no ticket.","After enabling Kerberos, the root user has the spnego user set for it 

{code}
[root@c6501 ~]# klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: HTTP/c6501.ambari.apache.org@EXAMPLE.COM

Valid starting     Expires            Service principal
02/18/15 22:14:51  02/19/15 22:14:51  krbtgt/EXAMPLE.COM@EXAMPLE.COM
	renew until 02/18/15 22:14:51
{code}

It appears that the issue is related to the agent-side scheduler and/or some job that is scheduled to run periodically. Apparently some job is kinit-ing with the SPNEGO identity as the running user (root in this case) without changing the ticket cache. Thus whenever the job runs the root user's ticket cache gets changed to contain the SPNEGO identity's ticket.
",kerberos keytabs,['ambari-agent'],AMBARI,Bug,Blocker,2015-02-25 01:17:16,30
12777236,Oozie failed to start in secured cluster for stacks 2.0 and 2.1,"On 2.0 and 2.1 stack oozie server failed with following error:
{noformat}
2015-02-23 16:23:54,474  WARN NativeCodeLoader:62 - SERVER[] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-23 16:23:54,821 FATAL Services:533 - SERVER[] USER[-] GROUP[-] E0100: Could not initialize service [org.apache.oozie.service.HadoopAccessorService], Login failure for oozie/_HOST@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab
org.apache.oozie.service.ServiceException: E0100: Could not initialize service [org.apache.oozie.service.HadoopAccessorService], Login failure for oozie/_HOST@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab
	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:182)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:127)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:98)
	at org.apache.oozie.service.Services.setServiceInternal(Services.java:372)
	at org.apache.oozie.service.Services.setService(Services.java:358)
	at org.apache.oozie.service.Services.loadServices(Services.java:291)
	at org.apache.oozie.service.Services.init(Services.java:212)
	at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:39)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4206)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4705)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:799)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:601)
	at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:675)
	at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:601)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:502)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1317)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:324)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1065)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:840)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1057)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)
	at org.apache.catalina.core.StandardService.start(StandardService.java:525)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:754)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:595)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)
Caused by: java.io.IOException: Login failure for oozie/_HOST@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:870)
	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:174)
	... 31 more
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:861)
	... 32 more
2015-02-23 16:23:54,833  INFO Services:539 - SERVER[] Shutdown
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-02-24 18:14:23,30
12776851,Kerberos: check kerb task should delete smoke user principal ,"After smoke test user is created during the Check Kerberos task at the beginning of the wizard, delete the principal because later in the wizard the user might have changed the principal name, making this principal unused. ",kerberos,['ambari-server'],AMBARI,Task,Critical,2015-02-23 14:59:26,30
12776758,Storm service check failed after enabling security with existing AD,"On last stage, storm service check failed 
{code}
1017 [main] INFO  backtype.storm.StormSubmitter - Generated ZooKeeper secret payload for MD5-digest: -5540876373091122649:-7113320937502691642
1021 [main] INFO  backtype.storm.security.auth.AuthUtils - Got AutoCreds []
1039 [main] WARN  org.apache.storm.curator.retry.ExponentialBackoffRetry - maxRetries too large (60000). Pinning to 29
1043 [main] INFO  backtype.storm.utils.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [2000] the maxSleepTimeMs [5] the maxRetries [60000]
1043 [main] WARN  backtype.storm.utils.StormBoundedExponentialBackoffRetry - Misconfiguration: the baseSleepTimeMs [2000] can't be greater than the maxSleepTimeMs [5].
1847 [main] INFO  org.apache.storm.zookeeper.Login - successfully logged in.
Exception in thread ""main"" java.lang.RuntimeException: javax.security.sasl.SaslException: Failure to initialize security context [Caused by GSSException: Invalid name provided (Mechanism level: Illegal character in realm name; one of: '/', ':', '' (600))]
	at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:99)
	at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:66)
	at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:52)
	at backtype.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:36)
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:211)
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:157)
	at storm.starter.WordCountTopology.main(WordCountTopology.java:77)
Caused by: javax.security.sasl.SaslException: Failure to initialize security context [Caused by GSSException: Invalid name provided (Mechanism level: Illegal character in realm name; one of: '/', ':', '' (600))]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.<init>(GssKrb5Client.java:150)
	at com.sun.security.sasl.gsskerb.FactoryImpl.createSaslClient(FactoryImpl.java:63)
	at javax.security.sasl.Sasl.createSaslClient(Sasl.java:372)
	at org.apache.thrift7.transport.TSaslClientTransport.<init>(TSaslClientTransport.java:72)
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:127)
	at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48)
	at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:97)
	... 6 more
Caused by: GSSException: Invalid name provided (Mechanism level: Illegal character in realm name; one of: '/', ':', '' (600))
	at sun.security.jgss.krb5.Krb5NameElement.getInstance(Krb5NameElement.java:127)
	at sun.security.jgss.krb5.Krb5MechFactory.getNameElement(Krb5MechFactory.java:95)
	at sun.security.jgss.GSSManagerImpl.getNameElement(GSSManagerImpl.java:202)
	at sun.security.jgss.GSSNameImpl.getElement(GSSNameImpl.java:472)
	at sun.security.jgss.GSSNameImpl.init(GSSNameImpl.java:201)
	at sun.security.jgss.GSSNameImpl.<init>(GSSNameImpl.java:170)
	at sun.security.jgss.GSSManagerImpl.createName(GSSManagerImpl.java:137)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.<init>(GssKrb5Client.java:108)
	... 12 more
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-02-23 02:52:22,30
12776742,Kerberos: fails when entering admin principal with blank password ,"Note: I don't believe the below is specific to add host, but related to the prompting and how the set admin cred works in case of a blank password. I hit this during testing of add host though.

- install cluster, kerberize
- add host (be sure to use a new browser so you know it will prompt for kerb admin credentials)
- got to the review part of add host, click deploy
- prompted for admin creds (as expected)
- tried messing around by putting in bad creds and that seemed to work...
- expect when I put in the right admin cred principal name (admin/admin) but a blank password. I was surprised it allowed me to click save (because the password field was blank)
- so I click save, dialog disappears and I am cannot get it to re-prompt.
- this is what it PUT and the response was blank...
 
{code}
[{""session_attributes"":{""kerberos_admin"":{""principal"":""admin/admin"",""password"":""""}}}]:
Response Headersview source
{code}

in ambari-server.log, nothing

{code}
17:58:05,860  INFO [qtp1257282095-603] AmbariManagementControllerImpl:1171 - Received a updateCluster request, clusterId=2, clusterName=MyCluster, securityType=null, request={ clusterName=MyCluster, clusterId=2, provisioningState=null, securityType=null, stackVersion=HDP-2.2, desired_scv=null, hosts=[] }
{code}

- back in wizard doesn't solve it. had to completely exit wizard and ambari web to start again

The overall issue is how the credentials are being validated.  If no password is being set, the command to test the credentials when using a MIT KDC generates the following command:
{code}
kadmin -p admin/admin -w """" -r EXAMPLE.COM -q 'get_principal admin/admin'
{code}

The empty password ({{-w """"}}) in the command creates an interactive session where the command is waiting for data on STDIN, thus hanging the process.

An empty password should not cause the same behavior when using Active Directory.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-02-22 22:50:38,30
12776671,Kerberos: regenerate keytabs not handled for all hosts,"1. Installed cluster on three hosts c6401, c6402, c6403
2. using oracle jdk 1.7, put JCE in place on all hosts
3. ambari-agent stop on c6403 (which just has DN, ZK and NM)
4. Enable kerberos, which means c6403 does not get keytabs
5. ambari-agent start on c6403
6. go to regen keytabs. Clicked to only do missing. c6403 does not get keytabs.
7. go to regen keytabs. just left the default which should do all. No hosts get the keytabs.

What I found is since the Kerberos client didn't get installed on c6403, the ""Set keytab kerberos client"" command is ""Host Role in invalid state"". I went to that host, and did install clients from the UI to get the kerberos client installed. Once that happened, I could then regen keytabs.

The main issue: Regen only works if all hosts can regen. Once c6403 did not have a client, and Host Role in invalid state, it didn't do keytabs for any other hosts.",kerberos keytabs,['ambari-server'],AMBARI,Bug,Critical,2015-02-21 23:23:40,30
12776155,Zookeeper start failed after upgrading secured cluster,"{code}
2015-02-13 11:32:12,437 - Error while executing command 'start':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_server.py"", line 78, in start
    zookeeper_service(action = 'start')
  File ""/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_service.py"", line 38, in zookeeper_service
    kinit_cmd = format(""{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal};"")
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/format.py"", line 83, in format
    return ConfigurationFormatter().format(format_string, args, **result)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/format.py"", line 47, in format
    result_protected = self.vformat(format_string, args, all_params)
  File ""/usr/lib64/python2.6/string.py"", line 549, in vformat
    result = self._vformat(format_string, args, kwargs, used_args, 2)
  File ""/usr/lib64/python2.6/string.py"", line 582, in _vformat
    result.append(self.format_field(obj, format_spec))
  File ""/usr/lib64/python2.6/string.py"", line 599, in format_field
    return format(value, format_spec)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 79, in __getattr__
    raise Fail(""Configuration parameter '""+self.name+""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'smokeuser_principal_name' was not found in configurations dictionary!
{code}",kerberos upgrade,['ambari-server'],AMBARI,Bug,Blocker,2015-02-19 14:01:37,30
12775424,Kerberos: Adding a service to a Kerberized cluster requires Kerberos-related tasks occur before INSTALL stage,"When adding a service to a _Kerberized_ cluster, the {{INSTALL}} stage for that service tends to fail due to missing Kerberos properties, principals and keytabs. 

To solve this issue, when a new service is added to a _Kerberized_ cluster the following needs to occur before that service can attempt to transition into it's {{INSTALLED}} state:
# Kerberos-specific properties need to be created or set
# Principals need to be created
# Keytab files need to be generated
# Keytab files need to be distributed 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-02-16 17:47:31,30
12775409,Security enabling fails after ambari only upgrade,"Enabling security hangs after ambari upgrade on centos (1.4.4 -> 2.0.0)

This appears to be an issue with how the Kerberos service is attached to stacks.",kerberos,['stacks'],AMBARI,Bug,Blocker,2015-02-16 16:58:13,30
12775384,"Enhance parameter configs for Pig, Files and CapSch views ","Enhanced the view config parameters to have placeholder, label and default values (where applicable). Also added the min ambari version property.",view-capacity-scheduler view-files view-pig,['ambari-views'],AMBARI,Improvement,Major,2015-02-16 15:07:01,125
12775130,Required Properties in Configure Kerberos Step,"After the cluster is deployed, tried to enable security. In the Configure kerberos step - Advanced krb5.conf tab, of the wizard there appears 2 new required properties as shown in the attachment. Please let us know the value of these properties if it has to be set by the user or if this should be present by default. This blocks security enabling",kerberos,['ambari-web'],AMBARI,Bug,Blocker,2015-02-14 12:38:04,30
12775022,Kerberos: Escape special characters in Distinguished Names used for queries in Active Directory,"Escape special characters in Distinguished Names used for queries in Active Directory, else query (or attribute updates) will fail with an error like

{code}
[LDAP: error code 1 - 000020D6: SvcErr: DSID-0310081B, problem 5012 (DIR_ERROR), data 0 ^@]
{code}

The following characters should be escaped using a {{\}}:
* Forward Slash {{/}}
* Comma	{{,}}
* Backslash character
* Pound sign (hash sign)	{{#}}
* Plus sign {{+}}
* Less than symbol	{{<}}
* Greater than symbol {{>}}
* Semicolon {{;}}
* Double quote (quotation mark) {{""}}
* Equal sign {{=}}
* Leading or trailing spaces
",kerberos,['ambari-server'],AMBARI,Task,Blocker,2015-02-13 20:42:34,30
12774667,When adding the Oozie service to a kerberized cluster OOZIE_SERVER doesn't start,"Oozie server fails to start with the error: ""Fail: Configuration parameter 'oozie.service.HadoopAccessorService.kerberos.principal' was not found in configurations dictionary!""

Steps to reproduce:
Create a non-kerberized cluster
I used the following blueprint
{code}
{   
  ""host_groups"" : [
    {
      ""name"" : ""host_group_1"",
      ""components"" : [      
        {
          ""name"" : ""NODEMANAGER""
        },
        {
          ""name"" : ""NAMENODE""
        },
        {
          ""name"" : ""HISTORYSERVER""
        },
        {
          ""name"" : ""ZOOKEEPER_SERVER""
        },
        {
          ""name"" : ""SECONDARY_NAMENODE""
        },
        {
          ""name"" : ""RESOURCEMANAGER""
        },  
        {
          ""name"" : ""APP_TIMELINE_SERVER""
        },        
        {
          ""name"" : ""DATANODE""
        },
        {
          ""name"" : ""YARN_CLIENT""
        },
        {
          ""name"" : ""ZOOKEEPER_CLIENT""
        },
        {
          ""name"" : ""MAPREDUCE2_CLIENT""
        }     
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""2.2""
  }
}
{code}
- manually unzip UnlimitedJCEPolicy
- manually install MIT KDC
- Using UI, kerberize the existing cluster
- Using the UI, add the Oozie service

OOZIE_SERVER failed to start and the above noted exception was from the log that is exposed via the UI for the oozie start operation.
According to Robert Levas this property is in the kerberos descriptor it should be set.
",keberos,['security'],AMBARI,Bug,Critical,2015-02-12 19:23:57,30
12774643,Set Keytab Kerberos Client operation fails,"Was installed cluster with HDFS, MapReduce2, YARN, Tez, ZooKeeper, AMS services. After that was added Kerberos service. Create Principals operation was failed with error:
{noformat}
2015-02-09 11:17:13,376 - Error while executing command 'set_keytab':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 59, in set_keytab
    KerberosScript.write_keytab_file()
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_common.py"", line 395, in write_keytab_file
    group=group)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 151, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 117, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 133, in action_create
    self.resource.group, mode=self.resource.mode, cd_access=self.resource.cd_access)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 65, in _ensure_metadata
    uid = _coerce_uid(user)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 46, in _coerce_uid
    raise Fail(""User %s doesn't exist."" % user)
{noformat}

Alive cluster: http://ec2-52-1-1-157.compute-1.amazonaws.com:8080/",kertberos,['stacks'],AMBARI,Bug,Blocker,2015-02-12 17:51:59,30
12774314,Set kdc_type in kerberos-env rather than krb5-conf configuration,"Currently, the {{kdc_type}} value is being set in the {{krb5-conf}} configuration, it needs to be set in the {{kerberos-env}} since the data lies outside the realm of the krb5.conf file. 
  ",kerberos,"['ambari-web', 'stacks']",AMBARI,Task,Major,2015-02-11 18:56:14,30
12774305,Kerberos: provide option to not generate kerb client krb5.conf,"In some environments, the kerb client krb5.conf is already managed on the machines and ambari cannot generate or modify it.

Need to provide option to not have ambari create/overwrite krb5.conf. See slide 2 of the attached for the proposed option to give users the ability to disable ambari trying to create/write the kerb client config.",kerberos,"['ambari-server', 'stacks']",AMBARI,Task,Critical,2015-02-11 18:36:38,30
12773917,Kerberos: missing config properties after enabling Kerberos,"After enabling Kerberos, testing with Ambari 2.0.0 Build #401, found this config is missed:

{code}
core-site/hadoop.http.authentication.kerberos.keytab : $KRB_KEYTAB_PATH/spnego.service.keytab
{code}",kerberos kerberos_descriptor,['stacks'],AMBARI,Bug,Major,2015-02-10 18:39:33,30
12773564,Kerberos: Kerberos service absent in stacks lower than HDP-2.2,"Add the Kerberos service (containing the Kerberos Client component) to the 2.0 and 2.1 stacks.  Since the Kerberos service is currently a _common service_, this should be done using service inheritance. ",common-services kerberos stack,['stacks'],AMBARI,Bug,Blocker,2015-02-09 20:33:22,24
12773019,Kerberos: Keytab regeneration not invoked when initiated via API,"Due to changes from some previous patch the response from the API call to regenerate keytabs is being clobbered. 

This was broken in patch for AMBARI-9406",kerberos keytabs,[],AMBARI,Bug,Blocker,2015-02-06 14:54:08,30
12771951,HDFS and Hive services checks fail when default FS is not HDFS,"HDFS service check fails with:
{code}
2015-01-27 09:15:54,459 - Error while executing command 'service_check':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/service_check.py"", line 53, in service_check
    bin_dir=params.hadoop_bin_dir
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 151, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 117, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/execute_hadoop.py"", line 55, in action_run
    environment = self.resource.environment,
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 151, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 117, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 277, in action_run
    raise ex
Fail: Execution of 'hadoop --config /etc/hadoop/conf dfsadmin -safemode get | grep OFF' returned 1. DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

{code}

Hive service check fails with:
{code}
2015-01-26 04:03:39,082 - Error while executing command 'service_check':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/service_check.py"", line 55, in service_check
    hcat_service_check()
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hcat_service_check.py"", line 69, in hcat_service_check
    bin_dir=params.execute_path
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 151, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 117, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/execute_hadoop.py"", line 55, in action_run
    environment = self.resource.environment,
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 151, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 117, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 277, in action_run
    raise ex
Fail: Execution of 'hadoop --config /etc/hadoop/conf fs -test -e /apps/hive/warehouse/hcatsmokeid000a0d00_date022615' returned 1.
 stdout:
{code}",hcfs,['stacks'],AMBARI,Bug,Major,2015-02-03 06:34:34,126
12771915,Kerberos: Do not validate host health or maintenance state when enabling Kerberos,Do not validate host health or maintenance state when enabling Kerberos,kerberos,['ambari-server'],AMBARI,Task,Major,2015-02-03 02:27:21,30
12771094,Service configurations are not updated as customized in the descriptor,"Though the Kerberos descriptor was saved with the customized value for smokeuser 
{noformat}(${cluster-env/smokeuser}/c1@${realm}){noformat} 

but the generated principal had the default value (i.e without /c1) and the corresponding configurations also had the default value 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-01-29 19:32:40,30
12770815,Implement Keytab regeneration,"Create API entry point to initiate Kerberos keytab regeneration for the cluster:
{code}
PUT /api/v1/clusters/{clustername}?regenerate_keytabs=true
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
  }
}
{code}

The entry point should invoke code to determine which principals need to be updated and then generate the following stages:
# Update Principal Passwords
# Generate Keytabs
# Distribute Keytab

This could be done in a method within {{org.apache.ambari.server.controller.KerberosHelper}} named {{regenerateKeytabs}} and flow similarly to {{org.apache.ambari.server.controller.KerberosHelper#toggleKerberos}}

A Server-side action implementation already exists for generating keytabs - see {{org.apache.ambari.server.serveraction.kerberos.CreateKeytabFilesServerAction}}.
A process is already in place to distribute keytabs - {{org/apache/ambari/server/controller/KerberosHelper.java:1192}}
A new Server-side action _may_ need to be created to update relavant principal passwords, however {{org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction}} may work for this, unaltered.

",kerberos keytabs,['ambari-server'],AMBARI,Task,Major,2015-01-28 22:13:24,30
12770559,"New blueprint ""add hosts"" api is broken as a result of a regression","The new ""add hosts"" api is now completely broken and could return several errors to the user depending on the topology assigned to the hosts being added.  This is a regression that was introduced in the patch for AMBARI-8868.  

The root issue here is AMBARI-4782.

The patch for AMBARI-8868 attempted to resolve its issue without resolving the root problem and in doing so, broke this functionality.

To fix this issue correctly, the root issue AMBARI-4782 needs to be resolved.  This will require coordination with the UI guys.



",blueprints,[],AMBARI,Bug,Blocker,2015-01-28 01:14:36,120
12770525,role_command_order.json should not be at stack level,"In current stack definitions, role_command_order.json is at the stack level.
For example: HDP/2.2/role_command_order.json

Service definitions are all nicely separated into different directories, like HDP/2.2/services/{HDFS|YARN}, but not the role_command_order. It would be neater to separate role_command_order per service and would be very useful while adding a new service to Ambari.

Looking for something as below,
- HDP/2.2/services/HDFS/role_command_order.json
- HDP/2.2/services/YARN/role_command_order.json
Ambari server while starting should merge all role_command_order.json and create dependencies accordingly.

This is extremely useful for custom services which are potentially added after the cluster install.
",feature_custom_service,['ambari-server'],AMBARI,New Feature,Major,2015-01-27 23:39:49,127
12770462,Implement unkerberize for kerberized cluster,"Implement the ability to disable Kerberos from a cluster that was previously configured for Kerberos.

This entails reverting configuration properties set when Kerberos was enabled to default values found in the stack. 

Principals will not be destroyed in the KDC and ketyab files will not be removed from hosts.",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2015-01-27 20:41:25,30
12770430,Remove toLowerCase() from userPrincipalName in default Kerberos principal create template,"Remove toLowerCase() from userPrincipalName in default Kerberos principal create template. This is creating an issue with principals that have upper-cased characters and Active Directory such that when kinit-ing, authenticating fails:

{code:title=kinit -V -k -t /etc/security/keytabs/spnego.service.keytab }
HTTP/c6501.ambari.apache.org
Using default cache: /tmp/krb5cc_0
Using principal: HTTP/c6501.ambari.apache.org@HDP01.LOCAL
Using keytab: /etc/security/keytabs/spnego.service.keytab
kinit: Preauthentication failed while getting initial credentials
{code}

An example of the offending template is as follows:
{code:title=from kerberos-env.xml}
{
  ""objectClass"": [""top"", ""person"", ""organizationalPerson"", ""user""],
  ""cn"": ""$principal_name"",
  #if( $is_service )
  ""servicePrincipalName"": ""$principal_name"",
  #end
  ""userPrincipalName"": ""$normalized_principal.toLowerCase()"",
  ""unicodePwd"": ""$password"",
  ""accountExpires"": ""0"",
  ""userAccountControl"": ""66048""
}
{code}",active_directory kerberos,"['ambari-server', 'stacks']",AMBARI,Task,Major,2015-01-27 18:56:17,30
12770386,cluster-env/security_enabled not set to true when Kerberos is enabled in cluster,"After enabling Kerberos, {{cluster-env/security_enabled}} is {{false}} when it should be {{true}}. This is causing component to not start up with Kerberos enabled and thus failing.
",keberos,['ambari-server'],AMBARI,Bug,Blocker,2015-01-27 15:59:26,30
12770268,Unittest fail with python 2.6,"Failed tests:
ERROR: test_main_test_stop (TestAmbariServer.TestAmbariServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/ambari/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/user/ambari/ambari-server/src/test/python/TestAmbariServer.py"", line 541, in test_main_test_stop
    self.assertIsNone(options.exit_message)
AttributeError: 'TestAmbariServer' object has no attribute 'assertIsNone'

ERROR: test_start (TestAmbariServer.TestAmbariServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/ambari/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/user/ambari/ambari-server/src/test/python/TestAmbariServer.py"", line 3312, in test_start
    _ambari_server_.start(args)
  File ""/home/user/ambari/ambari-common/src/main/python/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/home/user/ambari/ambari-server/src/main/python/ambari-server.py"", line 114, in start
    server_process_main(args)
  File ""/home/user/ambari/ambari-server/src/main/python/ambari_server_main.py"", line 348, in server_process_main
    raise FatalException(-1, AMBARI_SERVER_DIE_MSG.format(exitcode, configDefaults.SERVER_OUT_FILE))
FatalException: ""Fatal exception: Ambari Server java process died with exitcode <MagicMock name='Popen().returncode' id='39415248'>. Check /var/log/ambari-server/ambari-server.out for more information., exit code -1""",WinRefactor,['ambari-server'],AMBARI,Bug,Major,2015-01-27 04:54:07,128
12769963,"Kerberos: when unable to connect to KDC admin, need to inform user","This appears to be happening when the KDC and/or KDC Admin hosts do not point to a valid KDC, and when the relevant KDC does not _handle_ the indicated realm.

For the *host* issue, (in the cases that I tested), a {{org.apache.ambari.server.serveraction.kerberos.KerberosKDCConnectionException}} is being thrown but not caught to re-throw a {{java.lang.IllegalArgumentException}}, which would generate the expected 400 error.

For the *realm* issue, the following error string is not being captured by the logic to produce a {{org.apache.ambari.server.serveraction.kerberos.KerberosRealmException}}. 
{code}
kadmin: Cannot find KDC for requested realm while initializing kadmin interface
{code}
In any case, {{org.apache.ambari.server.serveraction.kerberos.KerberosRealmException}} is not being caught to re-throw a {{java.lang.IllegalArgumentException}}, which would generate the expected 400 error.
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-25 20:17:56,30
12769955,Kerberos: host/<hostname>@REALM principals are created (should not be created),"While generating principals, host/<hostname>@REALM principals are created.  These should not be created.

And they are ending-up in the resulting keytab. For example:

{code}
[root@c6402 keytabs]# klist -kt nn.service.keytab 
Keytab name: FILE:nn.service.keytab
KVNO Timestamp         Principal
---- ----------------- --------------------------------------------------------
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
{code}",kerberos keytabs,['ambari-server'],AMBARI,Task,Major,2015-01-25 18:19:57,30
12769795,Kerberos: Need stdout to show info on Kerberos-related tasks,"There's no stdout to indicate progress / success for the tasks for principal generation / keytab generation, etc.

Also, even just to know what command is actually being run to create the principal, that will help folks with debugging. Or is that only shown in case of a failure. Just want to make sure in the case of a problem, there is output there to help folks.
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-24 01:25:30,30
12769705,Automate Kerberos support for AMS,"We need to implement automatic keytab / principal for AMS in case of HDFS backed HDFS.
",kerberos kerberos_descriptor security,['stacks'],AMBARI,Task,Major,2015-01-23 18:22:12,30
12769623,Remove toLowerCase() from userPrincipalName in default Kerberos principal create template,"Remove toLowerCase() from userPrincipalName in default Kerberos principal create template. This is creating an issue with principals that have upper-cased characters and Active Directory such that when kinit-ing, authenticating fails:

{code:title=kinit -V -k -t /etc/security/keytabs/spnego.service.keytab }
HTTP/c6501.ambari.apache.org
Using default cache: /tmp/krb5cc_0
Using principal: HTTP/c6501.ambari.apache.org@HDP01.LOCAL
Using keytab: /etc/security/keytabs/spnego.service.keytab
kinit: Preauthentication failed while getting initial credentials
{code}

An example of the offending template is as follows:
{code:title=from kerberos-env.xml}
{
  ""objectClass"": [""top"", ""person"", ""organizationalPerson"", ""user""],
  ""cn"": ""$principal_name"",
  #if( $is_service )
  ""servicePrincipalName"": ""$principal_name"",
  #end
  ""userPrincipalName"": ""$normalized_principal.toLowerCase()"",
  ""unicodePwd"": ""$password"",
  ""accountExpires"": ""0"",
  ""userAccountControl"": ""66048""
}
{code}",active-directory active_directory kerberos,"['ambari-server', 'stacks']",AMBARI,Bug,Major,2015-01-23 12:06:50,30
12769417,MapReduce2 Service Check fails after enabling Kerberos with permission issue in local filesystem,"After enabling Kerberos MapReduce2 Service Check failed with issue writing to local file system:

{code}
Init:
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-x--- 4 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache

Kerb:
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 01:23 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-s--- 4 ambari-qa hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache

Can't create directory /hadoop/yarn/local/usercache/ambari-qa/appcache/application_1421889721625_0001 - Permission denied
main : user is ambari-qa
main : requested yarn user is ambari-qa
{code}

The service check does not fail when run before enabling Kerberos.


{code:title=Filesystem BEFORE enabling Kerberos}
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-x--- 4 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache
{code}

{code:title=Filesystem AFTER enabling Kerberos}
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 01:23 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-s--- 4 ambari-qa hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache
{code}

It appears that the user executing the task is {{ambari-qa}} after enabling Kerberos, there is no indication what user is executing the task before enabling Kerberos.",kerberos mapreduce,['ambari-agent'],AMBARI,Bug,Blocker,2015-01-22 19:55:48,30
12769280,Ensure enable/disable Kerberos logic should invoke only when state of security flag is changed,"The logic to enable or disable Kerberos is typically invoked when the Cluster resource is updated. This occurs for several reasons, not all of them indicate the state of Kerberos should be altered.  

By processing all updated to the Cluster resource, the enable/disable Kerberos may get invoked when not necessary causing _noise_ on the task list and potentially generating an error condition if the KDC administrator credentials are not available.  Certain states of the system will trigger the enable/disable Kerberos logic to perform tasks requiring the KDC administrator credentials. If not explicitly handing the security state change, this behavior is not desired. 

To solve the issue, test the request on the update Cluster resource to see if the security state property (cluster-env/security_enabled) has been altered, if so invoke enable/disable Kerberos logic; else do not invoke enable/disable Kerberos logic. 
",kerberos security,['ambari-server'],AMBARI,Bug,Blocker,2015-01-22 12:04:28,30
12769053,JAAS configuration file parser leaves trailing quote in quoted values,"The JAAS configuration file parser used in determining the security state of some components (like ZooKeeper and HBase) leaves trailing quote in quoted values. 

For example:

{code}
Client {
com.sun.security.auth.module.Krb5LoginModule required
useKeyTab=true
storeKey=true
useTicketCache=false
keyTab=""/etc/security/keytabs/hbase.service.keytab""
principal=""hbase/vp-ambari-ranger-med-0120-2.cs1cloud.internal@EXAMPLE.COM"";
};
{code}

The value for {{keytab}} is being parsed as {{/etc/security/keytabs/hbase.service.keytab""}} rather than {{/etc/security/keytabs/hbase.service.keytab}}.
",kerberos security,['ambari-agent'],AMBARI,Bug,Major,2015-01-21 18:34:23,30
12769049,Artifact resource doesn't properly preserve nested map structures for artifact data,"When an artifact resource is created with a nested map structure, the json representation of the resource flattens out the property names instead of preserving the original hierarchy.
",api,['ambari-server'],AMBARI,Bug,Major,2015-01-21 18:23:29,120
12768950,Add getUpdateDirectives to org.apache.ambari.server.api.resources.ResourceDefinition and use when handling PUT requests,"Add {{getUpdateDirectives}} method to {{org.apache.ambari.server.api.resources.ResourceDefinition}}:

{code}
  public Collection<String> getUpdateDirectives();
{code}

Add default implementation to {{org.apache.ambari.server.api.resources.BaseResourceDefinition}} to return an empty Set

Update {{org.apache.ambari.server.api.services.RequestFactory}} to process _update directives_ like it processes _create directives_ - see {{org.apache.ambari.server.api.services.RequestFactory#createPostRequest}}
",api,['ambari-server'],AMBARI,Task,Major,2015-01-21 11:07:55,30
12768863,Ambari Server setup to install and copy JCE policy file in-place (handle both Default / Custom JDK scenarios),"1) During ""ambari-server setup"", if user chooses JDK option 1 or 2 (where ambari automatically downloads JDK and JCE), setup should copy the JCE file to /resources and install the JCE on the ambari server.

2) Also need an ""ambari-server setup-jce"" command to put JCE zip in place on Ambari Server and install on Ambari Server in cases where user chose option 3 custom jdk during setup.

{code}
ambari-server setup-jce /path/to/downloaded/jce/policy/zip/file
{code}

1) user downloads JCE zip file
2) user runs ""ambari-server setup-jce /tmp/UnlimitedJCEPolicyJDK7.zip"" (for example)
3) setup-jce copies zip file into ambari-server /resources
4) setup-jce ambari.properties and set jce.name= to name of the zip file
5) setup-jce installs JCE on ambari-server
6) inform user to restart ambari-server
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-21 02:58:41,30
12768646,Add the ability to append a random value to values in LDAP attributes when generating principals in Active Directory,"Add ability to append a random value to values in LDAP attributes when generating principals in Active Directory.

For example the {{cn}} and {{sAMAccountName}} attributes must be unique.  In some caes the {{cn}} is not allowed to have {{/}} characters and in all cases the {{sAMAccountName}} is not allow to have {{/}} characters. Therefore to generate values for these attributes, the _instance_ part of the principal needs to be stripped off and a random string needs to be appended. 

This can be seen where the principal is {{nn/c6501.ambari.apache.org@EXAMPLE.COM}}.  The {{cn}} would typically be {{nn/c6501.ambari.apache.org}}.  Providing for a random string would allow the {{cn}} value to be something like {{nn-ythnskdtarsjko5fsdfdsb}}. Since the {{sAMAccountName}} can be at most 20 characters, it would be {{nn-ythnskdtarsjko5fs}}.

Since the generation of the attributes and values is done using a Velocity template, this random string will need to be generated and stored in the Velocity engine context before processing the template.
",active_directory kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-20 11:17:58,30
12767990,Keytab generation should use kerberos-env/encryption_types when creating key entries,When generating keytab files the set of keys to be generated should be determined by the set of encryption types listed in {{kerberos-env/encryption_types}},kerberos keytabs,['ambari-server'],AMBARI,Bug,Blocker,2015-01-16 11:12:45,30
12767982,Principal creation for Active Directory accounts should be configurable,"The properties used to create accounts in an Active Directory, related to principal creation, should be configurable such that a user may specify the required fields and their values (with variable replacement).

This may be done using a simple structure like XML or JSON, however a template facility (like Jinja2) may be more useful since conditional paths may be built in.  The template should be stored in the {{kerberos-env}} configuration.

An example of a need for a conditional path in a template is related to _service_ accounts vs _user_ accounts.  A _service_ account (such as nn/\_HOST@REALM) should have the {{servicePrincipalName}} field set to the service's principal, where this value shouldn't be set for a _user_ account (such as hdfs@REALM).
",active_directory kerberos,['ambari-server'],AMBARI,Improvement,Major,2015-01-16 10:58:34,30
12767850,Kerberos should support generation of host specific user principals,"The template for generating principals (names) should provide the ability inject the hostname in the principal that is generated.

Verify that there are no changes required on the agent side to deal with dynamic principal names.
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-01-15 21:52:05,48
12767780,Allow a set of property names to ignore to be passed into the predicate compiler,"The introduction of request directives to the query string requires that the request can tell the predicate compiler to ignore these directives. 

The compile method of PredicateCompiler is now overloaded allowing the caller to pass in a set  of property names to ignore.",api,['ambari-server'],AMBARI,Task,Major,2015-01-15 16:54:46,120
12767759,Test principal and keytab required for service check should be created as part of kerberos service check action,"Intercept call to execute SERVICE_CHECK for KERBEROS service and (if necessary) create and distribute test user keytab.

It will be hard coded that the test user information is taken from the ""smokeuser"" identity in the relevant clusters Kerberos descriptor.",kerberos,['ambari-server'],AMBARI,Improvement,Major,2015-01-15 15:43:59,30
12767642,Create new API endpoints for cluster and service kerberos descriptors,"Expose cluster and service kerberos descriptors via the REST API.

Introduce new artifact endpoints as sub-resources for the cluster and service endpoints.
Kerberos descriptors will be created and obtained via the new artifact endpoints.
The api has no notion of a kerberos descriptor, instead the descriptor is simply an artifact and the descriptor contents are provided via the 'artifact_data' field.

To get a cluster kerberos descriptor which was created for the cluster 'c1' with the name 'kerberos_descriptor':
{code}
GET http://AMBARI_SERVER:8080/api/v1/clusters/c1/artifacts/kerberos_descriptor

{
  ""href"" : ""http://172.18.192.1:8080/api/v1/clusters/c1/artifacts/kerberos_descriptor"",
  ""Artifacts"" : {
    ""artifact_name"" : ""kerberos_descriptor"",
    ""cluster_name"" : ""c1""
  },
  ""artifact_data"" : {
    ""identities"" : [
       ...
    ],
    ""services"" : [
      ...
    ],
    ""properties"" : {
      ...
    }
  }
}
{code}
To get a service kerberos descriptor which was created for the cluster 'c1' and service 's1' with the name 'kerberos_descriptor';:
{code}
GET http://AMBARI_SERVER:8080/api/v1/clusters/c1/services/s1/artifacts/kerberos_descriptor

{
  ""href"" : ""http://172.18.192.1:8080/api/v1/clusters/c1/services/s1/artifacts/kerberos_descriptor"",
  ""Artifacts"" : {
    ""artifact_name"" : ""kerberos_descriptor"",
    ""cluster_name"" : ""c1"",
    ""service_name"" : ""s1""
  },
  ""artifact_data"" : {
    ...
  }
}
{code}
To create a cluster kerberos descriptor for cluster 'c1' with the name 'kerberos_descriptor':
{code}
POST http://AMBARI_SERVER:8080/api/v1/clusters/c1/artifacts/kerberos_descriptor

{
  ""artifact_data"" : {
    ... // artifact contents 
  }  
}
{code}
",api api-addition kerberos,['ambari-server'],AMBARI,Technical task,Major,2015-01-15 03:15:17,120
12767580,Need to provide meaningful names for the stage context in Kerb API call response,"In the API response for Kerberizing the cluster, stage_context is set to ""Process Kerberos Operations"" for all operations.  This needs to be more specific (e.g., Generate Principals, Generate Keytabs, Distribute Keytabs, etc).  

Due to the current behavior, ""Kerberize Cluster"" page repeatedly shows ""Process Kerberos Operations"" one after another and it doesn't make much sense to the end user.",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-14 23:14:08,30
12767439,Split up principal components and realm in Kerberos descriptor,"Split up principal components and realm in Kerberos descriptor and flow throw application separately.  The realm may be optional in the Kerberos Principal descriptor, taking the value from the default realm.
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2015-01-14 14:40:56,30
12767438,Get kdc_type from kerberos-env rather than krb5-conf configuration,Get {{kdc_type}} from {{kerberos-env}} rather than {{krb5-conf}} configuration,kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-14 14:32:04,30
12767241,Various cleanup for CS view,"Various UI cleanup for Capacity Scheduler view:
- Added range control for setting queue capacity/max
- Added ability to show/hide peer queues
- Added support for optional queue properties
- Added instructions for working with vagrant and local build
- Remove show/edit controls
- Convert max AM to percent, not decimal, in UI
- Added support for App.testMode=true for UI dev
- Added support for label and placeholder text on instance config properties
- Up'd version to 0.3.0
- Small text cleanup in tez view readme.md and property-validator-view index.md ",capacity-scheduler,['contrib'],AMBARI,Improvement,Major,2015-01-13 20:00:48,125
12766515,Add principal type to Kerberos descriptor,"Add principal _type_ to Kerberos descriptor to declare whether is principal is a service principal or a user principal.

This is needed for Active Directory since service principals needs to be created differently than user principals. 
 ",kerberos kerberos_descriptor,"['ambari-server', 'stacks']",AMBARI,Task,Major,2015-01-09 19:50:42,30
12765927,Pass LDAP URL and Principal container DN to Active Directory operations handler,"Pass LDAP URL and Principal container DN to Active Directory operations handler.

The data will be stored in the {{kerberos-env}} configuration using the following properties:

* ldap_url - The URL to the Active Directory LDAP Interface
* container_dn - The distinguished name (DN) of the container used store service principals",active-directory kerberos ldap,['ambari-server'],AMBARI,Task,Major,2015-01-09 02:41:57,30
12765779,Capture security state from components for use in enabling or disabling Kerberos,"Update each component's security state using the security status value from the _heartbeat_ message.


",kerberos security,['ambari-server'],AMBARI,Task,Major,2015-01-08 16:05:23,30
12765539,Storm service components should indicate security state,"The Storm service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. DRPC_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/storm-jaas.conf’
** StormServer/keyTab
*** not empty
*** path exists and is readable
*** required
** StormServer/principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(drpc server master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. STORM_UI_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/storm-jaas.conf’
** StormClient/keyTab
*** not empty
*** path exists and is readable
*** required
** StormClient/principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(storm ui server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. NIMBUS
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/storm-jaas.conf’
** StormServer/keyTab
*** not empty
*** path exists and is readable
*** required
** StormServer/principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(nimbus master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477

_*Note*_: There may be additional work related to _REST gateway impersonation_",keberos storm,"['ambari-server', 'stacks']",AMBARI,Task,Major,2015-01-08 03:17:11,30
12765418,Create new API endpoints to obtain the stack and stack service kerberos descriptors,"Create new API endpoints for exposing stack and stack service kerberos descriptors.  These endpoints will provide the static descriptors which are defined in the stack definition and will be immutable.

There will be at most 1 descriptor for both the stack and each stack service.
Therefore, it doesn't make sense to add a kerberos_descriptor sub-resource since there will be at most one instance.

Instead, a new ""artifacts"" endpoint is being introduced.
The kerberos descriptor will be an artifact instance.

To obtain the kerberos descriptor for the HDP 2.2 stack:
{code}
GET http://AMBARI_SERVER:8080/api/v1/stacks/HDP/versions/2.2/artifacts/kerberos_descriptor

{
  ""href"" : ""http://172.18.192.1:8080/api/v1/stacks/HDP/versions/2.2/artifacts/kerberos_descriptor"",
  ""Artifacts"" : {
    ""artifact_name"" : ""kerberos_descriptor"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""2.2""
  },
  ""artifact_data"" : {
    ""identities"" : [
       ...
    ],
    ""services"" : [
      ...
    ],
    ""properties"" : {
      ...
    }
  }
}
{code}
To obtain the kerberos descriptor for the HDP 2.2 HDFS service:
{code}
GET http://AMBARI_SERVER:8080/api/v1/stacks/HDP/versions/2.2/services/HDFS/artifacts/kerberos_descriptor

{
  ""href"" : ""http://172.18.192.1:8080/api/v1/stacks/HDP/versions/2.2/services/HDFS/artifacts/kerberos_descriptor"",
  ""Artifacts"" : {
    ""artifact_name"" : ""kerberos_descriptor"",
    ""service_name"" : ""HDFS"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""2.2""
  },
  ""artifact_data"" : {
    ""components"" : [
      ...
    ]
  }
}
{code}
",api api-addition,['ambari-server'],AMBARI,Technical task,Major,2015-01-07 17:07:56,120
12765239,Ambari API uses HTTP Header Content-Type:text/plain when the content is JSON.,"Ambari responds with a *Content-Type:text/plain* when the content is actually JSON.

And the API only accepts (PUT/POST) of JSON data with ""Content-Type:text/plain"", but fails with the correct ""Content-Type:application/json"".

This breaks most HTTP/Restful clients, including the Knox UI proxy.

Here is a GET showing JSON content with the wrong ""Content-Type"" (with some headers redacted):
{code}
$ curl -i http://localhost:8080/api/v1/clusters
HTTP/1.1 200 OK
Content-Type: text/plain

{
  ""href"" : ""http://localhost:8080/api/v1/clusters"",
  ""items"" : [
    {
      ""href"" : ""http://localhost:8080/api/v1/clusters/clustername"",
      ""Clusters"" : {
        ""cluster_name"" : ""clustername"",
        ""version"" : ""HDP-2.4""
      }
    }
  ]
}
{code}

Attempt to send JSON with Content-Type:application/json fails and it although it responds with JSON data, it says its ""text/plain"".
{code}
$ curl -v -u admin\
  -H ""Content-Type: application/json"" -H x-requested-by:useless \
  http://localhost:8080/api/v1/stacks/HDP/versions/2.4/recommendations -d @temp.json

< HTTP/1.1 500 Server Error
< Content-Type: text/plain; charset=ISO-8859-1
{
  ""status"": 500,
  ""message"": ""Server Error""
}
{code}

It accepts JSON with Content-Type:text/plain:
{code}
$ curl -v -u admin\
  -H ""Content-Type: test/plain"" -H x-requested-by:useless \
  http://localhost:8080/api/v1/stacks/HDP/versions/2.4/recommendations -d @temp.json

> POST /api/v1/stacks/HDP/versions/2.4/recommendations HTTP/1.1
> Content-Type: test/plain
< HTTP/1.1 200 OK
< Content-Type: text/plain
{
  ""resources"" : [
    {
      ""href"" : ""http://localhost:8080/api/v1/stacks/HDP/versions/2.4/recommendations/8"",
... snipped the rest of the JSON since it's not needed ...
{code}

This is the file that was sent:
{code}
$ cat temp.json
    {
      ""recommend"" : ""configurations"",
      ""services"" : [ ""AMBARI_METRICS"",""HDFS"",""HIVE"",""MAPREDUCE2"",""PIG"",""TEZ"",""YARN"",""ZOOKEEPER"" ],
      ""hosts"" : [ ""ip-10-0-1-159.ec2.internal"",""ip-10-0-1-31.ec2.internal"",""ip-10-0-1-32.ec2.internal"",""ip-10-0-1-33.ec2.internal"" ]
    }
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2015-01-06 22:49:15,0
12765201,Design admin principal session expiration handling API call,"Provide the standard error code that will be returned along with the error message.

If administrative credentials are not available
{code:title=400 Bad Request}
{
  ""status"" : 400,
  ""message"" : ""java.lang.IllegalArgumentException: Missing KDC administrator credentials.\nThe KDC administrator credentials must be set in session by updating the relevant Cluster resource.This may be done by issuing a PUT to the api/v1/clusters/(cluster name) API entry point with the following payload:\n{\n  \""session_attributes\"" : {\n    \""kerberos_admin\"" : {\""principal\"" : \""(PRINCIPAL)\"", \""password\"" : \""(PASSWORD)\""}\n  }\n}""
}
{code}

If administrative credentials are not valid, for example, incorrect principal or password (or keytab)
{code:title=400 Bad Request}
{
  ""status"" : 400,
  ""message"" : ""java.lang.IllegalArgumentException: Invalid KDC administrator credentials.\nThe KDC administrator credentials must be set in session by updating the relevant Cluster resource.This may be done by issuing a PUT to the api/v1/clusters/(cluster name) API entry point with the following payload:\n{\n  \""session_attributes\"" : {\n    \""kerberos_admin\"" : {\""principal\"" : \""(PRINCIPAL)\"", \""password\"" : \""(PASSWORD)\""}\n  }\n}""
}
{code}",kdc_credentials kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-06 20:34:26,30
12765120,Identity references fail to deference for service-level references in Kerberos descriptor parser,"Identity references fail to deference for service-level references in Kerberos descriptor parser.  For example:

{code}
/SERVICE/identity
{code}

Global references appear to work. Example:

{code}
/identity
{code}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-01-06 14:50:33,30
12765113,Variable replacement fails for some (complicated) values in org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor#replaceVariables,"Variable replacement fails for some (complicated) values in {{org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor#replaceVariables}}.

For example 
{code}
hive.metastore.local=false,hive.metastore.uris=thrift://${host}:9083,hive.metastore.sasl.enabled=true,hive.metastore.execute.setugi=true,hive.metastore.warehouse.dir=/apps/hive/warehouse,hive.exec.mode.local.auto=false,hive.metastore.kerberos.principal=hive/_HOST@${realm}
{code}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-01-06 14:09:23,30
12764542,Use cluster property rather than cluster-env/security_enabled to enable or disable Kerberos,"Use a cluster property rather than {{cluster-env/security_enabled}} to enable or disable Kerberos.  Since {{cluster-env/security_enabled}} is used by services to determine if Kerberos is enabled or not, it should not be set before completing the process of enabling or disabling Kerberos.  To declare whether the cluster enable or disable Kerberos, a property on the cluster should be set.  The property should be called {{security_type}} and must have one of the following values:
* NONE
* KERBEROS 

By using {{cluster-env/security_enabled}}, the configuration property gets set to ""true"" before Kerberos is filly enabled.   This is causing issues with stopping services so that the updated Kerberos-related configurations can be set.

Example API call to enable Kerberos
{code:title=PUT /api/v1/clusters/c1}
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
  }
}
{code}

Example API call to disable Kerberos
{code:title=PUT /api/v1/clusters/c1}
{
  ""Clusters"" : {
    ""security_type"" : ""NONE""
  }
}
{code}",kerberos,['ambari-server'],AMBARI,Task,Blocker,2015-01-02 16:49:22,30
12764272,Fix unit tests in resource_management.TestSecurityCommons.TestSecurityCommons,Fix unit tests in `resource_management.TestSecurityCommons.TestSecurityCommons`,kerbeo,['ambari-agent'],AMBARI,Bug,Major,2014-12-30 20:48:08,30
12764126,fast_hdfs_resource should work against the Hadoop FileSystem interface,"fast_hdfs_resource works against DistributedFileSystem instead of the generic FileSystem Hadoop interface, which makes it incompatible with any other file system.",HDP,['contrib'],AMBARI,Bug,Major,2014-12-30 04:14:31,126
12764125,Tez and Mapred tarballs are not copied to the default FS when fs.defaultFs is not hdfs:///,There is an assumption in {{dynamic_variable_interpretation.py}} that the default FS is hdfs which prevent tarballs from being copied with other file systems. ,HDP,[],AMBARI,Bug,Major,2014-12-30 04:09:20,126
12764057,Distributed keytab files have the incorrect owner and group access controls,"Distributed keytab files have the incorrect owner and group access controls.  Keytab files have the following (generally incorrect) ACLs:

{code}
-rw-r---- 1 root root
{code}

ACLs should be applied as indicated by the Kerberos descriptor",acl kerberos keytabs,['stacks'],AMBARI,Bug,Major,2014-12-29 18:14:03,30
12763929,JobHistoryServer Fails to pass service check in Kerberized cluster,"JobHistoryServer Fails to pass service check in Kerberized cluster due to kerberos to local account mapping failure 

{code}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=jhs, access=READ_EXECUTE, inode=""/mr-history/done/2014"":mapred:hadoop:drwxrwx---
{code}

{{core-site}} {{auth_to_local}} fails to map {{jhs/_HOST}} to {{mapred}} user.
",kerberos kerberos_descriptor,"['ambari-server', 'stacks']",AMBARI,Bug,Blocker,2014-12-28 15:23:50,30
12763839,"Agent is spamming logs with tracebacks of ""Error while executing command \'security_status\""","The ambari-agent log is filling up with messages like

{code}
2014-12-24 21:14:02,209 - Error while executing command \'security_status\':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 150, in execute
    method = self.choose_method_to_execute(command_name)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 168, 
in choose_method_to_execute
    raise Fail(""Script \'{0}\' has no method \'{1}\'"".format(sys.argv[0], command_name))
Fail: Script \'/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py\' has no method \'security_status\'
{code}

This should be corrected by adding a default implementation to avoid such messages.",kerberos security,"['ambari-agent', 'stacks']",AMBARI,Task,Blocker,2014-12-26 17:34:06,30
12763557,Knox service components should indicate security state,"The Knox service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h2. KNOX_GATEWAY
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: knox_conf_dir + '/krb5JAASLogin.conf'
** keyTab
*** not empty
*** path exists and is readable
*** required
** principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos,"['ambari-agent', 'stacks']",AMBARI,Improvement,Major,2014-12-23 21:17:20,30
12763539,core-site properties defined in the kerberos descriptor for knox gateway component does not take effect,"The core-site properties defined in the kerberos descriptor for knox gateway component does not take effect.

This is due to an invalid Kerberos descriptor file.",kerberos kerberos_descriptor q,"['ambari-server', 'stacks']",AMBARI,Bug,Major,2014-12-23 20:04:41,30
12763325,Security state check must use a temporary cache that is to be destroyed when test is complete,"When performing a security state check, using {{kinit}}, the {{kinit}} call should specify a _temporary_ cache file which is to be destroyed after the test is complete.  

This can be accomplished using the {{kinit}} {{-c <cache file>}} option.
{code}
kinit -c <temporary file>...
{code}

The cache should be destroyed using {{kdestroy}} with its {{-c <cache file>}} option.
{code}
kdestroy -c <temporary file>
{code}
",kerberos security,['ambari-agent'],AMBARI,Task,Major,2014-12-22 22:22:54,30
12763238,Update Service Configurations Kerberos task fails when there is no work to do,The _Update Service Configurations_ Kerberos task (implemented by {{org.apache.ambari.server.serveraction.kerberos.UpdateKerberosConfigsServerAction}}) fails when there is no work to do.  This task should exit nicely if the data file indicating the workload are not available since this tends to indicate that there is no work to do.,kerberos server-side,['ambari-server'],AMBARI,Task,Major,2014-12-22 15:09:51,30
12763209,Fix Kerberos-related tasks to show friendly names in UI ops list,The Kerberos-related tasks show incorrect names in the UI ops list. More friendly names should be displayed rather than 'Ambari Server Action Execute' when displaying task list.,kerberos tasks,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2014-12-22 12:02:44,30
12763165,Configuration keys in Kerberos descriptors should allow for variable replacement,"Configuration keys in Kerberos descriptors should allow for variable replacement. For example a typical {{configurations}} block may look like 
{code}
""configurations"" : [
 ""core-site"": {
     ""hadoop.proxyuser.hive.groups"":""${hadoop-env/proxyuser_group}""
   },
  ""hive-site"": {
   .....
  },
  ""webhcat-site"": {
  ....
 }
]
{code}

However some configuration keys need to be dynamically generated and should be generated using the existing variable replacement feature in {{org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor}}.  For example: 
{code}
""configurations"": [
  ""core-site"": {
     ""hadoop.proxyuser.${hive-env/hive_user}.groups"":""${hadoop-env/proxyuser_group}""
   },
  ""hive-site"": {
   .....
  },
  ""webhcat-site"": {
  ....
 }
] 
{code}

The configuration key {{hadoop.proxyuser.$\{hive-env/hive_user\}.groups}} should be processed such that _$\{hive-env/hive_user\}_ is replaced some value like {{hive}} to yield  {{hadoop.proxyuser.hive.groups}} 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2014-12-22 05:19:09,30
12763147,Dynamically created keytab files containing keys created in an MIT KDC have the incorrect key number value,Dynamically created keytab files containing keys created in an MIT KDC have the incorrect key number value. This issue causes an authentication error when kinit-ing using the invalid keytab file.,kerberos keytabs server-side,['ambari-server'],AMBARI,Bug,Blocker,2014-12-22 01:02:53,30
12763002,Keytabs need to be created to include the encryption type of AES256 CTS mode with HMAC SHA1-96,"During automated keytab generation, an entry  with the following encryption type must be added else certain services will fail to start up or properly when Kerberos is enabled:

{code}AES256 CTS mode with HMAC SHA1-96{code}

For example, NAMENODE will fail with the following errors:

{code}
2014-12-19 21:45:56,101 WARN  server.AuthenticationFilter (AuthenticationFilter.java:doFilter(551)) - Authentication exception: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96)
org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:399)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1224)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:788)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at sun.security.jgss.spnego.SpNegoContext.GSS_acceptSecContext(SpNegoContext.java:875)
	at sun.security.jgss.spnego.SpNegoContext.acceptSecContext(SpNegoContext.java:548)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:366)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:348)
	... 23 more
Caused by: KrbException: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96
	at sun.security.krb5.KrbApReq.authenticate(KrbApReq.java:273)
	at sun.security.krb5.KrbApReq.<init>(KrbApReq.java:144)
	at sun.security.jgss.krb5.InitSecContextToken.<init>(InitSecContextToken.java:108)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:771)
	... 34 more
{code}",kerberos keytabs,['ambari-server'],AMBARI,Bug,Critical,2014-12-19 21:58:09,30
12762627,"View: Files, cleanup and rendering optimization",Files view rendering of directories with many files is slow and needs to be improved.,view-files,['ambari-views'],AMBARI,Improvement,Major,2014-12-18 17:16:51,7
12762617,Zookeeper service components should indicate security state,"The Zookeeper service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h2. ZOOKEEPER_SERVER
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.config_dir + '/zookeeper-env.xml'
** zookeeper_keytab_path
*** not empty
*** path exists and is readable
*** required
** zookeeper_principal_name
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(zookeeper_server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.",kerberos security stack,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-12-18 16:31:22,30
12762612,MapReduce service components should indicate security state,"The MAPREDUCE2 service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HISTORYSERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘mapred-site.xml’
** mapreduce.jobhistory.principal
*** not empty
*** required
** mapreduce.jobhistory.keytab
*** not empty
*** path exists and is readable
*** required
** mapreduce.jobhistory.webapp.spnego-principal
*** not empty
*** required
** mapreduce.jobhistory.webapp.spnego-keytab-file
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(mapreduce.jobhistory.principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
",kerberos security stack,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-12-18 15:56:04,30
12762579,"Fix HBase Kerberos Descriptor, HBASE_CLIENT is incorrect","Fix HBase Kerberos Descriptor, HBASE_CLIENT is incorrect.",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2014-12-18 14:27:13,30
12762453,Replace ${host} variable with relevant host in Kerberos Descriptors,Replace {{host}} variable with relevant host in Kerberos Descriptors,kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2014-12-18 01:08:05,30
12762415,Create Kerberos Descriptor Resource to be accessed via the REST API,"Create Kerberos Descriptor Resource to be available via REST API such that the resource is made available as 
* A sub-resource to a {{Stack Version}} resource 
** Default Kerberos descriptor (with full hierarchy) for the requested Stack Version, read from _kerberos.json_ files in relevant stack definition
* A sub-resource to a {{Stack Service}} resource 
** Default Kerberos descriptor for the requested Stack Service, read from _kerberos.json_ file in relevant service definition
* A sub-resource to a {{Cluster}} resource
** Actual Kerberos descriptor for the requested Cluster (full hierarchy, pulled from Ambari database, declared when enabling Kerberos)
* A sub-resource to a {{Service}} resource
** Actual Kerberos descriptor for the requested Service (pulled from Ambari database, declared when enabling Kerberos)

The Kerberos descriptor is to be set when when enabling Kerberos on the cluster. The cluster update PUT request is to contain a {{kerberos_descriptor}} block. This data is parsed into a {{org.apache.ambari.server.state.kerberos.KerberosDescriptor}} and used to update the {{org.apache.ambari.server.state.kerberos.KerberosDescriptor}} created from the stack definition.

The resulting Kerberos Descriptor is used to enable Kerberos and should be stored in the Ambari database to be recalled as necessary:
* When the resource is requested
* When a new host or service is added to the cluster
",kerberos kerberos_descriptor rest_api,['ambari-server'],AMBARI,Task,Major,2014-12-17 22:36:58,120
12762413,Create Kerberos Descriptor Database Tables,"Create Kerberos Descriptor database tables to store the active Kerberos descriptor data related to a particular cluster and its services.

Kerberos descriptors contain stack/cluster-level details as well as service-level details.

The data needs to be stored such that it may be retrieved to 
* generate a single {{org.apache.ambari.server.state.kerberos.KerberosServiceDescriptor}} for a particular service in a particular cluster
* generate the entire descriptor hierarchy for a particular cluster into a {{org.apache.ambari.server.state.kerberos.KerberosDescriptor}}
",kerberos kerberos_descriptor rest_api,['ambari-server'],AMBARI,Task,Major,2014-12-17 22:34:59,120
12762291,Session attributes should be set before performing cluster update operations,"When setting the Cluster session attributes, they should be set before performing the update operations since the attributes may be needed during the process.",session,['ambari-server'],AMBARI,Bug,Major,2014-12-17 14:03:32,30
12762036,Create a more secure way to obtain and handle KDC administrator credentials,"The current mechanism for obtaining and handling KDC administrator credentials is not particularly secure thus allowing any knowledgeable user to potentally gain access to them.

A new mechanism needs to be put in place to security store this data for at least the duration of a HTTP session and potentially longer in the event Ambari allow for long-term storage of this data. 

Ideally any solution is generic enough to handle secure data of any type.",encryption kdc_credentials kerberos security,['ambari-server'],AMBARI,Improvement,Major,2014-12-16 16:18:37,30
12761888,Inject Clusters object into KerberosServerAction,Add _injected_ {{org.apache.ambari.server.state.Clusters}} into {{org.apache.ambari.server.serveraction.kerberos.KerberosServerAction}} so that the relevant cluster object ({{org.apache.ambari.server.state.Cluster}}) may be retrieved and used to help get access to the KDC administrative credentials. ,kerberos,['ambari-server'],AMBARI,Task,Major,2014-12-15 22:59:26,30
12761832,Add method to retrieve KerberosDescriptor from AmbariMetaInfo,Add the ability to query the {{org.apache.ambari.server.api.services.AmbariMetaInfo}} instance for the KerberosDescriptor for the relevant stack.,kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2014-12-15 18:47:47,30
12761707,Failed to move MySQL Server in move Hive Metastore DB,"● Fresh install of the ambari­server.
● Create a new cluster with specific services. derby database
● Move the Hive Metastore DB (MySQL Server) to another host machine. :
Select Move Hive Metastore DB (MySQL Server) from the Service Actions drop down box
select the host to assign the new master to
click next.
click Deploy.
do manual steps according to wizard
click next.
Expected
move compleat successfully
Actual
error on step ""Start and Test services""
""Failed to move MySQL Server from c6402.ambari.apache.org host to c6401.ambari.apache.org host.""",ambari-web,['ambari-web'],AMBARI,Bug,Critical,2014-12-15 09:25:21,129
12761613,Kerberos wizard: API call to save krb5-conf configuration fails with server error,"Repro is available at http://162.216.149.62:8080

{code:title=ambari-server.log}
03:40:34,494 ERROR [qtp227646247-27] KerberosHelper:187 - Invalid 'kdc_type' value: mit-kdc
03:40:34,494 ERROR [qtp227646247-27] AbstractResourceProvider:338 - Caught AmbariException when modifying a resource
org.apache.ambari.server.AmbariException: Invalid 'kdc_type' value: mit-kdc
        at org.apache.ambari.server.controller.KerberosHelper.toggleKerberos(KerberosHelper.java:188)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.updateCluster(AmbariManagementControllerImpl.java:1324)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.updateClusters(AmbariManagementControllerImpl.java:1146)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider$2.invoke(ClusterResourceProvider.java:242)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider$2.invoke(ClusterResourceProvider.java:239)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:331)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider.updateResources(ClusterResourceProvider.java:239)
        at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:317)
        at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.update(PersistenceManagerImpl.java:100)
        at org.apache.ambari.server.api.handlers.UpdateHandler.persist(UpdateHandler.java:42)
        at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
        at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:103)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:72)
        at org.apache.ambari.server.api.services.ClusterService.updateCluster(ClusterService.java:149)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:708)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1329)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:166)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.apache.ambari.server.controller.FailsafeHandlerList.handle(FailsafeHandlerList.java:132)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:363)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
        at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:931)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:992)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:856)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
{code}",kerberos,['ambari-server'],AMBARI,Bug,Major,2014-12-14 11:17:25,30
12761543,Create orchestrator to manage enabling and disabling Kerberos on a cluster,Create orchestrator to manage enabling and disabling Kerberos on a cluster.  This  facility should be triggered from the Cluster resource update handler so updates to the cluster can be used to trigger it.,kerberos,['ambari-server'],AMBARI,Task,Major,2014-12-13 06:11:15,30
12761100,KerberosCredential class should encrypt and decrypt itself,"Due to the nature of the data,{{org.apache.ambari.server.serveraction.kerberos.KerberosCredential}} should provide a facility to encrypt and decrypt itself. 

For encryption, the caller should provide the KerberosCredential instance and a key.

For decryption, the caller should provide the encrypted data (base64-encoded string) and the key.
",kerberos,['ambari-server'],AMBARI,Task,Major,2014-12-11 16:31:32,30
12761092,GlusterFS stack doesn't allow to change fs.defaultFS,"When you create Hadoop cluster based on GlusterFS stack, there are properties
that are defined in core-site config file, but can't be changed via Ambari web
interface.

The config tab of GLUSTERFS service doesn't provide any way to edit properties
{{fs.defaultFS}} and {{fs.default.name}} (which is deprecated alias of the
previous one) from core-site.xml config file.
",glusterfs hcfs,['stacks'],AMBARI,Bug,Major,2014-12-11 15:47:59,130
12761080,Add Kerberos Configuration Metadata File Builder and Reader,"Provide a facility to write (build) and read a (temporary) file used to hold data needed for updating the configuration data of services to be configured for Kerberos.

The format of the file should be hidden from the user of this facility, but will probably be CSV.",configurations kerberos security,['ambari-server'],AMBARI,Task,Major,2014-12-11 15:03:20,30
12760848,Pass Injector to ServerActionExecutor so objects can be injected into ServerAction implementations,"Pass Injector to ServerActionExecutor so object can be injected into ServerAction implementations.  

This is needed so that server-side actions can get resources like the AmbariManagementController, Clusters, or etc...
",actions server-side,['ambari-server'],AMBARI,Task,Major,2014-12-10 20:38:01,30
12760814,"Issues with ""Test DB Connection"" in HiveServer2 Move Master Wizard","Scenario ""HiveServer2"":
● Fresh install of the ambari­server. 
● Create a new cluster with specific services. with internal hive DB
● Move the HiveServer2 to another host machine. :
Select Move HiveServer2 from the Service Actions drop down box
select the host to assign the new master to
go thru wizard to move HiveServer2
Expected
HiveServer2 successfully moved from host1 to host2
Actual
error on step ""Test DB Connection""
1 try to test DB on host1(rather than host2)
if move next from host2 to host3 it test on host1(host that installed at first)
2 mysql not started on host1 or host2",ambari-server ambari-web,"['ambari-server', 'ambari-web']",AMBARI,Bug,Critical,2014-12-10 17:42:47,129
12760651,"Create Kerberos Descriptors for HDFS, YARN, MAPREDUCE2, HBASE, and HIVE services","Create Kerberos descriptor files for HDFS, YARN, MAPREDUCE2, HBASE, and HIVE services.  Also include changes for stack-level descriptor.

These files are used to describe the identities and configurations needed to properly enable Kerberos.",hdfs kerberos kerberos_descriptor mapreduce service yarn,"['ambari-server', 'stacks']",AMBARI,Task,Major,2014-12-10 05:29:52,30
12760641,Allow for service-level Kerberos descriptor to contain multiple services,"Current Kerberos descriptor handlers assume only a single service may be identified in a service-level Kerberos descriptor file.  However services like YARN include the MAPREDUCE2 service, thus multiple services need to be acknowledged.
",kerberos kerberos_descriptor stack,['ambari-server'],AMBARI,Bug,Major,2014-12-10 03:41:20,30
12760570,weird directory suggestions upon Docker containers,"Ambari cluster install wizard recommends some directory settings (NameNode directories, ZooKeep directory etc.) based upon directories mounted on LInux system.

The recommendation has some good logic, briefly
1.  hit cluster API e.g. http://host:8080/api/v1/clusters/cluster1/hosts/agent1.mydomain.com
{code}
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com"",
Hosts: {
cluster_name: ""aaa"",
cpu_count: 8,
disk_info: [
{
available: ""5911904"",
used: ""3737524"",
percent: ""39%"",
size: ""10190100"",
type: ""rootfs"",
mountpoint: ""/""
},
{
available: ""5911904"",
used: ""3737524"",
percent: ""39%"",
size: ""10190100"",
type: ""ext4"",
mountpoint: ""/""
},
{
available: ""4005892"",
used: ""0"",
percent: ""0%"",
size: ""4005892"",
type: ""tmpfs"",
mountpoint: ""/dev""
},
{
available: ""65536"",
used: ""0"",
percent: ""0%"",
size: ""65536"",
type: ""tmpfs"",
mountpoint: ""/dev/shm""
},
{
available: ""22421136"",
used: ""15874140"",
percent: ""42%"",
size: ""38295276"",
type: ""xfs"",
mountpoint: ""/etc/resolv.conf""
},
{
available: ""22421136"",
used: ""15874140"",
percent: ""42%"",
size: ""38295276"",
type: ""xfs"",
mountpoint: ""/etc/hostname""
},
{
available: ""22421136"",
used: ""15874140"",
percent: ""42%"",
size: ""38295276"",
type: ""xfs"",
mountpoint: ""/etc/hosts""
}
],
host_health_report: """",
host_name: ""agent1.mydomain.com"",
host_state: ""HEALTHY"",
host_status: ""UNHEALTHY"",
ip: ""172.17.0.8"",
last_agent_env: {
stackFoldersAndFiles: [ ],
alternatives: [ ],
existingUsers: [ ],
existingRepos: [
""unable_to_determine""
],
installedPackages: [ ],
hostHealth: {
activeJavaProcs: [ ],
agentTimeStampAtReporting: 1418160197099,
serverTimeStampAtReporting: 1418160197173,
liveServices: [
{
desc: ""ntpd is stopped "",
name: ""ntpd"",
status: ""Unhealthy""
}
]
},
umask: 18,
transparentHugePage: """",
iptablesIsRunning: true,
reverseLookup: true
},
last_heartbeat_time: 1418160197173,
last_registration_time: 1418097149332,
maintenance_state: ""OFF"",
os_arch: ""x86_64"",
os_type: ""centos6"",
ph_cpu_count: 8,
public_host_name: ""agent1.mydomain.com"",
rack_info: ""/default-rack"",
total_mem: 8011788,
desired_configs: {
capacity-scheduler: {
default: ""version1""
},
cluster-env: {
default: ""version1""
},
core-site: {
default: ""version1""
},
ganglia-env: {
default: ""version1""
},
hadoop-env: {
default: ""version1""
},
hadoop-policy: {
default: ""version1""
},
hdfs-log4j: {
default: ""version1""
},
hdfs-site: {
default: ""version1""
},
mapred-env: {
default: ""version1""
},
mapred-site: {
default: ""version1""
},
nagios-env: {
default: ""version1""
},
pig-env: {
default: ""version1""
},
pig-log4j: {
default: ""version1""
},
pig-properties: {
default: ""version1""
},
tez-env: {
default: ""version1""
},
tez-site: {
default: ""version1""
},
yarn-env: {
default: ""version1""
},
yarn-log4j: {
default: ""version1""
},
yarn-site: {
default: ""version1""
},
zoo.cfg: {
default: ""version1""
},
zookeeper-env: {
default: ""version1""
},
zookeeper-log4j: {
default: ""version1""
}
}
},
host_components: [
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/DATANODE"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""DATANODE"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/GANGLIA_MONITOR"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""GANGLIA_MONITOR"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/GANGLIA_SERVER"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""GANGLIA_SERVER"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/HDFS_CLIENT"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""HDFS_CLIENT"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/MAPREDUCE2_CLIENT"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""MAPREDUCE2_CLIENT"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/NAGIOS_SERVER"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""NAGIOS_SERVER"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/NAMENODE"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""NAMENODE"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/NODEMANAGER"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""NODEMANAGER"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/PIG"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""PIG"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/SPARK_CLIENT"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""SPARK_CLIENT"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/TEZ_CLIENT"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""TEZ_CLIENT"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/YARN_CLIENT"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""YARN_CLIENT"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/ZOOKEEPER_CLIENT"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""ZOOKEEPER_CLIENT"",
host_name: ""agent1.mydomain.com""
}
},
{
href: ""http://ambari_automation_centos7:8080/api/v1/clusters/aaa/hosts/agent1.mydomain.com/host_components/ZOOKEEPER_SERVER"",
HostRoles: {
cluster_name: ""aaa"",
component_name: ""ZOOKEEPER_SERVER"",
host_name: ""agent1.mydomain.com""
}
}
]
}
{code}
2. Filter out ""/"", ""/home"", ""/boot""
3. Filter out devtmpfs, tmpfs vboxsf

The problem is, upon docker environment, some directories is concatenated with xfs mounts Docker uses
e.g. /etc/resolv.conf, /etc/hostname
Thus, recommended directory paths are weird 
e.g.
/etc/resolv.conf/hadoop/hdfs/namenode
/etc/hostname/hadoop/hdfs/namenode
/etc/hosts/hadoop/hdfs/namenode

",patch,['ambari-web'],AMBARI,Bug,Major,2014-12-09 21:51:04,131
12760497,Hard permissions should be handled on user defined data directories,Service data directories permissions should be set correctly with strict umask (such as 027).,ambari-agent,['ambari-agent'],AMBARI,Bug,Major,2014-12-09 18:17:41,129
12760062,Update Apache Directory Server Library from 1.5.5 to 2.0.0-M19,"Update Apache Directory Server Library from 1.5.5 to 2.0.0-M19 to obtain bug fixes needed for future patches. 

One fix in particular is DIRSERVER-1882 (https://issues.apache.org/jira/browse/DIRSERVER-1882).

Currently, the only dependency on this library are unit test cases:
* org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProviderTest
* org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProviderForDNWithSpaceTest
* org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProviderBaseTest

These need to be updated to support the new library and associated dependencies. ",apacheds kerberos ldap,['ambari-server'],AMBARI,Task,Major,2014-12-08 04:55:12,30
12759426,Provide a way to parse and handle Kerberos descriptors,"Provide the ability to read in Kerberos descriptor files (kerberos.json) from the stack at various levels (stack-level, service-level) and to merge them into a single hierarchy.  The composite Kerberos descriptor data will be used to control the UI (Kerberos Wizard - see AMBARI-7450).

An example stack-level Kerberos Descriptor:
{code}
{
  ""properties"": {
    ""realm"": ""${cluster-env/kerberos_domain}"",
    ""keytab_dir"": ""/etc/security/keytabs""
  },
  ""identities"": [
    {
      ""name"": ""spnego"",
      ""principal"": {
        ""value"": ""HTTP/_HOST@${realm}""
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/spnego.service.keytab"",
        ""owner"": {
          ""name"": ""root"",
          ""access"": ""r""
        },
        ""group"": {
          ""name"": ""${cluster-env/user_group}"",
          ""access"": ""r""
        }
      }
    }
  ],
  ""configurations"": [
  ]
}
{code}

An example service-level Kerberos Descriptor - HDFS:
{code}
{
  ""configurations"": [
    {
      ""core-site"": {
        ""hadoop.security.authentication"": ""kerberos"",
        ""hadoop.rpc.protection"": ""authentication; integrity; privacy"",
        ""hadoop.security.authorization"": ""true""
      }
    }
  ],
  ""components"": [
    {
      ""name"": ""NAMENODE"",
      ""identities"": [
        {
          ""name"" : ""namenode_nn"",
          ""principal"": {
            ""value"": ""nn/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.kerberos.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/nn.service.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.keytab.file""
          }
        },
        {
          ""name"" : ""namenode_host"",
          ""principal"": {
            ""value"": ""host/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.kerberos.https.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/host.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.keytab.file""
          }
        },
        {
          ""name"" : ""/spnego"",
          ""principal"": {
            ""configuration"": ""hdfs-site/dfs.web.authentication.kerberos.principal""
          },
          ""keytab"": {
            ""configuration"": ""hdfs/dfs.web.authentication.kerberos.keytab""
          }
        }
      ]
    },
    {
      ""name"": ""DATANODE"",
      ""identities"": [
        {
          ""name"" : ""datanode_dn"",
          ""principal"": {
            ""value"": ""dn/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.kerberos.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/dn.service.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.keytab.file""
          }
        },
        {
          ""name"" : ""datanode_host"",
          ""principal"": {
            ""value"": ""host/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.datanode.kerberos.https.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/host.keytab.file"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.keytab.file""
          }
        }
      ]
    },
    {
      ""name"": ""SECONDARY_NAMENODE"",
      ""identities"": [
        {
          ""name"" : ""secondary_namenode_nn"",
          ""principal"": {
            ""value"": ""nn/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.kerberos.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/snn.service.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.keytab.file""
          }
        },
        {
          ""name"" : ""secondary_namenode_host"",
          ""principal"": {
            ""value"": ""host/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.kerberos.https.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/host.keytab.file"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.keytab.file""
          }
        },
        {
          ""name"" : ""/spnego"",
          ""principal"": {
            ""configuration"": ""hdfs-site/dfs.web.authentication.kerberos.principal""
          },
          ""keytab"": {
            ""configuration"": ""hdfs/dfs.web.authentication.kerberos.keytab""
          }
        }
      ]
    }
  ]
}
{code}
",kerberos kerberos_descriptor stack,"['ambari-server', 'stacks']",AMBARI,Task,Major,2014-12-04 14:13:01,30
12759310,"Views: Files, UI and trash","- minor UI (do not link if file/directory is not readable)
- added a readme
- handle trash as a ""move"" operation",view-files,['ambari-views'],AMBARI,Improvement,Major,2014-12-03 23:22:19,125
12758848,Possibility to edit hadoop-env file in Ambari 1.7 with HDP_2.1.GlusterFS,"In Ambari 1.7 with HDFS is possible to edit hadoop-env file (*HDFS* -> *Configs* -> *Advanced hadoop-env*).
But this functionality is not available for Ambari 1.7 with GlusterFS stack. There is no similar section on *GLUSTERFS* -> *Configs* page and there is probably missing some other configuration possibilities (Advanced hadoop-policy,  Custom hadoop-policy).",glusterfs hcfs,[],AMBARI,Bug,Major,2014-12-02 09:08:20,130
12758841,Configuration parameter 'io.compression.codecs' missing in HDP 2.3.GlusterFS stack in ambari-2.1,"Installation of cluster via ambari-1.7, stack HDP 2.1.GlusterFS fails on task ""Oozie Client install"" with following error:
{noformat}
2014-12-02 08:58:09,369 - Error while executing command 'install':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 123, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_client.py"", line 31, in install
    self.configure(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_client.py"", line 34, in configure
    import params
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/params.py"", line 152, in <module>
    lzo_enabled = ""com.hadoop.compression.lzo"" in io_compression_codecs
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 79, in __getattr__
    raise Fail(""Configuration parameter '""+self.name+""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'io.compression.codecs' was not found in configurations dictionary!
{noformat}

When I add new property *io.compression.codecs* with value:
{noformat}org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec{noformat}
on page *Customize Services* -> *GLUSTERFS* -> *Custom core-site*, installation pass well.",glusterfs hcfs,[],AMBARI,Bug,Minor,2014-12-02 08:27:57,124
12758637,"Pig View: readme + blueprint, minor UI","Updates to the Pig View:

- added readme with ""getting started"" instructions
- added blueprint for ""getting started"" with single-node-cluster
- cleanup some CSS
- added icons to logs/results/delete on History page
- updated view config property descriptions
- modified jobs execution tab to have script closed by default and results open
- modified script CSS on jobs execution tab",view-pig,['ambari-views'],AMBARI,Improvement,Major,2014-12-01 13:01:17,125
12758493,Pig service components should indicate security state,"The Pig service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. PIG
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}
",kerberos pig security,"['ambari-server', 'stacks']",AMBARI,Improvement,Minor,2014-11-30 12:07:29,30
12758492,Oozie service components should indicate security state,"`The Oozie service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. OOZIE_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/oozie-site.xml’
** oozie.service.AuthorizationService.security.enabled
*** = “true”
*** required
** oozie.service.HadoopAccessorService.kerberos.enabled
*** = “true”
*** required
** local.realm
*** not empty
*** required
** oozie.authentication.type
*** = “kerberos”
*** required
** oozie.authentication.kerberos.principal
*** not empty
*** required
** oozie.authentication.kerberos.keytab
*** not empty
*** path exists and is readable
*** required
** oozie.authentication.kerberos.name.rules
*** not empty
*** required
** oozie.service.HadoopAccessorService.keytab.file
*** not empty
*** path exists and is readable
*** required
** oozie.service.HadoopAccessorService.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(oozie principal) && kinit(HadoopAccessorService principal) succeeds
`        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos oozie security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-30 11:52:34,30
12758488,Kafka service components should indicate security state,"The Kafka service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. KAFKA_BROKER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}",kafka kerberos security,"['ambari-server', 'stacks']",AMBARI,Improvement,Minor,2014-11-30 11:23:37,30
12758487,Hive service components should indicate security state,"The Hive service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HIVE_METASTORE
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘hive-site.xml’
** hive.metastore.sasl.enabled""
*** = ""true""
*** required 
** hive.server2.authentication
*** = “kerberos”
*** required
** hive.security.authorization.enabled
*** = “true”
*** required
** hive.metastore.kerberos.principal
*** not empty
*** required
** hive.metastore.kerberos.keytab.file
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(have metastore principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HIVE_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘hive-site.xml’
** hive.metastore.sasl.enabled""
*** = ""true""
*** required 
** hive.server2.authentication
*** = “kerberos”
*** required
** hive.security.authorization.enabled
*** = “true”
*** required
** hive.server2.authentication.kerberos.principal
*** not empty
*** required
** hive.server2.authentication.kerberos.keytab
*** not empty
*** path exists and is readable
*** required
** hive.server2.authentication.spnego.principal
*** not empty
*** required
** hive.server2.authentication.spnego.keytab
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hive server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. WEBHCAT_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘hive-site.xml’
** hive.metastore.sasl.enabled""
*** = ""true""
*** required 
** hive.server2.authentication
*** = “kerberos”
*** required
** hive.security.authorization.enabled
*** = “true”
*** required
* Configuration File: params.config_dir + ‘webhcat-site.xml’
** templeton.kerberos.secret
*** = “secret”
*** required
** templeton.kerberos.principal
*** not empty
*** required
** templeton.kerberos.keytab
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(webhcat server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HIVE_CLIENT
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",hcatalog hive kerberos metastore mysql security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-30 11:10:06,30
12758434,HBase service components should indicate security state,"The HBase service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HBASE_MASTER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hbase_conf_dir + ‘/hbase-site.xml’
** hbase.security.authentication
*** = “kerberos”
*** required
** hbase.security.authorization
*** = “true”
*** required
** hbase.master.keytab.file
*** not empty
*** path exists and is readable
*** required
** hbase.master.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hbase master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HBASE_REGIONSERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hbase_conf_dir + ‘hbase-site.xml’
** hbase.security.authentication
*** = “kerberos”
*** required
** hbase.security.authorization
*** = “true”
*** required
** hbase.regionserver.keytab.file
*** not empty
*** path exists and is readable
*** required
** hbase.regionserver.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hbase region server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477

_*Note*_: There may be additional work related to _REST gateway impersonation_",hbase kerberos security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-29 11:20:11,30
12758432,Flume service components should indicate security state,"The Flume service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. FLUME_HANDLER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}
",flume kerberos security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-29 10:51:42,30
12758430,Kerberos service components should indicate security state,"The Kerberos service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. KERBEROS_CLIENT
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
** config\['configurations']\['cluster-env']\['smokeuser_keytab'] 
*** not empty
*** path exists and is readable
*** required
** config\['configurations']\['cluster-env']\['smokeuser'] 
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(smoketest user) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}",kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Minor,2014-11-29 10:26:33,30
12758410,YARN service components should indicate security state,"The YARN service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. RESOURCEMANAGER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: /etc/hadoop/conf/yarn-site.xml
** yarn.timeline-service.enabled
*** = ""true""
*** required
** yarn.timeline-service.http-authentication.type
*** = ""kerberos""
*** required
** yarn.acl.enable
*** = ""true""
*** required
** yarn.resourcemanager.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.resourcemanager.principal
*** not empty
*** required
** yarn.resourcemanager.webapp.spnego-keytab-file
*** not empty
*** path exists and is readable
*** required
** yarn.resourcemanager.webapp.spnego-principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(resourcemanager principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. NODEMANAGER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: /etc/hadoop/conf/yarn-site.xml
** yarn.timeline-service.enabled
*** = ""true""
*** required
** yarn.timeline-service.http-authentication.type
*** = ""kerberos""
*** required
** yarn.acl.enable
*** = ""true""
*** required
** yarn.nodemanager.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.nodemanager.principal
*** not empty
*** required
** yarn.nodemanager.webapp.spnego-keytab-file
*** not empty
*** path exists and is readable
*** required
** yarn.nodemanager.webapp.spnego-principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(nodemanager principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. APP_TIMELINE_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: /etc/hadoop/conf/yarn-site.xml
** yarn.timeline-service.enabled
*** = ""true""
*** required
** yarn.timeline-service.http-authentication.type
*** = ""kerberos""
*** required
** yarn.acl.enable
*** = ""true""
*** required
** yarn.timeline-service.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.timeline-service.principal
*** not empty
*** required
** yarn.timeline-service.http-authentication.kerberos.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.timeline-service.http-authentication.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(nodemanager principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-29 00:15:44,30
12758407,Falcon service components should indicate security state,"The Falcon service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND. Each component should determine it's state as follows:

h3. FALCON_CLIENT
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. PseudoCode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

h3. FALCON_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.falcon_conf_dir + ‘/startup.properties’
** *.falcon.authentication.type
*** = “kerberos”
** *.falcon.service.authentication.kerberos.principal
*** not empty
*** required
** *.falcon.service.authentication.kerberos.keytab
*** not empty
*** required
*** path exists and is readable
** *.dfs.namenode.kerberos.principal
*** not empty
*** required
** *.falcon.http.authentication.type
*** = “kerberos”
** *.falcon.http.authentication.kerberos.principal
*** not empty
*** required
** *.falcon.http.authentication.kerberos.keytab
*** not empty
*** required
*** path exists and is readable

h4. Pseudocode
{code} 
if indicators imply security is on and validate
    if kinit(falcon principal) && kinit(http principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}
 
_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-28 23:29:34,30
12758405,HDFS service components should indicate security state,"The HDFS service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h2. NAMENODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: /params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.namenode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.namenode.kerberos.principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(namenode principal) && kinit(https principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. DATANODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.datanode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.datanode.kerberos.principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(datanode principal) && kinit(https principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. SECONDARY_NAMENODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.secondary.namenode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.secondary.namenode.kerberos.principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(namenode principal) && kinit(https principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. HDFS_CLIENT
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Env Params:  hadoop-env
** hdfs_user_keytab
*** not empty
*** path exists and is readable
*** required
** hdfs_user_principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hdfs user principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. JOURNALNODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: /params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.journalnode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.journalnode.kerberos.principal
*** not empty
*** required
h3. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

h2. ZKFC
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",agent kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-28 23:10:31,30
12758241,Make the fix for skip groupmod issue available in GlusterFS stacks,"The problem of groupmod errors AMBARI-5748 has been already fixed by introducing new ambari config option: ignore_groupsusers_create.

The problem is that this fix is not directly usable with GlusterFS stack, because the new option has been created in HDFS component which is not accessible (for obvious reasons: GlusterFS conflicts with HDFS).

For those reasons, the new config should be made available for GlusterFS stack as well.",glusterfs,['stacks'],AMBARI,Bug,Minor,2014-11-27 15:24:45,130
12757943,Create server-side actions to create kerberos principals and keytabs,"Create server-side actions to generate the Kerberos principals and keytabs.  These actions will be used when setting up Kerberos for services when Kerberizing a cluster. 
",kerberos server-side,['ambari-server'],AMBARI,New Feature,Major,2014-11-26 10:43:16,30
12757767,Update ConfigurationResourceProvider to handle Kerberos Administrative Credentials as a special case ,"Certain configuration settings need to handled in special-case scenarios. For example short-lived settings to be stored per request or session scope.  Or secure data the must not be stored in the Ambari database.

An example of this type of data is the administrative credentials used to manage a KDC server.   This _configuration_ data is short lived (per session) and sensitive. Therefore, it must be handled as a special case.  

To determine that a configuration request contains this data, the {{type}} of the configuration is to be used.  For this specific case, a configuration {{type}} of *_kerberos_admin_identity_* will trigger the special case to secure and store the administrative credentials in a file.  Ideally if the _session_ data was available (see AMBARI-8426) a session-based encryption key would be created and stored in session. That key would then be used to encrypt the data from this request. The encrypted data and key would then be retrieved from the _session_, decrypted, and used as needed. 



",api configuration kerberos session,['ambari-server'],AMBARI,Improvement,Major,2014-11-25 18:38:53,132
12757519,Kerberos wizard: Test kerberos command fails when no credentials specified for test user,"On Install aand Test kerberos page, If optional values for test principal and test keytab are left blank then test kerberos command (kerberos service check) fails.

This occurs because test user details are empty rather than missing from the request/configuration.",api,['ambari-server'],AMBARI,Bug,Major,2014-11-24 21:30:49,30
12757414,Provide access to session from resource handler/provider,"There should be a way to get access to the web server's session data from a (REST API) resource handler.  

This will allow a resource handler to access information such as a session encryption key that may be used to encrypt data during that session.  An example of this would be when performing Kerberos-related activities, the following flow can occur:

# Session encryption key is created
# User uploads KDC administrator credentials 
# administrator credential are encrypted using the session encryption key and persisted - maybe on disk, maybe in the Ambari database
# For every Kerberos administration action that needs to occur during that session, the administrative credentials may be loaded into memory, decrypted, used, and removed from memory 
# When the session terminates, the encryption key is lost and the persisted administrator credentials become lost
 

",encryption kerberos security session,['ambari-server'],AMBARI,New Feature,Major,2014-11-24 14:18:45,132
12756163,Refactor the OS-dependent Ambari Agent Windows components,See AMBARI-8317 for description of the refactoring work. Breaking the JIRA between ambari server and ambari agent.,WinRefactor,['ambari-agent'],AMBARI,Task,Major,2014-11-18 18:02:30,63
12755616,Components should indicate Security State (via ambari-agent),"In order to properly handle the automated installation or removal of a security infrastructure (like Kerberos) in the cluster, Ambari needs to know whether each component on the hosts of the cluster is properly _secured_ or not.  This information may be compared with data on the Ambari server to help determine what steps should be taken to ensure the cluster is in the correct _secured_ state.

To do this, the current and desired component security state is maintained in the Ambari database.  The Ambari server will update the desired state details according to whether the cluster is to be secured or not and whether the relevant service has enough metadata to be secured.  If the desired and actual security state details do not match, the Ambari server will take the necessary steps to work towards synchronization. 

In order for a component to indicate its security status, a new property needs to be returned in the {{STATUS_COMMAND}} response message (from the Ambari agent).  This property should be named ‘securityState’ and should have one of the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{ERROR}} - Indicates the component is not secured due to an error condition

To properly set this state value, a call needs to be executed per component querying for its specific state.  Due to the differences on how each component is secured and how it may be determined if security is setup what type is configured, and working is it properly, it is necessary for each component to have its own logic for determining this state. Therefore the ambari-agent process will need to call into the component’s configured (lifecycle) script and wait for its response - not unlike how it determines whether the component is up and running.

After the infrastructure is in place, each service definition needs to be updated to implement the new security status check function.  The function should perform the following steps:

* Determine if security is enabled or disabled
** If disabled, return ""UNSECURED""
** If enabled, determine what type of security is enabled
*** If Kerberos is configured
**** Perform tests (kinit?, ping KDC?) to determine if the configuration appears to be working
***** If working, return “SECURED_KERBEROS”
***** If not working, return “ERROR”
*** Else, return ""UNKNOWN""

If no function is available, the Ambari agent should return “UNKNOWN”.",kerberos states,['ambari-agent'],AMBARI,New Feature,Critical,2014-11-15 16:29:36,30
12755574,Requesting the config_groups collection resource with no registered configuration groups results in a 404 response,"GET http://AMBARI_HOST:8080/api/v1/clusters/c1/config_groups

{
  ""status"" : 404,
  ""message"" : ""The requested resource doesn't exist: ConfigGroup not found, ConfigGroup/cluster_name=c1""
}

The cluster c1 exists and has no configuration groups.  The above response is incorrect.  As per the api specification, it should return an empty collection.",api,['ambari-server'],AMBARI,Bug,Major,2014-11-15 03:14:39,132
12755470,Add Security State to Ambari database,"In order to track which services and components have been or need to be secured, several tables need to be updated with a {{security_state}} column to contain one the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{SECURING}} - Indicates the component is in the process of being secured
* {{UNSECURING}} - Indicates the component is in the process of being unsecured
* {{ERROR}} - Indicates the component is not secured due to an error condition

The following tables need to be updated:

* hostcomponentdesiredstate - To indicate whether the component needs security added or removed
* hostcomponentstate - To indicate whether the component is currently configured for security or not 
* servicedesiredstate - To indicate whether the service (and it components) should or should not be secured",kerberos,['ambari-server'],AMBARI,New Feature,Blocker,2014-11-14 20:16:50,30
12755143,Refactor the OS-dependent Ambari Server Windows components,"From the https://reviews.apache.org/r/27898/ feedback:

Refactor OS-dependent classes into contract classes and implementations connected by class factories. This includes but is not limited to: HeartbeatStopHandler, Hardware, AgentConfig, HostInfo, AmbariAgentService.",WinRefactor,['ambari-server'],AMBARI,Task,Major,2014-11-13 19:26:59,128
12754548,"Alerts UI: Provide ability to create, rename, duplicate, remove alert groups",The Manage Alert Groups dialog should have ability to add/remove/rename and duplicate alert groups per service. Changes should have the necessary alert definitions and notifications applied.,Alerts-UI,['ambari-web'],AMBARI,Task,Major,2014-11-11 19:24:43,102
12754487,Create facility to get and set kerberos plans via REST API,"To help with _kerberizing_ a cluster, a {{Kerberos Plan}} may be used set relevant security properties for a service.

A {{Kerberos Plan}} is essentially composite {{Kerberos Descriptor}} consisting of details for an entire cluster (as opposed to just a single service).  The details that a Kerberos Plan encapsulates are as follows:
* Cluster-wide Kerberos Identities
* Cluster-wide Kerberos-related configurations
* Service-specific Kerberos Identities
* Service-specific Kerberos-related configurations

An (pseudo) Kerberos Plan may be as follows:
{code}
{
  ""identities"": [
    ... cluster-wide identity specifications ...
  ],
  ""configurations"": [
    ... cluster-wide configuration specifications ...
  ],
  ""services"": [
    {
      ... service-specific specifications ...
      ""components"": [
        ... component-specific specifications ...
     ]
    }
  ]
}
{code}",api kerberos kerberos_plan resource,['ambari-server'],AMBARI,New Feature,Blocker,2014-11-11 15:34:56,30
12754141,Provide a way to get service-specific Kerberos descriptor via REST API,Provide a way for a caller via the REST API to get information about a service's Kerberos descriptor.  This information should probably be attached to a service resource response.,api service stack,['ambari-server'],AMBARI,New Feature,Major,2014-11-10 14:10:47,30
12754056,Ambari HDP 2.0.6+ stacks do not work with fs.defaultFS not being hdfs,"Right now changing the default file system does not work with the HDP 2.0.6+ stacks. Given that it might be common to run HDP against some other file system in the cloud, adding support for this will be super useful. One alternative is to consider a separate stack definition for other file systems, however, given that I noticed just 2 minor bugs needed to support this, I would rather extend on the existing code.

Bugs:
 - One issue is in Nagios install scripts, where it is assumed that fs.defaultFS has the namenode port number.
 - Another issue is in HDFS install scripts, where {{hadoop dfsadmin}} command only works when hdfs is the default file system.

Fix for both places is to extract the namenode address/port from {{dfs.namenode.rpc-address}} if one is defined and use it instead of relying on {{fs.defaultFS}}. 

Haven't included any tests yet (my first Ambari patch, not sure what is appropriate, so please comment).",HDP,['stacks'],AMBARI,Bug,Major,2014-11-10 04:52:58,126
12753855,Alerts UI: Create Manage Alerts Group dialog,"We want a dialog to manage alert groups. 
1. Users should be able to see the list of alert groups, and view the alert definitions configured in them. 
2. User should be able to add/remove alert definitions for that alert group.
3. User should be able to add/remove/rename/duplicate Alert Group.",Alerts-UI,['ambari-web'],AMBARI,Task,Major,2014-11-07 23:27:04,102
12753131,"Implement custom command for checking connectivity to KDC, via REST API","There needs to be a way, given the details about a KDC to verify that Ambari and (optionally) the nodes in the existing cluster can connect to it. 

From the cluster hosts, this test should test that the address and port combinations are reachable. 

From the Ambari server, this test should make sure the administrator credentials allow at least read access to the KDC.

As an example of a similar action, see how Oozie does this for DB check pre install.

",connectivity_ kdc kerberos,"['ambari-agent', 'ambari-server']",AMBARI,New Feature,Major,2014-11-05 19:01:37,133
12753119,Provide stage resource information via REST API,"Currently, it is possible to query Ambari (via the REST API) for details about _asynchronous_ requests and their related tasks. This useful when trying to obtain progress information.  However, some information necessary for the UI to indicate meaningful progress is not available.  This information is related to the stages that are generated. 

*NOTE:* Each _asynchronous_ request is broken down into 1 or more stages and each stage contains 1 or more tasks.

If stage information was available via the REST API, it would be possible for the caller (maybe a UI) to track high-level tasks (at the {{stage}} level) rather than each lower-level unit of work (at the {{task}} level).   

To allow for this, a new API resource (and associated handler) needs to be created.  The resource should be read-only (like {{requests}} and {{tasks}}), and should provide information stored in the {{stage}} table from the Ambari database.  

The following properties should be returned for each {{stage}}:

* stage_id
* request_id
* cluster_id
* request_context 
** _This should probably be renamed to something more appropriate, like stage_context, stage_name, or etc..._
* start_time
* end_time
* progress_percent
* status

It is expected that the resources would be queried using:

{code}
GET  /api/v1/clusters/{clusterid}/requests/{requestid}/stages
{code}

Also, some subset of the stage data should be provided when querying for details about a specific {{request}}, like in:

{code}
GET  /api/v1/clusters/{clusterid}/requests/{requestid}
{code}

See {{request}} and {{task}} resource for examples.


",api resources rest_api,['ambari-server'],AMBARI,New Feature,Major,2014-11-05 18:33:18,132
12753030,Usability: Do not force a mysql restart to apply Hive changes,Ambari currently requires all services under tab to be restarted when changes are made to the configuration. This includes forcing a mysql restart when Hive settings are changed. Mysql should only have to be restarted when settings specific to mysql get changed.,ambari-server,['ambari-server'],AMBARI,Improvement,Major,2014-11-05 12:55:24,129
12751972,Ambari Server HA Phase 1: Provide backup and restore capability,"If the Ambari server host is lost, there should be capability to provision a new Ambari server that can be brought up with most of the existing state. We can assume that the Database would be external and retains its state.
This requires:
Backup feature to periodically backup Ambari non-DB state: keystores, certificates, Ambari server state, agents states
Restore feature to start an Ambari server from a back up. Also, re-point agents to the new Ambari server.",ambari-agent ambari-server,"['ambari-agent', 'ambari-server']",AMBARI,Improvement,Critical,2014-10-31 14:23:16,129
12751820,Alerts UI: Create model and mapper for Alert-Notifications,"Alert notifications will be used by Alert definition and host level alert pages. For them we need to know more details about the notifications that were used. 

We need a model and mapper created for Alert Notifications. We need to refer to the API to see what exact fields have to mapped.


{{Alert-Target}} class is:
* id (number),
* name: (string),
* type: (string),  // notification type 'EMAIL' ...
* description: (string),
* properties: (string)
",Alerts-UI,['ambari-web'],AMBARI,Task,Major,2014-10-30 23:02:13,102
12751428,Provide better values for selected HBase configs,"1. retries.number should not be set (use the default instead)
2. Major compaction should be set to the 604... number rather than the 864... number Ambari is using now.
3. block multiplier should be 4 not 2.

In addition, hbase-env.sh uses a very small heap size for regionservers. This needs to be increased to 4GB.
",hdp,['stacks'],AMBARI,Improvement,Critical,2014-10-29 16:57:52,134
12751059,Usability: Ambari should handle hard permissions restrictions,"Ambari needs to support hard permission restrictions (umask 027) and should execute ownership and permission change commands automatically during the installation: change ownership of the created folders right after creation. This way, installation will not fail and internal users will be able to proceed as needed.",ambari-agent ambari-server hdp,"['ambari-agent', 'ambari-server', 'stacks']",AMBARI,Bug,Blocker,2014-10-28 11:32:00,129
12751053,Expose the ability to trigger all Host Checks on demand via API,Need a way to re-run Host Checks post deploy.,ambari-agent,['ambari-agent'],AMBARI,Improvement,Major,2014-10-28 10:51:15,129
12750780,Allow for server-side commands,"Ambari currently handles _client-/agent-side_ commands; however there is no ability to handle _server-side_ commands. Server-side commands should be specified as a task in a stage and managed along with the stage.

*Use Case:*  Generate principals and keytabs on the Ambari server before sending the keytabs to their relevant hosts.

*Implementation:*  To add the concept of a server-side task:
* update {{org.apache.ambari.server.serveraction.ServerAction}} to be an _abstract class_
** _server-side_ tasks must implement this class 
* reuse existing _host_role_command_ and _execution_command_ data
** _server-side_ tasks are to have a role of {{AMBARI_SERVER_ACTION}}
** _server-side_  execution command data should be encapsulated as JSON and specify the ServerAction implementation class and any needed payload data
* {{org.apache.ambari.server.actionmanager.ActionScheduler}} and {{org.apache.ambari.server.serveraction.ServerActionManagerImpl}} need to be updated to handle the execution of server-side tasks
** each _server-side_ task should be executed in its own thread.
*** _server_side_ tasks should be executed in (staged) order, serially - not in parallel
*** _server_side_ tasks should ensure not to mess up _stage_ ordering

",ambari-server commands server server-side tasks,['ambari-server'],AMBARI,New Feature,Major,2014-10-27 14:28:29,30
12749306,"ShellCommandUtil.Results class should be public, not package private","{{ShellCommandUtil.Results}} class should be _public_, _not package_ private. Because it is _package private_, any {{ShellCommandUtil}} public methods returning a {{ShellCommandUtil.Results}} is basically unusable outside of the {{org.apache.ambari.server.utils}} package.

Solution:  Make {{ShellCommandUtil.Results}} and its methods explicitly _public_",utils,['ambari-server'],AMBARI,Bug,Minor,2014-10-20 17:39:35,30
12748780,Error message missing details in OsFamily init method,"When initializing an instance of the OsFamily class, if the {{os_family.json}} file is not found, the error message specified in the thrown exception is missing the relevant details:

{code}
Could not load OS family definition from %s file
{code}

The absolute path to the file should be presented rather than {{%s}}.",ambari-server exception-reporting,['ambari-server'],AMBARI,Bug,Trivial,2014-10-17 01:20:50,30
12748377,Ability to support umask 027,"Support installing and managing a cluster with hard permission restrictions, such as umask 027",ambari-agent ambari-server hdp,"['ambari-agent', 'ambari-server', 'stacks']",AMBARI,Improvement,Blocker,2014-10-15 19:04:58,129
12747932,Fix AMBARI-7740 HDP 2.2 should read the base repo URLs from JSON file,"Currently, HDP 2.2 does not read the latest base repo URLs from a JSON file, unlike HDP 2.1.
Fixing additional file to complete AMBARI-7740
",hdp,['stacks'],AMBARI,Bug,Critical,2014-10-14 06:06:21,135
12747633,DataNode decommision error in secured cluster,"Decommissioning a DataNode from a secured cluster returns errors with the following messages

{code}
STDERR: 
2014-10-13 10:37:31,896 - Error while executing command 'decommission':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 111, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HDFS/package/scripts/namenode.py"", line 66, in decommission
    namenode(action=""decommission"")
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HDFS/package/scripts/hdfs_namenode.py"", line 70, in namenode
    decommission()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HDFS/package/scripts/hdfs_namenode.py"", line 145, in 

decommission
    user=hdfs_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 149, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 115, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 239, in action_run
    raise ex
Fail: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/dn.service.keytab dn/master-

6.amber.gbcl.net@AMBER.GBCLUSTER.NET;' returned 1. kinit: Client not found in Kerberos database while getting initial 

credentials
{code}

{code}
STDOUT:
2014-10-13 10:37:31,793 - File['/etc/hadoop/conf/dfs.exclude'] {'owner': 'hdfs', 'content': Template

('exclude_hosts_list.j2'), 'group': 'hadoop'}
2014-10-13 10:37:31,796 - Writing File['/etc/hadoop/conf/dfs.exclude'] because contents don't match
2014-10-13 10:37:31,797 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/dn.service.keytab dn/master-

6.amber.gbcl.net@AMBER.GBCLUSTER.NET;'] {'user': 'hdfs'}
2014-10-13 10:37:31,896 - Error while executing command 'decommission':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 111, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HDFS/package/scripts/namenode.py"", line 66, in decommission
    namenode(action=""decommission"")
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HDFS/package/scripts/hdfs_namenode.py"", line 70, in namenode
    decommission()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HDFS/package/scripts/hdfs_namenode.py"", line 145, in 

decommission
    user=hdfs_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 149, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 115, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 239, in action_run
    raise ex
Fail: Execution of '/usr/bin/kinit -kt /etc/security/keytabs/dn.service.keytab dn/master-

6.amber.gbcl.net@AMBER.GBCLUSTER.NET;' returned 1. kinit: Client not found in Kerberos database while getting initial 

credentials
{code}

The reason is that Ambar-agent uses DataNode principal to perform HDFS refresh, which should be done as NameNode. This error can be solved by letting Ambari-agent uses NameNode kerberos principal and keytab. Note that [AMBARI-5729|https://issues.apache.org/jira/browse/AMBARI-5729] solves similar issue for NodeManager.",patch,"['ambari-server', 'stacks']",AMBARI,Bug,Major,2014-10-13 01:56:11,136
12747427,HDP 2.2 should read the base repo URLs from JSON file,"Currently, HDP 2.2 does not read the latest base repo URLs from a JSON file, unlike HDP 2.1.
Supporting repo URL JSON file for HDP 2.2 makes QE's life easier in terms of picking up new HDP builds to test.
",hdp,['stacks'],AMBARI,Bug,Critical,2014-10-10 22:06:49,52
12745803,Validation required for adding a Group Name,"Modify ""Create New Configuration Group"" and ""Rename Configuration Group"" to not allow invalid characters in name. Display warning when user inputs anything other than alphanumerics, underscores, hyphens and spaces.",ambari-web,[],AMBARI,Bug,Minor,2014-10-03 19:07:36,137
12745801,HDFS config will not save ,"When modifying the HDS config or adding new properties, no success dialog appears. The save button would then be disabled as though save was successful. When trying to switch tabs, a dialogue would appear suggesting that there are unsaved changes. ",ambari-web,[],AMBARI,Bug,Major,2014-10-03 18:57:32,137
12745800,Correct evident spelling and grammatical errors in global strings,"Background Operations popup has ""Background Operations"" pluralized when there is only 1 operation.

Constants and Labels in messages.js have spelling errors.",ambari-web,[],AMBARI,Bug,Minor,2014-10-03 18:55:02,137
12744868,Create mahout definition for bigtop,Create mahout definition for bigtop,Mahout,['ambari-server'],AMBARI,Bug,Major,2014-09-30 05:58:45,138
12744090,Services shown in repository page are hardcoded,If you go to the Repository page (Admin/Repository) you see a list of services. These services are hardcoded and do not reflect all the services that are available. ,patch,['ambari-web'],AMBARI,Improvement,Minor,2014-09-25 18:36:33,139
12743796,Add Visibility Attributes to Services,"*Problem*
In a stack there may be one or more services that should not be installable, configurable, or managed via the Ambari web-based interface.  Such a service may need to be installed via some Ambari API call or manually. There is no way to specify these “visibility” attributes within a service’s definition; thus to “hide” a service, one-off code needs to be added to the different Ambari facilities.

*Solution*
Add visibility attributes to service definitions to describe how Ambari should expose services via front-end facilities.   The following Boolean attributes should be settable by services:
* installable
** indicates if Ambari can install the service - if not installable, the service should be hidden from “add service” features of Ambari
* managed
** indicates if Ambari can start and stop the service - if not managed, the service should be hidden from all views where management operations can occur
* monitored
** indicates if Ambari can monitor the service - if not monitored, status information should not be displayed
",installation service ui visibility,"['ambari-web', 'stacks']",AMBARI,Epic,Major,2014-09-24 15:33:11,140
12743516,Add UI Wizard to facilitate configuring cluster to use Kerberos,"Create a wizard in the web-base interface to help users configure Kerberos on the cluster. 

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12667192/AmbariClusterKerberization.pdf] for more information.",kerberos ui wizard,['ambari-web'],AMBARI,New Feature,Major,2014-09-23 13:16:55,48
12743515,Update API to enable configuring of services to use Kerberos,"Update API to enable the ability to configure services to use Kerberos.  The API call(s) should create necessary Kerberos identities (if necessary) and keytab files, distribute them to the relevant hosts, and update relevant service configurations. 

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12667192/AmbariClusterKerberization.pdf] for more information.
",api kerberos stack,['ambari-server'],AMBARI,New Feature,Major,2014-09-23 13:14:55,30
12743514,Create Kerberos Service,"Create a service to manage the (optional) Kerberos server (managed KDC) and client components.

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12671235/AmbariClusterKerberization.pdf] for more information.",component kdc kerberos stack,['stacks'],AMBARI,New Feature,Major,2014-09-23 13:10:42,30
12741218,Create stack definitions for Bigtop 0.8,Create service definitions for Bigtop stack. The BT 0.8 stack will be a sibling to HDP and PHD stacks.,bigtop stack,[],AMBARI,Story,Major,2014-09-13 00:39:48,141
12741216,"Create different Maven profiles to build Bigtop, HDP and PHD stacks","Enhance the current maven pom files to build vendor specific Ambari rpms - one each for Bigtop, HDP and PHD 
Create different maven profiles for PHD, HDP and Bigtop such that each build has the ability to pick up a specified vendor stack definition and ignore others. Default must be BT

E.g.: mvn install -P BT // To build Bigtop stack",bigtop build maven,[],AMBARI,Bug,Major,2014-09-13 00:37:33,141
12741112,Use latest repo url in blueprints rather than the default,"Use latest repo url in blueprints rather than the default.

When a cluster is provisioned using blueprint, the repositories is not using the latest repo defined in repoinfo.xml for the blueprint stack.",blueprints,['ambari-server'],AMBARI,Bug,Major,2014-09-12 17:30:41,142
12740985,Detailed response in 'view cluster information' under Server API documentation,"Response example can be changed from 
{color:red}
{code}
    {
    	""href"" : ""http://your.ambari.server/api/v1/clusters/c1"",
      	""Clusters"" : {
        	""cluster_name"" : ""c1"",
        	""cluster_id"" : 1,
        	""version"" : ""HDP-1.2.0""
      	},
      	""services"" : [
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/NAGIOS"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""NAGIOS""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/HCATALOG"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""HCATALOG""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/PIG"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
         			""service_name"" : ""PIG""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/MAPREDUCE"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""MAPREDUCE""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/GANGLIA"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""GANGLIA""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/HIVE"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""HIVE""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/HDFS"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""MyIE9"",
          			""service_name"" : ""HDFS""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/ZOOKEEPER"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
         	 		""service_name"" : ""ZOOKEEPER""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/HBASE"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""HBASE""
          		}
        	},
        	{
        		""href"" : ""http://your.ambari.server/api/v1/clusters/c1/services/OOZIE"",
        		""ServiceInfo"" : {
          			""cluster_name"" : ""c1"",
          			""service_name"" : ""OOZIE""
          		}
        	} 
    	],
      ""hosts"" : [
        {
          ""href"" : ""http://your.ambari.server/api/v1/clusters/c1/hosts/some.host"",
          ""Hosts"" : {
              ""cluster_name"" : ""c1"",
              ""host_name"" : ""some.host""
          }
        },
        {
          ""href"" : ""http://your.ambari.server/api/v1/clusters/c1/hosts/another.host"",
          ""Hosts"" : {
              ""cluster_name"" : ""c1"",
              ""host_name"" : ""another.host""
          }
        }
      ]
    }
{code}
{color}
to
{color:green}
{code}
    {
    	""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001"",
    	""Clusters"" : {
    		""cluster_id"" : 9,
    		""cluster_name"" : ""cluster001"",
    		""health_report"" : {
    			""Host/stale_config"" : 1,
    			""Host/maintenance_state"" : 0,
    			""Host/host_state/HEALTHY"" : 3,
    			""Host/host_state/UNHEALTHY"" : 0,
    			""Host/host_state/HEARTBEAT_LOST"" : 0,
    			""Host/host_state/INIT"" : 0,
    			""Host/host_status/HEALTHY"" : 3,
    			""Host/host_status/UNHEALTHY"" : 0,
    			""Host/host_status/UNKNOWN"" : 0,
    			""Host/host_status/ALERT"" : 0
    		},
    		""provisioning_state"" : ""INIT"",
    		""total_hosts"" : 3,
    		""version"" : ""HDP-2.0"",
    		""desired_configs"" : {
    			""capacity-scheduler"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1408514705943""
    			},
    			""core-site"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1409806913314""
    			},
    			""global"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1409806913314""
    			},
    			""hdfs-log4j"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1""
    			},
    			""hdfs-site"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1407908591996""
    			},
    			""mapred-site"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1408514705943""
    			},
    			""mapreduce2-log4j"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1408514705943""
    			},
    			""yarn-log4j"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1408514705943""
    			},
    			""yarn-site"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1408514705943""
    			},
    			""zoo.cfg"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1""
    			},
    			""zookeeper-log4j"" : {
    				""user"" : ""admin"",
    				""tag"" : ""version1""
    			}
    		}
    	},
    	""alerts"" : {
    		""summary"" : {
    			""CRITICAL"" : 1,
    			""OK"" : 2,
    			""PASSIVE"" : 0,
    			""WARNING"" : 0
    		}
    	},
    	""requests"" : [
    		{
    			""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/requests/304"",
    			""Requests"" : {
    			""cluster_name"" : ""cluster001"",
    			""id"" : 304
    			}
    		},
    		{
    			""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/requests/305"",
    			""Requests"" : {
    			""cluster_name"" : ""cluster001"",
    			""id"" : 305
    			}
    		}
    		],
    	""services"" : [
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/services/GANGLIA"",
    		""ServiceInfo"" : {
    		""cluster_name"" : ""cluster001"",
    		""service_name"" : ""GANGLIA""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/services/HDFS"",
    		""ServiceInfo"" : {
    		""cluster_name"" : ""cluster001"",
    		""service_name"" : ""HDFS""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/services/MAPREDUCE2"",
    		""ServiceInfo"" : {
    		""cluster_name"" : ""cluster001"",
    		""service_name"" : ""MAPREDUCE2""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/services/ZOOKEEPER"",
    		""ServiceInfo"" : {
    		""cluster_name"" : ""cluster001"",
    		""service_name"" : ""ZOOKEEPER""
    		}
    	}
    	],
    	""config_groups"" : [
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/config_groups/2"",
    		""ConfigGroup"" : {
    		 ""cluster_name"" : ""cluster001"",
    		  ""id"" : 2
    		}
    	}
    	],
    	""workflows"" : [ ],
    	""hosts"" : [
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/hosts/host1.domain.com"",
    		""Hosts"" : {
    		  ""cluster_name"" : ""cluster001"",
    		  ""host_name"" : ""host1.domain.com""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/hosts/host2.domain.com"",
    		""Hosts"" : {
    		  ""cluster_name"" : ""cluster001"",
    		  ""host_name"" : ""host2.domain.com""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/hosts/host3.domain.com"",
    		""Hosts"" : {
    		  ""cluster_name"" : ""cluster001"",
    		  ""host_name"" : ""host3.domain.com""
    		}
    	}
    	],
    	""configurations"" : [
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/configurations?type=core-site&tag=version1"",
    		""tag"" : ""version1"",
    		""type"" : ""core-site"",
    		""Config"" : {
    		  ""cluster_name"" : ""cluster001""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/configurations?type=global&tag=version1"",
    		""tag"" : ""version1"",
    		""type"" : ""global"",
    		""Config"" : {
    		  ""cluster_name"" : ""cluster001""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/configurations?type=hdfs-site&tag=version1"",
    		""tag"" : ""version1"",
    		""type"" : ""hdfs-site"",
    		""Config"" : {
    		  ""cluster_name"" : ""cluster001""
    		}
    	},
    	{
    		""href"" : ""http://your.ambari.server/api/v1/clusters/cluster001/configurations?type=zoo.cfg&tag=version1"",
    		""tag"" : ""version1"",
    		""type"" : ""zoo.cfg"",
    		""Config"" : {
    		  ""cluster_name"" : ""cluster001""
    		}
    	},
    	]
    }
{code}
{color}
",documentation,['documentation'],AMBARI,Improvement,Minor,2014-09-12 07:38:02,143
12739906,Ambari Automated Kerberization,"*Problem*
Manually installing and setting up Kerberos for a secure Hadoop cluster is error prone, largely manual and a potential source of configuration problems. It requires many steps where configuration files and credentials may need to be distributed across many nodes.  Because of this the process is time consuming and lead to a high probability of user error.

The problem is exacerbated when the cluster is modified by adding or removing nodes and services.

*Solution*
Use Ambari to secure the cluster using Kerberos.  By automating the process of setting up Kerberos, the repetitive tasks of distributing configuration details and credentials can be done in parallel to the nodes within the cluster.  This also negates most user-related errors due to the lack of interaction a user has with the process.  

See [^AmbariClusterKerberization.pdf] for more details.",active-directory authentication kerberos mit-kerberos security stack,"['ambari-server', 'security', 'stacks']",AMBARI,Epic,Major,2014-09-08 16:44:08,30
12739893,Implement logic in Ambari to support common services.,Implement logic in Ambari to support common services. See design discussed in AMBARI-7201.,common-services stack,['stacks'],AMBARI,New Feature,Major,2014-09-08 15:28:41,41
12739890,Common Services,"*Problem*
The current implementation of the Ambari stack does not allow for a common set of services to be defined and reused in vendor-specific stack definitions. Therefore in order for the same service to be made available to different stacks, it’s definition must be copied or inherited from stack to stack.  

*Solution*
There needs to be a repository of services that exist outside the scope of any vendor-specific stack, but are accessible to vendor-specific stacks via service inheritance. This set of services should be known as common services and are to be maintained by the community to ensure that changes do not break vendor-specific services that inherit from them. 

See [^CommonStackServicesTechnicalDocument.pdf] for more information.",common-services service stack,['stacks'],AMBARI,Epic,Major,2014-09-08 15:13:58,41
12739498,Enable Explicit Stack Service Inheritance,"Enable stacks to inherit individual services with the same stack or across stacks.

When creating a new version of a service definition, unchanged details and scripts need to be copied from the previous version to the new version.  Any changes made to the resources that have been copied need to be propagated to them. This can be tedious and time consuming as well as lead to user error while copying the changes.  Currently, one version of a stack can extend another version of the same stack and will inherit everything in the stack including potentially unwanted content.  This work is to allow single services to be inherited/extended as opposed to the entire stack.
",stack,['stacks'],AMBARI,Epic,Major,2014-09-05 15:20:31,120
12738960,Selecting all services in 2.1.GlusterFS stack results in never ending dialogs,"While installing 2.1.GlusterFS stack, I selected all services (HDFS and GlusterFS). Hitting Next resulted in an endless loop of dialogs where, HDFS would automatically be deselected, and services would complain they need HDFS. (attached dialog screenshots)",patch,['ambari-web'],AMBARI,Bug,Major,2014-09-03 23:07:54,124
12738796,Remove HDFS dependency on GLUSTERFS stack via metainfo.xml,Remove HDFS dependency on GLUSTERFS stack via metainfo.xml - by adding proper requiredServices property in YARN metainfo.xml,patch,['ambari-server'],AMBARI,Bug,Major,2014-09-03 13:00:10,124
12738511,Views: CapScheduler service endpoint for operator,"- Expose endpoint for operator setting
- cleanup unused code
- up'd version to 0.2.0",view-capacity-scheduler,['contrib'],AMBARI,Improvement,Major,2014-09-02 19:38:58,125
12738249,ipc.server.tcpnodelay should be enabled by default,"ipc.server.tcpnodelay should be enabled by default (core-site.xml)

",HDP,['stacks'],AMBARI,Bug,Major,2014-09-01 11:51:07,5
12737822,Capacity Scheduler View cleanup,Various UI cleanup for CS view,view-capacity-scheduler,['ambari-views'],AMBARI,Improvement,Major,2014-08-29 15:44:19,125
12737623,Cannot view Ambari web interface with more than 48 CPU cores.,"I am running Ambari and the full hadoop stack in a virtual machine on my 64-core server for evaluation before I deploy it to the base OS. When I set it up with a 4-core virtual machine it worked perfectly, however after increasing the core count to 64 I could no longer log in to Ambari. The browser seems to be attempting to load the page but receiving no response. I can't see anything unusual in the logs. I found that it works with up to 48 cores, but no more. 
I have a second identical virtual machine running just the hadoop client software with 64 cores without problems, so this looks like a bug in the Ambari web interface. Has anyone else experienced this?",newbie,[],AMBARI,Bug,Major,2014-08-29 00:50:42,28
12736084,Setting maintenance mode should return request information,"Currently when you set maintenance mode, it spawns an asynchronous operation to update nagios, but it does not return the request information to the client.  The client needs an easy way to poll that request to know when it has completed.

{noformat}
PUT http://c6401.ambari.apache.org:8080/api/v1/clusters/testcluster/hosts
{""RequestInfo"": {""query"": ""Hosts/host_name.in(c6404.ambari.apache.org)"", ""context"": ""Start Maintenance Mode""}, ""Body"": {""Hosts"": {""maintenance_state"": ""ON""}}}
{noformat}

Response is a 200 with an empty response body.

Alternatively:

{noformat}
PUT http://c6401.ambari.apache.org:8080/api/v1/clusters/testcluster/hosts/c6404.ambari.apache.org/host_components?fields=HostRoles/state
{""RequestInfo"": {""context"": ""Start Maintenance Mode""}, ""Body"": {""HostRoles"": {""maintenance_state"": ""ON""}}}
{noformat}

Same response.  200 OK, no body.

",api,['ambari-server'],AMBARI,Bug,Major,2014-08-22 15:43:09,13
12734599,Unable to run Ambari setup script on Amazon AMI (AWS) { Fix included } ,"Running ambari-server on Amazon AMI fails with the error message shown below. 

----------------
[ec2-user@ip-xx-xx-xx-xx ~]$ sudo ambari-server setup
Using python  /usr/bin/python2.6
Setup ambari-server
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 53, in <module>
    OS_VERSION = OSCheck().get_os_major_version()
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_check.py"", line 155, in get_os_major_version
    return OSCheck.get_os_version().split('.')[0]
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_check.py"", line 146, in get_os_version
    raise Exception(""Cannot detect os version. Exiting..."")
Exception: Cannot detect os version. Exiting…
----------------

This is affected by the python bug - http://bugs.python.org/issue18872 

--- FIX --- 

Change line number 32 in function linux_distribution() 

FROM 

linux_distribution = platform.linux_distribution()

TO 

linux_distribution = platform.linux_distribution(supported_dists=['system'])

-- FINAL CODE -- 

…..
 26 def linux_distribution():
 27   PYTHON_VER = sys.version_info[0] * 10 + sys.version_info[1]
 28 
 29   if PYTHON_VER < 26:
 30     linux_distribution = platform.dist()
 31   else:
 32     linux_distribution = platform.linux_distribution(supported_dists=['system'])
 33 
 34   return linux_distribution
…..",amazon ami install,"['ambari-server', 'ambari-web']",AMBARI,Bug,Minor,2014-08-16 16:11:09,144
12732401,Include configuration in exported blueprint,"Include both host group and cluster scoped configuration in an exported blueprint.

To export a blueprint, use the api:
AMBARI_HOST:8080/api/v1/clusters/:clustername?format=blueprint

The exported blueprint will contain the entire configuration for the associated cluster.  The only properties which are not included are those that are marked as ""input required"" in the stack.  All passwords are marked as required so they will not be exported.  Also, any hostnames in the configuration properties are replaced with a hostgroup token: %HOSTGROUP::group1%.  When a cluster is provisioned from the blueprint, the hostgroup tokens are resolved to host names for the target cluster. ",api blueprints configuration,[],AMBARI,New Feature,Major,2014-08-06 21:39:11,120
12731004,Hosts emit : Host Role Invalid State,"At times hosts move to Invalid state and restart of components throw ""Host role in invalid state"". 
One way to fix this issue is to ssh into the hosts and run ambari-agent restart

http://hortonworks.com/community/forums/topic/host-role-in-invalid-state/

Ambari web interface being single point to manage cluster must provide option to restart ambari-agent. It looses its purpose if one has to manually ssh into the hosts.",ambari-client ambari-web,['ambari-web'],AMBARI,Bug,Critical,2014-07-31 07:11:50,145
12727332,HBase RegionServer -Xmn must be configurable,"-Xmn parameter for HBase RegionServer is calculated with hardcoded value.

This patch allows you to configure values used to calculate this parameter (the maximum value for -Xmn and the ratio between -Xmn and -Xmx).

Without this fix HBase can be crash because of OutOfMemory when you do a YCSB test for example.",features patch,[],AMBARI,Bug,Critical,2014-07-15 13:11:47,146
12723944,Python client bootstrap hosts is broken,"Because we do a string escape on the ssh key prior to doing a json encode on it, we get double-escaped characters.  This leads to a bunch of \\n characters in the key, causing ssh to reject the key and the bootstrap to fail.",newbie patch,['ambari-web'],AMBARI,Bug,Minor,2014-06-26 19:15:37,147
12723942,Python client caches curl flags between requests causing problems,"Currently if you use the python client, if you issue a DELETE request followed by a GET request, that GET request becomes a DELETE request.  You can imagine why that's undesirable.

The problem is that we set the CUSTOMREQUEST field on the DELETE, but we don't unset it on the next GET.  pycurl sees it still set and assumes we're still doing a DELETE.",newbie patch,['ambari-web'],AMBARI,Bug,Major,2014-06-26 19:12:37,147
12723676,Capacity Scheduler config default in Ambari should not have unfunded queue config,"This is the default config in Ambari deployed clusters. We should remove the line 'yarn.scheduler.capacity.root.unfunded.capacity=50'.

yarn.scheduler.capacity.maximum-am-resource-percent=0.2
yarn.scheduler.capacity.maximum-applications=10000
yarn.scheduler.capacity.node-locality-delay=40
yarn.scheduler.capacity.root.acl_administer_queues=*
yarn.scheduler.capacity.root.capacity=100
yarn.scheduler.capacity.root.default.acl_administer_jobs=*
yarn.scheduler.capacity.root.default.acl_submit_jobs=*
yarn.scheduler.capacity.root.default.capacity=100
yarn.scheduler.capacity.root.default.maximum-capacity=100
yarn.scheduler.capacity.root.default.state=RUNNING
yarn.scheduler.capacity.root.default.user-limit-factor=1
yarn.scheduler.capacity.root.queues=default
yarn.scheduler.capacity.root.unfunded.capacity=50",patch,[],AMBARI,Bug,Major,2014-06-25 16:44:21,41
12722579,validate_topology directive when registering blueprint doesn't work,"When registering a blueprint, topology validation is done by default.  
If a user want's to register a blueprint and skip topology validation this can be done by specifying a directive in the query string: 
blueprints/b1?validate_topology=false

This isn't working, no 400 topology validation error is returned to the user but the blueprint isn't created.

",blueprints,[],AMBARI,Bug,Major,2014-06-19 20:14:23,120
12722398,Fix sub-resource names in /stacks API,"The /stacks api uses sub-resource names such as stackServices and serviceComponents instead of services and components which are the names of the resources specified in the URL.  These incorrect resource names would need to be used in any queries for stack resources.

For example:
To get stack service named HDFS the URL would be:
api/v1/stacks/HDP/versions/2.1/services/HDFS

But, if we wanted to do a query of for HDFS services across all versions:
api/v1/stacks/HDP/versions?stackServices/StackServices/service_name=HDFS

Instead this should be:
api/v1/stacks/HDP/versions?services/StackServices/service_name=HDFS

Fix all sub-resource names that are returned and fix sub-resource names used in queries and partial response.",api stack,[],AMBARI,Bug,Major,2014-06-19 01:49:01,120
12721739,HBASE won't install in 2.1.GlusterFS stack,"2014-06-17 13:28:57,145 - Error while executing command 'install':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 105, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase_client.py"", line 30, in install
    self.configure(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase_client.py"", line 36, in configure
    hbase(name='client')
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase.py"", line 92, in hbase
    hbase_TemplateConfig( 'regionservers')
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase.py"", line 129, in hbase_TemplateConfig
    template_tag = tag
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 149, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 115, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/template_config.py"", line 42, in action_create
    content = Template(template_name, extra_imports=self.resource.extra_imports)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 149, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 115, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 96, in action_create
    content = self._get_content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 136, in _get_content
    return content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/source.py"", line 47, in __call__
    return self.get_content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/source.py"", line 126, in get_content
    rendered = self.template.render(self.context)
  File ""/usr/lib/python2.6/site-packages/jinja2/environment.py"", line 891, in render
    return self.environment.handle_exception(exc_info, True)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/templates/regionservers.j2"", line 19, in top-level template code
    {% for host in rs_hosts %}{{host}}
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 75, in __getattr__
    raise Fail(""Configuration parameter '""+self.name+""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'slave_hosts' was not found in configurations dictionary!

stdout:   /var/lib/ambari-agent/data/output-64.txt

2014-06-17 13:28:55,962 - Package['unzip'] {}
2014-06-17 13:28:56,007 - Skipping installing existent package unzip
2014-06-17 13:28:56,008 - Package['curl'] {}
2014-06-17 13:28:56,036 - Skipping installing existent package curl
2014-06-17 13:28:56,036 - Package['net-snmp-utils'] {}
2014-06-17 13:28:56,068 - Skipping installing existent package net-snmp-utils
2014-06-17 13:28:56,069 - Package['net-snmp'] {}
2014-06-17 13:28:56,104 - Skipping installing existent package net-snmp
2014-06-17 13:28:56,105 - Execute['mkdir -p /tmp/HDP-artifacts/ ;   curl -kf   --retry 10 http://hwx56.rhs:8080/resources//jdk-7u45-linux-x64.tar.gz -o /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz'] {'environment': ..., 'not_if': 'test -e /usr/jdk64/jdk1.7.0_45/bin/java', 'path': ['/bin', '/usr/bin/']}
2014-06-17 13:28:56,114 - Skipping Execute['mkdir -p /tmp/HDP-artifacts/ ;   curl -kf   --retry 10 http://hwx56.rhs:8080/resources//jdk-7u45-linux-x64.tar.gz -o /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz'] due to not_if
2014-06-17 13:28:56,115 - Execute['mkdir -p /usr/jdk64 ; cd /usr/jdk64 ; tar -xf /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz > /dev/null 2>&1'] {'not_if': 'test -e /usr/jdk64/jdk1.7.0_45/bin/java', 'path': ['/bin', '/usr/bin/']}
2014-06-17 13:28:56,123 - Skipping Execute['mkdir -p /usr/jdk64 ; cd /usr/jdk64 ; tar -xf /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz > /dev/null 2>&1'] due to not_if
2014-06-17 13:28:56,124 - Execute['mkdir -p /tmp/HDP-artifacts/;     curl -kf --retry 10     http://hwx56.rhs:8080/resources//UnlimitedJCEPolicyJDK7.zip -o /tmp/HDP-artifacts//UnlimitedJCEPolicyJDK7.zip'] {'environment': ..., 'not_if': 'test -e /tmp/HDP-artifacts//UnlimitedJCEPolicyJDK7.zip', 'ignore_failures': True, 'path': ['/bin', '/usr/bin/']}
2014-06-17 13:28:56,133 - Skipping Execute['mkdir -p /tmp/HDP-artifacts/;     curl -kf --retry 10     http://hwx56.rhs:8080/resources//UnlimitedJCEPolicyJDK7.zip -o /tmp/HDP-artifacts//UnlimitedJCEPolicyJDK7.zip'] due to not_if
2014-06-17 13:28:56,133 - Group['hadoop'] {}
2014-06-17 13:28:56,134 - Modifying group hadoop
2014-06-17 13:28:56,222 - Group['users'] {}
2014-06-17 13:28:56,223 - Modifying group users
2014-06-17 13:28:56,306 - Group['users'] {}
2014-06-17 13:28:56,307 - Modifying group users
2014-06-17 13:28:56,440 - User['ambari-qa'] {'gid': 'hadoop', 'groups': [u'users']}
2014-06-17 13:28:56,440 - Modifying user ambari-qa
2014-06-17 13:28:56,458 - File['/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2014-06-17 13:28:56,459 - Execute['/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 2>/dev/null'] {'not_if': 'test $(id -u ambari-qa) -gt 1000'}
2014-06-17 13:28:56,469 - Skipping Execute['/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 2>/dev/null'] due to not_if
2014-06-17 13:28:56,469 - User['hbase'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2014-06-17 13:28:56,469 - Modifying user hbase
2014-06-17 13:28:56,483 - File['/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2014-06-17 13:28:56,485 - Execute['/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/mnt/brick1/hadoop/hbase 2>/dev/null'] {'not_if': 'test $(id -u hbase) -gt 1000'}
2014-06-17 13:28:56,500 - Skipping Execute['/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/mnt/brick1/hadoop/hbase 2>/dev/null'] due to not_if
2014-06-17 13:28:56,500 - Group['nagios'] {}
2014-06-17 13:28:56,500 - Modifying group nagios
2014-06-17 13:28:56,606 - User['nagios'] {'gid': 'nagios'}
2014-06-17 13:28:56,607 - Modifying user nagios
2014-06-17 13:28:56,620 - User['oozie'] {'gid': 'hadoop'}
2014-06-17 13:28:56,621 - Modifying user oozie
2014-06-17 13:28:56,631 - User['hcat'] {'gid': 'hadoop'}
2014-06-17 13:28:56,631 - Modifying user hcat
2014-06-17 13:28:56,641 - User['hcat'] {'gid': 'hadoop'}
2014-06-17 13:28:56,641 - Modifying user hcat
2014-06-17 13:28:56,650 - User['hive'] {'gid': 'hadoop'}
2014-06-17 13:28:56,651 - Modifying user hive
2014-06-17 13:28:56,661 - User['yarn'] {'gid': 'hadoop'}
2014-06-17 13:28:56,661 - Modifying user yarn
2014-06-17 13:28:56,671 - Group['nobody'] {}
2014-06-17 13:28:56,671 - Modifying group nobody
2014-06-17 13:28:56,771 - Group['nobody'] {}
2014-06-17 13:28:56,771 - Modifying group nobody
2014-06-17 13:28:56,864 - User['nobody'] {'gid': 'hadoop', 'groups': [u'nobody']}
2014-06-17 13:28:56,865 - Modifying user nobody
2014-06-17 13:28:56,881 - User['nobody'] {'gid': 'hadoop', 'groups': [u'nobody']}
2014-06-17 13:28:56,881 - Modifying user nobody
2014-06-17 13:28:56,892 - User['hdfs'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2014-06-17 13:28:56,893 - Modifying user hdfs
2014-06-17 13:28:56,903 - User['mapred'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2014-06-17 13:28:56,903 - Modifying user mapred
2014-06-17 13:28:56,914 - User['zookeeper'] {'gid': 'hadoop'}
2014-06-17 13:28:56,914 - Modifying user zookeeper
2014-06-17 13:28:56,924 - User['storm'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2014-06-17 13:28:56,924 - Modifying user storm
2014-06-17 13:28:56,934 - User['falcon'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2014-06-17 13:28:56,935 - Modifying user falcon
2014-06-17 13:28:56,945 - User['tez'] {'gid': 'hadoop', 'groups': [u'users']}
2014-06-17 13:28:56,945 - Modifying user tez
2014-06-17 13:28:57,082 - Repository['HDP-2.1.GlusterFS'] {'action': ['create'], 'mirror_list': None, 'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.1.2.0', 'components': ['HDP', 'main'], 'repo_file_name': 'HDP'}
2014-06-17 13:28:57,087 - File['/etc/yum.repos.d/HDP.repo'] {'content': InlineTemplate(...)}
2014-06-17 13:28:57,088 - Repository['HDP-UTILS-1.1.0.17'] {'action': ['create'], 'mirror_list': None, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.17/repos/centos6', 'components': ['HDP', 'main'], 'repo_file_name': 'HDP-UTILS'}
2014-06-17 13:28:57,090 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': InlineTemplate(...)}
2014-06-17 13:28:57,090 - Package['hbase'] {}
2014-06-17 13:28:57,108 - Skipping installing existent package hbase
2014-06-17 13:28:57,122 - Directory['/etc/hbase/conf'] {'owner': 'hbase', 'group': 'hadoop', 'recursive': True}
2014-06-17 13:28:57,122 - Directory['/mnt/brick1/hadoop/hbase'] {'owner': 'hbase', 'recursive': True}
2014-06-17 13:28:57,122 - Directory['/mnt/brick1/hadoop/hbase/local/jars'] {'owner': 'hbase', 'group': 'hadoop', 'recursive': True, 'mode': 0775}
2014-06-17 13:28:57,123 - XmlConfig['hbase-site.xml'] {'owner': 'hbase', 'group': 'hadoop', 'conf_dir': '/etc/hbase/conf', 'configurations': ...}
2014-06-17 13:28:57,126 - Generating config: /etc/hbase/conf/hbase-site.xml
2014-06-17 13:28:57,127 - File['/etc/hbase/conf/hbase-site.xml'] {'owner': 'hbase', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None}
2014-06-17 13:28:57,127 - Writing File['/etc/hbase/conf/hbase-site.xml'] because contents don't match
2014-06-17 13:28:57,128 - XmlConfig['hdfs-site.xml'] {'owner': 'hbase', 'group': 'hadoop', 'conf_dir': '/etc/hbase/conf', 'configurations': ...}
2014-06-17 13:28:57,131 - Generating config: /etc/hbase/conf/hdfs-site.xml
2014-06-17 13:28:57,131 - File['/etc/hbase/conf/hdfs-site.xml'] {'owner': 'hbase', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None}
2014-06-17 13:28:57,131 - Writing File['/etc/hbase/conf/hdfs-site.xml'] because contents don't match
2014-06-17 13:28:57,132 - File['/etc/hbase/conf/hbase-policy.xml'] {'owner': 'hbase', 'group': 'hadoop'}
2014-06-17 13:28:57,132 - TemplateConfig['/etc/hbase/conf/hbase-env.sh'] {'owner': 'hbase', 'template_tag': None}
2014-06-17 13:28:57,138 - File['/etc/hbase/conf/hbase-env.sh'] {'content': Template('hbase-env.sh.j2'), 'owner': 'hbase', 'group': None, 'mode': None}
2014-06-17 13:28:57,139 - TemplateConfig['/etc/hbase/conf/hadoop-metrics2-hbase.properties'] {'owner': 'hbase', 'template_tag': 'GANGLIA-RS'}
2014-06-17 13:28:57,141 - File['/etc/hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties-GANGLIA-RS.j2'), 'owner': 'hbase', 'group': None, 'mode': None}
2014-06-17 13:28:57,141 - TemplateConfig['/etc/hbase/conf/regionservers'] {'owner': 'hbase', 'template_tag': None}
2014-06-17 13:28:57,143 - File['/etc/hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'hbase', 'group': None, 'mode': None}
2014-06-17 13:28:57,145 - Error while executing command 'install':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 105, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase_client.py"", line 30, in install
    self.configure(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase_client.py"", line 36, in configure
    hbase(name='client')
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase.py"", line 92, in hbase
    hbase_TemplateConfig( 'regionservers')
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/scripts/hbase.py"", line 129, in hbase_TemplateConfig
    template_tag = tag
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 149, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 115, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/template_config.py"", line 42, in action_create
    content = Template(template_name, extra_imports=self.resource.extra_imports)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 149, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 115, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 96, in action_create
    content = self._get_content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 136, in _get_content
    return content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/source.py"", line 47, in __call__
    return self.get_content()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/source.py"", line 126, in get_content
    rendered = self.template.render(self.context)
  File ""/usr/lib/python2.6/site-packages/jinja2/environment.py"", line 891, in render
    return self.environment.handle_exception(exc_info, True)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/HBASE/package/templates/regionservers.j2"", line 19, in top-level template code
    {% for host in rs_hosts %}{{host}}
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 75, in __getattr__
    raise Fail(""Configuration parameter '""+self.name+""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'slave_hosts' was not found in configurations dictionary!",glusterfs hbase,[],AMBARI,Bug,Major,2014-06-17 17:29:41,124
12721437,Blueprint validation fails when dependency is deployed to multiple host groups,"When a component has a specified dependency and the number of instances of the component in the blueprint > 1, validation fails.  An example of this is multiple ZK server instances and any component such as Nimbus being deployed which have a dependency on ZOOKEEPER_SERVER.  Because there are 3 ZK server instances, the dependency should be satisfied but validation fails.

An example BP:
{code}
{
  ""host_groups"": [
      {
        ""name"": ""host1"",
        ""cardinality"": ""1"",
        ""components"": [
          {
            ""name"": ""NAMENODE""
          },
          {
            ""name"": ""SECONDARY_NAMENODE""
          },
          {
            ""name"": ""ZOOKEEPER_SERVER""
          },
          {
            ""name"": ""RESOURCEMANAGER""
          },
          {
            ""name"": ""HISTORYSERVER""
          },
          {
            ""name"": ""HBASE_MASTER""
          },
          {
            ""name"": ""NAGIOS_SERVER""
          },
          {
            ""name"": ""DATANODE""
          },
          {
            ""name"": ""NODEMANAGER""
          },
          {
            ""name"": ""HBASE_REGIONSERVER""
          },
          {
            ""name"": ""GANGLIA_MONITOR""
          },
          {
            ""name"": ""SUPERVISOR""
          },
          {
            ""name"": ""HDFS_CLIENT""
          },
          {
            ""name"": ""YARN_CLIENT""
          },
          {
            ""name"": ""MAPREDUCE2_CLIENT""
          },
          {
            ""name"": ""TEZ_CLIENT""
          },
          {
            ""name"": ""HBASE_CLIENT""
          },
          {
            ""name"": ""ZOOKEEPER_CLIENT""
          }
        ]
      },
      {
        ""name"": ""host2"",
        ""cardinality"": ""1"",
        ""components"": [
          {
            ""name"": ""ZOOKEEPER_SERVER""
          },
          {
            ""name"": ""GANGLIA_SERVER""
          },
          {
            ""name"": ""NIMBUS""
          },
          {
            ""name"": ""DRPC_SERVER""
          },
          {
            ""name"": ""STORM_UI_SERVER""
          },
          {
            ""name"": ""STORM_REST_API""
          },
          {
            ""name"": ""DATANODE""
          },
          {
            ""name"": ""NODEMANAGER""
          },
          {
            ""name"": ""HBASE_REGIONSERVER""
          },
          {
            ""name"": ""GANGLIA_MONITOR""
          },
          {
            ""name"": ""SUPERVISOR""
          },
          {
            ""name"": ""HDFS_CLIENT""
          },
          {
            ""name"": ""YARN_CLIENT""
          },
          {
            ""name"": ""MAPREDUCE2_CLIENT""
          },
          {
            ""name"": ""TEZ_CLIENT""
          },
          {
            ""name"": ""HBASE_CLIENT""
          },
          {
            ""name"": ""ZOOKEEPER_CLIENT""
          }
        ]
      },
      {
        ""name"": ""other_hosts"",
        ""cardinality"": ""1"",
        ""components"": [
          {
            ""name"": ""ZOOKEEPER_SERVER""
          },
          {
            ""name"": ""DATANODE""
          },
          {
            ""name"": ""NODEMANAGER""
          },
          {
            ""name"": ""HBASE_REGIONSERVER""
          },
          {
            ""name"": ""GANGLIA_MONITOR""
          },
          {
            ""name"": ""SUPERVISOR""
          },
          {
            ""name"": ""HDFS_CLIENT""
          },
          {
            ""name"": ""YARN_CLIENT""
          },
          {
            ""name"": ""MAPREDUCE2_CLIENT""
          },
          {
            ""name"": ""TEZ_CLIENT""
          },
          {
            ""name"": ""HBASE_CLIENT""
          },
          {
            ""name"": ""ZOOKEEPER_CLIENT""
          }
        ]
      }
    ],
    ""Blueprints"": {
      ""stack_name"": ""HDP"",
      ""stack_version"": ""2.1""
    }
}
{code}

Attempting to register this blueprint results in:
{code}
{
  ""status"" : 400,
  ""message"" : ""Cluster Topology validation failed.  Unresolved component dependencies: {host2={NIMBUS=[DependencyInfo[name=ZOOKEEPER/ZOOKEEPER_SERVER, scope=cluster, auto-deploy=true]]}}.  To disable topology validation and create the blueprint, add the following to the end of the url: '?validate_topology=false'""
}
{code}",blueprints,[],AMBARI,Bug,Major,2014-06-16 15:45:54,120
12720627,Add Gluster 2.1 Support for HDP,Implement 2.1.GlusterFS stack for version 1.6.1 and above.,2.1.GlusterFS,['infra'],AMBARI,New Feature,Major,2014-06-11 16:55:36,124
12718690,Blueprints don't support cluster creation when using existing DB,"When deploying a cluster via a blueprint where the MYSQL_SERVER component is not included in the topology but instead already exists, a 400 response is returned to the user even if the user properly configured the necessary configurations for an existing server.

{ 
""status"" : 400, ""message"" : ""Unable to update configuration properties with topology information. Component 'MYSQL_SERVER' is not mapped to any host group or is mapped to multiple groups."" 
}

This occurs when deploying the cluster when the configurations are updated with topology related information. For components that may be external, we need to check if the user updated the default hostname or port in the corresponding configuration and if they have, don't do any topology update on the property.",blueprints,[],AMBARI,Bug,Major,2014-06-05 17:51:04,120
12718679,ZK should not be required to be restarted after adding a host,"Zookeeper indicates that it needs restart after you do an AddHost. The new host added does not have Zookeeper and even if it did its unlikely that ZK requires restart when you do add host.
The requires restart logic for ZK should only be limited to delete host and only if the host has ZK server component.",patch,[],AMBARI,Bug,Critical,2014-06-05 17:18:17,128
12717659,startRrdcached.sh su command broken for users with no shell,"Line 39 of startRrdcached.sh will not work properly if the GMETAD_USER has its shell set to /sbin/nologin. This can be resolved in one of two ways:

1) remove the - after the su command on line 39
2) add ""-s /bin/bash"" after the GMETAD_USER variable on line 39",easyfix patch,[],AMBARI,Bug,Minor,2014-05-30 21:46:18,148
12714815,"When provisioning cluster via a blueprint, password property validation may not be complete","There are two issues here.

- Due to an earlier jira, nagios_web_password had a hard coded default since it wasn't yet specified in the stack.

- If multiple passwords are required in different configurations for the same host group, not all missing passwords are reported to the user

For the first issue the solution is to simply not provide a default value for the nagios_web_password.

For the second issue, needed to fix code that added missing properties to map so that they aren't overridden.",api blueprints,[],AMBARI,Bug,Major,2014-05-16 17:30:13,120
12713393,IllegalStateException when provisioning cluster via blueprint during password validation,"After submitting cluster create with a default password as well as some passwords specified in BP, a 500 response is returned to the user as a result of an IllegalStateException during password validation.
13:55:48,112 ERROR qtp752317744-20 BaseManagementHandler:62 - Caught a runtime exception while attempting to create a resource
java.lang.IllegalStateException
at java.util.HashMap$HashIterator.remove(HashMap.java:942)
at org.apache.ambari.server.controller.internal.ClusterResourceProvider.validatePasswordProperties(ClusterResourceProvider.java:379)
at org.apache.ambari.server.controller.internal.ClusterResourceProvider.processBlueprintCreate(ClusterResourceProvider.java:333)
at org.apache.ambari.server.controller.internal.ClusterResourceProvider.createResources(ClusterResourceProvider.java:148)
at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:218)
at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75)
at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:36)
at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:126)
at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:80)
at org.apache.ambari.server.api.services.ClusterService.createCluster(ClusterService.java:82)
",blueprints,[],AMBARI,Bug,Critical,2014-05-09 21:37:40,120
12712953,Provide basic validation for cluster create via a blueprint,"Do simple validation for cluster create with blueprint.

Validate the following:
- invalid blueprint name
- no host_groups field
- host group without name
- host group without hosts
- host group hosts without fqdn",api blueprints,[],AMBARI,Improvement,Major,2014-05-07 21:42:16,120
12712665,Validate required fields including passwords for blueprint cluster creation,"For blueprint creation, validate all non-password required properties have been  set in the blueprint.

For cluster creation via a blueprint, validate that all required password properties have been set in configuration or that a 'default_password' property has been included in the request.  Password properties can be set in the blueprint cluster or host group configurations or as part of the cluster create call as either cluster or host group properties.

",api blueprints,[],AMBARI,Improvement,Major,2014-05-06 20:23:07,120
12711650,Provide basic validation of fields for blueprint create api,"This task is for non topology related validation of blueprint create api calls.
The following validations are included:

- no blueprint name
- no host groups
- no host group name
- no host group components
- component specified with no name
- invalid component name for stack
- no/invalid stack name
- no/invalid stack version

Validation failures result in a 400 response with a descriptive message of the problem.",api blueprints,[],AMBARI,Improvement,Major,2014-05-01 01:59:34,120
12711190,Validate that a host is only associated with a single host group when provisioning via a blueprint,"When provisioning a cluster via a blueprint, there currently is a restriction that a host may only be associated with a single host group.  If a user attempts to map a host to multiple host groups, strange errors are reported back to the user which do not explain the problem.

This work will validate that a host is only mapped to one host group early and will provide a good error message to the user if there is an attempt to map multiple host groups to a host.

{
  ""status"" : 400,
  ""message"" : ""A host may only be mapped to a single host group at this time.  The following hosts are mapped to more than one host group: [john2.novalocal]""
} ",api blueprints,[],AMBARI,Improvement,Major,2014-04-29 14:53:34,120
12710973,Ambari REST API: Rename host-groups in cluster create to host_groups for consistency across api,"The host group field name is inconsistent across the blueprint and the cluster create body.
In blueprints it is ""host_groups"" and in the cluster create call it is ""host-groups"".
Change the field name in the cluster create call to ""host_groups"" as the rest of the api uses underscore.

{code}
{
  ""blueprint"" : ""single-node-test"",
  ""host_groups"" :[
    { 
      ""name"" : ""host_group_1"",  
      ""hosts"" : [          
        { 
          ""fqdn"" : ""test.novalocal""
        }
      ]
    }
  ]
}
{code}
",api blueprints,[],AMBARI,Improvement,Major,2014-04-28 17:17:21,120
12710623,"Rename stacks resources stackServices, serviceComponents and operatingSystems","Existing names are not consistent with the remaining api which uses an '_' instead of camel case. 
Also these names are unnecessarily long.  Since these names are not under the cluster resource, it is ok to reuse the names service and component for a more concise url.
So, the new stack api hierarchy would be:
stacks/versions/services/components/...
stacks/versions/operating_systems/...

Note This will not affect the stacks2 api that the UI uses.",api blueprints stack,[],AMBARI,Task,Major,2014-04-25 19:14:17,120
12710178,Add dependency related information to stacks in the REST API,"Added dependency related information to stacks in the REST API.  This information describes dependencies between components which are part of a stack.  

A new dependencies resources will be added as a child to the stacksServices/serviceComponent (soon to be renamed to services/component) resource.

For example:
GET http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER?fields=dependencies/*

{code}
{
  ""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER?fields=dependencies/*"",
  ""StackServiceComponents"" : {
    ""component_name"" : ""HBASE_MASTER"",
    ""service_name"" : ""HBASE"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  },
  ""dependencies"" : [
    {
      ""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER/dependencies/HDFS_CLIENT"",
      ""Dependencies"" : {
        ""component_name"" : ""HDFS_CLIENT"",
        ""dependent_component_name"" : ""HBASE_MASTER"",
        ""dependent_service_name"" : ""HBASE"",
        ""scope"" : ""host"",
        ""service_name"" : ""HDFS"",
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""1.3.3""
      },
      ""auto_deploy"" : {
        ""enabled"" : true
      }
    },
    {
      ""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER/dependencies/ZOOKEEPER_SERVER"",
      ""Dependencies"" : {
        ""component_name"" : ""ZOOKEEPER_SERVER"",
        ""dependent_component_name"" : ""HBASE_MASTER"",
        ""dependent_service_name"" : ""HBASE"",
        ""scope"" : ""cluster"",
        ""service_name"" : ""ZOOKEEPER"",
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""1.3.3""
      },
      ""auto_deploy"" : {
        ""enabled"" : true,
        ""location"" : ""HBASE/HBASE_MASTER""
      }
    }
  ]
}
{code}

",API stack,[],AMBARI,New Feature,Major,2014-04-23 22:51:21,120
12708301,Allow host group scoped configuration to be specified in Blueprint,"Allow configuration to be specified for host groups.  This configuration will provide overrides for all hosts assigned to the host group.  This configuration is specified inline within the host group. 

An example of a simple one host group blueprint with both cluster scoped and host group scoped configuration.

{code}
{
  ""configurations"" : [
    {
      ""core-site"" : {
        ""fs.trash.interval"" : ""480"",
        ""ipc.client.idlethreshold"" : ""8500"",
        ""my.awesome.property"" : ""excellent""
      }
    },
    {
      ""mapred-site"" : {
        ""tasktracker.http.threads"" : ""45""
      }
    }
  ],
  ""host_groups"" : [
    {
      ""name"" : ""host_group_1"",
      ""configurations"" : [
        {
          ""core-site"" : {
            ""fs.trash.interval"" : ""475""
          }
        }
      ],     
      ""components"" : [
        {
          ""name"" : ""HDFS_CLIENT""
        },
        {
          ""name"" : ""GANGLIA_SERVER""
        },
        {
          ""name"" : ""AMBARI_SERVER""
        },
        {
          ""name"" : ""MAPREDUCE_CLIENT""
        },
        {
          ""name"" : ""GANGLIA_MONITOR""
        },
        {
          ""name"" : ""DATANODE""
        },
        {
          ""name"" : ""NAMENODE""
        },
        {
          ""name"" : ""JOBTRACKER""
        },
        {
          ""name"" : ""HISTORYSERVER""
        },
        {
          ""name"" : ""SECONDARY_NAMENODE""
        },
        {
          ""name"" : ""NAGIOS_SERVER""
        },
        {
          ""name"" : ""TASKTRACKER""
        }
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""blueprint_name"" : ""single-node-test"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  }
}
{code} 

*Note:* Allowing service configurations which are specified external to the host group definition to allow for configuration reuse across host groups will be handled in a subsequent Jira.

 ",api blueprints,[],AMBARI,New Feature,Major,2014-04-14 16:15:31,120
12706853,Oozie directory on HDFS with incorrect owner,"From ambari 1.4.4-1.5.1, there is a puppet missing for Oozie server start, may cause oozie server startup error. Oozie server will not start.

The problem is , when oozie installed, the owner of  oozie's directory on HDFS (/user/oozie) is not oozie:hdfs, so puppet cannot copy any thing to /user/oozie and the the oozie server start process hangs up.",ambari-agent,['ambari-agent'],AMBARI,Bug,Critical,2014-04-06 16:46:49,149
12705582,Web UI does not work when CPUs > 64,"Ambari Web UI threads blocked when Server CPUs > 64, So the web ui will not response any thing till the browser timeout. In ambari-server.log it will logging a ""Insufficient Threads configuration"" It's should be a Jetty container bug.

So I change the ambari-server/src/main/org/apache/ambari/server/controller/AmbariServer.java

      serverForAgent.setThreadPool(new QueuedThreadPool(25));
      server.setThreadPool(new QueuedThreadPool(25));

to 

      serverForAgent.setThreadPool(new QueuedThreadPool(65));
      server.setThreadPool(new QueuedThreadPool(65));",patch performance,['ambari-server'],AMBARI,Improvement,Minor,2014-03-31 05:01:24,33
12704376,Allow custom configuration properties to be specified in blueprint,"Allow a user to specify custom properties in a blueprint. The UI currently allows custom properties to be set and these will eventually be exported to a blueprint.

Currently, if a property in a blueprint isn't specified in the stack configurations, it is ignored.",blueprints,[],AMBARI,Improvement,Major,2014-03-28 21:36:01,120
12703496,proxy user settings need to be set when provisioning via a blueprint,"In core-site, there are a bunch of proxyuser settings.

These are in the form of :
hadoop.proxyuser.<username>.hosts
hadoop.proxyuser.<username>.groups

These are dynamically generated property names because they contain the user name in the property name.
These are currently not present in the stack definitions and are generated by the UI.

These are currently not being set when provisioning via a blueprint.

Until this is completed, users that deploy clusters that require proxy user properties will have to add them to the blueprint manually.
For example:
{code}
{
    ""configurations"" : [
        { 
            ""core-site"" : {
               ""hadoop.proxyuser.oozie.hosts"" : ""*"",
               ""hadoop.proxyuser.oozie.groups"" : ""users"" ,
               ...
            } 
        }
    ]
    ...
}   
{code} 

For more on cluster scoped blueprint configuration see:
https://issues.apache.org/jira/browse/AMBARI-5114
",blueprints,[],AMBARI,Task,Major,2014-03-25 15:47:18,120
12703266,Improve User Experience during install - iptables warning,"improve the HostInfo.py (Host Check) during the ambari installer process, particularly for step 3 of the process where the nodes are registered and confirmed.  In particular this JIRA will address the erroneous iptables check that currently always returns a value of 0 which indicates in the script that iptables is running and active, even when the iptables have actually been flushed.  This results in an erroneous warning message.",Ambari HostCheck,['ambari-agent'],AMBARI,Improvement,Major,2014-03-24 18:19:14,124
12700827,Ambari Repo URL validator rejecting valid yum repo file:/// URL.,"We have a centralized YUM repos on a NFS server. Our yum repo  URLS look like the following:

file:///net/server1/export/courserepos/HortonWorks/HDP/centos6/2.x/updates/2.0.6.1/

Where /net/server1/export is accessed via autofs on the clients.

This is a valid, and more efficient and faster way to do yum repos in a datacenter as it does install-in-place instead of the download-to-yum-cache-and-then-install method that http yum repos use.

Please fix ambari to work with these type of valid yum repo URLs without resorting to clicking the ""skip repository base url validation"" box.

Here is the warning generated:

15:08:38,386  WARN [qtp1449984133-22] ServletHandler:514 - /api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat6/repositories/HDP-2.0.6
java.lang.ClassCastException: sun.net.www.protocol.file.FileURLConnection cannot be cast to java.net.HttpURLConnection
	at org.apache.ambari.server.controller.internal.URLStreamProvider.readFrom(URLStreamProvider.java:84)
	at org.apache.ambari.server.controller.internal.URLStreamProvider.readFrom(URLStreamProvider.java:136)
	at org.apache.ambari.server.controller.AmbariManagementControllerImpl.updateRespositories(AmbariManagementControllerImpl.java:2169)
	at org.apache.ambari.server.controller.internal.RepositoryResourceProvider$1.invoke(RepositoryResourceProvider.java:106)
	at org.apache.ambari.server.controller.internal.RepositoryResourceProvider$1.invoke(RepositoryResourceProvider.java:103)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:295)
	at org.apache.ambari.server.controller.internal.RepositoryResourceProvider.updateResources(RepositoryResourceProvider.java:103)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:239)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.update(PersistenceManagerImpl.java:99)
	at org.apache.ambari.server.api.handlers.UpdateHandler.persist(UpdateHandler.java:42)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:69)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:108)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:83)
	at org.apache.ambari.server.api.services.StacksService.updateRepository(StacksService.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:708)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1329)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:48)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:363)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:931)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:992)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:856)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:662)


This",easyfix,[],AMBARI,Bug,Major,2014-03-11 21:21:49,150
12695131,Support changing the cluster name of an existing hadoop cluster,"Ambari should support the ability to change the cluster name.

One use case is the desire to relocate your cluster without removing the data. Perhaps there is a network conflict at the new location - it should be straightforward to change the cluster name via Ambari.
",features,['ambari-agent'],AMBARI,New Feature,Critical,2014-02-14 02:14:44,151
12693139,Host registering failure from primary/agent os checking on centos6,"I am using Ambari (1.4.3.38) for hadoop cluster installation and management. All the cluster nodes are built on centos 6.0.

During the ambari server installation, ambari-server recognized the primary/cluster os as redhat6 (see ambari.properties). 
During the ambari agent bootstrap/host register, ambari-agent regonized the agent os as centos linux6 (see log). 

From log files (ambari-server.log, ambari-agent.log), I found the inconsistence caused the warning of ambari-agent bootstrapping and failure of host registering.

I'm still not sure why this happen, but I guess it's caused by the differene of os checking methods among ambari server side code, ambari-agent bootstrap script (os_type_check.sh,based on os release file) and registering script (Controller.py/Register.py based on os hardware profile) .

I just share to see if anyone can fix the issue.

BTW, for me, to solve the problem, I manually edited the script files to make it work temporarily:

To avoid warning of agent bootstrapping, in os_type_check.sh, add current_os=$RH6 above the echo line or add res=0 after case statement;
To make the node register work, in Controller.py, add data=data.replace('centos linux','redhat') before sending registering request;

Thanks.",patch,['ambari-agent'],AMBARI,Bug,Major,2014-02-04 16:42:03,5
12692203,Create new /blueprints REST endpoint,"Create a new /blueprints REST endpoint. This endpoint represents an 'abstract blueprint' or 'template' and doesn't contain cluster specific details such specific host information.

This initial jira will be limited to basic blueprint information and will not contain configuration elements.  These additional elements will be added in subsequent patches.

Available operations are get, create and delete.  Update is not supported because blueprints are immutable.

Example of a simple blueprint resource:
{code}
{
  ""href"" : ""http://172.18.193.10:8080/api/v1/blueprints/bp1"",
  ""host_groups"" : [
    {
      ""name"" : ""foo"",
      ""components"" : [
        {
          ""name"" : ""component2""
        },
        {
          ""name"" : ""component1""
        },
        {
          ""name"" : ""component4""
        },
        {
          ""name"" : ""component3""
        }
      ],
      ""cardinality"" : ""2""
    },
    {
      ""name"" : ""bar"",
      ""components"" : [
        {
          ""name"" : ""component5""
        }
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""blueprint_name"" : ""bp1"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  }
}
{code}



",api-addition blueprints,[],AMBARI,New Feature,Major,2014-01-30 01:55:08,120
12672941,Enable Security fails when ambari setup is rerun to set java_home to oracle jdk7  ,"* Run ambari-setup -s
* Install jdk7 at /usr/jdk1.7.0_40
* Run ambari-setup -j /usr/jdk1.7.0_40
* Install/usr/jdk1.7.0_40 on all hosts at JAVA_HOME path specified in above step
* Install jce-7 policy on all hosts and unzip it at /usr/jdk1.7.0_40/jre/lib/security.
* Go through installation wizard and Enable security.
* Enable security fails as ambari override all manually downloaded and unzipped jce-7 policy with jce-6 policy.",security,['ambari-agent'],AMBARI,Bug,Critical,2013-10-09 01:34:38,48
12672903,Ambari should start the HBase Thrift Server,Ambari doesn't start the HBase thrift server.  Clients like Hue require the thrift server for comms.,hbase thrift,['ambari-agent'],AMBARI,Improvement,Major,2013-10-08 21:43:50,152
12669527,"Security wizard: On NameNode HA mode, General category should have spnego principal and keytab field ","Earlier dfs.web.authentication.kerberos.keytab field was being used for NameNode and SNameNode component. So we planned to pull this key to NameNode category when HA is enabled as it's the only component then using the key.

After HDFS-5091 fix, journalNode also uses this key.

So instead of pulling this config key in NameNode section, it should be kept in General category and the description of this principal and keytab location field should be changed accordingly.",security,['ambari-web'],AMBARI,Bug,Major,2013-09-19 19:52:30,48
12669083,Ambari-client shell,"support Ambari-shell
Support a python shell to exceute amabri-client api with simple commandline commands

ambari>help
show_all_services
show_all_hosts",client,['ambari-web'],AMBARI,New Feature,Major,2013-09-17 19:03:13,147
12669081,Ambar-client updates for JIRA-3201,"Some issue with the patch given in JIRA-3201

add testcase for get_hosts , remove create_hosts/create_hosts as of now.
",client,['ambari-web'],AMBARI,Improvement,Minor,2013-09-17 19:00:10,5
12669055,"Security Wizard: Duplicate field for ""Path to keytab file"" on YARN tab (NodeManager section)","2 fields for ""Path to keytab file"" on YARN tab (NodeManager section)

In other sections (ResourceManager, Job History Server, etc.) those fields are called:
-Path to keytab file
-Path to spnego keytab file",security,['ambari-web'],AMBARI,Bug,Major,2013-09-17 17:31:47,48
12667899,Support client's ability to introspect a HDP cluster incase Ambari/postgres are removed,"Support python-client's ability to introspect a HDP cluster incase Ambari-DB tables are removed
and based on the ClusterModel/ServiceModel/HostModel repopulate the DB to a extent where the state of Ambari is restored.
This also could be useful when we a have old cluster to manage with Ambari.",client,['ambari-web'],AMBARI,Improvement,Major,2013-09-10 19:54:39,147
12667892,Support adding users in client,"adding of users via client
client.add_user(UserModel)",client,['ambari-web'],AMBARI,Improvement,Minor,2013-09-10 19:49:57,147
12667870,"Support client's ability to stream server logs, also request logs",stream logs for different commands/actions,client,['ambari-web'],AMBARI,Improvement,Minor,2013-09-10 18:41:22,147
12667869,Support client's ability to interact with ambari-agent,"support start/stop agent from client
change configurations
stream agent logs",client,['ambari-web'],AMBARI,Improvement,Minor,2013-09-10 18:40:12,147
12667861,Create new stack with Gluster support for 1.3.2 HDP version,"This feature extends the ability for Ambari to support an Hadoop Compatible File System outside of HDFS.  For this stack definition we are introducing the use of GlusterFS, but it provides a good road map for other Hadoop Compatible File Systems to also be able to leverage Ambari. The feature/patch does not remove the HDFS patch, but simply provides an alternative when selecting the services within the installer.",patch,[],AMBARI,New Feature,Major,2013-09-10 17:50:05,130
12667732,Secure cluster: Yarn service check fails after configuring yarn for spnego authentication.,Yarn smoke test uses REST api exposed by ResourceManager to get its status. After configuring web authentication yarn client that is assigned yarn service check needs to negotiate 401 HTTP authentication response received while using REST api.,security,['ambari-agent'],AMBARI,Bug,Major,2013-09-10 03:20:28,48
12667141,Security wizard: disabling security does not return to initial condition after enabling security fails.,"*Steps to reproduce:*
1. enable security WITHOUT pre-configuring kerberos on cluster and see failures on ""3. Start Services"";
2. disable security.
In the end DataNode fails on ALL hosts without a possibility to get started.

When you try to start DataNode manually it also ends with error:
{code}err: /Stage[2]/Hdp-hadoop::Datanode/Hdp-hadoop::Service[datanode]/Hdp::Exec[su - hdfs -c  'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec && /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/Exec[su - hdfs -c  'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec && /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/returns: change from notrun to 0 failed: su - hdfs -c  'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec && /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479{code}
",security,['ambari-agent'],AMBARI,Bug,Major,2013-09-05 17:03:04,48
12666967,dfs.journalnode.edits.dir value in the stack should be empty,"Property name: dfs.journalnode.edits.dir
Current Default Value: /grid/0/hdfs/journal
Stacks: HDP-2.0.5 & HDPLocal-2.0.5

This should be empty value, UI can assign the right dir path.",newbie,['ambari-server'],AMBARI,Bug,Major,2013-09-04 20:24:08,48
12666804,Enable Security wizard: Datanode start fails on HDP1.x stack.,For HDP-2.x we need to specify JSVC_HOME path. We don't need this for HDP-1.x stack. We are currently explicitly specifying a path for JSVC_HOME in hadoop-env.sh which doesn't exist on the host with HDP-1.x stack.,security,['ambari-agent'],AMBARI,Task,Major,2013-09-04 00:35:27,48
12666708,Amabri Client refactoring -2,"1)	The last patch had an issue with the testcase because of which it did not compile. I might have accidently added a  line while I was saving some of the files.(this is fixed in the new patch)
The trunk currently fails for ambari-client.


https://cwiki.apache.org/confluence/display/AMBARI/Ambari+python+Client 
has all the exposed methods

",client,['ambari-web'],AMBARI,Improvement,Major,2013-09-03 16:01:52,147
12665287,Service metrics call latency increases by a factor of 2 or 3 every 20-30 calls.,"Every 20-30 calls for service metrics, latency of the next 5 calls are a multiple of 2 or 3. This might be due to garbage collection. But basically in UI, the updates seem to take longer. 

The call UI makes is
{noformat}
/api/v1/clusters/${cluster}/services?fields=components/ServiceComponentInfo,components/host_components,components/host_components/HostRoles,components/host_components/metrics/jvm/memHeapUsedM,components/host_components/metrics/jvm/memHeapCommittedM,components/host_components/metrics/mapred/jobtracker/trackers_decommissioned,components/host_components/metrics/cpu/cpu_wio,components/host_components/metrics/rpc/RpcQueueTime_avg_time,components/host_components/metrics/flume/flume,components/host_components/metrics/yarn/Queue
{noformat}",performance,['ambari-web'],AMBARI,Bug,Major,2013-08-23 18:42:10,6
12665277,Powering off RM node increases API latency by a factor of 6,"On a 4 node cluster I was testing the below API call.
{noformat}
/api/v1/clusters/${cluster}/services?fields=components/ServiceComponentInfo,components/host_components,components/host_components/HostRoles,components/host_components/metrics/jvm/memHeapUsedM,components/host_components/metrics/jvm/memHeapCommittedM,components/host_components/metrics/mapred/jobtracker/trackers_decommissioned,components/host_components/metrics/cpu/cpu_wio,components/host_components/metrics/rpc/RpcQueueTime_avg_time,components/host_components/metrics/flume/flume,components/host_components/metrics/yarn/Queue
{noformat}

When everything was working the latency was ~500ms. 
I then powered off the RM node, and immediately the call latency spiked by 30 times (~15000ms) . After some time, it reduced, but still was 6 times the original latency (~3000ms). When the machine came back online, the call again fell back to its original ~500ms latency.

Images attached.",perfomance,['ambari-server'],AMBARI,Bug,Major,2013-08-23 18:03:21,153
12664951,"RPM has direct query mode, bootstrap.py should use it instead of needless pipe to grep","Here is my patch

From: Dax Kelson <dkelson@gurulabs.com>
Date: Wed, 21 Aug 2013 17:59:15 -0600
Subject: [PATCH] Improved RPM query for sudo

No need to run ""rpm -qa | grep sudo"" when
""rpm -q sudo"" works much faster and efficiently.

$ time rpm -qa | grep sudo
sudo-1.8.6p3-7.el6.x86_64

real	0m0.537s
user	0m0.323s
sys	0m0.161s

$ time rpm -q sudo
sudo-1.8.6p3-7.el6.x86_64

real	0m0.010s
user	0m0.009s
sys	0m0.000s
---
 ambari-server/src/main/python/bootstrap.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/ambari-server/src/main/python/bootstrap.py b/ambari-server/src/main/python/bootstrap.py
index f83739b..b4bcdd2 100755
--- a/ambari-server/src/main/python/bootstrap.py
+++ b/ambari-server/src/main/python/bootstrap.py
@@ -549,7 +549,7 @@ class BootStrap:
   def checkSudoPackage(self):
     try:
       """""" Checking 'sudo' package on remote hosts """"""
-      command = ""rpm -qa | grep sudo""
+      command = ""rpm -q sudo""
       pssh = PSSH(self.successive_hostlist, self.user, self.sshkeyFile, self.bootdir,\
                   errorMessage=""Error: Sudo command is not available. Please install the sudo command."",\
                   command=command)
-- 
1.8.3.1
",patch,[],AMBARI,Improvement,Minor,2013-08-22 00:08:12,154
12662429,Utility script to generate keytabs is broken,keytab Tar for each host is packaged including hostname. Untaring it on a host creates path starting with <hostanme>/<actual path>. Fix is to package the content inside the hostname directory excluding the hostname directory itself.,security,['ambari-server'],AMBARI,Bug,Major,2013-08-07 17:06:15,48
12661923,Can't add from UI some queues in capacity-scheduler.xml,"Properties of capacity-scheduler.xml are truncated by "","". It make impossible to create multiple queues, please see http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html",javascript,['ambari-web'],AMBARI,Bug,Major,2013-08-05 17:26:14,155
12658424,"Webhcat property ""templeton.hive.properties"" value in webhcat-site.xml is defined incorrect.","In a Ambari installed cluster, the property ""templeton.hive.properties"" in /etc/hcatalog/conf/webhcat-site.xml is set as:
{code}
<property>
    <name>templeton.hive.properties</name>
    <value>hive.metastore.local=false, hive.metastore.uris=thrift://ip-10-68-113-33.ec2.internal:9083, hive.metastore.sasl.enabled=true,hive.metastore.execute.setugi=true, hive.exec.mode.local.auto=false, hive.metastore.kerberos.principal=hive/_HOST@EXAMPLE.COM</value>
</property>
{code}
The properties are delimited by "", "" (comma followed by space). The server when passing properties to as --hiveconf includes space in the property name and so the correct properties don't get passed.",security,['ambari-web'],AMBARI,Bug,Major,2013-07-18 00:24:18,48
12658095,Security wizard: Relogin while on step3 without quitting the wizard throws JS error. ,"h6. Steps to reproduce:
* Go to step-3 (Generate principals and keytabs) of Enable security wizard.
* Restart Amabri server. 
* Refresh on step-3. ui will take you to login page.
* Entering correct credentials, user will be navigated again to step-3 of security wizard. At this point JS error is encountered.",security,['ambari-web'],AMBARI,Bug,Major,2013-07-16 19:32:37,48
12657438,Create a utility script to distribute all the keytabs generated by keytabs.sh,"python ambari-server/src/main/resources/scripts/distribute_keytabs.py -h
Options:
  -h, --help            show this help message and exit
  -d DIRPATH, --directory=DIRPATH
                        Path to the Directory containing tar files
  -i IDENTITY_FILE, --identity-file=IDENTITY_FILE
                        Path to the identity file
  -k KRB5_CONF, --krb5-conf=KRB5_CONF
                        Path to the krb5_conf file. (This is optional to distribute krb5.conf file)",security,['ambari-web'],AMBARI,Task,Major,2013-07-12 10:55:49,48
12657404,Security CSV cleanup,"* *Should say ""Hive Metastore and HiveServer2"", not just HiveServer2*. Even though they are co-located master components, let's make it clear this principal is for both
* *include keytab file column*. In addition to the keytab full path column (/etc/security/keytabs/jt.service.keytab), include a column with just the filename (jt.service.keytab). Easier to copy/paste/parse if you want to use the CSV file.",security,['ambari-web'],AMBARI,Improvement,Major,2013-07-12 04:25:25,48
12657384,Perf: Service summary view inefficiently binds to host components,"In {{ambari-web/app/views/main/service/info/summary.js#hostComponentsUpd()}}, is called per each hostComponent's host and master property change. On a 150 node cluster, we get like 300 calls just for this method.

Due to this, service_mapper, which usually maps in 600ms, takes now 5.8s. ",performance,['ambari-web'],AMBARI,Bug,Major,2013-07-12 00:14:08,122
12657370,Ask user to specify kerberos tool path instead of kinit path,"In the security wizard, we should ask user for folder location(s) for kerberos tools (e.g. kinit, kdestroy, etc.) instead of kinit path. We should allow for comma separated paths. The default value should be changed to /usr/bin.",security,['ambari-web'],AMBARI,Bug,Major,2013-07-11 23:12:57,48
12657298,Security wizard: wrong behavior for retry stage 2,"After Stage 2 failed and click on retry, Stage 3 starts, but Stage 2 remained failed.",security,['ambari-web'],AMBARI,Bug,Major,2013-07-11 18:11:27,48
12657193,UI Service mapper inefficiently parses server data,{{App.servicesMapper}} takes 2 seconds to map/parse server JSON into Ember objects with 150 hosts in the cluster.,perfomance,['ambari-web'],AMBARI,Bug,Major,2013-07-11 02:36:24,122
12656106,Unable to deploy secure cluster as generated site.pp file is incorrect,we are passing few parameters to the agents from ui that are used for ui purpose and parameter with the same name are also injected by server. Puppet doesn't allow duplicate declaration of the global variables which causes error.,security,['ambari-web'],AMBARI,Bug,Major,2013-07-03 19:59:20,48
12655982,Security wizard: Successive refreshes at some point results in blank screen.,"h5. This ticket addresses following issues:

* Refreshing at the moment when none of the stage is running brings up blank screen. The time window for this is small. Once entered in this state, user needs to quit the wizard and go through the wizard again to enable security.
* On Google chrome multiple refreshes occasionally cancels the request (snapshot_1.png). This calls ajax error callback function which sets the error flag and resets the request Id to its default value. Setting the error flag will mark the stage as failure and show retry button which can be accepted. But setting the request Id to its default value makes retry button fire a API for new request. We should persist the request Id to let the user poll on the same Id on hitting retry button. Ajax call error while polling request shouldn't reset requestId. Only task failures in the polled response data should reset request Id.
* Refresh on the step2 (Configure Services) navigates to step1 of the wizard.
* We keep persisting entire localDb on the server side with the help of an observable function. This is for multiple browser support. This is large amount of data and its noticed to have slower ui on refreshes as refresh  calls the function multiple time. We need to isolate other data and persist only security wizard specific data on the server.   ",security,['ambari-web'],AMBARI,Bug,Major,2013-07-03 07:58:55,48
12655705,Security wizard: Back button remains enabled after a stage fails.,Back button should be a computed property and navigation links should also be locked down again after hitting retry on a failure,security,['ambari-web'],AMBARI,Bug,Major,2013-07-02 05:54:28,48
12654919,Security Wizard: show which principals and keytabs need to be created on which hosts,"Currently it is very difficult to know what principals and keytabs need to be created on which hosts.
We should present this information to the end user in a format that is easy to consume.
The user running the wizard may not be the one who will be creating keytabs and principals. We can expose the capability to download a csv file and send it to the appropriate person who may parse the data to create a script to generate principals/keytabs (or do so manually).
Display the attached as a popup after Configure Services step is done.
Let's show it as a popup so that we don't affect any existing navigation/flow.
For generating the content:
Keytab paths are based on the user input
Principal names are based on the user input
NameNode host: show the nn and HTTP principals and keytab paths
JobTracker host: show the jt principal and keytab path
Oozie Server host: show the oozie and HTTP principals and keytab paths
Nagios Server host: show the nagios principal and keytab path
HBase Master host: show the hbase principal and keytab path
Hive Server host: show the hive principal and keytab path
WebHCat Server host: show the HTTP principal and keytab path
ZooKeeper Server host: show the zookeeper principal and keytab path
DataNode host: show the dn principal and keytab path
TaskTracker host: show the tt principal and keytab path
RegionServer host: show the hbase principal and keytab path
If there are duplicated principals on the same host, display it only once.
Clickng on ""Download CSV"" downloads the CSV file (""host-principal-keytab-list.csv""). The same content, except each row is a comma-delimited list with a \n at the end.",pull-request-available,['ambari-web'],AMBARI,Improvement,Critical,2013-06-26 12:17:41,19
12654829,Security wizard: Minimize usage of globals.,"* Resolution of this ticket includes reducing agent's dependency on global. Wherever possible retrieve information from site properties rather then global on agent end. 
* Resolution should also make web-ui to save only those set of secure global properties that are essentially required by agent via user input.",security,"['ambari-agent', 'ambari-web']",AMBARI,Improvement,Major,2013-06-25 21:57:03,48
12654442,"Add document of services, configurations, and components mapping","Ambari supports using API to install service to a cluster, and I consolidate the information of adding services to cluster by API.

It may helpful for those who want to add services or components into cluster by API.
",documentation,['documentation'],AMBARI,Improvement,Major,2013-06-24 07:03:22,156
12653976,Security Wizard: modify message that show up when starting services after the security configs have been set .,"After the security configurations have been saved, we show the success message ""Kerberos-based security has been enabled on your cluster.""

However, the wizard is not done and continues to start services, even thought the message implies that everything is done.

Let's change the message to say ""Kerberos-based security has been enabled on your cluster.  Please wait while services are started in secure mode.""

A similar messaging change should be made for the disable path as well.",security,['ambari-web'],AMBARI,Bug,Major,2013-06-20 17:56:55,48
12653874,Amabri Client refactoring -1,"Amabri Client refactoring -1
Refactored the Ambari client code structure to handle future changes and make it layered",client,['ambari-web'],AMBARI,Improvement,Minor,2013-06-20 06:57:44,147
12653127,"Security Wizard: Misc fixes for re-running the wizard, delay and pop-up.","This ticket covers following issues:
* On security wizard When a stage fails, going back to step-2 (Configure), refreshing on the page and again hitting next does not load service users that are required to form a core-site and oozie-site property.
* Reduce persist API calls as it saves entire localDb on server and subsequent calls to it produces delay.
* On Error of any of the stage, user should be able to quit without any pop-up dialog.",security,['ambari-web'],AMBARI,Bug,Major,2013-06-17 06:20:53,48
12652531,Security Wizard: webhcat Server start fails on enabling security,This happens when _templeton.kerberos.principal_ property is set to HTTP/_HOST@<realm name> instead of HTTP/<internal host name>@<realm name>,security,['ambari-web'],AMBARI,Bug,Major,2013-06-13 00:14:30,48
12652441,"Handle zero finish time, distinguish map and reduce stages better","Until a job is finished, its finishTime is 0.  The UI is displaying this as a negative duration.

Also, changing the border color to distinguish a map stage from a reduce stage isn't very effective.  Change the shape instead.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2013-06-12 17:31:20,157
12652276,"Security wizard: Relabel the three actions in Step 3 and don't link them before they are ""in progress""","Relabel the three actions shown:
1. Stopping Services -> Stop Services
2. Saving Configurations -> Save Configurations
3. Starting Services -> Start Services

Also, ""Starting Services"" is linked and clickable even before the task is started; it should only be linked and clickable once the task is started.",security,['ambari-web'],AMBARI,Improvement,Major,2013-06-11 21:04:06,48
12652128,Security Wizard: Misc fixes related to Navigation and user restriction.,"* Security wizard should be able to withstand browser crashes. This includes persisting cluster status on every step and every stage of deploy stage. 
* Cross icon should be non-functional only when stage-2(Applying configuration) of Deploy step is working.
* Refreshing the browser When stage3 ('Starting services') of Deploy stage shouldn't navigate to security page.
* Back button and Nav links to lower steps should be enabled when any stage fails but not if wizard goes through successfully.
 ",security,['ambari-web'],AMBARI,Bug,Major,2013-06-11 00:44:38,48
12651319,YARN/MR2 do not start after reconfiguring,"After successfully installing a cluster, I saved YARN configs (changed only 1 property). YARN service would not start with ResourceManager saying it cannot bind to port.

Turns out that certain properties in {{yarn-site.xml}} were set to null value.
{noformat}
""yarn.resourcemanager.admin.address"" : ""null"",
""yarn.resourcemanager.resource-tracker.address"" : ""null"",
""yarn.resourcemanager.scheduler.address"" : ""null"",
""yarn.resourcemanager.address"" : ""null"",
""yarn.log.server.url"" : ""null"",
{noformat}
Similar problem with MR2 also. The {{mapred-site.xml}} had 19 properties which were null.

{noformat}
""mapred.jobtracker.taskScheduler"" : ""null"",
""mapred.tasktracker.map.tasks.maximum"" : ""null"",
""mapred.hosts.exclude"" : ""null"",
....
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2013-06-06 12:30:41,19
12650047,Security fixes with HBase service check,"This ticket addresses following issues:
* Granting permission to ambari smoke user for hbase smoke test should be done in HBase service_check.pp
* Remove specifying of hbase config directory while starting hbase shell. 
* Exit out of HBase shell after granting permissions. ",security,['ambari-agent'],AMBARI,Bug,Major,2013-05-29 22:52:37,48
12649719,ambari-server start script (ambari-server.py) will never use SERVER_START_CMD_DEBUG,"The ambari-server.py start script has a command defined for starting the ambari-server in debug mode (SERVER_START_CMD_DEBUG, which turns on remote debugging), but there is currently no option supported that will force the script to use the debug start commaand.  
I propose adding a --debug option so that you can run ""ambari-server start --debug"" to activate remote debugging.",patch,['ambari-server'],AMBARI,Bug,Minor,2013-05-28 14:50:13,158
12647577,Use modified dependencies if a stack contains an HCFS service (Hadoop Compatible File System),Use stack metadata to determine if a stack contains an HCFS service and generate modified RoleCommandOrder dependencies if it does.,patch,['ambari-server'],AMBARI,Improvement,Major,2013-05-14 19:09:55,130
12647358,ambari-web modifications to allow for Hadoop Compatible Filesystems (HCFS),"Make modifications to ambari-web that will allow for the selection of a Hadoop Compatible Filesystem.  These changes include allowing HDFS to be unselected (either HDFS or HCFS must be selected).  If HCFS is chosen, push appropriate configuration (site-conf.xml) files during the install so that systems will work with HCFS as the underlying filesystem rather than HDFS.",patch,['ambari-server'],AMBARI,Improvement,Major,2013-05-13 19:06:55,158
12643880,Ambari-Web was build successfully but occur runtime error.,"i build ambari-web using source. when i replace app.js on the server with mine , it occur error: Uncaught TypeError: Object function (name) { .... has no method 'register' .

I found the app.js which i build contains code like this: window.require.register(..
And the code generated by Ambari installer like this: window.require.define(

",ambari-web brunch build,['infra'],AMBARI,Bug,Major,2013-04-22 07:21:35,52
12642472,Integrate Ambari with Accumulo,This task is for Google Summer of Code 2013.  The goal is to integrate Ambari with Accumulo to provide a more robust and extensible management layer for Accumulo clusters.,Accumulo,[],AMBARI,Epic,Major,2013-04-14 23:41:06,157
12642324,Set correct Oozie property for security instead of deprecated property.,"WARN ConfigUtils:542 - USER- GROUP- Using a deprecated configuration property oozie.service.AuthorizationService.security.enabled, should use oozie.service.AuthorizationService.authorization.enabled. Please delete the deprecated property in order for the new property to take effect.",security,[],AMBARI,Bug,Major,2013-04-12 21:42:08,48
12642103,Heatmap should display full details on mapped metric,"When you visit Ambari's Heatmap section and select one of the metrics (say HDFS>Bytes Read), the title says just 'Bytes Read'. This title should be more descriptive like 'HDFS Bytes Read' or better. Similar changes need to be made for other metrics.",newbie,['ambari-web'],AMBARI,Improvement,Minor,2013-04-11 18:58:25,159
12641755,package nagios-plugins-1.4.9-1 not found and Ambari fail to install nagios.,"My os is centos 6.3.
Ambari fail to install nagios because it can not found package nagios-plugins-1.4.9-1 when Ambari automatically uses yum install.
when i search the package using yum search nagios, there is no any nagios-plugins in the result.

please excuse my poor English.


",epel nagios-plugins,[],AMBARI,Bug,Major,2013-04-10 01:37:10,52
12641411,Few core-site properties vanished after seemingly benign reconfiguration.,UI metadata properties are not being reconfigured and retained on saving services.,client regresssion,['ambari-web'],AMBARI,Bug,Major,2013-04-08 17:49:42,48
12641076,Create ability to add alternate storage system that uses Ambari management tools - Stage 1,"Create ability to add alternate storage system that uses Ambari managment tools - Stage 1
--Allow Hadoop applications to run on alternative compatible storage layer ",features,"['ambari-server', 'documentation', 'infra', 'test']",AMBARI,New Feature,Major,2013-04-05 17:49:16,130
12640666,Security wizard navigation: Restrict user on step3 until the decision of the step is reached,"Right now there are three possible ways to move away from step3:
* Through browser back button.
* Through navigation menu.
* By hitting cross on the security wizard popup.
* By entering and hitting some other URL.",security,['ambari-web'],AMBARI,Improvement,Major,2013-04-03 23:58:43,48
12640270,Python REST client to invoke REST calls,"Ambari doesnt have a python REST client to invoke REST calls .Currently users have to depend upon curl command.

I have created the Ambari python client and attached as a zip.Please review and give your feedbacks

The zip contains a skeleton code with few of the resources supported.The features are :

Supported feature
===================
1)get_all_clusters
2)get cluster by name
3)get service by nmae
4)start/stop service

Once this skeleton code is reviewed i will checkin the rest of the resources/features
I currently support creation of cluster via this python client.



install python client on linux box
=============================='
1)unzip the attachment
2)execute 'python setup.py install  --record installation.txt'






example:
==========
from ambari_client.ambari_api import  AmbariClient 
client = AmbariClient(""localhost"", 8080, ""admin"",""admin"",version=1)
all_clusters = client.get_all_clusters()

   
cluster = client.get_cluster('test1')
   
serviceList = cluster.get_all_services()
  
    
for service in serviceList:
    print str(service.service_name)+"" = ""+str(service.state)
  

to start/stop service
-----------------------
ganglia = cluster.get_service(""GANGLIA"")       
ganglia.stop()
ganglia.start()

    


",REST,['ambari-server'],AMBARI,New Feature,Major,2013-04-02 10:46:49,147
12640136,Allow ambari-server client.api.port to be configured during setup.,"Allow client.api.port to be configured during ambari-server setup. 
Something like this;
ambari-server setup -j /usr/java/default -p 8085
",configuration,[],AMBARI,Improvement,Minor,2013-04-01 17:28:31,76
12639518,Confusion in Ambari installation document.  Add documentation on how to set up a local repo.,"Am bit confused in the documentation here http://incubator.apache.org/ambari/1.2.2/installing-hadoop-using-ambari/content/ambari-chap1-6.html

In step 1. , what does it mean by Set up the local mirror repositories as needed for HDP, HDP Utils and EPEL.?, what is HDP?",documentation,['documentation'],AMBARI,Bug,Major,2013-03-28 05:29:24,160
12637870,Security Wizard UI tweaks,"* On Admin > Security page, remove the text ""Enabling security is highly recommended"".

* Get rid of the alert box that says ""We will walk through add security wizard"" and just immediately popup the security wizard.

* Adjust styles on the popup as they are in other popups of web ui

* In Step 2, the content for the first tab should be shown by default.  Rename the tab label ""CLUSTER"" to ""Kerberos"" and rename the ""Kerberos"" category name to ""General"".

* ""primary name"" should be shown as ""Primary name"".",security,['ambari-web'],AMBARI,Improvement,Major,2013-03-20 00:48:31,48
12635984,Unable to Start Oozie server,"Unable to start Ozzie Server

I saw these logs in the agent log

ERROR 2013-03-08 05:29:24,338 puppetExecutor.py:179 - Error running puppet:

INFO 2013-03-08 05:29:24,338 puppetExecutor.py:189 - Output from puppet :
warning: Dynamic lookup of $lzo_enabled at /var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/service.pp:38 is deprecated.  Support will be removed in Puppet 2. 8.  Use a fully-qualified variable name (e.g., $classname::variable) or parameterized classes.
warning: Dynamic lookup of $ensure at /var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/service.pp:103 is deprecated.  Support will be removed in Puppet 2.8.   Use a fully-qualified variable name (e.g., $classname::variable) or parameterized classes.
notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed success fully
notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}6c658750c9389c43df43984f26af9f d0' to '{md5}15a2e1db3e7292793cf47316436373be'
notice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/Exe c[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/returns: put: org.apache.hadoop.security.AccessC ontrolException: Permission denied: user=oozie, access=WRITE, inode=""user"":hdfs:hdfs:rwxr-xr-x
notice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/Exe c[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/returns: chmod: could not get status for '/user/ oozie/share': File does not exist: /user/oozie/share
err: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/Exec[e xec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/returns: change from notrun to 0 failed: su - oozie  -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/m odules/hdp/manifests/init.pp:313
notice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/Anc hor[hdp::exec::exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share'::end]: Dependency Exec[exec su - ooz ie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share'] has failures: true
warning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share']/An chor[hdp::exec::exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie/share'::end]: Skipping because of failed d ependencies
notice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c  'cd /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - oozie -c  'cd  /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh'::begin]: Dependency Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmo d -R 755 /user/oozie/share'] has failures: true
warning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c  'cd /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - oozie -c  'c d /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh'::begin]: Skipping because of failed dependencies
notice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c  'cd /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - oozie -c  'cd /var/tmp/ooz ie && /usr/lib/oozie/bin/oozie-start.sh']: Dependency Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod -R 755 /user/oozie /share'] has failures: true
warning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c  'cd /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - oozie -c  'cd /var/tmp/oo zie && /usr/lib/oozie/bin/oozie-start.sh']: Skipping because of failed dependencies
notice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c  'cd /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - oozie -c  'cd  /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh'::end]: Dependency Exec[exec su - oozie -c 'hadoop dfs -put /usr/lib/oozie/share /user/oozie ; hadoop dfs -chmod  -R 755 /user/oozie/share'] has failures: true
warning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - oozie -c  'cd /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - oozie -c  'c d /var/tmp/oozie && /usr/lib/oozie/bin/oozie-start.sh'::end]: Skipping because of failed dependencies
notice: Finished catalog run in 6.19 seconds

INFO 2013-03-08 05:29:24,338 puppetExecutor.py:190 - Puppet exit code is 6
INFO 2013-03-08 05:29:24,339 puppetExecutor.py:143 - ExitCode : 6",oozie,[],AMBARI,Bug,Major,2013-03-08 08:42:01,147
12635750,"Oozie Check execute Fails, although install is success","I was able to create a 5 Node Ambari CLuster.
I was able to install HDFS, MApreduce, Nagios , Ganglia, Oozie, 
All the services have started.
Mapreduce check and HDFS check have passed.
But Ooozie check fails.

Also some time the puppet step :/usr/lib/oozie/bin/oozie-setup.sh -hadoop 0.20.200 /usr/lib/hadoop/ -extjs /usr/share/HDP-oozie/ext.zip
fails because it is unable to find ext.zip


notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at java.lang.reflect.Method.invoke(Method.java:597)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at java.security.AccessController.doPrivileged(Native Method)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at javax.security.auth.Subject.doAs(Subject.java:396)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Client.call(Client.java:1107)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at $Proxy1.addBlock(Unknown Source)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at java.lang.reflect.Method.invoke(Method.java:597)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at $Proxy1.addBlock(Unknown Source)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3686)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3546)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2600(DFSClient.java:2749)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2989)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 13/03/07 00:46:41 WARN hdfs.DFSClient: Error Recovery for null bad datanode[0] nodes == null
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 13/03/07 00:46:41 WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/ambari_qa/examples/apps/ssh/job.properties"" - Aborting...
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: copyFromLocal: java.io.IOException: File /user/ambari_qa/examples/apps/ssh/job.properties could only be replicated to 0 nodes, instead of 1
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 13/03/07 00:46:41 ERROR hdfs.DFSClient: Failed to close file /user/ambari_qa/examples/apps/ssh/job.properties
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/ambari_qa/examples/apps/ssh/job.properties could only be replicated to 0 nodes, instead of 1
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1637)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:757)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at java.lang.reflect.Method.invoke(Method.java:597)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at java.security.AccessController.doPrivileged(Native Method)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at javax.security.auth.Subject.doAs(Subject.java:396)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.Client.call(Client.java:1107)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at $Proxy1.addBlock(Unknown Source)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at java.lang.reflect.Method.invoke(Method.java:597)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at $Proxy1.addBlock(Unknown Source)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3686)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3546)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2600(DFSClient.java:2749)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2989)
notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: 13/03/07 00:46:42 WARN hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/ambari_qa/input-data/rawLogs/2010/01/01/01/40/_SUCCESS could only be replicated to 0 nodes, instead of 1
",oozie,[],AMBARI,Bug,Minor,2013-03-07 08:53:52,147
12631790,Wrong calculation of duration filter on apps page ,"Wrong calculation for the duration filter if we enter just number, without h, m or s to specify the unit. By default we should take seconds as the default unit.",client,['ambari-web'],AMBARI,Bug,Major,2013-02-11 19:13:33,48
12631317,Install progress dialog WARN icon + color ,"On the install/start/test task popup, two changes:

1) For items that get cancelled (since those are just WARN), the icon should be orange just like the WARN condition
2) Use the icon-minus instead of the icon-remove.",Installer client,['ambari-web'],AMBARI,Improvement,Major,2013-02-07 19:01:16,48
12631316,Mouse cursor hover behavior is strange on Job Browser,"When hovering the mouse cursor around Show All, Filtered, the mouse cursor changes to the ""hand"" icon as expected. To the right, the cursor turns into a hand even when hovering over areas where there's no link. Hovering over ""Clear filters"", the cursor does not turn into the ""hand"".",client,['ambari-web'],AMBARI,Bug,Major,2013-02-07 18:58:06,48
12631157,Error in filtering Configuration properties maintained at UI for WebHcat service.,"Right now there are no Webhcat global properties that we maintain on UI front. So this bug is not seen. But If we add any new global property for webHcat, it won't show up under webHcat service at step7 of installer.",client,['ambari-web'],AMBARI,Bug,Major,2013-02-06 22:43:55,48
12630785,Hive client is not installed on Nagios server host,We need to install Hive client on Nagios server for managing kerberos based secure cluster. Absence of hive-site.xml on nagios server results in hive check faliure.,Installer client,['ambari-web'],AMBARI,Task,Major,2013-02-05 00:01:57,48
12630031,Deploy progress returns to deploy screen (momentarily),"When the deployment progress bar finishes, it should advance to the install/start/test page as opposed to returning to review/deploy page. ",client,['ambari-web'],AMBARI,Bug,Major,2013-01-30 23:28:07,161
12630027,Inconsistent error/warning status in Deploy step; install stalls,"Top level is red
Host level is orange
Individual task level is all green. The statuses seem inconsistent.


Prior to this: 
Installation failed saying hbase packages were not available (likely due to a build happening)
Retry got me here..",client,['ambari-web'],AMBARI,Bug,Major,2013-01-30 23:17:32,161
12630019,Get rid of mvn warnings,"Every time we build project maven shows multiple warnings ( we need to get rid of such warnings ):
{code}
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
{code}
{code}
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-server:jar:1.2.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ org.apache.ambari:ambari:1.2.0-SNAPSHOT, /code/pom.xml, line 66, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
{code}",client,['ambari-web'],AMBARI,Bug,Major,2013-01-30 22:38:14,161
12630011,Make sure that Ambari Web renders all elements correctly when the browser width is 1024px or narrower (refactor),"Current behavior is to dynamically layout elements as the browser viewport size changes.  This causes elements to overlap, go outside the bounding box, positioned in a weird way, etc.
Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.
Bigger than 1024px should still utilize more screen space as it does today.",client,['ambari-web'],AMBARI,Improvement,Major,2013-01-30 22:09:05,161
12630001,Minor label cleanup on Jobs Charts popup,"Think this cleans up the UI a bit:

1) Change the top popup label from ""DAG/Charts"" to ""Job Charts""
2) Change the ""Charts"" tab to ""Timeline & Tasks""
3) Change the ""Job Tasks' View"" to ""Job Tasks"" and remove the apostrophe '",client,['ambari-web'],AMBARI,Improvement,Major,2013-01-30 21:14:09,161
12628569,Documentation discrepancy ,"In the following link, we're asking to modify /etc/ambari-agent/ambari-agent.ini. This file does not exist as part of the RPM, it's /etc/ambari-agent/conf/ambari-agent.ini 

http://incubator.apache.org/ambari/1.2.0/installing-hadoop-using-ambari/content/ambari-chap6-1.html

Kind regards,
Olivier",documentation,"['ambari-agent', 'documentation']",AMBARI,Bug,Minor,2013-01-21 16:12:57,52
12627681,Spelling of paste is incorrect on the documentation for Section 3.3 (Install Options),"The spelling of ""If you are using IE 9, the Choose File button does not appear. Use the text box to cut and past your private key manually."" is incorrect, it needs to say ""paste your private key"".

",documentation,['documentation'],AMBARI,Bug,Trivial,2013-01-15 16:03:30,52
12627513,Cluster summary after adding hosts does not reflect the correct number of hosts,"Currently, you have to refresh the page after running Add Hosts Wizard to reflect the correct number of hosts that includes all the newly added hosts.",ambari-1.2.0-release_notes,['ambari-web'],AMBARI,Bug,Critical,2013-01-14 21:46:24,52
12627512,User allowed to go back from host registration screen when it is in progress and there is no easy way to find out when bootstrap can be performed again,"Only one bootstrap process can be running on Ambari server at any given moment.
If the user goes back to Step 2 while a bootstrap is going on, the user is warned that bootstrap is already in progress and is prevented to Step 3 again if the previously executed bootstrap is still running.  Currently, there's no easy way to tell when the previous bootstrap finishes, so the user has to keep trying to proceed to Step 3 by hitting Next.  ",ambari-1.2.0-release_notes,[],AMBARI,Bug,Critical,2013-01-14 21:45:26,52
12627511,Alert status change does not change time for the alerts.,Alert status change does not change time for the alerts.,ambari-1.2.0-release_notes,[],AMBARI,Bug,Critical,2013-01-14 21:45:02,122
12627508,For live status checks we should only look at the run directories that we get from the server (only for hadoop and its eco system) and not all,Currently in Ambari the live status is checked by searching for pid files under /var/run. We should fix that to only use the pid directories that are attached as configs to the service.,ambari-1.2.0-release_notes,[],AMBARI,Bug,Critical,2013-01-14 21:39:02,151
12627488,Change the dashboard graph for HBase since its using cumulative metrics.,Change the dashboard graph for HBase since its using cumulative metrics.,ambari-1.2.0-release_notes,[],AMBARI,Bug,Critical,2013-01-14 19:44:41,52
12627486,Disk Info Metrics and memory usage sometimes do not show up for an hour or so.,"Disk Info Metrics and memory usage sometimes do not show up for an hour or so.
",ambari-1.2.0-release_notes,[],AMBARI,Bug,Critical,2013-01-14 19:42:22,120
12627002,Cannot save changes to ZooKeeper configuration,"
Stop service and modify:
Ticks to allow for sync at Runtime
Ticks to allow for sync at Init
Click on Save and apply changes.
Nothing happens. Check the ZK server config and parameters are unchanged




JS Console on Save and Apply
Uncaught Error: assertion failed: Must use Ember.set() to access this property vendor.js:1689
Ember.assert vendor.js:1689
o_defineProperty.set vendor.js:4091
(anonymous function) app.js:5554
(anonymous function) app.js:5553
App.MainServiceInfoConfigsController.Em.Controller.extend.getGlobConfigValue app.js:5537
(anonymous function) app.js:5504
App.MainServiceInfoConfigsController.Em.Controller.extend.loadUiSideConfigs app.js:5503
App.MainServiceInfoConfigsController.Em.Controller.extend.saveSiteConfigs app.js:5496
App.MainServiceInfoConfigsController.Em.Controller.extend.saveServiceConfigProperties app.js:5437
App.MainServiceInfoConfigsController.Em.Controller.extend.restartServicePopup app.js:5348
ActionHelper.registeredActions.(anonymous function).handler vendor.js:21104
(anonymous function) vendor.js:12896
f.event.dispatch vendor.js:92
h.handle.i",1.2.0,['ambari-web'],AMBARI,Bug,Blocker,2013-01-10 18:53:46,161
12626999,tmpfs filesystem being added to the list in the dir used by Ambari,"I saw this on a sles cluster. On EC2 they have a tmpfs mounted and Ambari picked it up.
Not sure what the ideal solution is but i feel tmpfs should not be included in the available mount points.
Also the tmpfs is being used in various directories that will have to change during the install.
Attached screenshots.",1.2.0,['ambari-web'],AMBARI,Bug,Major,2013-01-10 18:45:53,161
12626995,"On Notification Popup, clicking ""go to nagios UI"" doesn't load nagios UI","1) Cause notification to occur (for example, stop oozie)
2) On dashboard, click notification icon
3) On the notification popup, click ""go to nagios web UI""
4) nothing happens.
This issue is on IE9, Safari and Chrome. Works fine on Firefox.",1.2.0 ember,['ambari-web'],AMBARI,Bug,Critical,2013-01-10 18:42:19,161
12626892,"In some cases, clicking ""Register and Confirm"" button does not do anything","There can be only one bootstrap process running on the server at any given time.
When UI tries to issue a new bootstrap process (the user can go back to Step 2 and tries to bootstrap again), the server responds that it cannot. UI does not handle this condition gracefully and does nothing.

Steps to replicate:
In step 2, supply host names, SSH Key, and hit ""Register and Confirm""
Immediately hit ""Back"" in step 3.
Hit ""Register and Confirm"". This will not do anything until the bootstrap operation being done by the server has completed.",1.2.0 ember,['ambari-web'],AMBARI,Bug,Critical,2013-01-10 01:58:49,161
12626890,Resuming deploy for Installer/Add Hosts does not work if the browser is shut down during the start phase of deploy,Resuming deploy for Installer/Add Hosts does not work if the browser is shut down during the start phase of deploy step.,1.2.0 ember,['ambari-web'],AMBARI,Bug,Critical,2013-01-10 01:48:58,161
12626883,Changing users to non-default values causes servicecomponent live status scripts to report wrong results,"Changing user to hdfs2 ends up with pid file to end up as hadoop/hdfs2/hadoop-hdfs2-namenode.pid

This ends up showing namenode as down on the UI even though the process is up.

Likewise for MR. Probably true for Hbase too.

Other pid files:

/var/run/hadoop/mapred2/hadoop-mapred2-jobtracker.pid
/var/run/hadoop/mapred2/hadoop-mapred2-historyserver.pid
/var/run/hadoop/hdfs2/hadoop-hdfs2-secondarynamenode.pid ",patch,[],AMBARI,Bug,Blocker,2013-01-10 00:58:57,42
12624947,Install Wizard: Confirm host stuck at Preparing stage,"With the install wizard went to assign slaves page successfully, returned back to Welcome page, reentered data and then the UI got stuck at Confirm hosts page.",host,['ambari-web'],AMBARI,Bug,Major,2012-12-21 07:04:04,161
12624874,"After clicking the deploy button on the Add Hosts wizard, the user is always taken to the Installer Wizard Step 8 upon login","In Add Hosts wizard, when browser crashes anytime after clicking deploy button and relaunches the browser, the context is lost and taken back to 8th step of the installer wizard. ",1.2.0 ember host javascript wizard,['ambari-web'],AMBARI,Bug,Blocker,2012-12-20 19:27:43,161
12624547,Error handling when errors are encountered during preparation for deploy,"Currently, if any errors are encountered during preparation for deploy, the user is taken to the deploy page and the hosts will be shown as ""Waiting"" but nothing happens. This is bad UX.
At a minimum, we should prevent the user from proceeding and display an appropriate error message if any error is encountered after ""Deploy"" is clicked, but before we transition to Step 9.
We should also think about how a user can recover from this situation.
At this point, the deploy has not initiated, but certain API calls may have succeeded, so we may have incomplete info in the database. Currently there's no convenient way to ""rollback"".
We can either ask the user to clean the slate by reinitializing the database and try again (should succeed if the original problem was temporary).
We can also build more logic in the UI to retry, check if records already exist, etc...",client installer,['ambari-web'],AMBARI,Bug,Major,2012-12-18 23:15:55,161
12623874,Nagios install fails on SLES with incompatible nagios plugins,"{noformat}

plugins]/Hdp::Package::Process_pkg[nagios-plugins]/Package[nagios-plugins-1.4.9]/ensure: change from absent to present failed: Execution of '/usr/bin/zypper --quiet install -l -y nagios-plugins-1.4.9' returned 4: Problem: cannot install both nagios-plugins-1.4.13-1.37.1.x86_64 and nagios-plugins-1.4.9-1.x86_64\n Solution 1: Following actions will be done:\n  downgrade of nagios-plugins-1.4.13-1.37.1.x86_64 to nagios-plugins-1.4.9-1.x86_64\n  install nagios-plugins-1.4.9-1.x86_64 (with vendor change)\n    SUSE LINUX Products GmbH, Nuernberg, Germany  -->  Nagios Plugin Development Group\n Solution 2: do not ask to install a solvable providing nagios-plugins = 1.4.9\n\nChoose from above solutions by number or cancel [1/2/c] (c): c\n\nnotice: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-plugins]/Hdp::Package[nagios-plugins]/Hdp::Package::Process_pkg[nagios-plugins]/Anchor[hdp::package::nagios-plugins::end]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-plugins]/Hdp::Package[nagios-plugins]/Hdp::Package::Process_pkg[nagios-plugins]/Anchor[hdp::package::nagios-plugins::end]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-addons]/Hdp::Package[nagios-addons]/Hdp::Package::Process_pkg[nagios-addons]/Anchor[hdp::package::nagios-addons::begin]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-addons]/Hdp::Package[nagios-addons]/Hdp::Package::Process_pkg[nagios-addons]/Anchor[hdp::package::nagios-addons::begin]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-addons]/Hdp::Package[nagios-addons]/Hdp::Package::Process_pkg[nagios-addons]/Package[hdp_mon_nagios_addons]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-addons]/Hdp::Package[nagios-addons]/Hdp::Package::Process_pkg[nagios-addons]/Package[hdp_mon_nagios_addons]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-addons]/Hdp::Package[nagios-addons]/Hdp::Package::Process_pkg[nagios-addons]/Anchor[hdp::package::nagios-addons::end]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server::Packages/Hdp-nagios::Server::Package[nagios-addons]/Hdp::Package[nagios-addons]/Hdp::Package::Process_pkg[nagios-addons]/Anchor[hdp::package::nagios-addons::end]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server::Packages/Anchor[hdp-nagios::server::packages::end]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server::Packages/Anchor[hdp-nagios::server::packages::end]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server/Hdp::Directory[/etc/nagios]/File[/etc/nagios]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server/Hdp::Directory[/etc/nagios]/File[/etc/nagios]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server/Hdp::Directory[/usr/lib64/nagios/plugins]/File[/usr/lib64/nagios/plugins]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server/Hdp::Directory[/usr/lib64/nagios/plugins]/File[/usr/lib64/nagios/plugins]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Exec[mkdir -p /var/run/nagios]/Anchor[hdp::exec::mkdir -p /var/run/nagios::begin]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Exec[mkdir -p /var/run/nagios]/Anchor[hdp::exec::mkdir -p /var/run/nagios::begin]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Exec[mkdir -p /var/run/nagios]/Exec[mkdir -p /var/run/nagios]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Exec[mkdir -p /var/run/nagios]/Exec[mkdir -p /var/run/nagios]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Exec[mkdir -p /var/run/nagios]/Anchor[hdp::exec::mkdir -p /var/run/nagios::end]: Dependency Package[nagios-plugins-1.4.9] has failures: true\nwarning: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Exec[mkdir -p /var/run/nagios]/Anchor[hdp::exec::mkdir -p /var/run/nagios::end]: Skipping because of failed dependencies\nnotice: /Stage[2]/Hdp-nagios::Server/Hdp::Directory_recursive_create[/var/run/nagios]/Hdp::Directory[/var/run/nagios]/File[/var/run/nagio

{noformat}
",patch,['ambari-agent'],AMBARI,Bug,Blocker,2012-12-14 00:52:54,42
12614661,Stop events should be handled at all valid points for safe recovery,Support stopping at start failed state for recovery.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-11-03 20:45:49,162
12613411,Add ui option to either create a Postgres database for Hive and Oozie or choose existing database.,"h5. Hive
* By default Ambari will create PostgreSQL on the node hosting Hive Metastore.
* User can point to an existing MySQL or postgres database instead of ambari creating a new one.

h5. Oozie
* By default Ambari will create PostgreSQL on the node hosting Oozie server.
* User can point to an existing MySQL or postgres database instead of ambari creating a new one.  ",ambari-666 installer,['ambari-web'],AMBARI,Sub-task,Major,2012-10-25 00:28:21,48
12613062,Resolve all navigation related issues for Step6 (Slave and Client component) of installer wizard.,"Fix following issues:
* On single node install, not selecting MapReduce or HBase stops the wizard flow.
* On re-navigating the  step6, we can proceed even without selecting a single slave component.
",ambari-666 installer,['ambari-web'],AMBARI,Sub-task,Major,2012-10-22 22:12:17,48
12612076,Integrate basic set of rest APIs with ambari-web installer wizard,"On submission of step 8 (Review page), following APIs should be triggered:
1) Create cluster
2) Create selected Services
3) Create components for selected services
4) Create components for selected services on designated hosts.","ambari-666 installer,",['ambari-web'],AMBARI,Sub-task,Major,2012-10-16 18:00:43,48
12611664,Serve ambari-web from jetty,Serve ambari-web from jetty(http://localhost:8080).,ambari-666 installer,['ambari-web'],AMBARI,Sub-task,Major,2012-10-13 04:04:57,48
12611210,Select Masters Page: make zooKeeper addition/removal UI more organized,"When multiple zooKeepers are recommended, The plus icon should only appear next to the ZooKeeper server on the bottom, and the minus icon should appear next to all the ZooKeeper servers.",AMBARI-666 Installer,[],AMBARI,Sub-task,Major,2012-10-10 17:47:49,48
12609712,Introduce ManagementController interface,"Add the ManagementController interface, simple implementation and tests.",AMBARI-666,[],AMBARI,Sub-task,Major,2012-10-01 04:06:34,132
12608022,Complete Java side implementation of bootstrapping agent hosts.,Completing the java side implementation of bootstrap - except for the actual python script that does an ssh and yum install/repo copy.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-09-18 07:50:25,151
12606752,Getting hardware info on disks/cpu/others using facter and using it during registeration.,Using df and facter to get all the node information to be used during registration to the server.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-09-09 06:35:06,151
12606505,Initial work on node-fsm,Initial work for the Node FSM. Added license for a bunch of files.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-09-06 23:38:48,162
12606363,Add skeleton for Ambari agent that talks to the server and collects information for host.,Skeleton for the ambari agent with packaging.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-09-06 07:33:11,151
12606200,Add a simple server and artifact generation to run a server with a simple api check,Simple server implementation on jetty and jersey.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-09-05 05:03:07,151
12605654,Classes for request objects.,This jira defines classes that will store the request parameters from the API.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-08-31 08:16:56,163
12605558,More basic classes for new design,Further work on updates to basic classes introduced in AMBARI-685,AMBARI-666,[],AMBARI,Sub-task,Major,2012-08-30 18:07:55,162
12604924,Basic object structure/design for the server,"Basic classes to be used across the system and in the state machine.
",AMBARI-666,[],AMBARI,Sub-task,Major,2012-08-24 21:02:50,162
12604531,Seperate directory for ambari-server.,Ambari 0.1 had a more organized directory structure which got changed in 0.9. We should get back to similar structure where ambari-server and ambari-agent are in separate directories. Some code from Ambari 0.1 can also be re-used for example the package structure and configurations.,AMBARI-666,[],AMBARI,Sub-task,Major,2012-08-22 22:49:54,163
12604171,Going back to step 3 from step 5 in UI breaks DB ,HostRoles table is not updated properly on re-selecting services,patch,[],AMBARI,Bug,Major,2012-08-21 03:37:35,48
12603516,Ambari architectural changes,"*  First release of Ambari is underway and this jira is to facilitate next set of major features in Ambari. This jira proposes a few significant architectural changes towards following goals:
* Webservice APIs
* Pluggability of components: Particularly third party tools like puppet.
* Reduce dependencies
* Flexibility to add features. 
* Maintainability of code, unit testing, modular design.
* Improve performance.

A design proposal will be published soon.",AMBARI-666,[],AMBARI,Bug,Major,2012-08-14 21:57:37,163
12603393,Ambari should work on nodes with puppet already installed.,"If puppet is already installed on nodes, Ambari should work without requiring to remove the existing puppet. If the existing puppet version is incompatible with the puppet modules, Ambari should install puppet at a different location without affecting existing installation.",AMBARI-666,['ambari-agent'],AMBARI,Bug,Major,2012-08-14 07:07:45,151
12601260,"Current installation guide refers to CentOS5 only. For CentOS6.*, adding Hortonworks repo, solves a lot of hiccups for install.","Adding a repo for Hortoworks Data platform fixes the dependency issue for puppet, which fails when you try to install the agent. Also, it helps to mention to run yum install php, that was missing also on the install guide.

add the following to the documentation:

1. rpm -Uvh http://public-repo-1.hortonworks.com/HDP-1.0.1.14/repos/centos6/hdp-release-1.0.1.14-1.el6.noarch.rpm

2. yum install epel-release 

3. yum install php mod_ssl

in jdk download section, need to add to download jce_policy-6.zip as well.

I would also add that I had to modify /etc/httpd/conf.d/puppetmaster.conf and add my machinename before .pem on line 35, otherwise service wouldn't start.",documentation,"['ambari-agent', 'documentation', 'site']",AMBARI,Improvement,Trivial,2012-08-02 18:56:36,52
12560712,Documentation: Blank Web UI due to to missing php-posix module,"Amberi service is start successfully but web page is blank.
Apache error log are as 
PHP Fatal error:  Call to undefined function posix_getpid() in /usr/share/hmc/php/util/lock.php on line 26 

How to overcome
yum install php-process ",documentation,['documentation'],AMBARI,Bug,Critical,2012-06-15 04:05:45,164
12554996,Typo in javascript,Typo in javascript.,patch,['ambari-server'],AMBARI,Bug,Major,2012-05-12 01:40:09,165
12554975,Enable LZO should show checkbox instead of text,Currently the enable lzo option shows a text box that needs to be filled with true/false. Changing the UI element to checkbox.,javascript,[],AMBARI,Bug,Major,2012-05-11 21:13:53,165
