id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13391832,Reduce retry in Kubernetes test,"_kubernetes_ tests wait for cluster startup, checking some conditions with retry.  In worst case all conditions are checked 100 times with 3 seconds delay, so the test may take 15 minutes to fail.",pull-request-available,['test'],HDDS,Improvement,Minor,2021-07-26 08:29:14,0
13391786,Disable failing kubernetes test,"_kubernetes_ check is failing in CI at {{ozone}}.  It is probably related to persistent volumes.  I propose to disable the failing test temporarily, until it can be fixed.

Last successful run: https://github.com/apache/ozone/runs/3137129344
First failed run: https://github.com/apache/ozone/runs/3140001398

SCM init container fails to start due to:

{code}
Unable to create directory /data/metadata specified in configuration setting ozone.metadata.dirs
{code}",pull-request-available,"['kubernetes', 'test']",HDDS,Task,Major,2021-07-26 05:18:12,0
13391581,Install OS-specific flekszible,"{{kubernetes.sh}} installs {{flekszible}} for Linux, even on other platforms.",pull-request-available,['kubernetes'],HDDS,Bug,Major,2021-07-23 14:14:25,0
13391111,Bump jetty version to 9.4.41.v20210516,Please see: https://github.com/apache/ozone/pull/2449,pull-request-available,[],HDDS,Improvement,Major,2021-07-21 10:53:14,1
13390913,Bump Apache Ratis version to 2.1,Please see: https://github.com/apache/ozone/pull/2443,pull-request-available,[],HDDS,Improvement,Major,2021-07-20 11:35:39,1
13390903,Avoid long sleep in TestPeriodicVolumeChecker,TestPeriodicVolumeChecker requires at least 130 seconds due to two long sleeps.,pull-request-available,['test'],HDDS,Bug,Major,2021-07-20 10:52:55,0
13390689,Multi-raft style placement with permutations for offline data generator,Please see: https://github.com/apache/ozone/pull/2434,pull-request-available,[],HDDS,Improvement,Major,2021-07-19 12:06:29,1
13389978,Avoid refresh pipeline for S3 headObject,"S3 head uses OM API lookup key which refreshes pipeline info by contacting SCM.

For S3 head we donot require any pipeline info, we need very basic details like length, type, etag and last modification time. 

By removing pipeline info which is not required for HEAD object, HEAD API performance can be improved.

This is identified during looking up graphs from [~kerneltime] testing",pull-request-available,"['OM', 'S3']",HDDS,Improvement,Major,2021-07-16 07:25:11,2
13389655,Datanode HTTP server fails to start in ozonesecure due to wrong keytab name,"{noformat}
[main] ERROR ozone.HddsDatanodeService: HttpServer failed to start.
...
Caused by: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/datanode.keytab
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2021-07-14 15:41:43,0
13389649,Disallow same set of DNs to be part of multiple pipelines,"Currently, with multi raft feature ON, a data node can participate in multiple pipelines. But, same set of dns can be part of multiple pipelines . This leads to 2 problems:

1) In case of let's say 5 datanodes, as and when 3 datanodes register initially with multi-raft feature ON, those 3 datanodes will pair among themselves to create two pipelines thereby satisfying the default pipeline limit of 2 per dn. Next datanodes even after registering cannot take part in writes bcoz it needs a 6th datanode to come up to form the nest tri-set , thereby remain unutilized.

2) With same set of Datanodes being part of multiple plpelines, if one datanode fails, the source for container re-replication will be only the other set of 2 nodes and can beocme a bottleneck.

3) This uneven distribution of pipelines among dns will lead to load distribution getting uneven as well.

 

 ",pull-request-available,['SCM'],HDDS,Bug,Major,2021-07-14 14:55:59,3
13389417,Add acceptance test for Hadoop 3.3,"Hadoop 3.3 support was recently added (HDDS-3292) in Ozone.  We should run Hadoop/MapReduce acceptance tests for this version, too.",pull-request-available,['test'],HDDS,Task,Major,2021-07-13 15:25:57,0
13389400,Reduce usage of OmKeyLocationInfoGroup.getLocationList(),"Follow-up of HDDS-5384: check usages of {{OmKeyLocationInfoGroup.getLocationList()}} and {{getLocationListCount()}}, and replace with cheaper calls if possible.",performance,['OM'],HDDS,Improvement,Major,2021-07-13 14:05:32,0
13388403,Avoid eager string formatting in preconditions,"Some calls to {{Preconditions.check...}} eagerly format the error message, resulting in unnecessary allocations.",pull-request-available,[],HDDS,Improvement,Minor,2021-07-08 13:27:42,0
13388000,Intermittent test failure in TestSCMPipelineManager#testPipelineReload,"Failure first noticed in this CI run on upgrade branch: https://github.com/apache/ozone/runs/2969789739

Was able to reproduce failure on master after about 100 runs of the test locally.",pull-request-available,[],HDDS,Bug,Major,2021-07-06 18:56:16,0
13387882,Data buffers incorrectly filtered for Ozone Insight,"{{ozone insight logs -v datanode.dispatcher}} prints binary data ({{buffers: ""4w7u1Zp8@{""}})

{noformat}
[TRACE|org.apache.hadoop.ozone.container.common.impl.HddsDispatcher|OzoneProtocolMessageDispatcher] [service=DatanodeClient] [type=ReadChunk] request is processed. Response:
cmdType: ReadChunk
traceID: """"
result: SUCCESS
readChunk {
  blockID {
    containerID: 1
    localID: 107544261427200009
    blockCommitSequenceId: 34
  }
  chunkData {
    chunkName: ""107544261427200009_chunk_1""
    offset: 0
    len: 10
    checksumData {
      type: CRC32
      bytesPerChecksum: 1048576
      checksums: ""354d261247""
    }
  }
  dataBuffers {
    buffers: ""4w7u1Zp8@{""
    buffers: ""<redacted>""
  }
}
{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-07-06 09:28:40,0
13386148,Return latest version of key location for client on createKey/createFile,"HDDS-5243 was a patch for omitting unnecessary key locations for clients on reading. But the same warning of large response size observed in our cluster for putting data. The patch can also be ported for putting data, as long as until object versioning is supported.

My hypothesis is: The large message was originally, and possibly maybe due to this warning and sudden connection close from client side on reading large message in Hadoop IPC layer, from Ozone Manager - which causes hopeless 15 retries from RetryInvocationHandler. The retries create another entry in OpenKeyTable but they never moved to KeyTable because the key never gets commited.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2021-06-28 03:36:14,2
13386025,Avoid catching Error while creating Ozone client,"{{OzoneClientProducer}} catches any {{Throwable}} while creating the Ozone client and logs it at debug level.  I think this should be avoided for OOME at least, possibly other {{Error}} types.",pull-request-available,['S3'],HDDS,Bug,Major,2021-06-26 19:52:08,0
13385599,SCM terminated with exit status 1: null,"steps taken :
Running ozone fault injection tests which involve activating/deactivating pipelines.
In one of the tests, while activating pipeline, SCM terminated.SCM log:
{code:java}
2021-06-23 23:57:03,166 WARN org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline PipelineID=661b906a-fd6b-4c14-b6b5-8c0a12c67ff7 is not found in the pipeline Map. Pipeline may have been deleted already.
2021-06-23 23:57:03,168 ERROR org.apache.ratis.statemachine.StateMachine: Terminating with exit status 1: null
java.lang.IllegalMonitorStateException
	at java.base/java.util.concurrent.locks.ReentrantReadWriteLock$Sync.tryRelease(ReentrantReadWriteLock.java:372)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1302)
	at java.base/java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.unlock(ReentrantReadWriteLock.java:1147)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.updatePipelineState(PipelineStateManagerV2Impl.java:270)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:168)
	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:139)
	at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1690)
	at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:234)
	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:179)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-06-23 23:57:03,174 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
{code}",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-06-24 11:04:29,3
13384801,EC: Adopt EC related utility from Hadoop source repository,"It seems only a few key classes are required from Apache Hadoop to be reused. The goal here is investigating the build process and check how they can be re-used. 

One option is just copy the files with some minor updates (e.g. using ConfigurationSource interface instead of o.a.hadoop.Configuration) which would help to reduce dependencies.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-21 07:57:02,1
13384800,Datanode shutdown due to too many bad volumes in CI,"_Acceptance (secure)_ check is frequently failing, usually at S3 tests.  The root cause is that datanodes are shut down due to too many ""bad"" volumes.

{noformat:title=S3 Gateway log}
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
{noformat}

{noformat:title=SCM log}
Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
{noformat}

{noformat:title=Datanode log}
datanode_2  | 2021-06-19 13:26:08,010 [Periodic HDDS volume checker] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2021-06-19 13:36:08,013 [Periodic HDDS volume checker] WARN volume.StorageVolumeChecker: checkAllVolumes timed out after 600000 ms
datanode_2  | 2021-06-19 13:36:08,014 [Periodic HDDS volume checker] WARN volume.MutableVolumeSet: checkAllVolumes got 1 failed volumes - [/data/hdds/hdds]
datanode_2  | 2021-06-19 13:36:08,016 [Periodic HDDS volume checker] INFO volume.MutableVolumeSet: Moving Volume : /data/hdds/hdds to failed Volumes
datanode_2  | 2021-06-19 13:36:08,016 [Periodic HDDS volume checker] ERROR statemachine.DatanodeStateMachine: DatanodeStateMachine Shutdown due to too many bad volumes, check hdds.datanode.failed.data.volumes.tolerated and hdds.datanode.failed.metadata.volumes.tolerated
{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2021-06-21 07:55:10,0
13384781,Suppress logging of ServerNotLeaderException ,"When client tries to figure out leader, when it contacts the first node if it is not a leader, we see this kind of exception in the log.


{code:java}
2021-06-02 07:08:31,760 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from 172.27.161.194:45505
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:38e91e94-b4fe-4307-b3b0-f8c1e7e2d4d7 is not the leader. Suggested leader is Server:quasar-vudtrs-8.quasar-vudtrs.root.hwx.site:9961.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:191)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:92)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:15124)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:533)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887)
{code}
",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-21 06:20:16,2
13384220,Move SCMUpdateProtocol to hdds interface-server package,This is intended for OM/SCM and server only. ,pull-request-available,[],HDDS,Sub-task,Major,2021-06-16 18:56:30,4
13384151,[SCM-HA] SCM start failed with PipelineNotFoundException,"{code:java}
 2021-06-16 03:02:14,478 ERROR org.apache.ratis.statemachine.StateMachine: Terminating with exit status 1: PipelineID=fe572097-8689-42ae-83aa-ba86439c5a0e not found
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=fe572097-8689-42ae-83aa-ba86439c5a0e not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:121)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.getPipeline(PipelineStateManagerV2Impl.java:125)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.updatePipelineState(PipelineStateManagerV2Impl.java:250)
        at jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:168)
        at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:139)
        at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1690)
        at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:234)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:179)
        at java.base/java.lang.Thread.run(Thread.java:834)
2021-06-16 03:02:14,701 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:{code}",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-06-16 11:48:12,3
13384111,Allow to restrict available ReplicationConfig ,"Administrators/vendors may require restricting available replication configs. For example, to disable STANDALONE replication or restrict certain EC scheme.

This patch creates a simple validator which can enforce this validation rule.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-16 08:53:24,1
13384022,java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN,"Steps:
 # place a file under ozone from a secure cluster with kerberos and tls, say under ozone at: /s3v/blog-test1/1.txt
 # do a hdfs dfs cat of the file:

hdfs dfs -cat ofs://ozone1/s3v/blog-test/1.txt
{code}
21/06/11 2100 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
 21/06/11 2100 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
 21/06/11 2100 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
 Jun 11, 2021 901 PM [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider
 INFO: Java 9 ALPN API unavailable (this may be normal)
 Jun 11, 2021 901 PM [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider
 INFO: netty-tcnative unavailable (this may be normal)
 java.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_linux_x86_64_fedora, netty_tcnative_linux_x86_64, netty_tcnative_x86_64, netty_tcnative]
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:104)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:592)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:136)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.defaultSslProvider(GrpcSslContexts.java:225)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:145)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.forClient(GrpcSslContexts.java:94)
 at org.apache.hadoop.hdds.scm.XceiverClientGrpc.connectToDatanode(XceiverClientGrpc.java:181)
 :
 :
 Jun 11, 2021 901 PM [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider
 INFO: Jetty ALPN unavailable (this may be normal)
 java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.JettyTlsUtil.isJettyAlpnConfigured(JettyTlsUtil.java:64)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.findJdkProvider(GrpcSslContexts.java:249)
 :
 :
 cat: Exception getting XceiverClient: [org.apache.hadoop.ozone.shaded.com|http://org.apache.hadoop.ozone.shaded.com/].google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Could not find TLS ALPN provider; no working netty-tcnative, Conscrypt, or Jetty NPN/ALPN available
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-06-15 22:32:14,5
13383918,Avoid usage of lock in listStatus,"    // We don't take a lock in this path, since we walk the
    // underlying table using an iterator. That automatically creates a
    // snapshot of the data, so we don't need these locks at a higher level
    // when we iterate.

We have above snippet in listKeys, in listStatus also we use underlying table iterator, when iterating, we might skip the lock",ScaleTest pull-request-available,['OM'],HDDS,Improvement,Major,2021-06-15 10:44:38,2
13383881,Pipeline creator may miss one-shot run,"If {{BackgroundPipelineCreatorV2}} tries to trigger one-shot run while pipeline creation is in progress (before entering {{wait()}}), then its {{notifyAll()}} call is lost.  This can result in unnecessary wait, as it will only check if it needs to run after the wait is over.

I think this may contribute to tests timing out waiting for SCM safemode exit.",pull-request-available,['SCM'],HDDS,Bug,Major,2021-06-15 07:50:04,0
13383845,Wrong cache key for integration tests,"Cache key for integration tests is incomplete, hash of pom.xml files is missing.

{code:title=https://github.com/apache/ozone/runs/2818911609#step:4:1}
Run actions/cache@v2
  with:
    path: ~/.m2/repository
    key: maven-repo--8-
    restore-keys: maven-repo--8
  maven-repo-
  maven-repo-
...
Cache restored from key: maven-repo--8-
{code}",pull-request-available,['build'],HDDS,Bug,Major,2021-06-15 05:01:06,0
13383783,Adding debug log for block token verification,This can help test verification of block token usage. ,pull-request-available,[],HDDS,Improvement,Minor,2021-06-14 18:42:55,4
13383732,HTML report missing from acceptance results,"HTML report files are missing from acceptance test results:

{code}
cp: cannot stat '/mnt/ozone/hadoop-ozone/dev-support/checks/../../../target/acceptance/log.html': No such file or directory
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-06-14 13:32:56,0
13383699,Container report processing is single threaded,The container report handler is single thread in the SCM Event queue and the process is synchronized per datanode as well. Need to explore if it can be made multithreaded.,ScaleTest pull-request-available,['SCM'],HDDS,Improvement,Major,2021-06-14 10:00:38,2
13383398,Method not found: allocateBlock - when tracing is enabled,"Jaeger tracing is broken for key creation requests.

{code:title=steps}
cd hadoop-ozone/dist/target/ozone-*/compose/ozone
export COMPOSE_FILE=docker-compose.yaml:monitoring.yaml
# console 1
OZONE_REPLICATION_FACTOR=3 ./run.sh
# console 2
docker-compose exec -T scm ozone freon ockg -n1 -t1
{code}

{noformat:title=log}
om_1          | java.lang.NoSuchMethodException: Method not found: allocateBlock
om_1          | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:65)
om_1          | 	at com.sun.proxy.$Proxy36.allocateBlock(Unknown Source)
om_1          | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.allocateBlock(OMKeyRequest.java:130)
om_1          | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.preExecute(OMKeyCreateRequest.java:151)
om_1          | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:139)
om_1          | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om_1          | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122)
{noformat}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-06-11 11:24:54,0
13383350,Remove unncessary log added durig HDDS-5263,"        LOG.info(""bharat starting from sm"");",pull-request-available,[],HDDS,Bug,Minor,2021-06-11 08:19:55,2
13383275,Remove getRequestType method from OM request classes.,Follow up from HDDS-5244. The method is no longer needed.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-06-10 21:11:37,5
13382839,Avoid unncessary report processing log messages in follower,"Avoid unncessary logging in SCM followe during report processing.
{code:java}
scm3_1      | 2021-06-09 05:10:34,386 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@74c0ee39, cost 799.6us
scm3_1      | 2021-06-09 05:10:34,388 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3_1      |   id: ""c499d751-b5c7-4ddf-b3ab-b71c75128fca""
scm3_1      |   uuid128 {
scm3_1      |     mostSigBits: -4280153224896885281
scm3_1      |     leastSigBits: -5500101187051810870
scm3_1      |   }
scm3_1      | }
scm3_1      | isLeader: true
scm3_1      | bytesWritten: 0
scm3_1      |  from dn=74c73b20-b5c0-408d-ae95-2687d1835546{ip: 192.168.0.12, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}.
scm3_1      | org.apache.ratis.protocol.exceptions.NotLeaderException: Server 6d66a209-5955-4ddb-9fbd-8cc2f578a265@group-D6BFC1238401 is not the leader 68245269-1a37-4f90-a5e4-21c8c6b7e63d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)
scm3_1      | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)
scm3_1      | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
scm3_1      | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)
scm3_1      | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:214)
scm3_1      | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)
scm3_1      | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)
scm3_1      | 	at com.sun.proxy.$Proxy14.updatePipelineState(Unknown Source)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl.openPipeline(PipelineManagerV2Impl.java:271)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:124)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:91)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:50)
scm3_1      | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
scm3_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1      | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm2_1      | 2021-06-09 05:10:34,368 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2_1      |   id: ""c499d751-b5c7-4ddf-b3ab-b71c75128fca""
scm2_1      |   uuid128 {
scm2_1      |     mostSigBits: -4280153224896885281
scm2_1      |     leastSigBits: -5500101187051810870
scm2_1      |   }
scm2_1      | }
scm2_1      | isLeader: true
scm2_1      | bytesWritten: 0
scm2_1      |  from dn=74c73b20-b5c0-408d-ae95-2687d1835546{ip: 192.168.0.12, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}.
{code}
",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-09 05:13:02,2
13382650,Intermittent failure in TestOzoneManagerDoubleBufferWithOMResponse,"{code:title=https://github.com/elek/ozone-build-results/blob/9d152107f3e9deac65180cb23b7956433eb1c92a/2021/06/02/8212/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.txt#L5-L11}
testDoubleBufferWithMixOfTransactions(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 0.221 s  <<< FAILURE!
java.lang.AssertionError: expected:<16> but was:<1>
  ...
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBufferWithMixOfTransactions(TestOzoneManagerDoubleBufferWithOMResponse.java:197)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-06-08 08:09:55,0
13382634,BootStrapped SCM fails to bootstrap if it connects to another bootstrapped SCM first.,"GetSCMCertificate can happen non-leader SCM, as rootCA is only run on primary SCM.
So, when an SCM is bootstrapped, let's say it connects first to a bootstrapped SCM, we fail with a SCMSecurityResponse with status set to NOT_A_PRIMARY_SCM. As we return with a response, failOver will not happen.

*SCMSecurityProtocolClientSideTranslatorPB*
{code:java}
  private SCMSecurityResponse handleError(SCMSecurityResponse resp)
      throws SCMSecurityException {
    if (resp.getStatus() != SCMSecurityProtocolProtos.Status.OK) {
      throw new SCMSecurityException(resp.getMessage(),
          SCMSecurityException.ErrorCode.values()[resp.getStatus().ordinal()]);
    }
    return resp;
  }
{code}

To solve this issue, one possible solution is on server check if it is SCMSecurityException with errorCode NOT_A_PRIMARY_SCM return a RetriableWithFailOverException. In this way, FailOverProxyProvider performs failOver and Retry to the next SCM.

The exception message is available in comments.
",pull-request-available,"['SCM HA', 'Security']",HDDS,Bug,Blocker,2021-06-08 06:48:40,2
13382476,Intermittent failure in SCM Ratis integration test,"Some integration tests intermittently fail due to mini cluster not existing safe mode within 2 minutes timeout.  The problem is that pipeline creation interval is also 2 minutes.  It may happen that pipeline is created only while the cluster is being shut down due to timeout.

{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot-output.txt}
2021-06-02 03:21:03,005 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(151)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
...
2021-06-02 03:21:04,007 [Listener at 127.0.0.1/40677] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(224)) - Nodes are ready. Got 3 of 3 DN Heartbeats.
...
2021-06-02 03:22:59,107 [Listener at 127.0.0.1/40677] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(443)) - Shutting down the Mini Ozone Cluster
...
2021-06-02 03:23:03,031 [6d4e3dd1-e161-4c07-861b-817db46a0427@group-0D81E0660BF9-StateMachineUpdater] INFO  pipeline.PipelineStateManager (PipelineStateManagerV2Impl.java:addPipeline(101)) - Created pipeline Pipeline ... RATIS/THREE ...
{noformat}

{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt}
org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot  Time elapsed: 146.994 s  <<< ERROR!
java.util.concurrent.TimeoutException: 
  ...
  at org.apache.hadoop.ozone.MiniOzoneClusterImpl.waitForClusterToBeReady(MiniOzoneClusterImpl.java:217)
{noformat}

Related test failures:

{noformat}
2021/05/26/8113/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis.txt
2021/05/26/8118/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA.txt
2021/05/27/8142/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
2021/05/30/8164/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestOzoneManagerRestInterface.txt
2021/05/31/8166/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMSnapshot.txt
2021/05/31/8177/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestPipelineClose.txt
2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
2021/06/02/8193/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA.txt
2021/06/02/8211/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMSnapshot.txt
2021/06/02/8217/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
2021/06/07/8299/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
{noformat}",pull-request-available,"['SCM HA', 'test']",HDDS,Bug,Major,2021-06-07 12:54:39,0
13382401,Enhance freon streaing generator to support multiple threads,Please see: https://github.com/apache/ozone/pull/2306,pull-request-available,[],HDDS,Improvement,Major,2021-06-07 08:00:30,1
13382147,InterSCM protocol should be server-only,"InterSCM protocol is defined in client interface, but is concerned only with server-to-server communication.",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-04 13:18:55,0
13382124,OzoneDelegationTokenSecretManager breaks the interface contract of S3SecretManager,"S3SecretManager is a generic interface which supposed to return with the secret key for one specific AWS access key id.

It's a generic interface which may have multiple implementation.

Unfortunately, it's not possible to use any implementation as OzoneDelegationTokenSecretManager does an explicit cast to retrieve the MetadataManager.

Instead of breaking the abstract contract of interface it seems to be better to directly inject the required MetadataManager to the  OzoneDelegationTokenSecretManager which makes it possible to use an implementation.",pull-request-available,[],HDDS,Bug,Trivial,2021-06-04 10:39:31,1
13381838,testCRLStatusReportPublisher fails to create CRLInfo,"{code:title=https://github.com/apache/ozone/runs/2732634889#step:4:852}
[INFO] Running org.apache.hadoop.ozone.container.common.report.TestReportPublisher
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.665 s <<< FAILURE! - in org.apache.hadoop.ozone.container.common.report.TestReportPublisher
[ERROR] testCRLStatusReportPublisher(org.apache.hadoop.ozone.container.common.report.TestReportPublisher)  Time elapsed: 1.518 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.hadoop.hdds.security.x509.crl.CRLInfo.<init>(CRLInfo.java:47)
	at org.apache.hadoop.hdds.security.x509.crl.CRLInfo.<init>(CRLInfo.java:38)
	at org.apache.hadoop.hdds.security.x509.crl.CRLInfo$Builder.build(CRLInfo.java:219)
	at org.apache.hadoop.ozone.container.common.report.TestReportPublisher.testCRLStatusReportPublisher(TestReportPublisher.java:188)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-06-03 06:34:22,0
13381681,Handle SIGTERM to ensure clean shutdown of OM/DN/SCM,"Handle SIGTERM 15 with shutdown hook to properly/clean shutdown OM/DN/SCM.

In this way in OM HA/DN/SCM HA snapshot will be called and pending transactions will be flushed to DB.

",pull-request-available,"['Ozone Datanode', 'Ozone Manager']",HDDS,Improvement,Major,2021-06-02 11:45:39,2
13381669,Handle SIGTERM to ensure clean shutdown of SCM,"Handle SIGTERM 15 with shutdown hook to properly/clean shutdown SCM.

In this way in SCM HA, the snapshot will be called and pending transactions will be flushed to DB.

",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-06-02 10:22:49,2
13381420,[SCM-HA] SCM start failed with PipelineNotFoundException,"{code:java}
scm.log 
2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:028fed4a-0087-4b70-b6e3-11f18d739094
2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:a4b76016-dc24-47f2-a3ff-03c309fdcf9b
2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:ed9d4872-166d-41c6-96ab-437a44e4168b
2021-05-27 09:55:42,199 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-05-27T09:55:42.189Z].
2021-05-27 09:55:54,426 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:028fed4a-0087-4b70-b6e3-11f18d739094, CreationTimestamp2021-05-27T09:55:42.189Z] moved to OPEN state
2021-05-27 10:06:45,920 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=cd4a2a77-9715-4437-8d1d-3618a2c93103, PipelineID=ca6100b9-b42c-4b77-bef5-35a9b1e725f2, PipelineID=875b2073-4034-4374-bba6-39011294a280]
2021-05-27 10:06:45,932 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:DORMANT, leaderId:a4b76016-dc24-47f2-a3ff-03c309fdcf9b, CreationTimestamp2021-05-27T09:55:42.189Z] moved to CLOSED state
2021-05-27 10:06:57,921 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=cd4a2a77-9715-4437-8d1d-3618a2c93103, PipelineID875b2073-4034-4374-bba6-39011294a280, PipelineID=2878c722-84dc-40f9-b1c1-46ed0f8bcdd7]
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Scrubbing pipeline: id: PipelineID=875b2073-4034-4374-bba6-39011294a280 since it stays at CLOSED stage.
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode 028fed4a-0087-4b70-b6e3-11f18d739094
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode a4b76016-dc24-47f2-a3ff-03c309fdcf9b
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode ed9d4872-166d-41c6-96ab-437a44e4168b
2021-05-27 10:07:41,075 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:a4b76016-dc24-47f2-a3ff-03c309fdcf9b, CreationTimestamp2021-05-27T09:55:42.189Z] removed.
{code}
The logs indicate that, a pipeline got created, moved to open state, and then one of the datanodes went stale, thereby the pipeline moved to closed state. The pipeline got scrubbed by the pipeline scrubber and got deleted. 
{code:java}
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Scrubbing pipeline: id: PipelineID=875b2073-4034-4374-bba6-39011294a280 since it stays at CLOSED stage.{code}
Next update for the pipeline to be moved to close state as a part of  report from other datanodes in the pipeline will fail as the pipeline is removed from scm memory/db and hence scm terminates.

The solution would be to ignore PipelineNotFoundException in PipelineStateManagerV2Impl#updatePipelineState.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-06-01 10:00:08,3
13381270,Relocate classes copied from Hadoop,"Ozone has some test classes partially copied from Hadoop, located in the original packages.  Relocate these to avoid classpath conflict.",pull-request-available,[],HDDS,Task,Major,2021-05-31 11:40:30,0
13380905,Make XceiverClientManager creation when necessary in ContainerOperationClient,"ContainerOperation Client creates XceiverClientManager.

XceiverClientManager requires to getCA list.


{code:java}
      manager = new XceiverClientManager(conf,
          conf.getObject(XceiverClientManager.ScmClientConfig.class),
          caCertificates);
{code}

We can avoid listCA which is not required for most admin commands. It is required only for ChunkKeyHandler.

This will help when ACLS are configured for SCM security protocol where only admin/service principals can make calls to the SCMSecurityProtocol server, then we don't need to add all the users to them to make these commands work.

As for few of the commands like pipeline list, safe mode status we don't require admin privilege.
",pull-request-available,[],HDDS,Improvement,Major,2021-05-28 04:41:43,2
13380627,"Recon shows operational status as ""DECOMMISSIONING"" for ""DECOMMISSIONED"" DNs","Decommissioned nodes have ""DECOMMISSIONING"" operational status in recon.


RCA
Recon relies on DN heartbeats to understand Node operational state. If the node goes down before it reports itself as DECOMMISSIONED, then there is a loss of information on Recon side. It is the SCM that moves a node form DECOMMISSIONING to DECOMMISSIONED state first, then the Datanode persists the state change locally, and then heartbeats with the new state to SCM & Recon subsequently.  If the DN is shutdown before it can heartbeat the state change to Recon, then Recon lives with the stale information.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2021-05-26 21:20:07,5
13380541,Use built-in cancel support for duplicates,GitHub Actions now provides support to cancel duplicate workflows.  This usage of the {{cancel-workflow-runs}} action can be replaced.,pull-request-available,['build'],HDDS,Task,Major,2021-05-26 12:58:31,0
13380423,Datanode Report Publisher publishes one extra report after DN shutdown,"There is one extra report that gets published by Datanodes even after shutdown. Ideally, this should be avoided and DN should not publish reports after shutdown. ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-05-25 23:48:29,6
13380375,Handle unsecure cluster convert to secure cluster for SCM,"In SCM sub-ca certs are setup during init, if a cluster is converted to secure later, in else part of the scmInit, we need to initialize security.

OM handled this scenario, SCM also needs a similar fix.
https://github.com/apache/ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L963",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-25 15:29:43,2
13380303,Make ozonefs.robot execution repeatable,"Most of our smoketests can be executed multiple times, which helps us to debug the tests locally.

Unfortunately it's not the case ozonefs/ozonefs.robot. This test uses hard-coded bucket/volume names instead of using randomized bucket names.

For example:

{code}
docker-compose up -d --scale datanode=3
docker-compose exec scm bash
robot smoketest/ozonefs/ozonefs.robot

robot smoketest/ozonefs/ozonefs.robot
{code}

The second execution fails which makes harder to debug test problems.

It seems to be better to follow existing pattern and adding a random prefix to the user bucket names.",pull-request-available,[],HDDS,Bug,Major,2021-05-25 10:13:08,1
13379897,SCM should send token for CloseContainer command,Close container command from SCM fails due to lack of token.,pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2021-05-22 07:52:43,0
13379836,SCM may stay in safe mode forever due to incorrect open pipeline count,"After an unclean shutdown, SCM may never come out of the safe mode.
Attached a document to explain the problem and the proposal.
More description with log info is available in the comments.
",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-05-21 18:40:08,2
13379484,Avoid SCM call to get CA certs in non-HA from OM.,"OM loads up its signed cert, rootCA cert and CA cert during startup.
So to get CA list in OM, we can use certClient and get them, in this way we can avoid unncessary network call to OM.",pull-request-available,[],HDDS,Improvement,Major,2021-05-20 10:24:47,2
13379450,Fix fall back of config in SCM HA Cluster,"If config is appended with serviceId and nodeId use that, else fall back to config appended without service id and node id. If that is also not defined, fallback to a default value.

*Example:*
ozone.scm.grpc.port = 9898 is set

We should use this port for SCM Grpc Service, as there is no port config with serviceid and nodeid defined.

Current code behavior is if port config with service id and node id not defined read from the default value.


The problematic code is

{code:java}
        String ratisPortKey = ConfUtils.addKeySuffixes(OZONE_SCM_RATIS_PORT_KEY,
            serviceId, nodeId);
int ratisPort = conf.getInt(ratisPortKey, OZONE_SCM_RATIS_PORT_DEFAULT);
{code}

In this way fix similarly for all RPC services.
",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-20 07:25:30,2
13379329,Build integration tests with Maven cache,"Currently only _integration_ check uses the {{ozone-builder}} Docker image.  The image is getting outdated and takes some effort to periodically update.  Investigate if _integration_ can be changed to build with Maven cache, like _unit_.

Also, it seems execution of Hugo (which is available in the image) is sometimes getting stuck recently:

* https://github.com/apache/ozone/runs/2607134035?check_suite_focus=true#step:4:1393
* https://github.com/apache/ozone/runs/2601041142?check_suite_focus=true#step:4:1436
* https://github.com/apache/ozone/runs/2600238841?check_suite_focus=true#step:4:1436
* https://github.com/adoroszlai/hadoop-ozone/runs/2601105245?check_suite_focus=true#step:4:1436
* https://github.com/adoroszlai/hadoop-ozone/runs/2621383226#step:4:1393",pull-request-available,['build'],HDDS,Improvement,Critical,2021-05-19 17:01:09,0
13379260,EC: Create ECReplicationConfig on client side based on input string,"HDDS-5073 improves the existing ""ozone sh"" client to support ReplicationConfig. The input string is parsed to ReplicationConfig by the constructors of the ReplicationConfig classes with string parameters.

After merging this improvement to the EC branch we need to implement the same constructor for ECReplicationConfig.

There are multiple options here:

 1. Create an enum with ALL the possible ECReplicationConfig
 2. Use meaningful programmatic validation rules.

During the EC sync we agreed that 2nd option can be more flexible as we may have very huge configuration matrix with all the EC parameters.
 ",pull-request-available,[],HDDS,Sub-task,Major,2021-05-19 09:55:47,1
13379239,Wait for ever to obtain CA list which is needed during OM/DN startup,"Right now during OM/DN startup we wait a total duration of 300seconds and wait in between each attempt for 10seconds to obtain CA list and check the recieved CA list size is as expected count.

So, in case DN's are started before and SCM's have not completed bootstrap after 300 seconds  DN/OM will fail to start. This Jira is to add support for retry forever and with fixed sleep, in this way, DN's can come up even if they are started before bootstrapping SCMs.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-19 08:47:36,2
13379159,Fix OzoneContainer TLS configuration,The TLS configuration is used by CreatePipelineCommandHandler on datanode to create pipeline across datanodes. It currently does not allow enable/disable like other TLS configurations and does not use mTLS when it is needed here. ,pull-request-available,[],HDDS,Bug,Major,2021-05-18 23:38:23,4
13379091,Allow multiple OM request versions to be supported at same layout version (HDDS-2939).,Remove use of instance factory in OM request handling switch. Added a new point cut in the OM aspect that makes sure new OM requests can be brought in through a layout version.,pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Major,2021-05-18 15:55:17,5
13379078,Skip failing acceptance suite by default,"Acceptance tests are split into a few suites, which can be activated by setting {{OZONE_ACCEPTANCE_SUITE}}.  CI checks execute 3 splits for faster feedback:

 * {{secure}}
 * {{unsecure}}
 * {{misc}} + anything without {{suite}}

There are some other tests, tagged {{suite:failing}}, which exist only to test the behavior of the acceptance test runner when failures happen.

The goal of this task is to skip the {{failing}} suite by default, i.e. when acceptance tests are run without suite filter.  This would allow running all tests in sequence (e.g. in nightly job) while retaining the ability to run these special tests by setting {{OZONE_ACCEPTANCE_SUITE=failing}}.",pull-request-available,['test'],HDDS,Improvement,Major,2021-05-18 14:55:52,0
13378806,Disable animal-sniffer maven plugin,Please see: https://github.com/apache/ozone/pull/2252,pull-request-available,[],HDDS,Improvement,Major,2021-05-17 12:30:31,1
13378799,Add SSL support to the Ozone streaming API,"HDDS-5142 will introduce a new streaming API for closed container replication / snapshot download and other data movement.

For server2server communication we need to support mTLS. We should configure pure mTLS on the netty server ",pull-request-available,[],HDDS,Improvement,Major,2021-05-17 12:08:35,1
13378785,Require block token for more operations,"Require block token in datanode for the following operations:

* CompactChunk (currently unsupported operation)
* DeleteBlock
* DeleteChunk
* GetCommittedBlockLength
* ListChunk (currently unsupported operation)

Require container token for ListBlock (currently unsupported operation).

Do not require container token for ListContainer (currently unsupported operation), as it does not have container ID in the request.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2021-05-17 10:59:41,0
13378748,SCM subsequent init failed when previous scm init failed,"The problem is SCM init because we use a new clusterID when the version writing failed.


{code:java}
Could not initialize SCM version file
java.io.IOException: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING
	at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
	at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
	at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:71)
	at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:354)
	at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:371)
	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.initialize(SCMRatisServerImpl.java:115)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:925)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:173)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at picocli.CommandLine.executeUserObject(CommandLine.java:1952)
	at picocli.CommandLine.access$1100(CommandLine.java:145)
	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2326)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2291)
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:2465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:57)
Caused by: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING
{code}
",pull-request-available,['SCM HA'],HDDS,Sub-task,Critical,2021-05-17 08:45:38,2
13378478,replace HADOOP_ with OZONE_ prefix in the ozone-runner docker image,Please see: https://github.com/apache/ozone-docker-runner/pull/7,pull-request-available,[],HDDS,Improvement,Major,2021-05-14 12:19:42,1
13378472,Update copyright year of NOTICE in all Ozone repositories,"As [~jmclean]  [reported on the mailing list |https://lists.apache.org/thread.html/rb2bbf79cf136b1c129ff7205e319223c2a780af49ed270031d676ce2%40%3Cdev.ozone.apache.org%3E]:

bq. While it may not matter in our lifetime, copyright can expire so it would be best to put the published year in the NOTICE file rather than ""Copyright 2006 and onwards"".

We should update all of our notice files to 2021:

{code}
Copyright 2021 The Apache Software Foundation
{code}",pull-request-available,[],HDDS,Bug,Major,2021-05-14 11:41:35,1
13377890,ozone freon randomkeys failed after leader SCM node is down,"under .../compose/ozone-ha, create a HA cluster:

{code}
docker-compose up -d --scale datanode=3
{code}

Initial SCM roles as following:

{code}
bash-4.2$ ozone admin scm roles
[scm1:9865:FOLLOWER, scm2:9865:FOLLOWER, scm3:9865:LEADER]
{code}

Running freon random key generator as following:

{code}
ozone freon randomkeys --numOfVolumes=10 --numOfBuckets 50 --numOfKeys 50  --replicationType=RATIS --factor=THREE
{code}

While freon randomkeys was running, put all SCM nodes under blockade and stop leader SCM node:

blockade status:

{code}
NODE            CONTAINER ID    STATUS  IP              NETWORK    PARTITION  

                18f9c1e2d52f    UP      172.31.0.9      NORMAL                

ozone-ha_scm1_1                                                                

                25c74f0a9271    UP      172.31.0.6      NORMAL                

ozone-ha_scm2_1                                                                

                8808d10ccb3a    DOWN                    UNKNOWN               

ozone-ha_scm3_1
{code}
 

freon randomkeys failed with following error message:

Some test result msg as following:

{code}
6:00:30,131 [pool-2-thread-3] ERROR freon.RandomKeyGenerator: Exception while adding key: key-21-80493 in bucket: bucket-44-63818 of volume: vol-1-95998.
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No Route to Host from  om1/172.31.0.11 to scm3:9863 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  [http://wiki.apache.org/hadoop/NoRouteToHost]
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:604)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.openKey(OzoneManagerProtocolClientSideTranslatorPB.java:595)
at org.apache.hadoop.ozone.client.rpc.RpcClient.createKey(RpcClient.java:756)
at org.apache.hadoop.ozone.client.OzoneBucket.createKey(OzoneBucket.java:502)
at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:703)
at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:86)
at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:621)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at java.base/java.lang.Thread.run(Thread.java:834)
 44.10% |?????????????????????????????????????????????                                                        |  11024/25000 Time: 0:09:002021-05-11 06:00:37,231 [pool-2-thread-7] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-EA7B54107DBD->4c51bca8-cc0a-4c20-84dd-b5a7cb18c4ac
2021-05-11 06:00:37,231 [pool-2-thread-7] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
 100.00% |?????????????????????????????????????????????????????????????????????????????????????????????????????|  25000/25000 Time: 0:15:39
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No Route to Host from  om1/172.31.0.11 to scm:9863 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  [http://wiki.apache.org/hadoop/NoRouteToHost]
***************************************************
Status: Failed
Git Base Revision: 7a3bc90b05f257c8ace2f76d74264906f0f7a932
Number of Volumes created: 10
Number of Buckets created: 500
Number of Keys added: 24991
Ratis replication factor: THREE
Ratis replication type: RATIS
Average Time spent in volume creation: 00:00:00,114
Average Time spent in bucket creation: 00:00:01,263
Average Time spent in key creation: 00:02:48,698
Average Time spent in key write: 00:00:04,216
Total bytes written: 255907840
Total Execution time: 00:15:39,968
***************************************************
{code}

In this case, I'd expect the freon test would still finish successfully.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-11 15:25:50,2
13377849,Bump logical release name of Ozone 1.2,Please see: https://github.com/apache/ozone/pull/2238,pull-request-available,[],HDDS,Improvement,Trivial,2021-05-11 11:50:33,1
13377689,Intermittent failure in TestRatisPipelineProvider#testCreatePipelineWithFactorThree,"{{TestRatisPipelineProvider#testCreatePipelineWithFactorThree}} fails intermittently due to same set of nodes being selected for two 3-node pipelines.

2021/02/01/5639/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/02/11/5875/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/02/24/6111/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/04/06/7045/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/04/07/7095/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/04/09/7175/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt",pull-request-available,['test'],HDDS,Bug,Major,2021-05-10 21:08:44,0
13377621,Make admin check work for SCM HA cluster,"By default, the user started principal is added to scmAdminUsernames.


{code:java}
    String scmUsername = UserGroupInformation.getCurrentUser().getUserName();
    if (!scmAdminUsernames.contains(scmUsername)) {
      scmAdminUsernames.add(scmUsername);
    }
{code}


In HA cluster, when kinit with scm2 principal when scm1 is leader, we get access denied as we check getUserName() and also when adding to adminlist we use getUserName.

In OM we don't have this kind of issue, as getShortUserName() is used.


{code:java}
  String omSPN = UserGroupInformation.getCurrentUser().getShortUserName();
    if (!ozAdmins.contains(omSPN)) {
      ozAdmins.add(omSPN);
    }
{code}

And during admin check it compares with both userName and shortUserName.


{code:java}
if (ozAdmins.contains(callerUgi.getShortUserName()) ||
        ozAdmins.contains(callerUgi.getUserName()) ||
{code}

",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-10 12:18:27,2
13377620,Missing type-level Javadoc comments,"Javadoc comments used to be required on all public types (classes, interfaces, etc.).  In Checkstyle 8.20

bq. functionality for validating missing javadocs was split into a new check MissingJavadocType.

So missing javadoc on new types introduced since 2020/Oct/20 (when HDDS-4306 upgraded Checkstyle from 8.19 to 8.29) were not caught, and now we have ~50 violations.

{noformat}
hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/TableCache.java
 114: Missing a Javadoc comment.
hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/NodeDetails.java
 25: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMCertStore.java
 304: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/balancer/ContainerBalancer.java
 34: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMDBCheckpointProvider.java
 34: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/MockSCMHADBTransactionBuffer.java
 28: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/HASecurityUtils.java
 70: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/LongCodec.java
 24: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/EnumCodec.java
 28: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/BooleanCodec.java
 24: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/ListCodec.java
 28: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/StringCodec.java
 24: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/Codec.java
 23: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/CodecFactory.java
 32: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/GeneratedMessageCodec.java
 27: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMContext.java
 180: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMHANodeDetails.java
 63: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeUsageInfo.java
 27: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogStateManagerImpl.java
 44: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogStateManager.java
 29: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMHAConfiguration.java
 45: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMServiceManager.java
 27: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSequenceIDGenerator.java
 29: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBufferFactory.java
 42: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/utils/BufferUtils.java
 27: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBufferImpl.java
 23: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/ha/ConfUtils.java
 28: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/utils/ClientCommandsUtils.java
 23: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ha/RetriableWithNoFailoverException.java
 22: Missing a Javadoc comment.
hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/common/TestChecksumImplsComputeSameValues.java
 33: Missing a Javadoc comment.
hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMNodeInfo.java
 45: Missing a Javadoc comment.
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/statemachine/background/BlockDeletingService.java
 112: Missing a Javadoc comment.
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationSupervisor.java
 120: Missing a Javadoc comment.
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationServer.java
 126: Missing a Javadoc comment.
hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java
 130: Missing a Javadoc comment.
hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/replication/TestSimpleContainerDownloader.java
 41: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/containergenerator/BaseGenerator.java
 29: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkCRCBatch.java
 54: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkCRCStreaming.java
 71: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/debug/ExportContainer.java
 61: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/signature/SignatureInfo.java
 110: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3Owner.java
 27: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3BucketAcl.java
 68: Missing a Javadoc comment.
 99: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3Acl.java
 38: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/signature/TestAuthorizationV4QueryParser.java
 30: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/TestEmptyContentTypeFilter.java
 27: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneHAClusterImpl.java
 659: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHAKeyDeletion.java
 29: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/read/TestInputStreamBase.java
 60: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMSnapshot.java
 40: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemMissingParent.java
 42: Missing a Javadoc comment.
hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/ContainerHistory.java
 23: Missing a Javadoc comment.
hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/MultipartCryptoKeyInputStream.java
 34: Missing a Javadoc comment.
{noformat}",pull-request-available,[],HDDS,Bug,Major,2021-05-10 12:13:16,0
13377608,Allow suppressing deprecation warning for HADOOP_ variables,HDDS-4525 deprecated {{HADOOP_\*}} variables in Ozone in favor of corresponding {{OZONE_\*}} variables.  We should allow suppressing the warning for such variables to make upgrade easier.,pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2021-05-10 10:54:06,0
13377602,Use scm#checkLeader before processing client requests ,"Right now to check leader we use ScmContext#isLeader, this gets updated by notifyLeaderChanged.

But SCM server should start accepting requests when it is leader and isLeaderReady. 

We need isLeaderReady also because Statemachine should apply all the log committed transactions to start accepting requests.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-10 10:23:13,2
13377594,Fix scm roles command if one of the host is unresolvable,"{code:java}
while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/172.19.0.7:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
com.google.protobuf.ServiceException: java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: ""scm1"":9860; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost, while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1:9860 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7fc05067-c144-4c14-880b-e47f1e40599b is not the leader. Suggested leader is Server:scm3:9860.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:191)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:144)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:43838)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/172.19.0.2:9860 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.net.UnknownHostException): scm1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
	at java.base/java.net.InetAddress.getByName(InetAddress.java:1252)
	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.getRatisRoles(SCMRatisServerImpl.java:233)
	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getScmInfo(SCMClientProtocolServer.java:579)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getScmInfo(StorageContainerLocationProtocolServerSideTranslatorPB.java:506)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:149)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:43838)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/172.19.0.7:9860 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
{code}

If one of the host is unresolvable roles command keep on failing.
",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-10 09:51:53,2
13377580,Create docker image for Apache Ozone 1.1.0 release,Please see: https://github.com/apache/ozone-docker/pull/19,pull-request-available,[],HDDS,Improvement,Major,2021-05-10 08:35:29,1
13377573,Use https for JBoss repo,[Maven 3.8.1|https://maven.apache.org/docs/3.8.1/release-notes.html] blocks http repos by default.  Need to update JBoss repo definition in {{pom.xml}} to https.,pull-request-available,['build'],HDDS,Bug,Major,2021-05-10 07:07:54,0
13377090,Intermittent failure in TestOzoneRpcClient due to volume name conflict,"{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/05/06/7723/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient.txt}
Tests run: 80, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 175.065 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient
testListVolume  Time elapsed: 0.086 s  <<< FAILURE!
java.lang.AssertionError: expected:<20> but was:<21>
  ...
  at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testListVolume(TestOzoneRpcClientAbstract.java:1867)
{noformat}

{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/05/06/7723/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient-output.txt}
2021-05-06 13:33:08,991 [Time-limited test] INFO  rpc.RpcClient (RpcClient.java:createVolume(320)) - Creating Volume: vol-71454, with jenkins1001 as owner and space quota set to -1 bytes, counts quota set to -1
...
2021-05-06 13:33:09,650 [Time-limited test] INFO  rpc.RpcClient (RpcClient.java:createVolume(320)) - Creating Volume: vol-714-a-0-78727, with jenkins1001 as owner and space quota set to -1 bytes, counts quota set to -1
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2021-05-06 20:05:33,0
13376628,PSA: hugo 0.83.0 / 0.83.1 broke hadoop-hdds/docs/dev-support/bin/generate-site.sh,"After upgrading my hugo to 0.83.1, a maven build would fail at hadoop-hdds-docs for me.

{code}
[INFO] --- exec-maven-plugin:1.3.1:exec (default) @ hadoop-hdds-docs ---
Error: Error building site: "".../hadoop-hdds/docs/content/interface/ReconApi.md:1:1"": [BUG] goldmark: runtime error: slice bounds out of range [3503:3501]: create an issue on GitHub attaching the file in: /var/folders/yp/sxzthlnd45vdwcwvdldf_12r0000gp/T/hugo_bugs/goldmark_267fdfa526d051bdccb7cf9a98e0772d.txt
Start building sites …
goroutine 60 [running]:
runtime/debug.Stack(0x5f70c80, 0x6b7c280, 0xc00117c310)
	runtime/debug/stack.go:24 +0x9f
github.com/gohugoio/hugo/markup/goldmark.(*goldmarkConverter).Convert.func1(0xc000a2e6c0, 0xc000d25500, 0x2b24, 0x3038, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
{code}

ReconApi.md itself hasn't changed since Nov 2020, it's also unlikely that other markdowns have caused this. This issue can be simply repro'ed with:

{code}
./hadoop-hdds/docs/dev-support/bin/generate-site.sh
{code}

on hugo 0.83.0 or 0.83.1.

After reverting hugo to 0.82.1 (homebrew build), the problem goes away for me.

{code}
$ hugo version
hugo v0.82.1+extended darwin/amd64 BuildDate=unknown
{code}

PSA: Do not upgrade hugo to 0.83.0 or 0.83.1 if you want to compile hadoop-hdds-docs, it is still broken right now.
",pull-request-available,['build'],HDDS,Task,Major,2021-05-04 19:58:31,1
13376578,Replace GRPC based closed-container replication with Netty based streaming,"Today the closed containers are copied between datanodes as one big tar(.gz) file. Each datanode runs a GrpcReplicationService (with a grpc server) and when the SCM asks the destination-datanode to replicate data, it connects to the source datanode and retrieves the data.

This protocol is based on GRPC and very simple. After the first request (download(containerid)) the full container is streamed as a tar file in smaller chunks.

However, this protocol doesn't have any back-pressure  / traffic control handling. After the first request the FULL 5g container is sent back. 

This approach can fill up the netty buffers very easy:

{code}
	Exception in thread ""grpc-default-executor-0"" org.apache.ratis.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 2097152 byte(s) of direct memory (used: 3651141911, max: 3652190208)
	at org.apache.ratis.thirdparty.io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:802)
	at org.apache.ratis.thirdparty.io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:731)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:632)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:607)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:202)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.tcacheAllocateNormal(PoolArena.java:186)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:136)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:126)
	at org.apache.ratis.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:395)
	at org.apache.ratis.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
	at org.apache.ratis.thirdparty.io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:123)
	at org.apache.ratis.thirdparty.io.grpc.netty.NettyWritableBufferAllocator.allocate(NettyWritableBufferAllocator.java:51)
	at org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writeKnownLengthUncompressed(MessageFramer.java:227)
	at org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writeUncompressed(MessageFramer.java:168)
	at org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writePayload(MessageFramer.java:141)
	at org.apache.ratis.thirdparty.io.grpc.internal.AbstractStream.writeMessage(AbstractStream.java:65)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl.sendMessageInternal(ServerCallImpl.java:167)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl.sendMessage(ServerCallImpl.java:149)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onNext(ServerCalls.java:365)
	at org.apache.hadoop.ozone.container.replication.GrpcOutputStream.flushBuffer(GrpcOutputStream.java:124)
	at org.apache.hadoop.ozone.container.replication.GrpcOutputStream.write(GrpcOutputStream.java:90)
	at org.apache.hadoop.ozone.freon.ContentGenerator.write(ContentGenerator.java:76)
	at org.apache.hadoop.ozone.freon.ClosedContainerStreamGenerator.copyData(ClosedContainerStreamGenerator.java:19)
	at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:56)
	at org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolServiceGrpc.java:219)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2021-05-04 16:37:47,996 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:37:48,998 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:37:49,998 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)

{code}

This can be reproduced locally with a simple freon test. (See the code here: https://github.com/elek/ozone/tree/grpc-push)

The new freon test starts a GrpcServer and client. On server side the source is replaced with a simple `ContainerReplicationSource` which generates random 5g datastream (instead of reading container data from disk).

On the client side the replicator just downloads the container to the tmp location, but it's not moved to the final location.

This test works well for one container, but the test clearly shows that the full container data is streamed at the very beginning:

(Duplicated lines are removed)

{code}
2021-05-04 16:21:04,281 INFO  replication.DownloadAndDiscardReplicator (DownloadAndDiscardReplicator.java:replicate(62)) - Starting replication of container 0 from [7369fd21-7ee9-4780-a54b-5831e951ca9c{ip: 127.0.0.1, host: localhost, ports: [REPLICATION=41379], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}]
2021-05-04 16:21:04,434 INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (0) to other datanode
2021-05-04 16:21:05,269 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:06,270 INFO  freon.ProgressBar 
...
2021-05-04 16:21:10,275 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:11,275 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:11,791 INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 5368709120 bytes for container 0
...
2021-05-04 16:21:33,434 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:33,737 INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(190)) - Container 0 is downloaded to /tmp/container-copy/container-0.tar.gz
2021-05-04 16:21:34,434 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:34,690 INFO  replication.DownloadAndDiscardReplicator (DownloadAndDiscardReplicator.java:replicate(71)) - Container is downloaded but deleted, as you wished /tmp/container-copy/container-0.tar.gz
{code}

As you can see the full 5G data is sent out at 16:21:11 (after 6 seconds), but the data copy is finished only at  16:21:33 (22 more seconds).

Between the two time the majority of the container is kept in the GRPC/netty buffers.

As an experiment we can make the grpc client slow (GrpcReplicationClient):

{code}
    @Override
    public void onNext(CopyContainerResponseProto chunk) {
      try {
        try {
          Thread.sleep(1_000);
        } catch (InterruptedException e) {
          e.printStackTrace();
        }
        chunk.getData().writeTo(stream);
      } catch (IOException e) {
        response.completeExceptionally(e);
      }
    }
{code}

With this method we download the beginning of the container very slowly, and this is enough to get the exception above.

{code}
Exception in thread ""grpc-default-executor-0"" org.apache.ratis.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 2097152 byte(s) of direct memory (used: 3651141911, max: 3652190208)
{code}

Temporary it can be fixed with increasing the netty memory: -Dorg.apache.ratis.thirdparty.io.netty.maxDirectMemory=16000000000  but it's not a good long-term solution.

So we need to refactor the protocol to do a request/response chunk by chunk.

But we also have another problem. GRPC is not optimal for fast streaming.

The previous log showed that we replicated the container (5G) under 30 seconds (without reading the original container and without doing tar file compression).

This is 5 / 30 = 170 Mb / sec. (I wrote to a tmpfs on the destination side, but even my nvme is significant faster).

Based on [this|https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html] article the best (!) results (with C client!) were 1 Gb/s with GRPC. (with the explained black magic it is doubled).

Ansh Khanna earlier did some low-level benchmarking (for ratis streaming) which showed 5x difference between pure netty and GRPC:

Flatbuffers over GRPC
%CPU in Buffer Copying/Allocations: >10%
Time(in seconds): 16.44
 
Protobuffers over GRPC:
%CPU in Buffer Copying/Allocations: ~10%
Time(in seconds): 11.66

Netty Based Streaming
%CPU in Buffer Copying/Allocations: 0%
Time(in seconds): 2.7

Pure netty also supports zero copy async stream.

Summary:
 1. The current implementation should be refactored to avoid pushing the data
 2. Netty seems to be better for long-term solution

--> As a results it seems to be easier to create a POC with netty support and check how does it work.

Earlier I made an attempt which can be found here: https://github.com/elek/ozone/tree/close-container-replication-refactor

It's a generic interface which may also be used in https://issues.apache.org/jira/browse/HDDS-5142 But at least it can be used to compare the netty vs GRPC performance in this situation.",pull-request-available,[],HDDS,Bug,Major,2021-05-04 14:40:04,1
13376550,Avoid Maven connection errors in CI,"Github Actions builds intermittently fail due to Maven transfer errors.

Example:

{code:title=https://github.com/apache/ozone/runs/2454639477#step:6:2576}
Error:  Plugin org.apache.maven.plugins:maven-shade-plugin:3.2.4 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-shade-plugin:jar:3.2.4: Could not transfer artifact org.apache.maven.plugins:maven-shade-plugin:pom:3.2.4 from/to central (https://repo.maven.apache.org/maven2): Transfer failed for https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-shade-plugin/3.2.4/maven-shade-plugin-3.2.4.pom: Connection timed out (Read failed)
{code}",pull-request-available,['build'],HDDS,Task,Major,2021-05-04 12:17:12,0
13376514,Update commons-io to 2.8.0,"Similar to HADOOP-17683 we should update despite we don't use the vulnerable API.

https://nvd.nist.gov/vuln/detail/CVE-2021-29425

In Apache Commons IO before 2.7, When invoking the method FileNameUtils.normalize with an improper input string, like ""//../foo"", or ""\\..\foo"", the result would be the same value, thus possibly providing access to files in the parent directory, but not further above (thus ""limited"" path traversal), if the calling code would use the result to construct a path value.",pull-request-available,[],HDDS,Task,Major,2021-05-04 08:55:58,0
13376486,For AccessControlException do not perform failover,"For AccessControlException donot perform failOver, as there is no real need.
{code:java}
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1454)
        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.recommissionNodes(SCMClientProtocolServer.java:459)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.recommissionNodes(StorageContainerLocationProtocolServerSideTranslatorPB.java:646)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:317)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:155)
        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46954)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.8:9860 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
{code}
",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-04 06:43:04,2
13376413,Acceptance test may exit with 0 in case of error,"If any acceptance test fails, {{test-all.sh}} (and in turn {{acceptance.sh}}) should exit with error code (1).  But if a successful test is run after a failing one, it will now wrongly exit with success (0).

{code}
$ export OZONE_TEST_SELECTOR='failing1\|ozone-csi'
$ ./hadoop-ozone/dev-support/checks/acceptance.sh
...
$ echo $?
0
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-05-03 17:02:21,0
13375816,Use ReplicationConfig in OmKeyArgs,"During the implementation of HDDS-5145 I realized that OmKeyArgs also uses factor/type, it seems to be easier to convert it to replicationConfig as it's an in-memory class not a protobuf which is required to be persisted.

Having a half-baked patch planning to upload it soon.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-29 11:34:36,1
13375739,OM DB checkpoint servlet not accessible in a secure cluster,"When security and ACL is enabled, but not spnego, the OMDBCheckpointServlet throws an error:
{code:java}
10:02:29.094 PM ERROR OMDBCheckpointServlet 
 Permission denied: User principal 'dr.who' does not have access to /dbCheckpoint.
 This can happen when Ozone Manager is started with a different user.
 Please append 'dr.who' to OM 'ozone.administrators' config and restart OM to grant current user access to this endpoint.{code}

When Spnego is disabled, permissions cannot be checked since HTTP request will not have an identity (kerberos principal) which is causing this error.

",pull-request-available,['OM'],HDDS,Bug,Major,2021-04-29 04:54:48,6
13375689,Improve client and server logging,"On the OM server side, NotLeaderException and LeaderNotReadyExceptions can be suppressed. Otherwise, OM1 log is flooded with NotLeaderExceptions as clients always try OM1 before moving to the next OM. Instead we can change these exception logs to DEBUG.

Some BlockOutputStream and BlockOutputStreamEntryPool logs should be DEBUG level instead of INFO.
Running a 20GB put key operation resulted in the following console log:
{code:java}
ozone sh key put o3://ozone1/vol2/buck2/20GB /tmp/20GB
 21/03/08 19:01:05 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
 21/03/08 19:01:05 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
 21/03/08 19:01:05 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
 21/03/08 19:01:06 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
 21/03/08 19:01:06 INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-30FCE96A6E8D->c2e9a19c-15f3-4eae-ba46-83c763d2ee8d
 21/03/08 19:01:06 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851570_chunk_43 blockID conID: 74 locID: 105855717519851570 bcsId: 7856 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 74 in CLOSING state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851570_chunk_44 blockID conID: 74 locID: 105855717519851570 bcsId: 7856 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 74 in CLOSED state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_1 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_2 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:39 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_3 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_4 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:41 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:43 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:45 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:47 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:48 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:50 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:52 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:54 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:56 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:57 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:59 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:01 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:02 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:04 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:06 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:07 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:10 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:11 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:13 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:15 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:17 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:18 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:20 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:22 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:23 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:28 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:30 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:31 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:33 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
{code}",pull-request-available,[],HDDS,Improvement,Major,2021-04-28 21:09:17,7
13375560,Remove some Freon integration tests,"Some Freon integration tests are unnecessary, as it is already widely used in acceptance tests.",pull-request-available,[],HDDS,Test,Major,2021-04-28 09:45:26,0
13375289,Fix Suggested leader in Client,"Server returns suggested leader correctly, but on client when performing regex, extraction of suggested leader address is incorrect. The issue is due to the RegEx pattern to obtain the suggested leader.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-04-27 10:04:42,2
13374747,Intermittent test failure in TestContainerDeletionChoosingPolicy#testRandomChoosingPolicy,"Observed in this CI run: [https://github.com/apache/ozone/pull/2179/checks?check_run_id=2423463857]

Bundle is attached to issue.",pull-request-available,[],HDDS,Bug,Minor,2021-04-23 23:06:26,1
13374647,Extend Pipline/ReplicationConfig refactor with ECReplicationConfig,"HDDS-5047 started to use ReplicationConfig in Pipeline and pipeline related SCM service on master. ReplicationConfig was introduced in HDDS-5011 but it has two versions one for master and one for the EC branch.

Merging master after HDDS-5047 requires small modification in the code to make HDDS-5047 compatible the ec version of HDDS-5011:

 * During the proto serialization / deserialization we should use optional ECReplicationConfig (if exists).",pull-request-available,[],HDDS,Sub-task,Major,2021-04-23 12:06:59,1
13374638,Create github check to alert when dependency tree is changed,Please see: https://github.com/apache/ozone/pull/2177,pull-request-available,[],HDDS,Improvement,Major,2021-04-23 11:44:46,1
13374545,"Make generic streaming client/service for container re-replication,  data read, scm/om snapshot download","Currently, grpc client/server end points are used for container re-replication, reading data, scm snapshot download for SCM HA etc but have individual client and server implementations. The idea her is to define a generic interface for client/servers and use it across all components for better maintainability, better code reuse and easier debugging.",pull-request-available,"['OM', 'Ozone Client', 'Ozone Datanode', 'SCM', 'SCM HA']",HDDS,Bug,Major,2021-04-23 04:46:54,1
13374362,Refactor HddsUtils and HddsServerUtils,This Jira is to cleanup getScmAddress and other methods in finding SCM client/block address.,pull-request-available,['SCM HA'],HDDS,Task,Major,2021-04-22 09:54:46,2
13374188,Use timeout in github actions,"Default timeout for one GitHub action job is 6 hours. It turned out that in some unfortunate case the acceptance tests were pending at this time, for example in https://github.com/apache/ozone/runs/2053572861?check_suite_focus=true

We need to turn on stricter timeouts for each of our jobs  to avoid unnecessary build time usage.",pull-request-available,[],HDDS,Improvement,Major,2021-04-21 17:47:19,1
13374077,HDDS-5127. Fix getServiceList when SCM HA is enabled,"steps taken :
1. SCM HA enabled in ozone cluster.
2. Ran ozone sh volume list command.

 

exception seen :

-------------------
{noformat}
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Does not contain a valid host:port authority: <SCM CLIENT ADDRESSES> at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:213) at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:164) at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:153) at org.apache.hadoop.hdds.HddsUtils.getScmAddressForClients(HddsUtils.java:112) at org.apache.hadoop.ozone.om.OzoneManager.getServiceList(OzoneManager.java:2658) at org.apache.hadoop.ozone.om.OzoneManager.getServiceInfo(OzoneManager.java:2678) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getServiceList(OzoneManagerRequestHandler.java:451) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:176) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:190) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:132) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887){noformat}
 

this issue is due to ""ozone.scm.client.address"" config set in SCM HA setup which contains multiple SCM host addresses.",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-04-21 09:06:41,2
13373931,Only test ozonesecure with SCM Ratis disabled,"As [spotted|https://github.com/apache/ozone/pull/2052#discussion_r616570112] by [~elek], {{ozonesecure}} acceptance test is run twice: with SCM Ratis enabled and disabled.  This doubles execution time.  Now that {{ozonesecure-ha}} acceptance test also has SCM HA (implies Ratis enabled), I don't think it's necessary to run {{ozonesecure}} (single SCM) with Ratis enabled.",pull-request-available,['test'],HDDS,Improvement,Major,2021-04-20 19:17:29,0
13373805,Use the pre-created apache/ozone-testkrb5 image during secure acceptance tests,"Today ozonesecure compose clusters (and ozonesecure-ha and ozonesecure-mr) use an adhoc keytab issuer. The issuer is download during the image creation and uses a third party go lang application to create the keytabs on-demand.

As discussed earlier, it would be faster to use a dedicated, pre-built container image which includes the pre-created keytabs instead of issuing them on-the fly (keytab generation is slow + container creation is slow)

For each of the tagged images we can export to current keytabs to hadoop-ozone/dist/src/main/compose/ which can be mounted to to compose clusters.

It makes the overall acceptance test faster (instead of creating keytab, which is quite slow, we can start the cluster immediately). And we don't need to depend on an external utility app.

Pre-created keytabs are also more similar to production environment...

First test using the apache/ozone-testkrb5 from HDDS-4938

The time between starting test.sh script and first robot test:

master: 3:30 (01:43:08 --01:46:38)
this patch: 2:10 (12:59:29 13:02:39)

(note: there are some variances between different builds, and in general the patch build was a slower one. It can be even faster).

~",pull-request-available,[],HDDS,Improvement,Major,2021-04-20 09:04:41,1
13373785,SCM Reinitialization can end up leaking Ratis Segmented RaftLogWorker threads,"During SCM reinitialialisation, ratis server is spinned up to check if an existing ratis group exists or not, and closes the server without starting it. In ratis, the segmented raft log worker thraeds are started during init() itself but get closed during raftServer.close() only if the server transitions to RUNNING state which causes the issue.

 
{code:java}
Attaching to process ID 266710, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.232-b09
Deadlock Detection:No deadlocks found.Thread 266745: (state = BLOCKED)Locked ownable synchronizers:
    - NoneThread 266783: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=78, line=2078 (Compiled frame)
 - org.apache.ratis.util.DataBlockingQueue.poll(org.apache.ratis.util.TimeDuration) @bci=134, line=137 (Compiled frame)
 - org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run() @bci=16, line=292 (Interpreted frame)
 - org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$161.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)Locked ownable synchronizers:
    - NoneThread 266761: (state = BLOCKED)Locked ownable synchronizers:
    - NoneThread 266760: (state = BLOCKED)Locked ownable synchronizers:
    - NoneThread 266759: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.ref.ReferenceQueue.remove(long) @bci=59, line=144 (Compiled frame)
 - java.lang.ref.ReferenceQueue.remove() @bci=2, line=165 (Compiled frame)
 - java.lang.ref.Finalizer$FinalizerThread.run() @bci=36, line=216 (Interpreted frame)Locked ownable synchronizers:
    - NoneThread 266758: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Object.wait() @bci=2, line=502 (Compiled frame)
 - java.lang.ref.Reference.tryHandlePending(boolean) @bci=54, line=191 (Compiled frame)
 - java.lang.ref.Reference$ReferenceHandler.run() @bci=1, line=153 (Interpreted frame)Locked ownable synchronizers:
    - None
{code}",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-04-20 07:50:22,3
13373671,CRLInfo should include CRL Sequence ID,The CRLInfo class does not contain CRL Sequence ID and it would be good to include it while iterating the  CRLInfos as a list.,pull-request-available,['Security'],HDDS,Sub-task,Major,2021-04-19 17:50:31,6
13373626,Secure datanode/OM may exit if cannot connect to SCM,"Intermittent failure in secure acceptance tests indicates that datanode may fail to start up if SCM is not yet ready:

{noformat}
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
...
datanode_3  | 2021-04-19 08:20:29,030 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:dn@627dcb55b990
...
datanode_3  | 2021-04-19 08:20:57,660 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 627dcb55b990/172.26.0.4 to scm:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy18.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.26.0.10:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_3  | 2021-04-19 08:20:59,667 [main] ERROR ozone.HddsDatanodeService: Error while storing SCM signed certificate.
...
datanode_3  | 	at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.submitRequest(SCMSecurityProtocolClientSideTranslatorPB.java:104)
datanode_3  | 	at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.getDataNodeCertificateChain(SCMSecurityProtocolClientSideTranslatorPB.java:263)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.getSCMSignedCert(HddsDatanodeService.java:349)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:320)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:248)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:192)
...
datanode_3  | SHUTDOWN_MSG: Shutting down HddsDatanodeService at 627dcb55b990/172.26.0.4
{noformat}",pull-request-available,"['Ozone Datanode', 'SCM HA', 'Security']",HDDS,Bug,Critical,2021-04-19 13:54:05,2
13372469,TestStorageContainerManagerHttpServer fails in CI,"{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer
-------------------------------------------------------------------------------
Tests run: 3, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 302.762 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer
testHttpPolicy[0](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 60.956 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:110)

testHttpPolicy[1](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 180.331 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:116)

testHttpPolicy[2](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 60.223 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:110)
{noformat}",pull-request-available,['test'],HDDS,Bug,Critical,2021-04-14 18:13:48,0
13372371,Attempt to remove state from *UpgradeFinalizer classes.,Follow up from https://github.com/apache/ozone/pull/1998#discussion_r606369185.,pull-request-available,[],HDDS,Sub-task,Major,2021-04-14 14:46:20,5
13372181,Fix Install Snapshot Mechanism in SCMStateMachine,"With https://issues.apache.org/jira/browse/RATIS-1326 now fixed and ozone being updated with latest ratis, the idea here is to fix the installSnapshot behaviour in SCM HA.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-04-14 05:04:20,3
13371977,Adjust download pages to use Apache Ozone (tlp) artifacts,Please see: https://github.com/apache/ozone-site/pull/4,pull-request-available,[],HDDS,Improvement,Major,2021-04-13 08:19:30,1
13371769,[FSO] Reducing time of ozonefs acceptance testmatrix,"Today we execute ozonefs/ozonefs.robot with compose/ozone/test.sh with multiple matrix parameters:

{code}
for scheme in ofs o3fs; do
    for bucket in link bucket; do
       #test ozonefs/ozonefs.robot
   done
done
{code}

HDDS-2939 doubles these 4 executions with introducing the layout parameter (simple vs prefix).  At the same time the execution time of acceptance (unsecure) tests are increased from 37 minutes to 57 minutes.

I would suggest suggesting to use only selected tests from this 2 x 2 x 2 matrix.

For example:

bucket / o3fs / prefix
link / ofs / simple
bucket / ofs / prefix

 ",pull-request-available,[],HDDS,Sub-task,Major,2021-04-12 11:43:05,1
13371202,Add project separation and first stable release to the HISTORY.md,Please see: https://github.com/apache/ozone/pull/2149,pull-request-available,[],HDDS,Improvement,Major,2021-04-12 08:21:43,1
13370712,Ozone RPC client leaks KeyProvider instances,Ozone RPC Client currently create a key provider instance each time the getKeyProvider is invoked. The caller such as objectstore does not keep track of the returned KMS provider with a close(). This leads to leaks of resources associate with KMS provider (e.g. SSL). ,pull-request-available,[],HDDS,Bug,Major,2021-04-09 17:51:32,4
13370664,Include HISTORY.md/SECURITY.md/CONTRIBUTING.md in the release artifacts.,Please see: https://github.com/apache/ozone/pull/2140,pull-request-available,[],HDDS,Improvement,Major,2021-04-09 13:28:33,1
13370653,Bump version of common-compress,Please see: https://github.com/apache/ozone/pull/2139,pull-request-available,[],HDDS,Improvement,Major,2021-04-09 13:08:37,1
13370650,Create unit test for OzoneClient,See: https://github.com/apache/ozone/pull/2138,pull-request-available,[],HDDS,Sub-task,Major,2021-04-09 13:02:25,1
13370629,Fix key put implementation,Ozone Go client's {{key put}} command always (tries to) upload local {{/tmp/asd}}.  It should take the input local file from command-line parameter.,pull-request-available,['Go Client'],HDDS,Sub-task,Major,2021-04-09 10:56:14,0
13370594,[SCM HA Security] Enable s3 test suite for ozone-secure-ha,Enable s3 test suite for ozone-secure-ha docker which starts SCM/OM HA in secure env.,pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-04-09 08:47:29,2
13370353,[SCM HA Security] Remove code of not starting ozone services when Security is enabled on SCM HA cluster,"Now SCM HA Security is implemented, we can remove the code added as part of HDDS-4978
",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-04-08 10:58:00,2
13370344,Bump Guava version,Please see: https://github.com/apache/ozone/pull/2131,pull-request-available,[],HDDS,Improvement,Major,2021-04-08 10:10:55,1
13370342,Use ReplicationConfig on client side ,"HDDS-5011 introduced new ReplicationConfig with multiple implementations (Ratis/Standalone).

To make ozone sh client EC compatible we can use ReplicationConfig on the client side too: --replication parameter can be parsed from the string based on the ReplicationConfig. Today it can be THREE or ONE, but later ECReplicationConfig can support more sophisticated  schemes (like 3:2) ",pull-request-available,[],HDDS,Sub-task,Major,2021-04-08 10:07:36,1
13369988,[SCM HA Security] Fix duration of sub-ca certs,"Right now sub-ca certs use hdds.x509.default.duration.
This Jira proposes to use hdds.x509.max.duration similar to selfsigned certs",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-04-07 07:59:52,2
13369784,Use fixed vesion from pnpm to build recon,"Recon build is broken since yesterday due to a new pnpm@6.0.0 release

{code}
[INFO] Running 'npx pnpm config set store-dir ~/.pnpm-store' in /home/elek/projects/ozone/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web
[INFO] npx: installed 1 in 1.057s
[INFO] ERROR: This version of pnpm requires at least Node.js v12.17
[INFO] The current version of Node.js is v12.14.1
[INFO] Visit https://r.pnpm.io/comp to see the list of past pnpm versions with respective Node.js version support.
{code}

This is because the frontend maven plugin uses [npx|https://www.npmjs.com/package/npx] which downloads the required tool (pnpm in our case) on-demand if it's not available locally.

This download uses the latest version (by default).  

I recommend using a fixed version from pnpm to avoid any unexpected error when external tools is updated.",pull-request-available,[],HDDS,Bug,Blocker,2021-04-06 12:09:11,1
13369734,Fix project name in NOTICE.txt,Please see: https://github.com/apache/ozone/pull/2112,pull-request-available,[],HDDS,Improvement,Blocker,2021-04-06 09:15:53,1
13369730,Remove hadoop- prefixes from release artifacts (release branch),"When we create a release artifact today it has the `hadoop-` prefix in the names. Would be better to remove to avoid confusion.

Note: full `hadoop-` prefix is removed by HDDS-4936 (thanks to [~sammichen]), this patch is a very small subset of that patch just to fix the tar file name on the release branch.",pull-request-available,[],HDDS,Bug,Major,2021-04-06 09:00:29,1
13369711,Add a config to bypass clusterId validation for bootstrapping SCM,"IN SCM HA, the primary node starts up the ratis server while other bootstrapping nodes will get added to the ratis group. Now, if all the bootstrapping SCM's get stopped, the primary node will now step down from leadership as it will loose majority. If the bootstrapping nodes are now bootstrapped again,  the bootsrapping node will try to first validate the cluster id from the leader SCM with the persisted cluster id , but as there is no leader existing, bootstrapping wil keep on failing and retrying until it shuts down. 

The issue can be very easily simulated in kubernetes deployments, where bootstrap and init cmds are run repeatedly on every restart.

The Jira aims to bypass the cluster id validation if a bootstrapping node already has a cluster id.",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-04-06 07:33:45,3
13369167,[SCM HA Security] Make InterSCM grpc channel secure,"Currently, a bootstrapping SCM or a follower SCM can request a SCM rocks dn checkpoint from the leader via external grpc channel. The Jira here aims to ensure the channel is secured before any transfer takes place between SCMs.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Blocker,2021-04-01 16:51:27,2
13369156,Increase number of client retries/ failovers to OMs,"Currently in an OM HA setup, client by default does 15 failovers before giving up. If there are 3 OMs and a leader has not been elected yet, then the client will try each OM in a round robin way, wait for 2 secs and then try all the OMs again. This gives the client around 10 seconds before it exhausts all its failover attempts and fails. 

This Jira proposes to increase the failover attempts to 100.",pull-request-available,[],HDDS,Improvement,Major,2021-04-01 16:25:25,7
13369089,Make getScmInfo retry for a duration,"Previously during init of OM for getScmInfo we used to do RetryForEverWithFixedSleep, but during SCM HA we have removed this.

This Jira proposes to add a ceration duration to try getScmInfo, instead of retry forever with fixed sleep.

In a few docker tests CI run, we have seen this issue, after 15 retries Om init failed, as SCM is started later.


{code:java}
om1_1       | 2021-03-31 17:03:48,184 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2021-03-31 17:03:52,453 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:03:54,455 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:03:56,457 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:03:58,466 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:00,498 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:02,522 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:04,533 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:06,535 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:08,537 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:10,541 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:12,543 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:14,546 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:16,550 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:18,553 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:20,795 [main] ERROR om.OzoneManager: Could not initialize OM version file
om1_1       | org.apache.hadoop.ipc.RemoteException(org.apache.ratis.protocol.exceptions.NotLeaderException): Server 9cb7a7ae-4c40-401c-b1c6-55728c1f0907@group-C35E1BD0DE21 is not the leader
om1_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.triggerNotLeaderException(SCMRatisServerImpl.java:245)
om1_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:108)
om1_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13874)
om1_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
om1_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
om1_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
om1_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
om1_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
om1_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
om1_1       | 
{code}




",pull-request-available,[],HDDS,Bug,Major,2021-04-01 10:28:25,2
13369078,Remove backward direction dependency betweeon HDDS->Ozone,Please see: https://github.com/apache/ozone/pull/2106,pull-request-available,[],HDDS,Improvement,Major,2021-04-01 10:02:01,1
13368847,[SCM HA Security] Handle leader changes between SCMInfo and getSCMSigned Cert in OM,"This Jira is to handle leader change between getScmInfo and getScmSignedCert.

*Problem:*
*Leader is SCM1 - Returned SCMID is SCM1ID*
ScmInfo returns the leader SCMID

*Leader is SCM2 - SCM ID is SCM2ID*
getSCMSignedCert, during generate certificate it has a check compare the scmId passed in CSR, is same as current SCM scmID

In this case when the leader change between these 2 calls OM will fail to get a Certificate.

",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-31 10:53:11,2
13368824,Ensure failover to suggested leader if any for NotLeaderException,"Currently, ratis suggested leader info is not propagated to rpc clients for ratis request failures. Idea here is to propagate this info and perform failover accordingly if request fail with NotLeaderException.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-31 09:04:22,3
13368821,Add retry policy for ratis requests in SCM HA.,"For SCM HA, on certain exceptions , for example, LeaderNotReady, the requests must be retried on the same server. For exceptions, such as NotLeaderException, retry should happen along with a failover for rpc clients to the suggested leader if possible. The idea here is to address the retry policy requirements for SCM HA.",pull-request-available,[],HDDS,Bug,Major,2021-03-31 08:52:52,3
13368819,Add timeout support for ratis requests in SCM HA,"In SCM HA, the requests submitted to ratis leader can potentially hang. The idea here is to timeout the request if the response is not received after a certain threshold. ",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-03-31 08:51:28,3
13368671,Refactor Pipeline to use ReplicationConfig instead of factor/type,"HDDS-5011 introduces Java ReplicationConfig classes which can be used as a replacement of replicationType and replicationFactor.

First task is replacing type/factor with ReplicationConfig in Pipeline and related managers (PipelineManager BackgroundPipelineCreatorV2, PipelineStateManager...)

We can do it on the master without the EC related stuff... (later we will add the small part which is required for EC",pull-request-available,[],HDDS,Sub-task,Major,2021-03-30 15:00:58,1
13368669,Merge master with SCM HA changes into upgrade branch.,Merge master with SCM HA changes into HDDS-3698-nonrolling-upgrade branch.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-30 14:55:34,5
13367983,Apply merge/notification settings to ozone-site repo,Apply merge and notification settings via {{.asf.yaml}} from other Ozone repos to {{ozone-site}} repo.,pull-request-available,['website'],HDDS,Improvement,Minor,2021-03-26 18:09:05,0
13367789,Update wiki links in website,"The website has references to Hadoop wiki, from where pages were moved to new space.",pull-request-available,['website'],HDDS,Bug,Minor,2021-03-26 08:03:13,0
13367276,[SCM HA Security] Handle leader changes during bootstrap,https://github.com/apache/ozone/pull/2000#discussion_r599427607,pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-24 15:26:32,2
13367226,Cancel failing PR workflow runs,"Similar to HDDS-4988, we should also cancel PR builds that are already failing.  This is an improvement over HDDS-4933, where only concurrent builds of the same check could be cancelled.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-24 12:40:32,0
13367178,Upgrade Async Profiler,Async Profiler default output format is being changed in HDDS-5009 to one not supported by version 1.5.  We need to bump the version being installed in the image.,pull-request-available,['docker'],HDDS,Task,Minor,2021-03-24 08:16:50,0
13366982,Bump ratis version to 2.0.0,Please see: https://github.com/apache/ozone/pull/2076,pull-request-available,[],HDDS,Improvement,Blocker,2021-03-23 13:05:32,1
13366766,Upload upgrade design documentation to docs module.,"Documentation on upgrade design, how to perform an upgrade & upgrade framework developer primer.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-22 16:09:57,5
13366713,Introduce EC ReplicationConfig and Java based ReplicationConfig implementation,"SCM proto file should be extended to use ECReplicationConfig which can be de-serialized to a specific ReplicationConfiguration.

Note: this is the bare minimum version of HDDS-4882 which doesn't include the rafactor of the existing proto/persistent fields but de-/serialize them to the new java pojos.
 ",pull-request-available,[],HDDS,Sub-task,Major,2021-03-22 12:08:27,1
13366400,Multipart Upload fails due to partName mismatch,"We're running the official Ozone 1.0.0 release and facing S3 Multipart Upload failures with large files. The error message looks similar to that is reported in HDDS-3554 but we'd like to report what we've found so far to help the further investigation of this issue.
h1. The error message recorded in OM log

Please find the following error message excerpted from our OM. Forgive us we redacted some sensitive information such as username and keyname which imply our project's topic.
{code:java}
2021-03-14 07:48:41,947 [IPC Server handler 88 on default port 9862] ERROR org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: <REDACTED_KEYNAME> in Volume/Bucket s3v/<BUCKETNAME>
INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: <BUCKETNAME> key: <REDACTED_KEYNAME>. Provided Part info is { /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282, 4}, whereas OM has partName /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406
        at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:199)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:224)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:145)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)
        at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915){code}
Anyway, OM thinks the partName for the partNumber 4 is /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406 but COMPLETE_MULTIPART_UPLOAD request think it must be /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282. This discrepancy is the immediate cause for this error.
h1. OM audit log says both are correct

Please find attached om-audit-HOSTNAME-2021-03-14-19-53-09-1.log.gz (also redacted, sorry), it contains filtered output of our OM audit log, the lines which include <REDACTED_KEYNAME> and multipartList entry are remain.

Interestingly, according to the OM audit log, there're two COMMIT_MULTIPART_UPLOAD_PARTKEY operations exist for partNumber=4 and both operations were succeeded:

 
{code:java}
% zgrep partNumber=4, om-audit-HOSTNAME-2021-03-14-19-53-09-1.log.gz
2021-03-14 07:16:04,992 | INFO  | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=8388608, replicationType=RATIS, replicationFactor=ONE, partNumber=4, partName=/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282} | ret=SUCCESS | 
2021-03-14 07:18:11,828 | INFO  | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=8388608, replicationType=RATIS, replicationFactor=ONE, partNumber=4, partName=/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406} | ret=SUCCESS | 
%
{code}
 

OM seemed to have accepted both partName ending with 105884795658268282 and  105884791629180406 for partNumber 4. And COMPLETE_MULTIPART_UPLOAD operation was called with the prior partName but OM believed it had the latter partName for partNumber 4.

 
{code:java}
2021-03-14 07:48:41,947 | ERROR | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMPLETE_MULTIPART_UPLOAD {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=0, replicationType=
RATIS, replicationFactor=ONE, multipartList=[partNumber: 1
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791631605244""
, partNumber: 2
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791631539707""
, partNumber: 3
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791628262900""
, partNumber: 4
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282""
, partNumber: 5
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629245944""
, partNumber: 6
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629245943""
{code}
We can also find there're multiple COMMIT_MULTIPART_UPLOAD_PARTKEY operations for several partNumbers, such as partNumber 4, 13, 20, 45, 57, 67, 73, ... some partNumbers like 172 have more than three COMMIT_MULTIPART_UPLOAD_PARTKEY operations they're all succeeded.

 
h1. How to solve this issue?

At first we thought this issue is caused by race condition, but noticed that there're enough time between each COMMIT_MULTIPART_UPLOAD_PARKEY operation. We're not sure but noticed that write operations to OmMetadataManager are isolated with omMetadataManager.getLock().acquireWriteLock(BUCKET_LOCK, volumeName, bucketName);

 
{code:java}
     multipartKey = omMetadataManager.getMultipartKey(volumeName,
         bucketName, keyName, uploadID);

     // TODO to support S3 ACL later.

     acquiredLock = omMetadataManager.getLock().acquireWriteLock(BUCKET_LOCK,
         volumeName, bucketName);

     validateBucketAndVolume(omMetadataManager, volumeName, bucketName);

     String ozoneKey = omMetadataManager.getOzoneKey(
         volumeName, bucketName, keyName);

     OmMultipartKeyInfo multipartKeyInfo = omMetadataManager
         .getMultipartInfoTable().get(multipartKey);
{code}
 

So our question is, is it normal to have multiple COMMIT_MULTIPART_UPLOAD_PARTKEY operations for a partNumber, with different partNames?
h1. Other findings

This issue occurs less frequently with aws configure set default.s3.multipart_chunksize 256MB. Almost always fails with multipart_chunksize 8MB, 1GB in our environment.

 

 ",pull-request-available,"['OM', 'S3']",HDDS,Bug,Major,2021-03-19 17:00:16,2
13366337,Upgrade Jersey2 dependency,Upgrade Jersey 2.27 to 2.32+.,pull-request-available,[],HDDS,Task,Major,2021-03-19 12:11:49,0
13366252,[SCM HA Security] Make storeValidCertificate method idempotent,This Jira is to make storeValidCertificate idempotent so that during replay it does not cause any issues.,pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-19 05:47:00,2
13366176,Upgrade kotlin-stdlib,"Upgrade {{kotlin-stdlib}}, transitive dependency via {{jaeger-client}}, to 1.4.21+.",pull-request-available,['build'],HDDS,Task,Major,2021-03-18 20:28:59,0
13366149,Skip CI for draft pull requests,"Github pull requests can be created as (or converted to) draft.  This is useful for gathering early feedback from other contributors about proof-of-concept code.  Such code may not even compile, and style issues are usually ignored.

Since further commits are generally expected in this case, I think it makes sense to skip CI build/test until the PR is marked as ""ready for review"".

If CI feedback is desired, the author should enable workflows in their fork so we can check push builds there.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-18 17:33:34,0
13365949,Add guardrail for reserved buffer size when DN reads a chunk,"https://github.com/apache/ozone/blob/db3c50d9fc4b1048cd83074343cc00444f0b24f7/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L148

{code}
    long len = info.getLen();
    long offset = info.getOffset();
    ByteBuffer data = ByteBuffer.allocate((int) len);
{code}

DN reserves a byte buffer for client's chunk read request based on whatever client specifies, without check. This is bad. Client can send a message forcing DN to allocate up to 2GB memory per request. (Fortunately the grpc handler captures the OOM exception so DN doesn't crash)

Propose: add a guardrail check. XceiverServerGrpc set max inbound message size as 32MB (OZONE_SCM_CHUNK_MAX_SIZE). We should make sure the requested length is less than this too.",pull-request-available,[],HDDS,Improvement,Major,2021-03-17 21:38:56,0
13365652,Introduce First upgrade startup action and Pre-finalized state validation in Layout Feature.,"* Introduce 2 new phases of upgrade action hooks (per layout feature)
 *Prefinalized state validation* - A layout feature (version) can use this to validate that the component is not started up in a way to use it before finalization. For example, an SCM HA validation action can make sure HA is not enabled before finalization. 
*First Upgrade Start action* - Run exactly once when a component is started up after an upgrade with an unfinalized layout feature.
* Annotation based registration of layout actions to the layout features. After this change, an HDDS upgrade action be created like this.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-16 22:35:21,5
13365585,Cancel duplicate PR workflows,"CI runs for pull requests with multiple commits can take up too much resources.  We should cancel previous runs, as only the last one is considered for PR status.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-16 16:27:48,0
13365428,"[SCM HA Security] When Ratis is enabled, SCM secure cluster is not working","
{code:java}
scm_1       | 2021-03-16 05:39:38,023 [a3c3a50a-98c8-4517-b619-4e0715dc1ef7@group-9A1DAB6BC185-StateMachineUpdater] ERROR statemachine.StateMachine: Terminating with exit status 1: org.apache.hadoop.hdds.scm.server.SCMCertStore.storeValidCertificate(java.math.BigInteger, sun.security.x509.X509CertImpl, org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType)
scm_1       | com.google.protobuf.InvalidProtocolBufferException: org.apache.hadoop.hdds.scm.server.SCMCertStore.storeValidCertificate(java.math.BigInteger, sun.security.x509.X509CertImpl, org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType)
scm_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:164)
scm_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:134)
scm_1       | 	at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1659)
scm_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:232)
scm_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:177)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
{code}
",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-16 07:11:48,2
13365220,[SCM HA Security] Ozone services should be disabled in SCM HA enabled and security enabled cluster,"SCM HA security work is still in progress.

[~elek] Brought up the point that until before merge of SCM HA branch we should add safeguard check to fail bringing up the cluster",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-15 11:32:13,2
13363816,Extract check dependency installation from Github Actions workflow,"Add steps to check scripts for installing dependencies (eg. {{bats}} or {{spotbugs}}). This reduces complexity of Github-specific code in the workflow definition, and allows simpler prototyping/testing.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-11 10:55:58,0
13363613,Fix flaky test TestSCMInstallSnapshotWithHA#testInstallCorruptedCheckpointFailure,"The test assumes that a follower node must be behind the leader during the final validation assertions. Fix is to not start the follower server itself , so that it always lags behind the leader at any point of time.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 17:01:46,3
13363562,[SCM HA Security] Make CertStore DB updates for StoreValidateCertificate go via Ratis,"This Jira is to make DB updates of CertStore go via ratis, so that all SCM's can be in sync.

In this way, OM/DN/SCM Certs will be in sync across Ratis.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-10 14:12:15,2
13363560,[SCM HA Security] Implement listCAs and getRootCA API,"This Jira is to implement an API that lists all CA's of SCM nodes along with RootCA.

*Example output returned by this new API:*
SCM1 CA
SCM2 CA
SCM3 CA
SCM1 RootCA

And to implement getRootCA which returns the root CA which has signed the certificate for other SCMs.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-10 14:10:04,2
13363554,Return with exit code 0 in case of optional scm bootstrap/init ,"HDDS-4896 provides a very elegant way to initialize SCM HA in containerized environment:

Both `--init` and `--bootstrap` can be executed, but when `ozone.scm.primordial.node.id` is set, the init is ignored on non-primordial, the bootstrap ignored on primordial nodes.

To make it even easier to use in k8s environments I suggest to return with exit code 0 in these cases as init proces shouldn't be repeated in these cases ignoring the init/bootstrap is part of the expected workflow.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 13:37:22,1
13363550,Provide example k8s files to run full HA Ozone,We already have Kubernetes examples for Ozone cluster to show how HA can be supported in Kubernetes environment seems to be good idea to provide example for full HA Ozone cluster.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 13:28:44,1
13363540,NullPointerException during SCM init,"
{code:java}
scm1_1       | java.lang.NullPointerException
scm1_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.close(SCMStateMachine.java:297)
scm1_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.stop(StateMachineUpdater.java:130)
scm1_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:184)
scm1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
{code}

This is due to during SCM init, RatisServer is started with StateMachine initialized false.
",pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 12:36:33,2
13363445,[Doc] Add SCM HA Setup Doc,we need to display how to set up SCM HA.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 07:41:29,1
13363371,Reuse compiled binaries in combined coverage calculation,"Combined coverage calculation consists of:

# download coverage files created by separate checks
# merge coverage files
# full build
# extract classes from jars
# produce jacoco report

Instead of the full build, we could reuse the binaries created in _compile_ check, to save some time.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-09 20:58:30,0
13363282,Provide testkrb5 image for faster ozonesecure tests,Please see: https://github.com/apache/ozone-docker-testkrb5/pull/1,pull-request-available,[],HDDS,Improvement,Major,2021-03-09 14:42:35,1
13363276,Enhance SCMServerProtocol with using ReplicationConfig,"In HDDS-4882 a new ReplicationConfig is introduced. This patch shows how can it be used between OM and SCM on the protocol.

This patch is not a full refactor of SCM it focuses on the SCM protocol side only. Pipeline manager can be improved in follow-up patches...",pull-request-available,[],HDDS,Sub-task,Major,2021-03-09 14:10:58,1
13363225,Make CI checks fail faster,"Currently most CI checks run to completion even if other checks fail.  This is useful if we want to find as many problems as possible in a single CI run.  However, it is a waste of resources when trying to get a clean run before merge.

I propose to make checks fail faster for PR runs.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-09 09:04:34,0
13363139,Replace the usage of deprecated Junit#timeout() in Ozone.,This ticket is opened to replace the deprecated usage of Timeout(int millis) to  Timeout.seconds(). ,pull-request-available,[],HDDS,Test,Major,2021-03-09 01:37:52,4
13363054,Rename Apache Hadoop Ozone to Apache Ozone in pom and markdown files,Please see: https://github.com/apache/ozone/pull/2005,pull-request-available,[],HDDS,Improvement,Major,2021-03-08 16:09:12,1
13362937,[SCM HA Security] Integrate CertClient,"*This Jira is to implement*
1. Use RootCertificate server to issue certs for SCM
2. Use scmCertificatServer to issue certs for DN/OM. (This cert server got certs from RootCertificate Server)
3. Start RootCertificate server only on primary SCM.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-08 07:49:30,2
13362702,Refine the native authorizer parent context right check,"Current we map CREATE/DELETE to parent WRITE. All the other are just 1:1 map from child to parent. 

This may not work, e.g., child WRITE_ACL does not equal to parent WRITE_ACL

Here is the proposed new mapping:
    // Refined the parent context
    // OP         |CHILD     |PARENT

    // CREATE      NONE         WRITE
    // DELETE      DELETE       WRITE
    // WRITE       WRITE        WRITE
    // WRITE_ACL   WRITE_ACL    WRITE     (V1 WRITE_ACL=>WRITE)

    // READ        READ         READ
    // LIST        LIST         READ      (V1 LIST=>READ)
    // READ_ACL    READ_ACL     READ      (V1 READ_ACL=>READ)",pull-request-available,[],HDDS,Sub-task,Major,2021-03-05 20:52:57,4
13362440,Layout version should be available in DB for an un-finalized OM to be finalized through a Ratis snapshot.,"Although OM Finalization is a Ratis request, it updates a persistent location outside the OM state machine maintained ""RocksDB"". Hence, in a rare case where an unfinalized OM needs a Ratis snapshot to finalize (without any logs), the VERSION file will not be updated, and hence the OM will be stuck in that state.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-03-05 00:25:00,5
13362405,Remove mention of CSI support,"The Ozone website prominently mentions CSI support:

[https://ozone.apache.org/docs/1.0.0/]

[https://ozone.apache.org/docs/1.0.0/interface/csi.html]

Our docs give a false impression to users that CSI is fully functional and supported for persistent storage inside containers.

This support uses goofys+S3 gateway, so it is not appropriate for any serious usage. A real CSI solution should use an approach like the cBlocks prototype by building directly on top of HDDS containers with a real device driver.

Until that time we should not claim CSI support. Alternatively we should be honest with our users that it is a prototype and not suitable for serious usage.",pull-request-available,[],HDDS,Improvement,Critical,2021-03-04 20:05:26,1
13362390,Add Layout version information to Recon datanode info API.,Simple change to add Layout version information to Recon datanode info API.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2021-03-04 18:54:07,5
13362079,[SCM HA Security]  Create SCM Cert Client and change DefaultCA to allow self signed and intermediary,"This Jira is to implement
1. Create CertClient, which generates a public key, private key, and generates CSR with ClusterID, SCMID. 
2. Modify DefaultCA Server to work in 2 modes, SELF_SIGNED_CA and INTERMEDIARY_CA.
3. Modify SCMStorageConfig to persist SCM cert serial ID.


",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-03 11:32:27,2
13362058,Need a tool to upgrade current non-HA SCM node to single node HA cluster,"Need a tool to upgrade current non-HA SCM node to single node HA cluster

cc [~shashikant]",pull-request-available,[],HDDS,Sub-task,Major,2021-03-03 09:25:35,3
13361860,Avoid latest/master in Github Actions,We should avoid versions {{latest}} (for runners) and {{master}} (for actions) in Github Actions workflows.,pull-request-available,['build'],HDDS,Improvement,Minor,2021-03-02 12:33:41,0
13361844,SCM Ratis enable/disable switch ,"With SCM HA coming in, SCM would be able to operate with/without ratis. The aim of the Jira is to ensure the existing tests work with both Ratis enabled and disabled on SCM.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-02 12:05:34,3
13361833,Add simple CI check for docs,"The goal of this task is to add a separate check for building {{hadoop-hdds/docs}} with Hugo.

Benefits:

# separation of concerns: Currently docs are implicitly built in _integration_ checks.  If there is a syntax error in the docs, all of these fail due to the same cause.  (Used to be built in _findbugs_, _unit_, too, but these no longer have Hugo available.)
# better error reporting: The above checks do not collect compile errors in their summaries.  Thus one needs to dig into build logs to find any such problem.
",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-02 10:50:39,0
13361821,Merge basic CI checks,"Some of the CI checks have code duplication, which could be reduced by using matrix build.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-02 10:21:58,0
13361688,Useless message about node schema location,"SCM (and Recon) log this message during startup:

{code}
INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@6aa3bfc
{code}",pull-request-available,['SCM'],HDDS,Bug,Minor,2021-03-01 20:12:54,0
13361645,ozone admin datanode list filter by UUID broken,"{{ozone admin datanode list}} filter by UUID is broken:

{code}
$ ozone admin datanode list
Datanode: 8b7a289e-3449-49c7-a9d9-0dae7751559d (/rack2/10.5.0.8/ozone-topology_datanode_5_1.ozone-topology_net/3 pipelines)
...
$ ozone admin datanode list --id 8b7a289e-3449-49c7-a9d9-0dae7751559d
$
{code}",pull-request-available,['Ozone CLI'],HDDS,Bug,Minor,2021-03-01 16:22:34,0
13361546,Pre-install awscli in ozone-runner,awscli is installed for S3 tests at runtime.  It could be pre-installed in the ozone-runner docker image to save some test execution time.,pull-request-available,['docker'],HDDS,Improvement,Minor,2021-03-01 08:37:37,0
13361526,Fix removing local SCM when submitting request to other SCM.,This Jira is to fix an issue in removing self ID when building proxy objects to submit requests.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-01 06:58:33,2
13361423,[SCM HA Security] Add failover proxy to SCM Security Server Protocol,"This Jira is to add support for FailOverProxyProvider for SCMSecurityServer which is used by OM and Datanode. (In further jira's when security work is implemented, this API will be used by SCM also)",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-02-28 09:04:12,2
13361159, [SCM HA Security] Implement listCertificates based on role,"This Jira is to implement list certificates based on role.
HDDS-4861 added scm Certs to scmCert Table. In this Jira, listCerts will be modified to list all SCM Certificates.

(Once for other roles like SCM/DN they moved to new tables, listCertiificates will work for all the roles. This is not planned in this Jira)

This API will be used to get all the SCM certificates for SCM HA nodes during bootStrap SCM nodes startup.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-02-26 11:01:09,2
13360935,Bump jetty version,Please see: https://github.com/apache/ozone/pull/1964,pull-request-available,[],HDDS,Improvement,Major,2021-02-25 10:52:45,1
13360933,Bump jackson version number,Please see: https://github.com/apache/ozone/pull/1963,pull-request-available,[],HDDS,Improvement,Major,2021-02-25 10:40:48,1
13360548,[SCM HA Security] Implement generate SCM certificate,"This Jira is to implement generatePeerSCMCertificate which will be called during bootStrap and also during init by primary SCM. This is similar to how OM and DN gets certificate.

",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-02-24 08:14:33,2
13360341,Useless Maven cache cleanup,"The {{build-branch}} Github Actions workflow has a step to avoid caching Ozone artifacts:

{noformat:title=https://github.com/apache/ozone/blob/1cf90150410882cd1ef88cdef33a0de18674399e/.github/workflows/post-commit.yml#L221-L225}
      - name: Delete temporary build artifacts before caching
        run: |
          #Never cache local artifacts
          rm -rf ~/.m2/repository/org/apache/hadoop/hdds
          rm -rf ~/.m2/repository/org/apache/hadoop/ozone
{noformat}

There are 2 problems:

# the cache is in a different job
# there are no {{hdds}} and {{ozone}} subdirectories, only ones like {{hadoop-hdds-common}}, etc.",pull-request-available,['build'],HDDS,Bug,Minor,2021-02-23 13:54:26,0
13360108,Ruby S3 SDK never get authenticated by Ozone,"When the very first call by Ruby client against secure setup of Ozone, the server returns 400 no matter how valid the request is. See the attached ruby-sdk-patch.diff, which adds some tests on S3 auth header signature-to-sign generation. It consists of two test additions, the ""2"" is the one generated by boto3, the ""3"" is generated by aws-ruby-sdk. Both passes the additional tests, which are definitely valid.

However, when real HTTP request is sent by Ruby client, e.g. ozone-test.rb attached, it fails with 400. The header was like this (though the host names and domains are masked):

{quote}GET //ozone.example.com:9879/sandbox?list-type=2&max-keys=1 HTTP/1.1
Content-Type:
Accept-Encoding:
User-Agent: aws-sdk-ruby3/3.112.0 ruby/2.7.2 x86_64-linux aws-sdk-s3/1.88.1
Host: ozone.example.com:9879
X-Amz-Date: 20210222T110554Z
X-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
Authorization: AWS4-HMAC-SHA256 Credential=kota@EXAMPLE.COM/20210222/foobar/s3/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=0c9469f018f5
b3fd2cff6f8d4e4963f50aa71c6704def59527634404f5fc98a9
Content-Length: 0
Accept: */*{quote}

On the other hand, request headers made by boto3 was:

{quote}GET //ozone.example.com:9879/sandbox?list-type=2&encoding-type=url HTTP/1.1
Host: ozone.example.com:9879
Accept-Encoding: identity
User-Agent: Boto3/1.17.12 Python/3.9.1 Linux/5.10.14-arch1-1 Botocore/1.20.12
X-Amz-Date: 20210222T110829Z
X-Amz-Content-SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
Authorization: AWS4-HMAC-SHA256 Credential=kota@EXAMPLE.COM/20210222/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=94302f21cccac8832d3e
4fe25c5f6d8a0307188fb0e1b1983264339381d21dac{quote}

The difference of these requests are IMHO, ""Content-Type"" and ""Accept-Encoding"" are both empty in Ruby SDK. I'm afraid this error stems from partly Ruby SDK and partly from [Jetty Issue|https://github.com/eclipse/jetty.project/issues/2883]. The former sends empty header lines and the latter rejects them.

And the s3g debug log (only error'ish part) follows:
{quote}2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: chain=NoCacheFilter@5e600dd5==org.apache.hadoop.hdds.server.http.NoCacheFilter,inst=true,async=true-
>safety@63a12c68==org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter,inst=true,async=true->info-page-redirect@576d5deb==org.apache.hadoop.ozone.s3.RootPageDis
playFilter,inst=true,async=false->jaxrs@603a422==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=1,inst=true,async=false
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter NoCacheFilter@5e600dd5==org.apache.hadoop.hdds.server.http.NoCacheFilter,inst=true,async
=true
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter safety@63a12c68==org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter,inst=
true,async=true
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter info-page-redirect@576d5deb==org.apache.hadoop.ozone.s3.RootPageDisplayFilter,inst=true,
async=false
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call servlet jaxrs@603a422==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=1,inst=true
,async=false
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: sendError HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=false se=false i=tru
e al=0}
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.session: Leaving scope org.eclipse.jetty.server.session.SessionHandler367746789==dftMaxIdleSec=-1 dispatch=REQUEST, a
sync=false, session=null, oldsession=null, oldsessionhandler=null
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.Server: handled=true async=false committed=true on HttpChannelOverHttp@769bb34b{s=HttpChannelState@4893b376{s=HANDLIN
G rs=BLOCKING os=OPEN is=IDLE awp=false se=true i=true al=0},r=1,c=false/false,a=HANDLING,uri=https://ozone.example.com:9879/sandbox?list-type=2&ma
x-keys=1,age=2}
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: unhandle HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=false se=true i=true
al=0}
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: nextAction(false) SEND_ERROR HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=f
alse se=false i=false al=0}
{quote}",pull-request-available,['S3'],HDDS,Bug,Major,2021-02-22 12:49:55,1
13359617,More compatibility problem with DatanodeDetails.Port.Name.REPLICATION,"Some {{ozone fs}} operations still encounter the compatibility issue attempted to fix in HDDS-4731.

{code}
$ ozone fs -mkdir o3fs://bucket1.vol1/dir/
$ ozone fs -ls o3fs://bucket1.vol1/dir/
$ ozone fs -put /etc/passwd o3fs://bucket1.vol1/dir/
-put: No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION
$ ozone fs -ls o3fs://bucket1.vol1/dir/
-ls: No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION
{code}",pull-request-available,[],HDDS,Bug,Critical,2021-02-19 15:36:31,0
13359613,Intermittent failure in ozonesecure due to unable to allocate block,"{code}
scm_1       | 2021-02-19 02:23:28,815 [IPC Server handler 28 on default port 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 1 pipelines. Required 3. Found 0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:172)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:224)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:134)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:271)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:201)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:192)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:162)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:118)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13265)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
scm_1       | 2021-02-19 02:23:28,815 [IPC Server handler 28 on default port 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
{code}

https://github.com/elek/ozone-build-results/tree/master/2021/02/19/6000/acceptance-secure",pull-request-available,[],HDDS,Bug,Critical,2021-02-19 15:10:22,0
13359396,Add timestamp to Revoked Certs table in SCM DB,Revoked Certs table in SCM DB does not have a timestamp of revocation. We need to add this to keep track of when certificates got revoked.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-02-18 17:49:18,6
13359283,Make changes required for SCM admin commands to work with SCM HA,This Jira is to make changes required to make SCM commands works with SCM HA/non-HA,pull-request-available,[],HDDS,Sub-task,Major,2021-02-18 11:40:30,2
13359272,Use SCM service ID in finding SCM Datanode address.,Use SCM serviceID and nodeID to figure SCM Datanode address and port in HA from SCM address key and SCM Datanode port config or else fall back to ozone.scm.names,pull-request-available,[],HDDS,Sub-task,Major,2021-02-18 10:58:40,2
13358937,Replication failure in secure environment,"The following error prevents closed container replication if {{ozone.security.enabled}} is {{true}}.

{code}
java.lang.IllegalArgumentException: File does not contain valid certificates: certificate.crt
	at org.apache.ratis.thirdparty.io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:345)
	at org.apache.ratis.thirdparty.io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:294)
	at org.apache.hadoop.ozone.container.replication.GrpcReplicationClient.<init>(GrpcReplicationClient.java:81)
{code}

CC [~sodonnell]",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2021-02-16 20:44:00,0
13358901,Support wildcard(*) in ozone.administrators in SCM admin check,"OM supports wildcard * during admin access check, but SCM does not honor this. This Jira is to fix this issue.",pull-request-available,[],HDDS,Bug,Major,2021-02-16 16:34:31,2
13358635,Make SCM Generic config support HA Style,"To use a single config across the cluster, a few of the config keys in SCM need to support HA Style, and they need to be set to default config keys so that the conf can be reusable in other parts of code.

In this Jira, added the following config keys to support HA style config.

      OZONE_SCM_DATANODE_ADDRESS_KEY,
      OZONE_SCM_DATANODE_PORT_KEY,
      OZONE_SCM_DATANODE_BIND_HOST_KEY,
      OZONE_SCM_BLOCK_CLIENT_ADDRESS_KEY,
      OZONE_SCM_BLOCK_CLIENT_PORT_KEY,
      OZONE_SCM_BLOCK_CLIENT_BIND_HOST_KEY,
      OZONE_SCM_CLIENT_ADDRESS_KEY,
      OZONE_SCM_CLIENT_PORT_KEY,
      OZONE_SCM_CLIENT_BIND_HOST_KEY,
      OZONE_SCM_SECURITY_SERVICE_ADDRESS_KEY,
      OZONE_SCM_SECURITY_SERVICE_PORT_KEY,
      OZONE_SCM_SECURITY_SERVICE_BIND_HOST_KEY,
      OZONE_SCM_RATIS_PORT_KEY,
      OZONE_SCM_HTTP_BIND_HOST_KEY,
      OZONE_SCM_HTTPS_BIND_HOST_KEY,
      OZONE_SCM_HTTP_ADDRESS_KEY,
      OZONE_SCM_HTTPS_ADDRESS_KEY,
      OZONE_SCM_DB_DIRS,
      OZONE_SCM_ADDRESS_KEY",pull-request-available,[],HDDS,Sub-task,Major,2021-02-15 11:12:18,2
13358579,Implement scm --bootstrap command,"During SCM --bootstrap, the bootstrapping SCM node will connect to primary SCM node (already running) and get the cluster Id. Once security is implemented, it will also fetch the CSR root certificates from primary SCM during SCM --bootstrap phase. ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-15 05:04:15,3
13358539,Use SCM service ID in SCMBlockClient and SCM Client,"Use SCM service ID in SCMBlockClient and SCM Client

For existing installations, it is also important to fallback to use the ozone.scm.names as well

cc [~shashikant] [~nanda] [~bharat]",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-14 13:19:29,2
13358529,Add multiple SCM nodes to MiniOzoneCluster,This jira adds multiple SCMs to MiniOzoneCluster,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-14 10:49:51,3
13357999,Fresh deploy of Ozone must use the highest layout version by default,"By default, a new cluster should use the latest layout version while being deployed (--init). This is also true for Datanodes. If there is a new DN when the cluster is an unfinalized state, SCM layout version check will not let it become part of a pipeline until finalization.",pull-request-available,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Sub-task,Major,2021-02-10 18:53:01,5
13357716,Move Ratis group creation to scm --init phase,"Currently, the ratis group creation happens post start of scm service. The idea here is move the ratis group initialization to scm --init phase and use cluster id as the SCM HA group ID.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-09 09:07:31,3
13357478,Add install checkpoint in SCMStateMachine,"Currently, https://issues.apache.org/jira/browse/HDDS-4773 adds functionality to download a Rocks db checkpoint from leader node. The idea here is to add functionality to install the downloaded checkpoint and reinitialise SCMStateMachine with latest downloaded checkpoint.",pull-request-available,[],HDDS,Sub-task,Major,2021-02-08 09:22:39,3
13357398,Skip coverage check for PRs and in forks,"Currently _coverage_ CI check:

# calculates combined test coverage
# uploads it to Sonar only for Apache Ozone repo and only for builds on push/schedule
# stores combined coverage in GitHub Actions artifact

Thus for PR in Apache Ozone and for all builds in forks, it only stores coverage in the artifact.  These expire in 30 days and I don't think anybody really checks them manually.

I propose to completely skip _coverage_ check for PRs and in forks, instead of only skipping upload to Sonar.  This would save ~12 minutes for such builds.",pull-request-available,['build'],HDDS,Improvement,Major,2021-02-07 15:13:35,0
13356951,Separate source of Ozone container images to different repositories,"As discussed [here|https://lists.apache.org/thread.html/r222da5ce97d42f945f5bbd4a10bc8d5fab1f0a2324ad2de2ff19c860%40%3Cdev.ozone.apache.org%3E], the current repository (which includes source for apache/ozone-runer, apache/ozone-build, apache/ozone containers) can be separtaed to make it more straightforward to use.",pull-request-available,[],HDDS,Improvement,Major,2021-02-05 07:19:43,1
13356848,No appenders could be found for logger - in tests,"Some tests only output this:

{noformat}
log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdds.utils.db.RDBStore).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2021-02-04 20:35:29,0
13356841,Allow recon access /dbCheckpoint in ozonesecure-om-ha,"Fix the following permission issue in {{ozonesecure-om-ha}} environment:

{code}
om1_1        | ERROR om.OMDBCheckpointServlet: Permission denied: User principal 'recon/recon@EXAMPLE.COM' does not have access to /dbCheckpoint.
om1_1        | This can happen when Ozone Manager is started with a different user.
om1_1        | Please append 'recon/recon@EXAMPLE.COM' to OM 'ozone.administrators' config and restart OM to grant current user access to this endpoint.
{code}",pull-request-available,['docker'],HDDS,Improvement,Trivial,2021-02-04 19:43:23,0
13356462,Improve usability of ozone s3 getsecret output,Please see: https://github.com/apache/ozone/pull/1889,pull-request-available,[],HDDS,Improvement,Major,2021-02-03 10:41:43,1
13356422,Enable mTLS for Ratis in OM HA,The goal of this task is to enable TLS for Ratis in OM HA based on {{SecurityConfig#isGrpcTlsEnabled}}.,pull-request-available,['OM HA'],HDDS,Improvement,Major,2021-02-03 08:50:00,0
13356170,Merge SCMRatisSnapshotInfo and OMRatisSnapshotInfo into a single class,SCMRatisSnapshotInfo and OMRatisSnapshotInfo seem to be duplicate of each other. The idea is to merge them into a single class and use as is.,pull-request-available,[],HDDS,Sub-task,Major,2021-02-02 10:43:19,3
13356162,Disable spotbugs for (the empty) hadoop-ozone-datanode project,Please see: https://github.com/apache/ozone/pull/1878,pull-request-available,[],HDDS,Improvement,Major,2021-02-02 10:21:14,1
13356010,Disable Prevote  in Ratis for Ozone by default,"Pre-vote and Leader Lease are new features in Ratis which may not be required/not tested enough with ozone currently. The idea here is to disable these by default in ozone.

Leader lease is yet be committed : https://issues.apache.org/jira/browse/RATIS-1273",pull-request-available,[],HDDS,Bug,Blocker,2021-02-01 17:32:50,3
13355966,Add functionality to transfer Rocks db checkpoint from leader to follower,"As a part of install Snapshot notification from leader to follower, the follower needs to get the latest Rocks db checkpoint from the follower first . It then needs to reload the stateMachine from the latest state , and then start participating in the ring. This jira aims to add the transfer rocks db checkpointing functionality for SCM HA.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-01 14:56:30,3
13355711,Upgrade Ratis Thirdparty to 0.6.0,"{{ratis-thirdparty}} 0.6.0 was released recently, and now 0.6.0-SNAPSHOT is gone from the repos.",pull-request-available,['build'],HDDS,Task,Blocker,2021-01-31 08:29:35,0
13355652,Intermittent failure in TestOzoneManagerDoubleBufferWithDummyResponse,"{noformat}
Error:  Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.986 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse
Error:  testDoubleBufferWithDummyResponse(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse)  Time elapsed: 0.912 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.testDoubleBufferWithDummyResponse(TestOzoneManagerDoubleBufferWithDummyResponse.java:124)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2021-01-30 12:47:35,0
13355470,Update close-pending workflow for new repo,The Github Actions workflow that looks for pending PRs to be closed does not work currently because it is limited to the old {{hadoop-ozone}} repo.,pull-request-available,['CI'],HDDS,Task,Major,2021-01-29 16:15:49,0
13355444,Owner field of S3AUTHINFO type delegation token should be validated,"Delegation token of Ozone has two flavor:
 1. delegation token (based public key infrastructure provided by SCM CA)
 2. s3 token

S3 token includes all the information which is required to validate a S3 HTTP request: aws access key id, string2sign, signature. OM can check the signature based on all this information which are stored in the OzoneTokenInfo.

When the request is authenticated the owner field is used for all the following authentication. But the content of the follower field is not validated. It's filled by the S3g but any client can create a custom request where the Owner field contains a custom string.

1. To reproduce start an ozonesecure cluster where testuser2 is not an admin. (The easiest way to achieve this is removing the hadoop.security.auth_to_local settings, as in our ozonesecure environment all users are mapped to local root which is admin)

To make the test easier, groups can also be turned off:

{code}
 <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.NullGroupsMapping</value>
  </property>
{code}

2. Check if testuser2 is not an admin:

{code}
kinit -kt /etc/security/keytabs/testuser2.keytab testuser2/scm

klist
Ticket cache: FILE:/tmp/krb5cc_1000
Default principal: testuser2/scm@EXAMPLE.COM

Valid starting     Expires            Service principal
01/29/21 13:27:03  01/30/21 13:27:03  krbtgt/EXAMPLE.COM@EXAMPLE.COM
	renew until 02/05/21 13:27:03


ozone sh volume create /vol3
PERMISSION_DENIED User testuser2/scm@EXAMPLE.COM doesn't have CREATE permission to access volume vol3 null null
{code}

3. To create a s3 type delegation token we need valid string2sign and signature strings.

{code}
ozone s3 getsecret
{code}

Set the environment variables:

{code}
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=....
{code}

Try to create a bucket (will be denied) with --debug flag:

{code}
aws s3api --debug --endpoint=http://localhost:9878 create-bucket --bucket=bucket1
{code}

Copy the signature and string2sign from the output:

{code}
2021-01-29 15:03:52,269 - MainThread - botocore.auth - DEBUG - StringToSign:
AWS4-HMAC-SHA256
20210129T140352Z
20210129/us-west-1/s3/aws4_request
ff6c0c767b0292cf3459d02ae1199d4c7786f3cca2f383a46b442f19d964d996
2021-01-29 15:03:52,269 - MainThread - botocore.auth - DEBUG - Signature:
9830423f18ac1f90ec658d1b5c47bdd7765d67fdc0dc67393c162627bfa45789
{code}

And execute a java app:

{code}
  public static void main(String[] args) throws Exception {
    OzoneConfiguration conf = new OzoneConfiguration();
    conf.set(""ozone.om.address"", ""192.168.32.6"");

    String awsAccessId = ""testuser2/scm@EXAMPLE.COM"";

    UserGroupInformation.setConfiguration(conf);

    UserGroupInformation remoteUser =
        UserGroupInformation.createRemoteUser(awsAccessId, AuthMethod.TOKEN);

    final Text omService = SecurityUtil.buildTokenService(OmUtils.
        getOmAddressForClients(conf));
    OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();
    identifier.setTokenType(S3AUTHINFO);
    identifier.setStrToSign(""AWS4-HMAC-SHA256\n""
        + ""20210129T133557Z\n""
        + ""20210129/us-west-1/s3/aws4_request\n""
        + ""8fc985d9c7442c33d6f146ab123de49b18c83c4c6ccdfd182f10fc78691bdd53"");
    identifier.setSignature(
        ""044cf03375ea10b3e454b16887a1f5ce6ebb14d45b506dd7ac5e02fd0179ba7b"");
    identifier.setAwsAccessId(awsAccessId);
    identifier.setOwner(new Text(""testuser/scm@EXAMPLE.COM""));
    Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),
        identifier.getSignature().getBytes(UTF_8),
        identifier.getKind(),
        omService);

    remoteUser.addToken(token);

    OzoneClient client = remoteUser.doAs(
        (PrivilegedExceptionAction<OzoneClient>) () -> OzoneClientFactory.getRpcClient(conf));
    client.getObjectStore().createVolume(""vol2"");

  }
{code}

As a result /vol2 is created even if testuser2 is not an admin. Note: testuser IS an admin and setOwner used testuser instead of testuser2. 

A quick fix is to validate the owner field. A proper, long-term fix is disabling the s3 auth token type for client2server communication which can be done with HDDS-4440.",pull-request-available,[],HDDS,Bug,Blocker,2021-01-29 14:06:59,1
13355443,Merge ozone-ha and ozone-om-ha-s3,"Both {{ozone-ha}} and {{ozone-om-ha-s3}} run tests in OM HA environment.  I think they can be merged, both to reduce the number of environments and to save some runtime in CI.",pull-request-available,['test'],HDDS,Improvement,Major,2021-01-29 13:52:43,0
13355329,Intermittent failure in ozone-ha acceptance test,"{{ozone-ha}} acceptance test is failing intermittently with:

{code}
'Couldn't create RpcClient protocol' does not contain 'VOLUME_NOT_FOUND'
{code}

Examples:
* https://github.com/elek/ozone-build-results/tree/master/2021/01/27/5516/acceptance-misc/
* https://github.com/elek/ozone-build-results/tree/master/2021/01/28/5536/acceptance-misc/
* https://github.com/elek/ozone-build-results/tree/master/2021/01/28/5550/acceptance-misc/",pull-request-available,['test'],HDDS,Bug,Major,2021-01-29 04:39:15,0
13355166,Adjust classpath of ozone version to include log4j,Please see: https://github.com/apache/ozone/pull/1850,pull-request-available,[],HDDS,Improvement,Major,2021-01-28 11:47:26,1
13355158,Unnecessary WARNING to set OZONE_CONF_DIR,See: https://github.com/apache/ozone/pull/1849,pull-request-available,[],HDDS,Bug,Major,2021-01-28 11:31:53,1
13354439,Modularize upgrade test,"{{upgrade}} acceptance test verifies upgrade from Ozone 0.5.0 to ""snapshot"" (built from current sources).  In order to test upgrade from several earlier versions, it needs to be more modular.",pull-request-available,['test'],HDDS,Task,Minor,2021-01-25 13:32:12,0
13354372,Upgrade Ratis to 1.1.0-eb66796d-SNAPSHOT,Upgrade Ratis snapshot to {{1.1.0-eb66796d-SNAPSHOT}} as we need RATIS-1290 for HDDS-4730.,pull-request-available,[],HDDS,Task,Major,2021-01-25 10:41:34,0
13353904,Fix compatibility issue caused by new add enum DatanodeDetails.Port.Name.REPLICATION,"In tencent, we monthly deploy latest master to our production environment.

When running an upgrade test from old ozone version (at Dec 14 2020) to new ozone version (at Jun 14 2021), testDFSIO using jars of old ozone can not write to the new ozone cluster, met the error that 

No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION

We consider it is related to HDDS-4496. Separate client and server2server GRPC services of datanode.

 ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2021-01-22 05:37:51,0
13353671,Use separate Ratis admin and client ports,Use separate client/server/admin ports being introduced in RATIS-1290.,pull-request-available,[],HDDS,Improvement,Major,2021-01-21 07:51:24,0
13353630,Add token support for container admin operations,"HDDS-2321 disabled token based authentication for container admin commands part of the DataNode admin protocol as that caused problems with requests that are not going through Ozone Manager, as token based auth support is present only there currently.

Within this feature, the followings to be added:
- a new SCM request to get a new kind of token issued by the SCM
- the token would be short living, without renewal or cancellation signed by SCM
- the token will be required for container admin commands inside DataNodes
- the token will be supplied to container admin requests from command line client, and for commands arriving via DN heartbeat responses
- the token is validated on the DN side for every container admin command, and in case a token is not supplied or invalid the DN should reject the request.

Also it is part of the development to revisit all DN API requests and add the appropriate (OM or SCM) token based auth where applicable.",pull-request-available,[],HDDS,New Feature,Major,2021-01-21 02:36:19,0
13353493,Debug utility to export container data,Please see: https://github.com/apache/ozone/pull/1825,pull-request-available,[],HDDS,Improvement,Major,2021-01-20 09:34:39,1
13352284,Add JUnit5 dependency for integration tests,JUnit5 makes it easier to [run tests multiple times|https://junit.org/junit5/docs/current/user-guide/#writing-tests-repeated-tests] via {{\@RepeatedTest(N)}} annotation.  Adding JUnit5 dependency for integration tests would allow upgrading flaky tests one by one for easier debug.,pull-request-available,['test'],HDDS,Task,Major,2021-01-14 15:18:28,0
13352273,Upgrade Java for Sonar check,"bq. The version of Java installed in the scanner environment must be upgraded to at least Java 11 before 1 February 2021. Pre-11 versions of Java are already deprecated and scanners using them will stop functioning on that date. ([source|https://sonarcloud.io/documentation/appendices/end-of-support/]

https://github.com/apache/ozone/runs/1701483011#step:6:2695",pull-request-available,['build'],HDDS,Task,Major,2021-01-14 14:25:52,0
13352072,Handle CRLStatusReport got from DN heartbeats and persist them,We should store the state of CRLStatus of each DN in SCM. This can be done by handling the CRLStatusReport message in heartbeats received from DNs.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-01-13 18:30:11,6
13352025,Disable compression for closed-container replication,"During the measurement of closed container replication I found that the biggest bottleneck is the read side. 5 Gb container is replicated under ~3 minutes but ~2:30 was the downloading part.

Closed containers are replicated via GRPC. The source side creates an OutputStream on-the-fly (OnDemandContainerReplicationSource.java) and stream all the container content as a ""tar.gz"" archive to the client.

It turned out that the compression (the .gz part) is quite expensive:

I created a CLI tool to export containers to tar files (same logic as the replication but without streaming via GRPC, just saving to a file).

I have seen the 2:30 time to create the archive:

{code}
2021-01-13 05:51:25,302 [main] INFO debug.ExportContainer: Preparation is done
2021-01-13 05:53:53,472 [main] INFO debug.ExportContainer: Container is exported to /tmp/container-3.tar.gz
{code}

But when I removed the compression in TarContainerPacker.java, the speed was significant better (25 sec instead of the 150 sec)

{code}
2021-01-13 06:11:46,254 [main] INFO debug.ExportContainer: Preparation is done
2021-01-13 06:12:11,512 [main] INFO debug.ExportContainer: Container is exported to /tmp/container-3.tar
{code}

As a result I suggest turning off the compression for closed container replication.",pull-request-available,[],HDDS,Improvement,Critical,2021-01-13 14:32:33,1
13351890,Change default OM Node ID from UUID to a constant,"If a nodeID is not set explicitly (for a single node OM cluster), then it defaults to the OM storage ID which is an UUID string. Proposing to change this default to a constant (such as ""om1"") instead. 
This would help when a cluster is upgraded to HA for example. It is not straightforward to change the nodeID after Ratis server has been instantiated (as the nodeID is used to generate the RaftPeerID). Hence, it would be good to have a more readable nodeID by default. ",pull-request-available,[],HDDS,Bug,Major,2021-01-13 00:02:29,7
13351866,Intermittent failure in TestSCMRestart,"[~shashikant], I think this is caused by enabling multi-raft and creating more pipelines:

{code:title=https://github.com/elek/ozone-build-results/blob/master/2021/01/12/5149/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.txt}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 31.976 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart
testPipelineWithScmRestart(org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart)  Time elapsed: 0.01 s  <<< FAILURE!
java.lang.AssertionError: expected:<PipelineID=84d01728-7484-4ac8-9900-94ffbef8c967> but was:<PipelineID=c057a116-1d16-4def-985c-1c39482078eb>
  ...
  at org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.testPipelineWithScmRestart(TestSCMRestart.java:127)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-01-12 21:20:17,3
13351790,Support scanning content of DN rocksdb instances with current scheme.,Please see: https://github.com/apache/ozone/pull/1786,pull-request-available,[],HDDS,Improvement,Major,2021-01-12 14:35:05,1
13351683,NodeStateMap leaks internal representation of container sets,Please see: https://github.com/apache/ozone/pull/1782,pull-request-available,[],HDDS,Improvement,Major,2021-01-12 08:30:55,1
13351450,BlockInputStream should give up read retry if pipeline is not updated,"Found it during the usage of a data generator.

 1. I accidentally uploaded keys without checksum data.

  2. With this specific key, the client is moved to an endless loop instead of giving up after the first unexpected exceptions:

{code}
2021-01-11 13:01:50,031 INFO  storage.BlockInputStream (BlockInputStream.java:refreshPipeline(166)) - Unable to read information for block conID: 2 locID: 185 bcsId: 0 from pipeline PipelineID=206da15d-62f6-4e24-93d1-e2e805fc1376: Unexpected OzoneException: org.apache.hadoop.ozone.common.OzoneChecksumException: Original checksumData has no checksums
2021-01-11 13:01:50,047 ERROR scm.XceiverClientGrpc (XceiverClientGrpc.java:sendCommandWithRetry(408)) - Failed to execute command cmdType: ReadChunk
traceID: """"
containerID: 2
datanodeUuid: ""2c124e08-e8a5-4493-a41e-84797984e6a6""
readChunk {
  blockID {
    containerID: 2
    localID: 185
    blockCommitSequenceId: 0
  }
  chunkData {
    chunkName: ""chunk0""
    offset: 0
    len: 4194304
    checksumData {
      type: CRC32
      bytesPerChecksum: 1048576
    }
  }
}
 on the pipeline Pipeline[ Id: 7d5ed2da-7453-4113-b766-4100458dcc16, Nodes: 2c124e08-e8a5-4493-a41e-84797984e6a6{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:, CreationTimestamp2021-01-11T12:01:50.032Z].
2021-01-11 13:01:50,047 INFO  storage.BlockInputStream (BlockInputStream.java:refreshPipeline(166)) - Unable to read information for block conID: 2 locID: 185 bcsId: 0 from pipeline PipelineID=7d5ed2da-7453-4113-b766-4100458dcc16: Unexpected OzoneException: org.apache.hadoop.ozone.common.OzoneChecksumException: Original checksumData has no checksums
2021-01-11 13:01:50,062 ERROR scm.XceiverClientGrpc (XceiverClientGrpc.java:sendCommandWithRetry(408)) - Failed to execute command cmdType: ReadChunk
traceID: """"
containerID: 2
datanodeUuid: ""2c124e08-e8a5-4493-a41e-84797984e6a6""
readChunk {
  blockID {
    containerID: 2
    localID: 185
    blockCommitSequenceId: 0
  }
  chunkData {
    chunkName: ""chunk0""
    offset: 0
    len: 4194304
    checksumData {
      type: CRC32
      bytesPerChecksum: 1048576
    }
  }
}
 on the pipeline Pipeline[ Id: 3a4b5032-6b2f-4297-8c4b-89d715175bb1, Nodes: 2c124e08-e8a5-4493-a41e-84797984e6a6{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:, CreationTimestamp2021-01-11T12:01:50.048Z].
{code}

Please note that the two attempt happens in the same milliseconds.

The problematic part seems to be in the BlockInputStream:

{code}
      try {
        numBytesRead = current.read(b, off, numBytesToRead);
      } catch (IOException e) {
        handleReadError(e);
        continue;
      }
{code}

In case of system exceptions we should ""break"" from the loop instead of ""continue"".

(Normally it's not possible in a production cluster as the data is created with a bad client. But it has security implication: a malicious user can create similar keys which makes a DoS attack: all the clients will retry without sleep...)",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-01-11 12:14:34,0
13351159,Merge OMTransactionInfo with SCMTransactionInfo,"Requirements:
1. Rename OMTransactionInfo to TransactionInfo (use by both OM and SCM), same for the codec.
2. Add extra functions in https://github.com/apache/ozone/blob/HDDS-2823/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMTransactionInfo.java to OMTransactionInfo.

By doing so, we can use the TransactionInfo in HDDS-2823 and remove duplicate code.",pull-request-available,[],HDDS,Sub-task,Major,2021-01-08 20:15:40,3
13351097,No cleanup in TestOzoneFSWithObjectStoreCreate,TestOzoneFSWithObjectStoreCreate does not clean up mini clusters.,pull-request-available,['test'],HDDS,Bug,Major,2021-01-08 14:12:56,0
13350958,Support TDE for MPU Keys on Encrypted Buckets,This Jira is to support MPU on encrypted buckets.,pull-request-available,[],HDDS,New Feature,Major,2021-01-07 19:34:39,2
13349193,Block token verification failed: no READ permission for WriteChunk,"With HDDS-4558 committed, secure acceptance test logs increased considerably (over 1GB).

https://github.com/apache/ozone/actions/runs/462095579

I think the root cause is that {{WriteChunk}} request may need to also {{ReadChunk}}, but now it fails because it only has write access:

{code}
datanode_3  | 2021-01-05 10:41:23,067 [ChunkWriter-1-0] INFO impl.HddsDispatcher: Operation: ReadChunk , Trace ID:  , Message: Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission , Result: BLOCK_TOKEN_VERIFICATION_FAILED , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:214)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
datanode_3  | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:398)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:585)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$read$5(ContainerStateMachine.java:656)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.hdds.security.token.BlockTokenException: Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission
datanode_3  | 	at org.apache.hadoop.hdds.security.token.BlockTokenVerifier.verify(BlockTokenVerifier.java:131)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.validateBlockToken(HddsDispatcher.java:431)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:211)
datanode_3  | 	... 10 more
datanode_3  | 2021-01-05 10:41:23,083 [ChunkWriter-1-0] ERROR ratis.ContainerStateMachine: gid group-5BCDF056E270 : ReadStateMachine failed. cmd ReadChunk logIndex 4 msg : Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission Container Result: BLOCK_TOKEN_VERIFICATION_FAILED
{code}

CC [~elek] [~xyao]",pull-request-available,['Security'],HDDS,Bug,Critical,2021-01-05 12:30:19,0
13349013,OM handle expired certificate when verify token signature,Similar to HDDS-4571 but on OM side. ,pull-request-available,[],HDDS,Sub-task,Major,2021-01-04 22:35:41,4
13348935,Intermittent failure in MapReduce test due to existing output file,"MapReduce acceptance test may fail if the same random number is generated for both O3FS and OFS tests:

{noformat}
<msg timestamp=""20210104 11:37:50.503"" level=""INFO"">${random} = 58</msg>
...
<msg timestamp=""20210104 11:37:50.519"" level=""INFO"">${result} = o3fs://bucket1.volume1.om/wordcount-58.txt</msg>
...
<msg timestamp=""20210104 11:39:16.274"" level=""INFO"">${random} = 58</msg>
...
<msg timestamp=""20210104 11:39:16.295"" level=""INFO"">${result} = ofs://om/volume1/bucket1/wordcount-58.txt</msg>
...
FileAlreadyExistsException: Output directory ofs://om/volume1/bucket1/wordcount-58.txt already exists
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2021-01-04 12:29:12,0
13348906,Safemode wait may end without checking,"Intermittent failure in secure acceptance test:

{noformat:title=output}
Wait for safemode exit                                                | FAIL |
255 != 0
{noformat}

{noformat:title=log}
Running command 'ozone admin safemode wait -t 2 2>&1'.
${rc} = 255
${output} = WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Safe mode is not ended within the timeout period.
{noformat}

This is executed with safemode already OFF.  Also, notice that neither {{SCM is out of safe mode}}, nor {{SCM is in safe mode}} is shown.

It indicates that {{ozone admin safemode wait}} may conclude without actually checking status, if it takes more time to create SCM client than the given timeout:

{code:title=https://github.com/apache/ozone/blob/aacba62a1a1bdee45dad15e82ef64019ff67b767/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SafeModeWaitSubcommand.java#L60-L62}
    while (getRemainingTimeInSec() > 0) {
      try (ScmClient scmClient = scmOption.createScmClient()) {
        while (getRemainingTimeInSec() > 0) {
{code}

It is reproducible by adding {{sleep}} in {{createScmClient()}}.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2021-01-04 10:02:49,0
13348103,Disable coverage upload to codecov,Please see: https://github.com/apache/ozone/pull/1741,pull-request-available,[],HDDS,Improvement,Major,2020-12-28 10:04:17,1
13347100,Update bouncycastle to 1.67,"The latest bouncy castle contains important fixes. Ozone is not affected by the bcrypt issue, but it seems to be safer updating to the latest version.

(Thanks to [~aengineer] how reported this issue)",pull-request-available,[],HDDS,Improvement,Trivial,2020-12-21 08:17:42,1
13346650,Fix issues in 'prepare' operation with one OM down.,"On a 3 node OM HA setup, when a prepare is done with 1 OM down, it leads to a state where the leader and follower are fully prepared (Snapshot at last index and logs purged). When the 3rd node rejoins the quorum, it leads to an infinite installSnapshot loop between the leader and the 3rd node, and the system goes into a bad state until a restart is done.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-17 19:13:07,5
13346355,Enable Multi Raft by default in Ozone,"Currently in ozone cluster, when the no of datanodes is not multiple of 3, the only way to make use of all datanodes for read/write is to enable multiraft. The idea here is to enable this feature by default in ozone.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2020-12-16 12:55:04,3
13346284,envtoconf broken for .conf and few other formats,"{{envtoconf}} does not work for some output formats:

 * {{.env}}
 * {{.sh}}
 * {{.cfg}}
 * {{.conf}}

To reproduce:

{code:title=docker run -it --rm -e DUMMY.CONF_key=value apache/ozone:1.0.0}
Traceback (most recent call last):
  File ""/opt/hadoop/libexec/envtoconf.py"", line 117, in <module>
    Simple(sys.argv[1:]).main()
  File ""/opt/hadoop/libexec/envtoconf.py"", line 108, in main
    self.transform()
  File ""/opt/hadoop/libexec/envtoconf.py"", line 96, in transform
    content = transformer_func(content)
  File ""/opt/hadoop/libexec/transformation.py"", line 121, in to_conf
    for key, val in props:
ValueError: too many values to unpack
{code}",pull-request-available,['docker'],HDDS,Bug,Major,2020-12-16 07:43:48,0
13346251,"OM Terminates when adding acls to ""S3v"" volume","
{code:java}
2020-12-14 10:04:58,612 [OM StateMachine ApplyTransaction Thread - 0] ERROR ratis.OzoneManagerStateMachine: Terminating with exit status 1: Request cmdType: AddAcl
traceID: """"
clientId: ""client-CAE43266CDF2""
userInfo {
  userName: ""om/om-server@DEV.COM""
  remoteAddress: ""10.101.211.49""
  hostName: ""om-server""
}
addAclRequest {
  obj {
    resType: VOLUME
    storeType: OZONE
    path: ""/s3v""
  }
  acl {
    type: USER
    name: ""airflow@DEV.TAP""
    rights: ""\200""
    aclScope: ACCESS
  }
}
failed with exception
java.lang.IllegalArgumentException: Trying to set updateID to 213 which is not greater than the current value of 36028797018963967 for OMVolumeArgs{volume='s3v', admin='hadoop', owner='hadoop', creationTime='1606808208748', quota='1152921504606846976'}
       at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142)
...
       at java.base/java.lang.Thread.run(Thread.java:834)
2020-12-14 10:04:58,635 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG:
{code}
",pull-request-available,[],HDDS,Bug,Blocker,2020-12-16 05:20:18,2
13346036,Duplicate dependency hadoop-hdds-hadoop-dependency-server in datanode,"{code}
Warning:  Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-datanode:jar:1.1.0-SNAPSHOT
Warning:  'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdds-hadoop-dependency-server:jar -> duplicate declaration of version (?) @ line 41, column 17
Warning:  
Warning:  It is highly recommended to fix these problems because they threaten the stability of your build.
Warning:  
Warning:  For this reason, future Maven versions might no longer support building such malformed projects.
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-12-15 05:48:16,0
13345744,Coverage not updated since TLP,"Ozone's GitHub Actions CI workflow references the old {{hadoop-ozone}} repository, so coverage data has not been uploaded to Sonar and Codecov for two months.",pull-request-available,['build'],HDDS,Bug,Major,2020-12-13 17:34:52,0
13345620,TableCache Refactor to fix issues in cleanup never policy,"Right now we have 2 clean up policies.
1. Never
2. Manual

Never = Full Table Cache
Manual = Partial Table Cache

In OM, the main purpose of Table cache is for correctness. (Because OM return response after adding to cache, does not wait for double buffer flush to complete)

The current implementation has few problems.
1. Cleanup Policy Never uses ConcurrentSkipListMap, and its computeIfPresent is not atomic, so there can be a race condition between cleanup and requests adding to cache. (This might cause cleaning up entries which are not flushed to DB, and this can cause correctness issue)
2. Cleanup for override entries for full cache, never removes epoch entries.

*Proposal:*
1. Make TableCache based on cache type and have separate implementation for full cache and partial cache.
2. Fix FullCache issue, using the lock.
3. Fix evict cache logic for full cache to cleanup epoch entries for override entries.

",pull-request-available,[],HDDS,Improvement,Major,2020-12-12 01:38:20,2
13345593,Cleanup usage of volumeArgs in KeyRequests,"This Jira proposes to remove volumeArgs usage in the KeyRequests, after HDDS-4308, KeyRequests does not update volume used.

So, request/response classes do not require volumeArgs during bytes calculation.

This Jira proposes to cleanup usage of volumeArgs.",pull-request-available,[],HDDS,Bug,Major,2020-12-11 20:44:58,2
13345523,Add acceptance test for Ozone Client Key Validator,Add acceptance test for Freon's Ozone Client Key Validator.,pull-request-available,['test'],HDDS,Improvement,Major,2020-12-11 13:30:53,0
13345363,Intermittent failure in TestOzoneClientRetriesOnException#testMaxRetriesByOzoneClient,"TestOzoneClientRetriesOnException#testMaxRetriesByOzoneClient intermittently fails with:

{code:title=https://github.com/elek/ozone-build-results/blob/master/2020/12/08/4407/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException.txt}
testMaxRetriesByOzoneClient(org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException)  Time elapsed: 42.789 s  <<< FAILURE!
java.lang.AssertionError: Expected exception not thrown
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException.testMaxRetriesByOzoneClient(TestOzoneClientRetriesOnException.java:235)
{code}

The problem happens if the number of distinct containers used for the key is less than 4.  In this case max retries (of 3) is never reached.

Normal execution:

{code}
block: conID: 1 locID: 105357240184995843 bcsId: 0
block: conID: 2 locID: 105357240185061380 bcsId: 0
block: conID: 3 locID: 105357240185061381 bcsId: 0
block: conID: 4 locID: 105357240185061382 bcsId: 0
{code}

Failing execution:

{code}
block: conID: 1 locID: 105357233754603523 bcsId: 0
block: conID: 2 locID: 105357233754603524 bcsId: 0
block: conID: 1 locID: 105357233754669061 bcsId: 0
{code}",pull-request-available,['test'],HDDS,Bug,Critical,2020-12-10 19:10:52,0
13345293,TestDefaultCertificateClient misuses chars param of random(),"{{TestDefaultCertificateClient}} passes ""UTF-8"" to {{RandomStringUtils.random(int, String)}} with the intention to use UTF-8 encoding.  It was added in HDDS-1087 to fix (intermittent?) failure in CI.

However, the parameter is not for character encoding, rather:

{code}
     * @param chars  the String containing the set of characters to use,
     *  may be null, but must not be empty
{code}

So this results in values like: {{""--8F8T8U8T...""}}.",pull-request-available,['test'],HDDS,Bug,Minor,2020-12-10 12:52:16,0
13344944,Prepare client should check every OM individually for the prepared check based on Txn Id.,"* After getting a successful response for the OMPrepareRequest, the prepare client should use the Txn ID from the response to check every OM's preparation completeness.
* This JIRA is partially dependent on HDDS-4569 which plans to introduce a marker file at the end of preparation. In a follow up JIRA, this will be refined to take up that logic.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-09 00:34:40,5
13344653,Support Ozone block token with access mode check,Thanks [~elek] for reporting the issue. There is a TODO in the Ozone block token verifier to support this. Currently the block token is giving all access to the client without access mode check. This ticket is opened to set access mode properly for block token and validate them in the token verifier. ,pull-request-available,['Security'],HDDS,Bug,Major,2020-12-07 20:19:01,4
13344593,S3 and HCFS interoperability,"This jira will be used to track and finish the design discussion which is started in HDDS-4097.

This design doc defines the current behavior and defines a possible 3rd option which is not yet implemented. HDDS-4097 contains the implementation details of one specific option.  ",pull-request-available,[],HDDS,Bug,Major,2020-12-07 14:32:43,1
13344576,Add smoketest for ozonescripts environment,"{{start-ozone.sh}} and {{stop-ozone.sh}} are currently not exercised by any tests, so they can be inadvertently broken without CI feedback.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-12-07 13:26:31,0
13344310,ChunkInputStream should release buffer as soon as last byte in the buffer is read,"We currently wait for reading up till the Chunk EOF before releasing the buffers in ChunkInputStream (or when the stream is closed). Let's say a client reads first 3 MB of a chunk  of size 4MB and does not close the stream immediately. This would lead to the 3MB of data being cached in the ChunkInputStream buffers till the stream is closed.

Once HDDS-4552 is resolved, a chunk read from DN would return the chunk data as an array of ByteBuffers. After each ByteBuffer is read, it can be released. This would greatly help with optimizing memory usage of ChunkInputStream.",pull-request-available,[],HDDS,Improvement,Major,2020-12-04 23:57:04,7
13344309,Read data from chunk into ByteBuffer[] instead of single ByteBuffer,"When a ReadChunk operation is performed, all the data to be read from one chunk is read into a single ByteBuffer. 
{code:java}
#ChunkUtils#readData()
public static void readData(File file, ByteBuffer buf,
    long offset, long len, VolumeIOStats volumeIOStats)
    throws StorageContainerException {
  .....
  try {
    bytesRead = processFileExclusively(path, () -> {
      try (FileChannel channel = open(path, READ_OPTIONS, NO_ATTRIBUTES);
           FileLock ignored = channel.lock(offset, len, true)) {

        return channel.read(buf, offset);
      } catch (IOException e) {
        throw new UncheckedIOException(e);
      }
    });
  } catch (UncheckedIOException e) {
    throw wrapInStorageContainerException(e.getCause());
  }
  .....
  .....{code}
This Jira proposes to read the data from the channel and put it into an array of ByteBuffers each with a set capacity. This capacity can be configurable. 

This would help with optimizing Ozone InputStreams in terms of cached memory. Currently, data in ChunkInputStream is cached till either the stream is closed or the chunk EOF is reached. This sometimes leads to upto 4MB (default ChunkSize) of data being cached in memory per ChunkInputStream.

After the proposed change, we can optimize ChunkInputStream to release a ByteBuffer as soon as that ByteBuffer is read instead of waiting to read the whole chunk (HDDS-4553). Read I/O performance will not be affected as the read from DN still returns the requested length of data at one go. Only difference would be that the data would be returned in an array of ByteBuffer instead of a single ByteBuffer.",pull-request-available,[],HDDS,Improvement,Major,2020-12-04 23:49:06,7
13343864,Add a new OM admin command to submit the OMPrepareRequest.,"* Introduce a new OM client operation to ""prepare"" the OM quorum.
* As a first pass, the client will just submit the request (HDDS-4480) and print out the response (Txn ID)
* In a follow up JIRA, the subsequent steps to probe every individual OM for preparation completeness will be added.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-02 21:00:58,5
13343666,Remove refreshPipeline in listKeys ,"In few usecases like delete which iterates and listKeys to get KeyNames to deleteKeys, we donot need to refreshPipeline, and also in S3 listKeys also we don't need Keys with PipelineInfo.

Doing this will help in improving the performance of delete/rename/listKeys API.

As RpcClient is returning only info like KeyName, length, replication type,  factor, modificationTime, creationTime. So we don't really need an extra param, as previously even though server sent pipelineInfo it is not used/returned to the client. So we don't need extra param, we can remove refresh pipeline in OM Server.",pull-request-available,[],HDDS,Improvement,Major,2020-12-02 00:38:45,2
13343615,Add more unit tests for OM layout version manager.,"* Add test to make sure an unsupported OM request is rejected.
* Add test to make sure all new OM requests have the getRequestType method defined.
* Minor code refactor.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-01 17:19:14,5
13343531,Use fixed thread pool for closed container replication ,"Number of threads for closed container replications can be adjusted by the settings  {{hdds.datanode.replication.streams.limit}}. But this number is ignored today due to the misuse of {{ThreadPoolExecutor}}:

{code}
new ThreadPoolExecutor(
        0, poolSize, 60, TimeUnit.SECONDS,
        new LinkedBlockingQueue<>(),
        new ThreadFactoryBuilder().setDaemon(true)
            .setNameFormat(""ContainerReplicationThread-%d"")
            .build())
{code}

Here the minimal number of threads is 0 and the maximum number of the threads is the configured value.  Threads in the thread pool supposed to be scaled up, but it doesn't.

[From the JDK docs|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#ThreadPoolExecutor(int,%20int,%20long,%20java.util.concurrent.TimeUnit,%20java.util.concurrent.BlockingQueue)]:

bq. A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize()) according to the bounds set by corePoolSize (see getCorePoolSize()) and maximumPoolSize (see getMaximumPoolSize()). When a new task is submitted in method execute(java.lang.Runnable), [...] [AND]  If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full.

So if queue is not full (and {{LinkedBlockgingQueue}} is unbounded by default) the threads will never be created.

For a quick fix we can switch to use static thread pool instead of dynamic and always keep the required number of threads.",pull-request-available,[],HDDS,Sub-task,Major,2020-12-01 11:52:04,1
13343465,Update pipeline db when pipeline state is changed,In the new {{PipelineManager}} implementation we should update the pipeline table (RocksDB) when there is a pipeline state change.,pull-request-available,[],HDDS,Sub-task,Major,2020-12-01 05:13:55,3
13343327,Remove false-positive error logs from LeaseManager,"There is an error log which can be seen frequently in the log:

{code}
2020-11-30 15:35:58,539 [CommandWatcher-LeaseManager#LeaseMonitor] ERROR lease.LeaseManager (LeaseManager.java:run(238)) - Execution was interrupted 
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:234)
	at java.lang.Thread.run(Thread.java:748)
{code}

This log is introduced by HDDS-2561 to make Sonar happy, but it was not required as LeaseManager use the thread interrupt intentionally.

For a proper fix we can keep the logging on WARN level AND replace thread interrupts with thread notify/wait.
 ",pull-request-available,[],HDDS,Improvement,Trivial,2020-11-30 14:38:32,1
13343323,Replace Hadoop variables and functions in Ozone shell scripts with Ozone-specific ones,"Currently Ozone relies on {{HADOOP_\*}} environment variables (eg. {{HADOOP_HOME}}) for historical and practical reasons (code reuse).  This can lead to unexpected results if Hadoop and Ozone are both present on a node and they share their environment.  Eg. we had to implement a workaround for finding {{ozone-config.sh}} relative to the script being executed when {{HADOOP_HOME}} points to Hadoop, not Ozone (HDDS-1912 and HDDS-4450).

Another similar severe problem happens if we would like to access Ozone filesystem both via {{ozone}} and {{hadoop}} commands.  The latter needs shaded Ozone FS JAR in {{HADOOP_CLASSPATH}}.  The same {{HADOOP_CLASSPATH}} results in {{ClassNotFound}} for {{ozone}}.

The solution proposed in HDDS-1912:

bq. Long-term we may need to replace all the HADOOP_ environment variable with an OZONE_ environment variable, but that would be a significant bigger change.

This would allow using different classpaths, logging parameters, and more.

To be backward compatible, we should use existing {{HADOOP_}} variables as fallback for the corresponding {{OZONE_}} ones, but let {{OZONE_X}} take precedence over {{HADOOP_X}} if the former is defined.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Critical,2020-11-30 14:24:39,0
13343305,Create freon test to measure closed container replication,Create new freon test for container download,pull-request-available,[],HDDS,Sub-task,Major,2020-11-30 12:54:12,1
13342722,Datanodes should be able to persist and load CRL,Ozone Datanodes should be able to persist or load Certificate Revocation List (CRL) ,pull-request-available,['Security'],HDDS,Sub-task,Major,2020-11-25 23:01:49,6
13342622,Remove unused netty3 transitive dependency,"Ozone uses Netty either as direct dependency (ozone-csi) or from the ratis shaded dependency (for ratis gprc server). Both use Netty 4.x.

But netty 3 is also included in share/lib/ozone which is not required. The declared netty 3 version has security issues, we need to remove it to make it clear it's not used. (And make classpath safer)

It turned out that netty (and other dependencies) came with the test-jar dependencies used from Hadoop.

Based on the reference of Maven, compile time dependencies of a test dependency should be used as test dependency (https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html) but in this case it doesn't work:

{code}
cd hadoop-hdds/container-service
mvn dependency:tree

...
[INFO] +- org.apache.hadoop:hadoop-hdfs:test-jar:tests:3.2.1:test
[INFO] |  +- org.eclipse.jetty:jetty-server:jar:9.4.34.v20201102:test
[INFO] |  |  +- org.eclipse.jetty:jetty-http:jar:9.4.34.v20201102:test
[INFO] |  |  \- org.eclipse.jetty:jetty-io:jar:9.4.34.v20201102:test
[INFO] |  +- org.eclipse.jetty:jetty-util-ajax:jar:9.4.34.v20201102:test
[INFO] |  +- com.sun.jersey:jersey-core:jar:1.19:test
[INFO] |  |  \- javax.ws.rs:jsr311-api:jar:1.1.1:test
[INFO] |  +- com.sun.jersey:jersey-server:jar:1.19:test
[INFO] |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.11:compile
[INFO] |  +- commons-daemon:commons-daemon:jar:1.0.13:test
[INFO] |  +- javax.servlet:javax.servlet-api:jar:3.1.0:test
[INFO] |  +- io.netty:netty:jar:3.10.5.Final:compile
[INFO] |  +- org.apache.htrace:htrace-core4:jar:4.1.0-incubating:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-databind:jar:2.10.3:compile
[INFO] \- junit:junit:jar:4.11:test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
...
{code}

Here all the dependencies of the hadop-hdfs:test-jar suppposed to have test scope.

I didn't find the exact MVN issue, but found that there are multiple open issues related to transitive dependency resolution (can be the https://issues.apache.org/jira/browse/MNG-1378, but there are other open issues, too).

As a result, we should remain on the same side. I ssugest:

 1. Exclude ALL the TRANSITIVE test dependencies for hadoop test-jars. Hadoop test-jars can still be used, but if we need any other class, they should be requested with an explicit dependency

 2. hadoop-ozone-dependency-test should be used everywhere instead of using hadoop-hdfs or hadoop-common test jars (because it includes all the required excludes ;-) ) ",pull-request-available,[],HDDS,Bug,Major,2020-11-25 14:39:41,1
13342352,Support query parameter based v4 auth in S3g,"AWS S3 accepts authorization information both from headers and query parameter.

Ozone s3g implementation parses only the headers.",pull-request-available,[],HDDS,Improvement,Major,2020-11-24 12:00:07,1
13342313,Provide info on block size via FileSystem,Provide information about Ozone's configured block size via Hadoop-compatible file system implementations: {{getDefaultBlockSize()}} and {{getDefaultBlockSize(Path)}} in {{FileSystem}}.,pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2020-11-24 09:21:07,0
13342238,Reload OM State fail should terminate OM for any exceptions,"Right now OM is terminating only when IOException when reload terminate happens, OM should terminate in case of any exceptions.

In one of our customer issue we have seen reloadOMState failed with IllegalStateException due to a bug which is fixed by HDDS-3944 and caused below issue, as installSnapshot failed we donot update commitIndex, lastWrittenIndex in SegmentedRaftLogWorker.



{code:java}
2020-11-20 01:17:01,990 ERROR org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker: om3@group-0C953F6B62D0-SegmentedRaftLogWorker hit exception
java.lang.IllegalStateException: lastWrittenIndex == 134264, entry == term: 393
index: 134278
metadataEntry {
  commitIndex: 134277
}

        at org.apache.ratis.util.Preconditions.assertTrue(Preconditions.java:63)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$WriteLog.execute(SegmentedRaftLogWorker.java:507)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:302)
        at java.base/java.lang.Thread.run(Thread.java:834)
{code}

",pull-request-available,[],HDDS,Bug,Major,2020-11-24 01:33:10,2
13342190,Enable OM Ratis by default,"Currently, by default, Ratis is not enabled on an OM single node cluster. This Jira proposes to change the default single node OM to use Ratis server. 

OM Ratis has been tested extensively and is stable now. To convert a single node cluster to HA, the first step would be to upgrade the cluster to Ratis enabled cluster. 

A non-ratis single node cluster can be upgraded to  ratis-enabled single node cluster by just setting the \{{ozone.om.ratis.enable}} config to true and restarting the OM.",pull-request-available,['OM'],HDDS,Improvement,Major,2020-11-23 18:16:22,7
13342185,Recon File Size Count task throws SQL Exception.,"{code}
Caused by: org.jooq.exception.DataAccessException: SQL [insert into FILE_COUNT_BY_SIZE (volume, bucket, file_size, count) values (?, ?, ?, ?)]; [SQLITE_CONSTRAINT]  Abort due to constraint violation (UNIQUE constraint failed: FILE_COUNT_BY_SIZE.volume, FILE_COUNT_BY_SIZE.bucket, FILE_COUNT_BY_SIZE.file_size)
        at org.jooq_3.11.9.SQLITE.debug(Unknown Source)
        at org.jooq.impl.Tools.translate(Tools.java:2429)
        at org.jooq.impl.DefaultExecuteContext.sqlException(DefaultExecuteContext.java:832)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:364)
        at org.jooq.impl.TableRecordImpl.storeInsert0(TableRecordImpl.java:202)
        at org.jooq.impl.TableRecordImpl$1.operate(TableRecordImpl.java:173)
        at org.jooq.impl.RecordDelegate.operate(RecordDelegate.java:125)
        at org.jooq.impl.TableRecordImpl.storeInsert(TableRecordImpl.java:169)
        at org.jooq.impl.TableRecordImpl.insert(TableRecordImpl.java:157)
        at org.jooq.impl.TableRecordImpl.insert(TableRecordImpl.java:152)
        at org.jooq.impl.DAOImpl.insert(DAOImpl.java:175)
        at org.jooq.impl.DAOImpl.insert(DAOImpl.java:151)
        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.lambda$writeCountsToDB$0(FileSizeCountTask.java:209)
        at java.util.HashMap$KeySet.forEach(HashMap.java:933)
        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.writeCountsToDB(FileSizeCountTask.java:181)
        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.reprocess(FileSizeCountTask.java:100)
        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$reInitializeTasks$3(ReconTaskControllerImpl.java:175)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        ... 3 more
Caused by: org.sqlite.SQLiteException: [SQLITE_CONSTRAINT]  Abort due to constraint violation (UNIQUE constraint failed: FILE_COUNT_BY_SIZE.volume, FILE_COUNT_BY_SIZE.bucket, FILE_COUNT_BY_SIZE.file_size)
        at org.sqlite.core.DB.newSQLException(DB.java:941)
        at org.sqlite.core.DB.newSQLException(DB.java:953)
        at org.sqlite.core.DB.execute(DB.java:854)
        at org.sqlite.core.DB.executeUpdate(DB.java:895)
        at org.sqlite.jdbc3.JDBC3PreparedStatement.executeUpdate(JDBC3PreparedStatement.java:102)
        at org.jooq.tools.jdbc.DefaultPreparedStatement.executeUpdate(DefaultPreparedStatement.java:99)
        at org.jooq.impl.AbstractDMLQuery.execute(AbstractDMLQuery.java:629)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350)
        ... 17 more
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Critical,2020-11-23 17:45:19,5
13342121,Separate client and server2server GRPC services of datanode,"Today both the closed-container-replication service and datanode client service are exposed on the same datanode port which makes it impossible to use different network configuration (like mutual TLS). 

In this patch I propose to separated the two service and use two network port. ",incompatible pull-request-available,[],HDDS,Improvement,Critical,2020-11-23 12:41:32,1
13341983,Provide Guice module to inject configuration to annotated fields,"hadoop-hdds/config project provides a light-weight annotation based configuration interface. It supports to create an object with injection:


{code:java}
ReplicationConfig replicationConfig := ozoneConfig.getObject(replicationConfig); {code}
However, it seems to be hard to inject configuration to the service itself as usually we inject a lot of other dependencies to the constructor, not just the configuration.

 

One possible solution is using a Guice module. Guice is already used by recon, and this patch adds some optional modules to inject configuration variables to any service / instance if required.",pull-request-available,[],HDDS,Improvement,Major,2020-11-22 12:59:56,1
13341464,Use RatisServerImpl isLeader instead of periodic leader update logic in OM and isLeaderReady for read requests,"Previously RatisServer which is network-partitoned/split-brain does not step down from leader state, so we require custom logic to determine leader(But that is also not completely correct, as the role state is updated based on server state not a quorum based information)

Now, in Ratis leader steps down after the leader election max time out, so we can use RaftServer Api isLeader check. (RATIS-981 fixed this behavior)

This also fixes when serving read requests it should check leaderReady, not just isLeader because when it is leader, it might be still applying pending commit transactions, so there is a chance that acked transactions of write, might not be visible until we wait for isLeaderReady.
",pull-request-available,[],HDDS,Improvement,Major,2020-11-19 00:45:22,2
13341404,Datanodes should send last processed CRL sequence ID in heartbeats,Datanodes should include their latest processed CRL sequence ID in heartbeats to keep SCM updated on the status of CRL.  ,pull-request-available,['Security'],HDDS,Sub-task,Major,2020-11-18 18:15:49,6
13341403,SCM should be able to persist CRL,Ozone SCM should be able to persist Certificate Revocation List (CRL). We should add a new table in SCM DB definition to persist CRL and sequence id.,pull-request-available,['Security'],HDDS,Sub-task,Major,2020-11-18 18:13:05,6
13341235,With HA OM can send deletion blocks to SCM multiple times,"Currently OM deletion service iterates through deleted keys table and sends blocks for deletion to SCM. I can see that blocks for same key are sent 10 times from OM to SCM. This would lead to creation of 10 transactions for the same blocks in SCM.
Apparently the db is not updated and OM keeps sending the same set of keys for about 10 minutes.

cc [~bharat] [~hanishakoneru]",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2020-11-18 06:46:40,2
13340814,Update url in .asf.yaml to use TLP project,"Change URL on github page to the new TLP domain.

 

Thanks the report for [~cxorm]

https://github.com/apache/ozone/pull/1540#issuecomment-722217690

 ",pull-request-available,[],HDDS,Improvement,Trivial,2020-11-16 10:57:21,1
13340496,Reuse compiled binaries in acceptance test,"Save Ozone binaries from {{compile}} check, and use these in {{acceptance}} and {{kubernetes}} checks, instead of each check performing its own full build.  Total execution time is similar, but the dependent checks are started later, so we save GitHub Actions cycles.",pull-request-available,['build'],HDDS,Improvement,Major,2020-11-13 11:08:36,0
13340490,Add metrics for closed container replication,Today it's hard to understand what's going on and what is the performance of the replication.,pull-request-available,[],HDDS,Sub-task,Major,2020-11-13 10:25:58,1
13339996,Replicate closed container from random selected datanode ,"[~AlfredChang] reported a very particular case related to the data replication:

When a container is downloaded successfully but the content is invalid, the error the retry mechanism doesn't work very well. Downloader reports the download successfull therefore it won't be reported as failure, but import will fail again and again.

To make it more resilient we can select datanode randomly,",pull-request-available,[],HDDS,Sub-task,Major,2020-11-11 14:29:47,1
13339955,findbugs.sh couldn't be executed after a full build,"./hadoop-ozone/dev-support/checks/findbugs.sh -- which is a short-cut to execute the CI findbugs check locally -- couldn't be executed locally after a full build:

{code}
./hadoop-ozone/dev-support/checks/findbugs.sh
....
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3.451 s
[INFO] Finished at: 2020-11-11T11:42:40+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs (spotbugs) on project hadoop-hdds: Execution spotbugs of goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs failed: Java returned: 1 -> [Help 1]
[ERROR] 
{code}

The problem:

`target/classes` directory should be either empty/missing or it should contain java classes to make spotbugs work.

On github it works well as an empty checkout is tested. But locally it's possible that a dummy classpath file is created under `hadoop-hdds/target/classes` which breaks spotbug local execution.

The solution is easy: execute the classpath descriptor generation only if `src/main/java` dir exists.",pull-request-available,[],HDDS,Bug,Major,2020-11-11 10:44:44,1
13339912,Cannot run ozone if HADOOP_HOME points to Hadoop install,"Currently Ozone relies on {{HADOOP_\*}} environment variables (eg. {{HADOOP_HOME}}) for historical and practical reasons (code reuse).  If Hadoop and Ozone are both present on a node, and they share their environment, then {{ozone}} (and {{stop-ozone.sh}}) exits with error:

{code}
$ ozone help
ERROR: Cannot execute /usr/local/hadoop/libexec/ozone-config.sh.
{code}

The same problem was reported and fixed in HDDS-1912 for the {{start-ozone.sh}} script.  This task is limited to applying the changes from HDDS-1912 to the other two scripts.  Ideally Ozone should use its own environment variables, which is a much larger change.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-11-11 08:08:38,0
13339736,Duplicate refreshPipeline in listStatus,"{{KeyManagerImpl#listStatus}} issues duplicate {{refreshPipeline}} for each file.

HDDS-3824 moved {{refreshPipeline}} outside the bucket lock.  But HDDS-3658 added it back, while keeping the one outside the lock, probably as a result of merge conflict resolution.

Also, [~weichiu] pointed out that {{refreshPipeline}} supports lists, too, so the calls can be batched to reduce number of RPC.

CC [~msingh]",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-11-10 12:19:19,0
13339657,Owner info is not passed to authorizer for BUCKET/KEY create request,"HDDS-4088 add the owner info to the authorizer access check context. There is a bug in the getOwner info logic which is supposed to skip volume owner info only for volume create or volume root access. 

This ticket is opened to fix the issue above. ",pull-request-available,[],HDDS,Bug,Major,2020-11-10 05:02:15,4
13339651,Recon: Using Mysql database throws exception and fails startup,"Recon uses Derby as its SQL database by default. Switching it to Mariadb/Mysql throws an exception `Specified key was too long; max key length is 767 bytes`

Fix this issue by reducing the primary key length from VARCHAR(768) to VARCHAR(766) in ReconTaskSchemaDefinition.java. Also change the autocommit config to default to true since data is not persisted in MySql tables even after insert queries are executed with autocommit flag set to false by default.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-11-10 03:19:17,6
13339507,Design Doc: Use per-request authentication and persistent connections between S3g and OM,"Design doc will be uploaded, soon.",pull-request-available,[],HDDS,Improvement,Major,2020-11-09 11:21:22,1
13338742,Update Ratis version to latest snapshot,"This Jira aims to update ozone with latest Ratis snapshot which has a critical fix for ""Bootstrap new OM Node"" feature - HDDS-4330.",pull-request-available,[],HDDS,Bug,Major,2020-11-03 23:18:52,7
13338726,OM failover timeout is too short,"The current OM has one second failover timeout. This is too short as any network hiccup, system I/O or JVM GC pause could easily trigger a failover.

Example:
{noformat}
2020-10-29 09:02:46,557 WARN org.apache.ratis.server.impl.RaftServerImpl: om3@group-942F8267F22A-LeaderState: Lost leadership on term: 33. Election timeout: 1200ms. In charge for: 82665
0319ms. Conf: 32189729: [om1:rhelnn01.ozone.cisco.local:9872:0, om3:rhelnn03.ozone.cisco.local:9872:0, om2:rhelnn02.ozone.cisco.local:9872:0], old=null. Followers: [om3@group-942F8267F2
2A->om1(c34577386,m34577394,n34577395, attendVote=true, lastRpcSendTime=7, lastRpcResponseTime=0), om3@group-942F8267F22A->om2(c34577386,m34577261,n34577395, attendVote=true, lastRpcSen
dTime=7, lastRpcResponseTime=0)]
2020-10-29 09:02:46,558 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2236ms
No GCs detected
2020-10-29 09:02:46,562 INFO org.apache.ratis.server.impl.RaftServerImpl: om3@group-942F8267F22A: changes role from    LEADER to FOLLOWER at term 33 for stepDown
2020-10-29 09:02:46,563 INFO org.apache.ratis.server.impl.RoleInfo: om3: shutdown LeaderState
{noformat}

[~hanishakoneru] also thinks we should increase ratis leader election timeout too.

{noformat}
  <property>
    <name>ozone.om.ratis.minimum.timeout</name>
    <value>1s</value>
    <tag>OZONE, OM, RATIS, MANAGEMENT</tag>
    <description>The minimum timeout duration for OM's Ratis server rpc.
    </description>
  </property>

  <property>
    <name>ozone.om.leader.election.minimum.timeout.duration</name>
    <value>1s</value>
    <tag>OZONE, OM, RATIS, MANAGEMENT</tag>
    <description>The minimum timeout duration for OM ratis leader election.
      Default is 1s.
    </description
{noformat}",pull-request-available,['OM HA'],HDDS,Improvement,Critical,2020-11-03 21:16:16,7
13338676,Create unit test for SimpleContainerDownloader,"[~weichiu] reported that in a specific case the Datanode tried to download / replicate containers multiple times from the same datanode.

 

SimpleContainerDownload has a logic to try out all the available datanodes: this Jira creates a unit test t make sure the logic works well.",pull-request-available,[],HDDS,Sub-task,Major,2020-11-03 15:52:01,1
13338597,Update README with information how to report security issues,"security@ozone.apache.org is created, we can add in to the README.md and we can create the SECURITY.md file (Github naming convention)",pull-request-available,[],HDDS,Improvement,Major,2020-11-03 09:24:10,1
13338467,Misleading SCM web UI Safe mode status ," !Screen Shot 2020-11-01 at 11.46.40 PM.png! 

As an example, the above screenshot is for a SCM who was no container reported. However, the status message ""currentContainerThreshold 0.0 >= safeModeCutoff 0.99"" which is counterintuitive. We should make it easier to tell why SCM is still in safe mode.",pull-request-available,['SCM'],HDDS,Bug,Minor,2020-11-02 17:20:50,1
13338449,TestContainerMetrics is flaky,"TestContainerMetrics is flaky since HDDS-4359. Failed in following master builds:

{code}
2020/10/26/3569/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/27/3581/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/28/3591/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/29/3619/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/30/3628/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/30/3642/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/31/3650/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/31/3654/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
{code}

Some of the added assertions couldn't be guaranteed all the time:

{code}
      // ReadTime and WriteTime vary from run to run, only checking non-zero
      Assert.assertNotEquals(0L, getLongCounter(""ReadTime"", volumeIOMetrics));
      Assert.assertNotEquals(0L, getLongCounter(""WriteTime"", volumeIOMetrics));
{code}

In very lucky case the read/write time can be zero.",pull-request-available,['test'],HDDS,Bug,Major,2020-11-02 15:37:57,1
13338443,Simplify Ozone client code with configuration object,"In HDDS-4185 we agreed to introduce a new configuration for the Ozone client to adjust the behavior of incremental byte buffer.

When I started to add it I realized the code is already very complex as all the required configuration values are propagated manually to the Key/Block/ChunkOutputstream as induvidual constructor parameters.

In this patch I simplify the structure with using only one POJO with configuration annotations.",pull-request-available,[],HDDS,Improvement,Major,2020-11-02 15:02:41,1
13338373,Update README.md after TLP separation,"README.md can be updated with the new mailing lists and references to ""Hadoop subproject"" can be removed.",pull-request-available,[],HDDS,Improvement,Major,2020-11-02 09:16:31,1
13337773,Proxy failover is logging with out trying all OMS,"{code:java}
[root@uma-1 ~]# sudo -u hdfs hdfs dfs -ls o3fs://bucket.volume.ozone1/
20/10/28 23:37:50 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:73)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)
, while invoking $Proxy10.submitRequest over {om1=nodeId=om1,nodeAddress=uma-1.uma.root.hwx.site:9862, om3=nodeId=om3,nodeAddress=uma-3.uma.root.hwx.site:9862, om2=nodeId=om2,nodeAddress=uma-2.uma.root.hwx.site:9862} after 1 failover attempts. Trying to failover immediately.{code}

This issue in the Apache Ozone main branch will be fixed once Hadoop version is updated. For vendors/users who backport fix to their Hadoop version and have ozone compiled with that version, this fix will help them not to see first failovers till it finds leader OM.",pull-request-available,[],HDDS,Bug,Major,2020-10-29 00:48:29,2
13337670,Fix compilation issue in HDDS-3698-upgrade branch.,"{code}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-ozone-ozone-manager: Compilation failure: Compilation failure: 
Error:  /mnt/ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java:[691,6] not a statement
Error:  /mnt/ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java:[691,23] ';' expected
Error:  -> [Help 1]
{code}",pull-request-available,[],HDDS,Sub-task,Major,2020-10-28 13:35:54,5
13337648,Make raft log directory deletion configurable during pipeline remove,The idea here is to add a config to make raft log directory removal configurable during pipeline remove.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-10-28 11:27:50,3
13337644,Safe mode rule for piplelines should only consider open pipelines,"Currently, for safe mode we consider all pipelines existing in DB for safe mode exit criteria. It ma happen that, SCM has the pipelines craeted , but none of the participants datanodes ever created these datanodes. In such cases, SCM fails to come out of safemode as these pipelines are never reported back to SCM.

 

The idea here is to consider pipelines which are marked open during SCM startup.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-10-28 11:18:18,3
13337246,Ozone Data Generator for Fast Scale Test,"I've been working on this fun project and would like to share with the community.

 
h1. Synopsis

We want to prove Ozone runs well at scale, in terms of number of keys (billions of keys), as well as dense DataNodes where each DN has hundreds of TB or even PB-scale capacity.
h1. Challenge: Data generation

The challenge is to generate a huge data set fast so that we can benchmark the system quickly. No existing tool is capable at this scale. 

 
h1. Proposal:

The major bottleneck is OM’s key insertion performance. In addition, Ozone uses a single pipeline to write data, unless multi-raft is enabled.

 

Instead of using Ozone's client API to generate data, We should write directly to OM, SCM and DN’s rocksdb. RocksDB can support u[p to a million key|https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks] bulk load operations.

 

Similarly, we can skip the normal Ozone client write path; populate the container db and block files directly.

 

(more details in the design doc)",pull-request-available,['Tools'],HDDS,New Feature,Major,2020-10-26 16:37:33,1
13336987,Add Recon architecture to docs,Recon Architecture and details are missing in the docs - https://ci-hadoop.apache.org/view/Hadoop%20Ozone/job/ozone-doc-master/lastSuccessfulBuild/artifact/hadoop-hdds/docs/public/concept/ozonemanager.html,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-10-23 20:51:22,6
13336869,Make writeStateMachineTimeout retry count proportional to node failure timeout,"Currently, in ratis ""writeStateMachinecall"" gets retried indefinitely in event of a timeout. In case, where disks are slow/overloaded or number of chunk writer threads are not available for a period of 10s, writeStateMachine call times out in 10s. In cases like these, the same write chunk keeps on getting retried causing the same chunk of data to be overwritten. The idea here is to abort the request once the node failure timeout reaches.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-10-23 07:31:45,3
13334734,Bootstrap new OM node,"In a ratis enabled OM cluster, add support to bootstrap a new OM node and add it to OM ratis ring. ",pull-request-available,[],HDDS,New Feature,Major,2020-10-09 23:02:47,7
13334702,Expose Ratis retry config cache in OM,"This Jira is to expose config Ratis retry cache duration in OM, and also choose a sensible default value.",pull-request-available,[],HDDS,Sub-task,Major,2020-10-09 18:52:34,2
13334482,Potential resource leakage using BatchOperation,"there are a number of places in the code where BatchOperation is used but not closed. As a best practice, better to close them explicitly.

I have a stress test code that uses BatchOperation to insert into OM rocksdb. Without closing BatchOperation explicitly, the process crashes after just a few minutes.",pull-request-available,[],HDDS,Bug,Blocker,2020-10-08 18:43:00,2
13334327,Incompatible return codes from Ozone getconf -confKey,"It seems that the return codes of ozone getconf -confKey are different in 1.0 and after 1.0.

Looking at the code:

in old code:

/** Method to be overridden by sub classes for specific behavior. */
int doWorkInternal(OzoneGetConf tool, String[] args) throws Exception {


{code:java}
 String value = tool.getConf().getTrimmed(key);
 if (value != null) {
 tool.printOut(value);
 return 0;
 }
 tool.printError(""Configuration "" + key + "" is missing."");
 return -1;
}
{code}

with 1.0 code:
@Override
  public Void call() throws Exception {
    String value = tool.getConf().getTrimmed(confKey);
    if (value != null) {
      tool.printOut(value);
    } else {
      tool.printError(""Configuration "" + confKey + "" is missing."");
    }
    return null;
  }

We are returning null irrespective of the cases.
Some applications/tests depending on this codes.

Thanks [~nmaheshwari] for helping on debug and finding the issue.
 ",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-10-08 01:36:36,0
13334230,Compile error with Java 11,"{code:title=https://github.com/apache/hadoop-ozone/runs/1217093596#step:6:5632}
Error:  Failed to execute goal org.codehaus.mojo:aspectj-maven-plugin:1.10:compile (default) on project hadoop-ozone-ozone-manager: Execution default of goal org.codehaus.mojo:aspectj-maven-plugin:1.10:compile failed: Plugin org.codehaus.mojo:aspectj-maven-plugin:1.10 or one of its dependencies could not be resolved: Could not find artifact com.sun:tools:jar:11.0.8 at specified path /opt/hostedtoolcache/jdk/11.0.8/x64/../lib/tools.jar -> [Help 1]
{code}

https://github.com/mojohaus/aspectj-maven-plugin/issues/24#issuecomment-419077658",pull-request-available,['build'],HDDS,Bug,Major,2020-10-07 15:55:28,0
13334093,Upgrade to angular 1.8.0 due to CVE-2020-7676,"Angular versions < 1.8.0 are vulnerable to cross-site scripting

[https://nvd.nist.gov/vuln/detail/CVE-2020-7676]",pull-request-available,[],HDDS,Task,Major,2020-10-07 00:00:05,6
13334083,Ensure ObjectIDs are unique across restarts,"In a non-Ratis OM, the transaction index used to generate ObjectID is reset on OM restart. This can lead to duplicate ObjectIDs when the OM is restarted. ObjectIDs should be unique. 
HDDS-2939 and NFS are some of the features which depend on ObjectIds being unique.

To ensure that objectIDs are unique across restarts in non-ratis OM cluster, the transaction index should be updated in DB on every flush to DB. This can be done in a similar fashion to what is being done for ratis enabled cluster today. TransactionInfo table is updated with transaction index as part of every batch write operation to DB.

Also, and epoch number is introduced to ensure that objectIDs do not clash with older clusters in which this fix does not exist. From the 64 bits of ObjectID (long variable), 2 bits are reserved for epoch and 8 bits for recursive directory creation, if required. The most significant 2 bits of objectIDs is set to epoch. For clusters before HDDS-4315 there is no epoch as such. But it can be safely assumed that the most significant 2 bits of the objectID will be 00 (as it unlikely to reach trxn index > 2^62 in an existing cluster). From HDDS-4315 onwards, the Epoch for non-ratis OM clusters will be binary 01 (= decimal 1) and for ratis enabled OM cluster will be binary 10 (= decimal 2).

 ",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-10-06 22:35:48,7
13334077,OM Layout Version Manager init throws silent CNF error in integration tests.,"{code}
org.reflections.ReflectionsException: could not get type for name mockit.MockUp
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.reflections.Reflections.<init>(Reflections.java:182)
	at org.reflections.Reflections.<init>(Reflections.java:155)
	at org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.registerOzoneManagerRequests(OMLayoutVersionManagerImpl.java:122)
	at org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.init(OMLayoutVersionManagerImpl.java:100)
	at org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.initialize(OMLayoutVersionManagerImpl.java:83)
	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:363)
	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:930)
	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.createOMService(MiniOzoneHAClusterImpl.java:379)
	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.build(MiniOzoneHAClusterImpl.java:294)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.lang.ClassNotFoundException: mockit.MockUp
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 23 more
{code}",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-10-06 22:04:18,5
13333937,findbugs check succeeds despite compile error,"Findbugs check has been silently failing but reporting success for some time now.  The problem is that {{findbugs.sh}} determines exit code based on the number of findbugs failures.  If {{compile}} step fails, exit code is 0, ie. success.

{code:title=https://github.com/apache/hadoop-ozone/runs/1210535433#step:3:866}
2020-10-02T18:37:57.0699502Z [ERROR] Failed to execute goal on project hadoop-hdds-client: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-client:jar:1.1.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-common:jar:tests:1.1.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
{code}",pull-request-available,['build'],HDDS,Bug,Major,2020-10-06 13:53:57,0
13333931,Type-safe config design doc points to OM HA,"Abstract and links for http://hadoop.apache.org/ozone/docs/1.0.0/design/typesafeconfig.html are wrong, reference OM HA design doc.",pull-request-available,['documentation'],HDDS,Bug,Minor,2020-10-06 13:27:21,0
13330930,Ozone checkstyle rule can't be imported to IntelliJ,"CheckStyle: Move LineLength Check parent from TreeWalker to Checker, otherwise fail to import to latest IntelliJ

Similar issue has been reported here and I've verified the fix locally that the IntelliJ can import checkstyle rule after the fix. 
https://github.com/checkstyle/checkstyle/issues/2116
",pull-request-available,[],HDDS,Improvement,Major,2020-10-04 02:44:05,4
13330452,SCM CA certificate does not encode KeyUsage extension properly,"This could be problematic with strict security provider such as FIPS. The default non-FIPS provider such as SunJCE and BC provider work fine though. This ticket is opened to fix it. 


{code:java}
2020-09-30 12:01:52,962 ERROR org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer: Unable to initialize CertificateServer.
org.apache.hadoop.hdds.security.exception.SCMSecurityException: java.security.cert.CertificateParsingException: cannot construct KeyUsage: java.lang.IllegalArgumentException: illegal object in getInstance: com.safelogic.cryptocomply.asn1.DEROctetString
        at org.apache.hadoop.hdds.security.x509.certificate.utils.CertificateCodec.getPEMEncodedString(CertificateCodec.java:105)
        at org.apache.hadoop.hdds.security.x509.certificate.utils.CertificateCodec.writeCertificate(CertificateCodec.java:182)
        at org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer.generateRootCertificate(DefaultCAServer.java:495)
        at org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer.generateSelfSignedCA(DefaultCAServer.java:303)
  
{code}

",pull-request-available,['Security'],HDDS,Improvement,Major,2020-10-01 21:56:49,4
13330158,Use an interface in Ozone client instead of XceiverClientManager,"XceiverClientManager is used everywhere in the ozone client (Key/Block Input/OutputStream) to get a client when required.

To make it easier to create genesis/real unit tests, it would be better to use a generic interface instead of XceiverClientManager which can make it easy to replace the manager with a mock implementation.",pull-request-available,[],HDDS,Improvement,Major,2020-09-30 11:59:46,1
13330062,Ozone Client not working with Hadoop Version  < 3.2,"HDDS-3560 created new ProxyInfo object in case of IllegalAccessError exception. But, it does not return the new instance and causes NPE in Hadoop versions < 3.2


{code:java}
20/09/29 23:10:22 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:20/09/29 23:10:22 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:java.lang.NullPointerException at org.apache.hadoop.io.retry.RetryInvocationHandler.isRpcInvocation(RetryInvocationHandler.java:435) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:354) at com.sun.proxy.$Proxy10.submitRequest(Unknown Source) at org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransport.submitRequest(Hadoop3OmTransport.java:89) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:213) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1030) at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:175) at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:242) at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:113) at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:149) at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:51) at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:94) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:161) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3288) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3337) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3305) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361) at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:352) at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:250) at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233) at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103) at org.apache.hadoop.fs.shell.Command.run(Command.java:177) at org.apache.hadoop.fs.FsShell.run(FsShell.java:326) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) at org.apache.hadoop.fs.FsShell.main(FsShell.java:389)ls: Couldn't create RpcClient protocol
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-09-29 23:58:50,2
13329886,the icon of hadoop-ozone is bigger than ever,It could be a by-product of the introduction of the issue： https://issues.apache.org/jira/browse/HDDS-4166,pull-request-available,['documentation'],HDDS,Bug,Trivial,2020-09-29 08:26:14,1
13329759,Read is slow due to frequent calls to UGI.getCurrentUser() and getTokens(),"Ozone read operation turned out to be slow mainly because we do a new UGI.getCurrentUser for block token for each of the calls.

We need to cache the block token / UGI.getCurrentUserCall() to make it faster.

 !image-2020-09-28-16-19-17-581.png! 

To reproduce:

Checkout: https://github.com/elek/hadoop-ozone/tree/mocked-read

{code}
cd hadoop-ozone/client

export MAVEN_OPTS=-agentpath:/home/elek/prog/async-profiler/build/libasyncProfiler.so=start,file=/tmp/profile-%t-%p.svg

mvn compile exec:java -Dexec.mainClass=org.apache.hadoop.ozone.client.io.TestKeyOutputStreamUnit -Dexec.classpathScope=test
{code}",pull-request-available,[],HDDS,Improvement,Major,2020-09-28 14:19:20,0
13328985,Avoid logging chunk content in Ozone Insight,"HDDS-2660 added an insight point for the datanode dispatcher.  At trace level it logs all chunk content, which can be huge and contain control characters, so I think we should avoid it.",pull-request-available,['Tools'],HDDS,Improvement,Major,2020-09-23 12:13:43,0
13328981,Add more reusable byteman scripts to debug ofs/o3fs performance,"I am using https://byteman.jboss.org to debug the performance of spark + terage with different scripts. Some byteman scripts are already shared by HDDS-4095 or HDDS-342 but it seems to be a good practice to share the newer scripts to make it possible to reproduce performance problems.

For using byteman with Ozone, see this video:
https://www.youtube.com/watch?v=_4eYsH8F50E&list=PLCaV-jpCBO8U_WqyySszmbmnL-dhlzF6o&index=5",pull-request-available,[],HDDS,Improvement,Major,2020-09-23 12:01:11,1
13328819,Prepare for Upgrade step should purge the log after waiting for the last txn to be applied.,"This is a follow up task from HDDS-4227 in which the prepare upgrade/downgrade task should purge the Raft log immediately after waiting for the last txn to be applied. This is to make sure that we dont ""apply"" transactions in different versions of the code across the quorum. A lagging follower will use a Ratis snapshot to bootstrap itself on restart.",pull-request-available,[],HDDS,Sub-task,Major,2020-09-22 16:23:52,5
13328324,Use ClientID and CallID from Rpc Client to detect retry requests,"Use clientID and callID to uniquely identify the requests.
This will help in case when the request is retried for write requests, when the previous one is already processed, the previous result can be returned from the cache.",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2020-09-18 23:14:23,2
13327935,Update Ratis version to latest snapshot,This Jira aims to update ozone with latest Ratis snapshot which has a critical fix for OM HA - RATIS-1025.,pull-request-available,[],HDDS,Bug,Major,2020-09-16 23:35:17,7
13327540,Get API not working from S3A filesystem with Ozone S3,"TroubleShooting S3A mentions S3 compatible servers that donot support Etags will see this server

Refer [link|https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html] and look for below section content.
Using a third-party S3 implementation that doesn’t support eTags might result in the following error.

org.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://my-bucket/test/file.txt':
 Change detection policy requires ETag
  at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:153)
  at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:200)
  at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:346)
  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$2(Invoker.java:195)
  at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
  at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:193)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:215)
  at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:339)
  at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:372)


{code:java}
org.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processNewRevision(ChangeTracker.java:275)
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processMetadata(ChangeTracker.java:261)
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:195)
	at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:208)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:359)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$3(Invoker.java:223)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$5(Invoker.java:347)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:343)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:221)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:265)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:351)
	at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:464)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129)
	at org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:494)
	at org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:416)
	at org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:351)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:286)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:271)
	at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
	at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:266)
	at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)
	at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:237)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
get: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag
{code}
",OzoneS3 S3A,['S3'],HDDS,Bug,Major,2020-09-14 23:27:05,2
13327474,Support HADOOP_TOKEN_FILE_LOCATION for Ozone token CLI,"Currently, Ozone token CLI produce token in base64 encode format. This is not compatible with HADOOP_TOKEN_FILE_LOCATION and can't be used directly for Ozone/Hadoop CLI to authenticate. This ticket is opened to persist ozone token in a format that is compatible with HADOOP_TOKEN_FILE_LOCATION along with tests. ",pull-request-available,['Security'],HDDS,Improvement,Major,2020-09-14 15:46:44,4
13326735,"Implement a ""prepareForUpgrade"" step that applies all committed transactions onto the OM state machine.","*Why is this needed?*
Through HDDS-4143, we have a generic factory to handle multiple versions of apply transaction implementations based on layout version. Hence, this factory can be used to handle versioned requests across layout versions, whenever both the versions need to exist in the code (Let's say for HDDS-2939). 

However, it has been noticed that the OM ratis requests are still undergoing lot of minor changes (HDDS-4007, HDDS-4007, HDDS-3903), and in these cases it will become hard to maintain 2 versions of the code just to support clean upgrades. 

Hence, the plan is to build a pre-upgrade utility (client API) that makes sure that an OM instance has no ""un-applied"" transactions in this Raft log. Invoking this client API makes sure that the upgrade starts with a clean state. Of course, this would be needed only in a HA setup. In a non HA setup, this can either be skipped, or when invoked will be a No-Op (Non Ratis) or cause no harm (Single node Ratis).

*How does it work?*
Before updating the software bits, our goal is to get OMs to get to the  latest state with respect to apply transaction. The reason we want this is to make sure that the same version of the code executes the AT step in all the 3 OMs. In a high level, the flow will be as follows.

* Before upgrade, *stop* the OMs.
* Start OMs with a special flag --prepareUpgrade (This is something like --init,  which is a special state which stops the ephemeral OM instance after doing some work)
* When OM is started with the --prepareUpgrade flag, it does not start the RPC server, so no new requests can get in.
* In this state, we give every OM time to apply txn until the last txn.
* We know that at least 2 OMs would have gotten the last client request transaction committed into their log. Hence, those 2 OMs are expected to apply transaction to that index faster.
* At every OM, the Raft log will be purged after this wait period (so that the replay does not happen), and a Ratis snapshot taken at last txn.
* Even if there is a lagger OM which is unable to get to last applied txn index, its logs will be purged after the wait time expires.
* Now when OMs are started with newer version, all the OMs will start using the new code.
* The lagger OM will get the new Ratis snapshot since there are no logs to replay from.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-09-09 20:38:38,5
13326512,Revisit 'static' nature of OM Layout Version Manager.,"Investigate whether we can programmatically instantiate the OM Aspect so that we can move away from static nature of OM Layout Version Manager. Moving away from static behavior will help out with easy unit testing. 

cc [~pifta]",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-09-08 15:16:52,5
13326110,ResolveBucket during checkAcls fails,"In HA, in validateAndUpdateCache when resolveBucket, it checks the permission using checkAcls. But it will have not any RpcContext and it will fail with NPE in checkAcls when getting hostName.

For this same reason, we added the required information to check ACLs into OMRequest.


{code:java}
java.lang.NullPointerException
	at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1604)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3497)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3465)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3452)
	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.resolveBucketLink(OMKeyRequest.java:96)
	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:215)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:246)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-09-04 19:30:45,2
13326018,S3A Filesystem does not work with Ozone S3 in file system compat mode,"When *ozone.om.enable.filesystem.paths* is enabled

 

hdfs dfs -mkdir -p s3a://b12345/d11/d12 -> Success

hdfs dfs -put /tmp/file1 s3a://b12345/d11/d12/file1 -> fails with below error

 
{code:java}
2020-09-04 03:53:51,377 ERROR org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:b1234, Keyd11/d12/file1._COPYING_. Exception:{}
NOT_A_FILE org.apache.hadoop.ozone.om.exceptions.OMException: Can not create file: cp/k1._COPYING_ as there is already file in the given path
 at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:256)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:246)
 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748){code}
*Reason for this*
 S3A filesystem when create directory creates an empty file

*Now entries in Ozone KeyTable after create directory*
 d11/
 d11/d12

Because of this in OMFileRequest.VerifyInFilesPath fails with FILE_EXISTS_IN_GIVEN_PATH because d11/d12 is considered as file not a directory. (As in ozone currently, directories end with trailing ""/"")

So, when d11/d12/file is created, we check parent exists, now d11/d12 is considered as file and fails with NOT_A_FILE

When disabled it works fine, as when disabled during key create we do not check any filesystem semantics and also does not create intermediate directories.
{code:java}
[root@bvoz-1 ~]# hdfs dfs -mkdir -p s3a://b12345/d11/d12
[root@bvoz-1 ~]# hdfs dfs -put /etc/hadoop/conf/ozone-site.xml s3a://b12345/d11/d12/k1
[root@bvoz-1 ~]# hdfs dfs -ls s3a://b12345/d11/d12
Found 1 items
-rw-rw-rw-   1 systest systest       2373 2020-09-04 04:45 s3a://b12345/d11/d12/k1
{code}
 ",OzoneS3 S3A pull-request-available,[],HDDS,Sub-task,Blocker,2020-09-04 04:55:41,2
13325986,Fix table rendering and logo display in docs,"The Docs page does not render the table correctly in OM and SCM architecture pages.

Also, the ozone logo in the header is not visible.",pull-request-available,['documentation'],HDDS,Bug,Major,2020-09-03 21:18:14,6
13325938,upgrade docker environment does not work with KEEP_RUNNING=true,"Ozone {{upgrade}} Docker Compose environment fails if run with {{KEEP_RUNNING=true}}.  The variable is applied to both runs (pre- and post-upgrade), but pre-upgrade containers should be stopped anyway, since they will be replaced by the new ones.

{code}
$ cd hadoop-ozone/dist/target/ozone-1.1.0-SNAPSHOT/compose/upgrade
$ KEEP_RUNNING=true ./test.sh
...
Failed: IO error: While lock file: scm.db/LOCK: Resource temporarily unavailable
{code}",pull-request-available,"['docker', 'test']",HDDS,Bug,Minor,2020-09-03 14:22:07,0
13325887,Publish docker image for ozone 1.0.0,Docker image is based on the voted and approved artifacts which is availabe. We can create the image.,pull-request-available,[],HDDS,Improvement,Major,2020-09-03 09:40:07,1
13325812,Compile Ozone with multiple Java versions,Add matrix build in GitHub Actions for compiling Ozone with both Java 8 and 11.,pull-request-available,['build'],HDDS,Improvement,Major,2020-09-02 21:25:20,0
13325809,Failed to load existing service definition files: ...SubcommandWithParent,"{code}
[INFO] Apache Hadoop HDDS Tools ........................... FAILURE
...
[ERROR] Failed to load existing service definition files: java.nio.file.NoSuchFileException: hadoop-hdds/tools/target/classes/META-INF/services/org.apache.hadoop.hdds.cli.SubcommandWithParent
{code}",jdk11,['build'],HDDS,Bug,Major,2020-09-02 20:27:32,0
13325801,Add an endpoint in Recon to query Prometheus,Recon should have an endpoint to proxy requests to the configured prometheus instance.,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-09-02 19:06:52,6
13325776,Create a script to check AWS S3 compatibility,"Ozone S3G implements the REST interface of AWS S3 protocol. Our robot test based scripts check if it's possible to use Ozone S3 with the AWS client tool.

But occasionally we should check if our robot test definitions are valid: robot tests should be executed with using real AWS endpoint and bucket(s) and all the test cases should be passed.

This patch provides a simple shell script to make this cross-check easier.  ",pull-request-available,[],HDDS,Improvement,Major,2020-09-02 16:43:46,1
13325774,Range used by S3 MultipartUpload copy-from-source should be inclusive,"S3 API provides a feature to copy a specific range from an existing key.

Based on the documentation, this range definitions is inclusive:

https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part-copy.html

{quote}
-copy-source-range (string)

    The range of bytes to copy from the source object. The range value must use the form bytes=first-last, where the first and last are the zero-based byte offsets to copy. For example, bytes=0-9 indicates that you want to copy the first 10 bytes of the source. You can copy a range only if the source object is greater than 5 MB.
{quote}

But as it's visible from our [robot test|http://example.com], in our case we use exclusive range:

{code}
upload-part-copy ... --copy-source-range bytes=0-10485758
upload-part-copy ... --copy-source-range bytes=10485758-10485760
{code}

Based on this AWS documentation it will return with a (10485758 + 1) + 3 bytes long key,  which is impossible if our original source key is just 10485760.

I think the right usage to get the original key is the following:

{code}
upload-part-copy ... --copy-source-range bytes=0-10485757
upload-part-copy ... --copy-source-range bytes=10485758-10485759
{code}

(Note, this bug is found with the script in HDDS-4194, which showed that AWS S3 is working in different way).",pull-request-available,[],HDDS,Bug,Blocker,2020-09-02 16:40:40,1
13325479,Disable IncrementalByteBuffer by default in Ozone Client,"During the teragen test it was identified that the IncrementalByteBuffer is one of the biggest bottlenecks. 

In the PR of HDDS-4119 a long conversation has been started if it can be removed or we need other solution to optimize.

This jira is opened to continue the discussion and either remove or optimize the IncrementalByteByffer.",pull-request-available,[],HDDS,Improvement,Major,2020-09-01 08:36:00,1
13325118,Acceptance test logs missing if SCM fails to exit safe mode,"Acceptance test sometimes fails due to SCM not coming out of safe mode.  If this happens, the cluster is stopped without running Robot tests.  {{rebot}} command to process test results fails due to missing input, and acceptance check is abruptly stopped without fetching docker logs or running tests in other environments.",pull-request-available,['test'],HDDS,Bug,Major,2020-08-29 06:42:19,0
13325070,GitHub Actions cache does not work outside of workspace,"Ozone source is checked out for _acceptance_ and _kubernetes_ checks to {{/mnt}}, outside of {{GITHUB_WORKSPACE}}, and only after _Cache_ steps.  Therefore no files are found for which hash would be computed to be included in cache keys.

{code:title=https://github.com/apache/hadoop-ozone/blob/44acf78aec6c3a4e1c5fea3a43971144c6da9a4c/.github/workflows/post-commit.yml#L167-L171}
      - name: Cache for maven dependencies
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: maven-repo-${{ hashFiles('**/pom.xml') }}
{code}

Cache key is always {{maven-repo-}}:

{code:title=https://github.com/apache/hadoop-ozone/runs/1042358389#step:2:10}
Cache restored from key: maven-repo-
{code}

The same old cache is used for all builds, even if dependencies are changed in {{pom.xml}}, gradually resulting in more and more downloads during builds:

{code:title=https://github.com/apache/hadoop-ozone/runs/1036271227#step:9:680}
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/info/picocli/picocli/4.4.0/picocli-4.4.0.jar (389 kB at 2.4 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ratis/ratis-server/1.0.0/ratis-server-1.0.0.jar (380 kB at 2.4 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-api/2.13.3/log4j-api-2.13.3.jar (292 kB at 1.9 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ratis/ratis-proto/1.0.0/ratis-proto-1.0.0.jar (1.2 MB at 6.6 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-core/2.13.3/log4j-core-2.13.3.jar (1.7 MB at 8.5 MB/s)
{code}",pull-request-available,['build'],HDDS,Bug,Minor,2020-08-28 19:05:30,0
13324720,Directory and filename can end up with same name in a path,"Scenario:

Create Key via S3, and Create Directory through Fs.
 # open key -> /a/b/c
 # CreateDirectory -> /a/b/c
 # CommitKey -> /a/b/c

So, now in Ozone we will have directory and file with name ""c""

When created through Fs interface.
 # create file -> /a/b/c
 # CreateDirectory -> /a/b/c
 # CommitKey -> /a/b/c

So, now in Ozone we will have directory and file with name ""c""

 
 # InitiateMPU /a/b/c
 # Create Part1 /a/b/c
 # Commit Part1 /a/b/c
 # Create Directory /a/b/c
 # Complete MPU /a/b/c

So, now in Ozone, we will have directory and file with name ""c"".  In MPU this is one example scenario.

 

Few proposals/ideas to solve this:
 # Check during commit whether a directory already exists with same name. But disadvantage is after user uploads the entire data during last stage we fail.  (File system with create in progress acts similarly. Scenario: 1. vi t1 2. mkdir t1 3. Save t1: (Fail:""t1"" is a directory)
 # During create directory check are there any open key creation with same name and fail.

 

Any of the above approaches are not final, this Jira is opened to discuss this issue and come up with solution.",pull-request-available,[],HDDS,Bug,Major,2020-08-26 21:50:14,2
13324671,Increase default timeout in kubernetes tests,"Kubernetes tests are timing out sometimes. (eg. here: https://github.com/elek/ozone-build-results/tree/master/2020/08/26/2562/kubernetes)

Based on the log, SCM couldn't move out from safe mode. It's either a real issue or github environment is slow sometimes.

To make it clear what is the problem I propose to increase the default timeout from 90 sec to 300 sec (5 min).",pull-request-available,[],HDDS,Improvement,Major,2020-08-26 15:49:06,1
13324646,Archive container logs for kubernetes check,"_kubernetes_ check archives only Robot results.  It should also include logs from all pods, similar to compose-based acceptance tests.",pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-08-26 12:23:55,0
13324644,recon.api.TestEndpoints is flaky,"
Failed on the PR:
https://github.com/apache/hadoop-ozone/pull/1349

And on the master:

https://github.com/elek/ozone-build-results/blob/master/2020/08/25/2533/unit/hadoop-ozone/recon/org.apache.hadoop.ozone.recon.api.TestEndpoints.txt

and here:

https://github.com/elek/ozone-build-results/blob/master/2020/08/22/2499/unit/hadoop-ozone/recon/org.apache.hadoop.ozone.recon.api.TestEndpoints.txt",pull-request-available,[],HDDS,Bug,Blocker,2020-08-26 11:55:03,6
13324618,Implement OzoneFileStatus#toString,{{OzoneFileStatus}} should implement {{toString}} for debug purposes.,pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Minor,2020-08-26 09:33:32,0
13324467,Bump version to 1.1.0-SNAPSHOT on master,s/0.6.0-SNAPSHOT/1.1.0-SNAPSHOT/g,pull-request-available,[],HDDS,Improvement,Major,2020-08-25 12:43:33,1
13324327,Implement a factory for OM Requests that returns an instance based on layout version.,"* Add the current layout version (MLV) to the OM Ratis request. If there is no layout version   present, we can default to '0'.
* Implement Generic factory which stores different instances of Type 'T' sharded by a key & version. A single key can be associated with different versions of 'T'. This is to support a typical use case during upgrade to have multiple versions of a class / method / object and chose them based on current layout version at runtime. Before finalizing, an older version is typically needed, and after finalize, a newer version is needed.
* Using the generic factory, we scan all the different OM ""write"" requests and associate them with versions.
* Layout feature code refactoring. Added more comments and tests.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-08-24 18:44:22,5
13324306,Auto-close /pending pull requests after 21 days of inactivity,"Earlier we introduced a way to mark the inactive pull requests with ""pending"" label (with the help of /pending comment).

This pull requests introduce a new scheduled build which closes the ""pending"" pull requests after 21 days of inactivity.

IMPORTANT: Only the pull requests  which are pending on the author will be closed.

We should NEVER close a pull requests which are waiting for the attention of a committer.",pull-request-available,['build'],HDDS,Improvement,Major,2020-08-24 16:26:31,1
13324290,Update version number in upgrade tests,"Ozone 0.6.0 release is renamed to Ozone 1.0.0, but there are a few leftover references to 0.6.0, mostly in {{upgrade}} acceptance test.",pull-request-available,['test'],HDDS,Task,Minor,2020-08-24 14:53:26,0
13323056,Components with web interface should depend on hdds-docs,"[~sammichen] reported the problem that the ozone-0.6.0 branch couldn't be compiled after changing the version to 1.0.0.

{code}
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.0.2:unpack (copy-common-html) on project hadoop-hdds-container-service: Unable to find/resolve artifact.: Could not find artifact org.apache.hadoop:hadoop-hdds-docs:jar:1.0.0 in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
[ERROR]
{code}

Web components include the compiled docs (in case of hugo is on the path)

{code}
  <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-dependency-plugin</artifactId>
        <executions>
          <execution>
            <id>copy-common-html</id>
            <phase>prepare-package</phase>
            <goals>
              <goal>unpack</goal>
            </goals>
            <configuration>
              <artifactItems>
                <artifactItem>
                  <groupId>org.apache.hadoop</groupId>
                  <artifactId>hadoop-hdds-server-framework</artifactId>
                  <outputDirectory>${project.build.outputDirectory}
                  </outputDirectory>
                  <includes>webapps/static/**/*.*</includes>
                </artifactItem>
                <artifactItem>
                  <groupId>org.apache.hadoop</groupId>
                  <artifactId>hadoop-hdds-docs</artifactId>
                  <outputDirectory>${project.build.outputDirectory}/webapps/ozoneManager</outputDirectory>
                  <includes>docs/**/*.*</includes>
                </artifactItem>
              </artifactItems>
              <overWriteSnapshots>true</overWriteSnapshots>
            </configuration>
          </execution>
        </executions>
      </plugin>
{code}

But the explicit dependency between the container-service and hdds-docs accidentally missing. With adding a provided dependency, it can be fixed (maven will compile hdds-docs first)",pull-request-available,[],HDDS,Bug,Blocker,2020-08-17 13:53:48,1
13322745,Improve performance of the BufferPool management of Ozone client,"Teragen reported to be slow with low number of mappers compared to HDFS.

In my test (one pipeline, 3 yarn nodes) 10 g teragen with HDFS was ~3 mins but with Ozone it was 6 mins. It could be fixed with using more mappers, but when I investigated the execution I found a few problems reagrding to the BufferPool management.

 1. IncrementalChunkBuffer is slow and it might not be required as BufferPool itself is incremental
 2. For each write operation the bufferPool.allocateBufferIfNeeded is called which can be a slow operation (positions should be calculated).
 3. There is no explicit support for write(byte) operations

In the flamegraph it's clearly visible that with low number of mappers the client is busy with buffer operations. After the patch the rpc call and the checksum calculation give the majority of the time. ",pull-request-available,[],HDDS,Improvement,Blocker,2020-08-14 12:09:55,1
13322723,Allow running Kubernetes example tests on k3d,"Kubernetes example tests currently only run on local cluster.  With a minor change we can make the tests work on k3d, too.

bq. [k3d|https://k3d.io/] makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes.",pull-request-available,['test'],HDDS,Improvement,Major,2020-08-14 09:39:40,0
13322617, Normalize Keypath for listKeys.,"When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.

When listKeys uses given keyName(not normalized key path) as prefix and Starkey the list-keys will return empty result.

Similar to HDDS-4102, we should normalize startKey and keyPrefix.


",pull-request-available,[],HDDS,Sub-task,Major,2020-08-13 21:58:47,2
13322494,Bump log4j2 version,"There are bunch of bugfixes and improvements since the used 2.10:

https://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3",pull-request-available,[],HDDS,Improvement,Major,2020-08-13 08:47:18,1
13322090,Normalize Keypath for lookupKey,"When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.

Now when user tries to read the file from S3 using the keyName which user has used to create the Key, it will return error KEY_NOT_FOUND

The issue is, lookupKey need to normalize path, when ozone.om.enable.filesystem.paths is enabled. This is common API used by S3/FS. 
",pull-request-available,[],HDDS,Sub-task,Major,2020-08-11 18:02:07,2
13321868,S3/Ozone Filesystem inter-op,"This Jira is to implement changes required to use Ozone buckets when data is ingested via S3 and use the bucket/volume via OzoneFileSystem. Initial implementation for this is done as part of HDDS-3955. There are few API's which have missed the changes during the implementation of HDDS-3955. 

Attached design document which discusses each API,  and what changes are required.

Excel sheet has information about each API, from what all interfaces the OM API is used, and what changes are required for the API to support inter-operability.

Note: The proposal for delete/rename is still under discussion, not yet finalized. 
",pull-request-available,[],HDDS,New Feature,Major,2020-08-10 19:53:31,2
13321725,Writing delta to Ozone hangs when creating the _delta_log json,"I am testing writing delta, OSS not databricks, data to Ozone FS since my company is looking to replace Hadoop if feasible. However, whenever I write delta table, the parquet files are writing, the delta log directory is created, but the json is never writing. 

I am using the spark operator to submit a batch test job to write about 5mb of data.

Neither on the driver nor on the executor is there an error. The driver never finishes since the creation of the json hangs.

 

Code I used for testing spark operator and then I ran the pieces in the shell for testing. In the save path, update bucket and volume info for your data store.
{code:java}
package app.OzoneTest

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{BinaryType, StringType}

object CreateData {

  def main(args: Array[String]): Unit = {

    val spark: SparkSession = SparkSession
      .builder()
      .appName(s""Create Ozone Mock Data"")
      .enableHiveSupport()
      .getOrCreate()

    import spark.implicits._

    val df: DataFrame = Seq.fill(100000)
    {(randomID, randomLat, randomLong, randomDates, randomHour)}
      .toDF(""msisdn"", ""latitude"", ""longitude"", ""par_day"", ""par_hour"")
      .withColumn(""msisdn"", $""msisdn"".cast(StringType))
      .withColumn(""msisdn"", sha1($""msisdn"".cast(BinaryType)))
      .select(""msisdn"", ""latitude"", ""longitude"", ""par_day"", ""par_hour"")

    df
      .repartition(3, $""msisdn"")
      .sortWithinPartitions(""latitude"", ""longitude"")
      .write
      .partitionBy(""par_day"", ""par_hour"")
      .format(""delta"")
      .save(""o3fs://your_bucker.your_volume/location_data"")

  }

  def randomID: Int = scala.util.Random.nextInt(10) + 1

  def randomDates: Int = 20200101 + scala.util.Random.nextInt((20200131 - 20200101) + 1)

  def randomHour: Int = scala.util.Random.nextInt(24)

  def randomLat: Double = 13.5 + scala.util.Random.nextFloat()

  def randomLong: Double = 100 + scala.util.Random.nextFloat()
}
{code}",delta filesystem scala spark,['Ozone Filesystem'],HDDS,Bug,Major,2020-08-10 04:28:42,1
13321567,Adding Owner info for Authorizer plugin to honor owner access rights,Currently external authorizer does not have the owner info of the volume bucket keys when authorizing requests. Explicit rules/policies must be set before volume/bucket/key creation is allowed even for owner themselves. ,pull-request-available,[],HDDS,Improvement,Major,2020-08-07 17:26:41,4
13321518,Incomplete OzoneFileSystem statistics,OzoneFileSystem does not record some of the operations that are defined in [Statistic|https://github.com/apache/hadoop-ozone/blob/d7ea4966656cfdb0b53a368eac52d71adb717104/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/Statistic.java#L44-L75].,pull-request-available,['Ozone Filesystem'],HDDS,Bug,Minor,2020-08-07 13:40:29,0
13321378,Retry request on different OM on AccessControlException,"If a client attempts a request on an OM which has not caught up with leader OM and hence does have the delegation token, the request could fail with AccessControlException without trying it on other OMs.
On AccessControlException, all OMs must be tried once before the request is failed.",pull-request-available,['OM HA'],HDDS,Bug,Major,2020-08-06 20:16:09,7
13321332,Remove leftover robot.robot,"An unused Robot test ({{robot.robot}}) was accidentally added in HDDS-3612.  It was refactored to separate {{string_tests.robot}} and {{fs_tests.robot}}, but the original file was not removed.",pull-request-available,['test'],HDDS,Task,Trivial,2020-08-06 16:13:18,0
13321197,Client should not retry same OM on network connection failure,"Right now retry logic on client to OM is, it will try connect to OM1, if it is leader fine, else try with next OM and so on. If OM1, is down, client retries for 50 times when ipc.client.connect.max.retries is set to 50 and ipc.client.connect.retry.interval default to 1sec, so a total of 50seconds is spent in retry and then move to next OM. 
I think here client -> OM should have its own retry policy, in this way if the first OM is down, to complete request, the user does not need to wait for 50sec.

As ipc.client.connect.retry.interval and ipc.client.connect.max.retries  are common configurations for RPC, creating a new default retry policy with smaller values would be nice. 



{code:java}
20/08/06 00:21:29 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:30 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:31 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:32 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:33 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:34 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
{code}
",pull-request-available,"['OM HA', 'Ozone Client']",HDDS,Bug,Major,2020-08-06 00:25:49,7
13320980,Fix InstallSnapshot in OM HA,OzoneManagerStateMachine#notifyInstallSnapshotFromLeader() checks the incoming roleInfoProto and proceeds with install snapshot request only if the role is Leader. This check is wrong and the roleInfoProto will contain the self node ID and not the leaders.,pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-08-05 00:26:44,7
13320596,Wrong use of AtomicBoolean in HddsDatanodeService,"* {{AtomicBoolean isStopped}} should be {{final}}, not {{volatile}}, since the reference is not being changed
* {{stop()}} should use atomic {{getAndSet()}} instead of {{get()}} followed by {{set()}}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-08-03 05:59:25,0
13320552,Failed acceptance test missing from bundle,"Acceptance test run is stopped after any failed test, and its logs are missing from the bundle.

{code:title=misc suite}
hadoop27
hadoop31
hadoop32
ozone-csi
ozone-mr
ozone-om-ha-s3
ozone-topology
ozones3-haproxy
ozonesecure-mr
ozonesecure-om-ha
upgrade
{code}

* {{ozone-topology}} failed in [this run|https://github.com/apache/hadoop-ozone/runs/927545620], {{ozones3-haproxy}} and subsequent tests were skipped, {{ozone-topology}} robot log is missing from [bundle|https://github.com/apache/hadoop-ozone/suites/984510888/artifacts/12629978]
* {{ozone-secure}} failed in [this run|https://github.com/adoroszlai/hadoop-ozone/runs/933098576], it is the only test in {{secure}} suite, so {{acceptance-secure.zip}} was not even created",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2020-08-02 16:57:19,0
13320400,Cleanup GitHub workflow,"Cleanup GitHub workflow definition:

# Provide name for all steps
# Apply HDDS-4038 to new {{bats}} and {{kubernetes}} checks
# Apply HDDS-3877 to new {{bats}} check
# Fix mixed use of 2, 3, 4 spaces indentation
# Remove unnecessary job names (where job name and ID are the same)",pull-request-available,['build'],HDDS,Improvement,Major,2020-07-31 12:41:03,0
13319987,Add more ignore rules to the RAT ignore list,"We have separated rat ignore list for Hdds and Ozone projects but some files (which are ignored by .gitignore) not added to there. It's not a problem on github as rat is executed on a clean repo, but locally it can make the execution easier (exclude files which are already ignored).

Rats can read ignore list from VCS ignore file, unfortunately only from directory of the current project not from the root of the git repository.",pull-request-available,[],HDDS,Improvement,Trivial,2020-07-29 14:40:45,1
13319822,Deprecate ozone.s3g.volume.name,"HDDS-3612 introduced bucket links.
After this feature now we don't need this parameter, any volume/bucket can be exposed to S3 via using bucket links.

ozone bucket link srcvol/srcbucket destvol/destbucket

So now to expose any ozone bucket to S3G

For example, the user wants to expose a bucket named bucket1 under volume1 to S3G, they can run below command

{code:java}
ozone bucket link volume1/bucket1 s3v/bucket2
{code}

Now, the user can access all the keys in volume/bucket1 using s3v/bucket2 and also ingest data to the volume/bucket1 using using s3v/bucket2

This Jira is opened to remove the config from ozone-default.xml
And also log a warning message to use bucket links, when it does not have default value s3v.
",pull-request-available,['S3'],HDDS,Bug,Blocker,2020-07-28 18:54:07,2
13319704,Update documentation for the GA release,"HDDS-3413 is opened to add OM HA related documentation to the Ozone docs but it turned out that it contains additional out-of-date (and missing) information.

This issue is opened to track a big documentation update.",pull-request-available,['documentation'],HDDS,Task,Blocker,2020-07-28 08:33:37,1
13319693,Ozone /conf endpoint triggers kerberos replay error when SPNEGO is enabled ,"{code}
curl  -k --negotiate -X GET -u : ""https://quasar-jsajkc-8.quasar-jsajkc.root.hwx.site:9877/conf""
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 403 GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</title>
</head>
<body><h2>HTTP ERROR 403 GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</h2>
<table>
<tr><th>URI:</th><td>/conf</td></tr>
<tr><th>STATUS:</th><td>403</td></tr>
<tr><th>MESSAGE:</th><td>GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</td></tr>
<tr><th>SERVLET:</th><td>conf</td></tr>
</table>

</body>
</html>
{code}",pull-request-available,[],HDDS,Bug,Major,2020-07-28 07:46:55,4
13319606,Eliminate GitHub check warnings,"GitHub Actions warns that 

bq. The ""master"" branch is no longer the default branch name for actions/upload-artifact

and similarly for actions/checkout.

So we should use specific version instead of master.",pull-request-available,['build'],HDDS,Improvement,Trivial,2020-07-27 19:42:44,0
13319511,Make the acceptance test reports hierarchical,"Acceptance test reports of today  uses a generated name for each of the executed robot tests.

Instead of using a flat structure with generated name it seems to be better to use a hierarchical structure which represents the directory structure.",pull-request-available,[],HDDS,Improvement,Major,2020-07-27 09:44:20,1
13319495,Run author check without docker,"{{author.sh}} can be run in CI without docker container, since it's mostly just a recursive grep.",pull-request-available,['build'],HDDS,Improvement,Minor,2020-07-27 08:03:52,0
13319473,Run shell tests in CI,Shell scripts can be tested using [bats|https://github.com/bats-core/bats-core].  Such tests should be run as part of CI.,pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-07-27 06:35:28,0
13319221,Recon unable to add a new container which is in CLOSED state.,"{code}
2020-07-24 19:56:11,777 INFO org.apache.hadoop.ozone.recon.scm.ReconContainerManager: Exception while adding container #1 .
java.io.IOException: Pipeline PipelineID=ccfb3a54-848c-4ed2-91bf-a174267e3435 not found. Cannot add container #1
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.addNewContainer(ReconContainerManager.java:119)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.checkAndAddNewContainer(ReconContainerManager.java:92)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:62)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-07-24 19:56:11,777 ERROR org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler: Exception while checking and adding new container.
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=06bf6a83-9afb-4477-b18d-de4c6556ce4b not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:372)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:111)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:350)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.addNewContainer(ReconContainerManager.java:126)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.checkAndAddNewContainer(ReconContainerManager.java:92)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:62)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Blocker,2020-07-24 19:56:33,5
13319194,Suppress ERROR message when SCM attempt to create additional pipelines,"This gives false negative errors and can flood SCM log. 


{code:java}
scm_1       | 2020-07-24 16:39:51,756 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-07-24 16:39:51,757 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
{code}
",pull-request-available,[],HDDS,Improvement,Major,2020-07-24 16:43:47,4
13319173,Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true,"Sets ozone.om.enable.filesystem.paths=true, then starts the Ozone cluster.
{code:java}
[root~]$ ozone fs -mkdir o3fs://bucket2.vol2.ozone1/subdir2
[root~]$ ozone fs -mv o3fs://bucket2.vol2.ozone1/subdir2 o3fs://bucket2.vol2.ozone1/subdir2-renamedmv: Key not found /vol2/bucket2/subdir2
{code}
 ",pull-request-available,[],HDDS,Bug,Blocker,2020-07-24 13:24:20,2
13319135,Add test for creating encrypted key,Add acceptance test to create a key in encrypted bucket.,pull-request-available,['Ozone Manager'],HDDS,Test,Major,2020-07-24 09:52:22,0
13319052,Ozone s3 API return 400 Bad Request for head-bucket for non existing bucket,"Ozone s3 API returns 400 Bad Request for head-bucket for non-existing bucket.

hrt_qa$ aws s3api  --ca-bundle=/usr/local/share/ca-certificates/ca.crt --endpoint https://s3g:9879/  head-bucket --bucket fsdghj

An error occurred (400) when calling the HeadBucket operation: Bad Request

It should return 404 as per AWS documentation:
https://docs.aws.amazon.com/cli/latest/reference/s3api/head-bucket.html

A client error (404) occurred when calling the HeadBucket operation: Not Found ",pull-request-available,['S3'],HDDS,Bug,Blocker,2020-07-23 22:16:00,2
13319046,Organize Recon DBs into a 'DBDefinition'.,"* ReconNodeManager uses node db in an old format which is not part of ReconDBDefinition. Move the definition to ReconDBDefinition.
* Create DB Definition for Recon Container DB.
* Modify DBScanner tool to allow it to read Recon DBs. ",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-07-23 21:35:06,5
13318998,ACL commands like getacl and setacl should return a response only when Native Authorizer is enabled,"Currently, the getacl and setacl commands return wrong information when an external authorizer such as Ranger is enabled. There should be a check to verify if Native Authorizer is enabled before returning any response for these two commands.

If an external authorizer is enabled, it should show a nice message about managing acls in external authorizer.  ",pull-request-available,"['Ozone CLI', 'Ozone Manager']",HDDS,Task,Major,2020-07-23 16:49:15,2
13318928,Datanode log spammed by NPE,"{code}
datanode_1  | 2020-07-22 13:11:47,845 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 2 seconds.
datanode_1  | 2020-07-22 13:11:47,846 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,847 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,848 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,848 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,851 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
...
thread in pool for past 22 seconds.
datanode_1  | 2020-07-22 13:11:47,854 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
{code}

This increases acceptance test logs to several hundred MBs.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2020-07-23 12:29:55,0
13318926,Acceptance check may run against wrong commit,"For push builds, acceptance check may build and test a different commit than the one that was pushed.

The check for [HDDS-3991|https://github.com/apache/hadoop-ozone/commit/404ec6d0725cfe9c80aa912f150c6474037b10bb] built [HDDS-3933|https://github.com/apache/hadoop-ozone/commit/ff7b5a3367eccc0969bfd92a2cafe48899a2aaa5]:

{code:title=https://github.com/apache/hadoop-ozone/runs/898449998#step:4:30}
HEAD is now at ff7b5a336 HDDS-3933. Fix memory leak because of too many Datanode State Machine Thread (#1185)
{code}",pull-request-available,['build'],HDDS,Bug,Major,2020-07-23 12:26:42,0
13318805,FLAKY-UT: TestWatchForCommit#testWatchForCommitForGroupMismatchException,"[INFO] Running org.apache.hadoop.ozone.client.rpc.TestWatchForCommit
[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 211.911 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestWatchForCommit
[ERROR] testWatchForCommitForGroupMismatchException(org.apache.hadoop.ozone.client.rpc.TestWatchForCommit)  Time elapsed: 38.862 s  <<< ERROR!
java.io.IOException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:740)
	at org.apache.hadoop.ozone.container.TestHelper.waitForPipelineClose(TestHelper.java:220)
	at org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForGroupMismatchException(TestWatchForCommit.java:344)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.ratis.protocol.GroupMismatchException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.
	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:414)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:372)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:355)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:738)
	... 29 more",pull-request-available,['test'],HDDS,Sub-task,Major,2020-07-23 01:53:13,0
13318795,Update S3 related documentation,"HDDS-3993 created volume required for S3G during the OM startup.
So, remove the step that s3v volume needs to be created.",pull-request-available,[],HDDS,Bug,Major,2020-07-22 23:34:57,2
13318794,S3G startup fails when multiple service ids are configured.,"This Jira is to fix this TODO.

OzoneServiceProvider.java L59:
{code:java}
      // HA cluster.
      //For now if multiple service id's are configured we throw exception.
      // As if multiple service id's are configured, S3Gateway will not be
      // knowing which one to talk to. In future, if OM federation is supported
      // we can resolve this by having another property like
      // ozone.om.internal.service.id.
      // TODO: Revisit this later.
      if (serviceIdList.size() > 1) {
        throw new IllegalArgumentException(""Multiple serviceIds are "" +
            ""configured. "" + Arrays.toString(serviceIdList.toArray()));
{code}

      ",pull-request-available,[],HDDS,Bug,Major,2020-07-22 23:26:30,2
13318790,"Recon Overview page: The volume, bucket and key counts are not accurate","The counts shown in the overview page are not accurate due to the usage of ""rocksdb.estimate-num-keys"" to get the counts. Instead, keep track of accurate counts by updating the counter in a global table every time an event is triggered via FileSizeCount Task in Recon.  ",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-07-22 23:03:45,6
13318770,Recon should fallback to ozone.om.service.ids when the internal service id is not defined.,"Recon connects to OM via RPC using the ""ozone.om.internal.service.id"" to get updates. If the above config is not defined, but the ozone.om.service.ids is defined, Recon should use the latter as a fallback. Currently, a single Recon instance supports only 1 OM HA cluster at a time. Hence, if multiple ids are defined, Recon will pick the first.

Thanks to [~vivekratnavel] for reporting the issue.",pull-request-available,['Ozone Recon'],HDDS,Bug,Blocker,2020-07-22 19:42:15,5
13318768,Generate encryption info for the bucket outside bucket lock,"This Jira is to generate FileEncryption for a key outside the bucket lock.
As right now, we hold the lock when making a network call to KMS to obtain encryption info.",pull-request-available,"['Ozone Manager', 'Security']",HDDS,Improvement,Major,2020-07-22 19:26:38,2
13318759,Disallow MPU on encrypted buckets.,"With HDDS-3612 buckets created via ozone are also accessible via S3.
This has caused a problem when the bucket is encrypted, the keys are not encrypted on disk.

*2 Issues:*
1. On OM, for each part a new encryption info is generated. During complete Multipart upload, the encryption info is not stored in KeyInfo.
2. On the client, for part upload, the encryption info is silently ignored.

If we don't throw an error, on an encrypted bucket, key data is not encrypted on disks.
For 0.6.0 release, we can mark this as not supported, and this will be fixed in next release by HDDS-4005


",pull-request-available,[],HDDS,Bug,Blocker,2020-07-22 18:22:03,2
13318619,Split acceptance tests to reduce CI feedback time,"CI checks run integration tests in parallel using GitHub Actions matrix build.  I propose to use the same approach for acceptance tests, which are currently run in a single check, taking about 1.5 hours.

Also, some of the integration test splits could be combined to reduce the number of checks, as we have a limit on parallel actions.",pull-request-available,['test'],HDDS,Improvement,Major,2020-07-22 06:07:54,0
13318602,Shorten Ozone FS Hadoop compatibility module names,"The name of {{hadoop-ozone-filesystem-hadoopX}} modules is so long that Maven output looks ugly:

{code}
...
[INFO] Apache Hadoop Ozone Insight Tool ................... SUCCESS [  3.940 s]
[INFO] Apache Hadoop Ozone FileSystem Shaded .............. SUCCESS [02:28 min]
[INFO] Apache Hadoop Ozone FileSystem Hadoop 2.x compatibility SUCCESS [ 14.397 s]
[INFO] Apache Hadoop Ozone FileSystem Hadoop 3.x compatibility SUCCESS [ 13.095 s]
[INFO] Apache Hadoop Ozone Distribution ................... SUCCESS [ 12.380 s]
[INFO] Apache Hadoop Ozone Fault Injection Tests .......... SUCCESS [  0.725 s]
{code}",pull-request-available,['build'],HDDS,Improvement,Trivial,2020-07-22 04:17:39,0
13318585,Ozone certificate needs additional flags and SAN extension for GRPC TLS.,"Current Ozone certificate are good for sign/verify tokens but can't do SSL handshake. 

Here are a few missing pieces: 
1. Caused by: java.security.cert.CertificateException: No subject alternative names present
        at java.base/sun.security.util.HostnameChecker.matchIP(HostnameChecker.java:137)

2. Caused by: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
        at java.base/sun.security.validator.EndEntityChecker.checkTLSServer(EndEntityChecker.java:278)
",pull-request-available,[],HDDS,Improvement,Major,2020-07-22 00:17:51,4
13318582,Missing TLS client configurations to allow ozone.grpc.tls.enabled.,"As a result, when ozone.grpc.tls.enabled, RATIS THREE pipeline will not work as DN failed in SSL handshaking without the TLS configuration. 




",pull-request-available,[],HDDS,Improvement,Major,2020-07-22 00:14:19,4
13318363,Create volume required for S3G during OM startup,Create volume required for S3G operations during OM startup,pull-request-available,[],HDDS,Bug,Major,2020-07-21 00:10:14,2
13318225,Ignore protobuf lock files,"HDDS-3595 introduced a new build time check to make sure protobuf files are changed only with backward compatible way. A lock file which contains the structure of the proto files are committed to the repository and during the build the new state of the proto files are compared with the committed version.

Unfortunately the plugin always generate a new lock file, even if it should be updated only after the releases. To make it more safe (and less confusing) I suggest putting the lock files to the git ignore list",pull-request-available,[],HDDS,Improvement,Major,2020-07-20 10:36:44,1
13318220,Test Kubernetes examples with acceptance tests,"hadoop-ozone/dist/src/main/k8s/example directory contains example kubernetes resources to start Ozone in kubernetes environment. To make sure those resources are working and up-to-date I propose to test them during standard build.

K3s project provides a lightweight Kubernetes distribution which can be installed easily in Github Actions environment and kubernetes based clusters can be tested.",pull-request-available,['kubernetes'],HDDS,Improvement,Major,2020-07-20 10:20:40,1
13318171,Encrypted bucket creation failed with INVALID_REQUEST Encryption cannot be set for bucket links,"Bucket creation with encrypted key fails.

Steps:

# Created encryption key
# Created volume
# Tried to create bucket with encryption key

Result:

{code}
INVALID_REQUEST Encryption cannot be set for bucket links
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-07-20 06:22:27,0
13318156,Frequent failure in TestCommitWatcher#testReleaseBuffersOnException,"{code}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 84.876 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestCommitWatcher
testReleaseBuffersOnException(org.apache.hadoop.ozone.client.rpc.TestCommitWatcher)  Time elapsed: 44.888 s  <<< FAILURE!
java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at org.apache.hadoop.ozone.client.rpc.TestCommitWatcher.testReleaseBuffersOnException(TestCommitWatcher.java:320)
{code}

https://github.com/elek/ozone-build-results/tree/master/2020/07/20/1837/it-client
https://github.com/elek/ozone-build-results/tree/master/2020/07/19/1830/it-client
https://github.com/apache/hadoop-ozone/runs/885336971?check_suite_focus=true
https://github.com/elek/ozone-build-results/tree/master/2020/07/17/1811/it-client
https://github.com/apache/hadoop-ozone/runs/880734025?check_suite_focus=true",pull-request-available,['test'],HDDS,Bug,Major,2020-07-20 05:01:55,0
13317592,Update proto.lock files,HDDS-3807 and HDDS-3612 introduced new additions to proto files but failed to update proto.lock files.  ,pull-request-available,"['Ozone Datanode', 'Ozone Manager']",HDDS,Task,Major,2020-07-19 05:03:33,2
13317177,Use Duration for time in RatisClientConfig,Change parameter and return type of time-related config methods in {{RatisClientConfig}} to {{Duration}}.  This results in more readable parameter values and type safety.,pull-request-available,[],HDDS,Improvement,Major,2020-07-16 14:22:52,0
13317055,Validate KeyNames created in FileSystem requests.,"This jira is to validate KeyNames which are created with OzoneFileSystem.
Similar to how hdfs handles using DFSUtil. isValidName()",pull-request-available,[],HDDS,Bug,Major,2020-07-16 01:21:44,2
13317036,LDB scan fails to read from transactionInfoTable,"*[root@bv-cml-1 ~]# ozone debug ldb --db=/var/lib/hadoop-ozone/om/data/om.db/ scan --column_family=transactionInfoTable
Table with specified name does not exist*

This is because DBDefinition is missing transactionInfo table.",pull-request-available,[],HDDS,Bug,Major,2020-07-15 22:14:13,2
13316926,Remove leftover debug setting,HDDS-3821 [accidentally|https://github.com/apache/hadoop-ozone/pull/1101#issuecomment-658750232] set some [log level to DEBUG|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/resources/log4j.properties#L23-L24] for integration tests.,pull-request-available,['test'],HDDS,Bug,Minor,2020-07-15 13:03:35,0
13316909,SCM failed to start up for duplicated pipeline detected,"SCM LOG：
{code}
2020-07-15 19:25:09,768 [main] ERROR org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SCM start failed with exception
java.io.IOException: Duplicate pipeline ID PipelineID=db5966ec-140f-48d8-b0d6-e6f2ff777a77 detected.
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.addPipeline(PipelineStateMap.java:89)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(PipelineStateManager.java:53)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.initializePipelineState(SCMPipelineManager.java:165)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.<init>(SCMPipelineManager.java:100)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:410)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:281)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:213)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:624)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:144)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:119)


RocksDB dump, string,
rocksdb_ldb --db=scm.db scan --column_family=pipelines

$db5966ec-140f-48d8-b0d6-e6f2ff777a77ؑ????٬??????޹? : 
?
$02d3c9b4-7972-4471-a520-fff108b8d32e
                                     10.73.33.62
                                                10.73.33.62""

RATIS?M""

/default-rack???Ƕ?????Ő???? *?71-a520-fff108b8d32e:
$db5966ec-140f-48d8-b0d6-e6f2ff777a77ؑ????٬??????޹?2
?Yf?Hذ????wzw : 
?
$02d3c9b4-7972-4471-a520-fff108b8d32e
                                     10.73.33.62
                                                10.73.33.62""

RATIS?M""

HEX:
0x0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB001 : 0x0AAA010A2430326433633962342D373937322D343437312D613532302D666666313038623864333265120B31302E37332E33332E36321A0B31302E37332E33332E3632220A0A05524154495310824D220F0A0A5354414E44414C4F4E4510834D322430326433633962342D373937322D343437312D613532302D6666663130386238643332653A0D2F64656661756C742D7261636BA2061508F188C9CBC7B6F2E90210AEA6E3C590FEBF90A5011001180120012A3F0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB00132004085A7C1E5B42E
0xDB5966EC140F48D8B0D6E6F2FF777A77 : 0x0AAC010A2430326433633962342D373937322D343437312D613532302D666666313038623864333265120B31302E37332E33332E36321A0B31302E37332E33332E3632220A0A05524154495310824D220F0A0A5354414E44414C4F4E4510834D322430326433633962342D373937322D343437312D613532302D6666663130386238643332653A0D2F64656661756C742D7261636B4800A2061508F188C9CBC7B6F2E90210AEA6E3C590FEBF90A5011001180120012A3F0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB0013200409DFCAF8BB52E
{code}",pull-request-available upgrade-p0,[],HDDS,Bug,Blocker,2020-07-15 12:19:33,5
13316901,Ratis config key mismatch,"Some of the Ratis configurations in integration tests are not applied due to mismatch in config keys.
 # [Ratis|https://github.com/apache/incubator-ratis/blob/master/ratis-client/src/main/java/org/apache/ratis/client/RaftClientConfigKeys.java#L41-L53]: {{raft.client.rpc.watch.request.timeout}}
 [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java#L119-L122]: {{raft.client.watch.request.timeout}}
 # [Ratis|https://github.com/apache/incubator-ratis/blob/4db4f804aa90f9900cda08c79b54a45f80f4213b/ratis-server/src/main/java/org/apache/ratis/server/RaftServerConfigKeys.java#L470-L473]: {{raft.server.notification.no-leader.timeout}}
 [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/conf/DatanodeRatisServerConfig.java#L42]: {{raft.server.Notification.no-leader.timeout}}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-07-15 11:39:47,0
13316718,Avoid HddsProtos.PipelineID#toString,"{{PipelineID}} was recently changed to have integer-based ID in addition to the string ID.  Now log messages including {{PipelineID}} span multiple lines:

{code:title=https://github.com/elek/ozone-build-results/blob/92d31c9b58065b37a371c71c97b346f99163318d/2020/07/11/1626/acceptance/docker-ozone-ozone-freon-scm.log#L218-L223}
datanode_1  | 2020-07-11 13:07:00,540 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: ""8101dcbf-1a28-4f20-863a-0616b4e4bc4b""
datanode_1  | uuid128 {
datanode_1  |   mostSigBits: -9150790254504423648
datanode_1  |   leastSigBits: -8774694229384053685
datanode_1  | }
datanode_1  | .
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-07-14 14:46:23,0
13316677,Intermittent failure in Recon acceptance test due to mixed stdout and stderr,"Recon acceptance test failed with:

{code}
Check if Recon picks up OM data                                       | FAIL |
...
...,{""volume"":""vol-0-40306"",""bucket"":""bucket-0-15468"",""fileSize"":204
100  4177  100  4177    0     0   343k      0 --:--:-- --:--:-- --:--:--  370k
* Connection #0 to host recon left intact
8,""count"":10},...' does not contain '""fileSize"":2048,""count"":10'
{code}

It seems stdout and stderr was mixed, breaking {{""fileSize"":2048}} into two parts, thus search string was not found in the output.",pull-request-available,['test'],HDDS,Bug,Major,2020-07-14 11:19:17,0
13316504,Unable to list intermediate paths on keys created using S3G.,"Keys created via the S3 Gateway currently use the createKey OM API to create the ozone key. Hence, when using a hdfs client to list intermediate directories in the key, OM returns key not found error. This was encountered while using fluentd to write Hive logs to Ozone via the s3 gateway.
cc [~bharat]",pull-request-available,['Ozone Manager'],HDDS,New Feature,Blocker,2020-07-13 17:01:18,2
13315871,OM StateMachine unpause fails with NPE,"Noticed this NPE in OM logs for OM HA [acceptance test|https://github.com/apache/hadoop-ozone/pull/1173/checks?check_run_id=847204159]:

{code}
2020-07-07 20:54:23 WARN  RaftServerImpl:1247 - om2@group-D66704EFC61C: Failed to notify StateMachine to InstallSnapshot. Exception: java.lang.NullPointerException: When ratis is enabled indexToTerm should not be null
{code}",pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-07-09 11:38:20,0
13315733,Update proto.lock files,HDDS-426 introduced new additions to proto files but failed to update proto.lock files.  ,pull-request-available,"['Ozone CLI', 'Ozone Datanode']",HDDS,Task,Major,2020-07-08 21:31:02,6
13315673,Change latest snapshot log to debug,"In OM HA, followers log latest snapshot information twice per second:

{code}
om1_1       | 2020-07-08 15:46:47,097 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om2_1       | 2020-07-08 15:46:47,097 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om1_1       | 2020-07-08 15:46:47,604 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om2_1       | 2020-07-08 15:46:47,604 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om1_1       | 2020-07-08 15:46:48,110 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om2_1       | 2020-07-08 15:46:48,110 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
{code}

I think this should be debug-level message.",pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-07-08 16:09:20,0
13315327,Maven warning due to deprecated expression pom.artifactId,"{code:title=mvn clean}
[INFO] Scanning for projects...
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-interface-client:jar:0.6.0-SNAPSHOT
[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-common:jar:0.6.0-SNAPSHOT
[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.
...
{code}

Same warning in {{hadoop-hdds/pom.xml}} was fixed during review of HDDS-3875, but the one in {{hadoop-ozone/pom.xml}} was left.",pull-request-available,['build'],HDDS,Bug,Trivial,2020-07-07 07:27:07,0
13315266,Fix OMKeyDeletesRequest,"Cache of key should be updated in ValidateAndUpdateCache, as we return response once after adding to cache, and before DoubleBuffer flushes to disk using OmClientResponse#addToDBBatch.",pull-request-available,[],HDDS,Bug,Blocker,2020-07-07 00:36:14,2
13315225,Fix endpoint display in S3 Gateway webpage,Fix the way in which S3g endpoint is being displayed in S3 gateway webpage.,pull-request-available,['S3'],HDDS,Bug,Major,2020-07-06 19:45:11,6
13314840,ConcurrentModificationException in ContainerReportHandler.onMessage,"2020-07-03 14:51:45,489 [EventQueue-ContainerReportForContainerReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@8f6e7cb
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
        at java.util.HashMap$KeyIterator.next(HashMap.java:1469)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:343)
        at java.util.HashSet.<init>(HashSet.java:120)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:127)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-07-03 14:51:45,648 [EventQueue-ContainerReportForContainerReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@49d2b84b
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
        at java.util.HashMap$KeyIterator.next(HashMap.java:1469)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:343)
        at java.util.HashSet.<init>(HashSet.java:120)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:127)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Major,2020-07-03 07:05:07,4
13314556,Remove LevelDB configuration option for DN Metastore,"LevelDB support was removed for OM and SCM DBs but DN Metastore can still be configured to use LevelDB or RocksDB. 
This Jira proposes to remove LevelDB configuration option for DN Metastore (ozone.metastore.impl) and use RocksDB only.",pull-request-available,['Ozone Datanode'],HDDS,Task,Blocker,2020-07-01 21:14:15,7
13314484,Compile error in acceptance test on HDDS-2823,"{code}
[INFO] --- hadoop-maven-plugins:3.2.1:protoc (compile-protoc) @ hadoop-hdds-server-scm ---
[WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program ""protoc"": error=2, No such file or directory
[ERROR] stdout: []
{code}

https://github.com/apache/hadoop-ozone/runs/814218639",pull-request-available,['SCM HA'],HDDS,Bug,Blocker,2020-07-01 13:57:14,0
13314457,Duplicate dot in Prometheus endpoint config name,"{{HddsPrometheusConfig}} includes {{.}} at the end of the prefix, thus generated config name has double dot: {{hdds.prometheus..endpoint.token}}.",pull-request-available,[],HDDS,Bug,Minor,2020-07-01 11:27:05,0
13313784,Fix typo in pom.xml,ratis.thirdpary.version -> ratis.thirdparty.version,pull-request-available,[],HDDS,Bug,Minor,2020-06-27 18:39:15,2
13313673,Improve OM HA Robot tests,"This Jira aims to address the following:
1. Add robot test for Install Snapshot feature 
2. Fix the flakiness in OM HA robot tests (HDDS-3313)",pull-request-available,[],HDDS,Bug,Major,2020-06-26 19:21:01,7
13313592,Make OMHA serviceID optional if one (but only one) is defined in the config ,"om.serviceId is required on case of OM.HA in all the client parameters even if there is only one om.serviceId and it can be chosen.

My goal is:

 1. Provide better usability
 2. Simplify the documentation task ;-)

With using the om.serviceId from the config if 

 1. config is available
 2. om ha is configured 
 3. only one service is configured

It also makes easier to run the same tests with/without HA",pull-request-available,[],HDDS,Improvement,Major,2020-06-26 11:52:08,1
13313585,Do not fail CI check for log upload failure,"This workflow failed only due to temporary problem during artifact upload after successful tests: https://github.com/apache/hadoop-ozone/runs/809777550

To save time and resources, checks should not fail in this case.",pull-request-available,['CI'],HDDS,Improvement,Minor,2020-06-26 11:09:56,0
13313583,Display summary of failures as a separate job step,"Most CI checks print a summary (failed tests, checkstyle/rat violations, etc.) to stdout at the end of the test run, as well as into {{summary.txt}}.  Currently we have the following ways to view this output:

* drill down to the test step, scroll past lots of output
* download raw log, scroll past lots of output
* download artifact, unzip, open {{summary.txt}}

I propose displaying contents of {{summary.txt}} as a separate step.",pull-request-available,['CI'],HDDS,Improvement,Minor,2020-06-26 10:50:39,0
13313580,Package classpath files to the jar files instead of uploading them as artifacts,"During the earlier release we realized that Apache Nexus couldn't handle the classpath files well.

Classpath files are generated during the build and copied to the final distribution to use separated classpath definition for each of the services.

It turned out that the Apache Nexus couldn't handle it well (INFRA-18344)  and HDDS-1510 was an attempt to fix this problem. But during 0.5.0 release we have seen the same problem.

To make the 0.6.0 release more seamless I propose to update the logic and instead of copy the classpath file via an artifact I would pack/unpack them to/from the final artifact jar.",pull-request-available,['0.6.0'],HDDS,Bug,Critical,2020-06-26 10:38:27,1
13313517,Add resource core-site during loading of ozoneconfiguration,"If users add the properties of ozone to core-site, then during loading of OzoneConfiguration, it addsResource ozone-default.xml. This overrides the properties of ozone which are added to core-site.

To avoid this kind of override issue, we can addResource core-site.xml after ozone-default.xml",pull-request-available,[],HDDS,Bug,Major,2020-06-25 22:51:15,2
13313265,Support multi-part-upload with Freon S3 key generator,ozone freon s3kg is a key generator which uses the s3 interface. It uses simple put objects but it turned out that s3 mpu implementation has some specific problem. I would improve the key generator to make it possible to test mpu upload with AWS Java SDK.,pull-request-available,"['freon', 'S3']",HDDS,Improvement,Major,2020-06-24 14:19:32,1
13313095,Add ldb to ozone-runner docker image,Upgrade from Ozone 0.5.0 to 0.6.0 (snapshot) requires some workaround steps (at least for HDDS-3499).  Smoke test for upgrade requires a docker image with the tools necessary for the workaround.,pull-request-available,['docker'],HDDS,Task,Major,2020-06-23 18:20:19,0
13313075,Remove support to start Ozone and HDFS datanodes in the same JVM,"With a few thousands issues ago Ozone was integral part of Hadoop/HDFS project. At that time there were two options to start datanode:

 1. Start in a separated JVM

 2. Start in the same JVM with the HDFS

Today only 1 is the standard way, this is tested and working. 2nd is not working any more but still documented.

I propose to drop the support of this use case as I can't see any benefit to support it anymore:

 1. I think 100% of the users will use Ozone as a separated process not as a HDFS Datanode plugin
 2. Fixing the classpath issues is significant effort as the classpath of HDFS and Ozone are diverged.",pull-request-available,[],HDDS,Improvement,Major,2020-06-23 15:39:37,1
13313074,Datanode in compose/ozonescripts can't be started ,Datanode in compose/ozonscripts can't be started due to an old configuration (OzoneHddsDatanodeService is removed in HDDS-738),pull-request-available,[],HDDS,Bug,Major,2020-06-23 15:31:31,1
13312922,Add ratis.thirdparty.version in main pom.xml,"Introduce ratis.thirdparty.version in main pom.xml
This will help to override ratis.version if ozone is required to compile with a different version.",pull-request-available,[],HDDS,Bug,Major,2020-06-22 23:09:20,2
13312899,Change OMNotLeaderException logging to DEBUG,"In a OM HA setup, client tries the OM's in a round robin fashion to find the leader OM. It is not required to log the retry information for every client call. It creates noise on the console. Instead, we should just log the retry attempts only when all the retries fail. 


{code:java}
20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om1 is not the leader. Suggested leader is OM:om3.20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om1 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)
20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om2 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882){code}",pull-request-available,[],HDDS,Bug,Major,2020-06-22 20:06:39,7
13312241,Introduce Layout Feature interface in Ozone,"* Implement the concept of a 'Layout Feature' in Ozone (with sample usage in Ozone Manager), which defines a specific change in on-disk layout in Ozone.
* Every feature is associated with a layout version, and an API corresponding to the feature cannot be invoked (throws NOT_SUPPORTED_OPERATION) before finalization.
* Created an annotation based 'aspect' for ""guarding"" new APIs that are introduced by Layout Features. Check out TestOMLayoutFeatureAspect#testCheckLayoutFeature.
* Added sample features and tests for ease of review (To be removed before commit).
* Created an abstract VersionManager and an inherited OM Version manager to initialize features, check if feature is allowed, check need to finalize, do finalization.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-06-18 14:00:06,5
13312203,Configuration parsing of ozone insight should be based on fields,"Ozone insight command can print out configuration related to a specific ozone component with parsing the @Config object.

 

But the usage of config annotations has been changed since HDDS-2661. We should check the annotated fields not methods.",pull-request-available,[],HDDS,Bug,Major,2020-06-18 11:57:01,1
13312195,Intermittent failure in TestKeyManagerUnit#listMultipartUploads,"{code}
Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.637 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestKeyManagerUnit
listMultipartUploads(org.apache.hadoop.ozone.om.TestKeyManagerUnit)  Time elapsed: 0.102 s  <<< FAILURE!
java.lang.AssertionError: Creation date is too old
  ...
  at org.apache.hadoop.ozone.om.TestKeyManagerUnit.listMultipartUploads(TestKeyManagerUnit.java:148)
{code}

https://github.com/elek/ozone-build-results/blob/master/2020/06/14/1034/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.TestKeyManagerUnit.txt",pull-request-available,['test'],HDDS,Bug,Major,2020-06-18 11:24:58,0
13312190,Split Ozone FS acceptance tests,"Ozone FS acceptance test is currently a monolithic one, a single test case for each of the filesystems (ofs and o3fs).  Finding out which assertion failed is harder than necessary.",pull-request-available,['test'],HDDS,Improvement,Major,2020-06-18 10:46:30,0
13312158,Hadoop3 artifact should depend on the ozonefs-shaded,"ozonefs-hadoop3 is an all-in-one ozonefs client which can be used as a single jar file.

Unfortunately it uses wrong dependency (ozonefs-common instead of ozonefs-common-shaded) which means that it downloads additional dependencies (netty-all, ...) if it's used from maven.",pull-request-available,[],HDDS,Sub-task,Blocker,2020-06-18 08:36:51,1
13312097,Eliminate duplicated GitHub Actions workflow,"GitHub Actions workflows for Ozone CI have two flavors: post-commit and pull-request.  The only difference between the two is that Sonar is not updated for PRs:

{code}
-       - uses: ./.github/buildenv
-         if: github.repository == 'apache/hadoop-ozone'
-         with:
-           args: ./hadoop-ozone/dev-support/checks/sonar.sh
-         env:
-           SONAR_TOKEN: ${{ secrets.SONARCLOUD_TOKEN }}
-           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
{code}

This issue proposes to keep only the post-commit definition and execute {{sonar.sh}} conditionally.",pull-request-available,['build'],HDDS,Improvement,Minor,2020-06-18 02:52:50,0
13312079,Disable Ozone SPNEGO should not fall back to hadoop.http.authentication configuration ,"HDDS-3282 adds hdds.[compoment].http.auth.[type] for scm/dn and ozone.[component].http.auth.[type] for om/s3g/recon. The type can be simple and kerberos. When the type is kerberos, it will setup the spnego authentication filter for the servlets. 

However, when the filter prefix was not setup properly when the type is simple. As a result, it will fallback to configuration from hadoop.http.authentication.*.

This ticket is opened to fix the authentication filter prefix so that the simple configuration can be honored when SPNEGO is disabled. ",pull-request-available,[],HDDS,Bug,Major,2020-06-17 23:31:19,4
13311991,Schedule daily 2 builds from master branch build,"Mukul suggested to schedule cron based build to have more frequent data points to identify flaky tests.

We can start with two additional daily build which can be independent from the commit frequency (today we build master only after the commits).",build,['build'],HDDS,Improvement,Major,2020-06-17 15:44:55,1
13311637,Add the logic to distribute open containers among the pipelines of a datanode,A datanode can participate in multiple pipelines based on no of raft log disks as well the disk type. SCM should make the distribution of open containers among these set of pipelines evenly.,pull-request-available,[],HDDS,Sub-task,Major,2020-06-16 05:50:23,3
13311636,Make number of open containers on a datanode a function of no of volumes reported by it,The no of open containers on a datanode is to be driven by factor of no of data disks available multiplied by no of open containers per disk. The aim here is to add the logic here and verify it.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-06-16 05:48:35,3
13311634,Propagate raft log disks info to SCM from datanode,No of pipelines to be created on a datanode is to be driven by the no of raft log disks configured on datanode. The Jira here is to add support for propagation of raft log volume info to SCM.,pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2020-06-16 05:43:18,3
13311455,Config being reloaded with hdfs as 'fs.defaultFS',"Noticed performance degradation while running {{SynteticLoadGenerator}} benchmark test with {{fs.defaultFS}}=HDFS *Vs* {{fs.defaultFS}}=O3FS.
{code:java}
Running LoadGenerator against fileSystem: hdfs://vb0929.halxg.cloudera.com:8020
                       *Vs*
Running LoadGenerator against fileSystem: o3fs://bucket2.vol2/
{code}
 

Command to run SyntheticLoadGenerator:-
{code:java}
yarn jar /opt/cloudera/parcels/CDH-7.2.0-1.cdh7.2.0.p0.3738720/jars/hadoop-mapreduce-client-jobclient-3.1.1.7.2.0.0-236-tests.jar NNloadGenerator -writeProbability 1 -readProbability 0.00 -elapsedTime 120 -numOfThreads 300 -root o3fs://bucket2.vol2/fsperf-Jun-11-2020/
{code}
 

With HDFS as 'fs.defaultFS', I could see that configuration is reloading resources always for each FileContext#create API call and is causing delay in submitting requests to OM server.
{code:java}
""Thread-304"" #327 prio=5 os_prio=0 tid=0x00007f1609d42000 nid=0x2990b waiting for monitor entry [0x00007f15cb990000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.misc.URLClassPath.getNextLoader(URLClassPath.java:479)
        - waiting to lock <0x00000000f01097e8> (a sun.misc.URLClassPath)
        at sun.misc.URLClassPath.findResource(URLClassPath.java:224)
        at java.net.URLClassLoader$2.run(URLClassLoader.java:572)
        at java.net.URLClassLoader$2.run(URLClassLoader.java:570)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findResource(URLClassLoader.java:569)
        at java.lang.ClassLoader.getResource(ClassLoader.java:1089)
        at java.lang.ClassLoader.getResource(ClassLoader.java:1084)
        at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2803)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3059)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2991)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2871)
        - locked <0x00000000f1da5f10> (a org.apache.hadoop.hdds.conf.OzoneConfiguration)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1223)
        at org.apache.hadoop.ozone.OmUtils.isServiceIdsDefined(OmUtils.java:175)
        at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:125)
        at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:51)
        at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:167)
        at org.apache.hadoop.fs.DelegateToFileSystem.<init>(DelegateToFileSystem.java:54)
        at org.apache.hadoop.fs.ozone.OzFs.<init>(OzFs.java:41)
        at sun.reflect.GeneratedConstructorAccessor4.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:142)
        at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:180)
        at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:265)
        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:341)
        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:338)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:338)
        at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:330)
        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:85)
        at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.genFile(LoadGenerator.java:330)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.write(LoadGenerator.java:304)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.nextOp(LoadGenerator.java:271)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.run(LoadGenerator.java:236)
{code}
 

With O3FS as 'fs.defaultFS', there is no config#loading.
{code:java}
""Thread-400"" #424 prio=5 os_prio=0 tid=0x00007f1988fe6800 nid=0x58441 in Object.wait() [0x00007f1942080000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1541)
        - locked <0x00000000f60c3ab8> (a org.apache.hadoop.ipc.Client$Call)
        at org.apache.hadoop.ipc.Client.call(Client.java:1499)
        at org.apache.hadoop.ipc.Client.call(Client.java:1396)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
        at com.sun.proxy.$Proxy10.submitRequest(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
        - locked <0x00000000f60a1690> (a org.apache.hadoop.io.retry.RetryInvocationHandler$Call)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
        at com.sun.proxy.$Proxy10.submitRequest(Unknown Source)
        at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:424)
        at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createFile(OzoneManagerProtocolClientSideTranslatorPB.java:1632)
        at org.apache.hadoop.ozone.client.rpc.RpcClient.createFile(RpcClient.java:1088)
        at org.apache.hadoop.ozone.client.OzoneBucket.createFile(OzoneBucket.java:538)
        at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.createFile(BasicOzoneClientAdapterImpl.java:227)
        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.createOutputStream(BasicOzoneFileSystem.java:270)
        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.create(BasicOzoneFileSystem.java:250)
        at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1263)
        at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)
        at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:615)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)
        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
        at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.genFile(LoadGenerator.java:330)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.write(LoadGenerator.java:304)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.nextOp(LoadGenerator.java:271)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.run(LoadGenerator.java:236)
{code}",om-perf performance,[],HDDS,Bug,Major,2020-06-15 10:11:06,1
13311236,Allow running coverage locally,Currently {{coverage-report.sh}} only works in GitHub Actions environment.,pull-request-available,['build'],HDDS,Improvement,Minor,2020-06-13 05:02:58,0
13311233,No coverage reported for Ozone FS,"Ozone FS classes are ignored by JaCoCo agent, hence [reported coverage is 0%|https://sonarcloud.io/component_measures?id=hadoop-ozone&metric=coverage&selected=hadoop-ozone%3Ahadoop-ozone%2Fozonefs-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fozone&view=list].",pull-request-available,['build'],HDDS,Bug,Major,2020-06-13 04:38:33,0
13311130,Use Hadoop 2.7.2 for ozone-mr/hadoop27 acceptance tests,"Today 2.7.7 is used but to be compatible with older Hadoop (including 2.7.4 which is included in spark), it's easier to test with an older version",pull-request-available,[],HDDS,Sub-task,Major,2020-06-12 12:18:18,0
13311128,Separate client proto files of Ozone to separated subprojects,"Similar to the previous patch, we need to move the Ozone client interfaces to a separated project. It would help us to reuse it from different projects and monitor to backward compatibility.",pull-request-available,[],HDDS,Sub-task,Major,2020-06-12 12:15:48,1
13310916,Use matrix build for integration test,Integration test profiles can be simplified with matrix builds in GitHub Actions.,pull-request-available,['build'],HDDS,Improvement,Major,2020-06-11 12:56:22,0
13310885,Upload coverage even if tests failed,"PRs get code coverage feedback from Codecov thanks to changes implemented in HDDS-3726.  Codecov needs coverage data for each new revision on master.  However, _coverage_ check is currently [skipped|https://github.com/apache/hadoop-ozone/runs/759894317?check_suite_focus=true] if any tests fail.",pull-request-available,['build'],HDDS,Improvement,Major,2020-06-11 09:47:14,0
13310820,Block distribution in a pipeline among open containers is not uniform,"Currently, with concurrent allocate block calls, the block allocation among the open containers of a pipeline is not uniform as with concurrent logic, block allocation logic with last used notion does not hold up. The idea here is to address this.",pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-06-11 06:23:02,3
13310817,Reduce recon UI build time,"Hi [~vivekratnavel], [~elek],

The change in HDDS-3682 increased Recon build time by ~10 minutes, eg.:

{code:title=before}
[INFO] Apache Hadoop Ozone Recon .......................... SUCCESS [04:04 min]
{code}

{code:title=after}
[INFO] Apache Hadoop Ozone Recon .......................... SUCCESS [16:34 min]
{code}

Recon UI is built in _compile_, _acceptance_ and _coverage_ checks.  Since _coverage_ runs after all other checks, the increment affects overall CI response time twice.  (Effect was 3x before HDDS-3726, thanks for the fix in {{integration.sh}}.)

Is there any way to speed this up, eg. by installing dependencies in the ozone-build docker image?

before: https://github.com/apache/hadoop-ozone/actions/runs/130155633
pre-HDDS-3726: https://github.com/apache/hadoop-ozone/actions/runs/130301719
current: https://github.com/apache/hadoop-ozone/actions/runs/131217499",pull-request-available,['build'],HDDS,Bug,Critical,2020-06-11 06:07:17,6
13310801,Upgrading RocksDB version to avoid java heap issue,"Currently we have rocksdb 6.6.4 as major version and there are some jvm issues in tests (happened in [https://github.com/apache/hadoop-ozone/pull/1019]) related to rocksdb core dump. We may upgrade to 6.8.1 to avoid this issue.

{{JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni2954960755376440018.jnilib+0x602b8]  rocksdb::GetColumnFamilyID(rocksdb::ColumnFamilyHandle*)+0x8

See full dump at [https://the-asf.slack.com/files/U0159PV5Z6U/F0152UAJF0S/hs_err_pid90655.log?origin_team=T4S1WH2J3&origin_channel=D014L2URB6E](url)}}",pull-request-available,['upgrade'],HDDS,Bug,Major,2020-06-11 02:38:50,1
13310419,Fluentd writing to secure Ozone S3 API fails with 500 Error.,"Error on fluentd side.
{code}
opened
starting SSL for host1:9879...
SSL established
<- ""PUT /logs-bucket-1/logs/mytag/2020/06/05//202006052222_190411.gz HTTP/1.1\r\nContent-Type: application/x-gzip\r\nAccept-Encoding: \r\nUser-Agent: aws-sdk-ruby3/3.94.0 ruby/2.4.10 x86_64-l
inux aws-sdk-s3/1.63.0\r\nX-Amz-Storage-Class: STANDARD\r\nExpect: 100-continue\r\nContent-Md5: zGKVGGaD/U5WUK3cdWQiSA==\r\nHost: host1:9879\r\nX-Amz-Content-Sha256:
 277fe97f57a1127ee1a0765979ffd3270a6c18c6f75ff6a0f843e7163a338de2\r\nContent-Length: 44726\r\nX-Amz-Date: 20200608T190412Z\r\nAuthorization: AWS4-HMAC-SHA256 Credential=hdfs@ROOT.HWX.SITE/202
00608/us-east-1/s3/aws4_request, SignedHeaders=content-md5;content-type;expect;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-storage-class, Signature=11c1d0a43325d3f7b9d25dbd02023cef2
69b66f6a93fa4e1c935b52e3e372f70\r\nAccept: */*\r\n\r\n""
-> ""HTTP/1.1 100 Continue\r\n""
-> ""\r\n""
-> ""HTTP/1.1 500 Server Error\r\n""
-> ""Pragma: no-cache\r\n""
-> ""X-Content-Type-Options: nosniff\r\n""
-> ""X-FRAME-OPTIONS: SAMEORIGIN\r\n""
-> ""X-XSS-Protection: 1; mode=block\r\n""
-> ""Connection: close\r\n""
-> ""\r\n""
reading all...
-> """"
{code}
",pull-request-available,[],HDDS,Bug,Blocker,2020-06-09 15:07:47,5
13310385,Intermittent failure in TestDeleteWithSlowFollower,"TestDeleteWithSlowFollower failed soon after it was re-enabled in HDDS-3330.

{code:title=https://github.com/apache/hadoop-ozone/runs/753363338}
[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.647 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 0.163 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.junit.Assert.assertNotNull(Assert.java:631)
  at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)
{code}

CC [~shashikant] [~elek]",pull-request-available,['test'],HDDS,Sub-task,Major,2020-06-09 12:25:07,0
13310169,Add test coverage of the acceptance tests to overall test coverage ,Acceptance test coverage should be added to the generic coverage numbers. We have a lot of important tests there...,pull-request-available,[],HDDS,Improvement,Major,2020-06-08 15:29:43,1
13310141,Storage-class support for Ozone,"Use a storage-class as an abstraction which combines replication configuration, container states and transitions. 

See this thread for the detailed design doc:

 

[https://lists.apache.org/thread.html/r1e2a5d5581abe9dd09834305ca65a6807f37bd229a07b8b31bda32ad%40%3Cozone-dev.hadoop.apache.org%3E]
",pull-request-available,[],HDDS,Improvement,Major,2020-06-08 13:42:26,1
13310133,Rename framework to common-server,"I started to document the project hierarchy of Ozone and found that to subproject name is very confusing:

 

 1. framework: it supposed to be similar to the common but share the classes only between server side projects (om/scm/...). I propose to call it server-common

 2. container-service: While we have historical reasons why it's a separated project, but today we can call it datanode and remove hadoop-ozone/datanode.

 

While I can learn both the specific names, for new contributors it can help if we use more straightforward names.


In this patch I would like to fix the first item (framework --> common-server)",TriagePending pull-request-available,['build'],HDDS,Improvement,Major,2020-06-08 12:58:32,1
13309797,Reload old OM state if Install Snapshot from Leader fails,"Follower OM issues a pause on its services before installing new checkpoint from Leader OM (Install Snapshot). If this installation fails for some reason, the OM stays in paused state. It should be unpaused and the old state should be reloaded.",pull-request-available,['OM HA'],HDDS,Sub-task,Critical,2020-06-05 21:13:49,7
13309606,Upload code coverage to Codecov and enable checks in PR workflow of Github Actions,HDDS-3170 aggregates code coverage across all components. We need to upload the reports to codecov to be able to keep track of coverage and coverage diffs to be able to tell if a PR does not do a good job on writing unit tests.,pull-request-available,['build'],HDDS,Bug,Major,2020-06-05 05:27:20,6
13309478,Datanode configuration object has wrong values,"Regardless of values configured for {{hdds.datanode.replication.streams.limit}} and {{hdds.datanode.container.delete.threads.max}}, these config items always use 10 and 2, respectively.

Also, no warning is logged for invalid values ({{< 1}}).",Triaged pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-06-04 15:42:18,0
13309462,Datanode may fail to stop,"TestOzoneManagerListVolumes timed out after 2 minutes while trying to stop the mini cluster.  It seems one of the datanodes was stuck in an infinite loop trying to execute a task on a terminated executor:

{code:title=}
2020-06-03 15:28:19,475 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(221)) - Unable to finish the execution.
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@73c36b6c rejected from org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor@5c021707[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:181)
	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.execute(RunningDatanodeState.java:143)
	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:411)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:208)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:375)
	at java.lang.Thread.run(Thread.java:748)
{code}

{code:title=grep -c 'Unable to finish the execution' hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestOzoneManagerListVolumes-output.txt}
2087250
{code}

Test output eventually grew to 2.6GB.

https://github.com/apache/hadoop-ozone/runs/735169623",Triaged,['Ozone Datanode'],HDDS,Bug,Critical,2020-06-04 13:59:43,0
13309450,CSI smoketest fails if socket file is not created on time,"HDDS-3461 introduced a new CSI smoketest (to be sure that the CSI daemon can be started).

It was reverted because a failure on the master and commited after an additional check is added to wait until the CSI socket is created.

Unfortunately this check is bad. In some cases it can fail:

For example in here:

[https://github.com/jsoft88/hadoop-ozone/runs/734147343?check_suite_focus=true]

 
{code:java}
connection error: desc = ""transport: Error while dialing dial unix /tmp/csi.sock: connect: no such file or directory"" {code}
Thanks to [~jsoft88] , who reported this problem.",pull-request-available,['test'],HDDS,Bug,Major,2020-06-04 13:17:43,1
13309425,Compile of Ozone fails with JDK 11+,"@PostConstruct annotation is removed from JDK (it's Java EE) in the recent JDKs.

It's used by the Configuration annotation but we don't need to use Java EE annotations:
{code:java}
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-hdds-config: Compilation failure: Compilation failure: 
[ERROR] /Users/sbanerjee/ozone_fork/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationReflectionUtil.java:[20,24] cannot find symbol
[ERROR]  symbol:  class PostConstruct
[ERROR]  location: package javax.annotation
[ERROR] /Users/sbanerjee/ozone_fork/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationReflectionUtil.java:[139,38] cannot find symbol
[ERROR]  symbol:  class PostConstruct
{code}
 

Thanks  Shashikant for the bug report.",pull-request-available,['build'],HDDS,Bug,Major,2020-06-04 11:32:29,1
13309175,Merge archived jacoco coverage results,"HDDS-3635 started to archive the jacoco coverage data for each of the unit and integration tests (unit, it-*).

 

This patch introduces a new build step to combine all of them together and archive the coverage report in HTML as a build artifact.

Notes:

 1. acceptance test coverage is not yet included

 2. I decided to do it only for master (branch) builds as it requires a new build which adds ~15 minutes to the full build. As the coverage data is not (yet) used for PR we don't need to enable it (yet)

 3. We can further improve it to upload the merged data to somewhere (sonar?) Can be done in the next Jira",pull-request-available,['build'],HDDS,Bug,Major,2020-06-03 11:44:10,1
13308940,Update all the documentation to use ozonefs-hadoop2/3  instead of legacy/current,Using legacy jar is legacy. We need to updated all the docs.,pull-request-available,[],HDDS,Sub-task,Blocker,2020-06-02 12:18:40,1
13308939,Consider avoiding file lookup calls in writeChunk hotpath,"In getChunkFile internally, it invokes ""verifyChunkDirExists"". This causes file existence checks for the directory and throws IO exception accordingly. If the file is anyways going to be written, it is better to handle it later and throw the same exception. This could avoid file checks for every ""writechunk""

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L106]

 

File channels are cached anyways in ""OpenFiles"". So if we can avoid ""file.getAbsolutePath()"", this could save memory and resolving paths. 

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L118]

 

Also ""validateChunkForOverwrite"" can be optimised, as ""isOverWritePermitted"" would be false most of the times.

 

!Screenshot 2020-06-02 at 5.28.59 PM.png|width=835,height=510!

 ",Triaged performance,[],HDDS,Improvement,Blocker,2020-06-02 12:07:03,0
13308874,Number of open containers per pipeline should be tuned as per the number of disks on datanode,"Currently, ""ozone.scm.pipeline.owner.container.count"" is configured by default to 3. The default should ideally be a function of the no of disks on a datanode. A static value may lead to uneven utilisation during active IO .",Performance,['Ozone Datanode'],HDDS,Bug,Major,2020-06-02 06:26:20,3
13308787,Ozone Non-Rolling upgrades,Support for Non-rolling upgrades in Ozone.,pull-request-available,[],HDDS,New Feature,Major,2020-06-01 19:23:37,5
13308225,Remove replay logic from actual request logic,"HDDS-3476 used the transaction info persisted in OM DB during double buffer flush when OM is restarted. This transaction info log index and the term are used as a snapshot index. So, we can remove the replay logic from actual request logic. (As now we shall never have the transaction which is applied to OM DB will never be again replayed to DB)",pull-request-available,['OM HA'],HDDS,Sub-task,Blocker,2020-05-29 04:45:58,2
13308166,Recon UI: Add interactive visualization for file size counts,Include a histogram to interactively view file size counts across each volume/bucket,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-05-28 21:40:20,6
13308164,Recon: Add support to store file size counts in each volume/bucket,Update file size count task to keep track of file size counts across each volume/bucket. Make the necessary changes to the underlying schema.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-05-28 21:38:31,6
13308021,Remove usage of DFSUtil.addPBProtocol method,"Hadoop 3.3.0 upgraded protocol buffers to 3.7.1 and RPC code have been changed. This change will cause compile failure in Ozone.

Vinayakumar is fixing this in Hadoop-side (HADOOP-17046) but it would be better for Ozone to avoid the usage of Hadoop {{@Private}} classes to make Ozone a separate project from Hadoop.",Triaged pull-request-available,['build'],HDDS,Bug,Major,2020-05-28 12:19:11,0
13307486,OzoneManager start fails with RocksDB error on downgrade to older version.,"OM start fails with RocksDB error when downgrading to older version that does not have all the column families that may have been created  in the newer version.

{code}
java.io.IOException: Failed init RocksDB, db path : /tmp/ozone/data/metadata/om.db, exception :org.rocksdb.RocksDBE
xception You have to open all column families. Column families not opened: transactionInfoTable; status : InvalidAr
gument; message : You have to open all column families. Column families not opened: transactionInfoTable
        at org.apache.hadoop.hdds.utils.db.RDBStore.toIOException(RDBStore.java:159)
        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:141)
        at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:181)
        at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.start(OmMetadataManagerImpl.java:267)
        at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.<init>(OmMetadataManagerImpl.java:164)
        at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:478)
        at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:416)
        at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:884)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.start(OzoneManagerStarter.java:123)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOm(OzoneManagerStarter.java:78)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:66)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:37)
        at picocli.CommandLine.execute(CommandLine.java:1173)
        at picocli.CommandLine.access$800(CommandLine.java:141)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
        at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
        at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
        at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
        at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:75)
        at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:66)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:50)
Caused by: org.rocksdb.RocksDBException: You have to open all column families. Column families not opened: transact
ionInfoTable
        at org.rocksdb.RocksDB.open(Native Method)
        at org.rocksdb.RocksDB.open(RocksDB.java:290)
        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:97)
        ... 20 more
{code}

Thanks to [~bharat] for reporting this issue.",Triaged pull-request-available upgrade,['Ozone Manager'],HDDS,Bug,Critical,2020-05-26 17:07:01,5
13306668,Failed to delete chunk file due to chunk size mismatch,"LOGs

{noformat}
2020-05-19 13:45:30,493 [BlockDeletingService#8] WARN org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy: Chunk file doe not exist. chunk info :ChunkInfo{chunkName='104079328540607246_chunk_1, offset=0, len=4194304}
2020-05-19 13:45:30,493 [BlockDeletingService#8] ERROR org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}
2020-05-19 13:45:30,494 [BlockDeletingService#8] ERROR org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService: Failed to delete files for block #deleting#104079328540607246
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}
        at org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy.deleteChunks(FilePerChunkStrategy.java:286)
        at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDispatcher.deleteChunks(ChunkManagerDispatcher.java:111)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteBlock(KeyValueHandler.java:1043)
        at org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService$BlockDeletingTask.lambda$call$0(BlockDeletingService.java:286)
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}
{noformat}


chunk_1 is 4MB and chunk_2 is 1MB in block info.  
chunk_1 doesn't exit(might been deleted successfully)  and chunk_2 is 5MB on disk. ",Triaged pull-request-available,[],HDDS,Bug,Major,2020-05-22 04:09:50,0
13306566,Stop/Pause Background services while replacing OM DB with checkpoint from Leader,"When a follower OM needs to replace its DB with a checkpoint from Leader (to catch up on the transactions), it should pause or stop services which read/ write to the DB. 



During OM HA testing, found that OM could crash with JVM error on RocksDB. This happened because KeyDeletingService was trying to access a memory which is already freed up.
{code:java}
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f19de835af0, pid=1389, tid=1712
#
# JRE version: OpenJDK Runtime Environment (11.0.6+10) (build 11.0.6+10-LTS)
# Java VM: OpenJDK 64-Bit Server VM (11.0.6+10-LTS, mixed mode, sharing, tiered, compressed oops, concurrent mark sweep gc, linux-amd64)
# Problematic frame:
# C  [librocksdbjni10001996641283911793.so+0x1aeaf0]  Java_org_rocksdb_RocksIterator_seekToFirst0+0x0
#
# Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /opt/core.1389)
#
# An error report file with more information is saved as:
# /opt/hs_err_pid1389.log

{code}
From the hs_error log file:
{code:java}
---------------  T H R E A D  ---------------Current thread (0x00000000011a4000):  JavaThread ""KeyDeletingService#1"" daemon [_thread_in_native, id=1712, stack(0x00007f19d2443000,0x00007f19d2544000)]Stack: [0x00007f19d2443000,0x00007f19d2544000],  sp=0x00007f19d2541e78,  free space=1019k
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [librocksdbjni10001996641283911793.so+0x1aeaf0]  Java_org_rocksdb_RocksIterator_seekToFirst0+0x0
j  org.rocksdb.AbstractRocksIterator.seekToFirst()V+26
j  org.apache.hadoop.hdds.utils.db.RDBStoreIterator.<init>(Lorg/rocksdb/RocksIterator;)V+13
j  org.apache.hadoop.hdds.utils.db.RDBTable.iterator()Lorg/apache/hadoop/hdds/utils/db/TableIterator;+30
j  org.apache.hadoop.hdds.utils.db.TypedTable.iterator()Lorg/apache/hadoop/hdds/utils/db/TableIterator;+4
j  org.apache.hadoop.ozone.om.OmMetadataManagerImpl.getPendingDeletionKeys(I)Ljava/util/List;+8
j  org.apache.hadoop.ozone.om.KeyManagerImpl.getPendingDeletionKeys(I)Ljava/util/List;+5
j  org.apache.hadoop.ozone.om.KeyDeletingService$KeyDeletingTask.call()Lorg/apache/hadoop/hdds/utils/BackgroundTaskResult;+39
j  org.apache.hadoop.ozone.om.KeyDeletingService$KeyDeletingTask.call()Ljava/lang/Object;+1
J 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]
J 4802 c1 java.util.concurrent.Executors$RunnableAdapter.call()Ljava/lang/Object; java.base@11.0.6 (14 bytes) @ 0x00007f19f0c87214 [0x00007f19f0c870e0+0x0000000000000134]
J 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]
J 4802 c1 java.util.concurrent.Executors$RunnableAdapter.call()Ljava/lang/Object; java.base@11.0.6 (14 bytes) @ 0x00007f19f0c87214 [0x00007f19f0c870e0+0x0000000000000134]
J 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]
J 4954 c1 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run()V java.base@11.0.6 (57 bytes) @ 0x00007f19f0cfe10c [0x00007f19f0cfde40+0x00000000000002cc]

{code}",pull-request-available,['OM HA'],HDDS,Sub-task,Critical,2020-05-21 18:37:17,7
13306437,Archive jacoco coverage files for unit/integration tests,"Earlier we configured jacoco maven plugin to calculate the code coverage. But the jacoco.exec files (which contain the coverage information) are not stored in the artifacts created by the github actions.

We can add it easily.",pull-request-available,[],HDDS,Improvement,Major,2020-05-21 09:38:19,1
13306373,HddsDatanodeService cannot be started if HDFS datanode running in same machine with same user.,"since the service names are same and they both referring to same location for pid files, we can not start both services at once.

Tweak is to export HADOOP_PID_DIR to different location after starting one service and start other one.

It would be better to have different pid file names.

 

 
{noformat}
Umas-MacBook-Pro ozone-0.5.0-beta % bin/ozone --daemon start datanode
datanode is running as process 25167.  Stop it first.
{noformat}
 ",Triaged newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2020-05-21 06:09:49,1
13306173,Fix TestBlockOutputStreamWithFailures#test2DatanodesFailure,"[https://github.com/apache/hadoop-ozone/pull/767/checks?check_run_id=691722296] 
{code:java}
3083 at org.junit.Assert.fail(Assert.java:86) 


3084 at org.junit.Assert.assertTrue(Assert.java:41) 


3085 at org.junit.Assert.assertTrue(Assert.java:52) 


3086 at org.apache.hadoop.ozone.client.rpc.TestBlockOutputStreamWithFailures.test2DatanodesFailure(TestBlockOutputStreamWithFailures.java:335)

{code}",pull-request-available,[],HDDS,Sub-task,Major,2020-05-20 11:45:05,3
13306164,Remove FilteredClassloader and replace with maven based hadoop2/hadoop3 ozonefs generation,"As described in the parent issue, the final step is to create a Hadoop independent shaded client and hadoop2/hadoop3 related separated client jars.",pull-request-available,[],HDDS,Sub-task,Blocker,2020-05-20 11:10:22,1
13306022,Improve error message when GC parameters are not set,"Currently when GC parameters or any -XX are not set, it logs 

""No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS

It would be nice to improve this message with settings that are being set.",pull-request-available,[],HDDS,Bug,Major,2020-05-19 22:51:55,2
13306006,Implement getReadCopy in Table.,"Introduce a getReadCopy in table method.

As right now, get when a value exists in the cache it returns the cloned copy, so that when it used during double-buffer flush, if other threads modify the object during the flush time we will see some exceptions which are mentioned in HDDS-2344 and HDDS-2322. To avoid this, all the get() values returned are cloned copy if it exists in the cache.

But for a few of the requests like OMBucketCreateRequest, we need Volume info (OmVolumeArgs, but we don't use this info during double buffer flush, so we can safely get a cached copy without doing a clone.  In this Jira, fixed only Bucket requests. I Will file a new Jira to see where all this new API can be safely used.

",pull-request-available,[],HDDS,New Feature,Major,2020-05-19 20:55:15,2
13305925,Separate client/server/admin proto files of HDDS to separated subprojects ,"As described in the parent Jira admin/client/server protocols need different compatibility guarantees. It's better to separate them to separated maven project to make it easy to follow the changes.

I propose to create 3 new projects

hadoop-hdds/interface-client
hadoop-hdds/interface-server
hadoop-hdds/interface-admin

I called it -interface instead of -proto because we can include some basic Java classes not just proto files (for example utilities to serialize/deserialize proto files)

This first patch will include only the move without refactoring any RPC. While the new proto file names represent the proposed naming convention (Datanode/Scm + Server/Client/Admin + additional postfix) the name of the generated classes and the name of the RPC interfaces (Client/Server translator) not yet renamed to make the patch small.

Also: some methods should be moved between admin/server but it can be done in separated issue to make it easier to follow the change.",pull-request-available,[],HDDS,Sub-task,Major,2020-05-19 13:15:52,1
13305782,Call cleanup on tables only when double buffer has transactions related to tables.,"For volume/bucket table currently it is full cache, and we need to cleanup entries only when they are marked for delete.

So, it is unnecessary to call cleanup and waste the CPU resources on OM.

Similarly for other tables, when the double buffer has transaction entries that touch those tables, then only call cleanup.",Triaged pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-05-18 23:44:41,2
13305781,Remove S3Table from OmMetadataManager,"Remove the S3 table, after HDDS-3385, we don't have any use case for S3Table. We can remove this table from OmMetadataManager.",Triaged pull-request-available,[],HDDS,Bug,Major,2020-05-18 23:34:44,2
13305772,Fix JVMPause monitor start in OzoneManager,"Fix JVMPause monitor logic, right now it is started only in restart.

This should be started during OM start, and stopped during OM Stop. In restart() we can start this again.",pull-request-available,[],HDDS,Bug,Major,2020-05-18 22:06:29,2
13305747,Allow mounting bucket under other volume,"Step 2 from S3 [volume mapping design doc|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/design/ozone-volume-management.md#solving-the-mapping-problem-2-4-from-the-problem-listing]:

Implement a bind mount mechanic which makes it possible to mount any volume/buckets to the specific ""s3"" volume.",Triaged pull-request-available,['Ozone Manager'],HDDS,New Feature,Blocker,2020-05-18 19:28:24,0
13305671,Avoid to use Hadoop3.x IOUtils in Ozone Client ,To support Hadoop 2.x it's better to avoid to use Hadoop 3.x specific utility methods from IOUtils,pull-request-available,[],HDDS,Sub-task,Major,2020-05-18 12:58:44,1
13305064,Recon UI: Add visualization for file size distribution ,Recon has an API endpoint to get file size distribution in Ozone. Add visualization in Recon UI for this using histograms.,TriagePending,['Ozone Recon'],HDDS,Task,Major,2020-05-14 22:19:35,6
13304880,OM HA can be started with 3 isolated LEADER instead of one OM ring,"Steps to reproduce:

Imagine that I have 3 different om with the following DNS names:

{code}
ozone-om-0.ozone-om
ozone-om-1.ozone-om
ozone-om-2.ozone-om
{code}

I configured the three hosts as the following:

{code}
  OZONE-SITE.XML_ozone.om.nodes.omservice: om1,om2,om3
  OZONE-SITE.XML_ozone.om.address.omservice.om1: ozone-om-0
  OZONE-SITE.XML_ozone.om.address.omservice.om2: ozone-om-1
  OZONE-SITE.XML_ozone.om.address.omservice.om3: ozone-om-2
  OZONE-SITE.XML_ozone.om.ratis.enable: ""true""
{code}

But unfortunately the DNS is not reliable. All the hosts can resolve only the LOCAL hostname.

OMHANodeDetails.java ignores ALL the configuration which are not resolvable:

{code}
 if (!addr.isUnresolved()) {
          if (!isPeer && OmUtils.isAddressLocal(addr)) {
            localRpcAddress = addr;
            localOMServiceId = serviceId;
            localOMNodeId = nodeId;
            localRatisPort = ratisPort;
            found++;
          } else {
            // This OMNode belongs to same OM service as the current OMNode.
            // Add it to peerNodes list.
            // This OMNode belongs to same OM service as the current OMNode.
            // Add it to peerNodes list.
            peerNodesList.add(getHAOMNodeDetails(conf, serviceId,
                nodeId, addr, ratisPort));
          }
        }
{code}

As a result I will have 3 running server but each has 1 one-node Ratis ring (peerNodesList is empty as only the local hostname can be resolved).

Group ID is the same for all. But they have separated database and they work as separated OM which is VERY dangerous.

 1. Option one: we can accept any unresolved address and retry with connection create if it couldn't be connected

2. Option two: at least the error handling should be fixed. When I configured 3 om, there supposed to be 3 om.",Triaged pull-request-available,[],HDDS,Improvement,Critical,2020-05-14 08:46:53,7
13304224,Recon: Display leader count in Datanodes page,"Datanodes page should have ""Leader Count"" column that displays the leader count for how many Ratis pipelines the given datanode is elected as a leader.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-05-11 22:28:21,6
13304193,Add Recon UI lint checks and unit tests to Github Actions CI workflow,"Recon UI unit tests and linter checks should be added to Github actions CI. 

Optimization: Try to add a workflow in such a way that these checks are run only if there is a change related to Recon UI files. ",TriagePending,['Ozone Recon'],HDDS,Task,Major,2020-05-11 19:38:35,6
13303461,Recon UI: Add strict linter rules to improve code quality,Add a linter like ([xojs|https://github.com/xojs/xo]) and fix all linter errors and warnings.,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-05-07 18:12:37,6
13303379,Provide generic introduction / deep-dive slides as part of the documentation,"I think it's good to have a generic overview  of Ozonein presentation format and shared with the community. It can be used anytime by anybody to share details of Ozone on any meetup / conferences.

I am not sure what is the best place for this but documentation seems to be a natural choice:

 * This is version dependent (can be updated with new releases)
 * Topics and diagrams can be shared between documentations",pull-request-available,['documentation'],HDDS,Improvement,Major,2020-05-07 12:18:10,1
13303128,OzoneFS is slow compared to HDFS using Spark job,"Reported by ""Andrey Mindrin"" on the the-asf Slack:

{quote}
We have made a few tests to compare OZONE (0.4 and 0.5 on Cloudera Runtime 7.0.3 with 3 nodes) performance with HDFS and OZONE is slower in most cases. For example, Spark application with 18 containers that copies 6 Gb parquet file is about 50% slower on OzoneFS. The only one case shows the same performance - Hive queries over partitioned tables.

 simple spark code we used:

{code}
val file = spark.read.format(format).load(path_input)
file.write.format(format).save(path_output)
{code}

Tested on CSV file with 800 million records, 2 columns and parquet file converted from CSV above. Just copied file from HDFS to HDFS and from Ozone to Ozone. Application time is 1m 14s on HDFS and  1m 51s (+50%) on Ozone (parquet file). Ozone has default settings. (edited) 
{quote}",performance,[],HDDS,Improvement,Major,2020-05-06 14:31:45,1
13303106,Fix TestReadRetries,"{code:java}
-----------------------------------------------------------------Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 55.385 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestReadRetriestestPutKeyAndGetKeyThreeNodes(org.apache.hadoop.ozone.client.rpc.TestReadRetries)  Time elapsed: 10.086 s  <<< FAILURE!java.lang.AssertionError: expected same:<OPEN> was not:<CLOSING> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotSame(Assert.java:737) at org.junit.Assert.assertSame(Assert.java:680) at org.junit.Assert.assertSame(Assert.java:691) at org.apache.hadoop.ozone.client.rpc.TestReadRetries.testPutKeyAndGetKeyThreeNodes(TestReadRetries.java:181)
{code}",pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2020-05-06 12:57:02,3
13302149,Ensure consistent OM token service field in HA environment,"Currently OMFailoverProxyProvider#computeDelegationTokenService calculate the canonical token service name based on the enumeration order of the configured OM instances. An example service field can be like TS1: ""om1addr:port,om2addr:port,om3addr:port""

This could be problematic
1) clients have different omId to omRpcAddresses mappings
2) configuration enumeration orders are different among clients

Depend on the client configuration and enumeration order, the client may got its canonnical token service in different order like TS2: ""om2addr:port,om1addr:port,om3:addr:port""

MR/Yarn/Spark on Yarn relies on token service as key to check the UGI credential when building token cache map. When client got TS2 even though it has an OM token with TS1, client will try to collect OM token again. This will not work in YARN container (e.g., Spark on Yarn cluster mode) which may not have the kerberos ticket to fetch the token.

The proposed fix it to provide a consistent canonical token service for all OM clients in order.

 
",pull-request-available,['Security'],HDDS,Bug,Major,2020-04-30 16:37:55,4
13301792,Upgrade Ratis to 0.6.0-2816ea6-SNAPSHOT,Upgrade Ratis to {{0.6.0-2816ea6-SNAPSHOT}} to get the fix for RATIS-840.,pull-request-available,[],HDDS,Task,Critical,2020-04-29 13:02:44,0
13301707,Disable Jaeger tracing by default.,"Jaeger tracing is enabled by default, lets disable it.",pull-request-available,[],HDDS,Bug,Major,2020-04-29 05:15:43,3
13301548,Remove dependence on commons-lang,"Few parts of Ozone still use {{commons-lang}}, while most are already on {{commons-lang3}}.  Let's update those remaining usages and remove the unnecessary dependency.",pull-request-available,[],HDDS,Improvement,Minor,2020-04-28 12:07:51,0
13301545,OzoneFileStatus should not extend FileStatus,"FileStatus is a Hadoop specific class. The return type of getFileStatus OM call should be Hadoop independent and a simple POJO can be used.

OzoneFileSystem can create the appropriate FileStatus implementation based on the information in this simple pojo.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-04-28 11:50:43,1
13301543,Hide OMFailoverProxyProvider usage behind an interface,"OzoneManagerProtocolClientSideTranslatorPB uses OmFailoverProxyProvider to access OM HA, but this class is not supported in Hadoop 2.x environment.

It would be better to 
 1. separated ProtocolClientSideTranslator from the transport layer logic
 2. Remove implementation specific method from the OzoneManagerProtocol (getOMFailoverProxyProvider should be removed)
 3. Use a simple OMTransport interface to handle all the connection logic in one, isolated place",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-04-28 11:49:09,1
13301487,Address compatibility issue by SCM DB instances change,"After https://issues.apache.org/jira/browse/HDDS-3172, SCM now has one single rocksdb instance instead of multiple db instances. 

For running Ozone cluster, we need to address compatibility issues. One possible way is to have a side-way tool to migrate old metadata from multiple dbs to current single db.",Triaged,['SCM'],HDDS,Bug,Blocker,2020-04-28 08:39:19,1
13300907,Ozone start fails with NullPointerException in TLS enabled cluster,"{code}
	
SCM start failed with exception
java.lang.NullPointerException
	at org.apache.hadoop.hdds.conf.ConfigurationSource.getPassword(ConfigurationSource.java:112)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.getPassword(BaseHttpServer.java:348)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.loadSslConfToHttpServerBuilder(BaseHttpServer.java:311)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.newHttpServer2BuilderForOzone(BaseHttpServer.java:179)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.<init>(BaseHttpServer.java:95)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerHttpServer.<init>(StorageContainerManagerHttpServer.java:33)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:334)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:215)
{code}",pull-request-available,[],HDDS,Bug,Critical,2020-04-24 23:36:02,5
13300468,TestSCMNodeMetrics is flaky,"It failed without any reason during a a master build

[https://github.com/elek/ozone-build-results/blob/master/2020/04/22/808/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestSCMNodeMetrics.txt]

Checking it closely, I can't see any reason to have a MiniOzoneCluster there as it can be a simple unit test. The easiest fix seems to be a conversion to real unit test.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-04-23 07:27:46,1
13300115,Use persisted transaction info during OM startup in OM StateMachine,HDDS-3475 persisted transaction info into DB. This Jira is to use transactionInfo persisted to DB during OM startup. ,Triaged pull-request-available,['OM HA'],HDDS,Sub-task,Critical,2020-04-21 23:34:54,2
13300063,Use transactionInfo table to persist transaction information,"This Jira is to flush the transaction information of the term and last transaction index applied during double buffer flush to OM DB.
",pull-request-available,[],HDDS,Sub-task,Major,2020-04-21 19:04:02,2
13300046,Create transactionInfo Table in OmMetadataManager,"This Jira is to create a transaction info table which stores the current term and last transaction index applied to DB.
*In this Jira following will be done:*
1. introduce a new transaction info table which stores transactionInfo.
Key = TRANSACTIONINFO
value = currentTerm-transactionIndex
2. Add new UT's for this table.
3. Provide utility/helper methods to parse the transaction info table value",pull-request-available,[],HDDS,Sub-task,Major,2020-04-21 18:00:55,2
13299973,Update to latest Ratis Snapshot 0.6.0-490b689-SNAPSHOT,Update ozone to latest ratis snapshot.,pull-request-available,['build'],HDDS,Bug,Major,2020-04-21 11:44:03,3
13299906,Add third party jar versions as properties in pom.xml,"Currently, many third library dependencies are hardcoded in the dependency tag in the pom file. Idea is to re-organize the structure a bit by defining the jar version as a property in the pom file and reuse the defined version in the dependency tag

as done in https://issues.apache.org/jira/browse/HDDS-3468",pull-request-available,[],HDDS,Bug,Major,2020-04-21 08:03:55,0
13299898,Organize log4j dependency in pom.xml,"Currently, dependency of log4j in ozone is added as following:
{code:java}
<dependency>
  <groupId>log4j</groupId>
  <artifactId>log4j</artifactId>
  <version>1.2.17</version>
{code}
Idea here is to add log4j.version as a property in pom.xml and reuse the same while defining the dependency.",pull-request-available,['build'],HDDS,Bug,Major,2020-04-21 07:39:24,3
13299774,Use dedicated build partition for acceptance tests in github actions environment,Even with HDDS-3456 it seems to be a good idea to run acceptance test from /mnt in the github actions environment where we have dedicated 14GB space.,pull-request-available,[],HDDS,Improvement,Major,2020-04-20 18:16:17,1
13299687,Add acceptance test to smoketest CSI service startup,"Ozone CSI service implement Container Storage Interface specification to provide volumes for container orchestrators such as Yarn and Kubernetes.

We don't have any acceptance test which makes easy to break the classpath / functionality.

As a first step I would create a simple smoketest to check if the csi service can be started and the identitiy service can be called.",pull-request-available,[],HDDS,Improvement,Major,2020-04-20 12:56:41,1
13299599,Support Hadoop 2.x with build-time classpath separation instead of isolated classloader,"Apache Hadoop Ozone is a Hadoop subproject. It depends on the released Hadoop 3.2. But as Hadoop 3.2 is very rare in production, older versions should be supported to make it possible to work together with Spark, Hive, HBase and older clusters.

Our current approach is using classloader based separation (ozonefs ""legacy"" jar), which has multiple problems:

 1. It's quite complex and hard to debug
 2. It couldn't work together with security

The issue proposes to use a different approach
 1. Reduce the dependency on Hadoop (including the replacement of hadoop metrics and cleanup of the usage of configuration)
 2. Create multiple version from ozonefs-client with different compile time dependency. ",Triaged,['build'],HDDS,Improvement,Blocker,2020-04-20 07:47:20,1
13299239,Use UrlConnectionFactory to handle HTTP Client SPNEGO for ozone components,"Some of the places that need to be fixed, otherwise those http client won't be able to access the endpoints when SPNEGO is enabled on the server side. 

ReconUtils#makeHttpCall
OzoneManagerSnapshotProvider#getOzoneManagerDBSnapshot

The right API to use should be URLConnectionFactory
public URLConnection openConnection(URL url, boolean isSpnego)

The isSpnego should be based on OzoneSecurityUtil.isHttpSecurityEnabled()",pull-request-available,[],HDDS,Bug,Major,2020-04-17 22:09:58,4
13299075,Enable TestWatchForCommit test cases,Fix and enable TestWatchForCommit test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:15:01,3
13298990,Ozone documentation to be revised for OM HA Support,"As OM HA is now supported in current version , We might need to update all documentation pages wherever service id is applicable and any new parameters that we might need to configure in core-site.xml for service id access for remote clusters etc. And all volume/bucket/key CLI syntaxes including service id for OM HA enabled clusters should be included in documentation",TriagePending,['documentation'],HDDS,Bug,Major,2020-04-17 07:03:55,1
13298940,Switch Recon SQL DB to Derby.,"Recon currently uses Sqlite as its defacto SQL DB with an option to configure other JDBC compatible databases. However, on some platforms like the IBM power pc, this causes problems from compile time since it does not have the sqlite native driver. This task aims to change the default SQL DB used by Recon to Derby, but retains the out of the box support (no need to supply the driver) for Sqlite as well. ",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-04-17 03:09:59,5
13298880,Remove RetryInvocation INFO logging from ozone CLI output,"In OM HA failover proxy provider, RetryInvocationHandler logs error stack trace when client tries contacting non-leader OM. Instead we can just log a message that the failover will happen and not include the stack trace.
{code:java}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:174) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:110) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682), while invoking $Proxy16.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
{code}",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Improvement,Major,2020-04-16 18:06:42,7
13298741,Generate ozone specific version from type in FSProto.proto,"FSProtos.proto is copied from the Hadoop and used during the proto file generation **BUT** the types defined in FSProtos.proto are generated by Hadoop subproject.

This makes it hard to use Ozone with older Hadoop versions as the types of FSProtos are available only from Hadoop 3.x. 

An easy fix is to generate our own version based on the existing FSProtos.proto",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2020-04-16 08:57:35,1
13298704,Ozone audit entries could be consistent among volume creation with quota and update quota,"2020-04-14 09:44:55,089 | INFO  | OMAudit | user=root | ip=172.25.40.156 | op=CREATE_VOLUME

{admin=root, owner=hdfs, volume=hive2, creationTime=1586857495055, *quotaInBytes=1099511627776*, objectID=1792, updateID=7}

| ret=SUCCESS |

2020-04-14 09:58:09,634 | INFO  | OMAudit | user=root | ip=172.25.40.156 | op=SET_QUOTA

{volume=hive, *quota=536870912000*}

| ret=SUCCESS |

 

OMVolumeSetQuotaRequest.java -> auditMap.put(OzoneConsts.QUOTA,     String.valueOf(setVolumePropertyRequest.getQuotaInBytes()));

 

We can use OzoneConsts.QUOTA_IN_BYTES instead of OzoneConsts.QUOTA",pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-04-16 05:34:23,2
13298662,Extract test utilities to separate module,"TimedOutTestsListener cannot be added globally because it is in hadoop-hdds-common, which is not accessible in hadoop-hdds-config (since the latter is a dependency of the former).  The listener and related classes (GenericTestUtils, etc.) should be extracted into a separate module to be used by all others.",pull-request-available,"['build', 'test']",HDDS,Task,Major,2020-04-15 21:29:47,0
13298612,Update JaegerTracing,We currently use JaegerTracing 0.34.0. The latest is 1.2.0. We are several versions behind and should update. Note this update requires the latest version fo OpenTracing and has several breaking changes.,pull-request-available,[],HDDS,Task,Blocker,2020-04-15 16:34:37,1
13298601,Make jmh jar dependencies optional,"jmh dependencies used by `ozone genesis` are licensed by GPL + classpath exception

It's better to make it optional and download it on demand (and exclude it from the release package).",pull-request-available,[],HDDS,Improvement,Major,2020-04-15 15:31:20,1
13298461,Skip generation of encryptionkey for directory create operation,While reading code observed that we are generating encryption info for create directory operation. This jira is to skip the generation of encryption info.,pull-request-available,[],HDDS,Bug,Major,2020-04-15 05:04:28,2
13298435,Recon throws NPE in clusterState endpoint,"Recon throws NPE while trying to get number of volumes from omMetadataManager instance (omMetadataManager.getVolumeTable().getEstimatedKeyCount()).

This happens before Recon gets its first snapshot db from OM, because only after first snapshot db, omMetadataManager initializes its tables.
{code:java}
Unable to get Volumes count in ClusterStateResponse.
java.lang.NullPointerException
	at org.apache.hadoop.ozone.recon.api.ClusterStateEndpoint.getClusterState(ClusterStateEndpoint.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
	at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)
	at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)
	at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)
	at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1615)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:500)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
	at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-04-15 02:44:51,6
13298366,OM create key/file should not generate different data encryption key during validateAndUpdateCache,"The problem with generate different Data encryption key for the same file across different OM instances are that when the OM leader changes, the client may not be able to read the data correctly. ",pull-request-available,[],HDDS,Bug,Blocker,2020-04-14 18:28:55,2
13298364,Delegate admin ACL checks to Ozone authorizer plugin,"Currently, the admin operation check are not sent to authorizer plugin. As a result, the audit log are not shown up in plug-in like ranger authorizer. ",pull-request-available,[],HDDS,Improvement,Major,2020-04-14 18:20:24,4
13298346,Add bucket encryption key info to bucket create audit log,The current audit log does not include the bucket encryption key information. This ticket is opened to add that. ,Triaged pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-04-14 16:56:48,4
13298260,Simplify S3 -> Ozone volume mapping,"See the design doc for more details: 

https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/design/ozone-volume-management.md",backward-incompatible imcompatible pull-request-available,[],HDDS,Improvement,Critical,2020-04-14 12:34:34,0
13297884,OzoneManager starts 2 OzoneManagerDoubleBuffer for HA clusters,"OzoneManager starts 2 OzoneManagerDoubleBuffer for HA clusters. In the following example for 3 OM HA instances, 6 OzoneManagerDoubleBuffer instances were created.
{code}
➜  chaos-2020-04-12-20-21-11-IST grep canFlush stack1
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
{code}",MiniOzoneChaosCluster Triaged pull-request-available,"['Ozone Manager', 'test']",HDDS,Bug,Major,2020-04-12 15:02:24,2
13297772,OzoneManager group init failed because of incorrect snapshot directory location,"OzoneManager group init failed because of incorrect snapshot directory location

{code}
2020-04-11 20:07:57,180 [pool-59-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(44)) - raft.server.storage.dir = [/tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis] (custom)
2020-04-11 20:07:57,180 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(191)) - omNode-3: found a subdirectory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/snapshot
2020-04-11 20:07:57,181 [pool-59-thread-1] WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(197)) - omNode-3: Failed to initialize the group directory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/snapshot.  Ignoring it
java.lang.IllegalArgumentException: Invalid UUID string: snapshot
        at java.util.UUID.fromString(UUID.java:194)
        at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$0(RaftServerProxy.java:192)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at org.apache.ratis.server.impl.RaftServerProxy.lambda$initGroups$1(RaftServerProxy.java:189)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
        at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)
        at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
        at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
        at org.apache.ratis.server.impl.RaftServerProxy.initGroups(RaftServerProxy.java:186)
        at org.apache.ratis.server.impl.ServerImplUtils.newRaftServer(ServerImplUtils.java:41)
        at org.apache.ratis.server.RaftServer$Builder.build(RaftServer.java:76)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.<init>(OzoneManagerRatisServer.java:277)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.newOMRatisServer(OzoneManagerRatisServer.java:328)
        at org.apache.hadoop.ozone.om.OzoneManager.initializeRatisServer(OzoneManager.java:1249)
        at org.apache.hadoop.ozone.om.OzoneManager.restart(OzoneManager.java:1190)
        at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl.restartOzoneManager(MiniOzoneHAClusterImpl.java:229)
        at org.apache.hadoop.ozone.failure.Failures$OzoneManagerRestartFailure.lambda$fail$0(Failures.java:112)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
        at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)
        at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
        at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
        at org.apache.hadoop.ozone.failure.Failures$OzoneManagerRestartFailure.fail(Failures.java:109)
        at org.apache.hadoop.ozone.failure.FailureManager.fail(FailureManager.java:58)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-04-11 20:07:57,182 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(191)) - omNode-3: found a subdirectory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/b870c9eb-edfb-36b5-b758-d62218d261de
2020-04-11 20:07:57,183 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - omNode-3: addNew group-D62218D261DE:[omNode-3:localhost:12408, omNode-1:localhost:12396, omNode-2:localhost:12402] returns group-D62218D261DE:java.util.concurrent.CompletableFuture@2fc3d657[Not completed]
2020-04-11 20:07:57,183 [pool-1382-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(97)) - omNode-3: new RaftServerImpl for group-D62218D261DE:[omNode-3:localhost:12408, omNode-1:localhost:12396, omNode-2:localhost:12402] with OzoneManagerStateMachine:uninitialized
{code}",MiniOzoneChaosCluster pull-request-available,"['Ozone Manager', 'test']",HDDS,Bug,Major,2020-04-11 14:48:55,2
13297648,S3A failing complete multipart upload with Ozone S3,"
{code:java}
javax.xml.bind.UnmarshalException: unexpected element (uri:"""", local:""CompleteMultipartUpload""). Expected elements are <{http://s3.amazonaws.com/doc/2006-03-01/}CompleteMultipartUpload>,<{http://s3.amazonaws.com/doc/2006-03-01/}Part>
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleEvent(UnmarshallingContext.java:744)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:262)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:257)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportUnexpectedChildElement(Loader.java:124)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext$DefaultRootLoader.childElement(UnmarshallingContext.java:1149)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:574)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:556)
        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:168)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:509)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:374)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl$NSContentDriver.scanRootElementHook(XMLNSDocumentScannerImpl.java:613)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3132)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:852)

{code}


It seems http://s3.amazonaws.com/doc/2006-03-01/ is expected in the element.
But in class CompleteMultipartUploadRequest,  namespace http://s3.amazonaws.com/doc/2006-03-01/ is not defined here.

Reported by [~sammichen]





",pull-request-available,[],HDDS,Bug,Major,2020-04-10 17:04:28,2
13297558,Intermittent failure in TestDnRatisLogParser and TestOMRatisLogParser,"{code:title=https://github.com/apache/hadoop-ozone/pull/783/checks?check_run_id=576054872}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 18.987 s <<< FAILURE! - in org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser
[ERROR] testRatisLogParsing(org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser)  Time elapsed: 18.882 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser.testRatisLogParsing(TestDnRatisLogParser.java:75)
{code}

{code:title=https://github.com/apache/hadoop-ozone/pull/846/checks?check_run_id=608153672}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.606 s <<< FAILURE! - in org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser
[ERROR] testRatisLogParsing(org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser)  Time elapsed: 30.476 s  <<< FAILURE!
java.lang.AssertionError
  at org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser.testRatisLogParsing(TestOMRatisLogParser.java:112)
{code}

CC [~msingh]",pull-request-available,['test'],HDDS,Bug,Major,2020-04-10 07:40:40,0
13297513,Ozone filesystem jar should not include webapps folder,"hadoop-ozone-filesystem-lib-current jar includes webapps folder of hdds datanode. 

This should not be included in the filesystem jar.",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-04-10 00:15:12,6
13297252,Ensure OzoneConfiguration is initialized in OzoneClientFactory#getOzoneClient,"HDDS-3319 added a new method to get RPCClient based on the omserviceID in OzoneToken. However, some om.service.id compare logic is based on a Hadoop Configuration object. As a result, the om configuration may not be loaded if the Configuration is a Hadoop Configuration only (e.g., RM). ",pull-request-available,[],HDDS,Bug,Major,2020-04-09 03:17:12,4
13297219,Increase test timeout for ozonesecure-security robot tests,"In few CI runs, ozone-security robot tests are timing out at 5 minutes. 
[https://github.com/apache/hadoop-ozone/pull/784/checks?check_run_id=569548444]

We should increase the timeout to avoid this.",pull-request-available,[],HDDS,Bug,Major,2020-04-08 21:03:41,7
13297212,Intermittent failure in testContainerImportExport,"{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/571992849}
2020-04-08T20:30:49.0510599Z [ERROR] Tests run: 22, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.669 s <<< FAILURE! - in org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer
2020-04-08T20:30:49.0535678Z [ERROR] testContainerImportExport[1](org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.079 s  <<< ERROR!
2020-04-08T20:30:49.0552584Z java.io.IOException: request to write '4096' bytes exceeds size in header of '19906' bytes for entry 'db/LOG'
2020-04-08T20:30:49.0572746Z 	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.write(TarArchiveOutputStream.java:385)
2020-04-08T20:30:49.0572897Z 	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2147)
2020-04-08T20:30:49.0582579Z 	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)
2020-04-08T20:30:49.0593659Z 	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)
2020-04-08T20:30:49.0603340Z 	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)
2020-04-08T20:30:49.0613502Z 	at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includeFile(TarContainerPacker.java:225)
2020-04-08T20:30:49.0631425Z 	at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includePath(TarContainerPacker.java:215)
2020-04-08T20:30:49.0637525Z 	at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.pack(TarContainerPacker.java:155)
2020-04-08T20:30:49.0648504Z 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:549)
2020-04-08T20:30:49.0659852Z 	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testContainerImportExport(TestKeyValueContainer.java:233)
{code}",pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-08 20:40:37,0
13297060,Intermittent test failure related to a race conditon during PipelineManager close,"The test which is failed:

TestSCMNodeManager

The end of the log is:

{code}
2020-04-08 10:49:44,544 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(84)) - Error on execution message 19844615-0d70-4172-8c34-96e5b7295ef2{ip: 196.189.243.187, host: localhost-196.189.243.187, networkLocation: /default-rack, certSerialId: null}
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.finalizeAndDestroyPipeline(SCMPipelineManager.java:380)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:63)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-04-08 10:49:44,544 INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=fd1f9e92-2f90-43e7-8406-94ba6ac356b0, PipelineID=8d380e3c-b632-4bda-aa7a-554774fba09d]
2020-04-08 10:49:44,544 INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(373)) - Destroying pipeline:Pipeline[ Id: fd1f9e92-2f90-43e7-8406-94ba6ac356b0, Nodes: 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-08T10:49:37.441Z]
2020-04-08 10:49:44,544 INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(120)) - Pipeline Pipeline[ Id: fd1f9e92-2f90-43e7-8406-94ba6ac356b0, Nodes: 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:null, CreationTimestamp2020-04-08T10:49:37.441Z] moved to CLOSED state
2020-04-08 10:49:44,544 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(84)) - Error on execution message 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.finalizeAndDestroyPipeline(SCMPipelineManager.java:380)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:63)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-04-08 10:49:44,544 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(208)) - Send pipeline:PipelineID=e0e155c6-9fbe-46a7-b742-e805ea9baacf close command to datanode 30a24b04-1289-4c30-a28a-034edfe29e3d
2020-04-08 10:49:44,545 WARN  events.EventQueue (EventQueue.java:fireEvent(151)) - Processing of TypedEvent{payloadType=CommandForDatanode, name='Datanode_Command'} is skipped, EventQueue is not running
2020-04-08 10:49:44,544 INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 59bdd26b-05da-47d1-8c3f-8350d55d7299{ip: 248.147.58.17, host: localhost-248.147.58.17, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=17b032b7-b9c4-41eb-bba6-50106881886d, PipelineID=60de1ca6-4115-415b-bbf1-06b86113df94]
2020-04-08 10:49:44,576 WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2020-04-08 10:49:44,579 WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2020-04-08 10:49:44,579 WARN  db.DBDefinition (DBDefinition.java:createDBStoreBuilder(63)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
{code}",TriagePending flaky-test ozone-flaky-test,['test'],HDDS,Sub-task,Major,2020-04-08 12:33:36,1
13297051,Add check for import from shaded package,Add a checkstyle rule to reject import from shaded packages.  This would help avoid accidentally using shaded transitive dependencies.,pull-request-available,['build'],HDDS,Improvement,Major,2020-04-08 11:49:18,0
13296898,Add wait time between client retries to OM,"Currently, client keeps retrying to connect to leader OM in a tight loop and fails after configured number of retires/ failovers.
If the Leader OM is not ready, the client can timeout quickly. So, we should instead try each OM once, and then wait before retrying again. ",pull-request-available,[],HDDS,Sub-task,Major,2020-04-07 19:19:59,7
13296879,OM HA replay optimization,"This Jira is to improve the OM HA replay scenario.
Attached the design document which discusses about the proposal and issue in detail.
",Triaged,"['OM HA', 'Ozone Manager']",HDDS,Improvement,Major,2020-04-07 18:03:53,2
13296835,Remove unnecessary transitive hadoop-common dependencies on server side.,Similar to HDDS-3312 we can exclude dependencies coming from hadoop-ozone. (Eg. curator / zookeeper.),pull-request-available,[],HDDS,Improvement,Major,2020-04-07 14:05:26,1
13296109,Ozone admin shell commands do not print or log exceptions on failures,Ozone shell commands such as `ozone admin container create` does not print any information on failure which makes it very difficult to debug the underlying issue.,TriagePending,['Ozone CLI'],HDDS,Bug,Major,2020-04-03 20:05:41,6
13296055,Checkstyle fails for new modules/versions,{{checkstyle.sh}} fails for any new intermediate module (used by other modules) or new version (eg. bump from 0.5.0-SNAPSHOT to 0.6.0-SNAPSHOT).  It does not perform a complete build and tries to fetch these artifacts from Maven repo.,pull-request-available,['build'],HDDS,Bug,Major,2020-04-03 15:44:18,0
13296046,Move Ozone Shell from ozone-manager to tools,"Ozone Shell is currently part of the {{ozone-manager}} module.  I think it would be more at home in the {{tools}} module.

Also rename the package name {{ozShell}} to {{shell}}, as package names should be all lowercase.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-04-03 15:03:56,0
13296028,Rename CONTRIBUTION.md to CONTRIBUTING.md,"If we follow the naming convention suggested by github:

https://help.github.com/en/github/building-a-strong-community/setting-guidelines-for-repository-contributors

The contribution guideline will be displayed at multiple locations (eg. when a new contributor create a new PR).",pull-request-available,[],HDDS,Improvement,Trivial,2020-04-03 13:25:31,1
13295956,Metrics for Recon OzoneManager DB sync.,"Some useful metrics for tracking how Recon's OM DB requests are going on.

{code}
  @Metric(about = ""Number of OM snapshot requests made by Recon."")
  @Metric(about = ""Number of OM snapshot requests that failed."")
  @Metric(about = ""OM snapshot request latency"")
  @Metric(about = ""Number of OM delta requests made by Recon."")
  @Metric(about = ""Number of OM delta requests that failed."")
  @Metric(about = ""OM delta request latency"")
  @Metric(about = ""Total number of updates got through OM delta request"")
  @Metric(about = ""Average number of updates got per OM delta request"")
{code}",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-04-03 06:56:50,5
13295920,Recon unit tests cleanup.,"The Guice bindings required for some of the unit tests are hard to follow and need some cleanup.


*Work done*

* Created class to setup a recon test injector, with any combination of sub modules that are specified.
* Created class that provides a Recon SQL DB with all the tables created, and APIs to access the DAOs easily.
* Cleaned up injector usage in Recon tests.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-04-03 00:51:27,5
13295868,OM Client failover to next OM on NotLeaderException,"On receiving NotLeaderException from OM server, we should failover to the next OM in the list instead of the suggested Leader. This will ensure that all the OMs are contacted in a round robin way. Failing always to the suggested leader is not robust.",pull-request-available,[],HDDS,Sub-task,Major,2020-04-02 18:18:13,7
13295830,Recon UI: All the pages should auto reload,Overview page should auto refresh to fetch updated data and alerts if any.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-04-02 16:10:10,6
13295792,Simplify s3bucket -> ozone volume/bucket mapping,"The current s3bucket -> ozone volume/bucket mapping is very confusing. Let's improve it.

(See the design docs for more details).",pull-request-available,[],HDDS,Improvement,Major,2020-04-02 13:07:19,1
13295789,TestDeleteWithSlowFollower is still flaky,"{code}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 666.209 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 640.745 s  <<< ERROR!
java.io.IOException: INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:229)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:402)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:347)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:458)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:509)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
        at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)
{code}

I learned this from [~shashikant]

bq. we kill a datanode after some IO, SCM is out of safe mode by then . SCM takes time to destroy a pipeline and form a new one
bq. With only minimal set of dn in cluster, if we want to write again, we need to wait for a new pipeline to open up before writing again

Will turn off this test until the fix.
",TriagePending ozone-flaky-test pull-request-available,['test'],HDDS,Improvement,Major,2020-04-02 12:36:32,3
13295767,List design docs as part of the documentation page,"In HDDS-1659 a new generic way was introduced  to make the design docs more public (collect them as part of the documentation whether they are written as part of the docs or as separated pdf/google docs).

This patch:
 
 * List the design doc as part of the documentation
 * Adds link to some of the existing / earlier design docs

I believe that this can be a good extension to the documentation page as some of the low level internals are discussed only in design docs.",pull-request-available,['documentation'],HDDS,Improvement,Major,2020-04-02 11:17:07,1
13295698,Handle Resource Unavailable exception in OM HA,"Right now, when the future fails with an exception, we send that exception to the client, and retry with a new server. but when using ratis server when resource unavailable exception future fails with exceptionally. So, in this case we need to wrap the exception and retry to the same server with some retry policy like MultiLinearRandomRetry or some retry policy.

{code:java}
try {
 raftClientReply = server.submitClientRequestAsync(raftClientRequest)
          .get();
    } catch (Exception ex) {
      throw new ServiceException(ex.getMessage(), ex);
    }
{code}

",TriagePending,"['OM HA', 'Ozone Manager']",HDDS,Bug,Major,2020-04-02 06:22:29,2
13295660,OM Client fails with StringIndexOutOfBoundsException,"OM Client fails with StringIndexOutOfBoundsException: 

 
{code:java}
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: Caused by: java.io.IOException: Couldn't create RpcClient protocol
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:199)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:175)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:86)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:168)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:158)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3391)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:158)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3451)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3419)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:513)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:558)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	... 26 more
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at java.base/java.lang.String.substring(String.java:1841)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.computeDelegationTokenService(OMFailoverProxyProvider.java:207)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:84)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:208)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:192)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	... 39 more

 {code}
 

This can happen for two reasons:
 # If user has configured OM addresses incorrectly which are not resolvable to any host.
 # In Docker world when the client starts if all OM containers are down, then OM addresses will be unresolvable. ",pull-request-available,[],HDDS,Bug,Major,2020-04-02 00:01:08,2
13295610,StandAlone Pipelines are created in an infinite loop,"_BackgroundPipelineCreator_ keeps creating pipelines of configured Replication type and all available Replication factors until some exception occurs while creating the pipeline such as no more available nodes.

When Replication Type is set to STAND_ALONE, we do not check if a DN has already been used to create a pipeline of same factor or not and keep reusing the same DNs to create new pipelines. This causes the pipeline creation to happen in an infinite loop.",pull-request-available,[],HDDS,Bug,Critical,2020-04-01 19:23:39,7
13295584,Prometheus endpoint should have an option to be configured with Token based authentication. ,"Prometheus does not support targets which have Kerberos SPNEGO based authentication. Hence, on a secure cluster, if we have prometheus endpoint enabled, it makes sense to skip the authentication filter for it.",pull-request-available,[],HDDS,Bug,Major,2020-04-01 17:03:51,5
13295558,Handle HA for BasicOzoneClientAdapterImpl$Renewer#renew/cancel(),"Currently, only the getDelegationtoken has been updated with the proper HA client proxy setup. renew and cancel will fail in OM HA setup.

",pull-request-available,[],HDDS,Bug,Major,2020-04-01 15:23:07,4
13295554,Support /close command in the Github comments,"As it's discussed during the last community meeting we can close pending pull requests after 30 days. /close command can make it easier.

We can also add pending label to the pending pull requests.",pull-request-available,[],HDDS,Improvement,Major,2020-04-01 15:00:56,1
13295517,Checkstyle check fails silently in case of mvn related errors,"From the last master build:

https://github.com/apache/hadoop-ozone/runs/550746880?check_suite_focus=true

Checkstyle is failed due to a maven error:

{code}
[ERROR] Failed to execute goal on project hadoop-hdds-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-config:jar:0.6.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
[ERROR] Failed to execute goal on project hadoop-hdds-client: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-client:jar:0.6.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT, org.apache.hadoop:hadoop-hdds-config:jar:0.6.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
{code}

But it remained green.

We should fail the check if the maven run couldn't succeed.",pull-request-available,[],HDDS,Bug,Blocker,2020-04-01 11:40:03,1
13295515,Use EventQueue for delayed/immediate safe mode rule notification,"SCM is built from loosely coupled components which communicate with async event with each other.

Using the same abstraction (EventQueue) has the benefit that we can use the same visibility / testing tools such as the 'ozone insight' definition (which makes visible all the messages) or the test handler (which can wait until all the event queue messages are processed) 

During the review of HDDS-3221 it was suggested (by me) to use the EventQueue instead of the new SafeModeNotification interface. 

There was only one counter argument against it:

bq. I personally find the event queue logic hard to follow due to its async nature (you cannot just follow method calls in the IDE). Its not bad, but more difficult when you don't yet understand it, while registering some instances to be notified is easy to follow in an IDE. This is of course a subjective opinion :)

I respect this opinion, but I think it's better to use one abstraction and a consistent architecture inside one component (together with all the existing limitations). The EventQueue is not the only one possible solution, but an existing one. We can either design and switch to a new one or use the existing one.

In this patch I would like to show how the previous listener interface can be replaced by the EventQueue.

It (hopefully) shows that this is not complex, and in fact can help us to decouple different component from each other    ",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-04-01 11:26:06,1
13295478,Remove sever-side dependencies from hdds/ozone-common,"hadoop-ozone/common and hadoop-hdds/common projects are common between *client and server*. Therefore we should remove any server side utilities / dependencies from them as they would be added to the client classpath which makes harder the Ozone client / ozonefs adoption.

 * hadoop-hdds should depend on a minimal set of hadoop dependencies (can be managed a separated technical project / pom.xml)
* code shared between server side projects (and not with the client) should be moved to the framework
* OM related code should be moved to the ozone-manager instead of ozone/common",pull-request-available,[],HDDS,Bug,Critical,2020-04-01 09:29:04,1
13295453,OM logs not available for OM HA acceptance test,"Logs collected for OM HA acceptance test have no information about what's happening during the test.  Only the output of {{om --init}} is present:

{code:title=last few lines of log}
...
om1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-0d8a21ae-1a87-400b-bba3-d61cd4c01ce4
om1_1       | 2020-04-01 03:27:44 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG:
om1_1       | /************************************************************
om1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 2f4e39e19fea/172.21.0.6
om1_1       | ************************************************************/
om1_1       | Enabled profiling in kernel
{code}

Thus we have no information about why the test is failing intermittently:

https://github.com/apache/hadoop-ozone/runs/549544110",pull-request-available,['test'],HDDS,Bug,Major,2020-04-01 08:13:02,0
13295319,Update Ratis snapshot,"Update Ratis snapshot version to {{7c5b30d}}, which includes RATIS-816, required for HDDS-3023.",pull-request-available,['build'],HDDS,Task,Major,2020-03-31 17:52:25,0
13294858,TestFailureHandlingByClient.testDatanodeExclusionWithMajorityCommit is intermittent,See the attachments.,TriagePending flaky-test ozone-flaky-test,['test'],HDDS,Bug,Critical,2020-03-29 18:35:37,3
13294584,Ozone admin should always have read/write ACL permission on ozone objects,"Ozone admin should always have read/write acl permission to ozone objects. This way, if owner incorrectly set the acls and lose access, admin can always help to get acces back. 

",Triaged,['Security'],HDDS,Bug,Major,2020-03-28 01:14:13,4
13294562,Ozone admins getting Permission Denied error while creating volume ,"Even when a user is added to ozone.administrators,  Permission Denied error is thrown while creating a new volume.",pull-request-available,['Security'],HDDS,Bug,Major,2020-03-27 22:06:48,6
13294533,read operation failing when two container replicas are corrupted,"steps taken :

1) Mounted noise injection FUSE on all datanodes.

2) Write a key ( multi blocks)

3) Select one of the container ids ,  inject error on 2 container replicas for that container id.

4) Run GET key operation.

GET key operation fails intermittenly.

Error seen :

-------------

 
{noformat}
20/03/27 18:30:40 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
E 20/03/27 18:30:40 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
E 20/03/27 18:30:40 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
E 20/03/27 18:31:12 ERROR scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
E traceID: ""f80a51eaec481a1c:cbb8e92869015a53:f80a51eaec481a1c:0""
E containerID: 67
E datanodeUuid: ""96101390-2446-40e6-a54e-36e170497e57""
E readChunk {
E blockID {
E containerID: 67
E localID: 103896435892617248
E blockCommitSequenceId: 1010
E }
E chunkData {
E chunkName: ""103896435892617248_chunk_28""
E offset: 113246208
E len: 4194304
E checksumData {
E type: CRC32
E bytesPerChecksum: 1048576
E checksums: ""\034\376\313\031""
E checksums: "";U\225\037""
E checksums: ""\327m\332.""
E checksums: ""|\307\004E""
E }
E }
E }
E on the pipeline Pipeline[ Id: bce6316c-9690-452b-80e3-0f3590533444, Nodes: 96101390-2446-40e6-a54e-36e170497e57{ip: 172.27.111.129, host: quasar-olrywk-3.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}3e85204d-2399-43b5-952a-55b837eb4c1d{ip: 172.27.100.0, host: quasar-olrywk-1.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}5af0340a-6fee-4ce8-9f68-37fa35566a5a{ip: 172.27.73.0, host: quasar-olrywk-9.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:96101390-2446-40e6-a54e-36e170497e57, CreationTimestamp2020-03-27T03:36:51.880Z].
E Unexpected OzoneException: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 84603913ns. [remote_addr=/172.27.73.0:9859]]{noformat}
 

 

 ",fault_injection,['Ozone Datanode'],HDDS,Bug,Major,2020-03-27 18:37:06,3
13294514,Support Hadoop 3.3,Hadoop 3.3.1 is coming out soon. We should start testing Ozone on Hadoop 3.3,pull-request-available,[],HDDS,Task,Major,2020-03-27 17:05:07,0
13294510,Write operation when both OM followers are shutdown,"steps taken :
--------------
1. In OM HA environment, shutdown both OM followers.
2. Start PUT key operation.

PUT key operation is hung.

Cluster details : https://quasar-vwryte-1.quasar-vwryte.root.hwx.site:7183/cmf/home

Snippet of OM log on LEADER:


{code:java}
2020-03-24 04:16:46,249 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,249 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,250 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:46,250 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:46,750 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,750 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,750 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:46,750 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,250 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,251 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,251 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,251 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,751 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,751 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,752 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,752 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,252 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,252 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,252 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,252 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,752 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,752 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,753 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,753 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,254 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,254 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,254 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,254 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,754 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,754 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,754 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,754 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
{code}

Reported by [~nilotpalnandi]
",pull-request-available,[],HDDS,Bug,Major,2020-03-27 16:44:16,2
13294235,MiniOzoneChaosCluster exits because of deadline exceeding,"2020-03-26 21:26:48,869 [pool-326-thread-2] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 1: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.
grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now

{code}
2020-03-26 21:26:48,866 [pool-326-thread-2] ERROR loadgenerators.LoadExecutors (LoadExecutors.java:load(64)) - FileSystem LOADGEN: null Exiting due to exception
java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:359)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:281)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:259)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:119)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:199)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:133)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:254)
        at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:197)
        at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:63)
        at java.io.DataInputStream.read(DataInputStream.java:100)
        at org.apache.hadoop.ozone.utils.LoadBucket$ReadOp.doPostOp(LoadBucket.java:205)
        at org.apache.hadoop.ozone.utils.LoadBucket$Op.execute(LoadBucket.java:121)
        at org.apache.hadoop.ozone.utils.LoadBucket$ReadOp.execute(LoadBucket.java:180)
        at org.apache.hadoop.ozone.utils.LoadBucket.readKey(LoadBucket.java:82)
        at org.apache.hadoop.ozone.loadgenerators.FilesystemLoadGenerator.generateLoad(FilesystemLoadGenerator.java:54)
        at org.apache.hadoop.ozone.loadgenerators.LoadExecutors.load(LoadExecutors.java:62)
        at org.apache.hadoop.ozone.loadgenerators.LoadExecutors.lambda$startLoad$0(LoadExecutors.java:78)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:336)
        ... 20 more
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now
        at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:533)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:442)
        at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
       at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:700)
        at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
        at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:399)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:510)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:66)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:630)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:518)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:692)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:681)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
{code}",MiniOzoneChaosCluster,['Ozone Datanode'],HDDS,Bug,Blocker,2020-03-26 16:06:10,3
13294147,ozonesecure-mr test fails due to lack of disk space,{{ozonesecure-mr}} acceptance test is failing with {{No space available in any of the local directories.}},pull-request-available,['test'],HDDS,Bug,Major,2020-03-26 09:14:04,0
13294048,ozone.http.filter.initializers can't be set properly for SPNEGO auth,"After HDDS-2950, we change to use ozone's own initializer defined by ozone.http.filter.initializers instead the one configured with hadoop.http.filter.initializers.

The FilterInitializer interface was also forked from hadoop common  that prevents us from using org.apache.hadoop.security.AuthenticationFilterInitializer with the following error. 

This ticket is opened to fix it. ",pull-request-available,[],HDDS,Bug,Blocker,2020-03-26 00:22:44,4
13294045,Add timeouts to all robot tests,"We have seen in some CI runs that the acceptance test suit is getting cancelled as it runs for more than 6 hours. Because of this, the test results and logs are also not saved. 

This Jira aims to add a 5 minute timeout to all robot tests. In case some tests require more time, we can update the timeout. This would help to isolate the test which could be causing the whole acceptance test suit to time out.",pull-request-available,['test'],HDDS,Bug,Major,2020-03-25 23:39:20,7
13294040,Ozone BaseHTTPServer should honor ozone.security.enabled config,Ozone BaseHTTPServer tries to start HTTP server with Spnego Principal and Keytab even if ozone.security.enabled flag is set to false. It should honor ozone.security.enabled flag and start the server accordingly.,pull-request-available,['Security'],HDDS,Bug,Major,2020-03-25 23:09:33,6
13293713,Smoke Test: hdfs commands failing on hadoop 27 docker-compose,"Discovered by [~bharat] when testing 0.5.0-beta RC2.

 

 

issue when running hdfs commands on hadoop 27
docker-compose. I see the same test failing when running the smoke test.


$ docker exec -it c7fe17804044 bash

bash-4.4$ hdfs dfs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/kk

2020-03-22 04:40:14 WARN  NativeCodeLoader:60 - Unable to load
native-hadoop library for your platform... using builtin-java classes where
applicable

2020-03-22 04:40:15 INFO  MetricsConfig:118 - Loaded properties from
hadoop-metrics2.properties

2020-03-22 04:40:16 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot
period at 10 second(s).

2020-03-22 04:40:16 INFO  MetricsSystemImpl:191 - XceiverClientMetrics
metrics system started

-put: Fatal internal error

java.lang.NullPointerException: client is null

at java.util.Objects.requireNonNull(Objects.java:228)

at
org.apache.hadoop.hdds.scm.XceiverClientRatis.getClient(XceiverClientRatis.java:201)

at
org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:227)

at
org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:305)

at
org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:315)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:599)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:452)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:463)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:486)

at
org.apache.hadoop.ozone.[client.io|http://client.io/].BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:144)

at
org.apache.hadoop.ozone.client.io.KeyOutputStream.handleStreamAction(KeyOutputStream.java:481)

at
org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:455)

at
org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:508)

at
org.apache.hadoop.fs.ozone.OzoneFSOutputStream.close(OzoneFSOutputStream.java:56)

at
org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)

at
org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)

at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:62)

at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:120)

at
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:466)

at
org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:391)

at
org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:328)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:263)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:248)

at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:317)

at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:289)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:243)

at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:271)

at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:220)

at
org.apache.hadoop.fs.shell.CopyCommands$Put.processArguments(CopyCommands.java:267)

at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:201)

at org.apache.hadoop.fs.shell.Command.run(Command.java:165)

at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)

at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)

at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)

at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)


The same command when using ozone fs is working fine.

 docker exec -it fe5d39cf6eed bash

bash-4.2$ ozone fs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/kk

2020-03-22 04:41:10,999 [main] INFO impl.MetricsConfig: Loaded properties
from hadoop-metrics2.properties

2020-03-22 04:41:11,123 [main] INFO impl.MetricsSystemImpl: Scheduled
Metric snapshot period at 10 second(s).

2020-03-22 04:41:11,127 [main] INFO impl.MetricsSystemImpl:
XceiverClientMetrics metrics system started

bash-4.2$ ozone fs -ls o3fs://bucket1.vol1/

Found 1 items

-rw-rw-rw-   3 hadoop hadoop      17540 2020-03-22 04:41
o3fs://bucket1.vol1/kk",TriagePending,['test'],HDDS,Bug,Blocker,2020-03-25 04:34:34,1
13293648,CommitWatcher#watchForCommit does not timeout,"Seems the property *ozone.client.watch.request.timeout* was removed by HDDS-2920.  Note this is a client side property to wait for the future return. Without it, the client may wait for the future return forever in certain cases.  

",TriagePending,['Ozone Client'],HDDS,Bug,Major,2020-03-24 21:45:24,4
13293561,Intermittent timeout in integration tests,"Even after the changes done in HDDS-3086, some integration tests (especially in it-freon) are intermittently timing out.",TriagePending,[],HDDS,Bug,Critical,2020-03-24 14:02:22,3
13293402,Initialize Recon metrics for prometheus at /prom endpoint,"The endpoint /prom is available for Recon from Base HTTP server, but the metrics are not initialized.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-03-23 22:35:13,6
13293358,Bump version to 0.6.0-SNAPSHOT,"As 0.5.0 release is almost here, our daily builds should use a newer version.",pull-request-available,[],HDDS,Improvement,Major,2020-03-23 17:52:06,1
13292893,Provide message-level metrics from the generic protocol dispatch ,"Ozone RPC protocols (due to the limitation of protobuf) are very simple: There is only one message per service and the message is routed to the appropriate method based on the type. 

It makes very easy to add tracing / metrics as we have one generic dispatcher (OzoneProtocolMessageDispatcher.java) where we implemented opentracing support and basic metric collection.

In this patch I propose to improve this metric to add the message type to the metrics as a tag to make it easier to follow what's going on... (eg. see the distribution of the incoming message types)

We can also measure the total amount of time spent to serve one specific type of requests which can be used to calculate the moving average for latency.",pull-request-available,[],HDDS,Improvement,Major,2020-03-20 10:55:19,1
13292806,Recon should provide the list of datanodes that a missing container was present in.,"Currently, Recon does not have a way to determine the datanodes that had replica of a missing container. Add this information to the missing containers endpoint response. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-20 00:26:22,6
13292687,Fix Dropwizard metrics mapping for latest Ratis metrics,"Ratis use dropwizard metrics where the key parameters of the metrics (like group name or instance id) are part of the name of the metrics instead of using a tag.

For example
{code:java}
ratis.log_appender.851cb00a-af97-455a-b079-d94a77d2a936@group-C14654DE8C2C.follower_65f881ea-8794-403d-be77-a030ed79c341_match_index {code}
Instead of
{code:java}
ratis.log_appender_match_index{group=""group-C14654DE8C2C"",...} {code}
It makes hard to combine the same metrics (match_index) from different sources.

HDDS-2950 implemented a regexp based workaround, but the regexp doesn't match for the latest Ratis metrics.",pull-request-available,[],HDDS,Improvement,Major,2020-03-19 12:07:28,1
13292617,Fix retry interval default in Ozone client,"{code:java}
    <name>dfs.ratis.client.request.retry.interval</name>
    <value>1000ms</value>
{code}

Change the default value to 15s. As with 1s sleep, we will see retry happening at a very fast interval. 
In the billion object test, we see after changing the value to 15s, queue limit has never reached the limit.
",TriagePending billiontest pull-request-available,[],HDDS,Bug,Major,2020-03-19 04:43:46,2
13292454,2-way commit did not happen when WRITE failure injected in one of the datanodes of a piepeline,"This is an extension of bug HDDS-3214.

steps taken :

1) Mounted noise injection FUSE on all datanodes

2) Selected 1 datanode from each open pipeline (factor=3)

3) Injected WRITE FAILURE noise with error code - ENOENT on ""hdds.datanode.dir"" path of list of datanodes selected in step 2)

4) start PUT key operation of size  32 MB.

 

Observation :

----------------

PUT key operation failed. 

As there is a WRITE failure in one of the datanodes in the pipeline, 3 way commit should fail.

But it should proceed with 2-way commit and the operation should have been successful.

 

 ",fault_injection,[],HDDS,Bug,Major,2020-03-18 12:19:46,3
13292444,Ensure eviction of stateMachineData from cache only when both followers catch up,"Currently, the data in the StateMachineCache is evicted as soon as the applyTransaction call is issued for a transaction in Ratis. In our testing with keys in few kbs of size, it was figured that the data is evicted from the cache before append requests can be processed in a slightly slow follower thereby making leader read the chunk data from underlying fs/disk very frequently. This leads to slowing down the leader as well as well as overall throughput of the pipeline. 

The idea here is to ensure the data is evicted from the cache only when both followers have caught up with the match index. If a follower is really slow, it will eventually be marked slow after nodeFailureTimeout and pipeline will be destroyed.",Triaged,['Ozone Datanode'],HDDS,Bug,Major,2020-03-18 11:28:00,3
13292292,Tracing Ozone Manager DB write batch operations ,"Currently the OM DB write is asynchronously handled in OzoneManagerDoubleBuffer. But the OM response does not have traceID properly populated. As a result, we can't get insight for the DB part of the OM request handling. 

This ticket is opened to add traceID properly so that the addBatch and commitBatch cost of the request can be shown up in properly in Opentracing/Jaeger. ",pull-request-available,[],HDDS,Improvement,Major,2020-03-17 23:18:49,4
13292084,Add integration test for Recon FSCK.,Recon tracks the containers that are missing in the cluster. We have to add an integration tests that mimics this scenario to make sure there are no regressions along Recon's receipt of this information and subsequent processing. ,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-17 00:09:05,5
13292028,Datanode startup is slow due to iterating container DB 2-3 times,"During Datanode startup, for each container we iterate 2 times entire DB
1. For Setting block length
2. For finding delete Key count.

And for open containers, we do step 1 again.

*Code Snippet:*
*ContainerReader.java:*

*For setting Bytes Used:*
{code:java}
      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()
          .getRangeKVs(null, Integer.MAX_VALUE,
              MetadataKeyFilters.getNormalKeyFilter());

      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {
        BlockData blockData;
        try {
          blockData = BlockUtils.getBlockData(e.getValue());
          return blockData.getSize();
        } catch (IOException ex) {
          return 0L;
        }
      }).sum();
      kvContainerData.setBytesUsed(bytesUsed);
{code}

*For setting pending deleted Key count*

{code:java}
          MetadataKeyFilters.KeyPrefixFilter filter =
              new MetadataKeyFilters.KeyPrefixFilter()
                  .addFilter(OzoneConsts.DELETING_KEY_PREFIX);
          int numPendingDeletionBlocks =
              containerDB.getStore().getSequentialRangeKVs(null,
                  Integer.MAX_VALUE, filter)
                  .size();
          kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);
{code}

*For open Containers*

{code:java}
          if (kvContainer.getContainerState()
              == ContainerProtos.ContainerDataProto.State.OPEN) {
            // commitSpace for Open Containers relies on usedBytes
            initializeUsedBytes(kvContainer);
          }
{code}


*Jstack of DN during startup*
{code:java}
""Thread-8"" #34 prio=5 os_prio=0 tid=0x00007f5df5070000 nid=0x8ee runnable [0x00007f4d840f3000]
   java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksIterator.next0(Native Method)
        at org.rocksdb.AbstractRocksIterator.next(AbstractRocksIterator.java:70)
        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:195)
        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:155)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:158)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:191)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:168)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:146)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:101)
        at java.lang.Thread.run(Thread.java:748)
{code}
",billiontest pull-request-available,['Ozone Datanode'],HDDS,Improvement,Blocker,2020-03-16 17:52:36,2
13291780,Deprecate old Recon HTTP Server Keytab config key,The current config key for Recon HTTP Server Keytab file is `ozone.recon.keytab.file`. It needs to renamed to `ozone.recon.http.kerberos.keytab.file` for consistency.,pull-request-available,['Ozone Recon'],HDDS,Task,Minor,2020-03-15 07:18:07,6
13291779,Add unit tests to Recon Frontend,Add enzyme and jest libraries as dev dependencies and improve code coverage with unit tests in Recon UI.,Triaged,['Ozone Recon'],HDDS,Task,Major,2020-03-15 07:12:32,6
13291773,Intermittent failure in TestReconWithOzoneManager due to BindException,"TestReconWithOzoneManager may fail with BindException:

{code:title=https://github.com/apache/hadoop-ozone/pull/677/checks?check_run_id=507376007}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.707 s <<< FAILURE! - in org.apache.hadoop.ozone.recon.TestReconWithOzoneManager
org.apache.hadoop.ozone.recon.TestReconWithOzoneManager  Time elapsed: 19.706 s  <<< ERROR!
picocli.CommandLine$ExecutionException: Error while calling command (org.apache.hadoop.ozone.recon.ReconServer@23f74a49): java.net.BindException: Port in use: 0.0.0.0:36263
	...
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:534)
	at org.apache.hadoop.ozone.recon.TestReconWithOzoneManager.init(TestReconWithOzoneManager.java:109)
	...
Caused by: java.net.BindException: Port in use: 0.0.0.0:36263
	at org.apache.hadoop.hdds.server.http.HttpServer2.constructBindException(HttpServer2.java:1200)
	at org.apache.hadoop.hdds.server.http.HttpServer2.bindForSinglePort(HttpServer2.java:1222)
	at org.apache.hadoop.hdds.server.http.HttpServer2.openListeners(HttpServer2.java:1281)
	at org.apache.hadoop.hdds.server.http.HttpServer2.start(HttpServer2.java:1136)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:252)
	at org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:128)
	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:106)
	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50)
	at picocli.CommandLine.execute(CommandLine.java:1173)
	... 27 more
{code}

{code:title=test output}
2020-03-14 06:17:08,677 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(284)) - HTTP server of ozoneManager listening at http://0.0.0.0:36263
...
2020-03-14 06:17:11,589 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(170)) - Starting Web-server for recon at: http://0.0.0.0:36263
...
2020-03-14 06:17:12,756 [main] INFO  recon.ReconServer (ReconServer.java:start(125)) - Starting Recon server
2020-03-14 06:17:12,757 [main] INFO  http.HttpServer2 (HttpServer2.java:start(1139)) - HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:36263
...
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-03-15 06:14:08,0
13291618,Remove unused dependency version strings,"After the repo was split from hadoop, there are a few unused dependencies/version strings left in pom.xml. They can be removed.

Example: 

{code}
    <hbase.one.version>1.2.6</hbase.one.version>
    <hbase.two.version>2.0.0-beta-1</hbase.two.version>
{code}
There may be more.",newbie,[],HDDS,Task,Minor,2020-03-13 17:21:24,0
13291549,Rebalance integration tests,"With more integration tests enabled recently, it-client-and-hdds takes much more time (~50 mins) than other splits (20-25 mins), increasing overall delay in getting test results.",pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-03-13 13:08:21,0
13291510,Use DBStore instead of  MetadataStore in SCM,"The MetadataStore interface provides a generic view to any key / value store with a LevelDB and RocksDB implementation.

Since the early version of MetadataStore we also go the DBStore interface which is more andvanced (it supports DB profiles and ColumnFamilies).

To simplify the introduction of new features (like versioning or rocksdb tuning) we should use the new interface everywhere instead of the old interface.

We should update SCM and Datanode to use the DBStore instead of MetadataStore. ",backward-incompatible pull-request-available,[],HDDS,Improvement,Critical,2020-03-13 08:58:08,1
13291413,Fix issues in File count by size task.,"* Handle DELETE key operation correctly.
* Handle PUT key operation for an existing key.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-12 19:34:59,5
13291166,Integrate Recon missing containers UI with endpoint.,Integrate missing containers endpoint (/api/v1/containers/missing) with Recon UI.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-11 20:18:30,6
13291165,Add Recon endpoint to serve missing containers and its metadata.,Add a Recon API endpoint to serve missing containers information from Recon DB.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-11 20:16:50,6
13291071,Disable index and filter block cache for RocksDB,"During preformance tests It was noticed that the OM performance is dropped after 10-20 million of keys. (see the screenshot).

By default cache_index_and_filter_blocks is enabled for all of our RocksDB instances (see DBProfile) which is not the best option. (For example see this thread: https://github.com/facebook/rocksdb/issues/3961#)

With turning on this cache the indexes and bloom filters are cached **inside the block cache** which makes slower the cache when we have significant data.

Without turning it on (based on my understanding) all the indexes will remain open without any cache. With our current settings we have only a few number of sst files (even with million of keys) therefore it seems to be safe to turn this option off.

With turning this option of I was able to write >100M keys with high throughput. ",pull-request-available,[],HDDS,Improvement,Minor,2020-03-11 12:31:23,1
13291070,Bump RocksDB version to the latest one,"6.0.1 -- our current version from RocksDB -- released one year ago. Since than many new versions are released with important bug fixes.

I propose to update to the latest one...",pull-request-available,[],HDDS,Improvement,Minor,2020-03-11 12:30:23,1
13290917,Intermittent failure in Test2WayCommitInRatis,"Test2WayCommitInRatis may fail due to {{TimeoutIOException: Request #8 timeout 3s}} from Ratis while closing the container.  [~shashikant], can you please take a look? 
 Logs with RaftClient set to debug level attached.",pull-request-available,[],HDDS,Bug,Major,2020-03-10 21:33:24,3
13290901,Create REST API to serve Recon Dashboard and integrate with UI in Recon.,"Add a REST API to serve information required for recon dashboard

!Screen Shot 2020-03-10 at 12.10.41 PM.png!",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-10 19:11:18,6
13290799,Reduce number of chunkwriter threads in integration tests,"Integration tests run multiple datanodes in the same JVM.  Each datanode comes with 60 chunk writer threads by default (may be decreased in HDDS-3053).  This makes thread dumps (eg. produced by {{GenericTestUtils.waitFor}} on timeout) really hard to navigate, as there may be 300+ such threads.

Since integration tests are generally run with a single disk which is even shared among the datanodes, a few threads per datanode should be enough.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-03-10 12:55:21,0
13290642,Implement getIfExist in Table and use it in CreateKey/File,"With replay, now we use directly get() API.

Previously the code

OMKeyRequest.java

 
{code:java}
else if (omMetadataManager.getKeyTable().isExist(dbKeyName)) {
 // TODO: Need to be fixed, as when key already exists, we are
 // appending new blocks to existing key.
 keyInfo = omMetadataManager.getKeyTable().get(dbKeyName);{code}
 

Now for every create key/File we use get API, this is changed for replay
{code:java}
OmKeyInfo dbKeyInfo =
 omMetadataManager.getKeyTable().get(dbKeyName);
if (dbKeyInfo != null) {{code}

The proposal is to change get with getIfExist, and make use of keyMayExist.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Blocker,2020-03-09 20:34:44,2
13290532,Rename silently ignored tests,"Surefire plugin is configured to run {{Test*}} classes, but there are two test classes named {{*Test}}:

{code}
$ find */*/src/test/java -name '*Test.java' | xargs grep -l '@Test'
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/HddsServerUtilTest.java
hadoop-ozone/insight/src/test/java/org/apache/hadoop/ozone/insight/LogSubcommandTest.java
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-03-09 13:34:54,0
13290503,Create isolated environment for OM to test it without SCM,"OmKeyGenerator class from Freon can generate keys (open key + commit key). But this test tests both OM and SCM performance. It seems to be useful to have a method to test only the OM performance with faking the response from SCM.  

Can be done easily with the same approach what we have in HDDS-3023: A simple utility class can be implemented and with byteman we can replace the client calls with the fake method.",pull-request-available,[],HDDS,Improvement,Major,2020-03-09 10:59:23,1
13290498,Unit check fails to execute insight and mini-chaos-tests modules,"This was observed in unit check run for 0.5.0 RC.

{code:title=https://github.com/apache/hadoop-ozone/runs/490978126?check_suite_focus=true}
2020-03-06T19:13:08.6122969Z [ERROR] Failed to execute goal on project hadoop-ozone-insight: Could not resolve dependencies for project org.apache.hadoop:hadoop-ozone-insight:jar:0.5.0-beta: Could not find artifact org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
2020-03-06T19:13:08.6180318Z [ERROR] Failed to execute goal on project mini-chaos-tests: Could not resolve dependencies for project org.apache.hadoop:mini-chaos-tests:jar:0.5.0-beta: Failure to find org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in https://repository.apache.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced -> [Help 1]
{code}

Unit check skips {{integration-test}}, but these 2 modules depend on it.",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2020-03-09 10:10:22,0
13290495,Remove hard-coded SNAPSHOT version from GitHub workflows,Ozone's GitHub Actions workflows only work with SNAPSHOT versions due to hard-coded {{ozone-*-SNAPSHOT}} in target path.,pull-request-available,['build'],HDDS,Improvement,Major,2020-03-09 09:59:24,0
13290187,OM RpcClient fail with java.lang.IllegalArgumentException,"In OM HA cluster, when one of the om service is down, during creation of RpcClient it will fail with below error.

 

 
{code:java}
java.lang.IllegalArgumentException: java.net.UnknownHostException: om1
 at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:447)
 at org.apache.hadoop.ozone.om.ha.OMProxyInfo.<init>(OMProxyInfo.java:40)
 at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:115)
 at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:207)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:198)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:124)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:249)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:274)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:82)
 at picocli.CommandLine.execute(CommandLine.java:1173)
 at picocli.CommandLine.access$800(CommandLine.java:141)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
 at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
 at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
 at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
 at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
 at org.apache.hadoop.ozone.freon.Freon.execute(Freon.java:72)
 at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
 at org.apache.hadoop.ozone.freon.Freon.main(Freon.java:98)
Caused by: java.net.UnknownHostException: om1
 ... 22 more
 
{code}
 ",OMHATest pull-request-available,['OM HA'],HDDS,Sub-task,Blocker,2020-03-06 20:45:31,2
13289684,Skip KeyTable check in OMKeyCommit when ratis is disabled in OM.,"With replay logic, we have additional keyTable check to detect whether it is replay or not.

In non-HA case, we don't need this check. So this Jira is to skip that check in case of non-HA when ratis is not enabled.

 

*Ran simple test to know the perf impact:*
 
2295 Keys/sec with Additional Key Table check
2824 Keys/sec with removing that check
 

 

 ",pull-request-available,[],HDDS,Bug,Major,2020-03-05 01:27:32,2
13289442,DBUpdateResponse message could be much larger than ipc.maximum.data.length,"HDDS-1391 introduce a new OM RPC to allow Recon server to get delta of OM metadata update. However, the delta itself could be large. This causes ERROR on OM like below. 

Should we consider sending update in chunks over hadoop RPC instead of all in one piece (1.5 GB in this case)?

4:34:56.403 PM	WARN	OzoneManagerProtocolServerSideTranslatorPB	
##Response for request DBUpdates is too big size 1584040343
4:34:57.022 PM	WARN	Server	
Error serializing call response for call Call#12 Retry#15 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.17.112.109:58674
com.google.protobuf.CodedOutputStream$OutOfSpaceException: CodedOutputStream was writing to a flat byte array and ran out of space.
	at com.google.protobuf.CodedOutputStream.refreshBuffer(CodedOutputStream.java:828)
	at com.google.protobuf.CodedOutputStream.writeRawBytes(CodedOutputStream.java:959)
	at com.google.protobuf.CodedOutputStream.writeRawBytes(CodedOutputStream.java:905)
	at com.google.protobuf.CodedOutputStream.writeBytesNoTag(CodedOutputStream.java:386)
	at com.google.protobuf.CodedOutputStream.writeBytes(CodedOutputStream.java:229)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$DBUpdatesResponse.writeTo(OzoneManagerProtocolProtos.java)
	at com.google.protobuf.CodedOutputStream.writeMessageNoTag(CodedOutputStream.java:380)
	at com.google.protobuf.CodedOutputStream.writeMessage(CodedOutputStream.java:222)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OMResponse.writeTo(OzoneManagerProtocolProtos.java:15959)
	at org.apache.hadoop.ipc.Server.setupResponseForProtobuf(Server.java:3216)
	at org.apache.hadoop.ipc.Server.setupResponse(Server.java:3165)
	at org.apache.hadoop.ipc.Server.setupResponse(Server.java:3141)
	at org.apache.hadoop.ipc.Server.access$200(Server.java:139)
	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1061)
	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:858)
	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:844)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1001)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)",TriagePending,['Ozone Recon'],HDDS,Bug,Major,2020-03-04 06:29:26,5
13289350,Create REST API to serve Pipeline information and integrate with UI in Recon.,We need a REST API to serve Pipeline information in Recon and integrate with existing Recon UI.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-03 22:59:40,6
13289083,Fix TestSCMPipelineBytesWrittenMetrics,"In the test, we have Thread.sleep and then check the metric value. It will be better to use GenerictestUtils.waitFor and check the value of the metric. In few of the runs we have seen this test failed.
{code:java}
Thread.sleep(100 * 1000L);
metrics =
 getMetrics(SCMPipelineMetrics.class.getSimpleName());
for (Pipeline pipeline : cluster.getStorageContainerManager()
 .getPipelineManager().getPipelines()) {
 Assert.assertEquals(bytesWritten, getLongCounter(
 SCMPipelineMetrics.getBytesWrittenMetricName(pipeline), metrics));
}{code}",pull-request-available,[],HDDS,Bug,Major,2020-03-03 01:33:18,2
13289072,Freon work with OM HA,Make Freon commands work with OM HA,pull-request-available,[],HDDS,New Feature,Major,2020-03-02 23:58:05,2
13289065,"When ratis is enabled in OM, double Buffer metrics not getting updated ","DoubleBuffer metrics are not getting updated when ratis is enabled in OM.

There is no issue when ratis is not enabled, double buffer metrics are updating fine.
{code:java}
{""name"": ""Hadoop:service=OzoneManager,name=OzoneManagerDoubleBufferMetrics"",""modelerType"": ""OzoneManagerDoubleBufferMetrics"",""tag.Hostname"": ""hw13865.hitronhub.home"",""TotalNumOfFlushOperations"": 0,""TotalNumOfFlushedTransactions"": 0,""MaxNumberOfTransactionsFlushedInOneIteration"": 0,""FlushTimeNumOps"": 0,""FlushTimeAvgTime"": 0,""AvgFlushTransactionsInOneIteration"": 0},{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-03-02 23:11:47,5
13289052,Possible deadlock in LockManager,"{{LockManager}} has a possible deadlock.

# Number of locks is limited by using a {{GenericObjectPool}}.  If N locks are already acquired, new requestors need to wait.  This wait in {{getLockForLocking}} happens in a callback executed from {{ConcurrentHashMap#compute}} while holding a lock on a map entry.
# While releasing a lock, {{decrementActiveLockCount}} implicitly requires a lock on an entry in {{ConcurrentHashMap}}.",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-03-02 21:52:59,2
13288858,Add new Freon test for putBlock,The goal of this task is to introduce a new Freon test that issues putBlock commands.,pull-request-available,['test'],HDDS,Improvement,Major,2020-03-02 12:07:30,0
13288383,Fix race condition in Recon's container and pipeline handling.,"Fix the following issues in Recon
* Both the Incremental container report handler and the regular container report handler add new containers from SCM whenever they see a new container. This test and add step must be synchronized between the 2 handlers to avoid any inconsistent metadata state.
* NodeStateMap in allow does not addition of a single container to the Map of Node -> Set of Containers since it instantiates with a Collections.emptySet(), and then relies on a map.put() to update the value. Changing this to a ""new HashSet"" allows addition of a container one by one which is possible in Recon.
* Improve logging in Recon Container Manager when it receives a container report from a node before receiving the pipeline report for a newly created pipeline.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-02-28 19:42:12,5
13288382,Refactor 'Recon' in MiniOzoneCluster to use ephemeral port.,"Currently, Recon uses an ephemeral port only in the integration test for Recon. In all other integration tests, we end up using the default (9888) that causes failures in other integration tests that start up a Mini ozone cluster. In addition, we want to start up Recon in MiniOzoneCluster by explicitly requesting it rather than by default.",pull-request-available,[],HDDS,Bug,Major,2020-02-28 19:37:12,5
13288269,Intermittent timeout in TestOzoneManagerDoubleBufferWithOMResponse#testDoubleBuffer,"{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/474452740}
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 505.227 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
[ERROR] testDoubleBuffer(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 500.142 s  <<< ERROR!
java.lang.Exception: test timed out after 500000 milliseconds
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:394)
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:130)
{code}

Also in: https://github.com/apache/hadoop-ozone/pull/590/checks?check_run_id=467388979

CC [~bharat]",pull-request-available,['test'],HDDS,Bug,Critical,2020-02-28 12:11:20,0
13288225,Depend on lightweight ConfigurationSource interface instead of Hadoop Configuration,"To make it possible to create different client jars compiled with different version of Hadoop we need clear and Hadoop independent hdds-common (and hdds-client) projects.

(For more details about the motivation, check this design doc: https://lists.apache.org/thread.html/rd0ea00f958368e888db1947eb71e514fb977df0b7baaad928ac50e94%40%3Cozone-dev.hadoop.apache.org%3E)

Our current blocker is the usage of `org.apache.hadoop.conf.Configuration`. Configuration class is a heavyweight object from hadoop-common which introduce a lot of unnecessary dependencies. It also violates multiple [OOP principles|https://en.wikipedia.org/wiki/SOLID], for example the *Dependency inversion principle*.

To make our components more independent I propose to depend on a lightweight ConfigurationSource interface which includes all the required getXXX methods. OzoneConfiguration can implement that interface (and with older Hadoop we can create direct adapters).

",pull-request-available,[],HDDS,Sub-task,Major,2020-02-28 08:49:02,1
13288209,TestDeleteWithSlowFollower is failing intermittently,"{code}
[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 641.212 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 617.198 s  <<< ERROR!
java.io.IOException: INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:228)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:401)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:346)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:457)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:508)
	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
	at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:224)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-02-28 07:11:51,3
13288122,SCM does not exit Safe mode,"In a few scenarios, like Disks are gone, the datanode is not up or any other case, we may try to close pipelines.

If we close pipelines after SCM restart, SCM will not come out of safe mode. This is because of the current implementation where we get the count of the pipeline from DB when creating a SafeMode rule object. During this, if any pipeline is closed/removed from DB, the Rule does not know about it, and it PipelineSafeMode rule is never met, this causes a situation where we never come out of safe mode.

 

 

 ",Triaged,['SCM'],HDDS,Bug,Critical,2020-02-27 18:39:20,2
13288114,Save each output of smoketest executed multiple times,"Acceptance tests may invoke the same smoketest multiple times to verify behaviour in different states.  Currently output is saved to a file named based on _environment_, _test_ and _container_, so each execution's output overwrites the previous one.  We should check if the file already exists and add a suffix if necessary to avoid overwriting previous logs.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-02-27 17:56:27,0
13288109,Allow forced overwrite of local file,"{{ozone sh key get}} refuses to overwrite existing local file.  I would like to add a {{--force}} flag (default: false) to allow overriding this behavior, to make it easier to repeatedly get a key without forcing me to delete it locally first.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2020-02-27 17:40:53,0
13288074,Duplicate large key test,"{{TestDataValidate}} has 2 large key tests:

* {{ratisTestLargeKey}}
* {{standaloneTestLargeKey}}

But both of these test RATIS/3 replication since HDDS-675.  I think {{standaloneTestLargeKey}} can be removed.",pull-request-available,['test'],HDDS,Bug,Minor,2020-02-27 14:42:41,0
13287960,TestSCMNodeManager intermittent crash,"TestSCMNodeManager crashed in one of the runs, although it passes usually:

{code:title=https://github.com/apache/hadoop-ozone/pull/601/checks?check_run_id=471611827}
[ERROR] Crashed tests:
[ERROR] org.apache.hadoop.hdds.scm.node.TestSCMNodeManager
{code}

{code:title=hs_err_pid9082.log}
siginfo: si_signo: 11 (SIGSEGV), si_code: 2 (SEGV_ACCERR), si_addr: 0x00007f378cf6f340

Stack: [0x00007f37626fb000,0x00007f37627fc000],  sp=0x00007f37627f9e48,  free space=1019k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  0x00007f378cf6f340
C  [librocksdbjni3775377216204452319.so+0x2a05dd]  rocksdb::DB::Delete(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&)+0x4d
C  [librocksdbjni3775377216204452319.so+0x2a0641]  rocksdb::DBImpl::Delete(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&)+0x11
C  [librocksdbjni3775377216204452319.so+0x1a931a]  rocksdb::DB::Delete(rocksdb::WriteOptions const&, rocksdb::Slice const&)+0xba
C  [librocksdbjni3775377216204452319.so+0x19f3e0]  rocksdb_delete_helper(JNIEnv_*, rocksdb::DB*, rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int, int)+0x130
C  [librocksdbjni3775377216204452319.so+0x19f4a1]  Java_org_rocksdb_RocksDB_delete__J_3BII+0x41
j  org.rocksdb.RocksDB.delete(J[BII)V+0
j  org.rocksdb.RocksDB.delete([B)V+13
j  org.apache.hadoop.hdds.utils.RocksDBStore.delete([B)V+9
j  org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(Lorg/apache/hadoop/hdds/scm/pipeline/PipelineID;)V+35
j  org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(Lorg/apache/hadoop/hdds/scm/pipeline/Pipeline;)V+27
...
j  org.apache.hadoop.hdds.scm.node.DeadNodeHandler.destroyPipelines(Lorg/apache/hadoop/hdds/protocol/DatanodeDetails;)V+28
j  org.apache.hadoop.hdds.scm.node.DeadNodeHandler.onMessage(Lorg/apache/hadoop/hdds/protocol/DatanodeDetails;Lorg/apache/hadoop/hdds/server/events/EventPublisher;)V+6
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-27 08:04:23,0
13287915,Failure running integration test it-freon ,"Observed a time-out during pr-check/it-freon for HDDS-2940. Failure appears unrelated to the changes in the patch. 

[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 67.193 s - in org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations
2862
[INFO] Running org.apache.hadoop.ozone.freon.TestFreonWithDatanodeRestart
2863
[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 30.559 s - in org.apache.hadoop.ozone.freon.TestFreonWithDatanodeRestart
2864
[INFO] 
2865
[INFO] Results:
2866
[INFO] 
2867
[WARNING] Tests run: 16, Failures: 0, Errors: 0, Skipped: 3
2868
[INFO] 
2869
[INFO] ------------------------------------------------------------------------
2870
[INFO] BUILD FAILURE
2871
[INFO] ------------------------------------------------------------------------
2872
[INFO] Total time:  28:58 min
2873
[INFO] Finished at: 2020-02-26T17:55:42Z
2874
[INFO] ------------------------------------------------------------------------
2875
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork -> [Help 1]
2876
[ERROR] 
2877
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2878
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
2879
[ERROR] 
2880
[ERROR] For more information about the errors and possible solutions, please read the following articles:
2881
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
",pull-request-available,['freon'],HDDS,Bug,Major,2020-02-27 04:10:53,3
13287869,OM Delta updates request in Recon should work with secure Ozone Manager.,"* Introduced kerberos principal and keytab file configs for Recon
* Login Recon user with KDC while Recon starts up",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-26 23:06:59,6
13287717,Include output of timed out test in bundle,"Sometimes a unit/integration test does not complete, nor does it crash.  We should collect the output of such tests in the result bundle for analysis.

Example:

{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/469172863}
2020-02-26T08:15:58.2297584Z [INFO] Running org.apache.hadoop.ozone.freon.TestRandomKeyGenerator
2020-02-26T08:30:59.6189916Z [INFO] Running org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations
...
2020-02-26T08:32:47.6155975Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork
{code}

In this case TestRandomKeyGenerator had this problem.  It might be a bit tricky to find such tests, since these are not explicitly listed at the end, unlike failed or crashed tests.",pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-02-26 10:45:15,0
13287592,SCM scrub pipeline should be started after coming out of safe mode,"We should start scrubbing pipelines after SCM is out of safe mode.

Reasons to do this:
 # Right now, we do scrub pipeline as part of triggerPipelineCreation, now when we scrub pipelines in allocated state for more than ""ozone.scm.pipeline.allocated.timeout"", we might close some pipelines and with this, we might not be able to come out of safeMode. As in SafeModeRules, we get pipeline count from pipelineDB during initialization.

Example scenario:
 # Stop 3 Datanodes. 
 # Restart SCM.
 # Start Datanode after 6 mts. We shall never come out of safe mode, as pipeline in allocated state will meet scrubber time out condition.

To not to be in these kinds of scenarios, better thing to be done here is scrub pipelines after SCM out of the safe mode

 ",pull-request-available,[],HDDS,Bug,Blocker,2020-02-25 20:17:55,2
13287563,Datanodes unable to connect to recon in Secure Environment,"Datanodes throw this exception while connecting to recon.
{code:java}
datanode_1  | java.io.IOException: DestHost:destPort recon:9891 , LocalHost:localPort 6a99ad69685d/192.168.48.4:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  | java.io.IOException: DestHost:destPort recon:9891 , LocalHost:localPort 6a99ad69685d/192.168.48.4:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)datanode_1  |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)datanode_1  |  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)datanode_1  |  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)datanode_1  |  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)datanode_1  |  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)datanode_1  |  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)datanode_1  |  at org.apache.hadoop.ipc.Client.call(Client.java:1457)datanode_1  |  at org.apache.hadoop.ipc.Client.call(Client.java:1367)datanode_1  |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)datanode_1  |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)datanode_1  |  at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)datanode_1  |  at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)datanode_1  |  at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)datanode_1  |  at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)datanode_1  |  at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)datanode_1  |  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)datanode_1  |  at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)datanode_1  |  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)datanode_1  |  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)datanode_1  |  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)datanode_1  |  at java.base/java.lang.Thread.run(Thread.java:834)datanode_1  | Caused by: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:866)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)datanode_1  |  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)datanode_1  |  at org.apache.hadoop.ipc.Client.call(Client.java:1403)datanode_1  |  ... 14 moredatanode_1  | Caused by: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  |  at java.security.jgss/sun.security.krb5.PrincipalName.validateNameStrings(PrincipalName.java:174)datanode_1  |  at java.security.jgss/sun.security.krb5.PrincipalName.<init>(PrincipalName.java:397)datanode_1  |  at java.security.jgss/sun.security.krb5.PrincipalName.<init>(PrincipalName.java:471)datanode_1  |  at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:172)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.getServerPrincipal(SaslRpcClient.java:305)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:234)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:160)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)datanode_1  |  at java.base/java.security.AccessController.doPrivileged(Native Method)datanode_1  |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)datanode_1  |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)datanode_1  |  ... 17 more
{code}
Recon throws an exception while connecting to SCM:
{code:java}
recon_1     | 2020-02-25 17:48:14,506 [main] ERROR scm.ReconStorageContainerManagerFacade: Exception encountered while getting pipelines from SCM.recon_1     | 2020-02-25 17:48:14,506 [main] ERROR scm.ReconStorageContainerManagerFacade: Exception encountered while getting pipelines from SCM.recon_1     | java.io.IOException: DestHost:destPort scm:9860 , LocalHost:localPort recon/192.168.48.8:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1     |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)recon_1     |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)recon_1     |  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)recon_1     |  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)recon_1     |  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)recon_1     |  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)recon_1     |  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)recon_1     |  at org.apache.hadoop.ipc.Client.call(Client.java:1457)recon_1     |  at org.apache.hadoop.ipc.Client.call(Client.java:1367)recon_1     |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)recon_1     |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)recon_1     |  at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)recon_1     |  at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)recon_1     |  at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.listPipelines(StorageContainerLocationProtocolClientSideTranslatorPB.java:322)recon_1     |  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)recon_1     |  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)recon_1     |  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)recon_1     |  at java.base/java.lang.reflect.Method.invoke(Method.java:566)recon_1     |  at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)recon_1     |  at com.sun.proxy.$Proxy42.listPipelines(Unknown Source)recon_1     |  at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipelines(StorageContainerServiceProviderImpl.java:49)recon_1     |  at org.apache.hadoop.ozone.recon.scm.ReconStorageContainerManagerFacade.initializePipelinesFromScm(ReconStorageContainerManagerFacade.java:223)recon_1     |  at org.apache.hadoop.ozone.recon.scm.ReconStorageContainerManagerFacade.start(ReconStorageContainerManagerFacade.java:183)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:118)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:95)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:39)recon_1     |  at picocli.CommandLine.execute(CommandLine.java:1173)recon_1     |  at picocli.CommandLine.access$800(CommandLine.java:141)recon_1     |  at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)recon_1     |  at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)recon_1     |  at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)recon_1     |  at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)recon_1     |  at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)recon_1     |  at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)recon_1     |  at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:52)recon_1     | Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1     |  at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:760)recon_1     |  at java.base/java.security.AccessController.doPrivileged(Native Method)recon_1     |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)recon_1     |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:723)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:817)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)recon_1     |  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)recon_1     |  at org.apache.hadoop.ipc.Client.call(Client.java:1403)recon_1     |  ... 28 morerecon_1     | Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1     |  at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)recon_1     |  at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)recon_1     |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)recon_1     |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)recon_1     |  at java.base/java.security.AccessController.doPrivileged(Native Method)recon_1     |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)recon_1     |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)recon_1     |  ... 31 more
{code}
 ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-25 17:52:46,5
13287468,UpdateID check should be skipped for non-HA OzoneManager,"Delete key is failing . Here is the stack trace of the failure:

 

 
{noformat}
INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 26 which is not greater than the current value of 433 for OMKeyInfo{volume='vol-test-restartcomponentozonereaddata-1582093704', bucket='buck-test-restartcomponentozonereaddata-1582093704', key='ReadOzoneFile_1582093709', dataSize='10485760', creationTime='1582093712218', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862. Trying to failover immediately.
 
..
..
..
..
 
20/02/19 03:37:17 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 22 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862 after 15 failover attempts. Trying to failover immediately. E 2020-02-19 03:37:17,895 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(287)) - Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E 20/02/19 03:37:17 ERROR ha.OMFailoverProxyProvider: Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E Trying to set updateID to 23 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'}]
{noformat}
 

 ",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-02-25 11:04:09,2
13287409,OM crash during startup does not print any error message to log,"During code read, found a similar thing, we don't log for OM start also. As OM startup also using similar code for the startup.

 ",OMHATest,['Ozone Manager'],HDDS,Improvement,Major,2020-02-25 04:07:54,2
13287376,Fix Bug in Scrub Pipeline causing destory pipelines after SCM restart,"Currently, the scrubber is run as part of create pipeline. 

When SCM is started, scrubber is coming up and cleaning up all the containers in SCM. Because when loading pipelines, the pipelineCreationTimeStamp is set from when the pipeline is created.

 

Because of this, below condition is satisfied and destroying all the pipelines when SCM is restarted. This can be easily reproduced start SCM, wait for 10 minutes and restart SCM.

 
{code:java}
List<Pipeline> needToSrubPipelines = stateManager.getPipelines(type, factor,
 Pipeline.PipelineState.ALLOCATED).stream()
 .filter(p -> currentTime.toEpochMilli() - p.getCreationTimestamp()
 .toEpochMilli() >= pipelineScrubTimeoutInMills)
 .collect(Collectors.toList());
for (Pipeline p : needToSrubPipelines) {
 LOG.info(""srubbing pipeline: id: "" + p.getId().toString() +
 "" since it stays at ALLOCATED stage for "" +
 Duration.between(currentTime, p.getCreationTimestamp()).toMinutes() +
 "" mins."");
 finalizeAndDestroyPipeline(p, false);
}{code}
 

*Log showing scrubbing of pipeline*

 
{code:java}
2020-02-20 12:42:18,946 [RatisPipelineUtilsThread] INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: srubbing pipeline: id: PipelineID=35dff62d-9bfa-449b-b6e8-6f00cc8c1b6e since it stays at ALLOCATED stage for -1003 mins.{code}
 

 

 ",OMHATest pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-02-24 23:50:46,2
13287285,SCM startup failed  during loading containers from DB," This is happening because pipeline scrubber came and removed pipeline, and it closed pipeline and removed from DB and triggered close containers to set them to CLOSING. When SCM is restarted before close container command is handled and change the state to CLOSING, the below issue can happen.

 

This can happen in other scenarios like when safeModeHandler calls finalizeAndDestroyPipeline and do SCM restart. 

 

The root cause for this is Pipeline removed from DB and the container is in open state in this scenario, and when trying to get pipeline we will crash SCM due to the {{PipelineNotFoundException error.}}

{{}}
{code:java}
 2020-02-21 13:57:34,888 [main] ERROR org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SCM start failed with exception org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=35dff62d-9bfa-449b-b6e8-6f00cc8c1b6e not found at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133) at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.addContainerToPipeline(PipelineStateMap.java:110) at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addContainerToPipeline(PipelineStateManager.java:59) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.addContainerToPipeline(SCMPipelineManager.java:309) at org.apache.hadoop.hdds.scm.container.SCMContainerManager.loadExistingContainers(SCMContainerManager.java:121) at org.apache.hadoop.hdds.scm.container.SCMContainerManager.<init>(SCMContainerManager.java:107) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:412) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:283) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:215) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:612) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:142) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:117) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:66) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:42) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:55) 2020-02-21 13:57:34,892 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down StorageContainerManager at om-ha-1.vpc.cloudera.com/10.65.51.49 ************************************************************/{code}
{{}}",OMHATest pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-02-24 18:33:00,2
13287250,Ozone Filesystem should return real default replication,Ozone {{FileSystem}} implementation should return the actual configured replication factor for {{getDefaultReplication()}}.,pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-02-24 15:58:57,0
13287229,Get Key is hung when READ delay is injected in chunk file path,"Steps taken :

------------------
 # Mounted noise injection FUSE on all datanodes.
 # Write a key.
 # In one of the container replicas of the key, Inject READ delay of 5 seconds on chunk file directory path.
 # Run get Key operation.

Get Key operation is stuck and does not return any success/error .",fault_injection pull-request-available,[],HDDS,Bug,Major,2020-02-24 14:55:07,3
13287221,Add test to verify replication factor of ozone fs,Currently no test verifies that {{ozone fs}} creates keys for files with the expected replication factor.,pull-request-available,['test'],HDDS,Improvement,Minor,2020-02-24 14:14:13,0
13287203,Fix TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel,TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel is disabled due to intermittent issues. It should be fixed / rewritten or deleted.,TriagePending,['test'],HDDS,Sub-task,Major,2020-02-24 12:26:26,1
13287161,/retest github comment does not work,"Earlier we introduced an additional github workflow to process `/command` style commands from github comments. But in this case `/retest` didn't create the new commit to trigger a new build:

Pull request:

 https://github.com/apache/hadoop-ozone/pull/393

Run:

https://github.com/apache/hadoop-ozone/runs/463225330?check_suite_focus=true",pull-request-available,['CI'],HDDS,Bug,Major,2020-02-24 09:15:39,0
13286899,SCM crash during startup does not print any error message to log,"SCM start up failed due to a pipelineNotFoundException, there is no error message logged in to SCM log.

In the log file, we can see just below log message no reason for the crash is logged.

 

 
{code:java}
2020-02-20 15:37:56,079 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at xx.xx.xx/10.65.51.49
{code}
In the out file, we can see below, but not complete exception message.
{code:java}
PipelineID=xxxxx not found{code}
 

The actual reason for failure is not clearly logged if an exception has occurred during SCM startup.

 ",OMHATest pull-request-available,['SCM'],HDDS,Improvement,Major,2020-02-21 22:07:01,2
13286779,Decrease the number of the chunk writer threads,"As of now we create 60 threads (
dfs.container.ratis.num.write.chunk.threads) to write chunk data to the disk. As the write is limited by the IO I can't see any benefit to have so many threads. High number of thread means a high context switch overhead, therefore it seems to be more reasonable to use only a limited number of threads.

For example 10 threads should be enough even with 5 external disk.

If you know any reason to keep the number 60, please let me know...
",pull-request-available,[],HDDS,Improvement,Major,2020-02-21 12:48:27,1
13286749,Use meaningful name for ChunkWriter threads,ChunkWriter threads acreated with a naming schema 'pool-[x]-thread-[y]'. We can use better naming (especially as we have 60 threads...),pull-request-available,[],HDDS,Improvement,Major,2020-02-21 11:58:37,1
13286563,Fix Retry handling in ozone RPC Client,"Right now for all other exceptions other than serviceException we use FailOverOnNetworkException.

This Exception policy is created with 15 max fail overs and 15 retries. 

 
{code:java}
retryPolicyOnNetworkException.shouldRetry(
 exception, retries, failovers, isIdempotentOrAtMostOnce);{code}
*2 issues with this:*
 # When shouldRetry returns action FAILOVER_AND_RETRY, it will stuck with same OM, and does not perform failover to next OM.  As OMFailoverProxyProvider#performFailover() is a dummy call does not perform any failover.
 # When ozone.client.failover.max.attempts is set to 15, now with 2 policies with each set to 15, we will retry 15*2 times in worst scenario. 

 

 ",OMHA OMHATest pull-request-available,[],HDDS,Bug,Major,2020-02-20 20:34:39,2
13286422,Support running full Ratis pipeline from IDE (IntelliJ) ,"HDDS-1522 introduced a method to run full cluster in IntelliJ. The runner configurations can be copied with a shell script and a basic ozone-site.xml and log configuration to make it easy to run ozone from IDE.

Unfortunately this setup supports only one Datanode and it's harder to debug full Ozone pipeline (3 datanodes) from IDE.

This patch provides 3 different configuration for 3 datanodes with different ports to make it possible to run them on the same host from the IDE.",pull-request-available,[],HDDS,Improvement,Major,2020-02-20 09:14:37,1
13286349,Update Ratis version to 0.5.0 released.,Update Ozone to use latest released version of Ratis 0.5.0.,pull-request-available,[],HDDS,Task,Major,2020-02-19 23:25:51,4
13286231,Broken return code check in unit/integration,"HDDS-2915 fixed unit/integration check result in case of Maven error.  However, return code check was broken by output redirection via pipeline added in HDDS-2833 and HDDS-2960:

bq. The return status of a pipeline is the exit status of the last command, unless the pipefail option is enabled.",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2020-02-19 12:40:00,0
13286167,Cannot write 32MB chunks,"Writing 32MB chunks fails with various errors.

{code:title=steps to reproduce}
ozone freon dcg -t 1 -n 1 -s 33554432
{code}

1. With Ratis 0.5.0-90cd474-SNAPSHOT (used by current Ozone master):
{code}
org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: gRPC message exceeds maximum size 33554432: 33554686
  at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:524)
  at org.apache.ratis.thirdparty.io.grpc.internal.MessageDeframer.processHeader(MessageDeframer.java:387)
{code}

Which is strange, because [Datanode attempts to set max. message size|https://github.com/apache/hadoop-ozone/blob/4ba1932dab4692a9cc1bcfb8903ef650e32ec7ba/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/XceiverServerRatis.java#L204-L206] to 32MB + 16KB.

2. With Ratis built locally from current Ratis master (46f255cb):

{code}
Caused by: org.apache.ratis.protocol.StateMachineException: org.apache.ratis.server.raftlog.RaftLogIOException from Server ccb25fbf-9bd1-4094-a632-00f4168213bb@group-B1FA90A78F31: Log entry size 33554666 exceeds the max buffer limit of 33554432
	at org.apache.ratis.server.raftlog.RaftLog.appendImpl(RaftLog.java:178)
	at org.apache.ratis.server.raftlog.RaftLog.lambda$append$2(RaftLog.java:157)
	at org.apache.ratis.server.raftlog.RaftLogSequentialOps$Runner.runSequentially(RaftLogSequentialOps.java:68)
	at org.apache.ratis.server.raftlog.RaftLog.append(RaftLog.java:157)
	at org.apache.ratis.server.impl.ServerState.appendLog(ServerState.java:282)
	at org.apache.ratis.server.impl.RaftServerImpl.appendTransaction(RaftServerImpl.java:518)
	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:604)
{code}

With {{ozone.scm.chunk.size=32MB}} setting, {{ozone freon ockg -n 1 -t 1 -s 33554432}} also fails, but without apparent errors in the datanode log.",TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2020-02-19 07:25:38,2
13286129,OM HA- Client requests get LeaderNotReadyException after OM restart,"Scenario:

1.Set up OM HA cluster.

2. Perform some write operations.

3. Restart OM's.

4. Now try any write operation.

Below error will be thrown for 15 times, and finally, client request will fail.
{code:java}
 
2020-02-15 10:11:23,244 [qtp2025269734-19] INFO org.apache.hadoop.io.retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMLeaderNotReadyException): om1@group-D0D586AF6951 is in LEADER state but not ready yet.
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.processReply(OzoneManagerRatisServer.java:177)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.submitRequest(OzoneManagerRatisServer.java:136)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:162)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:118)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
        at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
, while invoking $Proxy81.submitRequest over nodeId=om1,nodeAddress=om-ha-1.vpc.cloudera.com:9862 after 1 failover attempts. Trying to failover immediately.
{code}
 ",OMHA OMHATest pull-request-available,[],HDDS,Bug,Major,2020-02-19 00:48:37,2
13286128,Key Rename should preserve the ObjectID,"On Key Renames, objectID should be preserved from the original Key. 
Currently it is being set to the new transactionLogIndex of the rename request.",OMHA pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-02-19 00:34:11,2
13286020,Use own version from InterfaceAudience/Stability version,"Current Ozone code uses the Hadoop version from @InterfaceAudience and @InterfaceStability annotations.

While Hadoop uses the annotations during the javadoc generation, in Ozone they are used only as markers as Ozone doesn't generate javadoc during the releases.

The two annotations are in the Hadoop common project. I propose to copy them and use the copied annotations instead of the original one. It would help us to reduce the dependencies on Hadoop (the hadoop-common which contains the original annotations has 87 transitive dependencies!!)",pull-request-available,[],HDDS,Improvement,Major,2020-02-18 15:40:18,1
13285828,README is missing from the source release tar,When we do a dist build with -Psrc the README.md of the root project is not packaged to the tar file which makes it impossible to do a build from the source package as the README.md is required by the dist script.,pull-request-available,[],HDDS,Bug,Blocker,2020-02-17 17:14:41,1
13285777,Create Freon test to test isolated Ratis LEADER,This is the pair/follow-up of HDDS-2974. It provides a new freon test which can set up a new Ratis ring. If the Datanode was instrumented earlier to mock leaders a real Ratis leader can be tested / measured without any overhead on the leader side...,pull-request-available,[],HDDS,Improvement,Major,2020-02-17 13:00:57,1
13285460,Datanode unable to close Pipeline after disk out of space,"Datanode gets into a loop and keeps throwing errors while trying to close pipeline


{code:java}
2020-02-14 00:25:10,208 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07: changes role from  FOLLOWER to CANDIDATE at term 6240 for changeToCandidate
2020-02-14 00:25:10,208 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=02e7e10e-2d50-4ace-a18b-701265ec9f07.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 is in candidate state for 31898494ms
2020-02-14 00:25:10,208 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start LeaderElection
2020-02-14 00:25:10,223 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032: begin an election at term 6241 for 0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,259 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032: Election REJECTED; received 0 response(s) [] and 2 exception(s); 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07:t6241, leader=null, voted=285cac09-7622-45e6-be02-b3c68ebf8b10, raftlog=285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-SegmentedRaftLog:OPENED:c4,f4,i14, conf=0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07: changes role from CANDIDATE to FOLLOWER at term 6241 for DISCOVERED_A_NEW_TERM
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown LeaderElection
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start FollowerState
2020-02-14 00:25:10,680 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-DD847EC75388->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12669,entriesCount=0,lastEntry=null
2020-02-14 00:25:10,752 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=7ad5ce51-d3fa-4e71-99f2-dd847ec75388.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31623987ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31618878ms
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.FollowerState: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-FollowerState: change to CANDIDATE, lastRpcTime:5021ms, electionTimeout:5017ms
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown FollowerState
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9: changes role from  FOLLOWER to CANDIDATE at term 6220 for changeToCandidate
2020-02-14 00:25:10,894 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=179ac1d0-e5d5-4898-bef7-0068fd2ea2c9.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 is in candidate state for 31805092ms
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start LeaderElection
2020-02-14 00:25:10,917 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033: begin an election at term 6221 for 0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033: Election REJECTED; received 0 response(s) [] and 2 exception(s); 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9:t6221, leader=null, voted=285cac09-7622-45e6-be02-b3c68ebf8b10, raftlog=285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-SegmentedRaftLog:OPENED:c0,f0,i8, conf=0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9: changes role from CANDIDATE to FOLLOWER at term 6221 for DISCOVERED_A_NEW_TERM
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown LeaderElection
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start FollowerState
2020-02-14 00:25:11,134 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-DD847EC75388->cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12669,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,218 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=7ad5ce51-d3fa-4e71-99f2-dd847ec75388.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31624453ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31619344ms
2020-02-14 00:25:11,347 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-2338B042C07B->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12579,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,361 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-2338B042C07B->cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12577,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,399 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=6a851c59-0345-4ad8-ac31-2338b042c07b.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31396085ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31391530ms
2020-02-14 00:25:11,406 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=6a851c59-0345-4ad8-ac31-2338b042c07b.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31396092ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31391537ms
2020-02-14 00:25:11,423 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-BA1E8724EE74->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12817,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,490 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=1ed1be53-b526-41af-bdf9-ba1e8724ee74.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31946345ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31945978ms
2020-02-14 00:25:11,909 INFO org.apache.ratis.server.impl.FollowerState: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-D506E1A1894E-FollowerState: change to CANDIDATE, lastRpcTime:5094ms, electionTimeout:5093ms
2020-02-14 00:25:11,909 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown FollowerState
{code}",TriagePending,['Ozone Datanode'],HDDS,Bug,Critical,2020-02-15 00:42:24,3
13285409, Ozone S3 CLI path command not working on HA cluster,"ozone s3 path <<bucketname>>

ozone s3 path are not working on OM HA cluster

 

Because these commands do not take URI as a parameter. And for shell in HA, passing URI is mandatory. 

 

Below is the output when running on OM HA cluster:

 
{code:java}
$ozone s3 path
Service ID or host name must not be omitted when ozone.om.service.ids is defined.
{code}
 

HDDS-2279 fixed only getsecret, and this is missed because parameter added om-service-id is applicable only getsecret command.

 

This Jira suggests to do, move parameter to common class named S3Handler, and make all S3 commands inherit this, so in future any new commands, this parameter will be applicable. 

 

 

Thank You [~chinseone] for reporting this issue.",OMHA OMHATest pull-request-available,"['OM HA', 'Ozone CLI', 'Ozone Manager', 'S3']",HDDS,Sub-task,Major,2020-02-14 20:17:29,2
13285251,Fix TestOzoneClientRetriesOnException.java,The test failure is bcoz the exception msg in the KeyOutputStream is overriden with a hardcoded string in case all failures where the test expects the underlying exception msg to be propagated.,pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2020-02-14 04:31:23,3
13285250,Fix TestContainerStateMachine.java,The Jira aims to enable {color:#172b4d}TestContainerStateMachine tests.{color},pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-02-14 04:30:46,3
13285249,Fix TestContainerStateMachineFailures.java,"The unit tests are written withe single node ratis into consideration. The expectation is the datanode fails, client should see an exception after next io as there is no new dn for new pipeline to form which is not happening as the cluster is created with multiple dns.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-02-14 04:30:12,3
13285213,Refreshing Recon UI results in 404,"Refreshing any URL of Recon UI (ex: [http://localhost:9888/Datanodes|http://vsubramanian-cdh-4.vpc.cloudera.com:9888/Datanodes]) throws 404 error.


{code:java}
HTTP ERROR 404 Not Found
URI:/Datanodes
STATUS:404
MESSAGE:Not Found
SERVLET:org.eclipse.jetty.servlet.DefaultServlet-4d9d1b69
{code}

This 404 should only happen with URLs that match ""/api"" pattern and not with the ""/"". ",TriagePending,['Ozone Recon'],HDDS,Bug,Critical,2020-02-13 23:12:33,6
13285074,parse and dump ozonemanager ratis segment file to printable text,"With RATIS-755, a log dump utility for ratis logs has been added. however to parse SM data, a toString supplier is needed to dump the log to printable form. This can be in the form of JSON , XML.

cc:  [~hanishakoneru][~bharat]",pull-request-available,[],HDDS,New Feature,Major,2020-02-13 12:56:20,2
13284749,OM HA stability issues,"To conclude a little, _+{color:#ff0000}major issues{color}+_ that I find:
 # When I do a long running s3g writing to cluster with OM HA and I stop the Om leader to force a re-election, the writing will stop and can never recover.

--updates 2020-02-20:

https://issues.apache.org/jira/browse/HDDS-3031 {color:#ff0000}fixes{color} this issue.

 

2. If I force a OM re-election and do a scm restart after that, the cluster cannot see any leader datanode and no datanodes are able to send pipeline reports, which makes the cluster unavailable as well. I consider this a multi-failover case when the leader OM and SCM are on the same node and there is a short outage happen to the node.

 

--updates 2020-02-20:

 When you do a jar swap for a new version of Ozone and enable OM HA while keeping the same ozone-site.xml as last time, if you've written some data into the last Ozone cluster (and therefore there are existing versions and metadata for om and scm), SCM cannot be up after the jar swap.

{color:#ff0000}Error logs{color}: PipelineID=aae4f728-82ef-4bbb-a0a5-7b3f2af030cc not found in scm out logs when scm process cannot be started.

 

--updates 2020-02-24:

After I add some logs to SCM starter:
Assuming SCM is only bounced after the leader OM is stopped
1. If SCM is bounced {color:#de350b}after{color} former leader OM is restarted, meaning all OMs are up, SCM will be bootstrapped correctly but there will be missing pipeline report from the node who doesn't have OM process on it (it's always him tho). This would cause all pipelines stay at ALLOCATED state and cluster will be in safemode. At this point, if I {color:#de350b}restart the blacksheep datanode{color}, it will come back and send the pipeline report to SCM and all pipelines will be at OPEN state.
2. If SCM is bounced {color:#de350b}before{color} the former leader OM is restarted, meaning not all OMs in ratis ring are up, SCM {color:#de350b}cannot{color} be bootstrapped correctly and it shows Pipeline not found.

 

Original posting:

Use S3 gateway to keep writing data into a specific s3 gateway endpoint. After the writer starts to work, I kill the OM process on the OM leader host. After that, the s3 gateway can never allow writing data and keeps reporting InternalError for all new coming keys.

Process Process-488:
{noformat}
 S3UploadFailedError: Failed to upload ./20191204/file1056.dat to ozone-test-reproduce-123/./20191204/file1056.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-489:
 S3UploadFailedError: Failed to upload ./20191204/file9631.dat to ozone-test-reproduce-123/./20191204/file9631.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-490:
 S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-491:
 S3UploadFailedError: Failed to upload ./20191204/file4220.dat to ozone-test-reproduce-123/./20191204/file4220.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-492:
 S3UploadFailedError: Failed to upload ./20191204/file5523.dat to ozone-test-reproduce-123/./20191204/file5523.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-493:
 S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
{noformat}

That's a partial list and note that all keys are different. I also tried re-enable the OM process on previous leader OM, but it doesn't help since the leader has changed. Also attach partial OM logs:
{noformat}
 2020-02-12 14:57:11,128 [IPC Server handler 72 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 72 on 9862, call Call#4859 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 2020-02-12 14:57:11,918 [IPC Server handler 159 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 159 on 9862, call Call#4864 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 2020-02-12 14:57:15,395 [IPC Server handler 23 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 23 on 9862, call Call#4869 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{noformat}
 

 

Also attach the ozone-site.xml config to enable OM HA:
{noformat}
<property>
 <name>ozone.om.service.ids</name>
 <value>OMHA</value>
 </property>
 <property>
 <name>ozone.om.nodes.OMHA</name>
 <value>om1,om2,om3</value>
 </property>
 <property>
 <name>ozone.om.node.id</name>
 <value>om1</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om1</name>
 <value>9.134.50.210:9862</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om2</name>
 <value>9.134.51.215:9862</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om3</name>
 <value>9.134.51.25:9862</value>
 </property>
 <property>
 <name>ozone.om.ratis.enable</name>
 <value>true</value>
 </property>
 <property>
 <name>ozone.enabled</name>
 <value>true</value>
 <tag>OZONE, REQUIRED</tag>
 <description>
 Status of the Ozone Object Storage service is enabled.
 Set to true to enable Ozone.
 Set to false to disable Ozone.
 Unless this value is set to true, Ozone services will not be started in
 the cluster.

Please note: By default ozone is disabled on a hadoop cluster.
 </description>
 </property>
{noformat}",OMHATest,['Ozone Manager'],HDDS,Bug,Blocker,2020-02-12 08:47:01,2
13284596,Move server-related shared utilities from common to framework,"hdds-common project is shared between all the client and server projects. hdds-server-framework project is shared between all the server side services.

To reduce unnecessary dependencies (to Hadoop, for example) we can move all the server-side related classes (eg. rocksdb layer, certificate tools) to the framework from the common.

We don't need the rocksdb utilities and certificate tools on the client side.",pull-request-available,[],HDDS,Improvement,Major,2020-02-11 14:22:18,1
13284523,Support github comment based commands,"Before we started to use github actions we had the opportunity to use some ""commands"" in github commants. For example when a `/label xxx` comment has been added to a PR, a bot added the label (by default just the committers can use labels, but with this approach it was possible for everyone).

 

Since the move to use github actions I got multiple question about re-triggering the test. Even it it's possible to do with pushing an empty commit (only by the owner or committer) I think it would be better to restore the support of comment commands.

 

This patch follows a very simple approach. The available commands are store in a separated subdirectory as shell scripts and they are called by a lightweight wrapper.",pull-request-available,[],HDDS,Improvement,Major,2020-02-11 08:33:47,1
13284447,Create REST API to serve Node information and integrate with UI in Recon.,We need a REST API in Recon to serve up information for the Datanodes page (HDDS-2827). The REST API can also include other useful methods present in NodeManager that gives the user information about the Nodes in the cluster.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-10 23:44:42,6
13284430,Add integration test for Recon's Passive SCM state.,"* Verify Recon gets pipeline, node and container report from Datanode.
* Verify SCM metadata state == Recon metadata state (Create pipeline , Close pipeline, create container)

",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-10 23:01:01,5
13284345,Handle existing volume/bucket in contract tests,"Contract tests use a random volume/bucket for each test case.  Volume/bucket creation fails if the same random number happens to be generated for another test case in the same class.

{code:title=https://github.com/apache/hadoop-ozone/runs/432759893}
2020-02-07T21:45:36.7489239Z [ERROR] Tests run: 18, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 43.647 s <<< FAILURE! - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus
2020-02-07T21:45:36.7492173Z [ERROR] testListStatusNoDir(org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus)  Time elapsed: 0.053 s  <<< ERROR!
2020-02-07T21:45:36.7493036Z VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
...
2020-02-07T21:45:36.7776813Z 	at org.apache.hadoop.ozone.TestDataUtil.createVolumeAndBucket(TestDataUtil.java:60)
2020-02-07T21:45:36.7779898Z 	at org.apache.hadoop.ozone.TestDataUtil.createVolumeAndBucket(TestDataUtil.java:93)
2020-02-07T21:45:36.7780272Z 	at org.apache.hadoop.fs.ozone.contract.OzoneContract.getTestFileSystem(OzoneContract.java:83)
2020-02-07T21:45:36.7784467Z 	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:181)
2020-02-07T21:45:36.7784916Z 	at org.apache.hadoop.fs.contract.AbstractContractGetFileStatusTest.setup(AbstractContractGetFileStatusTest.java:56)
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-02-10 15:11:34,0
13283951,Set the default value of grpc flow control window for ratis client and ratis server,"The grpc flow control window by default is set to 1Mb by default. During performance tests, it was observed that the flow control window has to be greater than the chunk size for optimum performance.",pull-request-available teragentest,"['Ozone Client', 'Ozone Datanode']",HDDS,Improvement,Major,2020-02-07 14:27:35,2
13283858,Intermittent timeout in TestBlockManager,"{code:title=https://github.com/apache/hadoop-ozone/runs/430663688}
2020-02-06T21:44:53.5319531Z [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.344 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.block.TestBlockManager
2020-02-06T21:44:53.5319796Z [ERROR] testMultipleBlockAllocation(org.apache.hadoop.hdds.scm.block.TestBlockManager)  Time elapsed: 1.167 s  <<< ERROR!
2020-02-06T21:44:53.5319942Z java.util.concurrent.TimeoutException: 
2020-02-06T21:44:53.5320496Z Timed out waiting for condition. Thread diagnostics:
2020-02-06T21:44:53.5320839Z Timestamp: 2020-02-06 09:44:52,261
2020-02-06T21:44:53.5320901Z 
2020-02-06T21:44:53.5321178Z ""Thread-26""  prio=5 tid=46 runnable
2020-02-06T21:44:53.5321292Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5321391Z         at java.lang.Thread.dumpThreads(Native Method)
2020-02-06T21:44:53.5326891Z         at java.lang.Thread.getAllStackTraces(Thread.java:1610)
2020-02-06T21:44:53.5327144Z         at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87)
2020-02-06T21:44:53.5327309Z         at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73)
2020-02-06T21:44:53.5327465Z         at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)
2020-02-06T21:44:53.5327618Z         at org.apache.hadoop.hdds.scm.block.TestBlockManager.testMultipleBlockAllocation(TestBlockManager.java:280)
2020-02-06T21:44:53.5388042Z         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-02-06T21:44:53.5388702Z         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-02-06T21:44:53.5388905Z         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-02-06T21:44:53.5389045Z         at java.lang.reflect.Method.invoke(Method.java:498)
2020-02-06T21:44:53.5389195Z         at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
2020-02-06T21:44:53.5389331Z         at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-02-06T21:44:53.5389662Z         at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
2020-02-06T21:44:53.5389776Z         at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-02-06T21:44:53.5389916Z         at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-02-06T21:44:53.5390040Z ""Signal Dispatcher"" daemon prio=9 tid=4 runnable
2020-02-06T21:44:53.5390156Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5390783Z ""EventQueue-CloseContainerForCloseContainerEventHandler""  prio=5 tid=32 in Object.wait()
2020-02-06T21:44:53.5390916Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5391019Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5391149Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5391299Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5391448Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5391587Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5391721Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5391844Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5391971Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5392100Z ""IPC Server idle connection scanner for port 43801"" daemon prio=5 tid=24 in Object.wait()
2020-02-06T21:44:53.5392227Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5392347Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5392463Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5392567Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5392694Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5393004Z ""Thread-28"" daemon prio=5 tid=48 timed_waiting
2020-02-06T21:44:53.5393121Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5393232Z         at java.lang.Thread.sleep(Native Method)
2020-02-06T21:44:53.5393352Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler.lambda$onMessage$0(SafeModeHandler.java:113)
2020-02-06T21:44:53.5393504Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler$$Lambda$38/725596393.run(Unknown Source)
2020-02-06T21:44:53.5393634Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5393927Z ""pool-9-thread-1""  prio=5 tid=45 timed_waiting
2020-02-06T21:44:53.5394061Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5394260Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5406780Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-02-06T21:44:53.5427435Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-02-06T21:44:53.5428120Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-02-06T21:44:53.5428601Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-02-06T21:44:53.5428758Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5428918Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5429052Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5429184Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5429297Z ""main""  prio=5 tid=1 timed_waiting
2020-02-06T21:44:53.5429405Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5429501Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5429775Z         at java.lang.Thread.join(Thread.java:1260)
2020-02-06T21:44:53.5429900Z         at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:26)
2020-02-06T21:44:53.5430034Z         at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:17)
2020-02-06T21:44:53.5430162Z         at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-02-06T21:44:53.5468996Z         at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-02-06T21:44:53.5469178Z         at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
2020-02-06T21:44:53.5469334Z         at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-02-06T21:44:53.5469466Z         at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-02-06T21:44:53.5469587Z         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
2020-02-06T21:44:53.5469708Z         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
2020-02-06T21:44:53.5469841Z         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
2020-02-06T21:44:53.5469967Z         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
2020-02-06T21:44:53.5470091Z         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
2020-02-06T21:44:53.5470215Z         at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
2020-02-06T21:44:53.5470339Z         at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
2020-02-06T21:44:53.5470449Z         at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
2020-02-06T21:44:53.5470577Z         at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
2020-02-06T21:44:53.5470706Z         at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-02-06T21:44:53.5470851Z         at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-02-06T21:44:53.5470989Z         at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-02-06T21:44:53.5471123Z         at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-02-06T21:44:53.5471252Z         at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-02-06T21:44:53.5471391Z         at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-02-06T21:44:53.5471525Z         at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-02-06T21:44:53.5471791Z         at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-02-06T21:44:53.5471932Z ""IPC Server idle connection scanner for port 37265"" daemon prio=5 tid=39 in Object.wait()
2020-02-06T21:44:53.5472061Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5472164Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5472277Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5472395Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5509093Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5509324Z ""Socket Reader #1 for port 37265""  prio=5 tid=38 runnable
2020-02-06T21:44:53.5509481Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5509580Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-02-06T21:44:53.5509772Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-02-06T21:44:53.5509940Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-02-06T21:44:53.5510121Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-02-06T21:44:53.5510287Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-02-06T21:44:53.5510624Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-02-06T21:44:53.5510740Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)
2020-02-06T21:44:53.5510912Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)
2020-02-06T21:44:53.5511550Z ""EventQueue-DatanodeCommandForDatanodeCommandHandler""  prio=5 tid=31 in Object.wait()
2020-02-06T21:44:53.5511733Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5511887Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5511993Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5512181Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5512357Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5512530Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5512725Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5512895Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5513065Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5513394Z ""ForkJoinPool.commonPool-worker-1"" daemon prio=5 tid=30 timed_waiting
2020-02-06T21:44:53.5513552Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5513695Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5513849Z         at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
2020-02-06T21:44:53.5753126Z         at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
2020-02-06T21:44:53.5806455Z         at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
2020-02-06T21:44:53.5807208Z ""EventQueue-DatanodeCommandForDatanodeCommandHandler""  prio=5 tid=49 in Object.wait()
2020-02-06T21:44:53.5807347Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5807455Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5807562Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5807703Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5807837Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5807968Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5808098Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5808388Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5808510Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5808851Z ""EventQueue-SafemodestatusForSafeModeHandler""  prio=5 tid=47 in Object.wait()
2020-02-06T21:44:53.5808977Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5809088Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5809206Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5809346Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5809471Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5809599Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5809731Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5809862Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5809982Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5810366Z ""Thread-14"" daemon prio=5 tid=29 timed_waiting
2020-02-06T21:44:53.5810463Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5810573Z         at java.lang.Thread.sleep(Native Method)
2020-02-06T21:44:53.5810706Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler.lambda$onMessage$0(SafeModeHandler.java:113)
2020-02-06T21:44:53.5810853Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler$$Lambda$38/725596393.run(Unknown Source)
2020-02-06T21:44:53.5810983Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5811095Z ""Finalizer"" daemon prio=8 tid=3 in Object.wait()
2020-02-06T21:44:53.5811193Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5811312Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5811435Z         at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-02-06T21:44:53.5811563Z         at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-02-06T21:44:53.5811691Z         at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-02-06T21:44:53.5811875Z ""IPC Server idle connection scanner for port 39545"" daemon prio=5 tid=16 in Object.wait()
2020-02-06T21:44:53.5811991Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5812106Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5812217Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5812333Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5812449Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5812553Z ""Reference Handler"" daemon prio=10 tid=2 in Object.wait()
2020-02-06T21:44:53.5812666Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5812784Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5812896Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5813013Z         at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-02-06T21:44:53.5813143Z         at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-02-06T21:44:53.5813253Z ""Socket Reader #1 for port 45489""  prio=5 tid=34 runnable
2020-02-06T21:44:53.5813364Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5813478Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-02-06T21:44:53.5813599Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-02-06T21:44:53.5813725Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-02-06T21:44:53.5813836Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-02-06T21:44:53.5813963Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-02-06T21:44:53.5814141Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-02-06T21:44:53.5814273Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)
2020-02-06T21:44:53.5814402Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)
2020-02-06T21:44:53.5814533Z ""Socket Reader #1 for port 39771""  prio=5 tid=42 runnable
2020-02-06T21:44:53.5814630Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5814744Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-02-06T21:44:53.5814866Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-02-06T21:44:53.5814990Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-02-06T21:44:53.5815117Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-02-06T21:44:53.5815222Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-02-06T21:44:53.5815342Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-02-06T21:44:53.5815472Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)
2020-02-06T21:44:53.5815601Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)
2020-02-06T21:44:53.5816007Z ""EventQueue-SafemodestatusForSafeModeHandler""  prio=5 tid=28 in Object.wait()
2020-02-06T21:44:53.5816130Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5816230Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5816350Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5816493Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5816633Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5816769Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5816906Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5817026Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5817155Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5817278Z ""IPC Server idle connection scanner for port 39771"" daemon prio=5 tid=43 in Object.wait()
2020-02-06T21:44:53.5817404Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5817520Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5817628Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5817728Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5817844Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5817967Z ""IPC Server idle connection scanner for port 45489"" daemon prio=5 tid=35 in Object.wait()
2020-02-06T21:44:53.5818089Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5818205Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5818301Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5818413Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5818533Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5818647Z ""process reaper"" daemon prio=10 tid=11 timed_waiting
2020-02-06T21:44:53.5818757Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5818864Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5818970Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-02-06T21:44:53.5819108Z         at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-02-06T21:44:53.5819245Z         at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-02-06T21:44:53.5819376Z         at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-02-06T21:44:53.5819581Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-02-06T21:44:53.5819716Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5819837Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5819951Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5820283Z ""surefire-forkedjvm-command-thread"" daemon prio=5 tid=9 runnable
2020-02-06T21:44:53.5820398Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5820509Z         at java.io.FileInputStream.readBytes(Native Method)
2020-02-06T21:44:53.5820617Z         at java.io.FileInputStream.read(FileInputStream.java:255)
2020-02-06T21:44:53.5820740Z         at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-02-06T21:44:53.5820867Z         at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-02-06T21:44:53.5820996Z         at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-02-06T21:44:53.5821131Z         at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-02-06T21:44:53.5821272Z         at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:390)
2020-02-06T21:44:53.5821432Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5821557Z ""IPC Server idle connection scanner for port 40327"" daemon prio=5 tid=20 in Object.wait()
2020-02-06T21:44:53.5821681Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5821796Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5821906Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5822020Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5822121Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5822442Z ""surefire-forkedjvm-ping-30s"" daemon prio=5 tid=10 timed_waiting
2020-02-06T21:44:53.5822562Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5822668Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5822785Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-02-06T21:44:53.5822918Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-02-06T21:44:53.5823066Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-02-06T21:44:53.5823218Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-02-06T21:44:53.5823353Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5823483Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5823618Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5823739Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5823800Z 
2020-02-06T21:44:53.5823847Z 
2020-02-06T21:44:53.5823955Z 	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)
2020-02-06T21:44:53.5824106Z 	at org.apache.hadoop.hdds.scm.block.TestBlockManager.testMultipleBlockAllocation(TestBlockManager.java:280)
2020-02-06T21:44:53.5824242Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-02-06T21:44:53.5824367Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-02-06T21:44:53.5824639Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-02-06T21:44:53.5824876Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-02-06T21:44:53.5825000Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
2020-02-06T21:44:53.5825204Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-02-06T21:44:53.5825339Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
2020-02-06T21:44:53.5825465Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-02-06T21:44:53.5825599Z 	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-07 07:51:31,0
13283797,Use getPropertiesByPrefix instead of regex in matching ratis client and server properties,"This Jira is to use getPropertiesByPrefix that match with ""datanode.ratis"" and set them during ratisClient and ratisServer creation.

 

Right now in the current code we use getValByRegex to match with regex, we can avoid that.

*2 Changes will go in this Jira.*

1.  Use getPropertiesByPrefix and set RatisClient and Server Conf.

2. Use the same prefix for RatisClient and Server Conf. (Right now we use prefix only for matching Ratis Server Conf)

 

cc [~aengineer]",pull-request-available teragentest,[],HDDS,Bug,Major,2020-02-06 22:43:02,2
13283787,Add metrics to OM DoubleBuffer,"Add a metric that will help in understanding the Average flush time which will help in understanding how the flush time increases over time.

Add another metric to show the average number of flush transactions in an iteration. This will show how many transactions are flushed in a single iteration over time.",pull-request-available,[],HDDS,Bug,Major,2020-02-06 21:51:33,2
13283590,Allocate Block failing with NPE,"When running teragen in one of the run observed this error.
{code:java}
05 14:43:16,635 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: java.lang.NullPointerException at java.util.Objects.requireNonNull(Objects.java:203) at java.util.Optional.<init>(Optional.java:96) at java.util.Optional.of(Optional.java:108) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineMetrics.incNumBlocksAllocated(SCMPipelineMetrics.java:118) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.incNumBlocksAllocatedMetric(SCMPipelineManager.java:520) at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.newBlock(BlockManagerImpl.java:265) at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:233) at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:188) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:159) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:117) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:98) at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:792) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createFile(OzoneManagerProtocolClientSideTranslatorPB.java:1596) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71) at com.sun.proxy.$Proxy18.createFile(Unknown Source) at org.apache.hadoop.ozone.client.rpc.RpcClient.createFile(RpcClient.java:1071) at org.apache.hadoop.ozone.client.OzoneBucket.createFile(OzoneBucket.java:538) at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.createFile(BasicOzoneClientAdapterImpl.java:208) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.createOutputStream(BasicOzoneFileSystem.java:256) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.create(BasicOzoneFileSystem.java:237) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1133) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1113) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1002) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:990) at org.apache.hadoop.examples.terasort.TeraOutputFormat.getRecordWriter(TeraOutputFormat.java:141) at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:659) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:779) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168){code}
 ",pull-request-available teragentest,[],HDDS,Bug,Major,2020-02-05 22:48:27,0
13283476,Acceptance test failures due to lack of disk space,"Recently acceptance tests have been failing with {{DISK_OUT_OF_SPACE}} error and/or timeouts.

{code}
Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE
...
DiskOutOfSpaceException: Out of space: The volume with the most available space (=5069360506 B) is less than the container size (=5368709120 B).
{code}",pull-request-available,['test'],HDDS,Bug,Critical,2020-02-05 13:28:32,0
13283377,Recon server start failed with CNF exception in a cluster with Auto TLS enabled.,"{code}
Caused by: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ssl/SslContextFactory$Server
        at org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1590)
        at org.apache.hadoop.hdds.server.BaseHttpServer.<init>(BaseHttpServer.java:83)
        at org.apache.hadoop.ozone.recon.ReconHttpServer.<init>(ReconHttpServer.java:35)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at com.google.inject.internal.DefaultConstructionProxyFactory$2.newInstance(DefaultConstructionProxyFactory.java:86)
        at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:105)
        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:267)
        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)
        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1103)
        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
        at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:145)
        at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)
        at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016)
        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)
        at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012)
        ... 13 more
Caused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.ssl.SslContextFactory$Server
        at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:583)
        at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
        ... 32 more
{code}
",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-02-05 06:02:23,5
13283341,Delete replayed entry from OpenKeyTable during commit,"During KeyCreate (and S3InitiateMultipartUpload), we do not check the OpenKeyTable if the key already exists. If it does exist and the transaction is replayed, we just override the key in OpenKeyTable. This is done to avoid extra DB reads.

During KeyCommit (or S3MultipartUploadCommit), if the key was already committed, then we do not replay the transaction. This would result in the OpenKeyTable entry to remain in the DB till it is garbage collected. 

To avoid storing stale entries in OpenKeyTable, during commit replays, we should check the openKeyTable and delete the entry if it exists.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Blocker,2020-02-05 00:07:17,7
13283218,ozonesecure acceptance test fails due to unexpected error message,"{code}
Create volume with non-admin user                                     | FAIL |
...
PERMISSION_DENIED Only admin users are authorized to create Ozone volumes. User: testuser2/scm@EXAMPLE.COM' does not contain 'Client cannot authenticate via'
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-04 10:36:52,0
13282995,Print Freon summary to log in non-interactive mode,"Freon prints progress to log in non-interactive mode since HDDS-2861, but summary statistics are still printed to standard output.",pull-request-available,['freon'],HDDS,Improvement,Minor,2020-02-03 15:01:41,0
13282460,Handle replay of OM Prefix ACL requests,"To ensure that Prefix acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMPrefixAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 19:31:24,7
13282347,Collect output of crashed tests,"Save output of crashed unit/integration tests to the diagnostic bundle, similar to how it's done for failed tests.",pull-request-available,['build'],HDDS,Improvement,Major,2020-01-30 10:05:09,0
13282296,Handle replay of OM Key ACL requests,"To ensure that Key acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 01:11:33,7
13282295,Handle replay of OM Volume ACL requests,"To ensure that volume acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMVolumeAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 01:09:03,7
13282287,Handle Replay of AllocateBlock request,"To ensure that allocate block operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMAllocateBlockRequest is made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 00:08:00,7
13282220,Unnecessary log messages in DBStoreBuilder,"DBStoreBuilder logs some table-related at INFO level.  This is fine for DBs that are created once per run, eg. OM or SCM, but Recon builds a new DB for each OM snapshot:

{code}
recon_1     | 2020-01-29 15:20:32,466 [pool-7-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/recon/om.snapshot.db_1580311232241
recon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: userTable
recon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: volumeTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: bucketTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTable
recon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: keyTable
recon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: deletedTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: openKeyTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTable
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3Table
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Table
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTable
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: dTokenTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTable
recon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: prefixTable
recon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTable
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: default
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default options. DBProfile.DISK
recon_1     | 2020-01-29 15:20:32,514 [pool-7-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB snapshot at /data/metadata/recon/om.snapshot.db_1580311232241.
{code}",pull-request-available,[],HDDS,Improvement,Minor,2020-01-29 15:41:08,0
13282027,Handle replay of S3 requests,"To ensure that S3 operations is idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

In this Jira, the following requests are made idempotent:
* S3InitiateMultipartUploadRequest
* S3MultipartUploadCommitPartRequest
* S3MultipartUploadCompleteRequest
* S3MultipartUploadAbortRequest",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-28 18:28:04,7
13282026,Ensure ozone manager service user is part of ozone.administrators,"Current we only add scm service principal to scmadmins at runtime. The ozone manager service principal is not honored as om admins. As a result, if user does not specify any user in 

ozone.administrators, they will not be able to create a volume. ",pull-request-available,[],HDDS,Sub-task,Major,2020-01-28 18:24:13,4
13282025,Improve usability issues of ozone.administrators,This is an umbrella Jira to improve usability issues around ozone.administrators.,Triaged,['Security'],HDDS,Improvement,Major,2020-01-28 18:19:54,4
13281922,Upgrade jetty to the latest 9.4 release,"The jetty which is used by web interfaces of Ozone is from the September of 2018.

Since then many bug and security fixes added to the Jetty project. I would suggest to use the latest jetty (from January of 2020).

As HttpServer2 (hadoop 3.2 class) has a strong dependency on the older version of Jetty (it uses SessionManager which is removed), it seems to be easier to clone HttpServer2 and move it to the Ozone project.

It provides us the flexibility:
 # To upgrade jetty independent from the Hadoop releases
 # To remove unused features
 # To support ozone style configuration
 # Add additional insights/metrics to our own HttpServer
 # Simplify the current server initialization (current BaseHttpServer class of hdds/ozone is a wrapper around the Hadoop class, but we can combine the two functionalities)",pull-request-available,[],HDDS,Improvement,Blocker,2020-01-28 11:40:27,1
13281816,Implement ofs://: Add robot tests for mkdir,We need to add extra robot tests case (in addition to the existing ones adapted from o3fs) for ofs.,pull-request-available,[],HDDS,Sub-task,Major,2020-01-27 23:03:56,4
13281800,Handle replay of KeyCommitRequest and DirectoryCreateRequest,"To ensure that key commit operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyCommitRequest is made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-27 20:48:00,7
13281759,Parameterize unit tests for chunk manager implementation,Some existing unit tests rely on implementation details of existing ChunkManager (eg. TestKeyValueContainerCheck).  These should run common test cases on both implementations.  Implementation-specific test cases should be extracted to separate test classes.,pull-request-available,['test'],HDDS,Task,Major,2020-01-27 16:44:48,0
13281320,Use regex to match with ratis  grpc properties when creating ratis server,"This Jira is to use regex which are matching withratis grpc properties and set them when creating ratis server. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis server, we can set them and use them without any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove in a clean up Jira to remove ozone config for ratis server or leave as it is for existing ones.

 

HDDS-2903 already taken care of properties matching with raft.server.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-01-24 01:41:16,2
13281237,Document bucket encryption option in shell/BucketCommands.md,"--bucketkey option is missing from shell/BucketCommands.md

 ",pull-request-available,[],HDDS,Improvement,Major,2020-01-23 16:44:57,4
13281228,Hive queries fail at readFully,"When running Hive queries on a 1TB dataset for TPC-DS tests, we started to see an exception coming out from FSInputStream.readFully.
This does not happen with a smaller 100GB dataset, so possibly multi block long files are the cause of the trouble, and the issue was not seen with a build from early december, so we most likely to blame a recent change since then. The build I am running with is from the hash 929f2f85d0379aab5aabeded8a4d3a5056777706 of master branch but with HDDS-2188 reverted from the code.

The exception I see:
{code}
Error while running task ( failure ) : attempt_1579615091731_0060_9_05_000029_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
        at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
        at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.io.IOException: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
        at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)
        at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)
        at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
        at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:532)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:178)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
        ... 16 more
Caused by: java.io.IOException: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:422)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
        ... 27 more
Caused by: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:126)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
        at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readStripeFooter(RecordReaderUtils.java:269)
        at org.apache.orc.impl.RecordReaderImpl.readStripeFooter(RecordReaderImpl.java:308)
        at org.apache.orc.impl.RecordReaderImpl.beginReadStripe(RecordReaderImpl.java:1089)
        at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1051)
        at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1219)
        at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1254)
        at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:284)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:67)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:83)
        at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:145)
        at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:135)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2046)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419)
        ... 28 more
{code}",pull-request-available,[],HDDS,Bug,Critical,2020-01-23 15:50:22,3
13280947,Remove hdfs-client dependency from hdds-common,"As of now we have a hdds-common > hdfs-client dependency but in reality we don't use an important thing only a few string utils.--

I would propose to remove this dependency to have a safer / smaller and more independent ozone package.",pull-request-available,[],HDDS,Improvement,Major,2020-01-22 11:08:59,1
13280913,Fix Pipeline#nodeIdsHash collision issue,"Currently SCM relies on the XOR of the three node ID hash to detect overlapped pipelines. Per offline discussion with [~aengineer], this could potentially give false positive result when detecting overlapped pipelines. This ticket is opened to fix the issue.  ",pull-request-available,[],HDDS,Sub-task,Major,2020-01-22 07:40:28,4
13280873,Remove ozone ratis server specific config keys,"Once HDDS-2903 went in, now we can use direct ratis server configurations in XceiverClientRatis. This Jira is to clean up the old configuration and add any new required configuration.",pull-request-available teragentest,[],HDDS,Bug,Major,2020-01-22 01:07:58,2
13280868,Remove ozone ratis client specific config keys,"HDDS-2896 went in, now we can use direct ratis client configurations in OzoneClient. This Jira is to clean up the old configuration and add any new required configuration.",pull-request-available teragentest,[],HDDS,Bug,Major,2020-01-22 00:30:46,2
13280790,Increase timeout of safe-mode exit in acceptance tests,"Acceptance tests seem to be randomly failing recently with SCM not exiting safe mode within the 90 seconds timeout.  This task proposes to increase the timeout to check if it helps.

Also, when acceptance test fails with this condition, containers are not stopped and logs not copied.  This should be fixed.

{code:title=example}
2020-01-21T10:48:10.2663685Z WARNING! Safemode is still on. Please check the docker-compose files
2020-01-21T10:48:10.2683558Z ERROR: Test execution of /home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31 is FAILED!!!!
2020-01-21T10:48:10.2712438Z cp: cannot stat '/home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31/result/robot-*.xml': No such file or directory
2020-01-21T10:48:10.2716708Z cp: cannot stat '/home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31/result/docker-*.log': No such file or directory
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-01-21 15:21:06,0
13280718,Add a different retry policy for watch requests,"Currently, every request submitted to client is retried with a default retry policy in ozone.

For watch requests , ideally the timeout is higher and there should be no retries. The idea is to address that.",Triaged pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2020-01-21 10:22:26,3
13280680,OM HA cli getserviceroles not working,"started docker based cluster with ""ozone.om.ratis.enable"" = true

OM started with ratis backend.

 
{noformat}
om_1        | 2020-01-20 09:02:48,116 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]om_1        | 2020-01-20 09:02:48,116 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]om_1        | 2020-01-20 09:02:49,213 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled clusterom_1        | 2020-01-20 09:02:49,279 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.18.0.2:9862om_1        | 2020-01-20 09:02:49,280 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefaultom_1        | 2020-01-20 09:02:49,294 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1        | 2020-01-20 09:02:49,315 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1        | 2020-01-20 09:02:50,268 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1        | 2020-01-20 09:02:50,309 [main] INFO util.log: Logging initialized @3941msom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: using custom profile for table: userTableom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTableom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: using custom profile for table: volumeTableom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: bucketTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: keyTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: deletedTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: openKeyTableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: s3Tableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Tableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: dTokenTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: prefixTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTableom_1        | 2020-01-20 09:02:50,433 [main] INFO db.DBStoreBuilder: using custom profile for table: defaultom_1        | 2020-01-20 09:02:50,433 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:defaultom_1        | 2020-01-20 09:02:50,435 [main] INFO db.DBStoreBuilder: Using default options. DBProfile.DISKom_1        | 2020-01-20 09:02:50,620 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirsom_1        | 2020-01-20 09:02:50,647 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872om_1        | 2020-01-20 09:02:50,682 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)om_1        | 2020-01-20 09:02:50,762 [main] INFO grpc.GrpcFactory: PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.om_1        | It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.om_1        | 2020-01-20 09:02:50,767 [main] INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.port = 9872 (custom)om_1        | 2020-01-20 09:02:50,768 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)om_1        | 2020-01-20 09:02:50,770 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)om_1        | 2020-01-20 09:02:50,771 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)om_1        | 2020-01-20 09:02:50,771 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)om_1        | 2020-01-20 09:02:51,110 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)om_1        | 2020-01-20 09:02:51,117 [main] INFO impl.RaftServerProxy: a7718018-f8c6-4b70-90b7-aadd8f920710: addNew group-C5BA1605619E:[a7718018-f8c6-4b70-90b7-aadd8f920710:om:9872] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@2b0b7e5a[Not completed]om_1        | 2020-01-20 09:02:51,123 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
 
{noformat}
 

Ran ""getserviceroles"" command from CLI. ""getserviceroles"" API not working

 
{noformat}
/opt/hadoop/bin/ozone admin om getserviceroles -id=omServiceIdDefault/opt/hadoop/bin/ozone admin om getserviceroles -id=omServiceIdDefaultCouldn't create RpcClient protocol exception:java.lang.IllegalArgumentException: Could not find any configured addresses for OM. Please configure the system with ozone.om.address at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:138) at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:208) at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:155) at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:190) at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:122) at org.apache.hadoop.ozone.admin.om.OMAdmin.createClient(OMAdmin.java:59) at org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand.call(GetServiceRolesSubcommand.java:49) at org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand.call(GetServiceRolesSubcommand.java:32) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.ozone.admin.OzoneAdmin.main(OzoneAdmin.java:66)Couldn't create RpcClient protocol{noformat}",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Bug,Major,2020-01-21 08:52:42,2
13280540,Unit check passes despite Maven error,"Currently unit tests are broken due to some Surefire exception (HDDS-2898), but unit check is still passing.",pull-request-available,['build'],HDDS,Bug,Critical,2020-01-20 12:00:01,0
13280289,Remove unusued BlockLocation protocol related classes,"We have two classes which use org.apache.hadoop.hdfs.protocol.DatanodeInfo (HDFS!!) inside common. They are not used any more.

Even if they keep memories from the good old times, I would suggest to remove them to reduce the Hadoop dependency...",pull-request-available,[],HDDS,Improvement,Major,2020-01-18 12:38:55,1
13280277,Remove default dependencies from hadoop-hdds/pom.xml,"There are two ways to add certain set of dependencies to all the maven projects.
 # You can add it to the parent project which will be inherited to all the children projects
 # You can add it only to the required project and will be used via transitive dependencies

I think the 2nd approach is safest as we might need to create a new child project *without* Hadoop dependencies which is not possible with the 1st approach.",pull-request-available,[],HDDS,Improvement,Major,2020-01-18 08:40:03,1
13280213, Use regex to match with ratis properties when creating ratis server,"This Jira is to use regex which are matching with ratis server and ratis grpc properties and set them when creating ratis server. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis server, we can set them and use them without any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove in a clean up Jira to remove ozone config for ratis server or leave as it is for existing ones.",pull-request-available teragentest,['Ozone Datanode'],HDDS,Bug,Major,2020-01-17 19:48:11,2
13280206,execute_robot_test on unknown/unavailable container should fail acceptance test,"Before the fix for HDDS-2897, {{recon}} container in {{ozonesecure}} environment exited with error soon after start.  While working on the fix [~avijayan] noticed that the following test case (which tries to run some test on {{recon}} container) does not catch this error:

{code}
execute_robot_test recon <any test>
{code}

Ideally this should fail, since {{recon}} container is down.  But all commands that reference the container are run with [{{set +e}} in effect|https://github.com/apache/hadoop-ozone/blob/1caf1e30865aa2b380a7fca6d87f5ae8034fee4e/hadoop-ozone/dist/src/main/compose/testlib.sh#L99-L107], so the container's unavailibility is basically ignored, and acceptance test passes.",pull-request-available,['test'],HDDS,Bug,Major,2020-01-17 19:15:54,0
13280095,Avoid logging NPE when space usage check is not configured,"If {{hdds.datanode.du.factory.classname}} is not configured, a harmless but annoying NPE appears in datanode log during startup.",pull-request-available,[],HDDS,Bug,Minor,2020-01-17 09:32:18,0
13280040,Build with test fails due to ClassFormatError due to hadoop-hdds-client test,"output of mvn clean package, not sure if it's normal
{code:java}
[INFO] Reactor Summary for Apache Hadoop Ozone Main 0.5.0-SNAPSHOT:
[INFO]
[INFO] Apache Hadoop Ozone Main ........................... SUCCESS [  0.789 s]
[INFO] Apache Hadoop HDDS ................................. SUCCESS [  2.994 s]
[INFO] Apache Hadoop HDDS Config .......................... SUCCESS [  4.201 s]
[INFO] Apache Hadoop HDDS Common .......................... SUCCESS [02:16 min]
[INFO] Apache Hadoop HDDS Client .......................... FAILURE [  1.052 s]
[INFO] Apache Hadoop HDDS Server Framework ................ SKIPPED
[INFO] Apache Hadoop HDDS Container Service ............... SKIPPED
[INFO] Apache Hadoop HDDS/Ozone Documentation ............. SKIPPED
[INFO] Apache Hadoop HDDS SCM Server ...................... SKIPPED
[INFO] Apache Hadoop HDDS Tools ........................... SKIPPED
[INFO] Apache Hadoop Ozone ................................ SKIPPED
[INFO] Apache Hadoop Ozone Common ......................... SKIPPED
[INFO] Apache Hadoop Ozone Client ......................... SKIPPED
[INFO] Apache Hadoop Ozone Manager Server ................. SKIPPED
[INFO] Apache Hadoop Ozone FileSystem ..................... SKIPPED
[INFO] Apache Hadoop Ozone Tools .......................... SKIPPED
[INFO] Apache Hadoop Ozone S3 Gateway ..................... SKIPPED
[INFO] Apache Hadoop Ozone CSI service .................... SKIPPED
[INFO] Apache Hadoop Ozone Recon CodeGen .................. SKIPPED
[INFO] Apache Hadoop Ozone Recon .......................... SKIPPED
[INFO] Apache Hadoop Ozone Integration Tests .............. SKIPPED
[INFO] Apache Hadoop Ozone Datanode ....................... SKIPPED
[INFO] Apache Hadoop Ozone In-Place Upgrade ............... SKIPPED
[INFO] Apache Hadoop Ozone Insight Tool ................... SKIPPED
[INFO] Apache Hadoop Ozone FileSystem Single Jar Library .. SKIPPED
[INFO] Apache Hadoop Ozone FileSystem Legacy Jar Library .. SKIPPED
[INFO] Apache Hadoop Ozone Distribution ................... SKIPPED
[INFO] Apache Hadoop Ozone Fault Injection Tests .......... SKIPPED
[INFO] Apache Hadoop Ozone Network Tests .................. SKIPPED
[INFO] Apache Hadoop Ozone Mini Ozone Chaos Tests ......... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:26 min
[INFO] Finished at: 2020-01-17T11:31:57+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-hdds-client: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test failed: java.lang.ClassFormatError: Illegal field name ""org.apache.hadoop.hdds.scm.storage.TestBlockInputStream$this"" in class org/apache/hadoop/hdds/scm/storage/TestBlockInputStream$DummyBlockInputStreamWithRetry -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-hdds-client
{code}",pull-request-available,[],HDDS,Bug,Critical,2020-01-17 04:05:11,1
13279983,Ozone recon Start failed due to Kerberos principal not being found.,"Ozone recon Start failed due to Kerberos principal not being found.
{code}
Caused by: javax.servlet.ServletException: javax.servlet.ServletException: Principal not defined in configuration
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180)
	at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)
	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:881)
	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349)
	at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406)
	at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368)
	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778)
	at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262)
	at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113)
	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
	at org.eclipse.jetty.server.Server.start(Server.java:427)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105)
	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
	at org.eclipse.jetty.server.Server.doStart(Server.java:394)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1199)
	... 13 more
Caused by: javax.servlet.ServletException: Principal not defined in configuration
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:137)
	... 35 more {code}",pull-request-available,[],HDDS,Bug,Major,2020-01-16 21:08:12,5
13279858,Use regex to match with ratis properties when creating ratis client,"This Jira is to use regex which are matching with ratis client and ratis grpc properties and set them when creating ratis client. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis client, we can set them and use them with out any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove a clean up Jira to remove ozone config for ratis client or leave as it is for existing ones.",pull-request-available teragentest,['Ozone Client'],HDDS,Bug,Major,2020-01-16 15:39:45,2
13279779,Generate only the required keytabs for docker based secure tests,"We have acceptance tests with the help of docker/docker-compose where we generate the keytab files on the fly with the help of a lightweight (unsecure) REST endpoint.

But this generation can be very slow especially when the DNS is slow. When I start a VPN the secure cluster can't be started in the 90 sec period. (>100 keytabs are generated and one keytab generation is ~5 sec).

The solutions is to generate only the *required* keystabs in each of the containers.

Instead of request all the possible keytabs in the *generic* docker-config:
{code:java}
KERBEROS_KEYTABS=dn om scm HTTP testuser testuser2 s3g {code}
We can defined the required keytabs per service (in docker-compose.yaml)
{code:java}
environment:
  KERBEROS_KEYTABS=scm HTTP{code}
With this approach ~20 keytab file will be generated instead of >100 and the secure tests will be significant faster.",pull-request-available,[],HDDS,Improvement,Major,2020-01-16 08:58:12,1
13279695,Handle replay of KeyDelete and KeyRename Requests,"To ensure that key deletion and rename operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyDeleteRequest and OMKeyRenameRequest are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-15 23:05:35,7
13279694,Handle replay of KeyPurge Request,"If KeyPurgeRequest is replayed, we do not want to purge the keys which were created after the original purge request was received. This could happen if a key was deleted, purged and then created and deleted again. The the purge request was replayed, it would purge the key deleted after the original purge request was completed.
Hence, to maintain idempotence, we should only purge those keys from DeletedKeys table that have updateID < transactionLogIndex of the request.
",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-15 23:02:26,7
13279555,Apache NiFi PutFile processor is failing with secure Ozone S3G," 

(1) Create a simple PutS3Object processor in NiFi

(2) The request from NiFi to S3g will fail with HTTP 500

(3) The exception in the s3g log:

 
{code:java}
 s3g_1       | Caused by: java.io.IOException: Couldn't create RpcClient protocol
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:197)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:173)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:74)
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:114)
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)
s3g_1       | 	... 92 more
s3g_1       | Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid S3 identifier:OzoneToken owner=testuser/scm@EXAMPLE.COM, renewer=, realUser=, issueDate=0, maxDate=0, sequenceNumber=0, masterKeyId=0, strToSign=AWS4-HMAC-SHA256
s3g_1       | 20200115T101329Z
s3g_1       | 20200115/us-east-1/s3/aws4_request
s3g_1       | (hash), signature=(sign), awsAccessKeyId=testuser/scm@EXAMPLE.COM{code}
 ",pull-request-available,[],HDDS,Bug,Critical,2020-01-15 10:24:15,1
13279441,Refactor Datanode StateContext to send reports and actions to all configured SCMs,"Currently the StateContext in Datanode has the set of actions and reports to be pushed to SCM(s) in the next Heartbeat. Since the state context is shared across all the configured SCM (& Recon) endpoints, an action or report is only pushed to one SCM server based on FCFS. This needs to be changed to a model every SCM server gets the reports and actions. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-14 21:07:55,5
13279379,Add config to tune replication level of watch requests in Ozone client,"Currently, while sending watch requests in ozone client, it sends watch requests with ratis replication level set to ALL_COMMITTED and in case it fails, it sends the request  with MAJORITY_COMMITTED semantics. The idea is to configure the replication level for watch requests so as to measure performance.",Triaged,['Ozone Client'],HDDS,Bug,Major,2020-01-14 15:13:30,3
13279227,Change the default client settings accordingly with change in default chunk size,"This Jira proposes to change the following values

ozone.client.stream.buffer.flush.size - 64MB -> 16MB

ozone.client.stream.buffer.max.size - 128 MB -> 32MB

ozone.scm.chunk.size - 16MB -> 4MB

 

In this way, the client-side holding buffer size will be reduced, and data is transferred at smaller size intervals.

 

This Jira is to discuss about these config settings changes and change these values accordingly.",pull-request-available,[],HDDS,Bug,Major,2020-01-14 00:14:24,2
13279225,Change the default setting of request.timeout.duration for ratis client and server,"This Jira proposes to change the default value of dfs.ratis.client.request.timeout.duration and dfs.ratis.server.request.timeout.duration.

 

In Teragen testing, we have seen many requests failing with timeout, so this Jira is to change the value of the above config from default 3s -> 10s",Triaged performance,[],HDDS,Bug,Critical,2020-01-14 00:10:53,2
13279033,Legacy (isolated) ozonefs couldn't work in secure environments,"The unsecure ozonefs is tested together with different version of Hadoop (2.7, 3.1, 3.2). The ""legacy"" (isolated) ozonefs package makes it possible to use it together with older Hadoop versions.

But the ozonesecure-mr (testing ozonefs with mapreduce in secure Ozone environment) is executed only with Hadoop 3.2 (which uses the ""current"" (shared) ozonefs instead of the ""legacy"" (isolated))

It turned on that the ""legacy"" ozonefs couldn't work in secure environment which means Ozone can be used only together with Hadoop 3.2 if the Hadoop/Ozone environment is secure

There are multiple problems. The first visible one is the following (run cd compose/ozonesecure-mr && ./test.sh after s/current/legacy/ in docker-config/docker-compose.yaml):
{code:java}
2020-01-13 10:03:39 ERROR OzoneClientFactory:193 - Couldn't create RpcClient protocol exception: 
java.io.IOException: DestHost:destPort om:9862 , LocalHost:localPort rm/172.21.0.10:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at sun.reflect.GeneratedConstructorAccessor2.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy13.submitRequest(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy13.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:394)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1283)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71) {code}",Triaged,[],HDDS,Bug,Critical,2020-01-13 09:58:56,1
13278790,Consolidate ObjectID and UpdateID from Info objects into one class,"We use ObjectID and BucketID in OMVolumeArgs, OMBucketInfo, OMKeyInfo and OMMultipartKeyInfo. We can consolidate by having these Info objects extend a ""WithObjectID"" class which can host the common fields - objectID and updateID.
",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-10 20:40:48,7
13278642,Add a config in ozone to tune max outstanding requests in raft client,"Add a config to tune the config value of ""raft.client.async.outstanding-requests.max"" config in raft client. ",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-01-10 08:13:19,2
13278633,ozone.recon.scm.db.dirs missing from ozone-default.xml,"{{ozone.recon.scm.db.dirs}} is reported by {{TestOzoneConfigurationFields}} to be missing from {{ozone-default.xml}}. If it is to be documented, then please add the property to {{ozone-default.xml}}. If it's a developer-only setting, please add as exception in {{TestOzoneConfigurationFields#addPropertiesNotInXml}}.

(Sorry for reporting this post-commit. {{TestOzoneConfigurationFields}} will be run by CI once we have integration tests enabled again.)",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-01-10 07:34:50,5
13278590,Handle replay of KeyCreate requests,"To ensure that key creation operation is idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyCreateRequest and OMFileCreateRequest are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-10 01:52:34,7
13278546,Add ObjectID and UpdateID to OMKeyInfo,"Similar to OMVolumeInfo and OMBucketInfo, we need objectID and updateID in OMKeyInfo as well to ensure that transactions are idempotent.
This Jira adds objectID and updateID to OMKeyInfo proto and sets these values when creating/ updating keys. ",pull-request-available,[],HDDS,Improvement,Major,2020-01-09 22:05:36,7
13278540,Generic Extensible Token support for Ozone,This is the umbrella Jira to add generic token support across ozone components. I will attach a design spec for review and comments. ,Triaged,['Security'],HDDS,New Feature,Major,2020-01-09 21:43:12,4
13278530,Intermittent failure in TestOzoneManagerRocksDBLogging,"{{TestOzoneManagerRocksDBLogging}} fails intermittently.  I think the problem is that RocksDB-specific string may appear in the log in the {{Finalizer}} thread after after OM is shutdown and restarted in {{Thread-1}}:

{code:title=output}
2020-01-09 16:17:29,372 [Thread-1] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:390] Shutdown: canceling all background work
2020-01-09 16:17:29,373 [Thread-1] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:563] Shutdown complete
2020-01-09 16:17:29,373 [Thread-1] INFO  om.OzoneManager (OzoneManager.java:restart(1127)) - OzoneManager RPC server is listening at localhost/127.0.0.1:33899
...
2020-01-09 16:17:29,561 [Thread-1] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(216)) - HTTP server of ozoneManager listening at http://0.0.0.0:34495
...
2020-01-09 16:17:37,988 [Finalizer] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:390] Shutdown: canceling all background work
2020-01-09 16:17:37,989 [Finalizer] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:563] Shutdown complete
{code}

Swapping the order of tests (ie. running with logging disabled first) would guarantee that stray messages from the logger do not affect it.",pull-request-available,['test'],HDDS,Bug,Minor,2020-01-09 20:24:35,0
13278506,TestOMDbCheckpointServlet fails due to real Recon,"TestOMDbCheckpointServlet verifies that no checkspoints have been taken before it invokes the servlet.  However, the same servlet is used by the actual Recon server in the mini cluster.

{code:title=test failure}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.313 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet
testDoGet(org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet)  Time elapsed: 30.192 s  <<< FAILURE!
java.lang.AssertionError
...
	at org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet.testDoGet(TestOMDbCheckpointServlet.java:155)
{code}

{code:title=output}
2020-01-09 16:13:56,913 [pool-61-thread-1] INFO  impl.OzoneManagerServiceProviderImpl (OzoneManagerServiceProviderImpl.java:syncDataFromOM(325)) - Syncing data from Ozone Manager.
2020-01-09 16:13:56,914 [pool-61-thread-1] INFO  impl.OzoneManagerServiceProviderImpl (OzoneManagerServiceProviderImpl.java:syncDataFromOM(356)) - Obtaining full snapshot from Ozone Manager
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-01-09 18:04:16,0
13278499,BindException in TestSCMRestart,"Recon exposes SCM-like RPC endpoints on (possibly) different port than SCM. However, when the RPC server [updates the config with the actual address|https://github.com/apache/hadoop-ozone/blob/046a06f02783da516179ee8d8d1bed862d22f78d/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeProtocolServer.java#L166-L168] after startup, it does so using the SCM-specific config key. Now that Recon is part of {{MiniOzoneCluster}}, this causes BindException in {{TestSCMRestart}} (and possibly other integration tests):
{code:java|title=output}
2020-01-09 16:07:45,370 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(775)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:36225
...
2020-01-09 16:07:51,594 [main] INFO  scm.ReconStorageContainerManager (ReconStorageContainerManager.java:start(91)) - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:38907
{code}
{code:java|title=test failure}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.653 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart
org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart  Time elapsed: 33.642 s  <<< ERROR!
java.net.BindException: Problem binding to [0.0.0.0:38907]
...
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.startRpcServer(StorageContainerManager.java:579)
	at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.<init>(SCMDatanodeProtocolServer.java:158)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:327)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:212)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:594)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.restartStorageContainerManager(MiniOzoneClusterImpl.java:295)
	at org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.init(TestSCMRestart.java:78)
{code}",pull-request-available,"['Ozone Recon', 'SCM']",HDDS,Bug,Major,2020-01-09 17:46:17,0
13278470,Support Freon progressbar in non-interactive environment,"Freon tests use the ProgressBar to show the current progress of the Freon tests.

 

Progress bar use ""\r"" to print out a nice, interactive progressbar to the same line.

 

Unfortunately this doesn't work very well when the standard output is redirected to a file (or in a containerized environment). I would suggest to use a simplified method using log4j in non-interactive environments.

 

The easiest way to do is to use ""System.console()"" java method. If it returns with null we are in a non interactive environment. We can also increase the logging period from 1s to 10s in case of non-interactive environment (==long term testing).",pull-request-available,[],HDDS,Improvement,Major,2020-01-09 14:41:17,1
13278456,Hugo error should be propagated to build,"Currently build is successful even if {{hugo}} encounters error.  Example:

{code}
Error: Error building site: ""hadoop-hdds/docs/content/start/_index.zh.md:39:119"": unterminated quoted string in shortcode parameter-argument: 'start
'
...
[INFO] BUILD SUCCESS
{code}

The error itself is [being fixed|https://github.com/apache/hadoop-ozone/pull/417/files#diff-3553ca3378ab646dea469e6a1c58c100] in HDDS-2726.  However, it would be nice to make such a Hugo error result in failed build, so that such issues can be caught by CI.",pull-request-available,['build'],HDDS,Bug,Major,2020-01-09 13:56:24,0
13278330,EncryptionInfo should be generated by leader OM,"Right now each OM generates file encryption Info. In OM HA, the leader should generate encryptionInfo, and followers need to use the same information and store it in OM DB.",Triaged,[],HDDS,Bug,Critical,2020-01-09 01:19:07,2
13278250,Ozone Recon fails to start while ozone install,"While trying to install ozone in cdpd cluster, ozone Recon failed to start.

{code}
exec /opt/cloudera/parcels/CDH-7.1.0-1.cdh7.1.0.p0.1728961/lib/hadoop-ozone/../../bin/ozone recon
log4j:ERROR Could not instantiate class [org.cloudera.log4j.redactor.RedactorAppender].
java.lang.ClassNotFoundException: org.cloudera.log4j.redactor.RedactorAppender
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:124)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:785)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
	at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
	at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
	at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
	at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
	at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)
	at org.apache.hadoop.ozone.recon.ReconServer.<clinit>(ReconServer.java:38)
log4j:ERROR Could not instantiate appender named ""redactorForRootLogger"".
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2

{code}",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-08 21:30:06,5
13278247,Recon getContainers API should return a maximum of 1000 containers by default.,"Following the example of OM and SCM, Recon should by default limit the number of containers returned in the getContainers API to 1000. Currently, the default is ALL.

Suggested by [~aengineer]",pull-request-available,[],HDDS,Sub-task,Major,2020-01-08 21:17:52,6
13278054,NPE in OzoneContainer Start,"{code:java}
Unable to communicate to SCM server at ozone-test-bh-3.vpc.cloudera.com:9861 for past 2400 seconds.
 java.io.IOException: java.lang.NullPointerException
 at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
 at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
 at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:70)
 at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:284)
 at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:296)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:433)
 at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:232)
 at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:110)
 at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: java.lang.NullPointerException
 at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.triggerHeartbeat(DatanodeStateMachine.java:384)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.sendPipelineReport(XceiverServerRatis.java:756)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.notifyGroupAdd(XceiverServerRatis.java:737)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.initialize(ContainerStateMachine.java:213)
 at org.apache.ratis.server.impl.ServerState.initStatemachine(ServerState.java:160)
 at org.apache.ratis.server.impl.ServerState.<init>(ServerState.java:112)
 at org.apache.ratis.server.impl.RaftServerImpl.<init>(RaftServerImpl.java:113)
 at org.apache.ratis.server.impl.RaftServerProxy.lambda$newRaftServerImpl$2(RaftServerProxy.java:208)
 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
 ... 3 more{code}",pull-request-available,[],HDDS,Bug,Major,2020-01-08 02:08:19,2
13278017,Some OM Ratis config properties missing from ozone-default.xml,"The following config properties missing from {{ozone-default.xml}} were pointed out by {{TestOzoneConfigurationFields}}:
 * {{ozone.om.ratis.client.request.timeout.duration}}
 * {{ozone.om.ratis.segment.preallocated.size}}",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2020-01-07 21:19:26,2
13277987,Handle Create container use case in Recon.,"CREATE container needs to be handled differently in Recon since these actions are initiated in the SCM, and Recon does not know about that. It should not throw ContainerNotFoundException when it suddenly sees a new container.

The idea is to let Recon ask SCM about the new container whenever it sees a new one.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-07 18:47:40,5
13277802,Add Recon tasks for tracking missing containers (FSCK) and syncing deleted pipelines from SCM.,"* Recon should track the list of containers that have no replicas in its own SQL DB. This information will be used to serve the Missing containers endpoint that returns the list of containers missing along with keys that were part of it.
* If SCM CLI is used to close a pipeline in SCM, Recon does not get any ACK from the Datanode. This patch adds a pipeline sync task in Recon that asks SCM for a list of pipelines and cleans up invalid pipelines.",pull-request-available,[],HDDS,Sub-task,Major,2020-01-06 23:27:41,5
13277800,Handle Datanode Pipeline & Container Reports reports in Recon,"Recon should be able to process Container and Pipeline reports from registered Datanodes. It still wont be able to understand new containers and pipelines though. Along with HDDS-2846, HDDS-2852 and HDDS-2869, Recon will be a fully functional passive SCM.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-06 23:24:23,5
13277742,update ozone to latest ratis,There are some critical fixes that went in latest ratis . Need the changes available in ozone.,pull-request-available,[],HDDS,Bug,Major,2020-01-06 17:13:49,3
13277728,Directly read into ByteBuffer if it has array,{{OzoneFSInputStream#read(ByteBuffer)}} can avoid buffer copy if the target buffer has an array by directly reading into it.,pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2020-01-06 16:10:35,0
13277720,Enable integrations tests for github actions,"When we switched to use github actions the integration tests are disabled due to the flakyness.

We should disable all the flaky tests and enable the remaining integration tests...",pull-request-available,['test'],HDDS,Improvement,Major,2020-01-06 14:48:33,0
13277246,Remove unnecessary sleep in acceptance test,"{{start_docker_env}} has a 10-second sleep after environment is up.  I think it was originally added to give SCM a chance to get out of safe mode after all datanodes are up.  Now that acceptance test has a proper {{wait_for_safemode_exit}} condition, I think the sleep is no longer necessary.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-01-02 17:45:03,0
13277159,Deduplicate KDC docker image definition,Docker image definition and associated files are duplicated between {{ozonesecure}} and {{ozonesecure-mr}} environments.  They should be defined in one place and referenced by both.,pull-request-available,['docker'],HDDS,Improvement,Minor,2020-01-02 08:33:59,0
13277134,Add initial UI of Pipelines in Recon,The Pipelines page in Recon should give the Recon admin users a detailed view of active Ratis Data Pipelines in Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this pipelines page is attached.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-02 05:16:01,6
13277132,Add initial UI of Datanodes in Recon,The Datanodes page in Recon should give the Recon admin users a detailed view of datanodes present in the Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this Datanodes page is attached.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-02 05:13:01,6
13277131,Add initial UI of Dashboard for Recon,The dashboard for Recon should give the Recon admin users an overview of Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this dashboard is attached.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-02 05:11:36,6
13276769,OM Ratis dir creation may fail,"OM Ratis dir creation is attempted from 2 threads:

# Ratis server initializes storage dir in async CompletableFuture
# OM init directly creates it

The latter may encounter the following exception:

{code}
java.lang.IllegalArgumentException: Unable to create path: /github/workspace/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bdc6aaad-da50-4196-ab1e-80aad183d7b2/omNode-2/ratis
 	at org.apache.hadoop.ozone.OmUtils.createOMDir(OmUtils.java:493)
 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:426)
 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:862)
 	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.createOMService(MiniOzoneHAClusterImpl.java:267)
 	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.build(MiniOzoneHAClusterImpl.java:199)
{code}

at:

{code:title=https://github.com/apache/hadoop-ozone/blob/529438a28882b085d1095feccf6c7d6782a6a833/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java#L448-L454}
  public static File createOMDir(String dirPath) {
    File dirFile = new File(dirPath);
    if (!dirFile.exists() && !dirFile.mkdirs()) {
      throw new IllegalArgumentException(""Unable to create path: "" + dirFile);
    }
    return dirFile;
  }
{code}

It may happen that {{exists()}} returns {{false}} because the dir is does not exist yet, but {{mkdirs()}} also returns {{false}} because the dir is created in the meantime by the Ratis thread.",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-12-29 14:27:13,0
13275975,Use try-with-resources or close resource in finally block,"In several files, resources are not closed in ""finally"" block.

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5rBvceSc1R0gwbF9Y1&resolved=false&severities=BLOCKER&types=BUG] 

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5rBvc0Sc1R0gwbF9ZC&resolved=false&severities=BLOCKER&types=BUG]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-c9KcVY8lQ4Zr3s&resolved=false&severities=BLOCKER&types=BUG]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-c9KcVY8lQ4Zr3r&resolved=false&severities=BLOCKER&types=BUG]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-cqKcVY8lQ4Zr3N&resolved=false&severities=BLOCKER&types=BUG]",pull-request-available sonar,[],HDDS,Bug,Major,2019-12-23 03:49:37,6
13275763,ITestOzoneContractSeek zero byte file failures,"{{ITestOzoneContractSeek}} fails at {{testReadFullyZeroByteFile}} and {{testSeekZeroByteFile}} with:

{code}
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:657)
	at java.util.ArrayList.get(ArrayList.java:433)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.seek(KeyInputStream.java:241)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.seek(OzoneFSInputStream.java:65)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65)
{code}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-12-20 17:03:13,0
13275704,Move ozonefs and tools minicluster tests to integration-test,"{{ozonefs}} and {{tools}} modules in {{hadoop-ozone}} have a mix of unit and integration tests.  This issue proposes to

# switch dependency order: let {{integration-test}} depend on these modules instead of the other way around
# move integration tests (those that use {{Mini*Cluster}}) from these modules to {{integration-test}}
# let {{unit}} check run remaining tests in these modules

This improves code coverage in CI.",pull-request-available,['test'],HDDS,Improvement,Major,2019-12-20 10:48:26,0
13275694,Read to ByteBuffer uses wrong offset,"{{OzoneFSInputStream#read(ByteBuffer)}} uses the target buffer's position for offsetting into the temporary array:

{code:title=https://github.com/apache/hadoop-ozone/blob/b834fa48afef4ee4c73577c7af564e1e97cb9d5b/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSInputStream.java#L90-L97}
  public int read(ByteBuffer buf) throws IOException {

    int bufInitPos = buf.position();
    int readLen = Math.min(buf.remaining(), inputStream.available());

    byte[] readData = new byte[readLen];
    int bytesRead = inputStream.read(readData, bufInitPos, readLen);
    buf.put(readData);
{code}

Given a buffer with capacity=10 and position=8, this results in the following:

 * {{readLen}} = 2 => {{readData.length}} = 2
 * {{bufInitPos}} = 8

So {{inputStream}} reads 2 bytes and writes it into {{readData}} starting at offset 8, which results in an {{IndexOutOfBoundsException}}.

offset should always be 0, since the temporary array is sized exactly for the length to read, and it has no extra data at the start.",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-12-20 09:47:37,0
13275594,Add ObjectID and updateID to BucketInfo to avoid replaying transactions,"This Jira has 2 objectives:
1. Add objectID and updateID to BucketInfo proto persisted to DB.
2. To ensure that bucket operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-12-19 20:55:10,7
13275187,Hadoop 3.1 acceptance test fails with apk command not found,"{{ozone-mr/hadoop31}} test is failing with:

{code}
sudo: apk: command not found
{code}

New hadoop:3.1.2 image is based on CentOS, not Alpine.",pull-request-available,['test'],HDDS,Bug,Critical,2019-12-18 08:13:53,0
13275167,Configure Goldmark renderer,"Starting with Hugo 0.60, the new Goldmark renderer is configured to skip HTML fragments.  This breaks the doc layout in a few places, eg.:

 * _Easy start_, _Recommended_ etc. headers
 * tables
 * warning {{div}}",pull-request-available,['documentation'],HDDS,Improvement,Major,2019-12-18 07:08:02,0
13275103,Handle Datanode registration and SCM Node management in Recon.,"* Datanode should use Recon address to register and heartbeat to it.
* Recon will act as a ""passive"" SCM to the Datanode. Here ""passive"" means that the Datanode will not get any commands from this Recon SCM.
* Integrating SCM Node Manager into Recon.",pull-request-available,[],HDDS,Sub-task,Major,2019-12-17 21:59:42,5
13274817,Test OM Volume and Bucket operations are idempotent,"On OM restarts, it is possible that already applied ratis logs will be replayed. Hence, we need to ensure that all OM write operations are idempotent.
This Jira aims to add unit tests for testing that OM volume and bucket related operations are idempotent. 
",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-12-16 18:36:37,7
13274622,Unnecessary calls to isNoneEmpty and isAllEmpty,{{isNoneEmpty}} and {{isAllEmpty}} check variable number of strings.  For single string they can be replaced with {{isNotEmpty}} and {{isEmpty}}.,pull-request-available,['S3'],HDDS,Bug,Trivial,2019-12-15 09:36:07,0
13274620,Wrong number of placeholders in log message,Fix log messages where number of placeholders does not match the number of parameters provided.,pull-request-available,[],HDDS,Bug,Major,2019-12-15 09:16:56,0
13274568,No need to try install awscli before each test,S3 acceptance test attempts to install {{awscli}} prior to each test case.  It is enough to do so before each suite.,pull-request-available,['test'],HDDS,Improvement,Minor,2019-12-14 13:44:52,0
13274546,Let GitHub Actions run acceptance check in parallel,"Currently GitHub Actions workflows are configured to run all checks in parallel, except acceptance test.  The rationale is that acceptance test takes the most time, and there is no reason to run it if a cheaper check catches some problem.

I propose to let GitHub Actions run acceptance test in parallel to address the following concerns:

# Although acceptance test is the slowest (~60 minutes), unit test also takes quite some time (~20-25 minutes).  Serializing these two checks increases the time to get feedback on PRs and commits by ~33-40%.
# For PRs and post-commit builds in forks, running all checks regardless of the result of independent checks allows authors to reduce the number of rounds they need to address any problems.
# For post-commit builds in Apache master, we expect all checks to pass.  However, checks sometime fail eg. due to transient network errors.  Skipping acceptance test due to such a problem in another check provides no benefit.",pull-request-available,['build'],HDDS,Improvement,Minor,2019-12-14 08:02:40,0
13274407,Enable multilingual Hugo features in ozone docs,"We need to reconfigure hugo to support multilingual site:

https://gohugo.io/content-management/multilingual/",pull-request-available,['documentation'],HDDS,Sub-task,Major,2019-12-13 12:42:38,1
13274404,Remove methods of internal representation from DatanodeAdminMontor interface ,"# DatanodeAdminMonitorInterface has some setter and getters but they are not part of the contract they are used by the implementation. An other implementation of the interface may require different setters. Therefore the interface should contain only the required fields.
 # DatanodeAdminMonitorInterface can be renamed to DatanodeAdminMonitor to follow the naming convention in the code (*Impl)
 # PipelineManager is unused field can be removed from DatanodeAdminMonitorImpl",pull-request-available,[],HDDS,Sub-task,Major,2019-12-13 12:21:29,1
13274224,Let ChunkManager read/write ChunkBuffer instead of ByteBuffer,Change {{ChunkManager}} read/write methods to accept/return {{ChunkBuffer}} instead of {{ByteBuffer}}.  This allows seamlessly passing multiple buffers without further interface change.,pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-12-12 18:53:36,0
13274223,Allow wrapping list of ByteBuffers with ChunkBuffer,"{{ChunkBuffer}} is a useful abstraction over {{ByteBuffer}} to hide whether it's backed by a single {{ByteBuffer}} or multiple ones ({{IncrementalChunkBuffer}}).  However, {{IncrementalChunkBuffer}} allocates its own {{ByteBuffer}} instances and only works with uniform buffer sizes.  The goal of this task is to allow wrapping an existing  {{List<ByteBuffer>}} in {{ChunkBuffer}}.",pull-request-available,[],HDDS,Improvement,Major,2019-12-12 18:51:16,0
13274084,Handle chunk increments in datanode,Let datanode handle incremental additions to chunks (data with non-zero offset).,pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-12-12 09:44:53,0
13273948,OM does not report JVM metrics,"JVM metrics are available from Datanode and SCM, but not from OM.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-12-11 19:30:56,0
13273672,Use Ozone specific LICENCE and NOTICE in the root of the git repo,"We have three places where we need the LICENSE/NOTICE files:

 
 #  In the binary tar file (binary specific license)
 #  In the src tar file (source specific license)
 # In the root of the git repository

1 and 2 are fine now (we have dedicated file under hadoop-ozone/dist) but as we have shared the repository with main hadoop the root LICENSE/NOTICE (3) still contains the hadoop specific content.

We need to use the files from (2) for (3).",pull-request-available,[],HDDS,Bug,Blocker,2019-12-10 17:27:06,1
13273669,Source tar file is not created during the relase build,"Thanks to [~dineshchitlangia] who reported this problem.

With a release build:
{code:java}
mvn clean install -Dmaven.javadoc.skip=true -DskipTests -Psign,dist,src -Dtar -Dgpg.keyname=$CODESIGNINGKEY {code}
The source package (*the* release) is not created.

In fact it's created, but the problem with the order of clean and install:
 * clean is executed in the root project
 * install is executed in the root project (creates hadoop-ozone/dist/target/..src.tar.gx
 * .....
 * clean is executed in the hadoop-ozone/dist project *(here the src package is deleted)*
 * install is executed in the hadoop-ozone/dist project

One possible fix is to move the creation of the src package to the hadoop-ozone/dist project (but do it from the project root)",pull-request-available,[],HDDS,Bug,Blocker,2019-12-10 17:13:22,1
13273596,Client failed to recover from ratis AlreadyClosedException exception,"Run teragen, and it failed to enter the mapreduce stage and print the following warnning message on console endlessly. 


{noformat}

19/12/10 19:23:54 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-FBD45C9313A5->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0
19/12/10 19:23:54 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}
19/12/10 19:26:16 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-7C5A7B5CFC31->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0
19/12/10 19:26:16 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}
19/12/10 19:28:38 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-B3D8C0052C4E->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0
19/12/10 19:28:38 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}

{noformat}




",TriagePending,['Ozone Client'],HDDS,Bug,Blocker,2019-12-10 11:47:48,3
13273334,HddsVolume#readVersionFile fails when reading older versions,"{{HddsVolume#layoutVersion}} is a version number, supposed to be used for handling upgrades from older versions.  Currently only one version is defined.  But should a new version be introduced, HddsVolume would fail to read older version file.  This is caused by a check in {{HddsVolumeUtil}} that only considers the latest version as valid:

{code:title=https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java#L137-L153}
  /**
   * Returns layOutVersion if it is valid. Throws an exception otherwise.
   */
  @VisibleForTesting
  public static int getLayOutVersion(Properties props, File versionFile) throws
      InconsistentStorageStateException {
    String lvStr = getProperty(props, OzoneConsts.LAYOUTVERSION, versionFile);

    int lv = Integer.parseInt(lvStr);
    if(DataNodeLayoutVersion.getLatestVersion().getVersion() != lv) {
      throw new InconsistentStorageStateException(""Invalid layOutVersion. "" +
          ""Version file has layOutVersion as "" + lv + "" and latest Datanode "" +
          ""layOutVersion is "" +
          DataNodeLayoutVersion.getLatestVersion().getVersion());
    }
    return lv;
  }
{code}

I think this should check whether the version number identifies a known {{DataNodeLayoutVersion}}.",Triaged upgrade,['Ozone Datanode'],HDDS,Improvement,Critical,2019-12-09 11:42:47,5
13273328,HddsVolume mixes ChunkLayOutVersion and DataNodeLayoutVersion,"{{HddsVolume}} [initializes {{layoutVersion}} using latest {{ChunkLayOutVersion}}|https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java#L268].  But when writing the same info to file, it [verifies {{layoutVersion}} matches the latest {{DataNodeLayoutVersion}}|https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java#L292-L293].  To satisfy the condition {{ChunkLayOutVersion}} and {{DataNodeLayoutVersion}} have to be in sync, which means only one of them is necessary.  I think the intention was to use {{DataNodeLayoutVersion}} in both cases, as {{ChunkLayOutVersion}} is for key-value container internal structure.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-12-09 11:11:58,0
13272954,Ozone Shell code cleanup,Sonar reports lots of duplication around {{AclHandler}} classes in Ozone Shell.,pull-request-available,[],HDDS,Improvement,Major,2019-12-07 16:54:33,0
13272939,OMException NOT_A_FILE missing space in the exception message,"{code:java}
ayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ./ozone fs -fs o3fs://bucket.hive -put -d ozone /dir/ozone/file
put: NOT_A_FILE: Can not create file: dir/ozone/fileas there is already file in the given path
{code}

""as"" got attached to the name of the file.

OMFileCreateRequest L211 and L246 : Need to add space before ""as""",newbie pull-request-available,[],HDDS,Bug,Trivial,2019-12-07 13:23:19,5
13272909,Refactor container response builders to hdds-common,"{{ContainerUtils}} and {{BlockUtils}} have some helper functions to build responses to container commands.  These would be useful for client-side unit tests, but {{client}} does not depend on {{container-service}} since the interfaces and messages it needs are defined in {{common}}.  This issue proposes to move these helpers to {{common}} to avoid duplicating the functionality for tests.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-12-07 10:05:01,0
13272880,Fix sonar issues in package org.apache.hadoop.ozone.recon.api.,Fix all sonar issues in package org.apache.hadoop.ozone.recon.api.,pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-12-07 00:34:30,5
13272728,Refactor common test utilities to hadoop-hdds/common,"Expose test code from {{hadoop-hdds/common}} to other modules.  Move some ""common"" test utilities.  Example: random {{DatanodeDetails}} creation.",pull-request-available,['test'],HDDS,Improvement,Major,2019-12-06 11:45:13,0
13272609,Fix updating lastAppliedIndex in OzoneManagerStateMachine,"This Jira is to fix a bug that has been caused by HDDS-2637.

Now we have applyTransactionMap where we put all entries of ratisTransactionIndex before complete.

 

And when updating lastAppliedIndex, now there is a chance that we updateLastAppliedIndex before DoubleBuffer flush has committed transactions to DB.

 

 

Let's take 1-10 are apply transaction entries which are not flushed, 11th is indexUpdate a metadata/conf entry transaction, now when notifyIndexUpdate is called with 11, we updateLastAppliedIndex to 11. (Which we should not do) This Jira is to fix this issue.",pull-request-available,[],HDDS,Bug,Major,2019-12-05 22:32:02,2
13272507,Add thread name to log pattern,"Ozone's default log4j patterns should include thread name, as it helps a bit in understanding events.",pull-request-available,[],HDDS,Improvement,Minor,2019-12-05 13:43:13,0
13272505,Acceptance test may fail despite success status,"Found this in a local acceptance test run:

{code}
Start freon testing                                                   | FAIL |
'2019-12-05 12:25:24,744 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2019-12-05 12:25:24,934 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-12-05 12:25:24,935 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2019-12-05 12:25:26,690 [main] INFO freon.RandomKeyGenerator: Number of Threads: 1
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Volumes: 5.
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Buckets per Volume: 5.
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Keys per Bucket: 5.
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Key size: 10240 bytes
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Buffer size: 4096 bytes
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: validateWrites : false
    [ Message content over the limit has been removed. ]
....util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

***************************************************
Status: Success
Git Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf
Number of Volumes created: 5
Number of Buckets created: 25
Number of Keys added: 125
Ratis replication factor: THREE
Ratis replication type: RATIS
Average Time spent in volume creation: 00:00:00,210
Average Time spent in bucket creation: 00:00:00,213
Average Time spent in key creation: 00:00:37,506
Average Time spent in key write: 00:01:42,157
Total bytes written: 1280000
Total Execution time: 00:02:31,516
***************************************************' contains 'ERROR'
{code}

Need to check if {{Status: Success}} is true (ie. if keys were indeed successfully created), and if so, {{ERROR}} in output should make not the test failed.",pull-request-available,"['freon', 'test']",HDDS,Bug,Minor,2019-12-05 13:39:16,0
13272398,Prometheus reports invalid metric type,"Promethues version 2.14.0

configuration set in promethues.yml, in which 10.120.110.183 is the master with OM and SCM. All others are datanodes.

{code}
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'ozone'
    metrics_path: /prom

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets:
      - ""10.120.110.183:8080""
      - ""10.120.110.183:8081""
      - ""10.120.139.122:9882""
      - ""10.120.139.111:9882""
      - ""10.120.113.172:9882""
      - ""10.121.124.44:9882""
{code}

The UP datanoe 122 is not part of the THREE factor pipeline.  The later is formed by the other 3 datanodes which are all DOWN. ",pull-request-available,[],HDDS,Bug,Major,2019-12-05 06:41:32,0
13272226,Use field based Config annotation instead of method based,"HDDS-2413 proposes an additional usage of the @Config annotation: to set configuration based on an existing configuration class.

But as of now we annotate the setters instead of the fields. To avoid annotation duplication (we need to read the values from the getters or the fields) I propose to switch to use field based annotations instead of setter based annotation.

I think it's more readable and additional validation (even the class level validations) can be done in a @PostConstruct method.",pull-request-available,[],HDDS,Improvement,Major,2019-12-04 13:22:19,1
13272190,Create insight point for datanode container protocol,The goal of this task is to create a new insight point for the datanode container protocol ({{HddsDispatcher}}) to be able to debug {{client<->datanode}} communication.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-12-04 11:25:41,1
13271888,Insight log level reset does not work,"{{ozone insight log}} command changes log level to debug or trace.  After streaming is stopped, it attempts to reset to info.  This does not seem to work, probably because the process is abruptly stopped (Ctrl-C).",pull-request-available,['Tools'],HDDS,Bug,Major,2019-12-03 08:11:20,0
13271626,Prefer execute() over submit() if the returned Future is ignored,"{{Future submit(Runnable)}} and {{void execute(Runnable)}} in {{ExecutorService}} have the same result.  If the returned {{Future}} is ignored, {{execute}} can be used instead of {{submit}} to avoid creating some objects.",pull-request-available,[],HDDS,Improvement,Minor,2019-12-02 07:37:05,0
13271520,Use pre-compiled Pattern in NetUtils#normalize,"{{NetUtils#normalize}} uses {{String#replaceAll}}, which creates a {{Pattern}} for each call.  It could be replaced with a pre-compiled {{Pattern}}.",pull-request-available,[],HDDS,Improvement,Minor,2019-11-30 19:13:11,0
13271512,Conditionally enable profiling at the kernel level,"Extend {{entrypoint.sh}} to set the kernel parameters required for profiling if the {{ASYNC_PROFILER_HOME}} environment variable (used by {{ProfileServlet}}) is set.

Ref:

{code:title=https://cwiki.apache.org/confluence/display/HADOOP/Java+Profiling+of+Ozone}
echo 1 > /proc/sys/kernel/perf_event_paranoid
echo 0 > /proc/sys/kernel/kptr_restrict
{code}",pull-request-available,['docker'],HDDS,Improvement,Minor,2019-11-30 17:04:27,0
13271503,Improve executor memory usage in new Freon tests,"New Freon tests (descendants of {{BaseFreonGenerator}}) suffer from a similar memory issue due to concurrency handling as HDDS-1785.

Steps to reproduce:

{code}
export HADOOP_OPTS='-Xmx256M -XX:+HeapDumpOnOutOfMemoryError'
ozone freon omkg -F ONE -n 33554432 -t 10 -p omkg
{code}

Freon attempts to submit 32M tasks to the executor, requiring at least 1.5GB memory.",pull-request-available,['freon'],HDDS,Improvement,Major,2019-11-30 14:33:39,0
13271333,TestOzoneManagerDoubleBufferWithOMResponse,"The test is flaky:

 

Example run: [https://github.com/apache/hadoop-ozone/runs/325281277]

 

Failure:
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
-------------------------------------------------------------------------------
Tests run: 3, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 5.31 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
testDoubleBufferWithMixOfTransactionsParallel(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 0.282 s  <<< FAILURE!
java.lang.AssertionError: expected:<32> but was:<29>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.junit.Assert.assertEquals(Assert.java:542)
        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBufferWithMixOfTransactionsParallel(TestOzoneManagerDoubleBufferWithOMResponse.java:247)
 {code}",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Blocker,2019-11-29 08:04:04,2
13271258,Start acceptance tests only if at least one THREE pipeline is available,"After HDDS-2034 (or even before?) pipeline creation (or the status transition from ALLOCATE to OPEN) requires at least one pipeline report from all of the datanodes. Which means that the cluster might not be usable even if it's out from the safe mode AND there are at least three datanodes.

It makes all the acceptance tests unstable.

For example in [this|https://github.com/apache/hadoop-ozone/pull/263/checks?check_run_id=324489319] run.
{code:java}
scm_1         | 2019-11-28 11:22:54,401 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode 548f146f-2166-440a-b9f1-83086591ae26
scm_1         | 2019-11-28 11:22:54,402 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c
scm_1         | 2019-11-28 11:22:54,404 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode 47dbb8e4-bbde-4164-a798-e47e8c696fb5
scm_1         | 2019-11-28 11:22:54,405 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 8dc4aeb6-5ae2-46a0-948d-287c97dd81fb, Nodes: 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}47dbb8e4-bbde-4164-a798-e47e8c696fb5{ip: 172.24.0.2, host: ozoneperf_datanode_2.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED]
scm_1         | 2019-11-28 11:22:56,975 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:22:58,018 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:23:01,871 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:23:02,817 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:23:02,847 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null} {code}
As you can see the pipeline is created but the the cluster is not usable as it's not yet reporter back by datanode_2:
{code:java}
scm_1         | 2019-11-28 11:23:13,879 WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Retrying get pipelines c
all once.
scm_1         | org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 0 nodes.{code}
 The quick fix is to configure all the compose clusters to wait until one pipeline is available. This can be done by adjusting the number of the required datanodes:
{code:java}
// We only care about THREE replica pipeline
int minHealthyPipelines = minDatanodes /
    HddsProtos.ReplicationFactor.THREE_VALUE; {code}
 ",pull-request-available,[],HDDS,Bug,Blocker,2019-11-28 16:01:00,1
13271173,TestTableCacheImpl is flaky,"Run(master): [https://github.com/apache/hadoop-ozone/runs/324342299]

 
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl
-------------------------------------------------------------------------------
Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.955 s <<< FAILURE! - in org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl
testPartialTableCacheWithOverrideAndDelete[0](org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl)  Time elapsed: 0.039 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<6>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl.testPartialTableCacheWithOverrideAndDelete(TestTableCacheImpl.java:308)

 {code}
*How to reproduce it locally?*

Replace the last tableCache.evict call of testPartialTableCacheWithOverrideAndDelete to System.out.println(tableCache.size()).

You will see that the cache size is 2 even before the cleanup therefore the next GeneriTestUtils.waitFor is useless (it doesn't guarantee that the cleanup is finished).

*Fix:*

I propose to call the cleanup sync with using the Impl class instead of the interface. It simplifies the test but still validates the behavior.",pull-request-available,[],HDDS,Bug,Major,2019-11-28 10:13:36,1
13271103,Handle LeaderNot ready exception in OzoneManager StateMachine and upgrade ratis to latest version.,This Jira is to handle LeaderNotReadyException in OM and also update to latest ratis version.,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-27 22:05:55,2
13270983,Let findbugs.sh skip frontend plugin for Recon,Findbugs/Spotbugs only checks Java code.  We can skip frontend plugin execution for Recon to save ~2 minutes.  Makes a difference mostly when running it locally.,pull-request-available,"['build', 'test']",HDDS,Improvement,Minor,2019-11-27 12:59:08,0
13270961,Fix TestContainerPersistence#testDeleteChunk,"{{TestContainerPersistence#testDeleteChunk}} is failing due to unexpected exception message.  This is caused by mix of two commits:

 * https://github.com/apache/hadoop-ozone/commit/fe7fccf2b changed actual message
 * https://github.com/apache/hadoop-ozone/commit/4a9174500 moved the test case from integration to unit

Each of these was tested without the other.",pull-request-available,['test'],HDDS,Bug,Minor,2019-11-27 11:23:09,0
13270902,NullPointerException in S3g,See attached log file,pull-request-available,['S3'],HDDS,Bug,Critical,2019-11-27 07:43:34,2
13270898,Ozone CLI: CreationTime/modifyTime of volume/bucket/key info are not formatted,"There is a regression from HDDS-738, where the time stamp formatting was lost during the json output. (Detail can be found from the removed methods from OzoneClientUtils#asVolumeInfo, asBucketInfo, asKeyInfo). As a result, only raw time is output in the json output for volume/bucket/key info. 
{code:java}
bash-4.2$ ozone sh vol create /vol1
obash-4.2$ ozone sh vol info /vol1
{
 ""metadata"" : { },
 ""name"" : ""vol1"",
 ""admin"" : ""hadoop"",
 ""owner"" : ""hadoop"",
 ""creationTime"" : 1574836577298,
 ""acls"" : [ {
 ""type"" : ""USER"",
 ""name"" : ""hadoop"",
 ""aclScope"" : ""ACCESS"",
 ""aclList"" : [ ""ALL"" ]
 }, {
 ""type"" : ""GROUP"",
 ""name"" : ""users"",
 ""aclScope"" : ""ACCESS"",
 ""aclList"" : [ ""ALL"" ]
 } ],
 ""quota"" : 1152921504606846976
}{code}",pull-request-available,[],HDDS,Bug,Major,2019-11-27 06:54:52,4
13270874,Make AuditMessage parameters strongly typed,Improve type safety in {{AuditMessage$Builder}} for methods {{forOperation}} and {{withResult}} by using existing {{interface AuditAction}} and {{enum AuditEventStatus}} respectively instead of Strings.,pull-request-available,[],HDDS,Improvement,Minor,2019-11-27 03:45:55,0
13270320,Skip sonar check in forks,"_unit_ step of Github Actions-based CI checks is failing for commits pushed to forks due to lack of {{SONARCLOUD_TOKEN}}.

Background: HDDS-2587 added Sonar check in post-commit workflow, publishing results to SonarCloud.  It does not work in forks, as it requires SonarCloud token.  This causes _unit_ step to fail completely.  Example: https://github.com/bharatviswa504/hadoop-ozone/runs/316829850",pull-request-available,['build'],HDDS,Bug,Major,2019-11-24 17:26:07,0
13270317,Avoid hostname lookup for invalid local IP addresses,"{{OzoneSecurityUtil#getValidInetsForCurrentHost}} performs hostname lookup for all local network interfaces, even for invalid addresses.  This significantly slows down some secure tests ({{TestHddsSecureDatanodeInit}}, {{TestSecureOzoneCluster}}) when run on a machine with special IPv6 network interfaces due to timeout reaching IPv6 DNS servers.

This issue proposes to disable the lookup for invalid addresses.",pull-request-available,[],HDDS,Improvement,Minor,2019-11-24 15:51:13,0
13270289,Expose SCMDatanodeProtocolServer RPC endpoint through Recon.,SCM should expose the 'SCMDatanodeProtocol' RPC protocol server as an endpoint. This is the first step in sending DN reports to Recon.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-11-24 05:51:18,5
13270185,Add config parameter for setting limit on total bytes of pending requests in Ratis,RATIS-714 introduced a config setting for limiting the max number of bytes of pending requests. This Jira aims to add a config in Ozone to set the same in DN Ratis.,pull-request-available,[],HDDS,Bug,Major,2019-11-22 22:00:16,7
13270180,Enable OM HA acceptance tests,OM HA robot tests were disable in HDDS-2533 as they were failing intermittently. HDDS-2454 fixes some issues in the HA tests. Creating this Jira so as to run re-enable the HA acceptance tests.,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-22 20:32:41,7
13270163,Fix listMultipartupload API,"This Jira is to fix listMultiparts API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listParts of a MPU key, it should use both in-memory cache and rocksdb mpu table to list parts of a mpu key.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-11-22 19:20:37,2
13270019,Fix Sonar issues in ReconTaskControllerImpl,https://sonarcloud.io/project/issues?directories=hadoop-ozone%2Frecon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Frecon%2Ftasks&id=hadoop-ozone&open=AW5md-jJKcVY8lQ4Zr9D&resolved=false,pull-request-available sonar,['Ozone Recon'],HDDS,Bug,Major,2019-11-22 05:47:40,5
13269884,Move plain unit tests out of integration tests,"Some test classes in {{integration-test}} are actually unit tests: they do not start a mini cluster, nor even multiple components.  These can be moved to the subprojects they belong to (eg. {{container-service}}.  The benefit is that it will be easier to spot if they are broken, since integration tests are executed less frequently.",pull-request-available,['test'],HDDS,Improvement,Major,2019-11-21 16:01:41,0
13269847,Provide command to wait until SCM is out from the safe-mode,"The safe mode can be checked with ""ozone scmcli safemode status"". But for acceptance tests there is no easy way to check if the cluster is ready to execute the tests (See HDDS-2606 for example).

One easy solution is to create a polling version from ""safemode status"".

""safemode wait --timeout ..."" can be blocked until the scm is out from the safe mode.

Wit proper safe mode rules (min datanodes + min pipline numbers) it can help us to check if the acceptance tests are ready to test.

Same command can be used in k8s as well to test if the cluster is ready to start the freon commands...",pull-request-available,['Tools'],HDDS,Improvement,Major,2019-11-21 13:31:37,1
13269798,Acceptance tests are flaky due to async pipeline creation,"Some of the acceptance tests are failing because the first Ratis.THREE pipeline is not created on time:

For example in a HA proxy acceptance test the first block allocation is tarted before moving the Ratis.THREE pipeline to OPEN state.
{code:java}
scm_1       | 2019-11-20 14:45:38,359 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 4d27f405-d257-4b1b-a7b3-51bbd57356db, Nodes: 2c010ef7-8c0e-45e3-b230-326bf759773b{ip: 172.25.0.7, host: ozones3-haproxy_datanode_2.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] moved to OPEN state
scm_1       | 2019-11-20 14:45:46,944 WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Retrying get pipelines call once.
scm_1       | org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 0 nodes.
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:153)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:58)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:155)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:198)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:187)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:159)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:117)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:98)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2019-11-20 14:45:46,944 INFO block.BlockManagerImpl: Could not find available pipeline of type:RATIS and factor:THREE even after retrying
scm_1       | 2019-11-20 14:45:46,945 ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2019-11-20 14:45:49,274 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: e208588c-9a16-4519-89dc-7bad5bae4331, Nodes: 1da10d32-12be-4328-bc0a-f3d8de21b056{ip: 172.25.0.8, host: ozones3-haproxy_datanode_3.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}d9edb776-ee02-48a1-8c73-40f33bc0d128{ip: 172.25.0.5, host: ozones3-haproxy_datanode_1.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}2c010ef7-8c0e-45e3-b230-326bf759773b{ip: 172.25.0.7, host: ozones3-haproxy_datanode_2.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] moved to OPEN state {code}",TriagePending,['SCM'],HDDS,Bug,Major,2019-11-21 10:44:29,1
13269794,Use LongSupplier to avoid boxing,"Freon's {{ProgressBar}} uses {{Supplier<Long>}}, which could be replaced with {{LongSupplier}} to avoid boxing.",pull-request-available,['freon'],HDDS,Improvement,Trivial,2019-11-21 10:25:58,0
13269787,Avoid unnecessary boxing in XceiverClientReply,{{XceiverClientReply#logIndex}} is unnecessarily boxed/unboxed.,pull-request-available,[],HDDS,Improvement,Minor,2019-11-21 10:01:55,0
13269732,Move chaos test to org.apache.hadoop.ozone.chaos package,This is a simple refactoring change where all the chaos test are moved to  org.apache.hadoop.ozone.chaos package,pull-request-available,['test'],HDDS,Bug,Major,2019-11-21 04:39:56,5
13269699,S3 RangeReads failing with NumberFormatException," 
{code:java}
2019-11-20 15:32:04,684 WARN org.eclipse.jetty.servlet.ServletHandler:
javax.servlet.ServletException: java.lang.NumberFormatException: For input string: ""3977248768""
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)
        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1780)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1609)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.eclipse.jetty.server.Server.handle(Server.java:539)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
{code}
 ",pull-request-available,[],HDDS,Bug,Major,2019-11-21 00:05:51,2
13269665,No tailMap needed for startIndex 0 in ContainerSet#listContainer,"{{ContainerSet#listContainer}} has this code:

{code:title=https://github.com/apache/hadoop-ozone/blob/3c334f6a7b344e0e5f52fec95071c369286cfdcb/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerSet.java#L198}
map = containerMap.tailMap(containerMap.firstKey(), true);
{code}

This is equivalent to:

{code}
map = containerMap;
{code}

since {{tailMap}} is a sub-map with all keys larger than or equal to ({{inclusive=true}}) {{firstKey}}, which is the lowest key in the map.  So it is a sub-map with all keys, ie. the whole map.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-20 20:05:50,0
13269613,Consolidate compose environments,"There are a few slightly different sample docker compose environments: ozone, ozoneperf, ozones3, ozone-recon. This issue proposes to merge these 4 by minor additions to ozoneperf:

 # add {{recon}} service from {{ozone-recon}}
 # run GDPR and S3 tests
 # expose datanode web port (eg. for profiling)

Plus: also run ozone-shell test (from basic suite), which is currently run only in ozonesecure

I also propose to rename {{ozoneperf}} to {{ozone}} for simplicity.

Consolidating these 4 environments would slightly reduce both code duplication and the time needed for acceptance tests.

CC [~elek]",pull-request-available,['docker'],HDDS,Improvement,Major,2019-11-20 15:24:52,0
13269563,Enable sonarcloud measurement as part of CI builds,"As sonarcloud has been started to use, it would be great to upload measurement data from github actions steps...",pull-request-available,[],HDDS,Improvement,Major,2019-11-20 10:28:38,1
13269503,Ensure resources are closed in Get/PutKeyHandler,"Use try-with-resources or close this ""FileOutputStream"" in a ""finally"" clause.

GetKeyHandler: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6HHKTfdBVcJdcVFsvC&open=AW6HHKTfdBVcJdcVFsvC]

 

Use try-with-resources or close this ""OzoneOutputStream"" in a ""finally"" clause.

PutKeyHandler: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6HHKRodBVcJdcVFsvB&open=AW6HHKRodBVcJdcVFsvB]

 ",newbie pull-request-available sonar,[],HDDS,Bug,Major,2019-11-20 04:48:26,0
13269501,Ozone client should refresh pipeline info if reads from all Datanodes fail.,"Currently, if the client reads from all Datanodes in the pipleine fail, the read fails altogether. There may be a case when the container is moved to a new pipeline by the time client reads. In this case, the client should request for a refresh pipeline from OM, and read it again if the new pipeline returned from OM is different. 

This behavior is consistent with that of HDFS.
cc [~msingh] / [~shashikant] / [~hanishakoneru]",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-11-20 04:41:57,5
13269498,Handle InterruptedException in OzoneManagerProtocolServerSideTranslatorPB,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-Z7KcVY8lQ4Zr1l&open=AW5md-Z7KcVY8lQ4Zr1l,newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:31:20,5
13269494,Handle InterruptedException in KeyOutputStream,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-m5KcVY8lQ4ZsAc&open=AW5md-m5KcVY8lQ4ZsAc,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:25:22,5
13269492,Handle InterruptedException in SCMPipelineManager,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6BMuREm2E_7tGaNiTh&open=AW6BMuREm2E_7tGaNiTh]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:23:32,5
13269490,Handle InterruptedException in ProfileServlet,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-x8KcVY8lQ4ZsIJ&open=AW5md-x8KcVY8lQ4ZsIJ,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:22:32,5
13269479,Handle InterruptedException in Scheduler,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-0nKcVY8lQ4ZsLH&open=AW5md-0nKcVY8lQ4ZsLH]

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:11:28,6
13269478,Handle InterruptedException in BackgroundService,"Fix 2 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-02KcVY8lQ4ZsLU&open=AW5md-02KcVY8lQ4ZsLU]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-02KcVY8lQ4ZsLV&open=AW5md-02KcVY8lQ4ZsLV]

 

 

 

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:10:00,6
13269477,Handle InterruptedException in XceiverClientSpi,"Fix 2 instances:

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2aKcVY8lQ4ZsNW&open=AW5md-2aKcVY8lQ4ZsNW]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2aKcVY8lQ4ZsNX&open=AW5md-2aKcVY8lQ4ZsNX]

 

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:08:44,6
13269476,Handle InterruptedException in CommitWatcher,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVw&open=AW5md-_8KcVY8lQ4ZsVw]

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:07:45,6
13269470,Sonar: Save and reuse Random object in GenesisUtil,[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-cLKcVY8lQ4Zr2o&open=AW5md-cLKcVY8lQ4Zr2o],pull-request-available,[],HDDS,Improvement,Major,2019-11-20 03:32:40,6
13269299,CI builds should use merged code state instead of the forked branch,"As of now the github actions based CI runs uses the branch of the PR which is the forked repo most of the time.

It would be better to force a rebase/merge (without push) before the builds to test the possible state after the merge not before.

For example if a PR branch uses elek/hadoop-ozone:HDDS-1234 and request a merge to apache/hadoop-ozone:master then the build should download the HDDS-1234 from elek/hadoop-ozone AND *rebase/merge* to the apache/hadoop-ozone *before* the build.

This merge is temporary just for the build/checks (no push at all).",pull-request-available,['build'],HDDS,Improvement,Major,2019-11-19 11:40:39,1
13269175,Add ozone.om.internal.service.id to OM HA configuration,"This Jira is to add ozone.om.internal.serviceid to let OM knows it belong to a particular service.

 

As now we have ozone.om.service.ids -≥ where we can define all service id's in a cluster.(This can happen if the same config is shared across the cluster)",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-11-18 22:03:48,2
13269074,TestOzoneManagerDoubleBufferWithOMResponse is flaky,"Flakiness can be reproduced locally. Usually it passes, but when I started to run it 100 times parallel with high cpu load it failed with the 3rd attempt (timed out)
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
-------------------------------------------------------------------------------
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 503.297 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
testDoubleBuffer(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 500.122 s  <<< ERROR!
java.lang.Exception: test timed out after 500000 milliseconds
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:382)
        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:385)
        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:129)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
 {code}
Independent from the flakiness I think a test where the timeout is 8 minutes and starts 1000 threads to insert 500 buckets (500_000 buckets all together) it's more like an integration test and would be better to move the slowest part to the integration-test project.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-18 13:28:30,2
13269015,Disable failing acceptance and unit tests,"As of now we have 40 pull requests in open change but the CI gate are broken by earlier commit.

 

I propose the following solution to make it possible to safely merge pull requests:
 # Disable the failing tests (we identified them at [https://github.com/apache/hadoop-ozone/pull/203)]
 # Commit the immediate fixes (like HDDS-2521)
 # Start a discussion on ozone-dev to use a more strict revert policy (revert immediately)",pull-request-available,[],HDDS,Improvement,Blocker,2019-11-18 09:03:04,1
13268960,BufferPool.releaseBuffer may release a buffer different than the head of the list,"{code}
//BufferPool
  public void releaseBuffer(ByteBuffer byteBuffer) {
    // always remove from head of the list and append at last
    ByteBuffer buffer = bufferList.remove(0);
    // Ensure the buffer to be removed is always at the head of the list.
    Preconditions.checkArgument(buffer.equals(byteBuffer));
    buffer.clear();
    bufferList.add(buffer);
    Preconditions.checkArgument(currentBufferIndex >= 0);
    currentBufferIndex--;
  }
{code}
In the code above, it expects buffer and byteBuffer are the same object, i.e.  buffer == byteBuffer.  However the precondition is checking buffer.equals(byteBuffer). Unfortunately, both buffer and byteBuffer have remaining() == 0 so that equals(..) returns true and the precondition does not catch the bug.
",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-11-18 00:07:50,0
13268922,Fix TestSecureOzoneCluster,"TestSecureOzoneCluster is failing with {{failure to login}}.

{code:title=https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2291-5997d/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestSecureOzoneCluster.txt}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.TestSecureOzoneCluster
-------------------------------------------------------------------------------
Tests run: 10, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 23.937 s <<< FAILURE! - in org.apache.hadoop.ozone.TestSecureOzoneCluster
testSCMSecurityProtocol(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 2.474 s  <<< ERROR!
org.apache.hadoop.security.KerberosAuthException: 
failure to login: for principal: scm/pr-hdds-2291-5997d-4279494527@EXAMPLE.COM from keytab /workdir/hadoop-ozone/integration-test/target/test-dir/TestSecureOzoneCluster/scm.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1847)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1215)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1008)
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:315)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.loginAsSCMUser(StorageContainerManager.java:508)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:254)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:212)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:600)
	at org.apache.hadoop.hdds.scm.HddsTestUtils.getScm(HddsTestUtils.java:91)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSCMSecurityProtocol(TestSecureOzoneCluster.java:299)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-11-17 14:45:03,0
13268908,Multipart upload failing with NPE,S3 multipart upload is [failing|https://elek.github.io/ozone-ci-03/pr/pr-hdds-2501-b5dhd/acceptance/summary.html#s1-s11-s5] with [NPE|https://github.com/elek/ozone-ci-03/blob/ddbaf4dd92ee5f855fea3e84c59b702fb2dda663/pr/pr-hdds-2501-b5dhd/acceptance/docker-ozones3-ozones3-s3-scm.log#L740-L747].,pull-request-available,['S3'],HDDS,Bug,Critical,2019-11-17 09:07:04,0
13268718,Ensure RATIS leader info is properly updated with pipeline report. ,HDDS-2034 added async pipeline creation and report handling to SCM. The leader information is not properly populated as manifested in the test failures from TestSCMPipelineManager#testPipelineReport. This ticket is opened to fix it. cc: [~sammichen],pull-request-available,[],HDDS,Bug,Major,2019-11-15 20:03:12,4
13268659,Code cleanup in EventQueue,"

https://sonarcloud.io/project/issues?fileUuids=AW5md-HgKcVY8lQ4ZrfB&id=hadoop-ozone&resolved=false",pull-request-available sonar,[],HDDS,Improvement,Major,2019-11-15 19:34:37,0
13268628,Fix Sonar issues in OzoneManagerServiceProviderImpl,Link to the list of issues : https://sonarcloud.io/project/issues?fileUuids=AW5md-HdKcVY8lQ4ZrUn&id=hadoop-ozone&resolved=false,pull-request-available sonar,['Ozone Recon'],HDDS,Bug,Minor,2019-11-15 17:10:21,5
13268625,Use isEmpty() to check whether the collection is empty or not in Ozone Manager module,"Ozone Manager has a number of instances where the following sonar rule is flagged.

bq. Using Collection.size() to test for emptiness works, but using Collection.isEmpty() makes the code more readable and can be more performant. The time complexity of any isEmpty() method implementation should be O(1) whereas some implementations of size() can be O(n).

An example of a flagged instance - https://sonarcloud.io/issues?myIssues=true&open=AW5md-W4KcVY8lQ4Zrv_&resolved=false
",pull-request-available sonar,['Ozone Manager'],HDDS,Bug,Major,2019-11-15 17:01:31,5
13268599,Code cleanup in replication package,Fix couple of [issues reported|https://sonarcloud.io/project/issues?directories=hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Freplication%2Chadoop-hdds%2Fcontainer-service%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Freplication&id=hadoop-ozone&resolved=false] in {{org.apache.hadoop.ozone.container.replication}} package.,pull-request-available sonar,[],HDDS,Improvement,Major,2019-11-15 15:30:04,0
13268575,Remove the hard-coded exclusion of TestMiniChaosOzoneCluster,"We excluded the execution of TestMiniChaosOzoneCluster from the hadoop-ozone/dev-support/checks/integration.sh because it was not stable enough.

Unfortunately this exclusion makes it impossible to use custom exclusion lists (-Dsurefire.excludesFile=....) as excludesFile can't be used if -Dtest=!... is already used.

I propose to remove this exclusion to make it possible to use different exclusion for different runs (pr check, daily, etc.)",pull-request-available,[],HDDS,Improvement,Major,2019-11-15 13:23:30,1
13268567,Remove keyAllocationInfo and replication info from the auditLog,"During the review of HDDS-2470 I found that the full keyLocationInfo is added to the audit log for s3 operations:

 
{code:java}

2019-11-15 12:34:18,538 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_KEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[]} | ret=SUCCESS |  2019-11-15 12:34:20,576 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_KEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[]} | ret=SUCCESS |  2019-11-15 12:34:20,626 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_BLOCK {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=THREE, keyLocationInfo=[], clientID=103141950132977668} | ret=SUCCESS |  2019-11-15 12:34:51,705 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {  containerBlockID {    containerID: 1    localID: 103141950135009280  }  blockCommitSequenceId: 2}offset: 0length: 3813createVersion: 0pipeline {  members {    uuid: ""eefe54e8-5723-458e-9204-207c6b97c9b3""    ipAddress: ""192.168.16.3""    hostName: ""ozones3_datanode_1.ozones3_default""    ports {      name: ""RATIS""      value: 9858    }    ports {      name: ""STANDALONE""      value: 9859    }    networkName: ""eefe54e8-5723-458e-9204-207c6b97c9b3""    networkLocation: ""/default-rack""  }  members {    uuid: ""ebf127d7-90a9-4f06-8fe5-a0c9c9adb743""    ipAddress: ""192.168.16.7""    hostName: ""ozones3_datanode_2.ozones3_default""    ports {      name: ""RATIS""      value: 9858    }    ports {      name: ""STANDALONE""      value: 9859    }    networkName: ""ebf127d7-90a9-4f06-8fe5-a0c9c9adb743""    networkLocation: ""/default-rack""  }  members {    uuid: ""9979c326-4982-4a4c-b34e-e70c1d825f5f""    ipAddress: ""192.168.16.6""    hostName: ""ozones3_datanode_3.ozones3_default""    ports {      name: ""RATIS""      value: 9858    }    ports {      name: ""STANDALONE""      value: 9859    }    networkName: ""9979c326-4982-4a4c-b34e-e70c1d825f5f""    networkLocation: ""/default-rack""  }  state: PIPELINE_OPEN  type: RATIS  factor: THREE  id {    id: ""69ba305b-fe89-4f5c-97cd-b894d5ee8f2b""  }  leaderID: """"}], partNumber=1, partName=/s3b607288814a5da737a92fb067500396e/bucket1/key1103141950132977668} | ret=SUCCESS |  2019-11-15 12:42:10,883 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=COMPLETE_MULTIPART_UPLOAD {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=0, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[], multipartList=[partNumber: 1partName: ""/s3b607288814a5da737a92fb067500396e/bucket1/key1103141950132977668""]} | ret=SUCCESS |  
 {code}
Including the full keyLocation info in the audit log may cause some problems:
 * It makes the the audit log slower
 * It makes harder to parse the audit log

I think it's better to separate the debug log (which can be provided easily with ozone insight tool) from the audit log. Therefore I suggest to remove the keyLocationInfo, replicationType, replicationFactor from the aduit log.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-15 12:50:16,0
13268565,Fix logic related to SCM address calculation in HddsUtils,"{{HddsUtils}} has 3 methods to calculate SCM address for various client types.  All have an unreachable {{if}} branch, because:

# {{iterator().next()}} throws exception for empty list
# {{getSCMAddresses}} never returns empty list anyway, it throws exception

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPX&open=AW5md-4qKcVY8lQ4ZsPX
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPY&open=AW5md-4qKcVY8lQ4ZsPY
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPW&open=AW5md-4qKcVY8lQ4ZsPW

Ideally code duplication among these methods should be reduced, too.

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPU&open=AW5md-4qKcVY8lQ4ZsPU
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPT&open=AW5md-4qKcVY8lQ4ZsPT
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPV&open=AW5md-4qKcVY8lQ4ZsPV

Complete list of issues in the same file:

https://sonarcloud.io/project/issues?fileUuids=AW5md-HhKcVY8lQ4Zrjn&id=hadoop-ozone&resolved=false",pull-request-available sonar,['SCM'],HDDS,Bug,Major,2019-11-15 12:45:48,0
13268531,Close FlushOptions in RDBStore,"{{FlushOptions}} should be closed after use.

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ4&open=AW5md-zwKcVY8lQ4ZsJ4
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ5&open=AW5md-zwKcVY8lQ4ZsJ5

Sonar also reported 15 further issues in the same file:

https://sonarcloud.io/project/issues?fileUuids=AW5md-HgKcVY8lQ4Zrga&id=hadoop-ozone&resolved=false",pull-request-available sonar,[],HDDS,Bug,Major,2019-11-15 09:41:27,0
13268436,Delegate Ozone volume create/list ACL check to authorizer plugin,"Today Ozone volume create/list ACL check are not sent to authorization plugins. This cause problem when authorization plugin is enabled. Admin still need to modify ozone-site.xml to change ozone.administrators to configure admin to create volume

 

This ticket is opened to have a consistent ACL check for all Ozone resources requests including admin request like volume create. This way, the admin defined by the authorization plugin can be honored during volume provision without restart ozone services. ",Triaged,"['Ozone Manager', 'Security']",HDDS,Improvement,Major,2019-11-14 22:27:27,6
13268398,Not enough arguments for log messages in GrpcXceiverService,"GrpcXceiverService has log messages with too few arguments for placeholders.  Only one of them is flagged by Sonar, but all seem to have the same problem.

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-69KcVY8lQ4ZsRZ&open=AW5md-69KcVY8lQ4ZsRZ",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-14 18:41:46,5
13268397,Ensure streams are closed,"* ContainerDataYaml: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6IKcVY8lQ4ZsQU&open=AW5md-6IKcVY8lQ4ZsQU
* OmUtils: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-hdKcVY8lQ4Zr76&open=AW5md-hdKcVY8lQ4Zr76",pull-request-available sonar,[],HDDS,Bug,Major,2019-11-14 18:38:26,0
13268393,Disable XML external entity processing,"Disable XML external entity processing in

* NodeSchemaLoader: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2nKcVY8lQ4ZsNm&open=AW5md-2nKcVY8lQ4ZsNm
* ConfigFileAppender:
** https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_uKcVY8lQ4ZsVY&open=AW5md-_uKcVY8lQ4ZsVY
** https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_uKcVY8lQ4ZsVZ&open=AW5md-_uKcVY8lQ4ZsVZ
* MultiDeleteRequestUnmarshaller: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-kDKcVY8lQ4Zr-N&open=AW5md-kDKcVY8lQ4Zr-N",pull-request-available sonar,['Security'],HDDS,Bug,Major,2019-11-14 18:14:11,0
13268331,Enable github actions for pull requests,"HDDS-2400 introduced a github actions workflow for each ""push"" event. It turned out that pushing to a forked repository doesn't trigger this event even if it's part of a PR.

 

We need to enable the execution for pull_request events:

References:

 [https://github.community/t5/GitHub-Actions/Run-a-GitHub-action-on-pull-request-for-PR-opened-from-a-forked/m-p/31147#M690]

[https://help.github.com/en/actions/automating-your-workflow-with-github-actions/events-that-trigger-workflows#pull-request-events-for-forked-repositories]
{noformat}
Note: By default, a workflow only runs when a pull_request's activity type is opened, synchronize, or reopened. To trigger workflows for more activity types, use the types keyword.{noformat}
 

 

 

 ",pull-request-available,[],HDDS,Improvement,Major,2019-11-14 14:30:16,1
13268251,Close streams in TarContainerPacker,"Ensure various streams are closed in {{TarContainerPacker}}:

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUH&open=AW5md-9bKcVY8lQ4ZsUH
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUL&open=AW5md-9bKcVY8lQ4ZsUL
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUK&open=AW5md-9bKcVY8lQ4ZsUK
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUJ&open=AW5md-9bKcVY8lQ4ZsUJ
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUI&open=AW5md-9bKcVY8lQ4ZsUI",pull-request-available sonar,['Ozone Datanode'],HDDS,Bug,Minor,2019-11-14 09:33:06,0
13268160,TableCache cleanup issue for OM non-HA,"In OM in non-HA case, the ratisTransactionLogIndex is generated by OmProtocolServersideTranslatorPB.java. And in OM non-HA validateAndUpdateCache is called from multipleHandler threads. So think of a case where one thread which has an index - 10 has added to doubleBuffer. (0-9 still have not added). DoubleBuffer flush thread flushes and call cleanup. (So, now cleanup will go and cleanup all cache entries with less than 10 epoch) This should not have cleanup those which might have put in to cache later and which are in process of flush to DB. This will cause inconsitency for few OM requests.

 

 

Example:

4 threads Committing 4 parts.

1st thread - part 1 - ratis Index - 3

2nd thread - part 2 - ratis index - 2

3rd thread - part3 - ratis index - 1

 

First thread got lock, and put in to doubleBuffer and cache with OmMultipartInfo (with part1). And cleanup is called to cleanup all entries in cache with less than 3. In the mean time 2nd thread and 1st thread put 2,3 parts in to OmMultipartInfo in to Cache and doubleBuffer. But first thread might cleanup those entries, as it is called with index 3 for cleanup.

 

Now when the 4th part upload came -> when it is commit Multipart upload when it gets multipartinfo it get Only part1 in OmMultipartInfo, as the OmMultipartInfo (with 1,2,3 is still in process of committing to DB). So now after 4th part upload is complete in DB and Cache we will have 1,4 parts only. We will miss part2,3 information.

 

So for non-HA case cleanup will be called with list of epochs that need to be cleanedup.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-14 00:21:47,2
13268149,Unregister ContainerMetadataScrubberMetrics on thread exit,{{ContainerMetadataScanner}} thread should call {{ContainerMetadataScrubberMetrics#unregister}} before exiting.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-11-13 23:00:27,0
13268144,Remove OzoneClient exception Precondition check,"If RaftCleintReply encounters an exception other than NotLeaderException, NotReplicatedException, StateMachineException or LeaderNotReady, then it sets success to false but there is no exception set. This causes a Precondition check failure in XceiverClientRatis which expects that there should be an exception if success=false.",TriagePending,[],HDDS,Bug,Major,2019-11-13 22:11:59,7
13268142,Fix code reliability issues found by Sonar in Ozone Recon module.,"sonarcloud.io has flagged a number of code reliability issues in Ozone recon (https://sonarcloud.io/code?id=hadoop-ozone&selected=hadoop-ozone%3Ahadoop-ozone%2Frecon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Frecon).

Following issues will be triaged / fixed.
* Double Brace Initialization should not be used
* Resources should be closed
* InterruptedException should not be ignored
",pull-request-available sonar,['Ozone Recon'],HDDS,Bug,Major,2019-11-13 21:57:18,5
13268141,Use try-with-resources while creating FlushOptions in RDBStore.,Link to the sonar issue flag - https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ4&open=AW5md-zwKcVY8lQ4ZsJ4. ,pull-request-available sonar,['Ozone Manager'],HDDS,Bug,Major,2019-11-13 21:50:41,5
13268130,Improve exception message for CompleteMultipartUpload,"When InvalidPart error occurs, the exception message does not have any information about partName and partNumber, it will be good to have this information.",pull-request-available,['S3'],HDDS,Bug,Major,2019-11-13 20:51:58,2
13268112,"Add partName, partNumber for CommitMultipartUpload","Right now when complete Multipart Upload is not printing partName and  partNumber into the audit log. This will help in analyzing audit logs for MPU.

 

 

2019-11-13 15:14:10,191 | INFO  | OMAudit | user=root | ip=xx.xx.xx.xx | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=ozone-test, key=plc_1570850798896_2991, dataSize=5242880, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {

  containerBlockID

{     containerID: 2     localID: 103129366531867089   }

  blockCommitSequenceId: 4978

}

offset: 0

length: 5242880

createVersion: 0

pipeline {

  leaderID: """"

  members {

    uuid: ""5d03aed5-cfb3-4689-b168-0c9a94316551""

    ipAddress: ""xx.xx.xx.xx""

    hostName: ""xx.xx.xx.xx""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""5d03aed5-cfb3-4689-b168-0c9a94316551""

    networkLocation: ""/default-rack""

  }

  members {

    uuid: ""a71462ae-7865-4ed5-b84e-60616df60a0d""

    ipAddress: ""9.134.51.25""

    hostName: ""9.134.51.25""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""a71462ae-7865-4ed5-b84e-60616df60a0d""

    networkLocation: ""/default-rack""

  }

  members {

    uuid: ""79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03""

    ipAddress: ""9.134.51.215""

    hostName: ""9.134.51.215""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03""

    networkLocation: ""/default-rack""

  }

  state: PIPELINE_OPEN

  type: RATIS

  factor: THREE

  id

{     id: ""ec6b06c5-193f-4c30-879b-5a12284dc4f8""   }

}

]} | ret=SUCCESS |",pull-request-available,['S3'],HDDS,Bug,Major,2019-11-13 19:02:12,2
13268103,Avoid changing client-side key metadata,Ozone RPC client should not change input map from client while creating keys.,pull-request-available,['Ozone Client'],HDDS,Bug,Minor,2019-11-13 18:23:24,0
13267996,Allow running Freon validators with limited memory,"Freon validators read each item to be validated completely into a {{byte[]}} buffer.  This allows timing only the read (and buffer allocation), but not the subsequent digest calculation.  However, it also means that memory required for running the validators is proportional to key size.

I propose to add a command-line flag to allow calculating the digest while reading the input stream.  This changes timing results a bit, since values will include the time required for digest calculation.  On the other hand, Freon will be able to validate huge keys with limited memory.",pull-request-available,['freon'],HDDS,Improvement,Major,2019-11-13 11:26:08,0
13267846,Avoid unnecessary allocations for FileChannel.open call,"{{ChunkUtils}} calls {{FileChannel#open(Path, OpenOption...)}}.  Vararg array elements are then added to a new {{HashSet}} to call {{FileChannel#open(Path, Set<? extends OpenOption>, FileAttribute<?>...)}}.  We can call the latter directly instead.",performance pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-12 22:09:51,0
13267834,Reduce unnecessary getServiceInfo calls,"OzoneManagerProtocolClientSideTranslatorPB.java Line 766-772 has multiple impl.getServiceInfo() which can be reduced by adding a local variable. 
{code:java}
 
resp.addAllServiceInfo(impl.getServiceInfo().getServiceInfoList().stream()
 .map(ServiceInfo::getProtobuf)
 .collect(Collectors.toList()));
if (impl.getServiceInfo().getCaCertificate() != null) {
 resp.setCaCertificate(impl.getServiceInfo().getCaCertificate()); {code}",pull-request-available,[],HDDS,Bug,Major,2019-11-12 20:57:55,4
13267744,Logging by ChunkUtils is misleading,"During a k8s based test I found a lot of log message like:
{code:java}
2019-11-12 14:27:13 WARN  ChunkManagerImpl:209 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='A9UrLxiEUN_testdata_chunk_4465025, offset=0, len=1024} {code}
I was very surprised as at ChunkManagerImpl:209 there was no similar lines.

It turned out that it's logged by ChunkUtils but it's used the logger of ChunkManagerImpl.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-11-12 15:10:17,1
13267415,Avoid list copy in ChecksumData,"{{ChecksumData}} is initially created with empty list of checksums, then it is updated with computed checksums, copying the list.  The computed list can be set directly.",performance pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-11 07:35:52,0
13267351,Add explicit base image version for images derived from ozone-runner,"{{ozone-om-ha}} and {{ozonescripts}} build images based on {{apache/ozone-runner}}.

Problem: They do not specify base image versions, so it defaults to {{latest}}.  If a new {{ozone-runner}} image is published on Docker Hub, developers needs to manually pull the {{latest}} image for it to take effect on these derived images.

Solution: Use explicit base image version (defined by {{OZONE_RUNNER_VERSION}} variable in {{.env}} file.",pull-request-available,['docker'],HDDS,Improvement,Minor,2019-11-10 13:38:16,0
13267214,Improve OM HA robot tests,"In one CI run, testOMHA.robot failed because robot framework SSH commands failed. This Jira aims to verify that the command execution succeeds.",pull-request-available,[],HDDS,Improvement,Major,2019-11-09 00:01:38,7
13267171,Use lazy string evaluation in preconditions,"Avoid eagerly evaluating error messages of preconditions (similarly to HDDS-2318, but there may be other occurrences of the same issue).",performance pull-request-available,[],HDDS,Improvement,Major,2019-11-08 18:49:12,0
13267078,Replace ToStringBuilder in BlockData,"{{BlockData#toString}} uses {{ToStringBuilder}} for ease of implementation.  This has a few problems:

# {{ToStringBuilder}} uses {{StringBuffer}}, which is synchronized
# the default buffer is 512 bytes, more than needed here
# {{BlockID}} and {{ContainerBlockID}} both use another {{StringBuilder}} or {{StringBuffer}} for their {{toString}} implementation, leading to several allocations and copies

The flame graph shows that {{BlockData#toString}} may be responsible for 1.5% of total allocations while putting keys.",performance pull-request-available,[],HDDS,Improvement,Minor,2019-11-08 10:49:49,0
13267063,Remove server side dependencies from ozonefs jar files,"During the review of HDDS-2427 we found that some of the server side dependencies (container-service, framework) are added to the ozonefs library jars. Server side dependencies should be excluded from the client side to make the client safer and the build faster.",pull-request-available,['Ozone Filesystem'],HDDS,Task,Major,2019-11-08 10:00:30,1
13266942,Exclude webapps from hadoop-ozone-filesystem-lib-current uber jar,"This has caused issue for DN UI loading.

hadoop-ozone-filesystem-lib-current-xx.jar is in the classpath which accidentally loaded Ozone datanode web application instead of Hadoop datanode application. This leads to the reported error. ",pull-request-available,[],HDDS,Bug,Major,2019-11-07 22:40:50,2
13266856,Completely disable tracer if hdds.tracing.enabled=false,"There is a config setting to enable/disable OpenTracing-based distributed tracing in Ozone ({{hdds.tracing.enabled}}).  However, setting it to false does not prevent tracer initialization, which causes unnecessary object allocations.",performance pull-request-available,[],HDDS,Improvement,Minor,2019-11-07 15:39:56,0
13266796,Set configuration variables from annotated java objects,"HDDS-1469 introduced a new method to handle configuration. Configuration can be injected directly to java objects which makes all the java constants unnecessary.

 

Almost.

 

To read the configuration it's enough to have an annotated java object. For example:

 
{code:java}
@ConfigGroup(prefix = ""hdds.scm"")
public class ScmConfig {
  private String principal;
  private String keytab;  @Config(key = ""kerberos.principal"",
        type = ConfigType.STRING,
        defaultValue = """",
        tags = { ConfigTag.SECURITY, ConfigTag.OZONE },
        description = ""This Kerberos principal is used by the SCM service.""
  )
  public void setKerberosPrincipal(String kerberosPrincipal) {
    this.principal = kerberosPrincipal; {code}
And the configuration can be set in ozone-site.xml

Unfortunately during the unit testing we need to inject the configuration variables programmatically which requires a String constant:
{code:java}
configuration.set(ScmConfig.ConfigStrings.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,      
            ""scm/"" + host + ""@"" + realm); {code}
I propose to implement a simple setter in the OzoneConfiguration which may help to set configuration based on an annotated configuration object instance:
{code:java}
OzoneConfiguration conf = new OzoneConfiguration();

SCMHTTPServerConfig httpConfig = SCMHTTPServerConfig(principal1,...);

conf.setFromObject(httpConfig){code}
This is the opposite direction of the existing OzoneConfiguration.getObject() and can be implemented with a similar approach.",pull-request-available,[],HDDS,Task,Major,2019-11-07 10:26:56,0
13266786,Define description/topics/merge strategy for the github repository with .asf.yaml,".asf.yaml helps to set different parameters on github repositories without admin privileges:

[https://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories]

This basic .asf.yaml defines description/url/topics and the allowed merge buttons.",pull-request-available,[],HDDS,Task,Major,2019-11-07 09:58:11,1
13266764,Ozoneperf docker cluster should use privileged containers,"The profiler [servlet|https://github.com/elek/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] (which helps to run java profiler in the background and publishes the result on the web interface) requires privileged docker containers.

 

This flag is missing from the ozoneperf docker-compose cluster (which is designed to run performance tests).

 

 ",pull-request-available,[],HDDS,Task,Major,2019-11-07 08:38:00,1
13266490,Reduce log level of per-node failure in XceiverClientGrpc,"When reading from a pipeline, client should not care if some datanode could not service the request, as long as the pipeline as a whole is OK.  The [log message|https://github.com/apache/hadoop-ozone/blob/2529cee1a7dd27c51cb9aed0dc57af283ff24e26/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L303-L304] indicating node failure was [increased to error level|https://github.com/apache/hadoop-ozone/commit/a79dc4609a975d46a3e051ad6904fb1eb40705ee#diff-b9b6f3ccb12829d90886e041d11395b1R288] in HDDS-1780.  This task proposes to change it back to debug.",pull-request-available,['Ozone Client'],HDDS,Task,Minor,2019-11-06 05:39:38,0
13266204,int2ByteString unnecessary byte array allocation,"{{int2ByteString}} implementations (currently duplicated in [RatisHelper|https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/ratis/RatisHelper.java#L280-L289] and [Checksum|https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/Checksum.java#L64-L73], but the first one is being removed in HDDS-2375) result in unnecessary byte array allocations:

# {{ByteString.Output}} creates 128-byte buffer by default, which is too large for writing a single int
# {{DataOutputStream}} allocates an [extra 8-byte array|https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/io/DataOutputStream.java#l204], used only for writing longs
# {{ByteString.Output}} also creates 10-element array for {{flushedBuffers}}",performance pull-request-available,[],HDDS,Bug,Minor,2019-11-04 22:20:44,0
13266096,Enable github actions based builds for Ozone,"Current PR checks are executed in a private branch based on the scripts in [https://github.com/elek/argo-ozone]

but the results are stored in a public repositories:

[https://github.com/elek/ozone-ci-q4|https://github.com/elek/ozone-ci-q3]

[https://github.com/elek/ozone-ci-03]

 

As we discussed during the community calls, it would be great to use github actions (or any other cloud based build) to make all the build definitions more accessible for the community.

[~vivekratnavel] checked CircleCI which has better reporting capabilities. But INFRA has concerns about the permission model of circle-ci:
{quote}it is highly unlikley we will allow a bot to be able to commit code (whether or not that is the intention, allowing circle-ci will make this possible, and is a complete no)
{quote}
See:

https://issues.apache.org/jira/browse/INFRA-18131

[https://lists.apache.org/thread.html/af52e2a3e865c01596d46374e8b294f2740587dbd59d85e132429b6c@%3Cbuilds.apache.org%3E]

 

Fortunately we have a clear contract. Or build scripts are stored under _hadoop-ozone/dev-support/checks_ (return code show the result, details are printed out to the console output). It's very easy to experiment with different build systems.

 

Github action seems to be an obvious choice: it's integrated well with GitHub and it has more generous resource limitations.

 

With this Jira I propose to enable github actions based PR checks for a few tests (author, rat, unit, acceptance, checkstyle, findbugs) as an experiment.

 ",pull-request-available,['build'],HDDS,Improvement,Major,2019-11-04 11:25:13,1
13265830,Remove usage of LogUtils class from ratis-common,"MiniOzoneChaoasCluster.java for setting log level it uses LogUtils from ratis-common. But this is removed from LogUtils as part of Ratis-508.

We can avoid depending on ratis for this, and use GenericTestUtils from hadoop-common test.

LogUtils.setLogLevel(GrpcClientProtocolClient.LOG, Level.WARN);",pull-request-available,[],HDDS,Bug,Major,2019-11-01 20:20:43,2
13265692,Fix calling cleanup for few missing tables in OM,"After DoubleBuffer flushes, we call cleanup cache to cleanup tables cache.

For few tables cleanup of cache is missed:
 # PrefixTable
 # S3SecretTable
 # DelegationTable",pull-request-available,[],HDDS,Bug,Major,2019-11-01 04:29:51,2
13265654,Handle Ozone S3 completeMPU to match with aws s3 behavior.,"# When uploaded 2 parts, and when complete upload 1 part no error
 # During complete multipart upload name/part number not matching with uploaded part and part number then InvalidPart error
 # When parts are not specified in sorted order InvalidPartOrder
 # During complete multipart upload when no uploaded parts, and we specify some parts then also InvalidPart
 # Uploaded parts 1,2,3 and during complete we can do upload 1,3 (No error)
 # When part 3 uploaded, complete with part 3 can be done",pull-request-available,"['Ozone Manager', 'S3']",HDDS,Bug,Major,2019-10-31 22:40:46,2
13265648,Ozone S3 Gateway allows bucket name with underscore to be created but throws an error during put key operation,"Steps to reproduce:
aws s3api --endpoint http://localhost:9878 create-bucket --bucket ozone_test

aws s3api --endpoint http://localhost:9878 put-object --bucket ozone_test --key ozone-site.xml --body /etc/hadoop/conf/ozone-site.xml

S3 gateway throws a warning:
{code:java}
javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : _
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : _
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1780)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1628)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	... 13 more
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-31 22:11:00,6
13265263,Large chunks during write can have memory pressure on DN with multiple clients,"During large file writes, it ends up writing {{16 MB}} chunks.  

https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java#L691

In large clusters, 100s of clients may connect to DN. In such cases, depending on the incoming write workload mem load on DN can increase significantly. 

",Triaged performance,['Ozone Datanode'],HDDS,Improvement,Major,2019-10-30 10:35:40,3
13265237,Consider reducing number of file::exists() calls during write operation,"When writing 100-200 MB files with multiple threads, observed lots of {{[file::exists(])}} checks.

For every 16 MB chunk, it ends up checking whether {{chunksLoc}} directory exists or not. (ref: [https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L239])

Also, this check ({{ChunkUtils.getChunkFile}}) happens from 2 places.

1.org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk

2.org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction

Note that these are folders and not actual chunk filenames. It would be helpful to reduce this check, if we track create/delete of these folders.",Triaged performance,['Ozone Datanode'],HDDS,Bug,Major,2019-10-30 09:15:53,0
13265138,"In ExcludeList, add if not exist only","Created based on comment from [~chinseone] in HDDS-2356

https://issues.apache.org/jira/browse/HDDS-2356?focusedCommentId=16960796&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16960796

 

 ",pull-request-available,[],HDDS,Bug,Major,2019-10-29 22:45:46,2
13265109,Use the Table.isExist API instead of get() call while checking for presence of key.,"Currently, when OM creates a file/directory, it checks the absence of all prefix paths of the key in its RocksDB. Since we don't care about the deserialization of the actual value, we should use the isExist API added in org.apache.hadoop.hdds.utils.db.Table which internally uses the more performant keyMayExist API of RocksDB.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-29 18:58:25,5
13265106,OM terminates with RocksDB error while continuously writing keys.,"Exception trace after writing around 800,000 keys.


{code}
2019-10-29 11:15:15,131 ERROR org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer: Terminating with exit status 1: During flush to DB encountered err
or in OMDoubleBuffer flush thread OMDoubleBufferFlushThread
java.io.IOException: Unable to write the batch.
        at org.apache.hadoop.hdds.utils.db.RDBBatchOperation.commit(RDBBatchOperation.java:48)
        at org.apache.hadoop.hdds.utils.db.RDBStore.commitBatchOperation(RDBStore.java:240)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:146)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.rocksdb.RocksDBException: unknown WriteBatch tag
        at org.rocksdb.RocksDB.write0(Native Method)
        at org.rocksdb.RocksDB.write(RocksDB.java:1421)
        at org.apache.hadoop.hdds.utils.db.RDBBatchOperation.commit(RDBBatchOperation.java:46)
        ... 3 more
{code}

Assigning to [~bharat] since he has already started work on this. ",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2019-10-29 18:41:07,2
13264797,Datanode pipeline is failing with NoSuchFileException,"Found it on a k8s based test cluster using a simple 3 node cluster and HDDS-2327 freon test. After a while the StateMachine become unhealthy after this error:
{code:java}
datanode-0 datanode java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: java.nio.file.NoSuchFileException: /data/storage/hdds/2a77fab9-9dc5-4f73-9501-b5347ac6145c/current/containerDir0/1/chunks/gGYYgiTTeg_testdata_chunk_13931.tmp.2.20830 {code}
Can be reproduced.",pull-request-available,[],HDDS,Bug,Critical,2019-10-28 10:53:27,1
13264785,Print out the ozone version during the startup instead of hadoop version,"Ozone components printing out the current version during the startup:

 
{code:java}
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = om/10.8.0.145
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.2.0
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-{code}
But as it's visible the build / compiled information is about hadoop not about hadoop-ozone.

(And personally I prefer to use a github compatible url instead of the SVN style -r. Something like:
{code:java}
STARTUP_MSG: build =  https://github.com/apache/hadoop-ozone/commit/8541c5694efebb58f53cf4665d3e4e6e4a12845c ; compiled by '....' on ...{code}
 ",Triaged pull-request-available,[],HDDS,Improvement,Major,2019-10-28 10:04:06,0
13264617,TestOzoneManagerDoubleBufferWithDummyResponse failing intermittently,"{noformat}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.479 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse
testDoubleBufferWithDummyResponse(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse)  Time elapsed: 1.404 s  <<< FAILURE!
java.lang.AssertionError
...
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.testDoubleBufferWithDummyResponse(TestOzoneManagerDoubleBufferWithDummyResponse.java:116)
{noformat}

* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2345-jsf2s/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.txt 
* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2272-bfh6s/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.txt
 ",pull-request-available,['test'],HDDS,Bug,Minor,2019-10-26 17:33:52,0
13264412,TestRatisPipelineProvider#testCreatePipelinesDnExclude is flaky,"TestRatisPipelineProvider#testCreatePipelinesDnExclude is flaky, failing in CI intermittently:

* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2360-9pxww/integration/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2352-cxhw9/integration/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt",pull-request-available,['test'],HDDS,Bug,Minor,2019-10-25 05:20:21,0
13264409,Add a OM metrics to find the false positive rate for the keyMayExist,Add a OM metrics to find the false positive rate for the keyMayExist.,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-25 04:54:27,5
13264316,Update Ratis snapshot to d6d58d0,"Update Ratis dependency version to snapshot [d6d58d0|https://github.com/apache/incubator-ratis/commit/d6d58d0], to fix memory issues (RATIS-726, RATIS-728).",pull-request-available,"['Ozone Client', 'Ozone Datanode']",HDDS,Task,Major,2019-10-24 19:40:50,0
13264247,Seeking randomly in a key with more than 2 blocks of data leads to inconsistent reads,"During Hive testing we found the following exception:

{code}
TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1569246922012_0214_1_03_000000_3:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: error iterating
    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
    at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
    at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
    at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
    at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
    at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
    at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: error iterating
    at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:80)
    at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426)
    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267)
    ... 16 more
Caused by: java.io.IOException: java.io.IOException: error iterating
    at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
    at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:366)
    at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
    at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
    at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
    at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
    at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
    ... 18 more
Caused by: java.io.IOException: error iterating
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:835)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:74)
    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:361)
    ... 24 more
Caused by: java.io.IOException: Error reading file: o3fs://hive.warehouse.vc0136.halxg.cloudera.com:9862/data/inventory/delta_0000001_0000001_0000/bucket_00000
    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1283)
    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.nextBatch(RecordReaderImpl.java:156)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$1.next(VectorizedOrcAcidRowBatchReader.java:150)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$1.next(VectorizedOrcAcidRowBatchReader.java:146)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:831)
    ... 26 more
Caused by: java.io.IOException: Inconsistent read for blockID=conID: 2 locID: 102851451236759576 bcsId: 14608 length=26398272 numBytesRead=6084153
    at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:176)
    at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
    at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:75)
    at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
    at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
    at org.apache.orc.impl.RecordReaderUtils.readDiskRanges(RecordReaderUtils.java:557)
    at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readFileData(RecordReaderUtils.java:276)
    at org.apache.orc.impl.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:1189)
    at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1057)
    at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1208)
    at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1243)
    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1279)
    ... 30 more
{code}

Evaluating the code path, the following is the issue:
given a file with more data than 2 blocks
when there are random seeks in the file to the end then to the beginning
then the read fails with the final cause of the exception above.

[~shashikant] has a solution already for this issue, which we have successfully tested internally with Hive, I am assigning this JIRA to him to post the PR.",pull-request-available,[],HDDS,Bug,Critical,2019-10-24 12:15:01,3
13264168,SCM log is full of AllocateBlock logs,"Make them Debug logs.

2019-10-24 03:17:43,087 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

scm_1       | 2019-10-24 03:17:43,088 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

scm_1       | 2019-10-24 03:17:43,089 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

scm_1       | 2019-10-24 03:17:43,093 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

 ",pull-request-available,['SCM'],HDDS,Task,Minor,2019-10-24 03:28:20,2
13264100,Client gets internal error instead of volume not found in secure cluster,"New Freon generators create volume and bucket if necessary.  This does not work in secure cluster for volume, but works for bucket:

{code}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozonesecure
$ docker-compose exec scm bash
$ kinit -kt /etc/security/keytabs/testuser.keytab testuser/scm@EXAMPLE.COM
$ ozone freon ockg -n 1
...
Check access operation failed for volume:vol1
...
Successful executions: 0
$ ozone sh volume create vol1
$ ozone freon ockg -n 1
...
2019-10-23 18:30:27,279 [main] INFO       - Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false
...
Successful executions: 1
{code}

The problem is that {{VOLUME_NOT_FOUND}} result is lost during ACL check, and client gets {{INTERNAL_ERROR}} instead.",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-10-23 18:38:25,0
13263742,Replication manager config has wrong description,"Replication manager's configuration for its own interval:

{code:title=https://github.com/apache/hadoop-ozone/blob/eb1d77e3206fab1a4ac0573507c9deb2b56b9ea1/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java#L808-L822}
    @Config(key = ""thread.interval"",
        type = ConfigType.TIME,
        defaultValue = ""300s"",
        tags = {SCM, OZONE},
        description = ""When a heartbeat from the data node arrives on SCM, ""
            + ""It is queued for processing with the time stamp of when the ""
            + ""heartbeat arrived. There is a heartbeat processing thread ""
            + ""inside ""
            + ""SCM that runs at a specified interval. This value controls how ""
            + ""frequently this thread is run.\n\n""
            + ""There are some assumptions build into SCM such as this ""
            + ""value should allow the heartbeat processing thread to run at ""
            + ""least three times more frequently than heartbeats and at least ""
            + ""five times more than stale node detection time. ""
            + ""If you specify a wrong value, SCM will gracefully refuse to ""
            + ""run. ""
            + ""For more info look at the node manager tests in SCM.\n""
            + ""\n""
            + ""In short, you don't need to change this.""
    )
{code}

duplicates SCM heartbeat interval doc:

{code:title=https://github.com/apache/hadoop-ozone/blob/eb1d77e3206fab1a4ac0573507c9deb2b56b9ea1/hadoop-hdds/common/src/main/resources/ozone-default.xml#L973-L991}
  <property>
    <name>ozone.scm.heartbeat.thread.interval</name>
    <value>3s</value>
    <tag>OZONE, MANAGEMENT</tag>
    <description>
      When a heartbeat from the data node arrives on SCM, It is queued for
      processing with the time stamp of when the heartbeat arrived. There is a
      heartbeat processing thread inside SCM that runs at a specified interval.
      This value controls how frequently this thread is run.

      There are some assumptions build into SCM such as this value should allow
      the heartbeat processing thread to run at least three times more
      frequently than heartbeats and at least five times more than stale node
      detection time. If you specify a wrong value, SCM will gracefully refuse
      to run. For more info look at the node manager tests in SCM.

      In short, you don't need to change this.
    </description>
  </property>
{code}",pull-request-available,['SCM'],HDDS,Bug,Minor,2019-10-22 08:24:58,0
13263699,Add immutable entries in to the DoubleBuffer for Volume requests.,"OMVolumeCreateRequest.java L159:
{code:java}
omClientResponse =
 new OMVolumeCreateResponse(omVolumeArgs,volumeList, omResponse.build());{code}
 

We add this to double-buffer, and double-buffer flushThread which is running in the background when picks up, converts to protoBuf and to ByteArray and write to rocksDB tables. So, during this conversion(This conversion will be done without any lock acquire), if any other request changes internal structure(like acls list) of OmVolumeArgs we might get ConcurrentModificationException.

 ",pull-request-available,[],HDDS,Task,Major,2019-10-22 05:07:07,2
13263654,Add immutable entries in to the DoubleBuffer for Bucket requests.,"OMBucketCreateRequest.java L181:

omClientResponse =
 new OMBucketCreateResponse(omBucketInfo,
 omResponse.build());

 

We add this to double-buffer, and double-buffer flushThread which is running in the background when picks up, converts to protoBuf and to ByteArray and write to rocksDB tables. So, during this conversion(This conversion will be done without any lock acquire), if any other request changes internal structure(like acls list) of OMBucketInfo we might get ConcurrentModificationException.

 ",pull-request-available,[],HDDS,Task,Major,2019-10-21 21:13:13,2
13263637,Validate tar entry path during extraction,Containers extracted from tar.gz should be validated to confine entries to the archive's root directory.,pull-request-available,[],HDDS,Improvement,Major,2019-10-21 19:27:30,0
13263543,Add OzoneManager to MiniOzoneChaosCluster,This jira proposes to add OzoneManager to MiniOzoneChaosCluster with OzoneHA implementation done. This will help in discovering bugs in Ozone Manager HA,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-21 13:26:08,7
13263401,Fix checkstyle errors,"Checkstyle errors intoduced in HDDS-2281:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2281-wfpgn/checkstyle/summary.txt}
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java
 465: Line is longer than 80 characters (found 81).
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ContainerTestHelper.java
 244: Line is longer than 80 characters (found 84).
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestContainerStateMachineFailures.java
 30: Unused import - org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException.
 506: &apos;;&apos; is preceded with whitespace.
 517: &apos;;&apos; is preceded with whitespace.
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-10-20 11:40:25,0
13263399,Fix TestKeyValueContainer#testRocksDBCreateUsesCachedOptions,"TestKeyValueContainer#testRocksDBCreateUsesCachedOptions, introduced in HDDS-2283, is failing:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2283-cnrrq/unit/hadoop-hdds/container-service/org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.txt}
testRocksDBCreateUsesCachedOptions(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.135 s  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<11>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testRocksDBCreateUsesCachedOptions(TestKeyValueContainer.java:406)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-10-20 11:25:15,0
13263389,Params not included in AuditMessage,"HDDS-2323 introduced the following Findbugs violation:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/trunk/trunk-nightly-20191020-r5wzl/findbugs/summary.txt}
M P UrF: Unread field: org.apache.hadoop.ozone.audit.AuditMessage$Builder.params  At AuditMessage.java:[line 106]
{noformat}

Which reveals that {{params}} is now not logged in audit messages:

{noformat}
2019-10-20 08:41:35,248 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=CREATE_VOLUME | ret=SUCCESS |
2019-10-20 08:41:35,312 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=CREATE_BUCKET | ret=SUCCESS |
2019-10-20 08:41:35,407 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=ALLOCATE_KEY | ret=SUCCESS |
2019-10-20 08:41:37,355 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=COMMIT_KEY | ret=SUCCESS |
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-10-20 08:42:29,0
13263388,Dummy chunk manager fails with length mismatch error,"HDDS-1094 added a config option ({{hdds.container.chunk.persistdata=false}}) to drop chunks instead of writing them to disk.  Currently this option triggers the following error with any key size:

{noformat}
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: data array does not match the length specified. DataLen: 16777216 Byte Array: 16777478
	at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDummyImpl.writeChunk(ChunkManagerDummyImpl.java:87)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleWriteChunk(KeyValueHandler.java:695)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:176)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:277)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:150)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:413)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:423)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$1(ContainerStateMachine.java:458)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-10-20 08:06:36,0
13263286,Enable sync option for OM non-HA ,"In OM non-HA when double buffer flushes, it should commit with sync turned on. As in non-HA when power failure/system crashes, the operations which are acknowledged by OM might be lost in this kind of scenario. (As in rocks DB with Sync false, the flush is asynchronous and it will not persist to storage system)

 

In HA, this is not a problem because the guarantee is provided by ratis and ratis logs.",pull-request-available,[],HDDS,Task,Major,2019-10-18 23:34:40,2
13263070,Random key generator can get stuck,"Freon's random key generator can get stuck waiting for completion (without any hint to what's happening) if object creation encounters any non-IOException.

Steps to reproduce:

# Start Ozone cluster with 1 datanode
# Start Freon (5K keys of size 1MB)

Result: after a few hundred keys progress stops.

{noformat}
$ docker-compose exec scm ozone freon rk --numOfThreads 1 --numOfVolumes 1 --numOfBuckets 1 --replicationType RATIS --factor ONE --keySize $(echo '2^20' | bc -lq) --numOfKeys $(echo '5 * 2^10' | bc -lq) --bufferSize $(echo '2^16' | bc -lq)
2019-10-18 10:44:45,224 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2019-10-18 10:44:45,381 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-18 10:44:45,381 INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2019-10-18 10:44:47,140 [main] INFO       - Number of Threads: 1
2019-10-18 10:44:47,145 [main] INFO       - Number of Volumes: 1.
2019-10-18 10:44:47,146 [main] INFO       - Number of Buckets per Volume: 1.
2019-10-18 10:44:47,146 [main] INFO       - Number of Keys per Bucket: 5120.
2019-10-18 10:44:47,147 [main] INFO       - Key size: 1048576 bytes
2019-10-18 10:44:47,147 [main] INFO       - Buffer size: 65536 bytes
2019-10-18 10:44:47,147 [main] INFO       - validateWrites : false
2019-10-18 10:44:47,151 [main] INFO       - Starting progress bar Thread.
...
 7.07% |????????                                                                                             |  362/5120 
{noformat}",pull-request-available,['freon'],HDDS,Bug,Major,2019-10-18 10:53:03,0
13263052,Support large-scale listing ,"Large-scale listing of directory contents takes a lot longer time and also has the potential to run into OOM. I have > 1 million entries in the same level and it took lot longer time with {{RemoteIterator}} (didn't complete as it was stuck in RDB::seek).

S3A batches it with 5K listing per fetch IIRC.  It would be good to have this feature in ozone as well.",TriagePending performance,['Ozone Manager'],HDDS,Bug,Critical,2019-10-18 09:16:22,7
13263050,Provide new Freon test to test Ratis pipeline with pure XceiverClientRatis,"[~xyao] suggested during an offline talk to implement one additional Freon test to test the ratis part only.

It can use XceiverClientManager which creates a pure XceiverClientRatis. The client can be used to generate chunks as the datanode accepts any container id / block id.

With this approach we can stress-test one selected ratis pipeline without having full end2end overhead of the key creation (OM, SCM, etc.)",pull-request-available,['freon'],HDDS,New Feature,Major,2019-10-18 09:01:56,1
13263048,Http server of Freon is not started for new Freon tests,"HDDS-2022 introduced new Freon tests but the Freon http server is not started for the new tests.

Freon includes a http server which can be turned on with the '–server' flag. It helps to monitor and profile the freon as the http server contains by default the prometheus and profiler servlets.

The server should be started if's requested.",pull-request-available,['freon'],HDDS,New Feature,Major,2019-10-18 08:58:59,1
13263047,BenchMarkDatanodeDispatcher genesis test is failing with NPE,"## What changes were proposed in this pull request?

Genesis is a microbenchmark tool for Ozone based on JMH ([https://openjdk.java.net/projects/code-tools/jmh/).]

 

Due to the recent Datanode changes the BenchMarkDatanodeDispatcher is failing with NPE:

 
{code:java}
java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.common.interfaces.Handler.<init>(Handler.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.<init>(KeyValueHandler.java:114)
	at org.apache.hadoop.ozone.container.common.interfaces.Handler.getHandlerForContainerType(Handler.java:78)
	at org.apache.hadoop.ozone.genesis.BenchMarkDatanodeDispatcher.initialize(BenchMarkDatanodeDispatcher.java:115)
	at org.apache.hadoop.ozone.genesis.generated.BenchMarkDatanodeDispatcher_createContainer_jmhTest._jmh_tryInit_f_benchmarkdatanodedispatcher0_G(BenchMarkDatanodeDispatcher_createContainer_jmhTest.java:438)
	at org.apache.hadoop.ozone.genesis.generated.BenchMarkDatanodeDispatcher_createContainer_jmhTest.createContainer_Throughput(BenchMarkDatanodeDispatcher_createContainer_jmhTest.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
 {code}
And this is the just the biggest problem there are a few other problems. I propose the following fixes:

*fix 1*: NPE is thrown because the 'context' object is required by KeyValueHandler/Handler classes.

In fact the context is not required, we need two functionalities/info from the context: the ability to send icr (IncrementalContainerReport) and the ID of the datanode.

Law of Demeter principle suggests to have only the minimum required information from other classes.

For example instead of having context but using only context.getParent().getDatanodeDetails().getUuidString() we can have only the UUID string which makes more easy to test (unit and benchmark) the Handler/KeyValueHandler.

This is the biggest (but still small change) in this patch: I started to use the datanodeId and an icrSender instead of having the full context.

*fix 2,3:* There were a few other problems. The scmId was missing if the writeChunk was called from Benchmark and and the Checksum was also missing.

*fix 4:* I also had a few other problems: very huge containers are used (default 5G) and as the benchmark starts with creating 100 containers it requires 500G space by default. I adjusted the container size to make it possible to run on local machine.

 

## How this patch can be tested?
{code:java}
./ozone genesis -benchmark=BenchMarkDatanodeDispatcher.writeChunk{code}
 ",pull-request-available,[],HDDS,New Feature,Major,2019-10-18 08:56:53,1
13262983,DoubleBuffer flush termination and OM shutdown's after that.,"om1_1       | 2019-10-18 00:34:45,317 [OMDoubleBufferFlushThread] ERROR      - Terminating with exit status 2: OMDoubleBuffer flush threadOMDoubleBufferFlushThreadencountered Throwable error

om1_1       | java.util.ConcurrentModificationException

om1_1       | at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1660)

om1_1       | at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)

om1_1       | at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)

om1_1       | at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)

om1_1       | at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)

om1_1       | at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)

om1_1       | at org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup.getProtobuf(OmKeyLocationInfoGroup.java:65)

om1_1       | at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)

om1_1       | at java.base/java.util.Collections$2.tryAdvance(Collections.java:4745)

om1_1       | at java.base/java.util.Collections$2.forEachRemaining(Collections.java:4753)

om1_1       | at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)

om1_1       | at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)

om1_1       | at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)

om1_1       | at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)

om1_1       | at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)

om1_1       | at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getProtobuf(OmKeyInfo.java:362)

om1_1       | at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.toPersistedFormat(OmKeyInfoCodec.java:37)

om1_1       | at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.toPersistedFormat(OmKeyInfoCodec.java:31)

om1_1       | at org.apache.hadoop.hdds.utils.db.CodecRegistry.asRawData(CodecRegistry.java:68)

om1_1       | at org.apache.hadoop.hdds.utils.db.TypedTable.putWithBatch(TypedTable.java:125)

om1_1       | at org.apache.hadoop.ozone.om.response.key.OMKeyCreateResponse.addToDBBatch(OMKeyCreateResponse.java:58)

om1_1       | at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.lambda$flushTransactions$0(OzoneManagerDoubleBuffer.java:139)

om1_1       | at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)

om1_1       | at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:137)

om1_1       | at java.base/java.lang.Thread.run(Thread.java:834)",pull-request-available,[],HDDS,Task,Major,2019-10-18 01:06:00,2
13262932,Ozone Block Token verify should not apply to all datanode cmd,"DN container protocol has cmd send from SCM or other DN, which do not bear OM block token like OM client. We should restrict the OM Block token check only for those issued from OM client. ",pull-request-available,[],HDDS,Bug,Major,2019-10-17 19:15:58,4
13262927,Negative value seen for OM NumKeys Metric in JMX.,"While running teragen/terasort on a cluster and verifying number of keys created on Ozone Manager, I noticed that the value of NumKeys counter metric to be a negative value !Screen Shot 2019-10-17 at 11.31.08 AM.png! .

",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-17 18:39:27,5
13262855,Avoid proto::tostring in preconditions to save CPU cycles,"[https://github.com/apache/hadoop-ozone/blob/61f4aa30f502b34fd778d9b37b1168721abafb2f/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerProtocolServerSideTranslatorPB.java#L117]

 

This ends up converting proto toString in precondition checks and burns CPU cycles. {{request.toString()}} can be added in debug log on need basis.

 

 ",performance pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-17 12:44:56,2
13262617,Support to skip recon and/or ozonefs during the build,"(I almost use this Jira summary: ""Fast-lane to ozone build"" It was very hard to resist...)

 

 The two slowest part of Ozone build as of now:


 # The (multiple) shading of ozonefs
 # And the frontend build/obfuscation of ozone recon

[~aengineer] suggested to introduce options to skip them as they are not required for the build all the time.

This patch introduces '-DskipRecon' and '-DskipShade' options to provide a faster way to create a *partial* build.",pull-request-available,[],HDDS,New Feature,Major,2019-10-16 12:11:05,1
13262515,Duplicate release of lock in OMKeyCommitRequest,"{noformat}
om_1        | 2019-10-16 05:33:57,413 [IPC Server handler 19 on 9862] ERROR      - Trying to release the lock on /bypdd/mybucket4, which was never acquired.
om_1        | 2019-10-16 05:33:57,414 WARN ipc.Server: IPC Server handler 19 on 9862, call Call#4 Retry#8 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.29.0.4:37018
om_1        | java.lang.IllegalMonitorStateException: Releasing lock on resource /bypdd/mybucket4 without acquiring lock
om_1        | 	at org.apache.hadoop.ozone.lock.LockManager.getLockForReleasing(LockManager.java:220)
om_1        | 	at org.apache.hadoop.ozone.lock.LockManager.release(LockManager.java:168)
om_1        | 	at org.apache.hadoop.ozone.lock.LockManager.writeUnlock(LockManager.java:148)
om_1        | 	at org.apache.hadoop.ozone.om.lock.OzoneManagerLock.unlock(OzoneManagerLock.java:364)
om_1        | 	at org.apache.hadoop.ozone.om.lock.OzoneManagerLock.releaseWriteLock(OzoneManagerLock.java:329)
om_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCommitRequest.validateAndUpdateCache(OMKeyCommitRequest.java:177)
{noformat}",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2019-10-16 05:39:52,0
13262505,Fix typo in ozone command,"{noformat:title=ozone}
Usage: ozone [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
...
insight       tool to get runtime opeartion information
...
{noformat}

Should be ""operation"".",pull-request-available,['Ozone CLI'],HDDS,Bug,Trivial,2019-10-16 04:44:39,0
13262470,Fix logic of RetryPolicy in OzoneClientSideTranslatorPB,"OzoneManagerProtocolClientSideTranslatorPB.java

L251: if (cause instanceof NotLeaderException) {
 NotLeaderException notLeaderException = (NotLeaderException) cause;
 omFailoverProxyProvider.performFailoverIfRequired(
 notLeaderException.getSuggestedLeaderNodeId());
 return getRetryAction(RetryAction.RETRY, retries, failovers);
 }

 

The suggested leader returned from Server is not used during failOver, as the cause is a type of RemoteException. So with current code, it does not use suggested leader for failOver at all and by default with each OM, it tries max retries.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Blocker,2019-10-15 21:57:59,7
13262454,Add support to add ozone ranger plugin to Ozone Manager classpath,"Currently, there is no way to add Ozone Ranger plugin to Ozone Manager classpath. 

We should be able to set an environment variable that will be respected by ozone and added to Ozone Manager classpath.

 

 ",pull-request-available,['Ozone Manager'],HDDS,Task,Major,2019-10-15 20:41:36,6
13262333,Optimise OzoneManagerDoubleBuffer::flushTransactions to flush in batches,"When running a write heavy benchmark, {{{color:#000000}org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.flushTransactions{color}}} was invoked for pretty much every write.

This forces {{cleanupCache}} to be invoked which ends up choking in single thread executor. Attaching the profiler information which gives more details.

Ideally, {{flushTransactions}} should batch up the work to reduce load on rocksDB.

 

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java#L130]

 

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java#L322]

 

 ",TriagePending performance,['Ozone Manager'],HDDS,Bug,Major,2019-10-15 10:55:10,2
13262307,Switch to centos with the apache/ozone-build docker image,"I realized multiple JVM crashes in the daily builds:

 
{code:java}

ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter9018689154779946208.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7569723928289175829tmp surefire_947955725320624341206tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139
      
      
        [ERROR] Crashed tests:
      
      
        [ERROR] org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
      
      
        [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter5429192218879128313.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7227403571189445391tmp surefire_1011197392458143645283tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139
      
      
        [ERROR] Crashed tests:
      
      
        [ERROR] org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
      
      
        [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter1355604543311368443.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire3938612864214747736tmp surefire_933162535733309260236tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139
      
      
        [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter9018689154779946208.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7569723928289175829tmp surefire_947955725320624341206tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139 {code}
 

Based on the crash log (uploaded) it's related to the rocksdb JNI interface.

In the current ozone-build docker image (which provides the environment for build) we use alpine where musl libc is used instead of the main glibc. I think it would be more safe to use the same glibc what is used in production.

I tested with centos based docker image and it seems to be more stable. Didn't see any more JVM crashes.",pull-request-available,[],HDDS,Improvement,Major,2019-10-15 07:20:16,1
13262155,Manage common pom versions in one common place,"Some of the versions (eg. ozone.version, hdds.version, ratis.version) are required for both ozone and hdds subprojects. As we have a common pom.xml it can be safer to manage them in one common place at the root pom.xml instead of managing them multiple times.",pull-request-available,['build'],HDDS,Improvement,Major,2019-10-14 12:56:11,1
13262135,Publish normalized Ratis metrics via the prometheus endpoint,"Latest Ratis contains very good metrics about the status of the ratis ring.

After RATIS-702 it will be possible to adjust the repoter of the Dropwizard based ratis metrics and export them directly to the /prom http endpoint (used by ozone insight and ratis).

Unfortunately Dropwizard is very simple, there is no tag support. All of the instance specific strings are part of the metric name. For example:
{code:java}
""ratis_grpc.log_appender.72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67@group""
 + ""-72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67""
 + "".grpc_log_appender_follower_75fa730a-59f0-4547""
 + ""-bd68-216162c263eb_latency"", {code}
In this patch I will use a simple method: during the export of the dropwizard metrics based on the well known format of the ratis metrics, they are converted to proper prometheus metrics where the instance information is included as tags:
{code:java}
ratis_grpc.log_appender.grpc_log_appender_follower_latency{instance=""72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67""}
 {code}
With this approach we can:

 1. monitor easily all the Ratis pipelines with one simple query

 2. Use the metrics for ozone insight which will show health state of the Ratis pipeline",pull-request-available,[],HDDS,Bug,Major,2019-10-14 10:41:46,1
13262119,Fix maven warning about duplicated metrics-core jar,"Maven build of Ozone is starting with a warning:
{code:java}
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-tools:jar:0.5.0-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: io.dropwizard.metrics:metrics-core:jar -> version 3.2.4 vs (?) @ line 94, column 17
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
 {code}
It's better to avoid it.",pull-request-available,['build'],HDDS,Bug,Major,2019-10-14 09:36:45,1
13262109,Enable Opentracing for new Freon tests,"HDDS-2022 introduced new freon tests, but the initial root span of opentracing is not created before the test execution. We need to enable opentracing to get better view about the executions of the new freon test.",pull-request-available,['freon'],HDDS,Improvement,Major,2019-10-14 09:08:55,1
13262105,ozoneperf compose cluster shouln't start freon by default,"During the original creation of the compose/ozoneperf we added an example freon execution to make it clean how the data can be generated. This freon process starts all the time when ozoneperf cluster is started (usually I notice it when my CPU starts to use 100% of the available resources).

Since the creation of this cluster definition we implemented multiple type of freon tests and it's hard predict which tests should be executed. I propose to remove the default execution of the random key generation but keep the opportunity to run any of the tests.",pull-request-available,['docker'],HDDS,Improvement,Major,2019-10-14 08:58:23,1
13262104,Display log of freon on the standard output,"HDDS-2042 disabled the console logging for all of the ozone command line tools including freon.

But freon is different, it has a different error handling model. For freon we need all the log on the console.

 1. To follow all the different errors

 2. To get information about the used (random) prefix which can be reused during the validation phase.

 

I propose to restore the original behavior for Ozone.",pull-request-available,[],HDDS,Improvement,Major,2019-10-14 08:42:08,1
13262103,Create a new HISTORY.md in the new repository,"During the apache/hadoop.git --> apache/hadoop-ozone.git move we rewrote the (git) history to simplify the work. Unfortunately some of the early work of HDDS-7280 is not part of the hadoop-ozone repository just the hadoop repository. As it suggested by Anu Engineer, we can explain this in a separated file and show how the origin of Ozone can be found.

 

cc [~aengineer]",pull-request-available,[],HDDS,Improvement,Major,2019-10-14 08:35:52,1
13262098,Create a new CONTRIBUTION.md for the new repository,Github supports CONTIRUTION.md which is displayed during the creation of a new Github PR. We can copy the content of the wiki page about how to contribut / how to build.,pull-request-available,[],HDDS,Improvement,Major,2019-10-14 07:59:16,1
13262097,Create Ozone specific README.md to the new hadoop-ozone repository,The current README is main Hadoop specific. We can create an ozone specific.,pull-request-available,[],HDDS,Improvement,Major,2019-10-14 07:58:32,1
13262023,Acceptance tests for OM HA,Add robot tests to test OM HA functionality.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-13 19:57:18,7
13261986,Put testing information and a problem description to the github PR template,"This is suggested by [~aengineer] during an offline discussion to add more information to the github PR template based on the template of ambari (by Vivek):

https://github.com/apache/ambari/commit/579cec8cf5bcfe1a1a0feacf055ed6569f674e6a",pull-request-available,[],HDDS,Improvement,Major,2019-10-13 08:31:00,1
13261708,GetBlock and ReadChunk commands should be sent to the same datanode,"I can be observed that the GetBlock and ReadChunk command is sent to 2 different datanodes. It should be sent to the same datanode to re-use the connection.

{code}
19/10/10 00:43:42 INFO scm.XceiverClientGrpc: Send command GetBlock to datanode 172.26.32.224
19/10/10 00:43:42 INFO scm.XceiverClientGrpc: Send command ReadChunk to datanode 172.26.32.231
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-10-11 06:28:53,7
13261649,scmcli pipeline list command throws NullPointerException,"ozone scmcli pipeline list
{noformat}
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
	at org.apache.hadoop.hdds.scm.XceiverClientManager.<init>(XceiverClientManager.java:98)
	at org.apache.hadoop.hdds.scm.XceiverClientManager.<init>(XceiverClientManager.java:83)
	at org.apache.hadoop.hdds.scm.cli.SCMCLI.createScmClient(SCMCLI.java:139)
	at org.apache.hadoop.hdds.scm.cli.pipeline.ListPipelinesSubcommand.call(ListPipelinesSubcommand.java:55)
	at org.apache.hadoop.hdds.scm.cli.pipeline.ListPipelinesSubcommand.call(ListPipelinesSubcommand.java:30)
	at picocli.CommandLine.execute(CommandLine.java:1173)
	at picocli.CommandLine.access$800(CommandLine.java:141)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
	at org.apache.hadoop.hdds.scm.cli.SCMCLI.main(SCMCLI.java:101){noformat}",pull-request-available,[],HDDS,Bug,Major,2019-10-10 21:17:38,4
13261629,ContainerStateMachine#handleWriteChunk should ignore close container exception ,"Currently, ContainerStateMachine#applyTrannsaction ignores close container exception.Similarly,ContainerStateMachine#handleWriteChunk call also should ignore close container exception.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-10-10 18:40:46,3
13261549,HddsUtils#CheckForException should not return null in case the ratis exception cause is not set,"HddsUtils#CheckForException checks for the cause to be set properly to one of the defined/expected exceptions. In case, ratis throws up any runtime exception, HddsUtils#CheckForException can return null and lead to NullPointerException while write.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-10-10 11:37:44,3
13261472,Ozone S3 CLI commands not working on HA cluster,"ozone s3 getSecret

ozone s3 path are not working on OM HA cluster

 

Because these commands do not take URI as a parameter. And for shell in HA, passing URI is mandatory. 

 

Below is the output when running on OM HA cluster:

 
{code:java}
$ozone s3 getsecret
Service ID or host name must not be omitted when ozone.om.service.ids is defined.
{code}
 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-10 01:05:45,2
13261469,Run S3 test suite on OM HA cluster,"This will add a new compose setup with 3 OM's and start SCM, S3G, Datanode.

Run the existing test suite against this new docker-compose cluster.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-10 00:47:15,2
13261237,Avoid buffer copying in GrpcReplicationService,"In GrpcOutputStream, it writes data to a ByteArrayOutputStream and copies them to a ByteString.",performance pull-request-available,[],HDDS,Improvement,Major,2019-10-09 06:13:14,0
13261236,Avoid buffer copying in GrpcReplicationClient,"In StreamDownloader.onNext, CopyContainerResponseProto is copied to a byte[] and then it is written out to the stream.",performance pull-request-available,[],HDDS,Improvement,Major,2019-10-09 06:10:42,0
13261234,Avoid buffer copying in ContainerStateMachine.loadSnapshot/persistContainerSet,"ContainerStateMachine:
- In loadSnapshot(..), it first reads the snapshotFile to a  byte[] and then parses it to ContainerProtos.Container2BCSIDMapProto.  The buffer copying can be avoided.
{code}
    try (FileInputStream fin = new FileInputStream(snapshotFile)) {
      byte[] container2BCSIDData = IOUtils.toByteArray(fin);
      ContainerProtos.Container2BCSIDMapProto proto =
          ContainerProtos.Container2BCSIDMapProto
              .parseFrom(container2BCSIDData);
      ...
    }
{code}

- persistContainerSet(..) has similar problem.",performance pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-10-09 05:49:45,0
13261233,Provide config for fair/non-fair for OM RW Lock,"Provide config in OzoneManager Lock for fair/non-fair for OM RW Lock.

Created based on review comments during HDDS-2244.",pull-request-available,[],HDDS,Improvement,Major,2019-10-09 05:12:34,2
13261192,Incorrect container checksum upon downgrade,"Container file checksum is calculated based on all YAML fields in a given Ozone version.  If the same container file is used in older Ozone, which has fewer fields, the expected checksum will be different.

Example: origin pipeline ID and origin node ID were added for HDDS-837 in Ozone 0.4.0.  Starting Ozone 0.3.0 with the same data results in checksum error.

{noformat}
datanode_1  | ... ERROR ContainerReader:166 - Failed to parse ContainerFile for ContainerID: 1
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container checksum error for ContainerID: 1.
datanode_1  | Stored Checksum: 7a6ec508d6e3796c5fe5fd52574b3d3437b0a0eaa4e053f7a96a5e39f4abb374
datanode_1  | Expected Checksum: fee023a02d3ced2f7b0b42c116cce5f03da6b57b29965ca878dc46d1213230b6
datanode_1  | 	at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.verifyChecksum(ContainerUtils.java:259)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerData(ContainerReader.java:180)
datanode_1  | 	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:142)
{noformat}",upgrade,['Ozone Datanode'],HDDS,Bug,Major,2019-10-08 20:45:48,5
13261161,Container metadata scanner interval mismatch,"Container metadata scanner can be configured to run at specific time intervals, eg. hourly ({{hdds.containerscrub.metadata.scan.interval}}).  However, the actual run interval does not match the configuration.  After a datanode restart, it runs in quick succession, later it runs at apparently random intervals.

{noformat:title=sample log}
datanode_1  | 2019-10-08 14:05:30 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 1, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 14:09:33 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 1, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
...
datanode_1  | 2019-10-08 14:09:33 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 28, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 14:21:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 29, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 14:21:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 30, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 15:30:38 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 31, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 16:45:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 32, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
{noformat}

The problem is that time elapsed is measured in nanoseconds, while the configuration is in milliseconds.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-10-08 17:51:31,0
13260867,integration.sh may report false negative,"Sometimes integration test run gets killed, and {{integration.sh}} incorrectly reports ""success"".  Example:

{noformat:title=https://github.com/elek/ozone-ci-q4/tree/ae930d6f7f10c7d2aeaf1f2f21b18ada954ea444/pr/pr-hdds-2259-hlwmv/integration/result}
success
{noformat}

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/ae930d6f7f10c7d2aeaf1f2f21b18ada954ea444/pr/pr-hdds-2259-hlwmv/integration/output.log#L2457}
/workdir/hadoop-ozone/dev-support/checks/integration.sh: line 22:   369 Killed                  mvn -B -fn test -f pom.ozone.xml -pl :hadoop-ozone-integration-test,:hadoop-ozone-filesystem,:hadoop-ozone-tools -Dtest=\!TestMiniChaosOzoneCluster ""$@""
{noformat}",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2019-10-07 11:34:25,0
13260863,Improve output of TestOzoneContainer,"TestOzoneContainer#testContainerCreateDiskFull fails intermittently (HDDS-2263), but test output does not reveal too much about the reason.  The goal of this task is to improve the assertion/output to make it easier to fix the failure.",pull-request-available,['test'],HDDS,Improvement,Minor,2019-10-07 10:59:34,0
13260852,SLEEP_SECONDS: command not found,"{noformat}
datanode_1  | /opt/hadoop/bin/docker/entrypoint.sh: line 66: SLEEP_SECONDS: command not found
datanode_1  | Sleeping for  seconds
{noformat}

Eg. https://raw.githubusercontent.com/elek/ozone-ci-q4/master/pr/pr-hdds-2238-79fll/acceptance/docker-ozonesecure-ozonesecure-s3-s3g.log",pull-request-available,['docker'],HDDS,Bug,Trivial,2019-10-07 10:06:18,0
13260747,Container Data Scrubber computes wrong checksum,"Chunk checksum verification fails for (almost) any file.  This is caused by computing checksum for the entire buffer, regardless of the actual size of the chunk.

{code:title=https://github.com/apache/hadoop/blob/55c5436f39120da0d7dabf43d7e5e6404307123b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerCheck.java#L259-L273}
            byte[] buffer = new byte[cData.getBytesPerChecksum()];
...
                v = fs.read(buffer);
...
                bytesRead += v;
...
                ByteString actual = cal.computeChecksum(buffer)
                    .getChecksums().get(0);
{code}

This results in marking all closed containers as unhealthy.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Critical,2019-10-06 06:44:08,0
13260645,Fix checkstyle issues in ChecksumByteBuffer,"hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBuffer.java
 84: Inner assignments should be avoided.
 85: Inner assignments should be avoided.
 101: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 102: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 103: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 104: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 105: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 106: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 107: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 108: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.",newbie pull-request-available,[],HDDS,Bug,Major,2019-10-04 21:11:06,6
13260611,Fix flaky unit testTestContainerStateMachine#testRatisSnapshotRetention,"Test always fails with assertion error:

{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.ozone.client.rpc.TestContainerStateMachine.testRatisSnapshotRetention(TestContainerStateMachine.java:188)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-10-04 17:33:04,5
13260578,Add an option to customize unit.sh and integration.sh parameters,"hadoop-ozone/dev-support/checks/unit.sh (and same with integration) provides an easy entrypoint to execute all the unit/integration test. But in same cases it would be great to use the script but further specify the scope of the test.

With this simple patch it will be possible to adjust the surefire parameters.",pull-request-available,[],HDDS,Task,Major,2019-10-04 14:00:31,1
13260565,Generated configs missing from ozone-filesystem-lib jars,"Hadoop 3.1 and 3.2 acceptance tests started failing with HDDS-1720, which added a new, annotated configuration class.

The [change itself|https://github.com/apache/hadoop/pull/1538/files] looks fine.  The problem is that the packaging process for {{ozone-filesystem-lib}} jars keeps only 1 or 2 {{ozone-default-generated.xml}} files.  With the new config in place, client configs are missing, so Ratis client gets evicted immediately due to {{scm.container.client.idle.threshold}} = 0.  This results in NPE:

{code:title=https://elek.github.io/ozone-ci-q4/pr/pr-hdds-1720-trunk-rd9ht/acceptance/summary.html#s1-s5-t1-k2-k2}
Running command 'hdfs dfs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/ozone-14607
...
-put: Fatal internal error
java.lang.NullPointerException: client is null
	at java.util.Objects.requireNonNull(Objects.java:228)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.getClient(XceiverClientRatis.java:208)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:234)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:332)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:310)
...
{code}",pull-request-available,"['build', 'Ozone Filesystem']",HDDS,Bug,Critical,2019-10-04 12:22:46,0
13260495,Use new ReadWrite lock in OzoneManager,Use new ReadWriteLock added in HDDS-2223.,pull-request-available,[],HDDS,Improvement,Major,2019-10-04 04:58:52,2
13260435,Optimize the refresh pipeline logic used by KeyManagerImpl to obtain the pipelines for a key,"Currently, while looking up a key, the Ozone Manager gets the pipeline information from SCM through an RPC for every block in the key. For large files > 1GB, we may end up making a lot of RPC calls for this. This can be optimized in a couple of ways

* We can implement a batch getContainerWithPipeline API in SCM using which we can get the pipeline info locations for all the blocks for a file. To keep the number of containers passed in to SCM in a single call, we can have a fixed container batch size on the OM side. _Here, Number of calls = 1 (or k depending on batch size)_
* Instead, a simpler change would be to have a map (method local) of ContainerID -> Pipeline that we get from SCM so that we don't need to make repeated calls to SCM for the same containerID for a key. _Here, Number of calls = Number of unique containerIDs_",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-03 20:38:38,5
13260360,Command line tool for OM Admin,"A command line tool (*ozone omha*) to get information related to OM HA. 
This Jira proposes to add the _getServiceState_ option for OM HA which lists all the OMs in the service and their corresponding Ratis server roles (LEADER/ FOLLOWER). 
We can later add more options to this tool.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-03 18:53:13,7
13260356,Fix TestOzoneFsHAUrls,[https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2162-pj84x/integration/hadoop-ozone/ozonefs/org.apache.hadoop.fs.ozone.TestOzoneFsHAURLs.txt],pull-request-available,[],HDDS,Bug,Major,2019-10-03 18:35:12,0
13260334,Container Data Scrubber spams log in empty cluster,"In an empty cluster (without closed containers) logs are filled with messages from completed data scrubber iterations (~3600 per second for me), if Container Scanner is enabled ({{hdds.containerscrub.enabled=true}}), eg.:

{noformat}
datanode_1  | 2019-10-03 15:43:57 INFO  ContainerDataScanner:114 - Completed an iteration of container data scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 6763, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0
{noformat} 

Also CPU usage is quite high.

I think:

# there should be a small sleep between iterations
# it should log only if any containers were scanned",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-10-03 15:51:25,0
13260331,KeyDeletingService throws NPE if it's started too early,"1. OzoneManager starts KeyManager

2. KeyManager starts KeyDeletingService

3. KeyDeletingService uses OzoneManager.isLeader()

4. OzoneManager.isLeader() uses omRatisServer

5. omRatisServer can be null (bumm)

 

Now the initialization order in OzoneManager:

 

new KeymanagerServer() *Includes start()!!!!*

omRatisServer initialization

start() (includes KeyManager.start())

 

The solution seems to be easy: start the key manager only from the OzoneManager.start() and not from the OzoneManager.instantiateServices()",pull-request-available,['Ozone Manager'],HDDS,Task,Major,2019-10-03 15:17:43,1
13260158,rat.sh fails due to ozone-recon-web/build files,"[ERROR] After correcting the problems, you can resume the build with the command
[ERROR] mvn <goals> -rf :hadoop-ozone-recon
[INFO] Build failures were ignored.
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/index.html
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/2.8943d5a3.chunk.css
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/2.8943d5a3.chunk.css.map
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/main.96eebd44.chunk.css
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/runtime~main.a8a9905a.js.map
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/runtime~main.a8a9905a.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/2.ea549bfe.chunk.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/main.5bb53989.chunk.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/2.ea549bfe.chunk.js.map
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/precache-manifest.1d05d7a103ee9d6b280ef7adfcab3c01.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/service-worker.js",pull-request-available,[],HDDS,Bug,Major,2019-10-02 17:58:07,0
13260111,ozone-build docker image is failing due to a missing entrypoint script,"We have a dedicated apache/ozone-build image which contains all of the required build and test tools to build ozone.

Unfortunately  it's not working as one shell script was not added to the original patch.

This patch (to the hadoop-docker-ozone repo!!) remove the requirement of the entrypoint.sh (no more docker in docker)

 

And installs additional tools (blockade, kubectl, mailsend)

 ",pull-request-available,"['build', 'docker']",HDDS,Task,Major,2019-10-02 13:05:43,1
13260085,test-single.sh cannot copy results,"Previously {{result}} directory was created by simply {{source}}-ing {{testlib.sh}}, but HDDS-2185 changed it to avoid lost results.  {{test-single.sh}} needs to be adjusted accordingly.

{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone
$ docker-compose up -d --scale datanode=3
$ ../test-single.sh scm basic/basic.robot
...
invalid output path: directory ""hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone/result"" does not exist
{noformat}",pull-request-available,['docker'],HDDS,Bug,Minor,2019-10-02 10:35:04,0
13260052,Invalid entries in ozonesecure-mr config,"Some of the entries in {{ozonesecure-mr/docker-config}} are in invalid format, thus they end up missing from the generated config files.

{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozonesecure-mr
$ ./test.sh # configs are generated during container startup
$ cd ../..

$ grep -c 'ozone.administrators' compose/ozonesecure-mr/docker-config
1
$ grep -c 'ozone.administrators' etc/hadoop/ozone-site.xml
0

$ grep -c 'yarn.timeline-service' compose/ozonesecure-mr/docker-config
5
$ grep -c 'yarn.timeline-service' etc/hadoop/yarn-site.xml
2

$ grep -c 'container-executor' compose/ozonesecure-mr/docker-config
3
$ grep -c 'container-executor' etc/hadoop/yarn-site.xml
0
{noformat}",pull-request-available,['docker'],HDDS,Bug,Minor,2019-10-02 07:01:48,0
13260009,Fix NPE in OzoneDelegationTokenManager#addPersistedDelegationToken,"The certClient was not initialized in proper order as a result, when OM restart with delegation token issued, the ozone delegation token secret manager NPE. ",pull-request-available,[],HDDS,Improvement,Blocker,2019-10-01 22:35:48,4
13259992,SCM fails to start in most unsecure environments due to leftover secure config,"Intermittent failure of {{ozone-recon}} and some other acceptance tests where SCM container is not available is caused by leftover secure config in {{core-site.xml}}.

Initially the config file is [empty|https://raw.githubusercontent.com/apache/hadoop/trunk/hadoop-hdds/common/src/main/conf/core-site.xml].  Various test environments populate it with different settings.  The problem happens when a test does not specify any config for {{core-site.xml}}, in which case the previous test's config file is retained.

{code}
scm_1       | 2019-10-01 19:42:05 WARN  WebAppContext:531 - Failed startup of context o.e.j.w.WebAppContext@1cc680e{/,file:///tmp/jetty-0.0.0.0-9876-scm-_-any-1272594486261557815.dir/webapp/,UNAVAILABLE}{/scm}
scm_1       | javax.servlet.ServletException: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/HTTP.keytab
scm_1       | 	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)
...
scm_1       | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:791)
...
scm_1       | Unable to initialize WebAppContext
scm_1       | 2019-10-01 19:42:05 INFO  StorageContainerManagerStarter:51 - SHUTDOWN_MSG:
scm_1       | /************************************************************
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at 8724df7131bb/192.168.128.6
scm_1       | ************************************************************/
{code}

The problem is intermittent due to ordering of test cases being different in different runs.  If a secure test is run earlier, more tests are affected.  If secure tests are run last, the issue does not happen.",pull-request-available,['docker'],HDDS,Bug,Major,2019-10-01 20:24:08,0
13259968,Fix loadup cache for cache cleanup policy NEVER,"During initial startup/restart of OM, if table has cache cleanup policy set to NEVER, we fill the table cache and also epochEntries. We do not need to add entries to epochEntries, as the epochEntries is used for eviction from the cache, once double buffer flushes to disk.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-01 17:34:35,2
13259925,Monitor datanodes in ozoneperf compose cluster,"ozoneperf compose cluster contains a prometheus but as of now it collects the data only from scm and om.

We don't know the exact number of datanodes (can be scaled up and down) therefor it's harder to configure the datanode host names. I would suggest to configure the first 10 datanodes (which covers most of the use cases)

How to test?
{code:java}
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozoneperf
docker-compose up -d
firefox http://localhost:9090/targets
  {code}
 ",pull-request-available,['docker'],HDDS,Task,Major,2019-10-01 14:15:39,1
13259852,TestSCMContainerPlacementRackAware has an intermittent failure,"For example from the nightly build:
{code:java}
  <testcase name=""testNoFallback[8]"" classname=""org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware"" time=""0.014"">
      
      
            <failure type=""java.lang.AssertionError"">java.lang.AssertionError
   
      
        	at org.junit.Assert.fail(Assert.java:86)
      
      
        	at org.junit.Assert.assertTrue(Assert.java:41)
      
      
        	at org.junit.Assert.assertTrue(Assert.java:52)
      
      
        	at org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware.testNoFallback(TestSCMContainerPlacementRackAware.java:276)
      
      
        	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      
      
        	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      
      
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      
      
        	at java.lang.reflect.Method.invoke(Method.java:498)
      
      
        	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 {code}
The problem is in the testNoFallback:

Let's say we have 11 nodes (from parameter) and we would like to choose 5 nodes (hard coded in the test).

As the first two replicas are chosen from the same rack an all the other from different racks it's not possible, so we except a failure.

But we have an assertion that the success count is at least 3. But this is true only if the first two replicas are placed to the rack1 (5 nodes) or rack2 (5nodes). If the replica is placed to the rack3 (one node) it will fail immediately:

 

Lucky case when we have success count > 3
{code:java}
 rack1 -- node1 
 rack1 -- node2 -- FIRST replica
 rack1 -- node3 -- SECOND replica
 rack1 -- node4
 rack1 -- node5 
 rack2 -- node6
 rack2 -- node7 -- THIRD replica
 rack2 -- node8
 rack2 -- node9 
 rack2 -- node10
 rack3 -- node11 -- FOURTH replica{code}
 The specific case when we have success count == 1, as we can't choose the second replica on rack3 (This is when the test is failing)
{code:java}
 rack1 -- node1 
 rack1 -- node2
 rack1 -- node3
 rack1 -- node4
 rack1 -- node5 
 rack2 -- node6
 rack2 -- node7
 rack2 -- node8
 rack2 -- node9 
 rack2 -- node10
 rack3 -- node11 -- FIRST replica{code}
 

 

 

 

 

 ",pull-request-available,[],HDDS,Improvement,Major,2019-10-01 09:28:39,1
13259738,Collect docker logs if env fails to start,"Occasionally some acceptance test docker environment fails to start up properly.  We need docker logs for analysis, but they are not being collected.

https://github.com/elek/ozone-ci-q4/blob/master/trunk/trunk-nightly-extra-20190930-74rp4/acceptance/output.log#L3765-L3768",pull-request-available,['test'],HDDS,Improvement,Major,2019-09-30 18:28:26,0
13259734,ContainerStateMachine should not be marked unhealthy if applyTransaction fails with closed container exception,"Currently, if applyTransaction fails, the stateMachine is marked unhealthy and next snapshot creation will fail. As a result of which the the raftServer will close down leading to pipeline failure. ClosedContainer exception should be ignored while marking the stateMachine unhealthy.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-30 17:52:34,3
13259624,Update Ratis to latest snapshot,This Jira aims to update ozone with latest ratis snapshot which has a crtical fix for retry behaviour on getting not leader exception in client.,pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-09-30 09:37:59,3
13259596,checkstyle.sh reports wrong failure count,"{{checkstyle.sh}} outputs files with checkstyle violations and the violations themselves on separate lines.  It then reports line count as number of failures.

{code:title=target/checkstyle/summary.txt}
hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java
 49: Unused import - org.apache.hadoop.ozone.om.OMMetadataManager.
{code}

{code:title=target/checkstyle/failures}
2
{code}",pull-request-available,['test'],HDDS,Bug,Trivial,2019-09-30 07:32:21,0
13259334,Recon does not handle the NULL snapshot from OM DB cleanly.,"{code}
2019-09-27 11:35:19,835 [pool-9-thread-1] ERROR      - Null snapshot location got from OM.
2019-09-27 11:35:19,839 [pool-9-thread-1] INFO       - Calling reprocess on Recon tasks.
2019-09-27 11:35:19,840 [pool-7-thread-1] INFO       - Starting a 'reprocess' run of ContainerKeyMapperTask.
2019-09-27 11:35:20,069 [pool-7-thread-1] INFO       - Creating new Recon Container DB at /tmp/recon/db/recon-container.db_1569609319840
2019-09-27 11:35:20,069 [pool-7-thread-1] INFO       - Cleaning up old Recon Container DB at /tmp/recon/db/recon-container.db_1569609258721.
2019-09-27 11:35:20,144 [pool-9-thread-1] ERROR      - Unexpected error :
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.reInitializeTasks(ReconTaskControllerImpl.java:181)
        at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:333)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.ozone.recon.tasks.ContainerKeyMapperTask.reprocess(ContainerKeyMapperTask.java:81)
        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$reInitializeTasks$3(ReconTaskControllerImpl.java:176)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-09-27 18:57:59,5
13259166,"Replication of Container fails with ""Only closed containers could be exported""","Replication of Container fails with ""Only closed containers could be exported""

cc: [~nanda]

{code}
2019-09-26 15:00:17,640 [grpc-default-executor-13] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(57)) - Streaming container data (37) to other
datanode
Sep 26, 2019 3:00:17 PM org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor run
SEVERE: Exception while executing runnable org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed@70e641f2
java.lang.IllegalStateException: Only closed containers could be exported: ContainerId=37
2019-09-26 15:00:17,644 [grpc-default-executor-17] ERROR replication.GrpcReplicationClient (GrpcReplicationClient.java:onError(142)) - Container download was unsuccessfull
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:527)
org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNKNOWN
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.exportContainer(KeyValueHandler.java:875)
        at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.exportContainer(ContainerController.java:134)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
        at org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
.java:64)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClient at org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolSCallListener.java:40)
erviceGrpc.java:217)
        at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
        at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls. at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
java:171)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClient at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
CallListener.java:40)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.ja at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
va:397)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
        at java.lang.Thread.run(Thread.java:748)

        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
:
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-27 01:17:59,2
13259144,Adding container related metrics in SCM,"This jira aims to add more container related metrics to SCM.
 Following metrics will be added as part of this jira:
 * Number of successful create container calls
 * Number of failed create container calls
 * Number of successful delete container calls
 * Number of failed delete container calls
 * Number of list container ops.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-09-26 22:21:11,2
13259120,Optimize Ozone CLI commands to send one ACL request to authorizers instead of sending multiple requests,"Currently, when trying to read a key, three requests are sent to the authorizer:
volume read, bucket read, key read.

 

It should instead be just one request to the authorizer.",TriagePending,['Ozone CLI'],HDDS,Bug,Major,2019-09-26 19:08:08,6
13259087,Implement LocatedFileStatus & getFileBlockLocations to provide node/localization information to Yarn/Mapreduce,"For applications like Hive/MapReduce to take advantage of the data locality in Ozone, Ozone should return the location of the Ozone blocks. This is needed for better read performance for Hadoop Applications.
{code}
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
{code}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-09-26 15:22:57,5
13259080,"ozone-mr test fails with No FileSystem for scheme ""o3fs""","HDDS-2101 changed how Ozone filesystem provider is configured.  {{ozone-mr}} tests [started failing|https://github.com/elek/ozone-ci/blob/2f2c99652af6b26a95f08eece9e545f0d72ccf45/pr/pr-hdds-2101-rtz55/acceptance/output.log#L255-L263], but it [wasn't noticed|https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-2101-rtz55/acceptance/result] due to HDDS-2185.

{code}
Running command 'ozone fs -mkdir /user'
${output} = mkdir: No FileSystem for scheme ""o3fs""
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-09-26 14:55:46,0
13259059,createmrenv failure not reflected in acceptance test result,"Part of the MR tests fail, but it's not reflected in the test report, which shows all green.

{noformat:title=https://github.com/elek/ozone-ci/blob/679228c146628cd4d1a416e1ffc9c513d19fb43d/pr/pr-hdds-2179-9bnxk/acceptance/output.log#L718-L730}
==============================================================================
hadoop31-createmrenv :: Create directories required for MR test               
==============================================================================
Create test volume, bucket and key                                    | PASS |
------------------------------------------------------------------------------
Create user dir for hadoop                                            | FAIL |
1 != 0
------------------------------------------------------------------------------
hadoop31-createmrenv :: Create directories required for MR test       | FAIL |
2 critical tests, 1 passed, 1 failed
2 tests total, 1 passed, 1 failed
==============================================================================
Output:  /tmp/smoketest/hadoop31/result/robot-hadoop31-hadoop31-createmrenv-scm.xml
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-09-26 13:45:03,0
13259017,Rename ozone scmcli to ozone admin,"Originally ozone scmcli designed to be used only by the developers. A very cryptic name is chosen intentionally to frighten away the beginner users.

As we realized recently we started to use ""ozone scmcli"" as a generic admin tool. More and more tools has been added which are useful not only for the developers but for the administrators.

Therefore I suggest to rename ""ozone scmcli"" to something more meaningful.

For example to ""ozone admin""

 ",pull-request-available,[],HDDS,Improvement,Major,2019-09-26 10:50:29,1
13258924,Ozone Manager should send correct ACL type in ACL requests to Authorizer,"Currently, Ozone manager sends ""WRITE"" as ACLType for key create, key delete and bucket create operation. Fix the acl type in all requests to the authorizer.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-09-25 23:54:24,6
13258884,ConfigFileGenerator fails with Java 10 or newer,"{code:title=mvn -f pom.ozone.xml -DskipTests -am -pl :hadoop-hdds-config clean package}
...
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hadoop-hdds-config ---
[INFO] Compiling 3 source files to hadoop-hdds/config/target/test-classes
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-hdds-config: Compilation failure
[ERROR] Can't generate the config file from annotation: hadoop-hdds/config/target/test-classes/ozone-default-generated.xml
{code}

The root cause is that new Java (I guess it's 9+, but tried only on 10+) throws a different {{IOException}} subclass: {{NoSuchFileException}} instead of {{FileNotFoundException}}.

{code}
java.nio.file.NoSuchFileException: hadoop-hdds/config/target/test-classes/ozone-default-generated.xml
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:374)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:425)
	at java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420)
	at java.base/java.nio.file.Files.newInputStream(Files.java:159)
	at jdk.compiler/com.sun.tools.javac.file.PathFileObject.openInputStream(PathFileObject.java:461)
	at java.compiler@13/javax.tools.ForwardingFileObject.openInputStream(ForwardingFileObject.java:74)
	at org.apache.hadoop.hdds.conf.ConfigFileGenerator.process(ConfigFileGenerator.java:62)
{code}
",pull-request-available,['build'],HDDS,Bug,Minor,2019-09-25 19:40:32,0
13258559,Dangling links in test report due to incompatible realpath,"Test summaries point to wrong locations, eg.:

{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/trunk/trunk-nightly-20190924-mj2km/integration/summary.md}
 * [org.apache.hadoop.ozone.scm.node.TestQueryNode](/tmp/log/trunk/trunk-nightly-20190924-mj2km/integration/workdir/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestQueryNode.txt) ([output](/tmp/log/trunk/trunk-nightly-20190924-mj2km/integration/workdir/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestQueryNode-output.txt/))
{code}

shouldn't include {{/workdir}}, nor {{/tmp/log/}}.

The root cause is that Busybox {{realpath}} does not accept options, rather returns absolute path:

{code:title=elek/ozone-build:20190825-1}
$ cd /etc
$ realpath --relative-to=$(pwd) motd
realpath: --relative-to=/etc: No such file or directory
/etc/motd
{code}

It worked previously because the docker image [was|https://github.com/elek/argo-ozone/commit/bad4b6747fa06c227dfcbff1f098f8d9c8179b79] based on a more complete Linux.

{code:title=elek/ozone-build:test}
$ cd /etc
$ realpath --relative-to=$(pwd) motd
motd
{code}

CC [~elek]",pull-request-available,['build'],HDDS,Bug,Major,2019-09-24 14:01:58,0
13258401,TestOzoneManagerDoubleBufferWithOMResponse sometimes fails with out of memory error,"testDoubleBuffer() in TestOzoneManagerDoubleBufferWithOMResponse fails with outofmemory exceptions at times in dev machines.

 ",pull-request-available,['Ozone Manager'],HDDS,Task,Major,2019-09-23 21:37:32,6
13258339,Hadoop31-mr acceptance test is failing due to the shading,"From the daily build:

{code}
 	Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/ozone/shaded/org/apache/http/client/utils/URIBuilder
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:138)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.CommandWithDestination.getRemoteDestination(CommandWithDestination.java:195)
	at org.apache.hadoop.fs.shell.CopyCommands$Put.processOptions(CopyCommands.java:259)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:175)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.ozone.shaded.org.apache.http.client.utils.URIBuilder
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 15 more
{code}

It can be reproduced locally with executing the tests:

{code}
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31
./test.sh
{code}",pull-request-available,[],HDDS,Bug,Major,2019-09-23 15:11:16,1
13258303,Some RPC metrics are missing from SCM prometheus endpoint,"In Hadoop metrics it's possible to register multiple metrics with the same name but with different tags. For example each RpcServere has an own metrics instance in SCM.

{code}
    ""name"" : ""Hadoop:service=StorageContainerManager,name=RpcActivityForPort9860"",
    ""name"" : ""Hadoop:service=StorageContainerManager,name=RpcActivityForPort9863"",
{code}

They are converted by PrometheusSink to a prometheus metric line with proper name and tags. For example:

{code}
rpc_rpc_queue_time60s_num_ops{port=""9860"",servername=""StorageContainerLocationProtocolService"",context=""rpc"",hostname=""72736061cbc5""} 0
{code}

The PrometheusSink uses a Map to cache all the recent values but unfortunately the key contains only the name (rpc_rpc_queue_time60s_num_ops in our example) but not the tags (port=...)

For this reason if there are multiple metrics with the same name, only the first one will be displayed.

As a result in SCM only the metrics of the first RPC server can be exported to the prometheus endpoint. 
",pull-request-available,[],HDDS,Bug,Major,2019-09-23 13:54:11,1
13258288,Freon fails if bucket does not exists,"{code:title=ozone freon ockg}
Bucket not found
...
Failures: 0
Successful executions: 0
{code}",pull-request-available,['Tools'],HDDS,Bug,Major,2019-09-23 13:18:00,0
13258155,om.db.checkpoints is getting filling up fast,"{{om.db.checkpoints}} is filling up fast, we should also clean this up.",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2019-09-22 15:39:42,5
13258060,"Add ""Replication factor"" to the output of list keys ","The output of ""ozone sh key list /vol1/bucket1"" does not include replication factor and it will be good to have it in the output.",pull-request-available,['Ozone CLI'],HDDS,Task,Major,2019-09-20 23:26:55,6
13258055,Make OM Generic related configuration support HA style config,"To have a single configuration to use across OM cluster, few of the configs like 

-OZONE_OM_KERBEROS_KEYTAB_FILE_KEY,-

-OZONE_OM_KERBEROS_PRINCIPAL_KEY,-

-OZONE_OM_HTTP_KERBEROS_KEYTAB_FILE,-

-OZONE_OM_HTTP_KERBEROS_PRINCIPAL_KEY need to support configs which append with service id and node id.-

 

Addressed OM_DB_DIRS, OZONE_OM_ADDRESS_KEY also in this patch.

 

This Jira is to fix the above configs.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-09-20 23:08:03,2
13258040,Add acceptance test for ozonesecure-mr compose,This will give us coverage of running basic MR jobs on security enabled OZONE cluster against YARN. ,pull-request-available,[],HDDS,Bug,Major,2019-09-20 21:24:41,4
13258034,Fix Race condition in ProfileServlet#pid,There is a race condition in ProfileServlet. The Servlet member field pid should not be used for local assignment. It could lead to race condition.,pull-request-available,[],HDDS,Bug,Major,2019-09-20 20:03:28,7
13258023,Fix Json Injection in JsonUtils,JsonUtils#toJsonStringWithDefaultPrettyPrinter() does not validate the Json String  before serializing it which could result in Json Injection.,pull-request-available,[],HDDS,Bug,Major,2019-09-20 19:02:02,7
13258018,checkstyle: print filenames relative to project root,"Currently {{checkstyle.sh}} prints files with violations using full path, eg:

{noformat:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190920-4x9x8/checkstyle/summary.txt}
...
/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadList.java
 23: Unused import - org.apache.hadoop.hdds.client.ReplicationType.
 24: Unused import - org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor.
/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadListParts.java
 23: Unused import - org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType.
/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartKeyInfo.java
 19: Unused import - org.apache.hadoop.hdds.client.ReplicationFactor.
 20: Unused import - org.apache.hadoop.hdds.client.ReplicationType.
 26: Unused import - java.time.Instant.
...
{noformat}

{{/workdir}} is specific to the CI environment.  Similarly, local checkout directory is specific to each developer.

Printing only path relative to project root ({{/workdir}} here) would make handling these paths easier (eg. reporting errors in JIRA or opening files locally for editing).",pull-request-available,['build'],HDDS,Improvement,Minor,2019-09-20 18:43:15,0
13257783,Fix alignment issues in HDDS doc pages,The cards in HDDS doc pages don't align properly and needs to be fixed.,pull-request-available,['documentation'],HDDS,Bug,Major,2019-09-19 21:21:15,6
13257634,Fix Checkstyle issues,"Unfortunately checkstyle checks didn't work well from HDDS-2106 to HDDS-2119. 

This patch fixes all the issues which are accidentally merged in the mean time. ",pull-request-available,[],HDDS,Improvement,Major,2019-09-19 10:06:35,1
13257571,Ozone client fails with OOM while writing a large (~300MB) key.,"{code}
dd if=/dev/zero of=testfile bs=1024 count=307200
ozone sh key put /vol1/bucket1/key testfile
{code}

{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) at org.apache.hadoop.hdds.scm.storage.BufferPool.allocateBufferIfNeeded(BufferPool.java:66) at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:234) at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129) at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:211) at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193) at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49) at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96) at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:117) at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:55) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141)
{code}",TriagePending,['Ozone Client'],HDDS,Bug,Major,2019-09-19 05:12:02,3
13257570,Ozone client prints the entire request payload in DEBUG level.,"In XceiverClientRatis.java:221, we have the following snippet where we have a DEBUG line that prints out the entire Container Request proto. 

{code}
      ContainerCommandRequestProto finalPayload =
          ContainerCommandRequestProto.newBuilder(request)
              .setTraceID(TracingUtil.exportCurrentSpan())
              .build();
      boolean isReadOnlyRequest = HddsUtils.isReadOnly(finalPayload);
      ByteString byteString = finalPayload.toByteString();
      LOG.debug(""sendCommandAsync {} {}"", isReadOnlyRequest, finalPayload);
      return isReadOnlyRequest ?
          getClient().sendReadOnlyAsync(() -> byteString) :
          getClient().sendAsync(() -> byteString);
{code}

This causes OOM while writing large (~300MB) keys. 

{code}
SLF4J: Failed toString() invocation on an object of type [org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos$ContainerCommandRequestProto]
Reported exception:
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3332)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649)
	at java.lang.StringBuilder.append(StringBuilder.java:202)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormatEscaper.escapeBytes(TextFormatEscaper.java:75)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormatEscaper.escapeBytes(TextFormatEscaper.java:94)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.escapeBytes(TextFormat.java:1836)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:436)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:376)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:338)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.print(TextFormat.java:325)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:449)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:376)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:338)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.print(TextFormat.java:325)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.access$000(TextFormat.java:307)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.print(TextFormat.java:68)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.printToString(TextFormat.java:148)
	at org.apache.ratis.thirdparty.com.google.protobuf.AbstractMessage.toString(AbstractMessage.java:117)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)
	at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)
	at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:252)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:221)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:302)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:310)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:601)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:459)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:240)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129)
SLF4J: Failed toString() invocation on an object of type [org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos$ContainerCommandRequestProto]
Reported exception:
java.lang.OutOfMemoryError: Java heap space
{code}",pull-request-available,[],HDDS,Bug,Major,2019-09-19 05:07:10,0
13257519,Update dependency versions to avoid security vulnerabilities,"The following dependency versions have known security vulnerabilities. We should update them to recent/ later versions.
 * Apache Thrift 0.11.0
 * Apache Zookeeper 3.4.13
 * Jetty Servlet 9.3.24",pull-request-available,[],HDDS,Bug,Major,2019-09-18 21:37:00,7
13257511,Replace findbugs with spotbugs,"Findbugs has been marked deprecated and all future work is now happening under SpotBugs project.

This Jira is to investigate and possibly transition to Spotbugs in Ozone

 

Ref1 - [https://mailman.cs.umd.edu/pipermail/findbugs-discuss/2017-September/004383.html]

Ref2 - [https://spotbugs.github.io/]

 

A turn off for developers is that IntelliJ does not yet have a plugin for Spotbugs - [https://youtrack.jetbrains.com/issue/IDEA-201846]",pull-request-available,[],HDDS,Improvement,Major,2019-09-18 21:17:50,0
13257434,Include dumpstream in test report,"Include {{*.dumpstream}} in the unit test report, which may help finding out the cause of {{Corrupted STDOUT}} warning of forked JVM.

{noformat:title=https://github.com/elek/ozone-ci/blob/5429d0982c3b13d311ec353dba198f2f5253757c/pr/pr-hdds-2141-4zm8s/unit/output.log#L333-L334}
[INFO] Running org.apache.hadoop.utils.TestMetadataStore
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /workdir/hadoop-hdds/common/target/surefire-reports/2019-09-18T12-58-05_531-jvmRun1.dumpstream
{noformat}",pull-request-available,['Tools'],HDDS,Improvement,Minor,2019-09-18 16:45:22,0
13257427,Optimize block write path performance by reducing no of watchForCommit calls,"Currently, the watchForCommit calls from client to Ratis server for All replicated semantics happens when the max buffer limit is reached which can potentially be called 4 times as per the default configs for a single full block write. The idea here is inspect and add optimizations to reduce the no of watchForCommit calls.",TriagePending,['Ozone Client'],HDDS,Bug,Major,2019-09-18 16:24:45,3
13257236,MR job failing on secure Ozone cluster,"Failing with below error:
Caused by: Client cannot authenticate via:[TOKEN, KERBEROS]
org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)
at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)
at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
at org.apache.hadoop.ipc.Client.call(Client.java:1403)
at org.apache.hadoop.ipc.Client.call(Client.java:1367)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
at com.sun.proxy.$Proxy79.submitRequest(Unknown Source)
at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy79.submitRequest(Unknown Source)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:332)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1163)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
at com.sun.proxy.$Proxy80.getServiceList(Unknown Source)
at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:248)
at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:167)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)
at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)
at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:102)
at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:155)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:268)
at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Blocker,2019-09-17 23:24:22,2
13257232,Rename classes under package org.apache.hadoop.utils,"Rename classes under package org.apache.hadoop.utils -> org.apache.hadoop.hdds.utils in hadoop-hdds-common

 

Now, with current way, we might collide with hadoop classes.",pull-request-available,[],HDDS,Bug,Major,2019-09-17 22:40:49,2
13257087,OM metrics mismatch (abort multipart request),AbortMultipartUpload failure count can be higher than request count.,pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-17 11:28:54,0
13257063,Missing total number of operations,Total number of operations is missing from some metrics graphs.,pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-17 08:59:03,0
13257000,Update BeanUtils and Jackson Databind dependency versions,"The following Ozone dependencies have known security vulnerabilities. We should update them to newer/ latest versions.
 * Apache Common BeanUtils version 1.9.3
 * Fasterxml Jackson version 2.9.5",pull-request-available,[],HDDS,Bug,Major,2019-09-17 01:16:53,7
13256936,OM bucket operations do not add up,"Total OM bucket operations may be higher than sum of counts for individual operation type, because S3 bucket operations are displayed in separate charts.",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-16 18:00:49,0
13256866,OM block allocation metric not paired with its failures,Block allocation count and block allocation failure count are shown in separate graphs.,pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-16 12:25:54,0
13256838,OM Metric mismatch (MultipartUpload failures),"{{incNumCommitMultipartUploadPartFails()}} increments {{numInitiateMultipartUploadFails}} instead of the counter for commit failures.

https://github.com/apache/hadoop/blob/85b1c728e4ed22f03db255f5ef34a2a79eb20d52/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java#L310-L312",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-16 10:18:10,0
13256832,OM Metrics graphs include empty request type,"Ozone Manager Metrics seems to include an odd empty request type ""s"".",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-09-16 09:42:51,0
13256684,TestKeyValueContainer is failing,"{{TestKeyValueContainer}} is failing with the following exception 
{noformat}
[ERROR] testContainerImportExport(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.173 s  <<< ERROR!
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:201)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:500)
	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testContainerImportExport(TestKeyValueContainer.java:235)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-09-14 09:43:48,0
13256632,Using dist profile fails with pom.ozone.xml as parent pom,The build fails with the {{dist}} profile. Details in a comment below.,pull-request-available,[],HDDS,Bug,Major,2019-09-13 22:20:12,1
13256599,Detailed Tools doc not reachable,"There are two doc pages for tools:
 * docs/beyond/tools.html
 * docs/tools.html

The latter is more detailed (has subpages for several tools), but it is not reachable (even indirectly) from the start page.  Not sure if this is intentional.

On a related note, it has two ""Testing tools"" sub-pages. One of them is empty and should be removed.",pull-request-available,['documentation'],HDDS,Bug,Major,2019-09-13 19:14:16,1
13256516,Random next links ,"_Next>>_ links at the bottom of some documentation pages seem to be out of order.

 * _Simple Single Ozone_ (""easy start"") should link to one of the intermediate level pages, but has no _Next_ link
 * _Building From Sources_ (ninja) should be the last (no _Next_ link), but points to _Minikube_ (intermediate)
 * _Pseudo-cluster_ (intermediate) should point to the ninja level, but leads to _Simple Single Ozone_ (easy start)",pull-request-available,['documentation'],HDDS,Bug,Blocker,2019-09-13 12:17:30,0
13256495,Broken logo image on category sub-pages,"Ozone logo at the top left is broken on sub-pages, eg. _Recipes / Monitoring with Prometheus_ and _Programming Interfaces / Java API_.

 !broken_image.png! ",pull-request-available,['documentation'],HDDS,Bug,Major,2019-09-13 09:47:58,0
13256411,Create a shaded ozone filesystem (client) jar,"We need a shaded Ozonefs jar that does not include Hadoop ecosystem components (Hadoop, HDFS, Ratis, Zookeeper).

A common expected use case for Ozone is Hadoop clients (3.2.0 and later) wanting to access Ozone via the Ozone Filesystem interface. For these clients, we want to add Ozone file system jar to the classpath, however we want to use Hadoop ecosystem dependencies that are `provided` and already expected to be in the client classpath.

Note that this is different from the legacy jar which bundles a shaded Hadoop 3.2.0.",pull-request-available,['build'],HDDS,Improvement,Blocker,2019-09-12 19:24:59,1
13256402,Remove hadoop classes from ozonefs-current jar,"We have two kind of ozone file system jars: current and legacy. current is designed to work only with exactly the same hadoop version which is used for compilation (3.2 as of now).

But as of now the hadoop classes are included in the current jar which is not necessary as the jar is expected to be used in an environment where  the hadoop classes (exactly the same hadoop classes) are already there. They can be excluded.",pull-request-available,[],HDDS,Improvement,Major,2019-09-12 18:06:18,1
13256305,ContainerStateMachine#writeStateMachineData times out,"The issue seems to be happening because the below precondition check fails in case two writeChunk gets executed in parallel and the runtime exception thrown is handled correctly in ContainerStateMachine.

 

HddsDispatcher.java:239
{code:java}
Preconditions
    .checkArgument(!container2BCSIDMap.containsKey(containerID));
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-12 09:02:26,3
13256106,XSS fragments can be injected to the S3g landing page  ,"VULNERABILITY DETAILS
There is a way to bypass anti-XSS filter for DOM XSS exploiting a ""window.location.href"".

Considering a typical URL:

scheme://domain:port/path?query_string#fragment_id

Browsers encode correctly both ""path"" and ""query_string"", but not the ""fragment_id"". 

So if used ""fragment_id"" the vector is also not logged on Web Server.

VERSION
Chrome Version: 10.0.648.134 (Official Build 77917) beta

REPRODUCTION CASE
This is an index.html page:


{code:java}
aws s3api --endpoint <script>document.write(window.location.href.replace(""static/"", """"))</script> create-bucket --bucket=wordcount</pre>
{code}


The attack vector is:
index.html?#<script>alert('XSS');</script>

* PoC:
For your convenience, a minimalist PoC is located on:
http://security.onofri.org/xss_location.html?#<script>alert('XSS');</script>

* References
- DOM Based Cross-Site Scripting or XSS of the Third Kind - http://www.webappsec.org/projects/articles/071105.shtml


reference:- 

https://bugs.chromium.org/p/chromium/issues/detail?id=76796",pull-request-available,['S3'],HDDS,Bug,Major,2019-09-11 12:35:54,1
13256103,Arbitrary file can be downloaded with the help of ProfilerServlet,"The LOC 324 in the file [ProfileServlet.java|https://github.com/apache/hadoop/blob/217bdbd940a96986df3b96899b43caae2b5a9ed2/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] is prone to an arbitrary file download:-
{code:java}
protected void doGetDownload(String fileName, final HttpServletRequest req,      final HttpServletResponse resp) throws IOException {

File requestedFile = ProfileServlet.OUTPUT_DIR.resolve(fileName).toAbsolutePath().toFile();{code}
As the String fileName is directly considered as the requested file.

 

Which is called at LOC 180 with HTTP request directly passed:-
{code:java}
if (req.getParameter(""file"") != null) {      doGetDownload(req.getParameter(""file""), req, resp);      
return;    
}
{code}
 ",pull-request-available,['Native'],HDDS,Bug,Major,2019-09-11 12:18:57,1
13256092,Refactor scm.container.client config,Extract typesafe config related to HDDS client with prefix {{scm.container.client}}.,pull-request-available,['SCM Client'],HDDS,Sub-task,Major,2019-09-11 10:21:00,0
13256040,TestContainerSmallFile#testReadWriteWithBCSId failure,"{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190910-vk757/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestContainerSmallFile.txt}
Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 384.415 s <<< FAILURE! - in org.apache.hadoop.ozone.scm.TestContainerSmallFile
testReadWriteWithBCSId(org.apache.hadoop.ozone.scm.TestContainerSmallFile)  Time elapsed: 364.439 s  <<< ERROR!
java.io.IOException: 
Failed to command cmdType: PutSmallFile
...
Caused by: org.apache.ratis.protocol.AlreadyClosedException: client-8C96F0B39BBE->72902ab5-0e57-412f-a398-68ab8a9029d1 is closed.
{code}

Hi [~shashikant], this failure is consistently reproducible starting with the [commit|https://github.com/apache/hadoop/commit/469165e6f29] for HDDS-1843.  Can you please check?",TriagePending,['test'],HDDS,Bug,Major,2019-09-11 07:22:43,3
13255980,Datanodes should retry forever to connect to SCM in an unsecure environment,"In an unsecure environment, the datanodes try upto 10 times after waiting for 1000 milliseconds each time before throwing this error:
{code:java}
Unable to communicate to SCM server at scm:9861 for past 0 seconds.
java.net.ConnectException: Call From scm/10.65.36.118 to scm:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy33.getVersion(Unknown Source)
	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:112)
	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 13 more
{code}
The datanodes should try forever to connect with SCM and not throw any errors.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-10 21:38:48,6
13255751,Avoid usage of hadoop projects as parent of hdds/ozone,"Ozone uses hadoop as a dependency. The dependency defined on multiple level:

 1. the hadoop artifacts are defined in the <dependency> sections
 2. both hadoop-ozone and hadoop-hdds projects uses ""hadoop-project"" as the parent

As we already have a slightly different assembly process it could be more resilient to use a dedicated parent project instead of the hadoop one. With this approach it will be easier to upgrade the versions as we don't need to be careful about the pom contents only about the used dependencies.",pull-request-available,[],HDDS,Improvement,Blocker,2019-09-09 22:32:47,1
13255722,TestContainerReplication fails due to unhealthy container,"{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190907-l8mkd/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.container.TestContainerReplication.txt}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.771 s <<< FAILURE! - in org.apache.hadoop.ozone.container.TestContainerReplication
testContainerReplication(org.apache.hadoop.ozone.container.TestContainerReplication)  Time elapsed: 12.702 s  <<< FAILURE!
java.lang.AssertionError: Container is not replicated to the destination datanode
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.apache.hadoop.ozone.container.TestContainerReplication.testContainerReplication(TestContainerReplication.java:153)
{code}

caused by:

{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190907-l8mkd/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.container.TestContainerReplication-output.txt}
java.lang.IllegalStateException: Only closed containers could be exported: ContainerId=1
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:525)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.exportContainer(KeyValueHandler.java:875)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.exportContainer(ContainerController.java:134)
	at org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource.java:64)
	at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)
{code}

Container is in unhealthy state because pipeline is not found for it in {{CloseContainerCommandHandler}}.",pull-request-available,['test'],HDDS,Bug,Major,2019-09-09 20:30:15,0
13255403,Ozone filesystem provider doesn't exist,"We don't have a filesystem provider in META-INF. 
i.e. following file doesn't exist.
{{hadoop-ozone/ozonefs/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}}

See for example
{{hadoop-tools/hadoop-aws/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Critical,2019-09-07 04:22:01,6
13255347,Ozone shell command prints out ERROR when the log4j file is not present.,"*Exception Trace*
{code}
log4j:ERROR Could not read configuration file from URL [file:/etc/ozone/conf/ozone-shell-log4j.properties].
java.io.FileNotFoundException: /etc/ozone/conf/ozone-shell-log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
	at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
	at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
	at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
	at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
	at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)
	at org.apache.hadoop.ozone.web.ozShell.Shell.<clinit>(Shell.java:35)
log4j:ERROR Ignoring configuration file [file:/etc/ozone/conf/ozone-shell-log4j.properties].
log4j:WARN No appenders could be found for logger (io.jaegertracing.thrift.internal.senders.ThriftSenderFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{
  ""metadata"" : { },
  ""name"" : ""vol-test-putfile-1567740142"",
  ""admin"" : ""root"",
  ""owner"" : ""root"",
  ""creationTime"" : 1567740146501,
  ""acls"" : [ {
    ""type"" : ""USER"",
    ""name"" : ""root"",
    ""aclScope"" : ""ACCESS"",
    ""aclList"" : [ ""ALL"" ]
  }, {
    ""type"" : ""GROUP"",
    ""name"" : ""root"",
    ""aclScope"" : ""ACCESS"",
    ""aclList"" : [ ""ALL"" ]
  } ],
  ""quota"" : 1152921504606846976
}
{code}


*Fix*
When a log4j file is not present, the default should be console.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-09-06 18:51:06,5
13255316,Ozone ACL document missing AddAcl API,"Current Ozone Native ACL APIs document looks like below, the AddAcl is missing.

 
h3. Ozone Native ACL APIs

The ACLs can be manipulated by a set of APIs supported by Ozone. The APIs supported are:
 # *SetAcl* – This API will take user principal, the name, type of the ozone object and a list of ACLs.
 # *GetAcl* – This API will take the name and type of the ozone object and will return a list of ACLs.
 # *RemoveAcl* - This API will take the name, type of the ozone object and the ACL that has to be removed.",pull-request-available,[],HDDS,Bug,Major,2019-09-06 16:26:19,4
13254999,Failing acceptance test - smoketests.ozonesecure-s3.MultipartUpload,"{{""smoketests.ozonesecure-s3.MultipartUpload.Test Multipart Upload with the simplified aws s3 cp API""}} acceptance test is failing.",TriagePending,['test'],HDDS,Bug,Major,2019-09-05 12:58:09,1
13254906,Remove the hard coded config key in ChunkManager,"We have a hard-coded config key in the {{ChunkManagerFactory.java.}}

 
{code}
boolean scrubber = config.getBoolean(
 ""hdds.containerscrub.enabled"",
 false);
{code}",pull-request-available,[],HDDS,Bug,Major,2019-09-05 03:43:50,6
13254529,Fix TestRatisPipelineProvider#testCreatePipelinesDnExclude,"
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider
-------------------------------------------------------------------------------
Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.374 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider
testCreatePipelinesDnExclude(org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider)  Time elapsed: 0.044 s  <<< ERROR!
org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 2 nodes.
	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:151)
	at org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.testCreatePipelinesDnExclude(TestRatisPipelineProvider.java:182)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


{code}
",pull-request-available,['test'],HDDS,Bug,Major,2019-09-04 02:14:17,5
13254474,Fix TestSecureOzoneManager,[https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-1909-plfbr/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestSecureOzoneManager.txt],pull-request-available,[],HDDS,Bug,Major,2019-09-03 20:26:39,4
13254472,Get/Renew DelegationToken NPE after HDDS-1909,[https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-1909-plfbr/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestSecureOzoneCluster.txt],pull-request-available,[],HDDS,Bug,Major,2019-09-03 20:25:22,4
13254328,Read fails because the block cannot be located in the container,"Read fails as the client is not able to read the block from the container.

{code}
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Unable to find the block with bcsID 2515 .Container 7 bcsId is 0.
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:536)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.lambd2a0$getValid1a9to-08-30 12:51:20,081 | INFO  | SCMAudit | user=msingh | ip=192.168.0.r103 |List$0(ContainerP
rotocolCalls.java:569)
{code}


The client eventually exits here
{code}
2019-08-30 12:51:20,081 [pool-224-thread-6] ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:readData(176)) - LOADGEN: Read key:pool-224-thread-6_330651 failed with ex
ception
ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(121)) - LOADGEN: Exiting due to exception
{code}",MiniOzoneChaosCluster pull-request-available,"['Ozone Client', 'Ozone Datanode']",HDDS,Bug,Blocker,2019-09-03 06:11:02,3
13254203,Tracing in OzoneManager call is propagated with wrong parent,"As you can see in the attached screenshot the OzoneManager.createBucket (server side) tracing information is the children of the freon.createBucket instead of the freon OzoneManagerProtocolPB.submitRequest.

To avoid confusion the hierarchy should be fixed (Most probably we generate the child span AFTER we already serialized the parent one to the message) ",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:49:45,0
13254198,Make SCMSecurityProtocol message based,"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

SCMSecurityProtocol.proto is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-02 09:21:41,1
13254197,Make StorageContainerLocationProtocolService message based,"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

StorageContainerLocationProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-02 09:20:48,1
13254196,Support filters in ozone insight point,"With Ozone insight we can print out all the logs / metrics of one specific component s (eg. scm.node-manager or scm.node-manager).

It would be great to support additional filtering capabilities where the output is filtered based on specific keys.

For example to print out all of the logs related to one datanode or related to one type of RPC request.

Filter should be a key value map (eg. --filter datanode=sjdhfhf,rpc=createChunk) which can be defined in the ozone insight CLI.

As we have no option to add additional tags to the logs (it may be supported by log4j2 but not with slf4k), the first implementation can be implemented by pattern matching.

For example in SCMNodeManager.processNodeReport contains trace/debug logs which includes the "" [datanode={}]"" part. This formatting convention can be used to print out the only the related information. ",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:19:43,1
13254195,Create insight point to debug one specific pipeline,"Wit the first implementation of ozone insight tool we had a demo insight-point to debug Ratis pipelines. It was not stable enough to include in the first patch, this patch is about fixing it.

The goal is to implement a new insight point (eg. datanode.pipeline) which can show information about one pipeline.

It can be done with retrieving the hosts of the pipeline and generate the loggers metrics (InsightPoint.getRelatedLoggers and InsightPoint.getMetrics) based on the pipeline information (same loggers should be displayed from all the three datanodes.

The pipeline id can be defined as a filter parameter which (in this case) should be required.",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:15:13,1
13254192,Make StorageContainerDatanodeProtocolService message based,"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

StorageContainerDatanodeProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-02 09:10:33,1
13254191,Create generic service facade with tracing/metrics/logging support,"We started to use a message based GRPC approach. Wen have only one method and the requests are routed based on a ""type"" field in the proto message. 

For example in OM protocol:

{code}
/**
 The OM service that takes care of Ozone namespace.
*/
service OzoneManagerService {
    // A client-to-OM RPC to send client requests to OM Ratis server
    rpc submitRequest(OMRequest)
          returns(OMResponse);
}
{code}

And 

{code}

message OMRequest {
  required Type cmdType = 1; // Type of the command

...
{code}

This approach makes it possible to use the same code to process incoming messages in the server side.

ScmBlockLocationProtocolServerSideTranslatorPB.send method contains the logic of:

 * Logging the request/response message (can be displayed with ozone insight)
 * Updated metrics
 * Handle open tracing context propagation.


These functions are generic. For example OzoneManagerProtocolServerSideTranslatorPB use the same (=similar) code.

The goal in this jira is to provide a generic utility and move the common code for tracing/request logging/response logging/metrics calculation to a common utility which can be used from all the ServerSide translators.",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:07:42,1
13254190,Improve the observability inside Ozone,"To improve the observability is a key requirement to achieve better correctness and performance with Ozone.

This jira collects some of the tasks which can provide better visibility to the ozone internals.

We have two main tools:

 * Distributed tracing (opentracing) can help to detected performance battlenecks
 * Ozone insight tool (a simple cli frontend for Hadoop metrics and log4j logging) can help to get better understanding about the current state/behavior of specific components.

Both of them can be improved to make it more powerful.",TriagePending,['Tools'],HDDS,Improvement,Major,2019-09-02 08:59:56,1
13253920,Integration tests create untracked file audit.log,"An untracked {{audit.log}} file is created during integration test run.  Eg:

{code}
$ mvn -Phdds -pl :hadoop-ozone-integration-test test -Dtest=Test2WayCommitInRatis
...
$ git status
...
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)

	hadoop-ozone/integration-test/audit.log
{code}",pull-request-available,['test'],HDDS,Bug,Trivial,2019-08-30 12:23:20,0
13253878,Add hdds.container.chunk.persistdata as exception to TestOzoneConfigurationFields,"HDDS-1094 introduced a new config key ([hdds.container.chunk.persistdata|https://github.com/apache/hadoop/blob/96f7dc1992246a16031f613e55dc39ea0d64acd1/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsConfigKeys.java#L241-L245]), which needs to be added to {{ozone-default.xml}}, too.

https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190830-rr75b/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestOzoneConfigurationFields.txt",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-08-30 09:26:41,0
13253864,Create Ozone specific LICENSE file for the Ozone source and binary packages,"With HDDS-2058 the Ozone (source) release package doesn't contains the hadoop sources any more. We need to create an adjusted LICENSE file for the Ozone source package (We already created a specific LICENSE file for the binary package which is not changed).

In the new LICENSE file we should include entries only for the sources which are part of the Ozone release.",pull-request-available,[],HDDS,Improvement,Blocker,2019-08-30 08:25:11,1
13253589,Fix TestOzoneManagerRatisServer failure,"{{TestOzoneManagerRatisServer}} is failing on trunk with the following error
{noformat}
[ERROR] verifyRaftGroupIdGenerationWithCustomOmServiceId(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerRatisServer)  Time elapsed: 0.418 s  <<< ERROR!
org.apache.hadoop.metrics2.MetricsException: Metrics source OzoneManagerDoubleBufferMetrics already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
	at org.apache.hadoop.ozone.om.ratis.metrics.OzoneManagerDoubleBufferMetrics.create(OzoneManagerDoubleBufferMetrics.java:50)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.<init>(OzoneManagerDoubleBuffer.java:110)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.<init>(OzoneManagerDoubleBuffer.java:88)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.<init>(OzoneManagerStateMachine.java:87)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.getStateMachine(OzoneManagerRatisServer.java:314)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.<init>(OzoneManagerRatisServer.java:244)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.newOMRatisServer(OzoneManagerRatisServer.java:302)
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerRatisServer.verifyRaftGroupIdGenerationWithCustomOmServiceId(TestOzoneManagerRatisServer.java:209)
...
{noformat}

(Thanks [~nandakumar131] for the stack trace.)",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-28 23:26:56,4
13253586,Separate the metadata directories to store security certificates and keys for different services,"Currently, certificates and keys are stored in ozone.metadata.dirs and this needs to be moved to specific metadata dir for each service.",Triage,['Security'],HDDS,Bug,Major,2019-08-28 23:19:13,4
13253564,Rat check failure in decommissioning.md,"{code}
hadoop-hdds/docs/target/rat.txt: !????? /var/jenkins_home/workspace/ozone/hadoop-hdds/docs/content/design/decommissioning.md
{code}",pull-request-available,['documentation'],HDDS,Bug,Minor,2019-08-28 19:41:56,0
13253554,Error while compiling ozone-recon-web,"The following error is seen while compiling {{ozone-recon-web}}
{noformat}
[INFO] Running 'yarn install' in /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web
[INFO] yarn install v1.9.2
[INFO] [1/4] Resolving packages...
[INFO] [2/4] Fetching packages...
[ERROR] (node:31190) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
[INFO] [3/4] Linking dependencies...
[ERROR] warning "" > less-loader@5.0.0"" has unmet peer dependency ""webpack@^2.0.0 || ^3.0.0 || ^4.0.0"".
[INFO] [4/4] Building fresh packages...
[ERROR] warning Error running install script for optional dependency: ""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents: Command failed.
[ERROR] Exit code: 1
[ERROR] Command: node install
[ERROR] Arguments:
[ERROR] Directory: /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents
[ERROR] Output:
[ERROR] node-pre-gyp info it worked if it ends with ok
[INFO] info This module is OPTIONAL, you can safely ignore this error
[ERROR] node-pre-gyp info using node-pre-gyp@0.12.0
[ERROR] node-pre-gyp info using node@12.1.0 | darwin | x64
[ERROR] node-pre-gyp WARN Using request for node-pre-gyp https download
[ERROR] node-pre-gyp info check checked for \""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/lib/binding/Release/node-v72-darwin-x64/fse.node\"" (not found)
[ERROR] node-pre-gyp http GET https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp http 404 https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp WARN Tried to download(404): https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp WARN Pre-built binaries not found for fsevents@1.2.8 and node@12.1.0 (node-v72 ABI, unknown) (falling back to source compile with node-gyp)
[ERROR] node-pre-gyp http 404 status code downloading tarball https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp ERR! build error
[ERROR] node-pre-gyp ERR! stack Error: Failed to execute 'node-gyp clean' (Error: spawn node-gyp ENOENT)
[ERROR] node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/node_modules/node-pre-gyp/lib/util/compile.js:77:29)
[ERROR] node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:196:13)
[ERROR] node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)
[ERROR] node-pre-gyp ERR! stack     at onErrorNT (internal/child_process.js:431:16)
[ERROR] node-pre-gyp ERR! stack     at processTicksAndRejections (internal/process/task_queues.js:84:17)
[ERROR] node-pre-gyp ERR! System Darwin 18.5.0
[ERROR] node-pre-gyp ERR! command \""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/target/node/node\"" \""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/node_modules/node-pre-gyp/bin/node-pre-gyp\"" \""install\"" \""--fallback-to-build\""
[ERROR] node-pre-gyp ERR! cwd /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents
[ERROR] node-pre-gyp ERR! node -v v12.1.0
[ERROR] node-pre-gyp ERR! node-pre-gyp -v v0.12.0
[ERROR] node-pre-gyp ERR! not ok
[ERROR] Failed to execute 'node-gyp clean' (Error: spawn node-gyp ENOENT)""
[INFO] Done in 102.54s.
{noformat}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-08-28 18:03:28,6
13253353,Datanodes fail to come up after 10 retries in a secure environment,"{code:java}
10:06:36.585 PM    ERROR    HddsDatanodeService    
Error while storing SCM signed certificate.
java.net.ConnectException: Call From jmccarthy-ozone-secure-2.vpc.cloudera.com/10.65.50.127 to jmccarthy-ozone-secure-1.vpc.cloudera.com:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
    at org.apache.hadoop.ipc.Client.call(Client.java:1457)
    at org.apache.hadoop.ipc.Client.call(Client.java:1367)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
    at com.sun.proxy.$Proxy15.getDataNodeCertificate(Unknown Source)
    at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.getDataNodeCertificateChain(SCMSecurityProtocolClientSideTranslatorPB.java:156)
    at org.apache.hadoop.ozone.HddsDatanodeService.getSCMSignedCert(HddsDatanodeService.java:278)
    at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:248)
    at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:211)
    at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:168)
    at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:143)
    at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:70)
    at picocli.CommandLine.execute(CommandLine.java:1173)
    at picocli.CommandLine.access$800(CommandLine.java:141)
    at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
    at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
    at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
    at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
    at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
    at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
    at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
    at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:126)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
    at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
    at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
    at org.apache.hadoop.ipc.Client.call(Client.java:1403)
    ... 21 more
{code}

Datanodes try to get SCM signed certificate for just 10 times with interval of 1 sec. When SCM takes a little longer to come up, datanodes throw an exception and fail.",pull-request-available,"['Ozone Datanode', 'Security']",HDDS,Bug,Major,2019-08-27 22:29:21,4
13253310,Partially started compose cluster left running,"If any container in the sample cluster [fails to start|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L24], all successfully started containers are left running.  This [prevents|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L59] any further acceptance tests from normal completion.  This is only a minor inconvenience, since acceptance test as a whole fails either way.",pull-request-available,"['docker', 'test']",HDDS,Bug,Minor,2019-08-27 17:40:14,0
13253188,Avoid log on console with Ozone shell,HDDS-1489 fixed several sample docker compose configs to avoid unnecessary messages on console when running eg. {{ozone sh key put}}.  The goal of this task is to fix the remaining ones.,pull-request-available,['docker'],HDDS,Bug,Minor,2019-08-27 06:21:07,0
13253150,Don't depend on DFSUtil to check HTTP policy,"Currently, BaseHttpServer uses DFSUtil to get Http policy. With this, when http policy is set to HTTPS on hdfs-site.xml, ozone http servers try to come up with HTTPS and fail if SSL certificates are not present in the required location.

Ozone web UIs should not depend on HDFS config to determine HTTP policy. Instead, it should have its own config to determine the policy. ",pull-request-available,['website'],HDDS,Task,Blocker,2019-08-26 23:12:19,4
13252912,Ozone client should retry writes in case of any ratis/stateMachine exceptions,"Currently, Ozone client retry writes on a different pipeline or container in case of some specific exceptions. But in case, it sees exception such as DISK_FULL, CONTAINER_UNHEALTHY or any corruption , it just aborts the write. In general, the every such exception on the client should be a retriable  exception in ozone client and on some specific exceptions, it should take some more specific exception like excluding certain containers or pipelines while retrying or informing SCM of a corrupt replica etc.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-08-26 05:34:22,3
13252834,Generate simplifed reports by the dev-support/checks/*.sh scripts,"hadoop-ozone/dev-support/checks directory contains shell scripts to execute different type of code checks (findbugs, checkstyle, etc.)

Currently the contract is very simple. Every shell script executes one (and only one) check and the shell response code is set according to the result (non-zero code if failed).

To have better reporting in the github pr build, it would be great to improve the scripts to generate simple summary files and save the relevant files for archiving.",pull-request-available,['build'],HDDS,Improvement,Major,2019-08-24 21:56:39,1
13252711,Overlapping chunk region cannot be read concurrently,"Concurrent requests to datanode for the same chunk may result in the following exception in datanode:

{code}
java.nio.channels.OverlappingFileLockException
   at java.base/sun.nio.ch.FileLockTable.checkList(FileLockTable.java:229)
   at java.base/sun.nio.ch.FileLockTable.add(FileLockTable.java:123)
   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.addToFileLockTable(AsynchronousFileChannelImpl.java:178)
   at java.base/sun.nio.ch.SimpleAsynchronousFileChannelImpl.implLock(SimpleAsynchronousFileChannelImpl.java:185)
   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.lock(AsynchronousFileChannelImpl.java:118)
   at org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils.readData(ChunkUtils.java:175)
   at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerImpl.readChunk(ChunkManagerImpl.java:213)
   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleReadChunk(KeyValueHandler.java:574)
   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:195)
   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:271)
   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
{code}

It seems this is covered by retry logic, as key read is eventually successful at client side.

The problem is that:

bq. File locks are held on behalf of the entire Java virtual machine. They are not suitable for controlling access to a file by multiple threads within the same virtual machine. ([source|https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileLock.html])

code ref: [{{ChunkUtils.readData}}|https://github.com/apache/hadoop/blob/c92de8209d1c7da9e7ce607abeecb777c4a52c6a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L175]",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2019-08-23 16:26:01,0
13252640,Update the Dockerfile of the official apache/ozone image and use latest 0.4.1 release,"The hadoop-docker-ozone repository contains the definition of the apache/ozone image. 

https://github.com/apache/hadoop-docker-ozone/tree/ozone-latest

It creates a docker packaging for the voted and released artifact, therefore it can be released after the final vote.

Since the latest release we did some modification in our Dockerfiles. We need to apply the changes to the official image as well. Especially:

 1. use ozone-runner as a base image instead of hadoop-runner
 2. rename ozoneManager service to om as we did everywhere
 3. Adjust the starter location (the script is moved to the released tar file)
 ",pull-request-available,[],HDDS,Improvement,Major,2019-08-23 11:25:16,1
13252619,rat.sh: grep: warning: recursive search of stdin,"Running {{rat.sh}} locally fails with the following error message (after the two Maven runs):

{code:title=./hadoop-ozone/dev-support/checks/rat.sh}
...
grep: warning: recursive search of stdin
{code}

This happens if {{grep}} is not the GNU one.

Further, {{rat.sh}} runs into: {{cat: target/rat-aggregated.txt: No such file or directory}} in subshell due to a typo, and so always exits with success:

{code}
$ ./hadoop-ozone/dev-support/checks/rat.sh
...
cat: target/rat-aggregated.txt: No such file or directory

$ echo $?
0
{code}",pull-request-available,['build'],HDDS,Bug,Minor,2019-08-23 09:35:53,0
13252601,Fix rat check failures in trunk,Several files in hadop-ozone do not have apache license headers and cause a failure in trunk. ,pull-request-available,[],HDDS,Task,Major,2019-08-23 08:20:09,6
13252597,Add additional freon tests,"Freon is a generic load generator tool for ozone (ozone freon) which supports multiple generation pattern.

As of now only the random-key-generator is implemented which uses ozone rpc client.

It would be great to add additional tests:

 * Test key generation via s3 interface
 * Test key generation via the hadoop fs interface
 * Test key reads (validation)
 * Test OM with direct RPC calls
",pull-request-available,['Tools'],HDDS,Improvement,Major,2019-08-23 08:01:27,1
13252549,Remove mTLS from Ozone GRPC,"Generic GRPC support mTLS for mutual authentication. However, Ozone has built in block token mechanism for server to authenticate the client. We only need TLS for client to authenticate the server and wire encryption. 

Remove the mTLS support also simplify the GRPC server/client configuration.",pull-request-available,[],HDDS,Improvement,Major,2019-08-22 23:35:02,4
13252477,Handle Set DtService of token in S3Gateway for OM HA,"When OM HA is enabled, when tokens are generated, the service name should be set with address of all OM's.

 

Current without HA, it is set with Om RpcAddress string. This Jira is to handle:
 # Set dtService with all OM address. Right now in OMClientProducer, UGI is created with S3 token, and serviceName of token is set with OMAddress, for HA case, this should be set with all OM RPC addresses.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Critical,2019-08-22 20:11:05,2
13252476,Handle Set DtService of token for OM HA,"When OM HA is enabled, when tokens are generated, the service name should be set with address of all OM's.

 

Current without HA, it is set with Om RpcAddress string. This Jira is to handle:
 # Set dtService with all OM address.
 # Update token selector to return tokens if there is a match with Service. Because SaslRpcClient calls token selector with server address.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-22 20:10:21,2
13252282,scm web ui should publish the list of scm pipeline by type and factor,"scm web ui should publish the list of scm pipeline by type and factor, this helps in monitoring the cluster in real time.",Triaged,['SCM'],HDDS,Bug,Major,2019-08-22 05:54:40,6
13252274,Wrong package for RatisHelper class in hadoop-hdds/common module.,"It is currently org.apache.ratis.RatisHelper. 

It should be org.apache.hadoop.hdds.ratis.RatisHelper.",pull-request-available,[],HDDS,Bug,Minor,2019-08-22 05:15:22,5
13252150,Don't depend on bootstrap/jquery versions from hadoop-trunk snapshot,The OM/SCM web pages are broken due to the upgrade in HDFS-14729 (which is a great patch on the Hadoop side). To have more stability I propose to use our own instance from jquery/bootstrap instead of copying the actual version from hadoop trunk which is a SNAPSHOT build.,pull-request-available,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2019-08-21 14:41:19,6
13252115,Basic acceptance test and SCM/OM web UI broken by Bootstrap upgrade,"{code:title=https://elek.github.io/ozone-ci/trunk/trunk-nightly-9stkx/acceptance/smokeresult/log.html#s1-s8-t1}
$ curl --negotiate -u : -s -I http://scm:9876/static/bootstrap-3.3.7/js/bootstrap.min.js 2>&1
HTTP/1.1 404 Not Found
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-21 12:02:53,0
13251954,Generate renewTime on OMLeader for GetDelegationToken,"Use renewTime generated by OM leader, across quorum of OM's.

 

Right now each OM generates renew time when updating token in-memory and DB.

*OzoneDelegationTokenSecretManager.java*

public long updateToken(Token<OzoneTokenIdentifier> token,
 OzoneTokenIdentifier ozoneTokenIdentifier) {
 long renewTime =
 ozoneTokenIdentifier.getIssueDate() + getTokenRenewInterval();

 

If different OM's have different token renew interval set, for the same token we will have different renewal time across a quorum of OM's.

 

This Jira is to fix this issue.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Blocker,2019-08-21 00:00:16,7
13251703,Merge OzoneManagerRequestHandler and OzoneManagerHARequestHandlerImpl,"Once HA and Non-HA code are merged to use newly OM HA code. We can merge these classes, and remove the unused code.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:25:16,2
13251698,Remove RatisClient in OM HA,"In OM, we use ratis server api's to submit request. We can remove the RatisClient code from OM, which is no more used in submitting requests to ratis.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:16:00,2
13251695,Fix ApplyTransaction error handling in OzoneManagerStateMachine,"Right now, applyTransaction calls validateAndUpdateCache. This does not return any error. We should check the OmResponse Status code, and see if it is critical error we should completeExceptionally and stop ratis server.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:12:14,2
13251693,Fix listParts API,"This Jira is to fix listParts API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listParts of a MPU key, it should use both in-memory cache and rocksdb mpu table to list parts of a mpu key.

 

No fix is required for this, as the information is retrieved from the MPU Key table, this information is not retrieved through RocksDB Table iteration. (As when we use get() this checks from cache first, and then it checks table)

 

Used this Jira to add an integration test to verify the behavior.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:09:47,2
13251690,Fix listkeys API,"This Jira is to fix listKeys API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listkeys, it should use both in-memory cache and rocksdb key table to list keys in a bucket.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:06:41,2
13251689,Fix listVolumes API,"This Jira is to fix lisVolumes API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listVolumes, it should use both in-memory cache and rocksdb volume table to list volumes for a user.

 

No fix is required for this, as the information is retrieved from the MPU Key table, this information is not retrieved through RocksDB Table iteration. (As when we use get() this checks from cache first, and then it checks table)

 

Used this Jira to add an integration test to verify the behavior.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:05:47,2
13251688,Fix listBucket API,"This Jira is to fix listBucket API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listBuckets, it should use both in-memory cache and rocksdb bucket table to list buckets in a volume.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:05:14,2
13251417,Fix checkstyle errors,There are checkstyle errors in ListPipelinesSubcommand.java that needs to be fixed.,pull-request-available,['SCM'],HDDS,Task,Major,2019-08-17 22:41:50,6
13251345,Implement default acls for bucket/volume/key for OM HA code,This Jira is to implement default ACLs for Ozone volume/bucket/key.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-16 22:16:20,2
13251119,Implement OM CancelDelegationToken request to use Cache and DoubleBuffer,"Implement OM CancelDelegationToken request to use OM Cache, double buffer.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-16 00:02:23,2
13251118,Implement OM RenewDelegationToken request to use Cache and DoubleBuffer,"Implement OM RenewDelegationToken request to use OM Cache, double buffer.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-16 00:01:59,2
13251100,Provide example ha proxy with multiple s3 servers back end.,"In this Jira, we shall provide docker-compose files where we start 3 s3 gateway servers, and ha-proxy is used to load balance these S3 Gateway Servers.

 

In this Jira, all are proxy configurations are hardcoded, we can make improvements to scale and automatically configure with environment variables as a future improvement. This is just a starter example.

 ",pull-request-available,[],HDDS,New Feature,Major,2019-08-15 21:07:44,2
13250863,Implement OM GetDelegationToken request to use Cache and DoubleBuffer,"Implement OM GetDelegationToken request to use OM Cache, double buffer.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-14 19:59:55,2
13250780,Wrong expected key ACL in acceptance test,"Acceptance test fails at ACL checks:

{code:title=https://elek.github.io/ozone-ci/trunk/trunk-nightly-wxhxr/acceptance/smokeresult/log.html#s1-s16-s2-t4-k2}
[ {
  ""type"" : ""USER"",
  ""name"" : ""testuser/scm@EXAMPLE.COM"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""root"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""superuser1"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""superuser1"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""READ"", ""WRITE"", ""READ_ACL"", ""WRITE_ACL"" ]
} ]' does not match '""type"" : ""GROUP"",
.*""name"" : ""superuser1*"",
.*""aclScope"" : ""ACCESS"",
.*""aclList"" : . ""READ"", ""WRITE"", ""READ_ACL"", ""WRITE_ACL""'
{code}

The test [sets user ACL|https://github.com/apache/hadoop/blob/0e4b757955ae8da1651b870c12458e3344c0b613/hadoop-ozone/dist/src/main/smoketest/basic/ozone-shell.robot#L123], but [checks group ACL|https://github.com/apache/hadoop/blob/0e4b757955ae8da1651b870c12458e3344c0b613/hadoop-ozone/dist/src/main/smoketest/basic/ozone-shell.robot#L125].  I think this passed previously due to a bug that was [fixed|https://github.com/apache/hadoop/pull/1234/files#diff-2d061b57a9838854d07da9e0eca64f31] by HDDS-1917.",pull-request-available,['test'],HDDS,Bug,Major,2019-08-14 11:59:56,0
13250760,Compile error due to leftover ScmBlockLocationTestIngClient file,"{code:title=https://ci.anzix.net/job/ozone/17667/consoleText}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /var/jenkins_home/workspace/ozone@2/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestIngClient.java:[65,8] class ScmBlockLocationTestingClient is public, should be declared in a file named ScmBlockLocationTestingClient.java
[ERROR] /var/jenkins_home/workspace/ozone@2/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestingClient.java:[65,8] duplicate class: org.apache.hadoop.ozone.om.ScmBlockLocationTestingClient
[INFO] 2 errors 
{code}",pull-request-available,['build'],HDDS,Bug,Blocker,2019-08-14 11:00:12,0
13250740,TestOzoneClientProducer fails with ConnectException,"{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/trunk/trunk-nightly-wxhxr/unit/hadoop-ozone/s3gateway/org.apache.hadoop.ozone.s3.TestOzoneClientProducer.txt}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.s3.TestOzoneClientProducer
-------------------------------------------------------------------------------
Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 222.239 s <<< FAILURE! - in org.apache.hadoop.ozone.s3.TestOzoneClientProducer
testGetClientFailure[0](org.apache.hadoop.ozone.s3.TestOzoneClientProducer)  Time elapsed: 111.036 s  <<< FAILURE!
java.lang.AssertionError: 
 Expected to find 'Couldn't create protocol ' but got unexpected exception: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
{code}

Log output (with local log4j config) reveals that connection is attempted to 0.0.0.0:9862:

{code:title=log output}
2019-08-14 10:49:14,225 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
{code}

The address 0.0.0.0:9862 was added as default in [HDDS-1920|https://github.com/apache/hadoop/commit/bf457797f607f3aeeb2292e63f440cb13e15a2d9].",pull-request-available,['test'],HDDS,Bug,Major,2019-08-14 09:15:00,0
13250452,Aged IO Thread exits on first read,"Aged IO Thread in {{TestMiniChaosOzoneCluster}} exits on first read due to exception:

{code}
2019-08-12 22:55:37,799 [pool-245-thread-1] INFO  ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(194)) - AGED LOADGEN: Started Aged IO Thread:2139.
...
2019-08-12 22:55:47,147 [pool-245-thread-1] ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(213)) - AGED LOADGEN: 0 Exiting due to exception
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.readData(MiniOzoneLoadGenerator.java:151)
	at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.startAgedFilesLoad(MiniOzoneLoadGenerator.java:209)
	at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$1(MiniOzoneLoadGenerator.java:235)
2019-08-12 22:55:47,149 [pool-245-thread-1] INFO  ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(219)) - Terminating IO thread:2139.
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-13 07:22:39,0
13250198,StackOverflowError in OzoneClientInvocationHandler,"Happens if log level for {{org.apache.hadoop.ozone.client}} is set to TRACE.

{code}
SLF4J: Failed toString() invocation on an object of type [com.sun.proxy.$Proxy85]
Reported exception:
java.lang.StackOverflowError
...
	at org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)
	at com.sun.proxy.$Proxy85.toString(Unknown Source)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)
	at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)
	at org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)
	at com.sun.proxy.$Proxy85.toString(Unknown Source)
...
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Trivial,2019-08-12 07:07:32,0
13250047,TestMiniChaosOzoneCluster may run until OOME,"{{TestMiniChaosOzoneCluster}} runs load generator on a cluster for supposedly 1 minute, but it may run indefinitely until JVM crashes due to OutOfMemoryError.

In 0.4.1 nightly build it crashed 29/30 times (and no tests were executed in the remaining one run due to some other error).

Latest:
https://github.com/elek/ozone-ci/blob/3f553ed6ad358ba61a302967617de737d7fea01a/byscane/byscane-nightly-wggqd/integration/output.log#L5661-L5662

When it crashes, it leaves GBs of data lying around.",pull-request-available,['test'],HDDS,Bug,Critical,2019-08-10 14:48:42,0
13250039,Wrong symbolic release name on 0.4.1 branch,"Should be Biscayne instead of Crater lake according to the Roadmap:

https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Road+Map
",pull-request-available,[],HDDS,Bug,Blocker,2019-08-10 13:38:51,1
13250037,S3 MPU part-list call fails if there are no parts,"If an S3 multipart upload is created but no part is upload the part list can't be called because it throws HTTP 500:

Create an MPU:

{code}
aws s3api --endpoint http://localhost:9999 create-multipart-upload --bucket=docker --key=testkeu                                         
{
    ""Bucket"": ""docker"",
    ""Key"": ""testkeu"",
    ""UploadId"": ""85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234""
}
{code}

List the parts:

{code}
aws s3api --endpoint http://localhost:9999 list-parts  --bucket=docker --key=testkeu --upload-id=85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234
{code}

It throws an exception on the server side, because in the KeyManagerImpl.listParts the  ReplicationType is retrieved from the first part:

{code}
        HddsProtos.ReplicationType replicationType =
            partKeyInfoMap.firstEntry().getValue().getPartKeyInfo().getType();
{code}

Which is not yet available in this use case.",pull-request-available,['S3'],HDDS,Bug,Major,2019-08-10 12:32:27,1
13250036,Missing or error-prone test cleanup,Some integration tests do not clean up after themselves.  Some only clean up if the test is successful.,pull-request-available,['test'],HDDS,Bug,Major,2019-08-10 11:38:51,0
13250014,S3 MPU can't be created with octet-stream content-type ,"This problem is reported offline by [~shanekumpf@gmail.com].

When aws-sdk-go is used to access to s3 gateway of Ozone it sends the Multi Part Upload initialize message with ""application/octet-stream"" Content-Type. 

This Content-Type is missing from the aws-cli which is used to reimplement s3 endpoint.

The problem is that we use the same rest endpoint for initialize and complete Multipart Upload request. For the completion we need the CompleteMultipartUploadRequest parameter which is parsed from the body.

For initialize we have an empty body which can't be serialized to CompleteMultipartUploadRequest.

The workaround is to set a specific content type from a filter which help up to create two different REST method for initialize and completion message.

Here is an example to test (using bogus AWS credentials).

{code}
curl -H 'Host:yourhost' -H 'User-Agent:aws-sdk-go/1.15.11 (go1.11.2; linux; amd64)' -H 'Content-Length:0' -H 'Authorization:AWS4-HMAC-SHA256 Credential=qwe/20190809/ozone/s3/aws4_request, SignedHeaders=content-type;host;x-amz-acl;x-amz-content-sha256;x-amz-date;x-amz-storage-class, Signature=7726ed63990ba3f4f1f796d4ab263f5d9c3374528840f5e49d106dbef491f22c' -H 'Content-Type:application/octet-stream' -H 'X-Amz-Acl:private' -H 'X-Amz-Content-Sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' -H 'X-Amz-Date:20190809T070142Z' -H 'X-Amz-Storage-Class:STANDARD' -H 'Accept-Encoding:gzip' -X POST 'http://localhost:9999/docker/docker/registry/v2/repositories/apache/ozone-runner/_uploads/2173f019-09c3-466b-bb7d-c31ce749d826/data?uploads
{code}

Without the patch it returns with HTTP 405 (Not supported Media Type).",pull-request-available,['S3'],HDDS,Bug,Major,2019-08-10 07:04:32,1
13249995,CertificateClient should not persist keys/certs to ozone.metadata.dir,"For example, when OM and SCM are deployed on the same host with ozone.metadata.dir defined. SCM can start successfully but OM can not because the key/cert from OM will collide with SCM.",pull-request-available,[],HDDS,Improvement,Major,2019-08-10 00:11:23,6
13249942,TestKeyManagerImpl.testLookupKeyWithLocation is failing,"{code}
[ERROR]   TestKeyManagerImpl.testLookupKeyWithLocation:757 expected:<102ad7e3-4226-4966-af79-2b12a56f83cb{ip: 32.53.16.224, host: localhost-32.53.16.224, networkLocation: /default-rack, certSerialId: null}> but was:<d3e07bc3-6d24-4d80-9cdb-057a475084e7{ip: 238.199.149.19, host: localhost-238.199.149.19, networkLocation: /default-rack, certSerialId: null}>
{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-08-09 17:06:52,4
13249939,Support copy during S3 multipart upload part creation,"Uploads a part by copying data from an existing object as data source

Documented here:

https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html",pull-request-available,['S3'],HDDS,Sub-task,Blocker,2019-08-09 17:01:22,1
13249930,Unused executor in SimpleContainerDownloader,"{{SimpleContainerDownloader}} has an {{executor}} that's created and shut down, but never used.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2019-08-09 15:41:17,0
13249697,Acceptance tests fail if scm webui shows invalid json,"Acceptance test of a nightly build is failed with the following error:

{code}
Creating ozonesecure_datanode_3 ... 
[7A[2K
Creating ozonesecure_kdc_1      ... [32mdone[0m
[7B[6A[2K
Creating ozonesecure_om_1       ... [32mdone[0m
[6B[8A[2K
Creating ozonesecure_scm_1      ... [32mdone[0m
[8B[1A[2K
Creating ozonesecure_datanode_3 ... [32mdone[0m
[1B[5A[2K
Creating ozonesecure_kms_1      ... [32mdone[0m
[5B[4A[2K
Creating ozonesecure_s3g_1      ... [32mdone[0m
[4B[2A[2K
Creating ozonesecure_datanode_2 ... [32mdone[0m
[2B[3A[2K
Creating ozonesecure_datanode_1 ... [32mdone[0m
[3Bparse error: Invalid numeric literal at line 2, column 0
{code}

https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-5b87q/acceptance/output.log

The problem is in the script which checks the number of available datanodes.

If the HTTP endpoint of the SCM is already started BUT not ready yet it may return with a simple HTML error message instead of json. Which can not be parsed by jq:

In testlib.sh:

{code}
  37   │   if [[ ""${SECURITY_ENABLED}"" == 'true' ]]; then
  38   │     docker-compose -f ""${compose_file}"" exec -T scm bash -c ""kinit -k HTTP/scm@EXAMPL
       │ E.COM -t /etc/security/keytabs/HTTP.keytab && curl --negotiate -u : -s '${jmx_url}'""
  39   │   else
  40   │     docker-compose -f ""${compose_file}"" exec -T scm curl -s ""${jmx_url}""
  41   │   fi \
  42   │     | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value'
{code}

One possible fix is to adjust the error handling (set +x / set -x) per method instead of using a generic set -x at the beginning. It would provide a more predictable behavior. In our case count_datanode should not fail evert (as the caller method: wait_for_datanodes can retry anyway).",pull-request-available,[],HDDS,Bug,Major,2019-08-08 15:54:34,1
13249660,Improve the visibility with Ozone Insight tool,"Visibility is a key aspect for the operation of any Ozone cluster. We need better visibility to improve correctnes and performance. While the distributed tracing is a good tool for improving the visibility of performance we have no powerful tool which can be used to check the internal state of the Ozone cluster and debug certain correctness issues.

To improve the visibility of the internal components I propose to introduce a new command line application `ozone insight`.

The new tool will show the selected metrics / logs / configuration for any of the internal components (like replication-manager, pipeline, etc.).

For each insight points we can define the required logs and log levels, metrics and configuration and the tool can display only the component specific information during the debug.

h2. Usage

First we can check the available insight point:

{code}
bash-4.2$ ozone insight list
Available insight points:


  scm.node-manager                     SCM Datanode management related information.
  scm.replica-manager                  SCM closed container replication manager
  scm.event-queue                      Information about the internal async event delivery
  scm.protocol.block-location          SCM Block location protocol endpoint
  scm.protocol.container-location      Planned insight point which is not yet implemented.
  scm.protocol.datanode                Planned insight point which is not yet implemented.
  scm.protocol.security                Planned insight point which is not yet implemented.
  scm.http                             Planned insight point which is not yet implemented.
  om.key-manager                       OM Key Manager
  om.protocol.client                   Ozone Manager RPC endpoint
  om.http                              Planned insight point which is not yet implemented.
  datanode.pipeline[id]                More information about one ratis datanode ring.
  datanode.rocksdb                     More information about one ratis datanode ring.
  s3g.http                             Planned insight point which is not yet implemented.
{code}

Insight points can define configuration, metrics and/or logs. Configuration can be displayed based on the configuration objects:

{code}
ozone insight config scm.protocol.block-location
Configuration for `scm.protocol.block-location` (SCM Block location protocol endpoint)

>>> ozone.scm.block.client.bind.host
       default: 0.0.0.0
       current: 0.0.0.0

The hostname or IP address used by the SCM block client  endpoint to bind


>>> ozone.scm.block.client.port
       default: 9863
       current: 9863

The port number of the Ozone SCM block client service.


>>> ozone.scm.block.client.address
       default: ${ozone.scm.client.address}
       current: scm

The address of the Ozone SCM block client service. If not defined value of ozone.scm.client.address is used

{code}

Metrics can be retrieved from the prometheus entrypoint:

{code}
ozone insight metrics scm.protocol.block-location
Metrics for `scm.protocol.block-location` (SCM Block location protocol endpoint)

RPC connections

  Open connections: 0
  Dropped connections: 0
  Received bytes: 0
  Sent bytes: 0


RPC queue

  RPC average queue time: 0.0
  RPC call queue length: 0


RPC performance

  RPC processing time average: 0.0
  Number of slow calls: 0


Message type counters

  Number of AllocateScmBlock: 0
  Number of DeleteScmKeyBlocks: 0
  Number of GetScmInfo: 2
  Number of SortDatanodes: 0
{code}

Log levels can be adjusted with the existing logLevel servlet and can be collected / streamd via a simple logstream servlet:

{code}
ozone insight log scm.node-manager
[SCM] 2019-08-08 12:42:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:43:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:44:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:45:37,393 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:46:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
{code}

The verbose mode can display the raw messages as well:

{code}
[SCM] 2019-08-08 13:16:37,398 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 13:16:37,400 [TRACE|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] HB is received from [datanode=ozone_datanode_1.ozone_default]: 
storageReport {
  storageUuid: ""DS-bffe6bee-1166-4502-acf5-57fc16c5aa98""
  storageLocation: ""/data/hdds""
  capacity: 470282264576
  scmUsed: 16384
  remaining: 205695963136
  storageType: DISK
  failed: false
}

{code}

h2. Use cases

Ozone insight can be used for any kind of debuging. Some problem examples from my yesterday

 1. Due to a cache problem the volumes were created twice without any error at the second time. With this tool I can check the state of the internal cache, or check if the volume is added to the rocksdb itself.

 2. After fixing this problem we found an DNS caching issue. The OM responded with an error but it was not clear where the error was propagated from (it was created in OzoneManagerProtocolClientSideTranslatorPB.handleError). With checking the traffic between SCM and OM it can be easy to track the origin of a specific error.
 
 4. After fixing this problem we found some pipline problem (reported later at HDDS-1933). With this tool I could check the content of the reports and messages to the pipeline manager.

 


h2. Implementation

We can implement the tool without any significant code change as it uses existing features:

 * Metrics can be downloaded from the `/prom` endpoint
 * Log Level can be set with the existing `/logLevel` servlet endpoint (from hadoop-common)
 * Log lines can be streamed with a very simple new servlet
 * Configuration can be displayed based on configuration points

A new interface can be introduced for `InsightPoint`s where all the affected logs/levels, metrics and config classes can be defined for each components.

Prometheus servlet endpoint can be changed to be turned on by default.",pull-request-available,[],HDDS,Sub-task,Major,2019-08-08 13:24:26,1
13249629,TestSecureOzoneCluster may fail due to port conflict,"{{TestSecureOzoneCluster}} fails if SCM is already running on same host.

Steps to reproduce:

# Start {{ozone}} docker compose cluster
# Run {{TestSecureOzoneCluster}} test

{noformat:title=https://ci.anzix.net/job/ozone/17602/consoleText}
[ERROR] Tests run: 10, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 49.821 s <<< FAILURE! - in org.apache.hadoop.ozone.TestSecureOzoneCluster
[ERROR] testSCMSecurityProtocol(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 6.59 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
	at org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)
	at org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSCMSecurityProtocol(TestSecureOzoneCluster.java:277)
...

[ERROR] testSecureOmReInit(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 5.312 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
	at org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)
	at org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSecureOmReInit(TestSecureOzoneCluster.java:743)
...

[ERROR] testSecureOmInitSuccess(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 5.312 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
	at org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)
	at org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSecureOmInitSuccess(TestSecureOzoneCluster.java:789)
...
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-08 10:06:47,0
13249515,OM started on recon host in ozonesecure compose ,"OM is started temporarily on {{recon}} host in {{ozonesecure}} compose:

{noformat}
recon_1     | 2019-08-07 19:41:46 INFO  OzoneManagerStarter:51 - STARTUP_MSG:
recon_1     | /************************************************************
recon_1     | STARTUP_MSG: Starting OzoneManager
recon_1     | STARTUP_MSG:   host = recon/192.168.16.4
recon_1     | STARTUP_MSG:   args = [--init]
...
recon_1     | SHUTDOWN_MSG: Shutting down OzoneManager at recon/192.168.16.4
...
recon_1     | 2019-08-07 19:41:52 INFO  ReconServer:81 - Initializing Recon server...
{noformat}",pull-request-available,['docker'],HDDS,Bug,Minor,2019-08-07 19:48:59,0
13249510,Cannot run ozone-recon compose due to syntax error,"{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-recon
$ docker-compose up -d --scale datanode=3
ERROR: yaml.scanner.ScannerError: mapping values are not allowed here
  in ""./docker-compose.yaml"", line 20, column 33
{noformat}",pull-request-available,['docker'],HDDS,Bug,Major,2019-08-07 18:45:36,0
13249501,Consolidate add/remove Acl into OzoneAclUtil class,"This Jira is created based on @xiaoyu comment on HDDS-1884

Can we abstract these add/remove logic into common AclUtil class as we can see similar logic in both bucket manager and key manager? For example,

public static boolean addAcl(List existingAcls, OzoneAcl newAcl)
public static boolean removeAcl(List existingAcls, OzoneAcl newAcl)

 

But to do this, we need both OmKeyInfo and OMBucketInfo to use list of OzoneAcl/OzoneAclInfo.

This Jira is to do that refactor, and also address above comment to move common logic to AclUtils.",pull-request-available,[],HDDS,Bug,Major,2019-08-07 18:12:49,4
13249345,The new caching layer is used for old OM requests but not updated,"HDDS-1499 introduced a new caching layer together with a double-buffer based db writer to support OM HA.

TLDR: I think the caching layer is not updated for new volume creation. And (slightly related to this problem) I suggest to separated the TypedTable and the caching layer.

## How to reproduce the problem?

1. Start a docker compose cluster
2. Create one volume (let's say `/vol1`)
3. Restart the om (!)
4. Try to create an _other_ volume twice!

```
bash-4.2$ ozone sh volume create /vol2
2019-08-07 12:29:47 INFO  RpcClient:288 - Creating Volume: vol2, with hadoop as owner.
bash-4.2$ ozone sh volume create /vol2
2019-08-07 12:29:50 INFO  RpcClient:288 - Creating Volume: vol2, with hadoop as owner.
```

Expected behavior is an error:

{code}
bash-4.2$ ozone sh volume create /vol1
2019-08-07 09:48:39 INFO  RpcClient:288 - Creating Volume: vol1, with hadoop as owner.
bash-4.2$ ozone sh volume create /vol1
2019-08-07 09:48:42 INFO  RpcClient:288 - Creating Volume: vol1, with hadoop as owner.
VOLUME_ALREADY_EXISTS 
{code}

The problem is that the new cache is used even for the old code path (TypedTable):

{code}
 @Override
  public VALUE get(KEY key) throws IOException {
    // Here the metadata lock will guarantee that cache is not updated for same
    // key during get key.

    CacheResult<CacheValue<VALUE>> cacheResult =
        cache.lookup(new CacheKey<>(key));

    if (cacheResult.getCacheStatus() == EXISTS) {
      return cacheResult.getValue().getCacheValue();
    } else if (cacheResult.getCacheStatus() == NOT_EXIST) {
      return null;
    } else {
      return getFromTable(key);
    }
  }
{code}

For volume table after the FIRST start it always returns with `getFromTable(key)` due to the condition in the `TableCacheImpl.lookup`:

{code}

  public CacheResult<CACHEVALUE> lookup(CACHEKEY cachekey) {

    if (cache.size() == 0) {
      return new CacheResult<>(CacheResult.CacheStatus.MAY_EXIST,
          null);
    }
{code}

But after a restart the cache is pre-loaded by the TypedTable.constructor. After the restart, the real caching logic will be used (as cache.size()>0), which cause a problem as the cache is NOT updated from the old code path.

An additional problem is that the cache is turned on for all the metadata table even if the cache is not required... 

## Proposed solution

As I commented at HDDS-1499 this caching layer is not a ""traditional cache"". It's not updated during the typedTable.put() call but updated by a separated component during double-buffer flash.

I would suggest to remove the cache related methods from TypedTable (move to a separated implementation). I think this kind of caching can be independent from the TypedTable implementation. We can continue to use the simple TypedTable everywhere where we don't need to use any kind of caching.

For caching we can use a separated object. It would make it more visible that the cache should always be updated manually all the time. This separated caching utility may include a reference to the original TypedTable/Table. With this approach we can separate the different responsibilities but provide the same functionality.",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2019-08-07 12:43:28,2
13249316,ozonesecure acceptance test broken by HTTP auth requirement,"Acceptance test is failing at {{ozonesecure}} with the following error from {{jq}}:

{noformat:title=https://github.com/elek/ozone-ci/blob/325779d34623061e27b80ade3b749210648086d1/byscane/byscane-nightly-ds7lx/acceptance/output.log#L2779}
parse error: Invalid numeric literal at line 2, column 0
{noformat}

Example compose environments wait for datanodes to be up:

{code:title=https://github.com/apache/hadoop/blob/9cd211ac86bb1124bdee572fddb6f86655b19b73/hadoop-ozone/dist/src/main/compose/testlib.sh#L71-L72}
  docker-compose -f ""$COMPOSE_FILE"" up -d --scale datanode=""${datanode_count}""
  wait_for_datanodes ""$COMPOSE_FILE"" ""${datanode_count}""
{code}

The number of datanodes up is determined via HTTP query of JMX endpoint:

{code:title=https://github.com/apache/hadoop/blob/9cd211ac86bb1124bdee572fddb6f86655b19b73/hadoop-ozone/dist/src/main/compose/testlib.sh#L44-L46}
     #This line checks the number of HEALTHY datanodes registered in scm over the
     # jmx HTTP servlet
     datanodes=$(docker-compose -f ""${compose_file}"" exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo' | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value')
{code}

The problem is that no authentication is performed before or during the request, which is no longer allowed since HDDS-1901:

{code}
$ docker-compose exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo'
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 401 Authentication required</title>
</head>
<body><h2>HTTP ERROR 401</h2>
<p>Problem accessing /jmx. Reason:
<pre>    Authentication required</pre></p>
</body>
</html>
{code}

{code}
$ docker-compose exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo' | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value'
parse error: Invalid numeric literal at line 2, column 0
{code}",pull-request-available,"['docker', 'test']",HDDS,Bug,Critical,2019-08-07 10:16:57,0
13249260,ozone sh bucket path command does not exist,ozone sh bucket path command does not exist but it is mentioned in the static/docs/interface/s3.html. The command should either be added back or a the documentation should be improved.,pull-request-available,"['documentation', 'Ozone Manager']",HDDS,Bug,Blocker,2019-08-07 06:32:03,0
13249216,TestOzoneManagerDoubleBufferWithOMResponse is flaky,"{noformat:title=https://ci.anzix.net/job/ozone/17588/testReport/org.apache.hadoop.ozone.om.ratis/TestOzoneManagerDoubleBufferWithOMResponse/testDoubleBuffer/}
java.lang.AssertionError: expected:<11> but was:<9>
...
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:362)
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:104)
{noformat}

{noformat:title=https://ci.anzix.net/job/ozone/17587/testReport/org.apache.hadoop.ozone.om.ratis/TestOzoneManagerDoubleBufferWithOMResponse/unit___testDoubleBuffer/}
java.lang.AssertionError: expected:<11> but was:<3>
...
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:362)
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:104)
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-06 21:36:49,0
13249067,hadoop-ozone-tools has integration tests run as unit,"HDDS-1735 created separate test runner scripts for unit and integration tests.

Problem: {{hadoop-ozone-tools}} tests are currently run as part of the unit tests, but most of them start a {{MiniOzoneCluster}}, which is defined in {{hadoop-ozone-integration-test}}.  Thus I think these tests are really integration tests, and should be run by {{integration.sh}} instead.  There are currently only 3 real unit tests:

{noformat}
hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/audit/parser/TestAuditParser.java
hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestProgressBar.java
hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/genconf/TestGenerateOzoneRequiredConfigurations.java
{noformat}

{{hadoop-ozone-tools}} tests take ~6 minutes.

Possible solutions in order of increasing complexity:

# Run {{hadoop-ozone-tools}} tests in {{integration.sh}} instead of {{unit.sh}} (This is similar to {{hadoop-ozone-filesystem}}, which is already run by {{integration.sh}} and has 2 real unit tests.)
# Move all integration test classes to the {{hadoop-ozone-integration-test}} module, and make it depend on {{hadoop-ozone-tools}} and {{hadoop-ozone-filesystem}} instead of the other way around.
# Rename integration test classes to {{\*IT.java}} or {{IT\*.java}}, add filters for Surefire runs.",pull-request-available,"['build', 'test']",HDDS,Improvement,Minor,2019-08-06 09:54:03,0
13249055,Only contract tests are run in ozonefs module,"{{hadoop-ozone-filesystem}} has 6 test classes that are not being run:

{code}
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestFilteredClassLoader.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFSInputStream.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileInterfaces.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemWithMocks.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFsRenameDir.java
{code}

{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-vxsck/integration/output.log}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDelete
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.956 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDelete
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.528 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractSeek
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.245 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractSeek
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractOpen
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.996 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractOpen
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.816 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.418 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.042 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractCreate
[WARNING] Tests run: 11, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 35.144 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractCreate
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRootDir
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.986 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRootDir
[INFO] 
[INFO] Results:
[INFO] 
[WARNING] Tests run: 92, Failures: 0, Errors: 0, Skipped: 2
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-06 08:46:28,0
13249046,Remove hadoop script from ozone distribution,"/bin/hadoop script is included in the ozone distribution even if we a dedicated /bin/ozone

[~arp] reported that it can be confusing, for example ""hadoop classpath"" returns with a bad classpath (ozone classpath <projectname>) should be used instead.

To avoid such confusions I suggest to remove the hadoop script from distribution as ozone script already provides all the functionalities.

It also helps as to reduce the dependencies between hadoop 3.2-SNAPSHOT and ozone as we use the snapshot hadoop script as of now.",pull-request-available,[],HDDS,Bug,Major,2019-08-06 08:10:21,1
13249041,Ozonescript example docker-compose cluster can't be started,"the compose/ozonescripts cluster provides an example environment to test the start-ozone.sh and stop-ozone.sh scripts.

It starts containers with sshd daemon but witout starting the ozone which makes it possible to start those scripts.

Unfortunately the docker files are broken since:
 * we switched from debian to centos with the base image
 * we started to use /etc/hadoop instead of /opt/hadoop/etc/hadoop for configuring the hadoop (workers file should be copied there)
 * we started to use jdk11 to execute ozone (instead of java8)

The configuration files should be updated according to these changes. 

# How to test this patch?

(1) Do a full build and try to start the ./compose/ozonescripts cluster (check the related README file in the directory):

```
docker-compose up -d
```

(2) start the ozone processes with ./start.sh (from compose/ozonescripts)

(3) wait and check if om/scm webui are working and you have 1 healthy datanode

",pull-request-available,[],HDDS,Bug,Major,2019-08-06 08:03:22,1
13248992,Fix OzoneBucket and RpcClient APIS for acl,"Fix addAcl,removeAcl in OzoneBucket to use newly added acl API's addAcl/removeAcl as part of HDDS-1739.

Remove addBucketAcls, removeBucketAcls from RpcClient. We should use addAcl/removeAcl.

 

And also fix @xiaoyu comment on HDDS-1900 jira. BucketManagerImpl#setBucketProperty() as they now require a different permission (WRITE_ACL instead of WRITE)?",pull-request-available,[],HDDS,Bug,Blocker,2019-08-06 02:30:01,2
13248984,Support Prefix ACL operations for OM HA.,+-HDDS-1608-+ adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-05 23:18:45,2
13248887,Cannot build hadoop-hdds-config from scratch in IDEA,"Building {{hadoop-hdds-config}} from scratch (eg. right after checkout or after {{mvn clean}}) in IDEA fails with the following error:

{code}
Error:java: Bad service configuration file, or exception thrown while constructing Processor object: javax.annotation.processing.Processor: Provider org.apache.hadoop.hdds.conf.ConfigFileGenerator not found
{code}",pull-request-available,['build'],HDDS,Bug,Minor,2019-08-05 14:10:30,0
13248792,Use new HA code for Non-HA in OM,This Jira is to use new HA code of OM in Non-HA code path.,pull-request-available,[],HDDS,New Feature,Major,2019-08-04 22:29:20,2
13248766,TestMultiBlockWritesWithDnFailures is failing,"TestMultiBlockWritesWithDnFailures is failing with the following exception
{noformat}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.992 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures
[ERROR] testMultiBlockWritesWithDnFailures(org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures)  Time elapsed: 30.941 s  <<< ERROR!
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:720)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.allocateBlock(OzoneManagerProtocolClientSideTranslatorPB.java:752)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool.allocateNewBlock(BlockOutputStreamEntryPool.java:248)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool.allocateBlockIfNeeded(BlockOutputStreamEntryPool.java:296)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:201)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:376)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:325)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:231)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193)
	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures.testMultiBlockWritesWithDnFailures(TestMultiBlockWritesWithDnFailures.java:144)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-04 13:56:15,0
13248765,TestOzoneRpcClientWithRatis is failing with ACL errors,"{noformat}
[ERROR] testNativeAclsForKey(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.176 s  <<< FAILURE!
java.lang.AssertionError: Current acls:,[user:nvadivelu:a[ACCESS], group:staff:a[ACCESS], group:everyone:a[ACCESS], group:localaccounts:a[ACCESS], group:_appserverusr:a[ACCESS], group:admin:a[ACCESS], group:_appserveradm:a[ACCESS], group:_lpadmin:a[ACCESS], group:com.apple.sharepoint.group.1:a[ACCESS], group:com.apple.sharepoint.group.2:a[ACCESS], group:_appstore:a[ACCESS], group:_lpoperator:a[ACCESS], group:_developer:a[ACCESS], group:_analyticsusers:a[ACCESS], group:com.apple.access_ftp:a[ACCESS], group:com.apple.access_screensharing:a[ACCESS], group:com.apple.access_ssh:a[ACCESS], group:com.apple.sharepoint.group.3:a[ACCESS]] inheritedUserAcl:user:remoteUser:r[ACCESS]

[ERROR] testNativeAclsForBucket(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.074 s  <<< FAILURE!
java.lang.AssertionError

[ERROR] testNativeAclsForPrefix(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.061 s  <<< FAILURE!
java.lang.AssertionError: Current acls:,[user:nvadivelu:a[ACCESS], group:staff:a[ACCESS], group:everyone:a[ACCESS], group:localaccounts:a[ACCESS], group:_appserverusr:a[ACCESS], group:admin:a[ACCESS], group:_appserveradm:a[ACCESS], group:_lpadmin:a[ACCESS], group:com.apple.sharepoint.group.1:a[ACCESS], group:com.apple.sharepoint.group.2:a[ACCESS], group:_appstore:a[ACCESS], group:_lpoperator:a[ACCESS], group:_developer:a[ACCESS], group:_analyticsusers:a[ACCESS], group:com.apple.access_ftp:a[ACCESS], group:com.apple.access_screensharing:a[ACCESS], group:com.apple.access_ssh:a[ACCESS], group:com.apple.sharepoint.group.3:a[ACCESS]] inheritedUserAcl:user:remoteUser:r[ACCESS]
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-04 13:20:31,4
13248702,Use dynamic ports for SCM in TestSCMClientProtocolServer and TestSCMSecurityProtocolServer,"We should use dynamic port for SCM in the following test-cases
* TestSCMClientProtocolServer
* TestSCMSecurityProtocolServer",newbie pull-request-available,['test'],HDDS,Improvement,Major,2019-08-03 07:28:24,5
13248699,Fix Ozone HTTP WebConsole Authentication,"This was found during integration testing where the http authentication is enabled but anonymous can still access the ozone http web console like scm:9876 or om:9874. This can be reproed with the following configurations added to the ozonesecure docker-compose.

{code}

CORE-SITE.XML_hadoop.http.authentication.simple.anonymous.allowed=false

CORE-SITE.XML_hadoop.http.authentication.signature.secret.file=/etc/security/http_secret

CORE-SITE.XML_hadoop.http.authentication.type=kerberos

CORE-SITE.XML_hadoop.http.authentication.kerberos.principal=HTTP/_HOST@EXAMPLE.COM

CORE-SITE.XML_hadoop.http.authentication.kerberos.keytab=/etc/security/keytabs/HTTP.keytab

CORE-SITE.XML_hadoop.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer

{code}

After debugging into the KerberosAuthenticationFilter, the root cause is the name of the keytab does not follow the AuthenticationFilter tradition. The fix is to change 

hdds.scm.http.kerberos.keytab.file to hdds.scm.http.kerberos.keytab and
hdds.om.http.kerberos.keytab.file to hdds.om.http.kerberos.keytab

I will also add an integration test for this under ozonesecure docker-compose. ",pull-request-available,[],HDDS,Bug,Major,2019-08-03 06:50:57,4
13248679,Remove UpdateBucket handler which supports add/remove Acl,"This Jira is to remove bucket update handler.

To add acl/remove acl we should use ozone sh bucket addacl/ozone sh bucket removeacl.

 

Otherwise, when security is enabled, old Bucket update handler, uses setBucketProperty and that checks acl acces for WRITE, whereas when add/remove Acl we should check access for WRITE_ACL.

 

If we have both ways, even if a USER does not have WRITE_ACL can still add/remove Acls on a bucket.

 

This Jira is to clean up the old code and fix this security issue.",pull-request-available,[],HDDS,Bug,Critical,2019-08-02 22:59:07,2
13248497,Suppress WARN log from NetworkTopology#getDistanceCost ,"When RackAwareness is enabled and client from outside, the distance calculation flood SCM log with the following messages. This ticket is opened to suppress the WARN log.

{code}

2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology
2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology
2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-08-02 06:12:58,4
13248490,Support Key ACL operations for OM HA.,+HDDS-1541+ adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-02 04:53:33,2
13248455,Fix bug in removeAcl in Bucket,"{code:java}
// When we are removing subset of rights from existing acl.
for(OzoneAcl a: bucketInfo.getAcls()) {
 if(a.getName().equals(acl.getName()) &&
 a.getType().equals(acl.getType())) {
 BitSet bits = (BitSet) acl.getAclBitSet().clone();
 bits.and(a.getAclBitSet());

 if (bits.equals(ZERO_BITSET)) {
 return false;
 }
 bits = (BitSet) acl.getAclBitSet().clone();
 bits.and(a.getAclBitSet());
 a.getAclBitSet().xor(bits);

 if(a.getAclBitSet().equals(ZERO_BITSET)) {
 bucketInfo.getAcls().remove(a);
 }
 break;
 } else {
 return false;
 }{code}
In for loop, if first one is not matching with name and type, in else we return false. We should iterate entire acl list and then return response.
}",pull-request-available,"['Ozone Manager', 'Security']",HDDS,Bug,Blocker,2019-08-02 00:50:38,2
13248165,Fix bug in checkAcls in OzoneManager,"For HA scenario, checkAcls we pass the param UGI and remoteAddress. They need to be used in the checkAcls. HDDS-909 changed this behavior and not using the passed params.",pull-request-available,[],HDDS,Bug,Major,2019-07-31 17:32:00,2
13248162,Support Bucket ACL operations for OM HA.,-HDDS-15+40+- adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-31 17:10:18,2
13248091,Design doc: decommissioning in Ozone,Design doc can be attached to the documentation. In this jira the design doc will be attached and merged to the documentation page.,design pull-request-available,[],HDDS,Sub-task,Major,2019-07-31 11:56:51,1
13248084,checkstyle error in ContainerStateMachine,"{noformat:title=https://ci.anzix.net/job/ozone/17488/artifact/build/checkstyle.out}
[ERROR] src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java:[186] (sizes) LineLength: Line is longer than 80 characters (found 85).
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-07-31 11:32:05,0
13248047,hadoop31-mapreduce fails due to wrong HADOOP_VERSION,"hadoop31-mapreduce fails with:

{noformat:title=https://elek.github.io/ozone-ci/byscane/byscane-nightly-gl52x/acceptance/smokeresult/log.html#s1-s2-t2-k2-k2}
JAR does not exist or is not a normal file: /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar
{noformat}

because 3.1 test is being run with {{HADOOP_VERSION=3}}:

{noformat:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-gl52x/acceptance/output.log}
Creating network ""hadoop31_default"" with the default driver
Pulling nm (flokkr/hadoop:3)...
3: Pulling from flokkr/hadoop
Digest: sha256:62e3488e64ff8c0406752fc4f263ae2549e04fedf02534469913c496c6a89d78
Status: Downloaded newer image for flokkr/hadoop:3
{noformat}

which has Hadoop 3.2.0 instead of 3.1.2:

{noformat:title=docker run -it --entrypoint /bin/bash flokkr/hadoop:3 -c 'ls -la /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples*'}
-rw-r--r--    1 hadoop   flokkr      316570 Jan  8  2019 /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar
{noformat}

{noformat:title=docker run -it --entrypoint /bin/bash flokkr/hadoop:3.1.2 -c 'ls -la /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples*'}
-rw-r--r--    1 hadoop   flokkr      316380 Jan 29  2019 /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar
{noformat}

This only happens with {{acceptance.sh}}, not when directly using {{test-all.sh}}, because the former explicitly defines {{HADOOP_VERSION}}:

{noformat:title=https://github.com/apache/hadoop/blob/d4ab9aea6f9cbcdcaf48b821e5be04b4e952b133/hadoop-ozone/dev-support/checks/acceptance.sh#L19}
export HADOOP_VERSION=3
{noformat}

so the correct value from {{.env}} file is ignored:

{noformat:title=https://github.com/apache/hadoop/blob/d4ab9aea6f9cbcdcaf48b821e5be04b4e952b133/hadoop-ozone/dist/src/main/compose/ozone-mr/hadoop31/.env#L21}
HADOOP_VERSION=3.1.2
{noformat}",pull-request-available,['test'],HDDS,Bug,Blocker,2019-07-31 08:49:22,0
13248027,hadoop27 acceptance test cannot be run,"{noformat:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-gl52x/acceptance/output.log}
Executing test in /workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27
The HADOOP_RUNNER_VERSION variable is not set. Defaulting to a blank string.
The HADOOP_IMAGE variable is not set. Defaulting to a blank string.
Removing network hadoop27_default
Network hadoop27_default not found.
The HADOOP_RUNNER_VERSION variable is not set. Defaulting to a blank string.
The HADOOP_IMAGE variable is not set. Defaulting to a blank string.
Creating network ""hadoop27_default"" with the default driver
no such image: apache/ozone-runner:: invalid reference format
ERROR: Test execution of /workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27 is FAILED!!!!
cp: cannot stat '/workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27/result/robot-*.xml': No such file or directory
{noformat}",pull-request-available,[],HDDS,Bug,Blocker,2019-07-31 07:20:35,0
13247999,Fix failures in TestS3MultipartUploadAbortResponse,[https://ci.anzix.net//job/ozone/17503//testReport/junit/org.apache.hadoop.ozone.om.response.s3.multipart/TestS3MultipartUploadAbortResponse/testAddDBToBatchWithParts/],pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-31 03:55:35,2
13247744,Fix entry clean up from openKeyTable during complete MPU,"# Initiate MPU adds entry to openKeyTable and multipartInfo table.
 # When completeMPU, we add the entry to keyTable and delete from multipartInfo table.

Deleting from openKeyTable is missing in complete MPU.",pull-request-available,"['Ozone Manager', 'S3']",HDDS,Bug,Major,2019-07-29 22:47:21,2
13247649,Remove anti-affinity rules from k8s minkube example,"HDDS-1646 introduced real persistence for k8s example deployment files which means that we need anti-affinity scheduling rules: Even if we use statefulset instead of daemonset we would like to start one datanode per real nodes.

With minikube we have only one node therefore the scheduling rule should be removed to enable at least 3 datanodes on the same physical nodes.

How to test:

{code}
 mvn clean install -DskipTests -f pom.ozone.xml
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/kubernetes/examples/minikube
minikube start
kubectl apply -f .
kc get pod
{code}

You should see 3 datanode instances.
",pull-request-available,['kubernetes'],HDDS,Bug,Blocker,2019-07-29 12:44:31,1
13247620,ConcurrentModification at PrometheusMetricsSink,"Encountered on {{ozoneperf}} compose env when running low on CPU:

{code}
om_1          | java.util.ConcurrentModificationException
om_1          | 	at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1493)
om_1          | 	at java.base/java.util.HashMap$ValueIterator.next(HashMap.java:1521)
om_1          | 	at org.apache.hadoop.hdds.server.PrometheusMetricsSink.writeMetrics(PrometheusMetricsSink.java:123)
om_1          | 	at org.apache.hadoop.hdds.server.PrometheusServlet.doGet(PrometheusServlet.java:43)
{code}",pull-request-available,[],HDDS,Bug,Minor,2019-07-29 09:59:40,0
13247311,Invalid Prometheus metric name from JvmMetrics,"{noformat}
target=http://scm:9876/prom msg=""append failed"" err=""invalid metric type \""_old _generation counter\""""
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-07-26 12:26:17,0
13247180,"Freon RandomKeyGenerator even if keySize is set to 0, it returns some random data to key"," 
{code:java}
***************************************************
Status: Success
Git Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf
Number of Volumes created: 1
Number of Buckets created: 1
Number of Keys added: 1
Ratis replication factor: THREE
Ratis replication type: STAND_ALONE
Average Time spent in volume creation: 00:00:00,002
Average Time spent in bucket creation: 00:00:00,000
Average Time spent in key creation: 00:00:00,002
Average Time spent in key write: 00:00:00,101
Total bytes written: 0
Total Execution time: 00:00:05,699
 
{code}
***************************************************

[root@ozoneha-2 ozone-0.5.0-SNAPSHOT]# bin/ozone sh key list /vol-0-28271/bucket-0-95211

[

{   ""version"" : 0,   ""md5hash"" : null,   ""createdOn"" : ""Fri, 26 Jul 2019 01:02:08 GMT"",   ""modifiedOn"" : ""Fri, 26 Jul 2019 01:02:09 GMT"",   ""size"" : 36,   ""keyName"" : ""key-0-98235"",   ""type"" : null }

]

 

This is because of the below code in RandomKeyGenerator:
{code:java}
for (long nrRemaining = keySize - randomValue.length;
 nrRemaining > 0; nrRemaining -= bufferSize) {
 int curSize = (int) Math.min(bufferSize, nrRemaining);
 os.write(keyValueBuffer, 0, curSize);
}
os.write(randomValue);
os.close();{code}
 ",pull-request-available,[],HDDS,Bug,Major,2019-07-26 01:07:04,2
13247165,Fix TableCacheImpl cleanup logic,"Currently in cleanup, we iterate over epochEntries and cleaup the entries from cache and epochEntries set.

 

epochEntries is a TreeSet<> which is not a concurrent datastructure of java. We may see issue some times, when cleanup tries to remove entries and some other thread tries to add entries to cache. So, we need to use some concurrent set over there.

 

During cluster testing, seen this some times randomly:
 
{code:java}
019-07-25 15:28:41,087 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9862, call Call#8974 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.65.15.233:35222 java.lang.NullPointerException at java.util.TreeMap.fixAfterInsertion(TreeMap.java:2295) at java.util.TreeMap.put(TreeMap.java:582) at java.util.TreeSet.add(TreeSet.java:255) at org.apache.hadoop.utils.db.cache.TableCacheImpl.put(TableCacheImpl.java:75) at org.apache.hadoop.utils.db.TypedTable.addCacheEntry(TypedTable.java:218) at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareCreateKeyResponse(OMKeyRequest.java:292) at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:188) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:134) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method){code}
 
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-25 23:04:54,2
13246913,Make changes required for Non-HA to use new HA code in OM.,"In this Jira following things will be implemented:
 # Make the necessary changes for non-HA code path to use Cache and DoubleBuffer.

 ## When adding to double buffer, return future. This future will be used in the non-HA path to wait for this, and when it is completed return response to the client.
 ## Add to double-buffer will happen inside validateAndUpdateCache. In this way, in non-HA, when multiple RPC handler threads are calling preExecute and validateAndUpdateCache, the order inserted in to double buffer will happen in the order requests are received.

 

In this Jira, we shall not convert non-ha code path to use this, as security and acl work is not completed to use this new model.

 

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-24 21:28:58,2
13246723,Fix typo in TestOmAcls,"In test class TestOmAcls.java, correct the typo {code}OzoneAccessAuthrizerTest{code}

{code:java}
class OzoneAccessAuthrizerTest implements IAccessAuthorizer {

  @Override
  public boolean checkAccess(IOzoneObj ozoneObject, RequestContext context)
      throws OMException {
    return false;
  }
{code}

Change {code}OzoneAccessAuthrizerTest{code} to {code}OzoneAccessAuthorizerTest{code}",newbie pull-request-available,['test'],HDDS,Test,Trivial,2019-07-24 04:39:45,0
13246691,Implement S3 Complete MPU request to use Cache and DoubleBuffer,"Implement S3 Complete MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-23 22:51:29,2
13246688,Fix TestOzoneManagerHA and TestOzoneManagerSnapShotProvider,"All tests in TestOzoneManagerHA are failing with below exception.

 

Broken by HDDS-1649. Not sure why this test is not running in CI. 

From PR HDDS-1845 run, not seeing this test run. 

[https://ci.anzix.net/job/ozone/17452/testReport/org.apache.hadoop.ozone.om/]

 
{code:java}
java.lang.Exception: test timed out after 300000 milliseconds
at java.lang.Object.wait(Native Method)
 at java.lang.Object.wait(Object.java:502)
 at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)
 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
 at org.apache.hadoop.ipc.Client.call(Client.java:1457)
 at org.apache.hadoop.ipc.Client.call(Client.java:1367)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:326)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1155)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:234)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:156)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
 at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:126)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
 
{code}
 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-23 22:26:37,2
13246440,Default value for checksum bytes is different in ozone-site.xml and code,"<property>
 <name>ozone.client.checksum.type</name>
 <value>CRC32</value>
 <tag>OZONE, CLIENT, MANAGEMENT</tag>
 <description>The checksum type [NONE/ CRC32/ CRC32C/ SHA256/ MD5] determines
 which algorithm would be used to compute checksum for chunk data.
 Default checksum type is SHA256.
 </description>
</property>

OzoneConfigKeys.java

public static final String OZONE_CLIENT_CHECKSUM_TYPE_DEFAULT = ""SHA256"";

 

HDDS-1149 changes in ozone-default.xml, but not changed in java code. This Jira is to fix this issue.

 

 ",pull-request-available,[],HDDS,Bug,Major,2019-07-23 00:28:30,2
13246395,Fix OMVolumeSetQuota|OwnerRequest#validateAndUpdateCache return response.,"OMVolumeSetQuotaRequest#validateAndUpdateCache Line 115, we should return 

OMVolumeSetQuotaResponse in the failure case.

 

{code}

return new OMVolumeCreateResponse(null, null,
 createErrorOMResponse(omResponse, ex));

{code}",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Minor,2019-07-22 19:22:13,2
13246353,Tune the stateMachineDataCache to a reasonable fraction of Datanode Heap,"Currently, the stateMachineData which is the actual chunk data is maintained in the stateMachineCache inside ContainerStateMachine. Right now, the cache expiry is time based as well sized as per the no of parallel write chunks possible in the datanode. In case of optimal throughput, we may need to tune to it to a fraction of heap configured for the datanode process.",Triaged,['Ozone Datanode'],HDDS,Bug,Major,2019-07-22 15:27:30,3
13246352,Undetectable corruption after restart of a datanode,"Right now, all write chunks use BufferedIO ie, sync flag is disabled by default. Also, Rocks Db metadata updates are done in Rocks DB cache first at Datanode. In case, there comes a situation where the buffered chunk data as well as the corresponding metadata update is lost as a part of datanode restart, it may lead to a situation where, it will not be possible to detect the corruption (not even with container scanner) of this nature in a reasonable time frame, until and unless there is a client IO failure or Recon server detects it over time. In order to atleast to detect the problem, Ratis snapshot on datanode should sync the rocks db file . In such a way, ContainerScanner will be able to detect this.We can also add a metric around sync to measure how much of a throughput loss it can incurr.

Thanks [~msingh] for suggesting this.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2019-07-22 15:20:28,3
13246100,Implement S3 Abort MPU request to use Cache and DoubleBuffer,"Implement S3 Abort MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-20 00:54:12,2
13246095,Fix TestSecureContainerServer,"java.io.IOException: org.apache.hadoop.metrics2.MetricsException: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!

at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
 at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:106)
 at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:169)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:138)
 at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:94)
 at org.apache.ratis.client.impl.RaftClientImpl.sendRequest(RaftClientImpl.java:279)
 at org.apache.ratis.client.impl.RaftClientImpl.groupAdd(RaftClientImpl.java:206)
 at org.apache.hadoop.ozone.RatisTestHelper.initXceiverServerRatis(RatisTestHelper.java:135)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.lambda$runTestClientServerRatis$4(TestSecureContainerServer.java:159)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServer(TestSecureContainerServer.java:184)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServerRatis(TestSecureContainerServer.java:155)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.testClientServerRatisGrpc(TestSecureContainerServer.java:131)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
 at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
 at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
 at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at org.apache.ratis.util.ReflectionUtils.instantiateException(ReflectionUtils.java:222)
 at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:104)
 ... 34 more
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
 at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:274)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$3(GrpcClientProtocolClient.java:140)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:167)
 ... 32 more
 Suppressed: java.lang.IllegalStateException: Failed to cast the object to class java.io.IOException
 at org.apache.ratis.util.IOUtils.readObject(IOUtils.java:204)
 at org.apache.ratis.util.IOUtils.bytes2Object(IOUtils.java:195)
 at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:93)
 at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:169)
 ... 32 more
 Caused by: java.lang.ClassCastException: Cannot cast org.apache.hadoop.metrics2.MetricsException to java.io.IOException
 at java.lang.Class.cast(Class.java:3369)
 at org.apache.ratis.util.IOUtils.readObject(IOUtils.java:200)
 ... 36 more",pull-request-available,[],HDDS,Bug,Major,2019-07-19 23:03:14,2
13246093,Fix TestSecureOzoneContainer,"org.apache.hadoop.metrics2.MetricsException: Metrics source StorageContainerMetrics already exists!

at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
 at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
 at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
 at org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics.create(ContainerMetrics.java:92)
 at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.<init>(OzoneContainer.java:93)
 at org.apache.hadoop.ozone.container.ozoneimpl.TestSecureOzoneContainer.testCreateOzoneContainer(TestSecureOzoneContainer.java:146)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

 ",pull-request-available,[],HDDS,Bug,Major,2019-07-19 22:55:29,2
13245956,Change the default value of ratis leader election min timeout to a lower value,"The default value of min leader election timeout currently is 5s(done with HDDS-1718) by default which is leading to leader election taking much longer time to timeout in case of network failures and leading to delayed creation of pipelines in the system. The idea is to change the default value to a lower value of ""2s"" for now.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-19 09:39:42,3
13245950,parent directories not found in secure setup due to ACL check,"ozonesecure-ozonefs acceptance test is failing, because {{ozone fs -mkdir -p}} only creates key for the specific directory, not its parents.

{noformat}
ozone fs -mkdir -p o3fs://bucket1.fstest/testdir/deep
{noformat}

Previous result:

{noformat:title=https://ci.anzix.net/job/ozone-nightly/176/artifact/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/result/log.html#s1-s16-t3-k2}
$ ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.[].keyName'
testdir/
testdir/deep/
{noformat}

Current result:

{noformat:title=https://ci.anzix.net/job/ozone-nightly/177/artifact/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/result/log.html#s1-s16-t3-k2}
$ ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.[].keyName'
testdir/deep/
{noformat}

The failure happens on first operation that tries to use {{testdir/}} directly:

{noformat}
$ ozone fs -touch o3fs://bucket1.fstest/testdir/TOUCHFILE.txt
ls: `o3fs://bucket1.fstest/testdir': No such file or directory
{noformat}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Blocker,2019-07-19 08:58:05,0
13245918,Improve logging for PipelineActions handling in SCM and datanode,"XceiverServerRatis should log the reason while sending the PipelineAction to the datanode.
Also on the PipelineActionHandler should also log the detailed reason for the action.",newbie pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2019-07-19 04:35:47,5
13245886,Load Snapshot info when OM Ratis server starts,"When Ratis server is starting it looks for the latest snapshot to load it. Even though OM does not save snapshots via Ratis, we need to load the saved snaphsot index into Ratis so that the LogAppender knows to not look for logs before the snapshot index. Otherwise, Ratis will replay the logs from beginning every time it starts up.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-18 22:25:59,7
13245698,RatisPipelineProvider#initializePipeline logging needs to be verbose on failures/errors,RatisPipelineProvider#initializePipeline does not logs the information about pipeline details and the failed nodes when initializePipeline fails. The debugging needs to be verbose to help in debugging.,Triaged newbie,['SCM'],HDDS,Bug,Major,2019-07-18 07:29:39,6
13245697,NPE in SCMCommonPolicy.chooseDatanodes,"Exception is in SCMCommonPolicy.chooseDatanodes
{code}
java.lang.NullPointerException
	at java.util.Objects.requireNonNull(Objects.java:203)
	at java.util.ArrayList.removeAll(ArrayList.java:693)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMCommonPolicy.chooseDatanodes(SCMCommonPolicy.java:112)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodes(SCMContainerPlacementRandom.java:74)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.TestContainerPlacementFactory.testDefaultPolicy(TestContainerPlacementFactory.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

cc : [~xyao] [~Sammi]",pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-18 07:27:20,0
13245696,BlockOutputStream#watchForCommit fails with UnsupportedOperationException when one DN is down,"When one of the datanode from the ratis pipeline is excluded by introducing network failure, the client write is failing with the following exception
{noformat}
2019-07-18 07:13:33 WARN  XceiverClientRatis:262 - 3 way commit failed on pipeline Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
E         java.util.concurrent.ExecutionException: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 2 and log index 9 is not yet replicated to ALL_COMMITTED
E         	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
E         	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)
E         	at org.apache.hadoop.hdds.scm.XceiverClientRatis.watchForCommit(XceiverClientRatis.java:259)
E         	at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:194)
E         	at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchOnLastIndex(CommitWatcher.java:157)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:348)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)
E         	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)
E         	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)
E         	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
E         	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
E         	at java.base/java.lang.Thread.run(Thread.java:834)
E         Caused by: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 2 and log index 9 is not yet replicated to ALL_COMMITTED
E         	at org.apache.ratis.client.impl.ClientProtoUtils.toRaftClientReply(ClientProtoUtils.java:245)
E         	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:254)
E         	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:249)
E         	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)
E         	at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
E         	at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
E         	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)
E         	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
E         	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
E         	... 3 more
E         2019-07-18 07:13:33 INFO  XceiverClientRatis:280 - Could not commit index 9 on pipeline Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] to all the nodes. Server fa65a457-155d-4bf3-8d1b-b0e11ec157ae has failed. Committed by majority.
E         2019-07-18 07:13:33 WARN  BlockOutputStream:354 - Failed to commit BlockId conID: 1 locID: 102461208643108865 bcsId: 9 on Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]. Failed nodes: [fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: null, host: null, networkLocation: /default-rack, certSerialId: null}]
E         2019-07-18 07:13:33 ERROR RandomKeyGenerator:730 - Exception while adding key: key-0-06904 in bucket: bucket-0-14179 of volume: vol-0-21379.
E         java.lang.UnsupportedOperationException
E         	at java.base/java.util.AbstractList.add(AbstractList.java:153)
E         	at java.base/java.util.AbstractList.add(AbstractList.java:111)
E         	at java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:356)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)
E         	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)
E         	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)
E         	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
E         	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
E         	at java.base/java.lang.Thread.run(Thread.java:834)

E         java.lang.UnsupportedOperationException
E         	at java.base/java.util.AbstractList.add(AbstractList.java:153)
E         	at java.base/java.util.AbstractList.add(AbstractList.java:111)
E         	at java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:356)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)
E         	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)
E         	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)
E         	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
E         	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
E         	at java.base/java.lang.Thread.run(Thread.java:834)
E
{noformat}",blockade,['Ozone Client'],HDDS,Bug,Major,2019-07-18 07:24:39,3
13245634,Fix numKeys metrics in OM HA,"When we commit the key, we should increment numKeys in Ozone. This metrics shows the current count of keys in ozone. This is missed in OMKeyCommitRequest logic.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-17 22:56:31,2
13245610,Implement S3 Commit MPU request to use Cache and DoubleBuffer,"Implement S3 Commit MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-17 19:21:44,2
13245308,Fix false warning from ozones3 acceptance test,"All acceptance passed but the results are marked failed due to the following warnings.

[https://ci.anzix.net/job/ozone/17381/RobotTests/log.html]

{code}

[ WARN ] Collapsing consecutive whitespace during parsing is deprecated. Fix ' # Bucket already is created in Test Setup.' in file '/opt/hadoop/smoketest/s3/bucketcreate.robot' on line 31.

{code}",pull-request-available,[],HDDS,Test,Minor,2019-07-16 19:10:43,4
13245281,Du while calculating used disk space reports that chunk files are file not found,"

{code}
2019-07-16 08:16:49,787 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Could not get disk usage information for path /data/3/ozone-0715
ExitCodeException exitCode=1: du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/b113dd390e68e914d3ff405f3deec564_stream_60448f
77-6349-48fa-ae86-b2d311730569_chunk_1.tmp.1.14118085': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/37993af2849bdd0320d0f9d4a6ef4b92_stream_1f68be9f-e083-45e5-84a9-08809bc392ed
_chunk_1.tmp.1.14118091': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a38677def61389ec0be9105b1b4fddff_stream_9c3c3741-f710-4482-8423-7ac6695be96b
_chunk_1.tmp.1.14118102': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a689c89f71a75547471baf6182f3be01_stream_baf0f21d-2fb0-4cd8-84b0-eff1723019a0
_chunk_1.tmp.1.14118105': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/f58cf0fa5cb9360058ae25e8bc983e84_stream_d8d5ea61-995f-4ff5-88fb-4a9e97932f00
_chunk_1.tmp.1.14118109': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a1d13ee6bbefd1f8156b1bd8db0d1b67_stream_db214bdd-a0c0-4f4a-8bc7-a3817e047e45_chunk_1.tmp.1.14118115': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/8f8a4bd3f6c31161a70f82cb5ab8ee60_stream_d532d657-3d87-4332-baf8-effad9b3db23_chunk_1.tmp.1.14118127': No such file or directory

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)
        at org.apache.hadoop.util.Shell.run(Shell.java:901)
        at org.apache.hadoop.fs.DU$DUShell.startRefresh(DU.java:62)
        at org.apache.hadoop.fs.DU.refresh(DU.java:53)
        at org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:181)
        at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2019-07-16 16:59:02,0
13245264,Prometheus metrics are broken for datanodes due to an invalid metric,"Datanodes can't be monitored with prometheus any more:

{code}
level=warn ts=2019-07-16T16:29:55.876Z caller=scrape.go:937 component=""scrape manager"" scrape_pool=pods target=http://192.168.69.76:9882/prom msg=""append failed"" err=""invalid metric type \""apache.hadoop.ozone.container.common.transport.server.ratis._csm_metrics_delete_container_avg_time gauge\""""
{code}


",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-07-16 16:32:58,0
13245115,TestRatisPipelineCreateAndDestory#testPipelineCreationOnNodeRestart times out,"{code:java}
Error Message
test timed out after 30000 milliseconds
Stacktrace
java.lang.Exception: test timed out after 30000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:382)
	at org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineCreateAndDestory.waitForPipelines(TestRatisPipelineCreateAndDestory.java:126)
	at org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineCreateAndDestory.testPipelineCreationOnNodeRestart(TestRatisPipelineCreateAndDestory.java:121)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-16 05:53:16,0
13245114,TestWatchForCommit#testWatchForCommitForRetryfailure fails as a result of no leader election for extended period of time ,"{code:java}
org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$407/213850519@1a8843a2 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=1000ms)
Stacktrace
java.util.concurrent.ExecutionException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$407/213850519@1a8843a2 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=1000ms)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForRetryfailure(TestWatchForCommit.java:345)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
The client here retries times with a delay of 1 sec between each retry but leader eleactiocouldnot complete.
{code:java}
2019-07-12 19:30:46,451 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46: receive RaftClientReply:client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 5931fd83-b899-480e-b15a-ecb8e7f7dd46 is not the leader (null). Request must be sent to leader., logIndex=0, commits[5931fd83-b899-480e-b15a-ecb8e7f7dd46:c-1]
2019-07-12 19:30:47,469 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->d83929f1-c4db-499d-b67f-ad7f10dd7dde: receive RaftClientReply:client-6C83DC527A4C->d83929f1-c4db-499d-b67f-ad7f10dd7dde@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server d83929f1-c4db-499d-b67f-ad7f10dd7dde is not the leader (null). Request must be sent to leader., logIndex=0, commits[d83929f1-c4db-499d-b67f-ad7f10dd7dde:c-1]
2019-07-12 19:30:48,504 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46: receive RaftClientReply:client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 5931fd83-b899-480e-b15a-ecb8e7f7dd46 is not the leader (null). Request must be sent to leader., logIndex=0, commits[5931fd83-b899-480e-b15a-ecb8e7f7dd46:c-1]
2019-07-12 19:30:49,540 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509: receive RaftClientReply:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 73bdd98d-b003-44ff-a45b-bd12dfd50509 is not the leader (null). Request must be sent to leader., 
{code}",Triaged,['Ozone Client'],HDDS,Bug,Major,2019-07-16 05:47:37,3
13245099,Implement S3 Initiate MPU request to use Cache and DoubleBuffer,"Implement S3 Initiate MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-16 05:03:20,2
13245087,shellcheck.sh does not work on Mac,"# {{shellcheck.sh}} does not work on Mac
{code}
find: -executable: unknown primary or operator
{code}
# {{$OUTPUT_FILE}} only contains problems from {{hadoop-ozone}}, not from {{hadoop-hdds}}",pull-request-available,[],HDDS,Bug,Minor,2019-07-16 03:05:06,0
13245077,Add Eviction policy for table cache,"In this Jira we will add eviction policy for table cache.

In this Jira, we will add 2 eviction policies for the cache.

NEVER, // Cache will not be cleaned up. This mean's the table maintains full cache.
AFTERFLUSH // Cache will be cleaned up, once after flushing to DB.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-16 00:19:31,2
13245021,Make Topology Aware Replication/Read non-default for ozone 0.4.1   ,"This helps stablize the ozone-0.4.1 release and fix HDDS-1705, HDDS-1751, HDDS-1713 and HDDS-1770 for 0.5. ",pull-request-available,[],HDDS,Sub-task,Major,2019-07-15 17:18:16,4
13244990,Result of author check is inverted,"h2. What changes were proposed in this pull request?
h2. 
Fix:

 1. author check fails when no violations are found
 2. author check violations are duplicated in the output

Eg. https://ci.anzix.net/job/ozone-nightly/173/consoleText says that:


{code:java}
The following tests are FAILED:

[author]: author check is failed (https://ci.anzix.net/job/ozone-nightly/173//artifact/build/author.out/*view*/){code}


but no actual `@author` tags were found:

{code}
$ curl -s 'https://ci.anzix.net/job/ozone-nightly/173//artifact/build/author.out/*view*/' | wc
       0       0       0
{code}

h2. How was this patch tested?

{code}
$ bash -o pipefail -c 'hadoop-ozone/dev-support/checks/author.sh | tee build/author.out'; echo $?
0

$ wc build/author.out
       0       0       0 build/author.out

$ echo '// @author Tolkien' >> hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java

$ bash -o pipefail -c 'hadoop-ozone/dev-support/checks/author.sh | tee build/author.out'; echo $?
./hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java:// @author Tolkien
1

$ wc build/author.out
       1       3     108 build/author.out
{code}",pull-request-available,[],HDDS,Bug,Major,2019-07-15 14:14:28,0
13244955,Add goofyfs to the ozone-runner docker image,"Goofys is a s3 fuse driver which is required for the ozone csi setup.

As of now it's installed in hadoop-ozone/dist/src/main/docker/Dockerfile from a non-standard location (because it couldn't be part of hadoop-runner earlier as it's ozone specific).

It should be installed to the ozone-runner from a canonical goffys release URL.",pull-request-available,[],HDDS,Bug,Blocker,2019-07-15 12:37:48,1
13244719,Implement S3 Delete Bucket request to use Cache and DoubleBuffer,"Implement S3 Bucket write requests to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-12 22:48:03,2
13244595,Acceptance test of ozone-topology cluster is failing,"Since HDDS-1586 the smoketests of the ozone-topology compose file is broken:
{code:java}
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml
must specify at least one container source
Stopping datanode_2 ... 
Stopping datanode_3 ... 
Stopping datanode_4 ... 
Stopping scm        ... 
Stopping om         ... 
Stopping datanode_1 ... 
[6A[2K
Stopping datanode_2 ... [32mdone[0m
[6B[4A[2K
Stopping datanode_4 ... [32mdone[0m
[4B[1A[2K
Stopping datanode_1 ... [32mdone[0m
[1B[5A[2K
Stopping datanode_3 ... [32mdone[0m
[5B[3A[2K
Stopping scm        ... [32mdone[0m
[3B[2A[2K
Stopping om         ... [32mdone[0m
[2BRemoving datanode_2 ... 
Removing datanode_3 ... 
Removing datanode_4 ... 
Removing scm        ... 
Removing om         ... 
Removing datanode_1 ... 
[1A[2K
Removing datanode_1 ... [32mdone[0m
[1B[2A[2K
Removing om         ... [32mdone[0m
[2B[5A[2K
Removing datanode_3 ... [32mdone[0m
[5B[4A[2K
Removing datanode_4 ... [32mdone[0m
[4B[6A[2K
Removing datanode_2 ... [32mdone[0m
[6B[3A[2K
Removing scm        ... [32mdone[0m
[3BRemoving network ozone-topology_net
[ ERROR ] Reading XML source '/var/jenkins_home/workspace/ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-topology/result/robot-*.xml' failed: No such file or directory

Try --help for usage information.
ERROR: Test execution of /var/jenkins_home/workspace/ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-topology is FAILED!!!!{code}",pull-request-available,[],HDDS,Bug,Blocker,2019-07-12 12:00:46,0
13244480,Fix kerberos principal error in Ozone Recon,"Recon fails to startup in a kerberized cluster with the following error:


{code:java}
Failed startup of context o.e.j.w.WebAppContext@2009f9b0{/,file:///tmp/jetty-0.0.0.0-9888-recon-_-any-2565178148822292652.dir/webapp/,UNAVAILABLE}{/recon} javax.servlet.ServletException: javax.servlet.ServletException: Principal not defined in configuration at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180) at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139) at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873) at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349) at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406) at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368) at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778) at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.server.Server.start(Server.java:427) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.server.Server.doStart(Server.java:394) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1140) at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:175) at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:102) at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:61)
{code}",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-07-12 00:26:43,6
13244440,Datanodes takeSnapshot should delete previously created snapshots,"Right now, after after taking a new snapshot, the previous snapshot file is left in the raft log directory. When a new snapshot is taken, the previous snapshots should be deleted.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-11 19:17:29,5
13244369,OOM error in Freon due to the concurrency handling,"HDDS-1532 modified the concurrent framework usage of Freon (RandomKeyGenerator).

The new approach uses separated tasks (Runnable) to create the volumes/buckets/keys.

Unfortunately it doesn't work very well in some cases.
 # When Freon starts it creates an executor with fixed number of threads (10)
 # The first loop submits numOfVolumes (10) VolumeProcessor tasks to the executor
 # The 10 threads starts to execute the 10 VolumeProcessor tasks
 # Each VolumeProcessor tasks creates numOfBuckets (1000) BucketProcessor tasks. All together 10000 tasks are submitted to the executor.
 # The 10 threads starts to execute the first 10 BucketProcessor tasks, they starts to create the KeyProcessor tasks: 500 000 * 10 tasks are submitted.
 # At this point of the time no keys are generated, but the next 10 BucketProcessor tasks are started to execute..
 # To execute the first key creation we should process all the BucketProcessor tasks which means that all the Key creation tasks (10 * 1000 * 500 000) are created and added to the executor
 # Which requires a huge amount of time and memory",pull-request-available,[],HDDS,Bug,Blocker,2019-07-11 14:30:54,0
13244213,Latency metric for applyTransaction in ContainerStateMachine,"applyTransaction is invoked from the Ratis pipeline and the ContainerStateMachine

uses a async executor to complete the task.

 

We require a latency metric to track the performance of log apply operations in the state machine. This will measure the end-to-end latency of apply which includes the queueing delay in the executor queues. Combined with the latency measurement in HddsDispatcher, this will be an indicator if the executors are overloaded.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-07-11 03:56:47,5
13244108,TestFailureHandlingByClient tests are flaky,"The tests seem to fail bcoz , when the datanode goes down with stale node interval being set to a low value, containers may get closed early and client writes might fail with closed container exception rather than pipeline failure/Timeout exceptions as excepted in the tests. The fix made here is to tune the stale node interval.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-07-10 13:09:25,3
13244070,TestWatchForCommit tests are flaky,"The tests have become flaky bcoz once  nodes are shutdown inn Ratis pipeline, a watch request can either be received at server at a server and fail with NotReplicatedException or sometimes it fails with StatusRuntimeExceptions from grpc which both need to be accounted for in the tests. Other than that, HDDS-1384 also causes bind exception to e thrown intermittently which in turn shuts down the miniOzoneCluster. To overcome this, the test class has been refactored as well.",pull-request-available,[],HDDS,Bug,Major,2019-07-10 10:08:05,3
13243962,Fix image name in some ozone docker-compose files,"The docker compose file has invalid reference to scm images, which fails the docker-compose up with errors like below. This ticket is opened to fix them.

 
{code:java}
ERROR: no such image: apache/ozone-runner::20190617-2: invalid reference format}

or 

ERROR: no such image: apache/ozone-runner:latest:20190617-2: invalid reference format{code}",pull-request-available,[],HDDS,Bug,Major,2019-07-09 20:45:30,4
13243793,Make OM KeyDeletingService compatible with HA model,"Currently OM KeyDeletingService directly deletes all the keys in DeletedTable after deleting the corresponding blocks through SCM. For HA compatibility, the key purging should happen through the OM Ratis server. This Jira introduces PurgeKeys request in OM protocol. This request will be submitted to OMs Ratis server after SCM deletes blocks corresponding to deleted keys.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-09 01:46:19,7
13243234,Fix hidden errors in acceptance tests,"[~bharatviswa] pinged me offline with the problem that in some cases the smoketest is failing even if the reports are green:
{code:java}
All smoke tests are passed, but CI is showing as Failed.

https://ci.anzix.net/job/ozone/17284/RobotTests/log.html
https://github.com/apache/hadoop/pull/1048{code}
The root cause is a few typo after HDDS-1698, which can be fixed with the uploaded PR.

*What is the problem?*

In case of any error during the test execution the smoketest is failed. In this case because the typo in two docker-compose.yaml files two of the tests can't be started.

But there is no separated robot test report and the error is visible only in the console.

*How did it happen?*

The ACL work improved some intermittency in the acceptance tests. HDDS-1698 is committed because the acceptance tests were failed with ACL errors which hide the real error (the test was red anyway).

 ",pull-request-available,[],HDDS,Bug,Blocker,2019-07-04 15:23:25,1
13243231,Use vendor neutral s3 logo in ozone doc,"In HDDS-1639 we restructured the ozone documentation and a new overview page is added to the main page.

This page contains an official aws logo, As [~bharatviswa] reported we are not sure about the exact condition to use logos / trademarks from Amazon. It's better to remain on the safe side and use a neutral S3 label.

In this patch the aws logo is replaced with an orange cloud + s3 text.",pull-request-available,['documentation'],HDDS,Bug,Major,2019-07-04 15:12:48,1
13243065,Fix class hierarchy for KeyRequest and FileRequest classes.,"The patch looks mostly fine to me. A few minor comments. -and one type error that needs to be fixed.-

I would like to see the class hierarchy refactored in a follow up patch. {{OMFileCreateRequest}}should not extend {{OMKeyCreateRequest}}. Instead they should both extend an abstract class that encapsulates the common functionality.

Generally deriving from _concrete_ classes is a bad idea.

 

This Jira is created based on [~arp] comment during review of HDDS-1731",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-03 18:18:58,2
13242825,Use ExecutorService in OzoneManagerStateMachine,"In the current code in applyTransaction we have 

CompletableFuture<Message> future = CompletableFuture
 .supplyAsync(() -> runCommand(request, trxLogIndex)); We are using ForkJoin#commonPool.

With the current approach we have 2 issues:
 # Thread exhausts when using this common pool.
 # Not a good practice of using common pool. Found some issues in our testing by using similarly in RatisPipelineUtils.
 # OM DB's across replica can be out of sync when the apply transactions are applied in out of order.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-02 18:41:37,2
13242770,Datanode unable to find chunk while replication data using ratis.,"Leader datanode is unable to read chunk from the datanode while replicating data from leader to follower.
Please note that deletion of keys is also happening while the data is being replicated.

{code}
2019-07-02 19:39:22,604 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1
014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#70:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782
2019-07-02 19:39:22,605 ERROR impl.ChunkManagerImpl (ChunkUtils.java:readData(161)) - Unable to find the chunk file. chunk info : ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3
-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048}
2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(990)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: Failed appendEntries as latest snapshot (9770) already h
as the append entries (first index: 1)
2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1
014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#71:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782
2019-07-02 19:39:22,605 INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: ReadChunk : Trace ID: 4216d461a4679e17:4216d461a4679e17:0:0 : Message: Unable to find the c
hunk file. chunk info ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048} : Result: UNABLE_TO_FIND_CHUNK
2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(990)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: Failed appendEntries as latest snapshot (9770) already h
as the append entries (first index: 2)
2019-07-02 19:39:22,606 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1
014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#72:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782
19:39:22.606 [pool-195-thread-19] ERROR DNAudit - user=null | ip=null | op=READ_CHUNK {blockData=conID: 3 locID: 102372189549953034 bcsId: 0} | ret=FAILURE
java.lang.Exception: Unable to find the chunk file. chunk info ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048}
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:346) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:476) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$getCachedStateMachineData$2(ContainerStateMachine.java:495) ~[hadoop-hdds-container-service-0.5.0-SN
APSHOT.jar:?]
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4767) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3965) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4764) ~[guava-11.0.2.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.getCachedStateMachineData(ContainerStateMachine.java:494) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.ja
r:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$readStateMachineData$4(ContainerStateMachine.java:542) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-02 14:14:53,3
13242682,Ozone Client should randomize the list of nodes in pipeline for reads,"Currently the list of nodes returned by SCM are static and are returned in the same order to all the clients. Ideally these should be sorted by the network topology and then returned to client.

However even when network topology in not available, then SCM/client should randomly sort the nodes before choosing the replica's to connect.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-07-02 08:35:27,5
13242320,Cleanup 2phase old HA code for Key requests.,"HDDS-1638 brought in HA code for Key operations like allocateBlock,createKey etc., 

Old code changes which are added as part of HDDS-1250 and HDDS-1262 for allocateBlock and openKey.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-29 22:01:30,2
13242256,Create separated unit and integration test executor dev-support scripts,"hadoop-ozone/dev-support/checks directory contains multiple helper script to execute different type of testing (findbugs, rat, unit, build).

They easily define how tests should be executed, with the following contract:

 * The problems should be printed out to the console

 * in case of test failure a non zero exit code should be used

 

The tests are working well (in fact I have some experiments with executing these scripts on k8s and argo where all the shell scripts are executed parallel) but we need some update:

 1. Most important: the unit tests and integration tests can be separated. Integration tests are more flaky and it's better to have a way to run only the normal unit tests

 2. As HDDS-1115 introduced a pom.ozone.xml it's better to use them instead of the magical ""am pl hadoop-ozone-dist"" trick--

 3. To make it possible to run blockade test in containers we should use - T flag with docker-compose

 4. checkstyle violations are printed out to the console",pull-request-available,[],HDDS,Improvement,Major,2019-06-28 23:51:33,1
13242059,Implement File CreateFile Request to use Cache and DoubleBuffer,"In this Jira, we shall implement createFile request according to the HA model, and use cache and double buffer.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-27 23:53:25,2
13242026,Implement File CreateDirectory Request to use Cache and DoubleBuffer,"In this Jira, we shall implement createDirectory request according to the HA model, and use cache and double buffer.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-27 20:52:52,2
13241569,Use generation of resourceName for locks in OzoneManagerLock,"In this Jira, we shall use generate Resourcename from actual resource names like volume/bucket/user/key inside OzoneManagerLock. In this way, users using these locking API's no need to worry of calling these additional API of generateResourceName in OzoneManagerLockUtil. And this also reduces code during acquiring locks in OM operations.",pull-request-available,[],HDDS,Sub-task,Major,2019-06-25 19:19:37,2
13241308,pv-test example to test csi is not working,"[~rmaruthiyodan] reported two problems regarding to the pv-test example in csi examples folder.

pv-test folder contains an example nginx deployment which can use an ozone PVC/PV to publish content of a folder via http.

Two problems are identified:
 * The label based matching filter of service doesn't point to the nginx deployment
 * The configmap mounting is missing from nginx deployment",pull-request-available,[],HDDS,Bug,Blocker,2019-06-24 17:13:47,1
13240985,Create new OzoneManagerLock class,"This Jira is to use bit manipulation, instead of hashmap in OzoneManager lock logic. And also this Jira follows the locking order based on the document attached to HDDS-1672 jira.

This Jira is created based on [~anu] comment during review of HDDS-1672.

Not a suggestion for this patch. But more of a question, should we just maintain a bitset here, and just flip that bit up and down to see if the lock is held. Or we can just maintain 32 bit integer, and we can easily find if a lock is held by Xoring with the correct mask. I feel that might be super efficient. [@nandakumar131|https://github.com/nandakumar131] . But as I said let us not do that in this patch.

 

This Jira will add new class, integration of this new class into code will be done in a new jira. 

Clean up of old code also will be done in new jira.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-06-22 00:05:04,2
13240949,Use the bindings in ReconSchemaGenerationModule to create Recon SQL tables on startup,"Currently the table creation is done for each schema definition one by one. 

Setup sqlite DB and create Recon SQL tables.
cc [~vivekratnavel], [~swagle]",newbie pull-request-available,['Ozone Recon'],HDDS,Task,Major,2019-06-21 20:18:33,5
13240940,Client Metrics are not being pushed to the configured sink while running a hadoop command to write to Ozone.,Client Metrics are not being pushed to the configured sink while running a hadoop command to write to Ozone.,pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-06-21 19:12:05,5
13240939,Add ability to configure RocksDB logs for Ozone Manager,"While doing performance testing, it was seen that there was no way to get RocksDB logs for Ozone Manager. Along with Rocksdb metrics, this may be a useful mechanism to understand the health of Rocksdb while investigating large clusters. ",pull-request-available,[],HDDS,Task,Major,2019-06-21 19:10:49,5
13240936,Increase ratis log segment size to 1MB.,"While testing out ozone with long running clients which continuously write data, it was noted ratis logs were rolled 1-2 times every second. This adds unnecessary overhead to the pipeline thereby affecting write throughput. Increasing the size of the log segment to 1MB will decrease the overhead.",pull-request-available,['Ozone Datanode'],HDDS,Task,Major,2019-06-21 19:02:23,5
13240851,MR Job fails as OMFailoverProxyProvider has dependency hadoop-3.2,"Mapreduce Jobs are failing with exception ??Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient exception??

Ozone hadoop-ozone-filesystem-lib-current.jar copied to HDP cluster's hadoop and mapreduce classpath under :

{code:java}
/usr/hdp/3.1.0.0-78/hadoop/lib/hadoop-ozone-filesystem-lib-current-0.5.0-SNAPSHOT.jar
/usr/hdp/3.1.0.0-78/hadoop-mapreduce/hadoop-ozone-filesystem-lib-current-0.5.0-SNAPSHOT.jar
{code}

Excerpt from exception :
{code:java}
2019-06-21 10:07:57,982 ERROR [main] org.apache.hadoop.ozone.client.OzoneClientFactory: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient exception:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)
	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)
	at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.lang.VerifyError: Cannot inherit from final class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
	... 33 more
2019-06-21 10:07:57,985 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state INITED
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:299)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)
	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)
	at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)
	... 11 more
Caused by: java.lang.VerifyError: Cannot inherit from final class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
	... 28 more
2019-06-21 10:07:57,987 ERROR [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:299)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)
	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)
	at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)
	... 11 more
Caused by: java.lang.VerifyError: Cannot inherit from final class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
	... 28 more
2019-06-21 10:07:57,988 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient

End of LogType:syslog
***********************************************************************
{code}


PFA YARN application logs for detailed exception.
",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Blocker,2019-06-21 11:58:20,7
13240844,Smoketest results are generated with an internal user,"[~eyang] reported the problem in HDDS-1609 that the smoketest results are generated a user (the user inside the docker container) which can be different from the host user.

There is a minimal risk that the test results can be deleted/corrupted by an other users if the current user is different from uid=1000

I opened this issue because [~eyang] said me during an offline discussion that HDDS-1609 is a more complex issue and not only about the ownership of the test results.

I suggest to handle the two problems in different way. With this patch, the permission of the test result files can be fixed easily.

In HDDS-1609 we can discuss about general security problems and try to find generic solution for them.

Steps to reproduce _this_ problem:
 # Use a user which is different from uid=1000
 # Create a new ozone build (mvn clean install -f pom.ozone.xml -DskipTests)
 # Go to a compose directory (cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/)
 # Execute tests (./test.sh)
 # check the ownership of the results (ls -lah ./results)

Current result: the owner of the result files are the user uid=1000

Expected result: the owner of the files should be always the current user (even if the current uid is different)

 ",pull-request-available,[],HDDS,Bug,Minor,2019-06-21 11:19:28,1
13240801,"When restart om with Kerberos, NPException happened at addPersistedDelegationToken ","the error stack:
{code:java}
2019-06-21 15:17:41,744 [main] INFO - Loaded 11 tokens
2019-06-21 15:17:41,745 [main] INFO - Loading token state into token manager.
2019-06-21 15:17:41,748 [main] ERROR - Failed to start the OzoneManager.
java.lang.NullPointerException
at org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.addPersistedDelegationToken(OzoneDelegationTokenSecretManager.java:371)
at org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.loadTokenSecretState(OzoneDelegationTokenSecretManager.java:358)
at org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.<init>(OzoneDelegationTokenSecretManager.java:96)
at org.apache.hadoop.ozone.om.OzoneManager.createDelegationTokenSecretManager(OzoneManager.java:608)
at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:332)
at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:941)
at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)
2019-06-21 15:17:41,753 [pool-2-thread-1] INFO - SHUTDOWN_MSG:

{code}",TriagePending,['Ozone Manager'],HDDS,Bug,Major,2019-06-21 07:34:37,4
13240607,Publish JVM metrics via Hadoop metrics,"In ozone metrics can be published with the help of hadoop metrics (for example via PrometheusMetricsSink)

The basic jvm metrics are not published by the metrics system (just with JMX)

I am very interested about the basic JVM metrics (gc count, heap memory usage) to identify possible problems in the test environment.

Fortunately it's very easy to turn it on with the help of org.apache.hadoop.metrics2.source.JvmMetrics.",pull-request-available,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Improvement,Major,2019-06-20 08:27:24,1
13240605,TestScmSafeNode is flaky,"org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode is failed at last night with the following error:
{code:java}
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode(TestScmSafeMode.java:285) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}
Locally it can be tested but it's very easy to reproduce by adding an additional sleep DataNodeSafeModeRule:
{code:java}
+++ b/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/DataNodeSafeModeRule.java
@@ -63,7 +63,11 @@ protected boolean validate() {
 
   @Override
   protected void process(NodeRegistrationContainerReport reportsProto) {
-
+    try {
+      Thread.sleep(3000);
+    } catch (InterruptedException e) {
+      e.printStackTrace();
+    }{code}
This is a clear race condition:

DatanodeSafeModeRule and ContainerSafeModeRule are processing the same events but it can be possible (in case of an accidental sleep) that the container safe mode rule is done, but DatanodeSafeModeRule didn't process the new event (yet).

As a result the test execution will continue:
{code:java}
GenericTestUtils
    .waitFor(() -> scm.getCurrentContainerThreshold() == 1.0, 100, 20000);
{code}
(This line is waiting ONLY for the ContainerSafeModeRule).

The fix is easy, let's wait for the processing of all the async events:
{code:java}
EventQueue eventQueue =
    (EventQueue) cluster.getStorageContainerManager().getEventQueue();
eventQueue.processAll(5000L);{code}
As we are sure that the events are already sent to the EventQueue (because we have the previous waitFor), it should be enough.",pull-request-available,"['SCM', 'test']",HDDS,Bug,Critical,2019-06-20 08:17:12,1
13240059,Optimize Ozone Recon build time ,"Currently, hadoop-ozone-recon node_modules folder is copied to target folder and this takes a lot of time while building hadoop-ozone project. Reduce the build time by excluding node_modules folder.",pull-request-available,[],HDDS,Task,Major,2019-06-17 20:48:38,6
13240009,Switch to use apache/ozone-runner in the compose/Dockerfile,"Since HDDS-1634 we have an ozone specific runner image to run ozone with docker-compose based pseudo clusters.

As the new apache/ozone-runner image is uploaded to the dockerhub we can switch our scripts and use the new image.",pull-request-available,['docker'],HDDS,Improvement,Critical,2019-06-17 17:40:13,1
13240005,RocksDB use separate Write-ahead-log location for RocksDB.,"This will help on production systems where WAL logs and db actual data files will be in a different location. During compaction, it will not affect actual writes.

 

Suggested by [~msingh]",pull-request-available,[],HDDS,Bug,Major,2019-06-17 17:08:32,2
13239968,TestNodeReportHandler is failing with NPE,"{code:java}
FAILURE in ozone-unit-076618677d39x4h9/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.txt
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.scm.node.TestNodeReportHandler
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.43 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.node.TestNodeReportHandler
testNodeReport(org.apache.hadoop.hdds.scm.node.TestNodeReportHandler)  Time elapsed: 0.288 s  <<< ERROR!
java.lang.NullPointerException
    at org.apache.hadoop.hdds.scm.node.SCMNodeManager.<init>(SCMNodeManager.java:122)
    at org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.resetEventCollector(TestNodeReportHandler.java:53)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

2019-06-16 23:52:29,345 INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(119)) - Entering startup safe mode.

{code}",pull-request-available,"['SCM', 'test']",HDDS,Bug,Critical,2019-06-17 15:17:41,1
13239813,RDBTable#iterator should disabled caching of the keys during iterator,"Iterator normally do a bulk load of the keys, this causes thrashing of the actual keys in the DB.

This option is documented here:-
https://github.com/facebook/rocksdb/wiki/Basic-Operations#cache",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-06-16 16:59:06,2
13239812,RDBTable#isExist should use Rocksdb#keyMayExist,"RDBTable#isExist can use Rocksdb#keyMayExist, this avoids the cost of reading the value for the key.

Please refer, 
https://github.com/facebook/rocksdb/blob/7a8d7358bb40b13a06c2c6adc62e80295d89ed05/java/src/main/java/org/rocksdb/RocksDB.java#L2184",pull-request-available,[],HDDS,Improvement,Major,2019-06-16 16:54:06,5
13239655,Implement S3 Create Bucket request to use Cache and DoubleBuffer,"Implement S3 Bucket write requests to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-14 22:31:50,2
13239388,"Recon: Add support for ""start"" query param to containers and containers/{id} endpoints","* Support ""start"" query param to seek to the given key in RocksDB.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-06-13 22:33:30,6
13239387,OM should create Ratis related dirs only if ratis is enabled,"In OM, Ratis related dirs (storage, snapshot etc.) should only be created if OM ratis is enabled.",pull-request-available,[],HDDS,Bug,Major,2019-06-13 22:33:10,7
13239295,TestEventWatcher.testMetrics is flaky,"TestEventWatcher is intermittent. (Failed twice out of 44 executions).

Error is:

{code}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.764 s <<< FAILURE! - in org.apache.hadoop.hdds.server.events.TestEventWatcher
testMetrics(org.apache.hadoop.hdds.server.events.TestEventWatcher)  Time elapsed: 2.384 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<3>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdds.server.events.TestEventWatcher.testMetrics(TestEventWatcher.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

In the test we do the following:

 1. fire start-event1
 2. fire start-event2
 3. fire start-event3
 4. fire end-event1
 5. wait

Usually the event2 and event3 are timed out and event1 is completed but in case of an accidental time between 3 and 4 (in fact between 1 and 4) the event1 also can be timed out.

I improved the unit test and fixed the metrics calculation (completed message should be incremented only if it's not yet timed out).",pull-request-available,['test'],HDDS,Bug,Major,2019-06-13 14:25:37,1
13239270,Create missing parent directories during the creation of HddsVolume dirs,"I started to execute all the unit tests continuously (in kubernetes with argo workflow).

Until now I got the following failures (number of failures / unit test name):

```
      1 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir
      1 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
      3 org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware
     31 org.apache.hadoop.ozone.container.common.TestDatanodeStateMachine
     31 org.apache.hadoop.ozone.container.common.volume.TestVolumeSet
      1 org.apache.hadoop.ozone.freon.TestDataValidateWithSafeByteOperations
```

TestVolumeSet is also failed locally:

{code}
2019-06-13 14:23:18,637 ERROR volume.VolumeSet (VolumeSet.java:initializeVolumeSet(184)) - Failed to parse the storage location: /home/elek/projects/hadoop/hadoop-hdds/container-service/target/test-dir/dfs
java.io.IOException: Cannot create directory /home/elek/projects/hadoop/hadoop-hdds/container-service/target/test-dir/dfs/hdds
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.initialize(HddsVolume.java:208)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:179)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:72)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume$Builder.build(HddsVolume.java:156)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.createVolume(VolumeSet.java:311)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.initializeVolumeSet(VolumeSet.java:165)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.<init>(VolumeSet.java:130)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.<init>(VolumeSet.java:109)
	at org.apache.hadoop.ozone.container.common.volume.TestVolumeSet.testFailVolumes(TestVolumeSet.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}

The problem here is that the parent directory of the volume dir is missing. I propose to use hddsRootDir.mkdirs() instead of hddsRootDir.mkdir() which creates the missing parent directories.
",pull-request-available,[],HDDS,Bug,Major,2019-06-13 12:27:35,1
13239202,Default image name for kubernetes examples should be ozone and not hadoop,"During the build the kubernetes example files are adjusted to use a specific docker image name.

By default it should be the apache/ozone:${VERSION} to make it possible to use the examples without any build from the release artifact. With the examples of the release artifact the user can use the latest released apache/ozone:${VERSION} from docker hub.

For development build the image can be set with -Ddocker.image (or -Dozone.docker.image with HDDS-1667).

Unfortunately -- due to a small typo -- apace/hadoop image is used by default instead of apache/ozone.  ",pull-request-available,['docker'],HDDS,Bug,Blocker,2019-06-13 08:49:14,1
13239198,Auditparser robot test shold use a world writable working directory,"When I tried to reproduce a problem which is reported by [~eyang], I found that the auditparser robot test uses the /opt/hadoop directory as a working directory to generate the audit.db export.

/opt/hadoop is may or may not be writable, it's better to use /tmp instead.",pull-request-available,['test'],HDDS,Bug,Major,2019-06-13 08:16:17,1
13239152,Cleanup Volume Request 2 phase old code,This Jira is to clean up the old 2 phase HA code for Volume requests.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-13 01:05:10,2
13238936,Improve locking in OzoneManager,"In this Jira, we shall follow the new lock ordering. In this way, in volume requests we can solve the issue of acquire/release/reacquire problem. And few bugs in the current implementation of S3Bucket/Volume operations.

 

Currently after acquiring volume lock, we cannot acquire user lock. 

This is causing an issue in Volume request implementation, acquire/release/reacquire volume lock.

 

Case of Delete Volume Request: 
 # Acquire volume lock.
 # Get Volume Info from DB
 # Release Volume lock. (We are releasing the lock, because while acquiring volume lock, we cannot acquire user lock0
 # Get owner from volume Info read from DB
 # Acquire owner lock
 # Acquire volume lock
 # Do delete logic
 # release volume lock
 # release user lock

 

We can avoid this acquire/release/reacquire lock issue by making volume lock as low weight. 

 

In this way, the above deleteVolume request will change as below
 # Acquire volume lock
 # Get Volume Info from DB
 # Get owner from volume Info read from DB
 # Acquire owner lock
 # Do delete logic
 # release owner lock
 # release volume lock. 

Same issue is seen with SetOwner for Volume request also.

During HDDS-1620 [~arp] brought up this issue. 

I am proposing the above solution to solve this issue. Any other idea/suggestions are welcome.

This also resolves a bug in setOwner for Volume request.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-06-12 05:52:03,2
13238902,Add limit support to /api/containers and /api/containers/{id} endpoints,Add support for limit query param to limit the results of /api/containers and /api/containers/\{id} endpoints,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-06-11 23:48:19,6
13238771,SCM startup is failing if network-topology-default.xml is part of a jar,"network-topology-default.xml can be loaded from file or classpath. But the NodeSchemaLoader assumes that the files on the classpath can be opened as a file. It's true if the file is in etc/hadoop (which is part of the classpath) but not true if the file is packaged to a jajr file:

{code}
scm_1         | 2019-06-11 13:18:03 INFO  NodeSchemaLoader:118 - Loading file from jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar!/network-topology-default.xml
scm_1         | 2019-06-11 13:18:03 ERROR NodeSchemaManager:74 - Failed to load schema file:network-topology-default.xml, error:
scm_1         | java.lang.IllegalArgumentException: URI is not hierarchical
scm_1         | 	at java.io.File.<init>(File.java:418)
scm_1         | 	at org.apache.hadoop.hdds.scm.net.NodeSchemaLoader.loadSchemaFromFile(NodeSchemaLoader.java:119)
scm_1         | 	at org.apache.hadoop.hdds.scm.net.NodeSchemaManager.init(NodeSchemaManager.java:67)
scm_1         | 	at org.apache.hadoop.hdds.scm.net.NetworkTopologyImpl.<init>(NetworkTopologyImpl.java:63)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:382)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:275)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:208)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:586)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:139)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:115)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:67)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:42)
scm_1         | 	at picocli.CommandLine.execute(CommandLine.java:1173)
scm_1         | 	at picocli.CommandLine.access$800(CommandLine.java:141)
scm_1         | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
scm_1         | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
scm_1         | 	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
scm_1         | 	at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
scm_1         | 	at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
scm_1         | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
scm_1         | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:56)
scm_1         | Failed to load schema file:network-topology-default.xml, error:
{code}

The quick fix is to keep the current behaviour but read the file from classloader.getResourceAsStream() instead of classloader.getResource().toURI()",pull-request-available,[],HDDS,Bug,Blocker,2019-06-11 13:46:48,1
13238720,Add liveness probe to the example k8s resources files,"In kubernetes resources we can define livebess probes which can help to detect any failure. If the define port is not available the pod will be rescheduled.

We need to add the liveness probes to our k8s resource files.

Note: We shouldn't add readiness probes. Readiness probe is about the service availability. The service/dns can be available only after the service is restarted. This is not good for us as:

 * We need DNS resolution during the startup (See OzoneManager.loadOMHAConfigs)
 * We already implemented retry in case of missing DNS entries",pull-request-available,[],HDDS,Sub-task,Major,2019-06-11 09:37:57,1
13238619,Improve logic in openKey when allocating block,"We set size as below

{code}
final long size = args.getDataSize() >= 0 ?
 args.getDataSize() : scmBlockSize;
{code}
 

and create OmKeyInfo with below size set. But when allocating Block for openKey, we use as below.

allocateBlockInKey(keyInfo, args.getDataSize(), currentTime);

 

I feel here, we should use size which is set above so that we allocate at least a block when the openKey call happens.",pull-request-available,[],HDDS,Bug,Major,2019-06-10 19:57:52,2
13238135,Define the process to add proposal/design docs to the Ozone subproject,"We think that it would be more effective to collect all the design docs in one place and make it easier to review them by the community.

We propose to follow an approach where the proposals are committed to the hadoop-hdds/docs project and the review can be the same as a review of a PR",pull-request-available,[],HDDS,Task,Major,2019-06-07 07:26:48,1
13238098,RaftRetryFailureException & AlreadyClosedException should not exclude pipeline from client,"This problem can be seen at https://builds.apache.org/job/hadoop-multibranch/job/PR-846/6/testReport/org.apache.hadoop.ozone.client.rpc/TestBCSID/testBCSID/.

As seen here, after a RaftRetryFailureException, the pipeline is excluded from the pipeline and that leads to SCM create a new pipeline. Creation of a new pipeline might not be possible in a test cluster because of limited number of nodes.

{code}
2019-06-06 22:29:23,311 WARN  KeyOutputStream - Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-AD0A1CB44582->73f367e6-7f91-4409-b4d3-b831e0bfb585@group-31FAD62742D6, cid=1, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$313/1466662004@60d08041 for 180 attempts with RetryLimited(maxAttempts=180, sleepTime=1000ms) on the pipeline Pipeline[ Id: 27d23af1-7180-42f5-b3c7-31fad62742d6, Nodes: 73f367e6-7f91-4409-b4d3-b831e0bfb585{ip: 172.17.0.2, host: 5e847226af57, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]. The last committed block length is 0, uncommitted data length is 5 retry count 0
2019-06-06 22:29:23,343 WARN  BlockManagerImpl - Pipeline creation failed for type:RATIS factor:ONE. Retrying get pipelines call once.
org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 1 using 0 nodes.
	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:151)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:57)
	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:149)
	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:190)
	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
	at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)
	at 
{code}",Triaged,['Ozone Client'],HDDS,Bug,Major,2019-06-07 03:27:19,3
13237876,Ensure container state on datanode gets synced to disk whenever state change happens,"Currently, whenever there is a container state change, it updates the container but doesn't sync.

The idea is here to is to force sync the state to disk everytime there is a state change.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-06-06 05:31:49,3
13237858,"Add option to ""ozone scmcli printTopology"" to order the output acccording to topology layer","Add option to order the output acccording to topology layer.
For example, for /rack/node topolgy, we can show,
State = HEALTHY
/default-rack:
ozone_datanode_1.ozone_default/172.18.0.3
ozone_datanode_2.ozone_default/172.18.0.2
ozone_datanode_3.ozone_default/172.18.0.4
/rack1:
ozone_datanode_4.ozone_default/172.18.0.5
ozone_datanode_5.ozone_default/172.18.0.6
For /dc/rack/node topology, we can either show
State = HEALTHY
/default-dc/default-rack:
ozone_datanode_1.ozone_default/172.18.0.3
ozone_datanode_2.ozone_default/172.18.0.2
ozone_datanode_3.ozone_default/172.18.0.4
/dc1/rack1:
ozone_datanode_4.ozone_default/172.18.0.5
ozone_datanode_5.ozone_default/172.18.0.6

or

State = HEALTHY
default-dc:
default-rack:
ozone_datanode_1.ozone_default/172.18.0.3
ozone_datanode_2.ozone_default/172.18.0.2
ozone_datanode_3.ozone_default/172.18.0.4
dc1:
rack1:
ozone_datanode_4.ozone_default/172.18.0.5
ozone_datanode_5.ozone_default/172.18.0.6",pull-request-available,[],HDDS,Sub-task,Major,2019-06-06 01:58:42,4
13237828,HddsDispatcher should not shutdown volumeSet,"Currently both OzoneContainer#stop() and HddsDispatcher#stop() both invoke volumeSet.shutdown() explicitly to shutdown the same volume set.

 

In addition, OzoneContainer#stop() will invoke HddsDispatcher#stop(). Since the volume set object is created by OzoneContainer object, it should be the responsibility of OzoneContainer to shutdown. This ticket is opened to remove the volumeSet.shutdown() from HddsDispatcher#stop(). 

 

There are benchmark tools relies on HddsDispatcher#stop() to shutdown volumeSet object, that we could fix with explict volumeSet#shutdown call. ",pull-request-available,[],HDDS,Bug,Major,2019-06-05 22:01:34,4
13237819,Fix Ozone tests leaking volume checker thread,There are a few test leaking hdds volume checker thread. This ticket is opened to fix them. ,pull-request-available,[],HDDS,Bug,Major,2019-06-05 21:35:56,4
13237782,"On installSnapshot notification from OM leader, download checkpoint and reload OM state","Installing a DB checkpoint on the OM involves following steps:
 1. When an OM follower receives installSnapshot notification from OM leader, it should initiate a new checkpoint on the OM leader and download that checkpoint through Http. 
 2. After downloading the checkpoint, the StateMachine must be paused so that the old OM DB can be replaced with the new downloaded checkpoint. 
 3. The OM should be reloaded with the new state . All the services having a dependency on the OM DB (such as MetadataManager, KeyManager etc.) must be re-initialized/ restarted. 
 4. Once the OM is ready with the new state, the state machine must be unpaused to resume participating in the Ratis ring.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-05 18:22:52,7
13237764,Recon config tag does not show up on Ozone UI.,Recon tag does not show up on the list of tags on /conf page. ,pull-request-available,[],HDDS,Bug,Major,2019-06-05 16:59:11,5
13237720,Support real persistence in the k8s example files ,"Ozone release contains example k8s deployment files to make it easier to deploy Ozone to kubernetes. As of now we use emptyDir everywhere, we should support the configuration of host volumes (hostPath or Local Persistent volumes).

The big question here is the default:

* Make the examples easy to start and ephemeral
* Make the examples more safe, by default (but couldn't be started without additional administration).

(Note this conversation is started in the review of HDDS-1508)

Xiaoyu:  Can we support mount hostVolume for datanode daemons?

Marton: Yes, we can.

AFAIK there are two options:
 * using [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)
 * or with [Local PersistentVolumes](https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/)

The first one requires the knowledge of directory names on the host.
The second one is recommended but it requires the creation of PersistentVolumes or install a PersistentVolume provider

I am not sure what is the best approach, my current proposal is:

 * Use empty dir everywhere to make it easier to start simple ozone cluster
 * Provide simple option to turn on any of theses persistence (the kubernetes files are generated and the generation can be parametrized)
 * Document how to customize the kubernetes resources files

Summary: it's question of the defaults:
 
  1. Use a complex, but persistent solution, which may not work out of the box   
  2. Use a simple, but ephemeral solution (as default)

I started to use (2) but I am open to change.

",pull-request-available,[],HDDS,Sub-task,Critical,2019-06-05 12:33:01,1
13237491,Csi server fails because transitive Netty dependencies,"CSI server can't be started because an ClassNotFound exception.

It turned out that with using the new configuration api we got old netty jar files as transitive dependencies. (hdds-configuration depends on hadoop-common, hadoop-commons depends on the word)

We should exclude all the old netty version from the classpath of the CSI server.",pull-request-available,[],HDDS,Bug,Blocker,2019-06-04 13:20:14,1
13237438,Reduce the size of recon jar file,"hadoop-ozone-recon-0.5.0-SNAPSHOT.jar is 73 MB, mainly because the node_modules are included (full typescript source, eslint, babel, etc.):

{code}
unzip -l hadoop-ozone-recon-0.5.0-SNAPSHOT.jar | grep node_modules | wc
{code}

Fix me if I am wrong, but I think node_modules is not required in the distribution as the dependencies are already included in the compiled javascript files.

I propose to remove the node_modules from the jar file.
",pull-request-available,['Ozone Recon'],HDDS,Improvement,Major,2019-06-04 09:37:36,6
13237425,Restructure documentation pages for better understanding,"Documentation page should be updated according to the recent changes:

In the uploaded PR I modified the following:

 #  Pages are restructured to use a similar structure what is intruced on the wiki by [~anu]. (Getting started guides are separated for different environments)
 # The width of the menu is increased (to make it more readable)
 # The logo is moved from the main page from the menu (to get more space for the menu items)
 # 'Requirements' section is added to each 'Getting started' page
 # Test tools / docker image / kubernetes pages are imported from the wiki. ",pull-request-available,['documentation'],HDDS,Improvement,Blocker,2019-06-04 08:41:54,1
13237383,Implement Key Write Requests to use Cache and DoubleBuffer,"Implement Key write requests to use OM Cache, double buffer. 

In this Jira will add the changes to implement key operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-04 02:33:34,2
13237232,Tracing id is not propagated via async datanode grpc call,"Recently a new exception become visible in the datanode logs, using standard freon (STANDLAONE)

{code}
datanode_2  | 2019-06-03 12:18:21 WARN  PropagationRegistry$ExceptionCatchingExtractorDecorator:60 - Error when extracting SpanContext from carrier. Handling gracefully.
datanode_2  | io.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 7576cabf-37a4-4232-9729-939a3fdb68c4WriteChunk150a8a848a951784256ca0801f7d9cf8b_stream_ed583cee-9552-4f1a-8c77-63f7d07b755f_chunk_1
datanode_2  | 	at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:49)
datanode_2  | 	at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:34)
datanode_2  | 	at io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)
datanode_2  | 	at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)
datanode_2  | 	at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)
datanode_2  | 	at io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)
datanode_2  | 	at org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:102)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
datanode_2  | 	at org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
{code}

It turned out that the tracingId propagation between XCeiverClient and Server doesn't work very well (in case of Standalone and async commands)

 1. there are many places (on the client side) where the traceId filled with  UUID.randomUUID().toString();  
 2. This random id is propagated between the Output/InputStream and different part of the clients
 3. It is unnecessary, because in the XceiverClientGrpc and XceiverClientGrpc the traceId field is overridden with the real opentracing id anyway (sendCommand/sendCommandAsync)
 4. Except in the XceiverClientGrpc.sendCommandAsync where this part is accidentally missing.

Things to fix:

 1. fix XceiverClientGrpc.sendCommandAsync (replace any existing traceId with the good one)
 2. remove the usage of the UUID based traceId (it's not used)
 3. Improve the error logging in case of an invalid traceId on the server side.",pull-request-available,[],HDDS,Bug,Major,2019-06-03 12:29:29,1
13237216,Maintain docker entrypoint and envtoconf inside ozone project,"During an offline discussion with [~eyang] and [~arp], Eric suggested to maintain the source of the docker specific start images inside the main ozone branch (trunk) instead of the branch of the docker image.

With this approach the ozone-runner image can be a very lightweight image and the entrypoint logic can be versioned together with the ozone itself.

An other use case is a container creation script. Recently we [documented|https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Docker+images] that hadoop-runner/ozone-runner/ozone images are not for production (for example because they contain development tools).

We can create a helper tool (similar what Spark provides) to create Ozone container images from any production ready base image. But this tool requires the existence of the scripts inside the distribution.

(ps: I think sooner or later the functionality of envtoconf.py can be added to the OzoneConfiguration java class and we can parse the configuration values directly from environment variables.

In this patch I copied the required scripts to the ozone source tree and the new ozone-runner image (HDDS-1634) is designed to use it from this specific location.
",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 11:10:51,1
13237212,Introduce a new ozone specific runner image,"Ozone compose files use apache/hadoop-runner to provide a fixed environment to run any Ozone distribution.

 It can be better to use separated hadoop-runner and ozone-runner:

 1. To make it easier to include Ozone specific behaviour (For example goofys install, scm/om initialization)
 2. To make it clean which feature is required by all the subprojects of Hadoop and which one is Ozone specific (base on the comment from [~eyang] in HADOOP-16092)
 3. for hadoop-runner we maintain two tags (jdk11/jdk8/latest). And it seems to be hard to maintain all of them. jdk8 is required only for hadoop and with separating hadoop-runner/ozone-runner we can use only one simple branch for ozone-runner development (and we can create incremental fixed tags very easily)",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 10:43:05,1
13237182,Update rat from 0.12 to 0.13 in hadoop-runner build script,"We have a new rat, the old one is not available. The url should be updated.",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 08:29:19,1
13237179,Make the hadoop home world readable and avoid sudo in hadoop-runner,"[~eyang] reporeted in HDDS-1609 that the hadoop-runner image can be started *without* mounting a real hadoop (usually, it's ounted) AND using a different uid:

{code}
docker run -it  -u $(id -u):$(id -g) apache/hadoop-runner bash
docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""chdir to cwd (\""/opt/hadoop\"") set in config.json failed: permission denied"": unknown.
{code}

There are two blocking problems here:

 * the /opt/hadoop directory (which is the CWD inside the container) is 700 instead of 755
 * The usage of sudo in started scripts (sudo is not possible if the real user is not added to the /etc/passwd)

Both of them are addressed by this patch.",pull-request-available,[],HDDS,Improvement,Trivial,2019-06-03 08:19:45,1
13237176,Fix auditparser smoketests,"In HDDS-1518 we modified the location of the var and config files inside the container.

There are three problems with the current auditparser smokest:

 1. The default audit log4j files are not part of the new config directory (fixed with HDDS-1630)
 2. The smoketest is executed in scm container instead of om
 3. The log directory is hard coded

The 2 and 3 will be fined in this patch. ",pull-request-available,['test'],HDDS,Improvement,Major,2019-06-03 08:04:59,1
13237174,Copy default configuration files to the writeable directory,"HDDS-1518 separates the read-only directories (/opt/ozone, /opt/hadoop) from the read-write directories (/etc/hadoop, /var/log/hadoop). 

The configuration directory and log directory should be writeable and to make it easier to run the docker-compose based pseudo clusters with *different* host uid we started to use different config dir.

But we need all the defaults in the configuration dir. In this patch I add a small fragments to the hadoop-runner image to copy the default files (if available).",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 08:00:53,1
13237161,Tar file creation can be optional for non-dist builds,"Ozone tar file creation is a very time consuming step. I propose to make it optional and create the tar file only if the dist profile is enabled (-Pdist)

The tar file is not required to test ozone as the same content is available from hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT which is enough to run docker-compose pseudo clusters, smoketests. 

If it's required, the tar file creation can be requested by the dist profile.
 
On my machine (ssd based) it can cause 5-10% time improvements as the tar size is ~500MB and it requires a lot of IO.",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 06:27:34,1
13237158,Fix the execution and return code of smoketest executor shell script,"Problem: Some of the smoketest executions were reported to green even if they contained failed tests.

Root cause: the legacy test executor (hadoop-ozone/dist/src/main/smoketest/test.sh) which just calls the new executor script (hadoop-ozone/dist/src/main/compose/test-all.sh) didn't handle the return code well (the failure of the smoketests should be signalled by the bash return code)

This patch:
 * Fixes the error code handling in smoketest/test.sh
 * Fixes the test execution in compose/test-all.sh (should work from any other directories)
 * Updates hadoop-ozone/dev-support/checks/acceptance.sh to use the newer test-all.sh executor instead of the old one.",pull-request-available,['test'],HDDS,Bug,Blocker,2019-06-03 06:06:24,1
13237155,Make the version of the used hadoop-runner configurable,"During an offline discussion with [~arp] and [~eyang] we agreed that it could be more safe to fix the tag of the used hadoop-runner images during the releases.

It also requires fix tags from hadoop-runner, but after that it's possible to use the fixed tags.

This patch makes it possible to define the required version/tag in pom.xml

 1. the default hadoop-runner.version is added to all .env files  during the build
 2. If a variable is added to the .env, it can be used from docker-compose files AND can be overridden by environment variables (it makes it possible to define custom version during a local run) ",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 05:54:45,1
13236973,ConcurrentModificationException when SCM has containers of different owners,"{{SCMBlockProtocolServer#allocateBlock}} throws {{ConcurrentModificationException}} when SCM has containers of different owners.

{code}
2019-05-31 13:53:16,808 WARN org.apache.hadoop.hdds.scm.container.SCMContainerManager: Container allocation failed for pipeline=Pipeline[ Id: 3e0eec4d-67d1-4582-a9e9-e68b0a340de6, Nodes: abaea3d2-a8c1-47de-8cdb-7cc5ed8f23a6{ip: 10.17.219.50, host: v
c1340.halxg.cloudera.com, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] requiredSize=268435456 {}
java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1211)
        at java.util.TreeMap$KeyIterator.next(TreeMap.java:1265)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:473)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:394)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:203)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
{code}",pull-request-available,[],HDDS,Bug,Critical,2019-05-31 21:32:18,5
13236952,Refactor operations inside the bucket lock in OM key write.,"There are a few steps that are done inside the OM bucket lock that are lock invariant and can be done outside the lock. This patch refactors those steps. It also adds an isExist API in the metadata store so that we dont need to deserialize the byte[] to Object while doing a simple _table.get(key) != null_ check. 

On applying the patch, the OM + SCM (With dummy datanodes) write performance improves by around 3 - 6x based on number of existing keys in the OM RocksDB. 

Thanks to [~nandakumar131] who helped with this patch. ",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2019-05-31 19:09:05,5
13236726,Implement Volume Write Requests to use Cache and DoubleBuffer,"Implement Volume write requests to use OM Cache, double buffer. 

In this Jira will add the changes to implement volume operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-31 00:28:23,2
13236714,Support volume acl operations for OM HA.,[HDDS-1539] adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-30 22:59:02,4
13236710,Merge code for HA and Non-HA OM requests for bucket,"In this Jira, we shall use the new code added in HDDS-1551 for Non-HA flow.

 

This Jira modifies the bucket requests only, further requests will be handled in further Jira's.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-30 22:11:13,2
13236449,Container Missing in the datanode after restart,"Container missing on the datanode after a restart.

{code}
08:10:44.308 [pool-2131-thread-1] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 34 locID: 102182684750055212 bcsId: 6198} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 34 has been lost and and cannot be recreated on this DataNode
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:207) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:149) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$0(ContainerStateMachine.java:385) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
{code}",MiniOzoneChaosCluster,['Ozone Datanode'],HDDS,Bug,Major,2019-05-30 09:59:24,3
13236296,applyTransaction failure should not be lost on restart,"If the applyTransaction fails in the containerStateMachine, then the container should not accept new writes on restart,.

This can occur if
# chunk write applyTransaction fails
# container state update to UNHEALTHY also fails
# Ratis snapshot is taken
# Node restarts
# container accepts new transactions",pull-request-available,[],HDDS,Bug,Blocker,2019-05-29 17:36:42,3
13236220,Create smoketest for non-secure mapreduce example,"We had multiple problems earlier with the classpath separation and the internal ozonefs classloader. Before fixing all the issues I propose to create a smoketest to detect if the classpath separation is broken again .

As a first step I created an smoketest/ozone-mr environment (based on the  work of [~xyao], which is secure) and a smoketest 

Possible follow-up works:

 * Adapt the test.sh for the ozonesecure-mr
 * Include test runs with older hadoop versions ",pull-request-available,['test'],HDDS,Improvement,Major,2019-05-29 11:45:45,1
13236073,Implement AuditLogging for OM HA Bucket write requests,"In this Jira, we shall implement audit logging for OM HA Bucket write requests.

As now we cannot use userName,  IpAddress from Server API's as these will be null, because the requests are executed under GRPC context. So, in our AuditLogger API's we need to pass username and remoteAddress which we can get from OMRequest after HDDS-1600 and use these during audit logging.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-28 18:46:59,2
13236055,ContainerReader#initializeUsedBytes leaks DB reference,"This was caught by the New ContainerCache with reference counting from HDDS-1449. The root cause is an unclosed KeyValueBlockIterator from ContainerReader#initializeUsedBytes.

I will post a patch shortly, which will fix some UT failures exposed by -HDDS-1449,- such as TestBCSID#testBCSID, etc.",pull-request-available,[],HDDS,Bug,Major,2019-05-28 17:33:59,4
13235880,Fix TestContainerPersistence#testDeleteBlockTwice,[https://ci.anzix.net/job/ozone/16899/testReport/org.apache.hadoop.ozone.container.common.impl/TestContainerPersistence/testDeleteBlockTwice/],pull-request-available,[],HDDS,Task,Major,2019-05-27 23:00:34,2
13235874,Implement updating lastAppliedIndex after buffer flush to OM DB.,This Jira is to implement updating lastAppliedIndex in OzoneManagerStateMachine once after the buffer is flushed to OM DB. ,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-27 21:36:21,2
13235857,Add userName and IPAddress as part of OMRequest.,"In OM HA, the actual execution of request happens under GRPC context, so UGI object which we retrieve from ProtobufRpcEngine.Server.getRemoteUser(); will not be available.

In similar manner ProtobufRpcEngine.Server.getRemoteIp().

 

So, during preExecute(which happens under RPC context) extract userName and IPAddress and add it to the OMRequest, and then send the request to ratis server.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-27 18:13:22,2
13235852,Fix TestReplicationManager and checkstyle issues.,"When working on HDDS-1551, found some test failures which are not related to HDDS-1551.

This is caused by HDDS-700. 

 

This has not caught by Jenkins run because our Jenkins run does not run UT's for all the sub-modules. In this case, it should have run UT's for hadoop-hdds-server-scm, as there are some changes in src/test files in that module, but still, it has not run for it. I think Jenkins run for ozone project is not properly setup.

[https://ci.anzix.net/job/ozone/16895/testReport/]

 ",pull-request-available,[],HDDS,Task,Major,2019-05-27 17:24:56,2
13235850,Fix Ozone checkstyle issues on trunk,"Some small checkstyle issues are accidentally committed with HDDS-700.

Trivial fixes are coming here...
",pull-request-available,[],HDDS,Bug,Blocker,2019-05-27 16:57:33,1
13235840,Remove hdds-server-scm dependency from ozone-common,"I noticed that the hadoop-ozone/common project depends on hadoop-hdds-server-scm project.

The common projects are designed to be a shared artifacts between client and server side. Adding additional dependency to the common pom means that the dependency will be available for all the clients as well.

(See the attached artifact about the current, desired structure).

We definitely don't need scm server dependency on the client side.

The code dependency is just one class (ScmUtils) and the shared code can be easily moved to the common.",pull-request-available,[],HDDS,Bug,Major,2019-05-27 15:18:53,1
13234966,Fix TestFailureHandlingByClient tests,"The test failures are caused bcoz the test relies on KeyoutputStream#getLocationList() to validate the no of preallocated blocks, but it has been changed recently to exclude the empty blocks. The fix is mostly to use KeyOutputStream#getStreamEntries() to get the no of preallocated blocks.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-05-22 18:19:25,3
13234741,Create OMDoubleBuffer metrics,"This Jira is to implement OMDoubleBuffer metrics, to show metrics like.
 # flushIterations.
 # totalTransactionsflushed.

 

Any other related metrics. This Jira is created based on the comment by [~anu] during HDDS-1512 review.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-21 22:40:00,2
13234739,Add libstdc++ to ozone build docker image,"libstdc++ is required for node install in alpine builds. Otherwise we get this error:


{code:java}
[ERROR] node: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory{code}",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2019-05-21 22:28:04,6
13234718,Add RocksDB metrics to OM,"RocksDB statistics need to sinked to hadoop-metrics2 for Ozone Manager to understand how OM behaves under heavy load.
Example: ""rocksdb.bytes.written""

https://github.com/facebook/rocksdb/wiki/Statistics

",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-05-21 20:41:48,5
13234652,Rename k8s-dev and k8s-dev-push profiles to docker and docker-push,"Based on the feedback from [~eyang] I realized that the names of the k8s-dev and k8s-dev-push profiles are not expressive enough as the created containers can be used not only for kubernetes but can be used together with any other container orchestrator.

I propose to rename them to docker/docker-push.",Triaged,[],HDDS,Improvement,Major,2019-05-21 15:54:39,1
13234180,IllegalArgumentException while processing container Reports,"IllegalArgumentException while processing container Reports

{code}
2019-05-19 23:15:04,137 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(88)) - Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@1a117ebc
java.lang.IllegalArgumentException
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:178)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:85)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:124)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:97)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:46)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-05-19 17:46:00,3
13234155,Datanode exits because Ratis fails to shutdown ratis server ,"Datanode exits because Ratis fails to shutdown ratis server 

{code}
2019-05-19 12:07:19,276 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(965)) - 80747533-f47c-43de-85b8-e70db448c63f: inconsistency entries. Reply:99930d0a-72ab-4795-a3ac-f3c
fb61ca1bb<-80747533-f47c-43de-85b8-e70db448c63f#3132:FAIL,INCONSISTENCY,nextIndex:9057,term:33,followerCommit:9057
2019-05-19 12:07:19,276 WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(320)) - e143b976-ab35-4555-a800-7f05a2b1b738: Failed to close GRPC server
java.io.InterruptedIOException: e143b976-ab35-4555-a800-7f05a2b1b738: shutdown server with port 64605 failed
        at org.apache.ratis.util.IOUtils.toInterruptedIOException(IOUtils.java:48)
        at org.apache.ratis.grpc.server.GrpcService.closeImpl(GrpcService.java:160)
        at org.apache.ratis.server.impl.RaftServerRpcWithProxy.lambda$close$2(RaftServerRpcWithProxy.java:76)
        at org.apache.ratis.util.LifeCycle.lambda$checkStateAndClose$2(LifeCycle.java:231)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:251)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:229)
        at org.apache.ratis.server.impl.RaftServerRpcWithProxy.close(RaftServerRpcWithProxy.java:76)
        at org.apache.ratis.server.impl.RaftServerProxy.lambda$close$4(RaftServerProxy.java:318)
        at org.apache.ratis.util.LifeCycle.lambda$checkStateAndClose$2(LifeCycle.java:231)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:251)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:229)
        at org.apache.ratis.server.impl.RaftServerProxy.close(RaftServerProxy.java:313)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.stop(XceiverServerRatis.java:432)
        at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.stop(OzoneContainer.java:201)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.close(DatanodeStateMachine.java:270)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.stopDaemon(DatanodeStateMachine.java:394)
        at org.apache.hadoop.ozone.HddsDatanodeService.stop(HddsDatanodeService.java:449)
        at org.apache.hadoop.ozone.HddsDatanodeService.terminateDatanode(HddsDatanodeService.java:429)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:208)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:349)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.awaitTermination(ServerImpl.java:282)
        at org.apache.ratis.grpc.server.GrpcService.closeImpl(GrpcService.java:158)
        ... 19 more
{code}",MiniOzoneChaosCluster TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2019-05-19 07:34:49,5
13233847,Implement Bucket Write Requests to use Cache and DoubleBuffer,"Implement Bucket write requests to use OM Cache, double buffer.

And also in OM previously we used to Ratis client for communication to Ratis server, instead of that use Ratis server API's.

 

In this Jira will add the changes to implement bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-16 19:33:43,2
13233541,"Support default Acls for volume, bucket, keys and prefix","Add dAcls for volume, bucket, keys and prefix",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:32,4
13233540,"Implement addAcl,removeAcl,setAcl,getAcl  for Prefix","Implement addAcl,removeAcl,setAcl,getAcl  for Prefix",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:18,4
13233539,Create Radix tree to support ozone prefix ACLs ,Create Radix tree to support ozone prefix ACLs.,pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:03,4
13233513,testSCMSafeModeRestrictedOp is failing consistently,"The test is failing with the following stack trace.
{code}
[ERROR] testSCMSafeModeRestrictedOp(org.apache.hadoop.ozone.om.TestScmSafeMode)  Time elapsed: 9.79 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeModeRestrictedOp(TestScmSafeMode.java:304)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-05-15 15:48:35,4
13233378,Disable the sync flag by default during chunk writes in Datanode,"Currently, by default while doing the chunk writes on datanodes, the sync flag is ON by default. This needs to be turned off by default.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-05-15 05:45:38,3
13233220,Publish ozone 0.4.0 to the dockerhub,"As 0.4.0-alpha is released we can publish the latest stable version on dockerhub.

Required steps:

 * Update ozone-latest branch in apache/hadoop-docker-ozone repository.
 * Create a new branch ozone-0.4.0 from the ozone-latest

Dockerhub is configured to create new tags from any branch which starts with ozone-:

branchname: ozone-XXXX --> docker image: apache/ozone:XXXX

Note: this releasing is in sync with ASF policies (See LEGAL-270)

bq. The main Docker Hub at hub.docker.com is a public-facing downstream
distribution channel – similar to Maven Central, PyPI, Debian package
management, etc.

bq. It is appropriate to distribute official releases through downstream channels, but inappropriate to distribute unreleased materials through them. (That's
why having `latest` on hub.docker.com point to git `master` is problematic.)
See Apache's formal Release Policy and Release Distribution Policy documents


This is a downstream distribution of the already voted and release ozone package. Please note that the Dockerfile points to the official download page. 

The ozone version command can show that we have exactly the same, voted and released bits inside: 

{code}
docker run apache/ozone:latest ozone version
                  //////////////                 
               ////////////////////              
            ////////     ////////////////        
           //////      ////////////////          
          /////      ////////////////  /         
         /////            ////////   ///         
         ////           ////////    /////        
        /////         ////////////////           
        /////       ////////////////   //        
         ////     ///////////////   /////        
         /////  ///////////////     ////         
          /////       //////      /////          
           //////   //////       /////           
             ///////////     ////////            
               //////  ////////////              
               ///   //////////                  
              /    0.4.0-alpha(Badlands)

Source code repository https://github.com/apache/hadoop.git -r 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b
Compiled by ajay.kumar on 2019-04-30T03:25Z
Compiled with protoc 2.5.0
From source with checksum 45e58ba9203a1b4470e183bf90281b20

Using HDDS 0.4.0-alpha
Source code repository https://github.com/apache/hadoop.git -r 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b
Compiled by ajay.kumar on 2019-04-30T03:24Z
Compiled with protoc 2.5.0
From source with checksum 57412e0def0317aed91721fb7ef5
{code}

How to test:

1. Single image version

{code}
./build.sh

docker run -p 9878:9878 -p 9876:9876  apache/ozone:latest

ls > /tmp/testfile
aws s3api --endpoint http://localhost:9878/ create-bucket --bucket=bucket1
aws s3 --endpoint http://localhost:9878 cp --storage-class REDUCED_REDUNDANCY /tmp/testfile  s3://bucket1/testfile
{code}

Yes, it's an ozone cluster in one line.

2. pseudo-cluster version

{code}
./build.sh
docker tag apache/ozone:latest  apache/ozone:0.4.0

mkdir /tmp/ozonetest
cd /tmp/ozonetest
#The required config files are inlined!!!
docker run apache/ozone:latest cat docker-config > docker-config
docker run apache/ozone:latest cat docker-config.yaml > docker-config.yaml

docker-compose up -d
docker-compose scale datanode=3

ls > /tmp/testfile
aws s3api --endpoint http://localhost:9878/ create-bucket --bucket=bucket1
aws s3 --endpoint http://localhost:9878 cp /tmp/testfile  s3://bucket1/testfile
{code}",pull-request-available,[],HDDS,Task,Major,2019-05-14 11:44:22,1
13233115,Mapreduce failure when using Hadoop 2.7.5,"Integrate Ozone(0.4 branch) with Hadoop 2.7.5, ""hdfs dfs -ls /"" can pass, while teragen  failed. 

When add  -verbose:class to java options, it shows that class KeyProvider is loaded twice by different classloader while it is only loaded once when execute  ""hdfs dfs -ls /"" 

All jars under share/ozone/lib are added into hadoop classpath except ozone file system current lib jar.
",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Blocker,2019-05-14 02:15:15,1
13233018,Provide intellij runConfiguration for Ozone components,"Sometimes I need to start ozone cluster from intellij to debug issues. It's possible but it requires to create many runConfiguration object inside my IDE.

I propose here to share the intellij specific runtimeConfigs to make it easy for anybody (who uses intellij) to run full ozone cluster from the IDE (1 datanode only).",pull-request-available,[],HDDS,Task,Major,2019-05-13 16:24:42,1
13232697,Use /etc/ozone for configuration inside docker-compose ,"As [~eyang] reported the docker-compose clusters write the config files with uid=1000. In case of the build is created with different user (eg id=401) the hadoop user inside the container (id=100) can't work to the ozone/etc/hadoop directory.

I propose to generate the configuration file to /etc/hadoop (And add that directory to the classpath). In that case the volume mount of the ozone distribution folder can be read only.",pull-request-available,[],HDDS,Sub-task,Major,2019-05-10 15:54:33,1
13232692,AllocateBlock call fails with ContainerNotFoundException,"In allocateContainer call,  the container is first added to pipelineStateMap and then added to container cache. If two allocate blocks execute concurrently, it might happen that one find the container to exist in the pipelineStateMap but the container is yet to be updated in the container cache, hence failing with CONTAINER_NOT_FOUND exception.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-05-10 15:47:44,3
13232530,Implement DoubleBuffer in OzoneManager,"This Jira is created to implement DoubleBuffer in OzoneManager to flush transactions to OM DB.

 
h2. Flushing Transactions to RocksDB:

We propose using an implementation similar to the HDFS EditsDoubleBuffer.  We shall flush RocksDB transactions in batches, instead of current way of using rocksdb.put() after every operation. At a given time only one batch will be outstanding for flush while newer transactions are accumulated in memory to be flushed later.

 

In DoubleBuffer it will have 2 buffers one is currentBuffer, and the other is readyBuffer. We add entry to current buffer, and we check if another flush call is outstanding. If not, we flush to disk Otherwise we add entries to otherBuffer while sync is happening.

 

In this if sync is happening, we shall add new requests to other buffer and when we can sync we use *RocksDB batch commit to sync to disk, instead of rocksdb put.*

 

Note: If flush to disk is failed on any OM, we shall terminate the OzoneManager, so that OM DB’s will not diverge. Flush failure should be considered as catastrophic failure.

 

Scope of this Jira is to add DoubleBuffer implementation, integrating to current OM will be done in further jira's.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-09 20:39:41,2
13232456,Classpath files are deployed to the maven repository as pom/jar files,"1. Classpath files are plain text files which are generatede for each ozone projects. Classpath files are used to defined the classpath of a module (om, scm, etc) based on the maven classpath.

Example classpath file:

{code}
classpath=$HDDS_LIB_JARS_DIR/kerb-simplekdc-1.0.1.jar:$HDDS_LIB_JARS_DIR/hk2-utils-2.5.0.jar:$HDDS_LIB_JARS_DIR/jackson-core-2.9.5.jar:$HDDS_LIB_JARS_DIR/ratis-netty-0.4.0-fe2b15d-SNAPSHOT.jar:$HDDS_LIB_JARS_DIR/protobuf-java-2.5.0.jar:... 
{code}

Classpath files are maven artifacts and copied to share/ozone/classpath in the distribution

2. 0.4.0 was the first release when we deployed the artifacts to the apache nexus. [~ajayydv] reported the problem that the staging repository can't be closed: INFRA-18344

It turned out that the classpath files are uploaded with jar extension to the repository. We deleted all the classpath files manually and the repository became closable.

To avoid similar issues we need to fix this problem and make sure that the classpath files are not uploaded to the repository during a 'mvn deploy' or uploaded but with a good extension.

ps: I don't know the exact solution yet, but I can imagine that bumping the version of maven deploy plugin can help. Seems to be a bug in the plugin.

ps2: This is blocker as we need to fix it before the next release",pull-request-available,['build'],HDDS,Bug,Blocker,2019-05-09 12:00:31,1
13232453,TestBlockOutputStreamWithFailures#test2DatanodesFailure fails intermittently,"The test fails because, the test expects a exception after 2 datanodes failures to be of type RaftRetryFailureException. But it might happen that, the pipeline gets destroyed quickly then actual write executes over Ratis, hence it will fail with GroupMismatchhException in such case.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-05-09 11:51:57,3
13232411,Provide example k8s deployment files for the new CSI server,Issue HDDS-1382 introduced a new internal CSI server. We should provide example deployment files to make it easy to deploy it to any kubernetes cluster.,pull-request-available,[],HDDS,Sub-task,Major,2019-05-09 09:00:10,1
13232175,Add metrics for Ozone Ratis performance,"This jira will add some metrics for Ratis pipeline performance
1) number of bytes written
2) number Read state Machine calls

3) no of Read StateMachine Fails",pull-request-available,[],HDDS,Bug,Major,2019-05-08 08:25:29,3
13232076,Allocate block failures in client should print exception trace.,"The following error is seen intermittently in the Ozone client logs while writing large keys. We need to log the entire exception trace to find out more about the failure.

{code}
19/04/22 10:13:32 ERROR io.KeyOutputStream: Try to allocate more blocks for write failed, already allocated 0 blocks for this write.
{code}",pull-request-available,[],HDDS,Improvement,Major,2019-05-07 17:58:08,5
13232068,OzoneManager Cache,"In this Jira, we shall implement a cache for Table.

As with OM HA, we are planning to implement double buffer implementation to flush transaction in a batch, instead of using rocksdb put() for every operation. When this comes in to place we need cache in OzoneManager HA to handle/server the requests for validation/returning responses.

 

This Jira will implement Cache as an integral part of the table. In this way users using this table does not need to handle like check cache/db. For this, we can update get API in the table to handle the cache.

 

This Jira will implement:
 # Cache as a part of each Table.
 # Uses this cache in get().
 # Exposes api for cleanup, add entries to cache.

Usage to add the entries in to cache will be done in further jira's.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-07 17:31:14,2
13231920,Support partial chunk reads and checksum verification,"BlockInputStream#readChunkFromContainer() reads the whole chunk from disk even if we need to read only a part of the chunk.
This Jira aims to improve readChunkFromContainer so that only that part of the chunk file is read which is needed by client plus the part of chunk file which is required to verify the checksum.



For example, lets say the client is reading from index 120 to 450 in the chunk. And let's say checksum is stored for every 100 bytes in the chunk i.e. the first checksum is for bytes from index 0 to 99, the next for bytes from index 100 to 199 and so on. To verify bytes from 120 to 450, we would need to read from bytes 100 to 499 so that checksum verification can be done.",pull-request-available,[],HDDS,Improvement,Major,2019-05-07 04:04:42,7
13231824,Generated chunk size name too long.,"Following exception is seen in SCM logs intermittently. 

{code}
java.lang.RuntimeException: file name 'chunks/2a54b2a153f4a9c5da5f44e2c6f97c60_stream_9c6ac565-e2d4-469c-bd5c-47922a35e798_chunk_10.tmp.2.23115' is too long ( > 100 bytes)
{code}

We may have to limit the name of the chunk to 100 bytes.",pull-request-available,[],HDDS,Bug,Critical,2019-05-06 16:56:28,3
13231680,Ozone KeyInputStream seek() should not read the chunk file,"KeyInputStream#seek() calls BlockInputStream#seek() to adjust the buffer position to the seeked position. As part of the seek operation, the whole chunk is read from the container and stored in the buffer so that the buffer position can be advanced to the seeked position. 

We should not read from disk on a seek() operation. Instead, for a read operation, when the chunk file is read and put in the buffer, at that time, we can advance the buffer position to the previously seeked position.",pull-request-available,[],HDDS,Bug,Major,2019-05-05 22:08:18,7
13231347,Bootstrap React framework for Recon UI,"Bootstrap React with Typescript, Ant, LESS and other necessary libraries for Recon UI. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-05-02 20:09:02,6
13231110,Use strongly typed codec implementations for the S3Table,"HDDS-864 added the implementation for Strongly typed codec implementation for the tables of OmMetadataManager.

 

Tables which are added as part of S3 Implementation are not using this. This Jira is address to this.",pull-request-available,[],HDDS,Improvement,Major,2019-05-01 17:53:11,2
13230785,Update S3.md documentation,"HDDS-791 implemented range get operation.

 

S3.md documentation has below line: 

GET Object | implemented | Range headers are not supported

 

This should be updated to remove the part `Range headers are not supported`.",newbie pull-request-available,[],HDDS,Bug,Major,2019-04-29 22:59:24,6
13230702,Provide k8s resources files for prometheus and performance tests,"Similar to HDDS-1412 we can further improve the available k8s resources with providing example resources to:

1) install prometheus
2) execute freon test and check the results.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-29 13:45:43,1
13230404,Fix OzoneContainer start method,"In OzoneContainer start() we have 
{code:java}
startContainerScrub();
writeChannel.start();
readChannel.start();
hddsDispatcher.init();
hddsDispatcher.setScmId(scmId);{code}
 

Suppose here if readChannel.start() failed due to some reason, from VersionEndPointTask, we try to start OzoneContainer again. This can cause an issue for writeChannel.start() if it is already started. 

 

Fix the logic such a way that if service is started, don't attempt to start the service again. Similar changes needed to be done for stop().",newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-26 22:56:11,5
13230403,"""ozone.scm.datanode.id"" config should take path for a dir and not a file","Currently, the ozone config ""ozone.scm.datanode.id"" takes file path as its value. It should instead take dir path as its value and assume a standard filename ""datanode.id""",newbie pull-request-available,['Ozone Datanode'],HDDS,Task,Minor,2019-04-26 22:53:17,6
13230095,Generate default configuration fragments based on annotations,"See the design doc in the parent jira for more details.

In this jira I introduce a new annotation processor which can generate ozone-default.xml fragments based on the annotations which are introduced by HDDS-1468.

The ozone-default-generated.xml fragments can be used directly by the OzoneConfiguration as I added a small code to the constructor to check ALL the available ozone-default-generated.xml files and add them to the available resources.

With this approach we don't need to edit ozone-default.xml as all the configuration can be defined in java code.

As a side effect each service will see only the available configuration keys and values based on the classpath. (If the ozone-default-generated.xml file of OzoneManager is not on the classpath of the SCM, SCM doesn't see the available configs.) 

",pull-request-available,[],HDDS,Sub-task,Major,2019-04-25 13:26:53,1
13230062,Inject configuration values to Java objects,"According to the design doc in the parent issue we would like to support java configuration objects which are simple POJO but the fields/setters are annotated. As a first step we can introduce the OzoneConfiguration.getConfigObject() api which can create the config object and inject configuration.

Later we can improve it with annotation processor which can generate the ozone-default.xml.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-25 09:24:22,1
13229902,Fix content and format of Ozone documentation,"During the review of HDDS-1457 I realized that the current documentation contains many outdated information regarding the usage of docker, build commands or s3 usage.

The security information is also rendered in an incorrect way.

The png files for the prometheus page are missing (were included in the patch of HDDS-846 but missing from the commit).",pull-request-available,['documentation'],HDDS,Bug,Blocker,2019-04-24 15:17:44,1
13229680,"Stop the datanode, when any datanode statemachine state is set to shutdown","Recently we have seen an issue, in InitDatanodeState, there is error during create Path for volume. We set the state to shutdown and this has caused DatanodeStateMachine to stop, but datanode is still running. In this case we should stop Datanode, otherwise, user will know about this when running ozone commands or when user observed metrics like healthy nodes.

 

cc [~vivekratnavel]",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-23 17:51:01,2
13229660,Inconsistent naming convention with Ozone Kerberos configuration,"In SetupSecureOzone.md, the naming convention for keytab files are different from code.

{code}
hdds.scm.http.kerberos.keytab
ozone.om.http.kerberos.keytab
{code}

In ozone-default.xml, it is looking for:

{code}
hdds.scm.http.kerberos.keytab
ozone.om.http.kerberos.keytab.file
{code}

For the non http version of keytab, they are branded as:

{code}
hdds.scm.kerberos.keytab.file
ozone.om.kerberos.keytab.file
{code}

It is best to shorten the name to remove .file suffix from the code to be consistent with Hadoop naming convention.  The second nitpick is hdds and ozone prefix.  Is there a good reason to have distinct prefix for both that work closely together?  How about hadoop.ozone prefix?  From usability point of view, the current prefix are very confusing.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-23 16:21:56,4
13229410,SCMBlockManager findPipeline and createPipeline are not lock protected,"SCM BlockManager may try to allocate pipelines in the cases when it is not needed. This happens because BlockManagerImpl#allocateBlock is not lock protected, so multiple pipelines can be allocated from it. One of the pipeline allocation can fail even when one of the existing pipeline already exists.


{code}
2019-04-22 22:34:14,336 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 6f4bb2d7-d660-4f9f-bc06-72b10f9a738e, Nodes: 76e1a493-fd55-4d67-9f5
5-c04fd6bd3a33{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}2b9850b2-aed3-4a40-91b5-2447dc5246bf{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}12248721-ea6a-453f-8dad-fc7fbe692f
d2{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,386 INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - e17b7852-4691-40c7-8791-ad0b0da5201f: shutdown LeaderElection
2019-04-22 22:34:14,388 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 552e28f3-98d9-41f3-86e0-c1b9494838a5, Nodes: e17b7852-4691-40c7-879
1-ad0b0da5201f{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}fd365bac-e26e-4b11-afd8-9d08cd1b0521{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}9583a007-7f02-4074-9e26-19bc18e29e
c5{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,388 INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e17b7852-4691-40c7-8791-ad0b0da5201f: start FollowerState
2019-04-22 22:34:14,388 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 5383151b-d625-4362-a7dd-c0d353acaf76, Nodes: 80f16ad6-3879-4a64-a3c
7-7719813cc139{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}082ce481-7fb0-4f88-ac21-82609290a6a2{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}dd5f5a70-0217-4577-b7a2-c42aa139d1
8a{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,389 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: be4854e5-7933-4caa-b32e-f482cf500247, Nodes: 6e2356f1-479d-498b-876
a-1c90623c498b{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}8ac46d94-9975-4eea-9448-2618c69d7bf3{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}a3ed36a1-44ca-47b2-b9b3-5aeef04595
18{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,390 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 21e368e2-f82a-4c61-9cc3-06e8de22ea6b, Nodes: 82632040-5754-4122-b187-331879586842{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}923c8537-b869-4085-adcb-0a9accdcd089{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}c6d790bf-e3a6-4064-acb5-f74796cd38a9{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,390 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: cccbc2ed-e0e2-4578-a8a2-94f4b645be52, Nodes: 91ae6848-a778-43be-a4a1-5855f7adc0d8{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}8f330a03-40e2-4bd1-9b43-5e05b13d89f0{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}4f3070dc-650b-48d7-87b5-d2076104e7b4{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,392 ERROR block.BlockManagerImpl (BlockManagerImpl.java:allocateBlock(192)) - Pipeline creation failed for type:RATIS factor:THREE
org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 2 nodes 20 healthy nodes 20 all nodes.
        at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:122)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:57)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:148)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:190)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:7533)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
2019-04-22 22:34:14,395 ERROR block.BlockManagerImpl (BlockManagerImpl.java:allocateBlock(213)) - Unable to allocate a block for the size: 16384, type: RATIS, factor: THREE
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-22 17:24:48,5
13229398,Fix nightly run failures after HDDS-976,[https://ci.anzix.net/job/ozone-nightly/72/testReport/],pull-request-available,[],HDDS,Bug,Minor,2019-04-22 16:11:23,4
13229384,RatisPipelineProvider should only consider open pipeline while excluding dn for pipeline allocation,"While allocation pipelines, Ratis pipeline provider considers all the pipelines irrespective of the state of the pipeline. This can lead to case where all the datanodes are up but the pipelines are in closing state in SCM.",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-22 14:20:33,5
13228497,Add handling of NotReplicatedException in OzoneClient,"In MiniOzoneChaosCluster some of the calls fail with NotReplicatedException. This Exception needs to be handled in OzoneClient

{code}
2019-04-17 10:13:47,254 INFO  client.GrpcClientProtocolService (GrpcClientProtocolService.java:lambda$processClientRequest$0(264)) - Failed RaftClientRequest:client-43B95E0E3BE0->1ebec547-8cf8-4466-bf43-ea9f19fb546b@group-1B28E0BF6CBC, cid=800, seq=0, Watch-ALL_COMMITTED(234), Message:<EMPTY>, reply=RaftClientReply:client-43B95E0E3BE0->1ebec547-8cf8-4466-bf43-ea9f19fb546b@group-1B28E0BF6CBC, cid=800, FAILED org.apache.ratis.protocol.NotReplicatedException: Request with call Id 800 and log index 234 is not yet replicated to ALL_COMMITTED, logIndex=234, commits[1ebec547-8cf8-4466-bf43-ea9f19fb546b:c267, 7b200ef5-7711-437d-a9bc-ad0e18fdf6bb:c267, ffbfb65f-a622-466d-b6e8-47038cc15e0b:c226]
{code}",MiniOzoneChaosCluster,[],HDDS,Bug,Major,2019-04-17 05:00:39,3
13227855,TestCommitWatcher#testReleaseBuffersOnException fails with IllegalStateException,"the test is failing with the following exception

{code}
java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
	at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:191)
	at org.apache.hadoop.ozone.client.rpc.TestCommitWatcher.testReleaseBuffersOnException(TestCommitWatcher.java:277)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}


https://ci.anzix.net/job/ozone-nightly/63/testReport/org.apache.hadoop.ozone.client.rpc/TestCommitWatcher/testReleaseBuffersOnException/
",ozone-flaky-test,['Ozone Client'],HDDS,Bug,Major,2019-04-13 08:51:20,3
13227662,TestDatanodeStateMachine is flaky,"TestDatanodeStateMachine is flaky.
 It has failed in the following build
 [https://builds.apache.org/job/PreCommit-HDDS-Build/2650/artifact/out/patch-unit-hadoop-hdds.txt]
 [https://builds.apache.org/job/hadoop-multibranch/job/PR-661/6/artifact/out/patch-unit-hadoop-hdds_container-service.txt]
 [https://builds.apache.org/job/PreCommit-HDDS-Build/2635/artifact/out/patch-unit-hadoop-hdds.txt]

Stack trace:
{noformat}
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)
	at org.apache.hadoop.ozone.container.common.TestDatanodeStateMachine.testStartStopDatanodeStateMachine(TestDatanodeStateMachine.java:166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   TestDatanodeStateMachine.testStartStopDatanodeStateMachine:166 ? Timeout Timed...
{noformat}",ozone-flaky-test pull-request-available,['test'],HDDS,Bug,Major,2019-04-12 09:11:00,0
13227467,Ozone compose files are not compatible with the latest docker-compose,"I upgraded my docker-compose to the latest available one (1.24.0)

But after the upgrade I can't start the docker-compose based cluster any more:

{code}
./test.sh 
-------------------------------------------------
Executing test(s): [basic]

  Cluster type:      ozone
  Compose file:      /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/smoketest/../compose/ozone/docker-compose.yaml
  Output dir:        /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/smoketest/result
  Command to rerun:  ./test.sh --keep --env ozone basic
-------------------------------------------------
ERROR: In file /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozone/docker-config: environment variable name 'LOG4J2.PROPERTIES_appender.rolling.file 
{code}

It turned out that the line of LOG4J2.PROPERTIES_appender.rolling.file contains an unnecessary space which is not accepted by the latest docker-compose any more.",pull-request-available,[],HDDS,Bug,Blocker,2019-04-11 14:05:16,1
13227465,Support multi-container robot test execution,"The ./smoketest folder in the distribution package contains robotframework based test scripts to test the main behaviour of Ozone.

The tests have two layers:

1. robot test definitions to execute commands and assert the results (on a given host machine)
2. ./smoketest/test.sh which starts/stops the docker-compose based environments AND execute the selected robot tests inside the right hosts

The second one (test.sh) has some serious limitations:

1. all the tests are executed inside the same container (om):

https://github.com/apache/hadoop/blob/5f951ea2e39ae4dfe554942baeec05849cd7d3c2/hadoop-ozone/dist/src/main/smoketest/test.sh#L89

Some of the tests (ozonesecure-mr, ozonefs) may require the flexibility to execute different robot tests in different containers.

2. The definition of the global test set is complex and hard to understood. 

The current code is:
{code}
   TESTS=(""basic"")
   execute_tests ozone ""${TESTS[@]}""
   TESTS=(""auditparser"")
   execute_tests ozone ""${TESTS[@]}""
   TESTS=(""ozonefs"")
   execute_tests ozonefs ""${TESTS[@]}""
   TESTS=(""basic"")
   execute_tests ozone-hdfs ""${TESTS[@]}""
   TESTS=(""s3"")
   execute_tests ozones3 ""${TESTS[@]}""
   TESTS=(""security"")
   execute_tests ozonesecure .
{code} 

For example for ozonesecure the TESTS is not used. And the usage of bash lists require additional complexity in the execute_tests function.

I propose here a very lightweight refactor. Instead of including both the test definitions AND the helper methods in test.sh I would separate them.

Let's put a test.sh to each of the compose directories. The separated test.sh can include common methods from a main shell script. For example:

{code}

source ""$COMPOSE_DIR/../testlib.sh""

start_docker_env

execute_robot_test scm basic/basic.robot

execute_robot_test scm s3

stop_docker_env

generate_report

{code}

This is a more clean and more flexible definition. It's easy to execute just this test (as it's saved to the compose/ozones3 directory. And it's more flexible.

Other example, where multiple containers are used to execute tests:

{code}

source ""$COMPOSE_DIR/../testlib.sh""

start_docker_env

execute_robot_test scm ozonefs/ozonefs.robot



export OZONE_HOME=/opt/ozone

execute_robot_test hadoop32 ozonefs/hadoopo3fs.robot

execute_robot_test hadoop31 ozonefs/hadoopo3fs.robot

stop_docker_env

generate_report
{code}

With this separation the definition of the helper methods (eg. execute_robot_test or stop_docker_env) would also be simplified.",pull-request-available,['test'],HDDS,Bug,Major,2019-04-11 13:50:14,1
13227308,Fix shellcheck errors in start-chaos.sh,"Fix the following shellcheck errors in start-chaos.sh:
{code}
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:18:6: note: Use $(..) instead of legacy `..`. [SC2006]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:27:19: note: Double quote to prevent globbing and word splitting. [SC2086]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:28:20: note: Double quote to prevent globbing and word splitting. [SC2086]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:31:33: note: Double quote to prevent globbing and word splitting. [SC2086]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:35:23: note: Double quote to prevent globbing and word splitting. [SC2086]
{code}",newbie pull-request-available,['test'],HDDS,Bug,Major,2019-04-10 19:58:46,0
13227261,TestOzoneClientRetriesOnException.testGroupMismatchExceptionHandling is flaky,[https://ci.anzix.net/job/ozone/16618/testReport/org.apache.hadoop.ozone.client.rpc/TestOzoneClientRetriesOnException/testGroupMismatchExceptionHandling/],TriagePending ozone-flaky-test,['test'],HDDS,Bug,Major,2019-04-10 17:23:23,3
13227230,TestOzoneManagerHA.testMultipartUploadWithOneOmNodeDown is flaky,"TestOzoneManagerHA.testMultipartUploadWithOneOmNodeDown is flaky, we get the below exception when it fails.

{code}
org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client client-04649B8D5AF3->RAFT is closed.
 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommand(OzoneManagerRatisClient.java:133)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:83)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
Caused by: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client client-04649B8D5AF3->RAFT is closed.
 at org.apache.ratis.util.SlidingWindow$Client.alreadyClosed(SlidingWindow.java:350)
 at org.apache.ratis.util.SlidingWindow$Client.submitNewRequest(SlidingWindow.java:224)
 at org.apache.ratis.client.impl.RaftClientImpl.sendAsync(RaftClientImpl.java:207)
 at org.apache.ratis.client.impl.RaftClientImpl.sendAsync(RaftClientImpl.java:174)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendRequestAsync(OzoneManagerRatisClient.java:208)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommandAsync(OzoneManagerRatisClient.java:168)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommand(OzoneManagerRatisClient.java:132)
 ... 11 more
Caused by: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-04649B8D5AF3->omNode-1@group-523986131536, cid=71, seq=1*, RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$396/1529424209@7ae5da75 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms)
 at org.apache.ratis.client.impl.RaftClientImpl.newRaftRetryFailureException(RaftClientImpl.java:383)
 at org.apache.ratis.client.impl.RaftClientImpl.handleAsyncRetryFailure(RaftClientImpl.java:388)
 at org.apache.ratis.client.impl.RaftClientImpl.lambda$sendRequestAsync$14(RaftClientImpl.java:370)
 at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
 at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.completeReplyExceptionally(GrpcClientProtocolClient.java:329)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.access$000(GrpcClientProtocolClient.java:245)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:257)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:248)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
 at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)
 at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
 at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
{code}",TriagePending ozone-flaky-test,['test'],HDDS,Bug,Major,2019-04-10 16:27:19,7
13227228,TestCloseContainerCommandHandler is flaky,"TestCloseContainerCommandHandler.testCloseContainerViaStandalone is flaky, we get the below exception when it fails.

{code}
org.apache.ratis.protocol.NotLeaderException: Server a200dff7-f26d-4be3-addd-e8e0ca569ae0 is not the leader (null). Request must be sent to leader.
	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:448)
	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:419)
	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:514)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$7(RaftServerProxy.java:333)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$5(RaftServerProxy.java:328)
	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:109)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$6(RaftServerProxy.java:328)
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)
	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:327)
	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:333)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.submitRequest(XceiverServerRatis.java:485)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.TestCloseContainerCommandHandler.createContainer(TestCloseContainerCommandHandler.java:310)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.TestCloseContainerCommandHandler.testCloseContainerViaStandalone(TestCloseContainerCommandHandler.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

Full log is uploaded.",ozone-flaky-test pull-request-available,[],HDDS,Bug,Major,2019-04-10 16:22:23,0
13227222,Provide example k8s deployment files as part of the release package,"In HDDS-872 we added Dockerfile and skaffold definition to run dev builds on kubernetes. But it would be great to include example k8s resource definitions helping the deployment of ozone to any kubernetes cluster.

In this patch I will

1. Add k8s resources files to the release tar file to deploy basic ozone cluster
2. Add Dockerfile to the release tar file to create custom ozone image any time
3. Add additional maven profiles to build and push development docker images.
4. We don't need skaffold any more as the maven based approach is more flexible (we can support multiple k8s definitions)

To easily support multiple type of configuration (simple ozone, minikube, csi) we need a basic set of k8s resources files and additional transformations to generate the ready-to-use files for each specific use-cases.

The easiest way to do this is adopting the existing structure from https://github.com/flokkr/k8s and use https://github.com/elek/flekszible tool. But the tool itself is not required at runtime as we generate all the required k8s resources files during the development and add the results to the version control. ",pull-request-available,[],HDDS,Sub-task,Major,2019-04-10 16:04:40,1
13227028,Avoid usage of commonPool in RatisPipelineUtils,"We use parallelStream in during createPipline, this internally uses commonPool. Use Our own ForkJoinPool with parallelisim set with number of processors.",pull-request-available,[],HDDS,Improvement,Major,2019-04-09 19:38:00,2
13226761,KeyOutputStream writes fails after max retries while writing to a closed container,Currently a Ozone Client retries a write operation 5 times. It is possible that the container being written to is already closed by the time it is written to. The key write will fail after retrying multiple times with this error. This needs to be fixed as this is an internal error.,MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-08 18:22:26,7
13226746,Remove unused ScmBlockLocationProtocol from ObjectStoreHandler,"When I analyzed the usages of the available RPC protocols in Ozone I found that the ScmBlockLocationProtocol is not used in ObjectStore at all.

I would propose to remove it...",pull-request-available,[],HDDS,Improvement,Major,2019-04-08 16:27:50,1
13226462,Convert all OM Key related operations to HA model,"In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like volume not found or some conditions which  are not satisfied like when Key not found during rename, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-05 22:29:54,2
13226433,Concurrency issue in SCMConnectionManager#getValues,"{{testStartStopDatanodeStateMachine}} is flaky, causing [occasional pre-commit build failures|https://builds.apache.org/job/hadoop-multibranch/job/PR-691/1/artifact/out/patch-unit-hadoop-hdds_container-service.txt].  HDDS-1332 added some logging to find out more about the cause.

I think the problem is not test-specific, and is caused by the following: {{SCMConnectionManager#scmMachines}} is a plain {{HashMap}}, guarded by a {{ReadWriteLock}} in most places where it's used, except {{getValues()}}.  The method also returns the values collection without any write protection (though currently none of the callers modify it).",pull-request-available,[],HDDS,Bug,Major,2019-04-05 20:16:47,0
13226328,Avoid the usage of signal handlers in datanodes of the MiniOzoneClusters,"[~arpaga] showed me a problem that TestQueryNode.testHealthyNodesCount is failed in the CI check of HDDS-1339.

According to the logs the test is timed out because only 4 datanodes are started out of the 5.

The log also contained an exception from one datanode:

{code}
2019-04-04 00:26:33,583 WARN  ozone.HddsDatanodeService (LogAdapter.java:warn(59)) - failed to register any UNIX signal loggers: 
java.lang.IllegalStateException: Can't re-install the signal handlers.
    at org.apache.hadoop.util.SignalLogger.register(SignalLogger.java:77)
    at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:718)
    at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:707)
    at org.apache.hadoop.ozone.HddsDatanodeService.createHddsDatanodeService(HddsDatanodeService.java:126)
    at org.apache.hadoop.ozone.HddsDatanodeService.createHddsDatanodeService(HddsDatanodeService.java:108)
    at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.createHddsDatanodes(MiniOzoneClusterImpl.java:552)
{code}

The code which requires the signal handler is the following (signal handler is registered in the startupShutdownMessage)

{code}
  /**
   * Create an Datanode instance based on the supplied command-line arguments.
   * <p>
   * This method is intended for unit tests only. It suppresses the
   * startup/shutdown message and skips registering Unix signal handlers.
   *
   * @param args        command line arguments.
   * @param conf        HDDS configuration
   * @param printBanner if true, then log a verbose startup message.
   * @return Datanode instance
   */
  private static HddsDatanodeService createHddsDatanodeService(
      String[] args, Configuration conf, boolean printBanner) {
    if (args.length == 0 && printBanner) {
      StringUtils
          .startupShutdownMessage(HddsDatanodeService.class, args, LOG);
      return new HddsDatanodeService(conf);
    } else {
      new HddsDatanodeService().run(args);
      return null;
   }
{code}

As you can read from the comment it's expected to be called with printBanner=false to avoid the creation of the signal handler. 

Note: In the startupShutdownMessage method a new signal handler is registered and signal handlers can be registered only once:

{code}
//SignalLogger
  void register(final LogAdapter LOG) {
   if (registered) {
      throw new IllegalStateException(""Can't re-install the signal handlers."");
    }
 ....
{code}

We have a dedicated method to create datanode service for the unit tests. The only thing what we need is to turn OFF the signal handler registration here. (The following code fragment shows the original state where the signal handler creation is requested with the true parameter value)

{code}
  @VisibleForTesting
  public static HddsDatanodeService createHddsDatanodeService(
      String[] args, Configuration conf) {
    return createHddsDatanodeService(args, conf, true);
  }
{code}",pull-request-available,[],HDDS,Bug,Major,2019-04-05 09:13:41,1
13226312,Recon start fails due to changes in Aggregate Schema definition (HDDS-1189).,Recon Server start fails due to ClassNotFound exception when looking for org.apache.hadoop.ozone.recon.ReconServer. ,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-04-05 07:34:42,5
13226304,Key write fails with BlockOutputStream has been closed exception,"Key write fails with BlockOutputStream has been closed

{code}
2019-04-05 11:24:47,770 ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(102)) - LOADGEN: Create key:pool-431-thread-9-2092651262 failed with exception, but skipping
java.io.IOException: BlockOutputStream has been closed.
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.checkOpen(BlockOutputStream.java:662)
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:245)
        at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:131)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:325)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:287)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
        at java.io.OutputStream.write(OutputStream.java:75)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:100)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:143)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-04-05 06:15:51,3
13226268,Convert all OM Bucket related operations to HA model,"In this jira, we shall convert all OM Bucket related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like bucket not found or some conditions which i have not satisfied like when deleting bucket should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-04 21:53:59,2
13226212,Add ability in OM to serve delta updates through an API.,"Added an RPC end point to serve the set of updates in OM RocksDB from a given sequence number.
This will be used by Recon (HDDS-1105) to push the data to all the tasks that will keep their aggregate data up to date. ",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-04-04 18:06:46,5
13226206,Fix OzoneS3 Gateway server due to exclusion of hk2-api,Some hk2 transitive dependencies were mistakenly excluded in HDDS-1358 to solve maven enforcer plugin issues. This jira cleans that up. ,pull-request-available,['S3'],HDDS,Bug,Major,2019-04-04 17:43:06,5
13226202,ConcurrentModificationException in TestMiniChaosOzoneCluster,"TestMiniChaosOzoneCluster is failing with the below exception
{noformat}
[ERROR] org.apache.hadoop.ozone.TestMiniChaosOzoneCluster  Time elapsed: 265.679 s  <<< ERROR!
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)
	at java.util.ArrayList$Itr.next(ArrayList.java:859)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.stop(MiniOzoneClusterImpl.java:350)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:325)
	at org.apache.hadoop.ozone.MiniOzoneChaosCluster.shutdown(MiniOzoneChaosCluster.java:130)
	at org.apache.hadoop.ozone.TestMiniChaosOzoneCluster.shutdown(TestMiniChaosOzoneCluster.java:92)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",ozone-flaky-test pull-request-available,['test'],HDDS,Bug,Major,2019-04-04 17:35:32,1
13226144,Make the ozonesecure-mr environment definition version independent,"The MapReduce example project on branch ozone-0.4 contains 0.5.0-SNAPSHOT references in the dir:

hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozonesecure-mr

After HDDS-1333 (which introduce filtering) it will be straightforward to always use the current version.",pull-request-available,[],HDDS,Bug,Major,2019-04-04 14:55:54,1
13226129,TestBlockOutputStreamWithFailures is failing,"TestBlockOutputStreamWithFailures is failing with the following error

{noformat}
2019-04-04 18:52:43,240 INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(140)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@1f6c0e8a
2019-04-04 18:52:43,240 INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(203)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@1f6c0e8a
2019-04-04 18:52:43,241 ERROR server.GrpcService (ExitUtils.java:terminate(133)) - Terminating with exit status 1: Failed to start Grpc server
java.io.IOException: Failed to bind
  at org.apache.ratis.thirdparty.io.grpc.netty.NettyServer.start(NettyServer.java:253)
  at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:166)
  at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:81)
  at org.apache.ratis.grpc.server.GrpcService.startImpl(GrpcService.java:144)
  at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)
  at org.apache.ratis.server.impl.RaftServerRpcWithProxy.start(RaftServerRpcWithProxy.java:69)
  at org.apache.ratis.server.impl.RaftServerProxy.lambda$start$3(RaftServerProxy.java:300)
  at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)
  at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:298)
  at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:419)
  at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:186)
  at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:169)
  at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:338)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.BindException: Address already in use
  at sun.nio.ch.Net.bind0(Native Method)
  at sun.nio.ch.Net.bind(Net.java:433)
  at sun.nio.ch.Net.bind(Net.java:425)
  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
  at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:130)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
  at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1358)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
  at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:1019)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
  at org.apache.ratis.thirdparty.io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:366)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)
  at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
  ... 1 more
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-04-04 13:41:26,1
13226118,[Ozone Upgrade] Create the project skeleton with CLI interface,"Ozone In-Place upgrade tool is a tool to upgrade hdfs data to ozone data without data movement.

In this jira I will create a skeleton project with the cli interface without any business logic.",pull-request-available,['upgrade'],HDDS,Sub-task,Major,2019-04-04 13:13:09,1
13226110,Create customized CSI server for Ozone,"CSI (Container Storage Interface) is a vendor neutral storage interface specification for container orchestrators. CSI support is implemented in YARN, Kubernetes and Mesos. Implementing a CSI server makes it easy to mount disk volumes for containers.

See https://github.com/container-storage-interface/spec for more details about the spec.

Until now we used https://github.com/CTrox/csi-s3 server to support CSI specification. Using an ozone specific CSI server would have the following advantages:

 * We can provide additional functionalities (as we have access to the internal Ozone API not just the very generic s3 api).
 * Security setup can be synchronized.
 * Increased stability
 * Simplified deployment (only the minimal set of the components are required to be installed)

The CSI specification itself is very simple (https://github.com/container-storage-interface/spec/blob/master/csi.proto) at least the part which is required for Ozone.

We can use various fuse s3 driver to mount the ozone buckets via s3.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-04 12:39:45,1
13225883,Convert all OM Volume related operations to HA model,"In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like volume not found or some conditions which i have not satisfied like when deleting volume should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-03 18:26:48,2
13225852,OM failed to start with incorrect hostname set as ip address in CSR,"OM failed to start after HDDS-1355.

{code}

om_1 | 2019-04-03 16:23:50 ERROR OzoneManager:865 - Failed to start the OzoneManager.
om_1 | java.lang.IllegalArgumentException: IP Address is invalid
om_1 | at org.bouncycastle.asn1.x509.GeneralName.<init>(Unknown Source)
om_1 | at org.apache.hadoop.hdds.security.x509.certificates.utils.CertificateSignRequest$Builder.addAltName(CertificateSignRequest.java:205)
om_1 | at org.apache.hadoop.hdds.security.x509.certificates.utils.CertificateSignRequest$Builder.addIpAddress(CertificateSignRequest.java:197)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.getSCMSignedCert(OzoneManager.java:1387)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1018)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:971)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:928)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)

{code}",pull-request-available,[],HDDS,Bug,Blocker,2019-04-03 16:31:32,4
13225784,Datanode exits while executing client command when scmId is null,"Ozone Datanode exits with the following error, this happens because DN hasn't received a scmID from the SCM after registration but is processing a client command.

{code}
2019-04-03 17:02:10,958 ERROR storage.RaftLogWorker (ExitUtils.java:terminate(133)) - Terminating with exit status 1: df6b578e-8d35-44f5-9b21-db7184dcc54e-RaftLogWorker failed.
java.io.IOException: java.lang.NullPointerException: scmId cannot be null
        at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
        at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
        at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:83)
        at org.apache.ratis.server.storage.RaftLogWorker$StateMachineDataPolicy.getFromFuture(RaftLogWorker.java:76)
        at org.apache.ratis.server.storage.RaftLogWorker$WriteLog.execute(RaftLogWorker.java:354)
        at org.apache.ratis.server.storage.RaftLogWorker.run(RaftLogWorker.java:219)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException: scmId cannot be null
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:110)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:243)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:165)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:350)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:224)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:149)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$0(ContainerStateMachine.java:385)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run$$$capture(CompletableFuture.java:1590)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-03 11:36:21,7
13225726,ContainerStateMap cannot find container while allocating blocks.,"ContainerStateMap cannot find container while allocating blocks.

{code}
org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #14
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:542)
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.getContainerInfo(ContainerStateMap.java:189)
        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.getContainer(ContainerStateManager.java:483)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainer(SCMContainerManager.java:195)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:466)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:387)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:201)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:7533)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-03 07:54:31,2
13225623,"KeyOutputStream, close after write request fails after retries, runs into IllegalArgumentException","In this code, the stream is closed via try with resource.

{code}
      try (OzoneOutputStream stream = ozoneBucket.createKey(keyName,
          bufferCapacity, ReplicationType.RATIS, ReplicationFactor.THREE,
          new HashMap<>())) {
        stream.write(buffer.array());
      } catch (Exception e) {
        LOG.error(""LOADGEN: Create key:{} failed with exception"", keyName, e);
        break;
      }
{code}

Here, the write call fails correctly as expected, However the close doesn't fail with the same exception.

The exception stack stack is as following

{code}
2019-04-03 00:52:54,116 ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(101)) - LOADGEN: Create key:pool-431-thread-9-81262222 failed with exception
java.io.IOException: Retry request failed. retries get failed due to exceeded maximum allowed retries number: 5
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:492)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:287)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
        at java.io.OutputStream.write(OutputStream.java:75)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:99)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:137)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.lang.IllegalArgumentException
                at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)
                at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:643)
                at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
                at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:100)
                ... 5 more
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-04-02 20:03:07,3
13225610,Download RocksDB checkpoint from OM Leader to Follower,"If a follower OM is lagging way behind the leader OM or in case of a restart or bootstrapping, a follower OM might need RocksDB checkpoint from the leader to catch up with it. This is because the leader might have purged its logs after taking a snapshot.
 This Jira aims to add support to download a RocksDB checkpoint from leader OM to follower OM through a HTTP servlet. We reuse the DBCheckpoint servlet used by Recon server. ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-02 19:00:19,7
13225602,Command Execution in Datanode fails becaue of NPE,"The command execution on the datanode is failing with the following exception.

{code}
2019-04-02 23:56:30,434 ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(196)) - Unable to finish the execution.
java.lang.NullPointerException
        at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:179)
        at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.execute(RunningDatanodeState.java:89)
        at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:354)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:183)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:338)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-02 18:31:34,2
13225234,ozone.metadata.dirs doesn't pick multiple dirs,"{{ozone.metadata.dirs}} doesn't pick comma(,) separated paths.
 It only picks one path as opposed to the property name _ozone.metadata.dir{color:#FF0000}s{color}_
{code:java}
   <property>
      <name>ozone.metadata.dirs</name>
      <value>/data/data1/meta,/home/hdfs/data/meta</value>
   </property>
{code}
{code:java}
2019-03-31 18:44:54,824 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
SCM initialization succeeded.Current cluster id for sd=/data/data1/meta,/home/hdfs/data/meta/scm;cid=CID-1ad502d1-0104-4055-838b-1208ab78f35c
2019-03-31 18:44:55,079 INFO server.StorageContainerManager: SHUTDOWN_MSG:
{code}
{code:java}
[hdfs@localhost ozone-0.5.0-SNAPSHOT]$ ls //data/data1/meta,/home/hdfs/data/meta/scm/current/VERSION
VERSION
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-01 10:26:44,0
13225219,Invalid metric type due to fully qualified class name,"Prometheus reports the following error ({{ozoneperf}} compose dir):

{code}
prometheus_1 | ... msg=""append failed"" err=""invalid metric type \""apache.hadoop.hdds.scm.server._scm_container_metrics_deleted_containers gauge\""""
{code}

Metric name cannot contain dots.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-01 09:40:28,0
13224878,Recon Server REST API not working as expected.,Guice Jetty integration that is being used for Recon Server API layer is not working as expected. Fixing that in this JIRA.,pull-request-available,[],HDDS,Sub-task,Critical,2019-03-29 17:05:07,5
13224849,ozone s3 shell command has confusing subcommands,"Let's check the potential subcommands of ozone sh:

{code}
[hadoop@om-0 keytabs]$ ozone sh
Incomplete command
Usage: ozone sh [-hV] [--verbose] [-D=<String=String>]... [COMMAND]
Shell for Ozone object store
      --verbose   More verbose output. Show the stack trace of the errors.
  -D, --set=<String=String>

  -h, --help      Show this help message and exit.
  -V, --version   Print version information and exit.
Commands:
  volume, vol  Volume specific operations
  bucket       Bucket specific operations
  key          Key specific operations
  token        Token specific operations
{code}

This is fine, but for ozone s3:

{code}
[hadoop@om-0 keytabs]$ ozone s3
Incomplete command
Usage: ozone s3 [-hV] [--verbose] [-D=<String=String>]... [COMMAND]
Shell for S3 specific operations
      --verbose   More verbose output. Show the stack trace of the errors.
  -D, --set=<String=String>

  -h, --help      Show this help message and exit.
  -V, --version   Print version information and exit.
Commands:
  getsecret    Returns s3 secret for current user
  path         Returns the ozone path for S3Bucket
  volume, vol  Volume specific operations
  bucket       Bucket specific operations
  key          Key specific operations
  token        Token specific operations
{code}

This list should contain only the getsecret/path commands and not the volume/bucket/key subcommands.",pull-request-available,[],HDDS,Bug,Major,2019-03-29 15:06:45,1
13224799,Metrics scm_pipeline_metrics_num_pipeline_creation_failed keeps increasing because of BackgroundPipelineCreator,"There is a {{BackgroundPipelineCreator}} thread in SCM which runs in a fixed interval and tries to create pipelines. This BackgroundPipelineCreator uses {{IOException}} as exit criteria (no more pipelines can be created). In each run of BackgroundPipelineCreator we exit when we are not able to create any more pipelines, i.e. when we get IOException while trying to create the pipeline. This means that {{scm_pipeline_metrics_num_pipeline_creation_failed}} value will get incremented in each run of BackgroundPipelineCreator.",newbie pull-request-available,['SCM'],HDDS,Improvement,Minor,2019-03-29 11:40:32,5
13224673,NoClassDefFoundError when running ozone genconf,"{{ozone genconf}} fails due to incomplete classpath.

Steps to reproduce:

# [build and run Ozone|https://cwiki.apache.org/confluence/display/HADOOP/Development+cluster+with+docker]
# run {{ozone genconf}} in one of the containers:

{code}
$ ozone genconf /tmp
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/sun/xml/bind/v2/model/annotation/AnnotationReader
  at java.lang.ClassLoader.defineClass1(Native Method)
...
  at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:242)
  at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:234)
  at javax.xml.bind.ContextFinder.find(ContextFinder.java:441)
  at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:641)
  at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:584)
  at org.apache.hadoop.hdds.conf.OzoneConfiguration.readPropertyFromXml(OzoneConfiguration.java:57)
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.generateConfigurations(GenerateOzoneRequiredConfigurations.java:103)
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.call(GenerateOzoneRequiredConfigurations.java:73)
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.call(GenerateOzoneRequiredConfigurations.java:50)
  at picocli.CommandLine.execute(CommandLine.java:919)
...
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.main(GenerateOzoneRequiredConfigurations.java:68)
Caused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.model.annotation.AnnotationReader
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 36 more
{code}

{{AnnotationReader}} is in {{jaxb-core}} jar, which is not in the {{hadoop-ozone-tools}} classpath.",pull-request-available,['build'],HDDS,Bug,Major,2019-03-28 18:52:50,0
13224642,Fix checkstyle issue in TestDatanodeStateMachine,"The following tests are FAILED:
 
[checkstyle]: checkstyle check is failed ([https://ci.anzix.net/job/ozone-nightly/44/checkstyle/])",pull-request-available,[],HDDS,Improvement,Minor,2019-03-28 17:00:07,4
13224467,Implement GetS3Secret to use double buffer and cache.,"In Om HA getS3Secret  should happen only leader OM.

 

 

The reason is similar to initiateMultipartUpload. For more info refer HDDS-1319 

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-28 01:45:46,2
13224466,Remove hard-coded version ozone-0.5.0 from ReadMe of ozonesecure-mr docker-compose,"As we are releasing ozone-0.4, we should not have hard-coded ozone-0.5 for trunk. 

The proposal is to use the following to replace it

{{cd}} {{$(git rev-parse --show-toplevel)}}{{/hadoop-ozone/dist/target/ozone-}}{{*-SNAPSHOT}}{{/compose/ozone}}",pull-request-available,[],HDDS,Improvement,Blocker,2019-03-28 01:31:33,4
13224372,TestOzoneManagerHA#testOMProxyProviderFailoverOnConnectionFailure fails intermittently,"The test fails intermittently. The link to the test report can be found below.

[https://builds.apache.org/job/PreCommit-HDDS-Build/2582/testReport/]
{code:java}
java.net.ConnectException: Call From ea902c1cb730/172.17.0.3 to localhost:10174 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:310)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createVolume(OzoneManagerProtocolClientSideTranslatorPB.java:343)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.createVolume(RpcClient.java:275)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)
	at com.sun.proxy.$Proxy86.createVolume(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy86.createVolume(Unknown Source)
	at org.apache.hadoop.ozone.client.ObjectStore.createVolume(ObjectStore.java:100)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.createVolumeTest(TestOzoneManagerHA.java:162)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testOMProxyProviderFailoverOnConnectionFailure(TestOzoneManagerHA.java:237)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 49 more
{code}",TriagePending,['test'],HDDS,Sub-task,Major,2019-03-27 17:10:17,7
13224370,TestContainerReplication#testContainerReplication fails intermittently,"The test fails intermittently. The link to the test report can be found below.

https://builds.apache.org/job/PreCommit-HDDS-Build/2582/testReport/
{code:java}
java.lang.AssertionError: Container is not replicated to the destination datanode
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.apache.hadoop.ozone.container.TestContainerReplication.testContainerReplication(TestContainerReplication.java:139)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-03-27 17:02:50,1
13224156,Add List Containers API for Recon,"Recon server should support ""/containers"" API that lists all the containers",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-03-26 21:59:38,6
13224110,Implement Ratis Snapshots on OM,"For bootstrapping and restarting OMs, we need to implement snapshots in OM. The OM state maintained by RocksDB will be checkpoint-ed on demand. Ratis snapshots will only preserve the last applied log index by the State Machine on disk. This index will be stored in file in the OM metadata dir.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-26 18:31:32,7
13224001,Handle GroupMismatchException in OzoneClient,"If a pipeline gets destroyed in ozone client, ozone client may hit GroupMismatchException from Ratis. In cases as such, client should exclude the pipeline and retry write to a different block.",Blocker,['Ozone Client'],HDDS,Improvement,Blocker,2019-03-26 10:23:42,3
13223996,Better block allocation policy in SCM,"In case, a client hits close container exception and asks SCM to allocate block for the next write, SCM might allocate blocks from containers which are almost full and can get closed while the client write is still on. Ozone client by default tries 5 times on hitting an exception to write Data to a different block in case it hits CloseContainerException 5 times, it will give up.

The issue was found while testing with HDDS-1295.",Triaged,['SCM'],HDDS,Improvement,Major,2019-03-26 10:04:39,3
13223732,OzoneFileSystem can't work with spark/hadoop2.7 because incompatible security classes,"The current ozonefs compatibility layer is broken by: HDDS-1299.

The spark jobs (including hadoop 2.7) can't be executed any more:

{code}
2019-03-25 09:50:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProviderTokenIssuer
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
        at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)
        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:757)
        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:724)
        at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:45)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 43 more
{code}",pull-request-available,[],HDDS,Bug,Major,2019-03-25 10:18:17,1
13223462,In DatanodeStateMachine join check for not null,[https://builds.apache.org/job/PreCommit-HDDS-Build/2565/testReport/org.apache.hadoop.ozone.scm.node/TestSCMNodeMetrics/testNodeReportProcessingFailure/],pull-request-available,[],HDDS,Bug,Major,2019-03-22 23:32:05,2
13223459,Add a docker compose for Ozone deployment with Recon.,"* Add a docker compose for Ozone deployment with Recon.
* Test out Recon container key service. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-03-22 23:04:05,5
13223251,TestOzoneManagerHA seems to be flaky,"TestOzoneManagerHA failed once with the following error:
{code}
[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 105.931 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOzoneManagerHA
[ERROR] testOMRetryProxy(org.apache.hadoop.ozone.om.TestOzoneManagerHA)  Time elapsed: 21.781 s  <<< FAILURE!
java.lang.AssertionError: expected:<30> but was:<10>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testOMRetryProxy(TestOzoneManagerHA.java:305)
{code}
",pull-request-available,['test'],HDDS,Bug,Major,2019-03-22 05:25:15,7
13223198,Hugo errors when building Ozone,"I see some odd hugo errors when building Ozone, even though I am not building docs.
{code}
$ mvn -B -q clean compile install -DskipTests=true -Dmaven.javadoc.skip=true -Dmaven.site.skip=true -DskipShade -Phdds

Error: unknown command ""0.4.0-SNAPSHOT"" for ""hugo""
Run 'hugo --help' for usage.
.../hadoop-hdds/docs/target
Error: unknown command ""0.4.0-SNAPSHOT"" for ""hugo""
Run 'hugo --help' for usage.
.../hadoop-hdds/docs/target
{code}",pull-request-available,[],HDDS,Bug,Major,2019-03-21 21:34:42,0
13222923,Fix MalformedTracerStateStringException on DN logs,"Have seen many warnings on DN logs. This ticket is opened to track the investigation and fix for this.

{code}

2019-03-20 19:01:33 WARN PropagationRegistry$ExceptionCatchingExtractorDecorator:60 - Error when extracting SpanContext from carrier. Handling gracefully.
io.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 2c919331-9a51-4bc4-acee-df57a8dcecf0
 at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:42)
 at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:32)
 at io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)
 at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)
 at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)
 at io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)
 at org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:96)
 at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
 at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
 at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
 at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
 at org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
 at org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46)
 at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)
 at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686)
 at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
 at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-03-20 19:19:31,4
13222921,KeyOutputStream#write throws ArrayIndexOutOfBoundsException when running RandomWrite MR examples,"Repro steps:

{code} 

jar $HADOOP_MAPRED_HOME/hadoop-mapreduce-examples-*.jar randomwriter -Dtest.randomwrite.total_bytes=10000000  o3fs://bucket1.vol1/randomwrite.out

{code}

 

Error Stack:

{code}

2019-03-20 19:02:37 INFO Job:1686 - Task Id : attempt_1553108378906_0002_m_000000_0, Status : FAILED
Error: java.lang.ArrayIndexOutOfBoundsException: -5
 at java.util.ArrayList.elementData(ArrayList.java:422)
 at java.util.ArrayList.get(ArrayList.java:435)
 at org.apache.hadoop.hdds.scm.storage.BufferPool.getBuffer(BufferPool.java:45)
 at org.apache.hadoop.hdds.scm.storage.BufferPool.allocateBufferIfNeeded(BufferPool.java:59)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:215)
 at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:130)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:311)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:273)
 at org.apache.hadoop.fs.ozone.OzoneFSOutputStream.write(OzoneFSOutputStream.java:46)
 at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
 at java.io.DataOutputStream.write(DataOutputStream.java:107)
 at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1444)
 at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:83)
 at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:670)
 at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
 at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
 at org.apache.hadoop.examples.RandomWriter$RandomMapper.map(RandomWriter.java:199)
 at org.apache.hadoop.examples.RandomWriter$RandomMapper.map(RandomWriter.java:165)
 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-03-20 19:06:18,3
13222490,Fix asf license errors,"HDDS-1250 added a few new files. In few of them, it is missing adding asf license header.

[https://github.com/apache/hadoop/pull/591]

 

Yetus has not reported about them. I think Yetus is broken in warning asf license errors.",pull-request-available,[],HDDS,Bug,Major,2019-03-19 05:11:14,2
13222489,Test ScmChillMode testChillModeOperations failed,"[https://ci.anzix.net/job/ozone-nightly/35/testReport/junit/org.apache.hadoop.ozone.om/TestScmChillMode/testChillModeOperations/]

 ",pull-request-available,[],HDDS,Bug,Major,2019-03-19 04:58:54,2
13222357,Fix SCM CLI does not list container with id 1,"In HDDS-1263 it is changed to handle the list containers with containerID 1 by changing the actual logic of listContainers in ScmContainerManager.java. But now with this change, it is contradicting with the javadoc.

From [~nandakumar131] comments

https://issues.apache.org/jira/browse/HDDS-1263?focusedCommentId=16794865&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16794865

 

I agree this will be the way to fix it.",pull-request-available,[],HDDS,Bug,Minor,2019-03-18 16:16:40,6
13222242,Support TokenIssuer interface for running jobs with OzoneFileSystem,"This ticket is opened to add TokenIssuer interface support to OzoneFileSystem so that MR and Spark jobs can run with OzoneFileSystem in secure mode. 

 

 ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-03-18 06:54:48,4
13222221,Fix checkstyle issue from Nightly run,"[https://ci.anzix.net/job/ozone-nightly/32/checkstyle/moduleName.588460772/fileName.-1184872187/]

 ",pull-request-available,[],HDDS,Sub-task,Major,2019-03-18 05:14:09,4
13222136,ExcludeList shoud be a RPC Client config so that multiple streams can avoid the same error.,"ExcludeList right now is a per BlockOutPutStream value, this can result in multiple keys created out of the same client to run into same exception",MiniOzoneChaosCluster,['Ozone Client'],HDDS,Bug,Major,2019-03-17 10:59:36,3
13222134,ExcludeList#getProtoBuf throws ArrayIndexOutOfBoundsException,"ExcludeList#getProtoBuf throws ArrayIndexOutOfBoundsException because getProtoBuf uses parallelStreams

{code}
2019-03-17 16:24:35,774 INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 3
	at java.util.ArrayList.add(ArrayList.java:463)
	at org.apache.hadoop.hdds.protocol.proto.HddsProtos$ExcludeListProto$Builder.addContainerIds(HddsProtos.java:12904)
	at org.apache.hadoop.hdds.scm.container.common.helpers.ExcludeList.lambda$getProtoBuf$3(ExcludeList.java:89)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool.helpComplete(ForkJoinPool.java:1870)
	at java.util.concurrent.ForkJoinPool.externalHelpComplete(ForkJoinPool.java:2467)
	at java.util.concurrent.ForkJoinTask.externalAwaitDone(ForkJoinTask.java:324)
	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:405)
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
	at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
	at org.apache.hadoop.hdds.scm.container.common.helpers.ExcludeList.getProtoBuf(ExcludeList.java:89)
	at org.apache.hadoop.hdds.scm.protocolPB.ScmBlockLocationProtocolClientSideTranslatorPB.allocateBlock(ScmBlockLocationProtocolClientSideTranslatorPB.java:100)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy22.allocateBlock(Unknown Source)
	at org.apache.hadoop.ozone.om.KeyManagerImpl.allocateBlock(KeyManagerImpl.java:275)
	at org.apache.hadoop.ozone.om.KeyManagerImpl.allocateBlock(KeyManagerImpl.java:246)
	at org.apache.hadoop.ozone.om.OzoneManager.allocateBlock(OzoneManager.java:2023)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.allocateBlock(OzoneManagerRequestHandler.java:631)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:231)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
, while invoking $Proxy28.submitRequest over null(localhost:59024). Trying to failover immediately.
2019-03-17 16:24:35,783 INFO  om.KeyManagerImpl (KeyManagerImpl.java:allocateBlock(271)) - allocate block key:pool-9-thread-7-1581351327 exclude:datanodes:containers:#6#1#9#5pipelines:
{code}",MiniOzoneChaosCluster,['SCM'],HDDS,Bug,Major,2019-03-17 10:56:19,3
13221966,Set OmKeyArgs#refreshPipeline flag properly to avoid reading from stale pipeline,"After HDDS-1138, the OM client will not talk to SCM directly to fetch the pipeline info. Instead the pipeline info is returned as part of the keyLocation cached by OM. 

 

In case SCM pipeline is changed such as closed, the client may get invalid pipeline exception. In this case, the client need to getKeyLocation with OmKeyArgs#refreshPipeline = true to force OM update its pipeline cache for this key. 

 

An optimization could be queue a background task to update all the keyLocations that is affected when OM does a refreshPipeline. (This part can be done in 0.5)
{code:java}
oldpipeline->newpipeline{code}
 ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-03-15 17:26:04,4
13221718,Implement actions need to be taken after chill mode exit wait time,"# Destroy and close the pipelines
 # Close all the containers on the pipeline.
 # trigger for pipeline creation",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-03-14 16:36:02,2
13221705,Adjust default values of pipline recovery for more resilient service restart,"As of now we have a following algorithm to handle node failures:

1. In case of a missing node the leader of the pipline or the scm can detected the missing heartbeats.
2. SCM will start to close the pipeline (CLOSING state) and try to close the containers with the remaining nodes in the pipeline
3. After 5 minutes the pipeline will be destroyed (CLOSED) and a new pipeline can be created from the healthy nodes (one node can be part only one pipwline in the same time).

While this algorithm can work well with a big cluster it doesn't provide very good usability on small clusters:

Use case1:

Given 3 nodes, in case of a service restart, if the restart takes more than 90s, the pipline will be moved to the CLOSING state. For the next 5 minutes (ozone.scm.pipeline.destroy.timeout) the container will remain in the CLOSING state. As there are no more nodes and we can't assign the same node to two different pipeline, the cluster will be unavailable for 5 minutes.

Use case2:

Given 90 nodes and 30 pipelines where all the pipelines are spread across 3 racks. Let's stop one rack. As all the pipelines are affected, all the pipelines will be moved to the CLOSING state. We have no free nodes, therefore we need to wait for 5 minutes to write any data to the cluster.

These problems can be solved in multiple ways:

1.) Instead of waiting 5 minutes, destroy the pipeline when all the containers are reported to be closed. (Most of the time it's enough, but some container report can be missing)
2.) Support multi-raft and open a pipeline as soon as we have enough nodes (even if the nodes already have a CLOSING pipelines).

Both the options require more work on the pipeline management side. For 0.4.0 we can adjust the following parameters to get better user experience:

{code}
  <property>
    <name>ozone.scm.pipeline.destroy.timeout</name>
    <value>60s</value>
    <tag>OZONE, SCM, PIPELINE</tag>
    <description>
      Once a pipeline is closed, SCM should wait for the above configured time
      before destroying a pipeline.
    </description>

  <property>
    <name>ozone.scm.stale.node.interval</name>
    <value>90s</value>
    <tag>OZONE, MANAGEMENT</tag>
    <description>
      The interval for stale node flagging. Please
      see ozone.scm.heartbeat.thread.interval before changing this value.
    </description>
  </property>
 {code}

First of all, we can be more optimistic and mark node to stale only after 5 mins instead of 90s. 5 mins should be enough most of the time to recover the nodes.

Second: we can decrease the time of ozone.scm.pipeline.destroy.timeout. Ideally the close command is sent by the scm to the datanode with a HB. Between two HB we have enough time to close all the containers via ratis. With the next HB, datanode can report the successful datanode. (If the containers can be closed the scm can manage the QUASI_CLOSED containers)

We need to wait 29 seconds (worst case) for the next HB, and 29+30 seconds for the confirmation. --> 66 seconds seems to be a safe choice (assuming that 6 seconds is enough to process the report about the successful closing)",pull-request-available,[],HDDS,Bug,Critical,2019-03-14 16:02:48,1
13221693,Fix the dynamic documentation of basic s3 client usage,"S3 gateway has a default web page to display a generic message if you open the endpoint in the browser:

http://localhost:9878/static/

It also contains a simple example to use the endpoint:

{code}
This is an endpoint of Apache Hadoop Ozone S3 gateway. Use it with any AWS S3 compatible tool with setting this url as an endpoint

For example with aws-cli:

aws s3api --endpoint http://localhost:9878/static/ create-bucket --bucket=wordcount

For more information, please check the documentation. 
{code}

Unfortunately the endpoint is wrong here, the static should be removed from the url.

The trivial fix is to move the ) in the js code>  

",pull-request-available,['S3'],HDDS,Bug,Major,2019-03-14 14:59:41,1
13221592,Fix the findbug issue caused by HDDS-1163,https://ci.anzix.net/job/ozone-nightly/30/findbugs/,newbie,[],HDDS,Bug,Minor,2019-03-14 05:38:51,5
13221544,"""ozone sh s3 getsecret"" throws Null Pointer Exception for unsecured clusters","{code:java}
hadoop@e72da2270499:~$ ozone sh s3 getsecret {code}
{code:java}
2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862). Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 1 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 2 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 3 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 4 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 5 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 6 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 7 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 8 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 9 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 ERROR OMFailoverProxyProvider:235 - Failed to connect to OM. Attempted 10 retries and 10 failovers java.lang.NullPointerException
{code}",pull-request-available,['Ozone CLI'],HDDS,Bug,Blocker,2019-03-13 23:42:29,6
13221530,Remove Parametrized in TestOzoneShell,"HDDS-1068 removed RestClient from the TestOzoneShell.java.

So now we don't need to be parameterized in the test anymore. We can directly test with RpcClient.",newbie pull-request-available,[],HDDS,Bug,Minor,2019-03-13 22:06:30,6
13221527,SCM CLI does not list container with id 1,"Steps to reproduce
 # Create two containers 

{code:java}
ozone scmcli create
ozone scmcli create{code}

 # Try to list containers

{code:java}
hadoop@7a73695402ae:~$ ozone scmcli list --start=0
 Container ID should be a positive long. 0
hadoop@7a73695402ae:~$ ozone scmcli list --start=1 
{ 
""state"" : ""OPEN"",
""replicationFactor"" : ""ONE"",
""replicationType"" : ""STAND_ALONE"",
""usedBytes"" : 0,
""numberOfKeys"" : 0,
""lastUsed"" : 274660388,
""stateEnterTime"" : 274646481,
""owner"" : ""OZONE"",
""containerID"" : 2,
""deleteTransactionId"" : 0,
""sequenceId"" : 0,
""open"" : true 
}{code}

There is no way to list the container with containerID 1.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-03-13 21:44:12,6
13221524,In OM HA OpenKey call Should happen only leader OM,"In OM HA, currently, when openKey is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB and also clientID will be generated by each OM node.

 

The proposed approach is:

1. In startTransaction, call openKey and the response returned should be used to create a new OmRequest object and used in setting the transaction context. And also modify the ozoneManager and KeymanagerImpl to handle the code with and without ratis.

 

This Jira also implements HDDS-1319. ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-13 21:39:47,2
13221491,Create Recon Server lifecyle integration with Ozone.,"* Create the lifecycle scripts (start/stop) for Recon Server along with Shell interface like the other components.
 * Verify configurations are being picked up by Recon Server on startup.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Critical,2019-03-13 18:27:57,6
13221419,OzoneFS classpath separation is broken by the token validation,"hadoop-ozone-filesystem-lib-legacy-0.4.0-SNAPSHOT.jar can't work any more together with older hadoop version, after the change of HDDS-1183.

{code}
2019-03-13 13:48:51 WARN  FileSystem:3170 - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.ozone.OzoneFileSystem could not be instantiated
2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration
2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2332)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2297)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3208)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3240)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3291)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3259)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:470)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:245)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:228)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:175)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:378)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 21 more
{code}

And Ozone file system current jar is compatible only with hadoop 3.2 after the latest HA change. It means that ozonefs is broken everywhere where the hadoop version is older than 3.2.",pull-request-available,[],HDDS,Improvement,Blocker,2019-03-13 14:21:57,1
13221108,Fix checkstyle issue from Nightly run,https://ci.anzix.net/job/ozone-nightly/28/checkstyle/,pull-request-available,[],HDDS,Sub-task,Major,2019-03-12 12:37:30,4
13220954,In OM HA AllocateBlock call where connecting to SCM from OM should not happen on Ratis,"In OM HA, currently when allocateBlock is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB. The problem with this is, every OM allocateBlock and appends new BlockInfo into OMKeyInfom and also this a correctness issue. (As all OM's should have the same block information for a key, even though eventually this might be changed during key commit)

 

The proposed approach is:

1. Calling SCM for allocation of block will happen outside of ratis, and this block information is passed and writing to DB will happen via Ratis.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-11 22:23:03,2
13220795,TestSecureOzoneRpcClient fails intermittently," 

TestSecureOzoneRpcClient fails intermittently with the following exception.
{code:java}
java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:338)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:238)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:131)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:310)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:271)
	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.uploadPart(TestOzoneRpcClientAbstract.java:2188)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.doMultipartUpload(TestOzoneRpcClientAbstract.java:2131)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testMultipartUpload(TestOzoneRpcClientAbstract.java:1721)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.waitOnFlushFutures(BlockOutputStream.java:543)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:333)
	... 35 more
Caused by: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:417)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:573)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:556)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:415)
	... 6 more
{code}
cc : [~ajayydv] [~xyao]

 ",Triaged,['test'],HDDS,Bug,Major,2019-03-11 09:47:10,4
13220793,Bump trunk ozone version to 0.5.0,"ozone-0.4 branch is working, we need to update the trunk version.",pull-request-available,[],HDDS,Improvement,Major,2019-03-11 09:36:32,1
13220753,Add ozone delegation token utility subcmd for Ozone CLI,"This allow running dtutil with integration test and dev test for demo of Ozone security.

 

 

 

 ",pull-request-available,[],HDDS,Sub-task,Major,2019-03-11 06:34:44,4
13220749,OM delegation expiration time should use Time.now instead of Time.monotonicNow,"Otherwise, we will set incorrect the exp date of OM delegation like below: 

{code}
ozone dtutil print /tmp/om.dt
 
File: /tmp/om.dt
Token kind               Service              Renewer         Exp date     URL enc token
--------------------------------------------------------------------------------
OzoneToken               om:9862              yarn            *1/8/70 12:03 PM*
{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-03-11 06:18:10,4
13220659,"In s3 when bucket already exists, it should just return location ","In S3 for a create bucket request, when bucket already exists it should just return the location.

This was broken by HDDS-1068.",pull-request-available,[],HDDS,Bug,Major,2019-03-10 04:16:31,2
13220506,Fix check style issues caused by HDDS-1196,"Jenkins has not reported any issues, but found when working on another jira.

https://github.com/apache/hadoop/pull/567",pull-request-available,[],HDDS,Bug,Minor,2019-03-08 17:27:01,2
13220245,Fix incorrect Ozone ClientProtocol KerberosInfo annotation,The serverPrincipal should be OMConfigKeys.OZONE_OM_KERBEROS_PRINCIPAL_KEY instead of ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,pull-request-available,[],HDDS,Sub-task,Major,2019-03-07 15:59:32,4
13220243,BaseHttpServer NPE is HTTP policy is HTTPS_ONLY,This needs to be fixed when Ozone is running inside DN as plugin and DN is running using non-privilege HTTPS port. ,pull-request-available,[],HDDS,Sub-task,Major,2019-03-07 15:55:21,4
13220097,Recon Container DB service definition,"* Define the Ozone Recon container DB service definition. 
",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-03-07 05:12:55,5
13220061,Add ChillMode metrics,"This Jira is to add few of the chill mode metrics:
 # NumberofHealthyPipelinesThreshold
 # currentHealthyPipelinesCount
 # NumberofPipelinesWithAtleastOneReplicaThreshold
 # CurrentPipelinesWithAtleastOneReplicaCount
 # ChillModeContainerWithOneReplicaReportedCutoff
 # CurrentContainerCutoff

 ",pull-request-available,[],HDDS,Improvement,Major,2019-03-06 23:55:08,2
13219830,Avoid extra buffer copy during checksum computation in write Path,"The code here does a buffer copy to to compute checksum. This needs to be avoided.
{code:java}
/**
 * Computes checksum for give data.
 * @param byteString input data in the form of ByteString.
 * @return ChecksumData computed for input data.
 */
public ChecksumData computeChecksum(ByteString byteString)
    throws OzoneChecksumException {
  return computeChecksum(byteString.toByteArray());
}

{code}",pushed-to-craterlake,['Ozone Client'],HDDS,Improvement,Major,2019-03-06 07:53:18,3
13219689,ozone-filesystem jar missing in hadoop classpath,hadoop-ozone-filesystem-lib-*.jar is missing in hadoop classpath.,pull-request-available,"['Ozone Filesystem', 'Ozone Manager']",HDDS,Bug,Major,2019-03-05 22:59:02,6
13219661,Provide docker-compose for OM HA,**This Jira proposes to add docker-compose file to run local pseudo cluster with OM HA (3 OM nodes).,pull-request-available,"['docker', 'OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-05 21:35:41,7
13219614,Restructure code to validate the response from server in the Read path,"In the read path, the validation of the response while reading the data from the datanodes happen in XceiverClientGrpc as well as additional  Checksum verification happens in Ozone client to verify the read chunk response. The aim of this Jira is to modify the function call to take a validator function as a part of reading data so all validation can happen in a single unified place.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-03-05 17:22:37,3
13219584,Remove TestContainerSQLCli unit test stub,"In HDDS-447 we removed the support the 'ozone noz' cli tool which was a rocksdb/leveldb to sql exporter.

But still we have the unit test for the tool (in fact only the skeleton of the unit test, as the main logic is removed). Even worse this unit test is failing as it calls System.exit:

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-tools: There are test failures.
[ERROR] 
[ERROR] Please refer to /testptch/hadoop/hadoop-ozone/tools/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
{code}

I think this test can be deleted.",pull-request-available,[],HDDS,Sub-task,Major,2019-03-05 15:33:59,1
13219550,TestContainerActionsHandler.testCloseContainerAction has an intermittent failure,"It's failed multiple times during the CI builds:

{code}
Error Message

Wanted but not invoked:
closeContainerEventHandler.onMessage(
    #1,
    org.apache.hadoop.hdds.server.events.EventQueue@3d3fcdb0
);
-> at org.apache.hadoop.hdds.scm.container.TestContainerActionsHandler.testCloseContainerAction(TestContainerActionsHandler.java:64)
Actually, there were zero interactions with this mock.
{code}

The fix is easy: we should call queue.processAll(1000L) to wait for the processing of all the events.
",pull-request-available,[],HDDS,Sub-task,Major,2019-03-05 12:44:36,1
13219415,Refactor ChillMode rules and chillmode manager,"# Make the chillmodeExitRule abstract class and move common logic for all rules into this.
 # Update test's for chill mode accordingly.",pull-request-available,[],HDDS,Improvement,Major,2019-03-04 22:29:36,2
13219376,Change hadoop-runner and apache/hadoop base image to use Java8,"{code}

kms_1           | Exception in thread ""main"" java.lang.NoClassDefFoundError: javax/activation/DataSource

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.<clinit>(RuntimeBuiltinLeafInfoImpl.java:457)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.<init>(RuntimeTypeInfoSetImpl.java:65)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85)

kms_1           | at com.sun.xml.bind.v2.model.impl.ModelBuilder.<init>(ModelBuilder.java:156)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.<init>(RuntimeModelBuilder.java:93)

kms_1           | at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473)

kms_1           | at com.sun.xml.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:319)

kms_1           | at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170)

kms_1           | at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145)

kms_1           | at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:236)

kms_1           | at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

kms_1           | at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

kms_1           | at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

kms_1           | at java.base/java.lang.reflect.Method.invoke(Method.java:566)

kms_1           | at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:186)

kms_1           | at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:146)

kms_1           | at javax.xml.bind.ContextFinder.find(ContextFinder.java:350)

kms_1           | at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:446)

kms_1           | at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:409)

kms_1           | at com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.<init>(WadlApplicationContextImpl.java:103)

kms_1           | at com.sun.jersey.server.impl.wadl.WadlFactory.init(WadlFactory.java:100)

kms_1           | at com.sun.jersey.server.impl.application.RootResourceUriRules.initWadl(RootResourceUriRules.java:169)

kms_1           | at com.sun.jersey.server.impl.application.RootResourceUriRules.<init>(RootResourceUriRules.java:106)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1359)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)

{code}",pull-request-available,[],HDDS,Sub-task,Blocker,2019-03-04 19:51:21,1
13219300,Enable tracing for the datanode read/write path,"HDDS-1150 introduced distributed for ozone components. But we have no trace context propagation between the clients and Ozone Datanodes.

As we use Grpc and Ratis on this RPC path the full tracing could be quite complex: we should propagate the trace id in Ratis and include it in all the log entries.

I propose a simplified solution here: to trace only the StateMachine operations.

As Ratis is a library we provide the implementation of the appropriate Raft elements especially the StateMachine and the raft messages. We can add the tracing information to the raft messages (in fact, we already have this field) and we can restore the tracing context during the StateMachine operations.

This approach is very simple (only a few lines of codes) and can show the time of the real write/read operations, but can't see the internals of the Ratis operations.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-03-04 14:23:10,1
13219278,Support plain text S3 MPU initialization request,"S3 Multi-Part-Upload (MPU) is implemented recently in the Ozone s3 gateway. We have extensive testing with using 'aws s3api' application which is passed.

But it turned out that the more simple `aws s3 cp` command fails with _405 Media type not supported error_ message

The root cause of this issue is the JAXRS implementation of the multipart upload method:

{code}
  @POST
  @Produces(MediaType.APPLICATION_XML)
  public Response multipartUpload(
      @PathParam(""bucket"") String bucket,
      @PathParam(""path"") String key,
      @QueryParam(""uploads"") String uploads,
      @QueryParam(""uploadId"") @DefaultValue("""") String uploadID,
      CompleteMultipartUploadRequest request) throws IOException, OS3Exception {
    if (!uploadID.equals("""")) {
      //Complete Multipart upload request.
      return completeMultipartUpload(bucket, key, uploadID, request);
    } else {
      // Initiate Multipart upload request.
      return initiateMultipartUpload(bucket, key);
    }
  }
{code}

Here we have a CompleteMultipartUploadRequest parameter which is created by the JAXRS framework based on the media type and the request body. With _Content-Type: application/xml_ it's easy: the JAXRS framework uses the built-in JAXB serialization. But with plain/text content-type it's not possible as there is no serialization support for CompleteMultipartUploadRequest from plain/text.

",pull-request-available,['S3'],HDDS,Bug,Blocker,2019-03-04 13:25:21,1
13218984,Test SCMChillMode failing randomly in Jenkins run,java.lang.Thread.State: TIMED_WAITING at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460) at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362) at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389) at org.apache.hadoop.ozone.om.TestScmChillMode.testSCMChillMode(TestScmChillMode.java:286) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74),pull-request-available pushed-to-craterlake,[],HDDS,Sub-task,Major,2019-03-01 19:12:07,2
13218850,Handle Datanode volume out of space,"steps taken :

--------------------
 # create 40 datanode cluster.
 # one of the datanodes has less than 5 GB space.
 # Started writing key of size 600MB.

operation failed:

Error on the client:

----------------------------
{noformat}
Fri Mar 1 09:05:28 UTC 2019 Ruuning /root/hadoop_trunk/ozone-0.4.0-SNAPSHOT/bin/ozone sh key put testvol172275910-1551431122-1/testbuck172275910-1551431122-1/test_file24 /root/test_files/test_file24
original md5sum a6de00c9284708585f5a99b0490b0b23
2019-03-01 09:05:39,142 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:39,578 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,368 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,450 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,457 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 does not exist
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:393)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,535 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,617 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,741 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,814 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,815 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 does not exist
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:393)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
java.nio.BufferOverflowException
 at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:189)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:213)
 at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:128)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:307)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:268)
 at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
 at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
 at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:111)
 at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:53)
 at picocli.CommandLine.execute(CommandLine.java:919)
 at picocli.CommandLine.access$700(CommandLine.java:104)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
 at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
 at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
 at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
 at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
 at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
 at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
 at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95){noformat}
 

ozone.log

-----------------

 
{noformat}
2019-03-01 09:05:33,248 [IPC Server handler 17 on 9889] DEBUG (OzoneManagerRequestHandler.java:137) - Received OMRequest: cmdType: CreateKey
traceID: ""5f169cde0a4c8a4e:79f0b64c3329c0ba:5f169cde0a4c8a4e:0""
clientId: ""client-86810A76C95E""
createKeyRequest {
 keyArgs {
 volumeName: ""testvol172275910-1551431122-1""
 bucketName: ""testbuck172275910-1551431122-1""
 keyName: ""test_file24""
 dataSize: 629145600
 type: RATIS
 factor: THREE
 isMultipartKey: false
 }
}
,
2019-03-01 09:05:33,255 [IPC Server handler 17 on 9889] DEBUG (KeyManagerImpl.java:465) - Key test_file24 allocated in volume testvol172275910-1551431122-1 bucket testbuck172275910-1551431122-1
2019-03-01 09:05:38,229 [IPC Server handler 8 on 9889] DEBUG (OzoneManagerRequestHandler.java:137) - Received OMRequest: cmdType: AllocateBlock
traceID: ""5f169cde0a4c8a4e:fe6c4bdb75978062:5f169cde0a4c8a4e:0""
clientId: ""client-86810A76C95E""
allocateBlockRequest {
 keyArgs {
 volumeName: ""testvol172275910-1551431122-1""
 bucketName: ""testbuck172275910-1551431122-1""
 keyName: ""test_file24""
 dataSize: 629145600
 }
 clientID: 20622763490697872
}
,
2019-03-01 09:05:38,739 [grpc-default-executor-17] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:38,790 [grpc-default-executor-17] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:38,800 [grpc-default-executor-17] DEBUG (ContainerStateMachine.java:358) - writeChunk writeStateMachineData : blockId containerID: 79
localID: 101674591075108132
blockCommitSequenceId: 0
 logIndex 3 chunkName f6508b585fbd0b834b2139939467ac03_stream_8101b9db-a724-4690-abe1-c7daa2630326_chunk_1
2019-03-01 09:05:38,801 [grpc-default-executor-17] DEBUG (ContainerStateMachine.java:365) - writeChunk writeStateMachineData completed: blockId containerID: 79
localID: 101674591075108132
blockCommitSequenceId: 0
 logIndex 3 chunkName f6508b585fbd0b834b2139939467ac03_stream_8101b9db-a724-4690-abe1-c7daa2630326_chunk_1
2019-03-01 09:05:38,978 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:0 detected, adding dummy entries
2019-03-01 09:05:38,979 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:1 detected, adding dummy entries
2019-03-01 09:05:38,980 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:2 detected, adding dummy entries
2019-03-01 09:05:38,981 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:38,981 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:39,357 [grpc-default-executor-18] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:896673a239485fcd:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:39,358 [grpc-default-executor-18] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:896673a239485fcd:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE
 
{noformat}
 ",Triaged,['Ozone Client'],HDDS,Bug,Major,2019-03-01 09:41:22,7
13218808,Fix ClassNotFound issue with javax.xml.bind.DatatypeConverter used by DefaultProfile,"The ozonesecure docker-compose has been changed to use hadoop-runner image based on Java 11. Several packages/classes have been removed from Java 8 such as 

javax.xml.bind.DatatypeConverter.parseHexBinary

 This ticket is opened to fix issues running ozonesecure docker-compose on java 11.",pull-request-available,[],HDDS,Sub-task,Major,2019-03-01 05:12:30,4
13218790,In Healthy Pipeline rule consider pipelines with all replicationType and replicationFactor,"In the current HealthyPipelineRule, we considered only pipeline type ratis and replication factor 3 pipelines for 10%.

 

This Jira is to consider all the pipelines with all replication factor for the 10% threshold. (Means each pipeline-type with factor should meet 10%)",TriagePending,['SCM'],HDDS,Bug,Major,2019-03-01 01:23:38,2
13218769,Add a ChillMode handler class ,"Condition for to Start Replication monitor thread.
 # Exit Chill mode
 # Time out (configurable value) default to 5 minutes. Additional time out is added to give some additional time for datanodes to report and make pipelines healthy.

So, once we are out of chillmode, we fire ChillModeStatus, this ReplicationTimer Class will listen to that event, and wait for a configurable time, and then emit replicationEnabled.

 

The current code, when we are out of chill mode, we set replication enabled.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-28 23:25:00,2
13218720,Refactor ContainerChillModeRule and DatanodeChillMode rule,"The main intention of this Jira is to have all rules look in a similar way of handling events.

In this Jira, did the following changes:
 # Both DatanodeRule and ContainerRule implements EventHandler and listen for NodeRegistrationContainerReport
 # Update ScmChillModeManager not to handle any events. (As each rule need to handle an event, and work on that rule)",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-28 18:50:50,2
13218566,Replace Ozone Rest client with S3 client in smoketests and docs,"As it's discussed in the the parent jira the rest support for Ozone Client protocol can be removed to use S3 Rest API instead of that.

Some of the unit tests are already disabled, so it seems to be better to remove it from the documentation (and from the smoketests).",pull-request-available,['test'],HDDS,Sub-task,Major,2019-02-28 08:28:56,1
13218453,Healthy pipeline Chill Mode rule to consider only pipelines with replication factor three,"Few offline comments from [~nandakumar131]
 # We should not process pipeline report from datanode again during calculations.
 # We should consider only replication factor 3 ratis pipelines.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-27 21:29:09,2
13218270,Support getDelegationToken API for OzoneFileSystem,This includes addDelegationToken/renewDelegationToken/cancelDelegationToken so that MR jobs can collect tokens correctly upon job submission time. ,pull-request-available,[],HDDS,Sub-task,Major,2019-02-27 06:59:55,4
13218198,Pipeline Rule where atleast one datanode is reported in the pipeline,"h2. Pipeline Rule with configurable percentage of pipelines with at least one datanode reported:

In this rule we consider when at least  90% of pipelines have at least one datanode reported. 

 

This rule satisfies, when we exit chill mode, Ozone clients will have at least one open replica for reads to succeed. (We can increase this threshold default from 90%, if we want to see fewer failures during reads after exit chill mode.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-26 23:44:49,2
13217960,Healthy pipeline Chill Mode Rule,"This Jira is to implement one of the chill mode rule.

*Pipeline Rule with configurable percentage of open pipelines with all datanodes reported*: In this rule, we consider when at least 10% of the pipelines have all 3 data node's reported (if it is a 3 node ratis ring), and one node reported if it is a one node ratis ring. (The percentage is configurable via ozone configuration parameter).

This rule satisfies, once the SCM is restarted, and after exiting chill mode, we have enough pipelines to give for clients for writes to succeed.

 ",pull-request-available,['SCM'],HDDS,Bug,Major,2019-02-26 01:14:14,2
13217900,Serve read requests directly from RocksDB,"We can directly server read requests from the OM's RocksDB instead of going through the Ratis server. OM should first check its role and only if it is the leader can it server read requests. 

There can be a scenario where an OM can lose its Leader status but not know about the new election in the ring. This OM could server stale reads for the duration of the heartbeat timeout but this should be acceptable (similar to how Standby Namenode could possibly server stale reads till it figures out the new status).",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-02-25 19:45:08,7
13217188,Disable failing test which are tracked by a separated jira,"As I wrote in the description of the parent Jira I propose to disable (@Ignore) the unit tests which are failing while we are fixing them to get clean Jira response from the PreCommit builds.

All the tests are tracked in a separated jira.",pull-request-available,['test'],HDDS,Sub-task,Major,2019-02-21 12:42:03,1
13217185,TestOzoneManagerHA.testTwoOMNodesDown is failing with ratis error,"h3. Error Message
{code:java}
org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-4D77D2A8F653->omNode-3@group-523986131536, cid=9, seq=0 RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$373/2067504307@6afa0221 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms){code}
Stacktrace
{code:java}
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-4D77D2A8F653->omNode-3@group-523986131536, cid=9, seq=0 RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$373/2067504307@6afa0221 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:586) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createVolume(OzoneManagerProtocolClientSideTranslatorPB.java:230) at org.apache.hadoop.ozone.web.storage.DistributedStorageHandler.createVolume(DistributedStorageHandler.java:179) at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testCreateVolume(TestOzoneManagerHA.java:153) at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testTwoOMNodesDown(TestOzoneManagerHA.java:138) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}
 ",pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 12:32:24,7
13217143,Add tracing to the client side of StorageContainerLocationProtocol and OzoneManagerProtocol,"The best way to display the network performance in the tracing server is to start a new span (=save tracing information) on both of the client and and server side.

This issue is about improving the client side tracing of OzoneManager and StorageLocationManager.

The easiest way to turn on tracing for a class which implements an interface is to create a java proxy with TracingUtil.createProxy. With this utility we can execute the required tracing methods (create span/close span) around the original methods without adding any boilerplate code.

Thanks to the current design hadoop rpc calls are create by the *ClientSideTranslatorPB classes which implements the original protocol interface (eg. StorageContainerLocationProtocolClientSideTranslatorPB implements StorageContainerLocationProtocol) which means that it can easily instrumented by TracingUtil.createProxy.

The only thing what we need is to use the interface everywhere (StorageContainerLocationProtocol) instead of the implementation (ClientSideTranslator) as a client.

The only required method which is not published in the ClientSideTranslator is the close method. With adding the close method to the interface (extends Closable) we are able to use the interface everywhere which can be instrumented to send the tracing information.",pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 09:28:20,1
13217127,Add trace information for the client side of the datanode writes,The XCeiverClients can be traced on the client side to get some information about the time of chunk writes / block puts.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 08:32:18,1
13217126,Propagate the tracing id in ScmBlockLocationProtocol,The tracing propagation is not yet enabled for the ScmBlockLocationProtocol. We can't see the internal calls between OM and SCM. We need to propagate it at least for the allocateBlock call.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 08:29:38,1
13217050,"After allocating container, we are not adding to container DB.","If we don't do that, we get an error when handling container report for open containers.

As they don't exist in container DB.

 
{code:java}
scm_1           | at java.lang.Thread.run(Thread.java:748)
scm_1           | 2019-02-21 00:00:32 ERROR ContainerReportHandler:173 - Received container report for an unknown container 1 from datanode e2733c00-162b-4993-a986-f6104f5008d8{ip: 172.18.0.2, host: 4f4e683d86c3} {}
scm_1           | org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #1
scm_1           | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:543)
scm_1           | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.updateContainerReplica(ContainerStateMap.java:230)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerStateManager.updateContainerReplica(ContainerStateManager.java:565)
scm_1           | at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerReplica(SCMContainerManager.java:393)
scm_1           | at org.apache.hadoop.hdds.scm.container.ReportHandlerHelper.processContainerReplica(ReportHandlerHelper.java:74)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:159)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:110)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:51)
scm_1           | at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm_1           | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
scm_1           | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
{code}
 

 ",pull-request-available,['SCM'],HDDS,Bug,Major,2019-02-21 01:04:01,2
13217011,Adding container related metrics in SCM,"This jira aims to add more container related metrics to SCM.
 Following metrics will be added as part of this jira:
 * Number of containers
 * Number of open containers
 * Number of closed containers
 * Number of quasi closed containers
 * Number of closing containers

Above are already handled in HDDS-918.
 * Number of successful create container calls
 * Number of failed create container calls
 * Number of successful delete container calls
 * Number of failed delete container calls

Handled in HDDS-2193.
 * Number of successful container report processing
 * Number of failed container report processing
 * Number of successful incremental container report processing
 * Number of failed incremental container report processing

These will be handled in this jira.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-20 19:25:12,2
13216977,Add optional web server to the Ozone freon test tool,"Recently we improved the default HttpServer to support prometheus monitoring and java profiling.

It would be very useful to enable the same options for freon testing:

 1. We need a simple way to profile freon and check the problems

 2. Long running freons should be monitored

We can create a new optional FreonHttpServer which includes all the required servlets by default.",pull-request-available,['Tools'],HDDS,Improvement,Major,2019-02-20 16:49:29,1
13216932,Ensure stateMachineData to be evicted only after writeStateMachineData completes in ContainerStateMachine cache,"Currently, when we write StateMachineData, we first write to cache followed by write to disk. The entry in the cache can get evicted while the actual write is happening in case write is very slow. The purpose of this Jira is to ensure the cache eviction only after writeChunk completes.",TriagePending,['Ozone Datanode'],HDDS,Improvement,Major,2019-02-20 13:57:07,3
13216742,Update DBCheckpointSnapshot to DBCheckpoint,"Instead of DBCheckpointSnapshot to DBCheckpoint, as internally we used RocksDBCheckpoint. It is little confusing to have both checkpoint and snapshot. And update similarly in other places.",pull-request-available,[],HDDS,Sub-task,Minor,2019-02-19 20:19:05,2
13216737,Fix findbugs issues caused by HDDS-1085.,The list of issues can be found here - https://ci.anzix.net/job/ozone/106/findbugs/.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-19 20:07:09,5
13216728,OzoneManager should return the pipeline info of the allocated block along with block info,"Currently, while a block is allocated from OM, the request is forwarded to SCM. However, even though the pipeline information is present with the OM for block allocation, this information is passed through to the client.

This optimization will help in reducing the number of hops for the client by reducing 1 RPC round trip for each block allocated.",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Bug,Major,2019-02-19 19:13:19,4
13216724,Add metric counters to capture the RocksDB checkpointing statistics.,"As per the discussion with [~anu] on HDDS-1085, this JIRA tracks the effort to add metric counters to capture ROcksDB checkpointing performance. 

From [~anu]'s comments, it might be interesting to have 3 counters – or a map of counters.
* How much time are we taking for each CheckPoint
* How much time are we taking for each Tar operation – along with sizes
* How much time are we taking for the transfer.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-02-19 19:09:13,5
13216571,Create robot test for Ozone TDE support,"HDDS-1041 implemented TDE for Ozone and added the KMS server to the compose/ozonesecure cluster definition.

We need a simple robot framework based test to try out TDE from command line.

This task requires a working ozonesecure docker-compose cluster first.",beta1 newbie,[],HDDS,Test,Major,2019-02-19 09:39:11,4
13216440,Fix findbug/checkstyle errors in hadoop-hdds projects,"HDDS-1114 fixed all the findbug/checkstyle problems but in the mean time new patches are committed with newer error.

Here I would like to cleanup the projects again.

(Except the static field in RatisPipelineProvider which will be ignored in this patch and tracked in HDDS-1128)",pull-request-available,[],HDDS,Bug,Major,2019-02-18 17:39:16,1
13216066,Add a config to disable checksum verification during read even though checksum data is present in the persisted data,"Currently, if the checksum is computed during data write and persisted in the disk, we will always end up verifying it while reading. This Jira aims to selectively disable checksum verification during reads even though checksum info is present in the data stored.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-02-15 19:57:20,2
13216050,OM get the certificate from SCM CA for token validation,This is needed when the OM received delegation token signed by other OM instances and it does not have the certificate for foreign OM.,newbie,[],HDDS,Sub-task,Major,2019-02-15 18:40:00,4
13216000,Add async profiler to the hadoop-runner base container image,"HDDS-1116 provides a simple servlet to execute async profiler (https://github.com/jvm-profiling-tools/async-profiler) thanks to the Hive developers.

To run it in the docker-composed based example environments we should add it to the apache/hadoop-runner base image. 

Note: The size is not significant, the downloadable package is 102k.

",pull-request-available,['docker'],HDDS,Improvement,Major,2019-02-15 14:45:40,1
13215998,Add java profiler servlet to the Ozone web servers,"Thanks to [~gopalv] we learned that [~prasanth_j] implemented a helper servlet in Hive to initialize new [async profiler|https://github.com/jvm-profiling-tools/async-profiler] sessions and provide the svg based flame graph over HTTP. (see HIVE-20202)

It seems to very useful as with this approach the profiling could be very easy.

This patch imports the servlet from the Hive code base to the Ozone code base with minor modification (to make it work with our servlet containers)

 * The two servlets are unified to one
 * Streaming the svg to the browser based on IOUtils.copy 
 * Output message is improved

By default the profile servlet is turned off, but you can enable it with 'hdds.profiler.endpoint.enabled=true' ozone-site.xml settings. In that case you can access the /prof endpoint from scm,om,s3g. 

You should upload the async profiler first (https://github.com/jvm-profiling-tools/async-profiler) and set the ASYNC_PROFILER_HOME environment variable to find it. ",pull-request-available,[],HDDS,Improvement,Major,2019-02-15 14:40:03,1
13215974,Provide ozone specific top-level pom.xml,"Ozone build process doesn't require the pom.xml in the top level hadoop directory as we use hadoop 3.2 artifacts as parents of hadoop-ozone and hadoop-hdds. The ./pom.xml is used only to include the hadoop-ozone/hadoop-hdds projects in the maven reactor.

From command line, it's easy to build only the ozone artifacts:

{code}
mvn clean install -Phdds  -am -pl :hadoop-ozone-dist  -Danimal.sniffer.skip=true  -Denforcer.skip=true
{code}

Where: '-pl' defines the build of the hadoop-ozone-dist project
and '-am' defines to build all of the dependencies from the source tree (hadoop-ozone-common, hadoop-hdds-common, etc.)

But this filtering is available only from the command line.

With providing a lightweight pom.ozone.xml we can achieve the same:

 * We can open only hdds/ozone projects in the IDE/intellij. It makes the development faster as IDE doesn't need to reindex all the sources all the time + it's easy to execute checkstyle/findbugs plugins of the intellij to the whole project.
 * Longer term we should create an ozone specific source artifact (currently the source artifact for hadoop and ozone releases are the same) which also requires a simplified pom.

In this patch I also added the .mvn directory to the .gitignore file.

With 
{code}
mkdir -p .mvn && echo ""-f ozone.pom.xml"" > .mvn/maven.config"" you can persist the usage of the ozone.pom.xml for all the subsequent builds (in the same dir)

How to test?

Just do a 'mvn -f ozonze.pom.xml clean install -DskipTests'",pull-request-available,['build'],HDDS,Improvement,Major,2019-02-15 12:35:25,1
13215973,Fix findbugs/checkstyle/accepteance errors in Ozone,"Unfortunately as the previous two big commits (error handling HDDS-1068, checkstyle HDDS-1103) are committed in the same time a few new errors are introduced during the rebase.

This patch will fix the remaining 5 issues (+ a type in the acceptance test executor) ",pull-request-available,[],HDDS,Bug,Major,2019-02-15 12:25:07,1
13215917,Remove default dependencies from hadoop-ozone project,"There are two ways to define common dependencies with maven:

  1.) put all the dependencies to the parent project and inherit them
  2.) get all the dependencies via transitive dependencies

TLDR; I would like to switch from 1 to 2 in hadoop-ozone

My main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.

Using transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them. ",pull-request-available,['build'],HDDS,Improvement,Major,2019-02-15 08:57:06,1
13215851,OzoneManager NPE reading private key file.,"{code}

ozoneManager_1  | 2019-02-14 23:21:51 ERROR OzoneManager:596 - Unable to read key pair for OM.

ozoneManager_1  | org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:638)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:594)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManagerIfNecessary(OzoneManager.java:1216)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.start(OzoneManager.java:1007)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:768)

ozoneManager_1  | Caused by: java.lang.NullPointerException

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:635)

ozoneManager_1  | ... 4 more

ozoneManager_1  | 2019-02-14 23:21:51 ERROR OzoneManager:772 - Failed to start the OzoneManager.

ozoneManager_1  | java.lang.RuntimeException: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:597)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManagerIfNecessary(OzoneManager.java:1216)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.start(OzoneManager.java:1007)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:768)

ozoneManager_1  | Caused by: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:638)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:594)

ozoneManager_1  | ... 3 more

ozoneManager_1  | Caused by: java.lang.NullPointerException

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:635)

ozoneManager_1  | ... 4 more

ozoneManager_1  | 2019-02-14 23:21:51 INFO  ExitUtil:210 - Exiting with status 1: java.lang.RuntimeException: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | 2019-02-14 23:21:51 INFO  OzoneManager:51 - SHUTDOWN_MSG: 

{code}",test-badlands,[],HDDS,Sub-task,Major,2019-02-14 23:26:15,4
13215751,Add mechanism in Recon to obtain DB snapshot 'delta' updates from Ozone Manager.,"*Some context*

The FSCK server will periodically invoke this OM API passing in the most recent sequence number of its own RocksDB instance. The OM will use the RockDB getUpdateSince() API to answer this query. Since the getUpdateSince API only works against the RocksDB WAL, we have to configure OM RocksDB WAL (https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log) with sufficient max size to make this API useful. If the OM cannot get all transactions since the given sequence number (due to WAL flushing), it can error out. In that case the FSCK server can fall back to getting the entire checkpoint snapshot implemented in HDDS-1085.",pull-request-available,[],HDDS,Sub-task,Major,2019-02-14 14:21:35,5
13215734,Fix rat/findbug/checkstyle errors in ozone/hdds projects,"Due to the partial Yetus checks (see HDDS-891) recent patches and merge introduced many new checkstyle/rat/findbugs errors.

I would like to fix them all.",pull-request-available,[],HDDS,Bug,Major,2019-02-14 12:57:53,1
13215716,Confusing error log when datanode tries to connect to a destroyed pipeline,"steps taken:

--------------------
 # created 5 datanode cluster.
 # shutdown 2 datanodes
 # started the datanodes again.

One of the datanodes was shut down.

exception seen :

 
{noformat}
2019-02-14 07:37:26 INFO LeaderElection:230 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8 got exception when requesting votes: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.
 at java.util.concurrent.FutureTask.report(FutureTask.java:122)
 at java.util.concurrent.FutureTask.get(FutureTask.java:192)
 at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:214)
 at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:146)
 at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:102)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
 at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
 at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:83)
 at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:187)
 at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$0(LeaderElection.java:188)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-02-14 07:37:26 INFO LeaderElection:46 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: Election PASSED; received 1 response(s) [6a0522ba-019e-4b77-ac1f-a9322cd525b8<-61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5#0:OK-t7] and 1 exception(s); 6a0522ba-019e-4b77-ac1f-a9322cd525b8:t7, leader=null, voted=6a0522ba-019e-4b77-ac1f-a9322cd525b8, raftlog=6a0522ba-019e-4b77-ac1f-a9322cd525b8-SegmentedRaftLog:OPENED, conf=3: [61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5:172.20.0.8:9858, 6a0522ba-019e-4b77-ac1f-a9322cd525b8:172.20.0.6:9858, 0f377918-aafa-4d8a-972a-6ead54048fba:172.20.0.3:9858], old=null
2019-02-14 07:37:26 INFO LeaderElection:52 - 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.
2019-02-14 07:37:26 INFO RoleInfo:130 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: shutdown LeaderElection
2019-02-14 07:37:26 INFO RaftServerImpl:161 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8 changes role from CANDIDATE to LEADER at term 7 for changeToLeader
2019-02-14 07:37:26 INFO RaftServerImpl:258 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: change Leader from null to 6a0522ba-019e-4b77-ac1f-a9322cd525b8 at term 7 for becomeLeader, leader elected after 1066ms
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-02-14 07:37:26 INFO GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-02-14 07:37:26 INFO GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
2019-02-14 07:37:26 INFO RoleInfo:139 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: start LeaderState
2019-02-14 07:37:26 INFO RaftLogWorker:303 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8-RaftLogWorker: Rolling segment log-3_4 to index:4
2019-02-14 07:37:26 INFO RaftLogWorker:403 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8-RaftLogWorker: Rolled log segment from /data/metadata/ratis/134f574e-c1b0-4556-a206-fd6fa533f1fb/current/log_inprogress_3 to /data/metadata/ratis/134f574e-c1b0-4556-a206-fd6fa533f1fb/current/log_3-4
2019-02-14 07:37:26 INFO RaftServerImpl:354 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: set configuration 5: [61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5:172.20.0.8:9858, 6a0522ba-019e-4b77-ac1f-a9322cd525b8:172.20.0.6:9858, 0f377918-aafa-4d8a-972a-6ead54048fba:172.20.0.3:9858], old=null at 5
/opt/starter.sh: line 162: 13 Killed $@
 
 
{noformat}
 ",TriagePending newbie pushed-to-craterlake test-badlands,['Ozone Datanode'],HDDS,Bug,Critical,2019-02-14 10:32:29,3
13215258,Configuration tab in OM/SCM ui is not displaying the correct values,"Configuration tab in OM/SCM ui is not displaying the correct/configured values, rather it is displaying the default values.

!image-2019-02-12-19-47-18-332.png!
{code:java}
[hdfs@freonnode10 hadoop]$ curl -s http://freonnode10:9874/conf | grep ozone.om.handler.count.key
<property><name>ozone.om.handler.count.key</name><value>40</value><final>false</final><source>ozone-site.xml</source></property>
{code}",pull-request-available,"['Ozone Manager', 'SCM']",HDDS,Bug,Critical,2019-02-12 14:19:31,6
13215248,Use Java 11 JRE to run Ozone in containers,"As of now we use opendk 1.8.0 in the Ozone containers.

Java 9 and Java 10 introduces advanced support for the resource management of the containers and not all of them are available from the latest release of 1.8.0. (see this blog for more details: https://medium.com/adorsys/jvm-memory-settings-in-a-container-environment-64b0840e1d9e)

I propose to switch to use Java 11 in the containers and test everything with Java 11 at runtime.

Note: this issue is just about the runtime jdk not about the compile time JDK.",pull-request-available,[],HDDS,Sub-task,Major,2019-02-12 13:23:14,1
13215227,Disable OzoneFSStorageStatistics for hadoop versions older than 2.8,"HDDS-1033 introduced OzoneFSStorageStatistics for OzoneFileSystem. It uses the StorageStatistics which is introduced in HADOOP-13065 (available from the hadoop2.8/3.0).

Using older hadoop (for example hadoop-2.7 which is included in the spark distributions) is not possible any more even with using the isolated class loader (introduced in HDDS-922).

Fortunately it can be fixed:
 # We can support null in storageStatistics field with checking everywhere before call it.
 # We can create a new constructor of OzoneClientAdapterImpl without using OzoneFSStorageStatistics): If OzoneFSStorageStatistics is not in the method/constructor signature we don't need to load it.
 # We can check the availability of HADOOP-13065 and if the classes are not in the classpath we can skip the initialization of the OzoneFSStorageStatistics",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2019-02-12 12:35:26,1
13215141,Fix TestDefaultCertificateClient#testSignDataStream,"Investigate failure of TestDefaultCertificateClient#testSignDataStream in jenkins run. 

https://builds.apache.org/job/PreCommit-HDDS-Build/2217/testReport/org.apache.hadoop.hdds.security.x509.certificate.client/TestDefaultCertificateClient/testSignDataStream/

",blocker pull-request-available security,[],HDDS,Sub-task,Major,2019-02-12 03:05:59,4
13215120,Create an OM API to serve snapshots to Recon server,"We need to add an API to OM so that we can serve snapshots from the OM server.
 - The snapshot should be streamed to fsck server with the ability to throttle network utilization (like TransferFsImage)",pull-request-available,[],HDDS,Sub-task,Major,2019-02-11 23:45:23,5
13214494,Implement RetryProxy and FailoverProxy for OM client,"RPC Client should implement a retry and failover proxy provider to failover between OM Ratis clients. The failover should occur in two scenarios:
# When the client is unable to connect to the OM (either because of network issues or because the OM is down). The client retry proxy provider should failover to next OM in the cluster.
# When OM Ratis Client receives a response from the Ratis server for its request, it also gets the LeaderId of server which processed this request (the current Leader OM nodeId). This information should be propagated back to the client. The Client failover Proxy provider should failover to the leader OM node. This helps avoid an extra hop from Follower OM Ratis Client to Leader OM Ratis server for every request.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-02-08 00:06:52,7
13214365,Temporarily disable the security acceptance tests by default in Ozone,"Because HDDS-1019 and HDDS-1018 (and HDDS-1038?) the security acceptance tests are not working. 

I propose to remove them from daily tests as currently we can't get any meaningful results from Jenkins.

Security tests can be run manually with the test.sh script.",pull-request-available,"['Security', 'test']",HDDS,Improvement,Major,2019-02-07 13:19:31,1
13214364,Improve the error propagation for ozone sh,"As of now the server side (om, scm) errors are not propagated to the client.

For example if ozone is started with one single datanode:

{code}
docker-compose exec ozoneManager ozone sh key  put -r THREE /vol1/bucket1/test2 NOTICE.txt             
Create key failed, error:KEY_ALLOCATION_ERROR
{code}

There is no information here about the missing datanodes, or missing pipelines.

There are multiple problems which should be fixed:

1. type safety

In ScmBlockLocationProtocolClientSideTranslatorPB the server (om) side exceptions are transformed to IOException where the original status is added to the message: 

For example:

{code}
 throw new IOException(""Volume quota change failed, error:"" + resp.getStatus());
{code}

In s3 gateway it's very hard to handle the different errors in a proper way. The current code:

{code}
if (!ex.getMessage().contains(""KEY_NOT_FOUND"")) {
            result.addError(
                new Error(keyToDelete.getKey(), ""InternalError"",
                    ex.getMessage()));
{code}

2. message

The exception message is not propagated in the om response just the status code

3. status code and error message are handled in a different way

To propagate error code and status code to the client we need to handle them in the same way.  But the Status field is part of the specific response objects (CreateVolumeRequest) and not the OMRequest. I propose to put both StatusCode and error message to the OMRequest.

4. The status codes in OzoneManagerProtocol.proto/Status enum is not in sync with OmException.ResultCodes.

It would be easy to use the same strings for both enums. With a unit test we can ensure that they have the same names in the same order.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-02-07 13:11:21,1
13214063,List Multipart uploads in a bucket,"This Jira is to implement in ozone to list of in-progress multipart uploads in a bucket.

[https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html]

 ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-02-06 02:39:09,1
13213646,TestCloseContainerByPipeline#testIfCloseContainerCommandHandlerIsInvoked fails intermittently," 
{code:java}
java.lang.StackOverflowError
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.getSubject(Subject.java:297)
at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:569)
at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getEncodedBlockToken(ContainerProtocolCalls.java:578)
at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:318)
at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:602)
at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:464)
at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:480)
at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:137)
at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:489)
at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:501)
at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:501)
{code}
The failure is happening because, ozone client receives a CONTAINER_NOT+OPEN exception from daranode, and it allocates a new and retries to write. But every allocate block call to SCM allocates a block on the same quasi closed container and hence client retries indefinitely and ultimately runs out of stack space.

Logs below indicate 3 successive block allocations from SCM in quasi closed container.
{code:java}
15:15:26.812 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189852894070 bcId: 0} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state

15:15:26.818 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189853352823 bcId: 0} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state

15:15:26.825 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189853746040 bcId: 0} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state
{code}
 

 

 ",newbie,['SCM'],HDDS,Improvement,Major,2019-02-04 09:57:07,3
13213168,Support Service Level Authorization for Ozone,In a secure Ozone cluster. Datanodes fail to connect to SCM on {{StorageContainerDatanodeProtocol}}. ,Security,[],HDDS,Sub-task,Major,2019-01-31 20:04:43,4
13212686,Allow option for force in DeleteContainerCommand,"Right now, we check container state if it is not open, and then we delete container.

We need a way to delete the containers which are open, so adding a force flag will allow deleting a container without any state checks. (This is required for delete replica's when SCM detects over-replicated, and that container to delete can be in open state)",pull-request-available,[],HDDS,Bug,Major,2019-01-30 00:56:16,2
13212367,Handle replication of closed containers in DeadNodeHanlder,"This Jira is to do one of the TODO mentioned in the DeadNodeHandler

// TODO: Check replica count and call replication manager.

 

As Right now, when a node is dead, replication for the closed containers is not triggered.",pull-request-available,[],HDDS,Bug,Major,2019-01-29 01:11:29,2
13212335,Handle DeleteContainerCommand in the SCMDatanodeProtocolServer,"Right now, in the SCMDatanodeProtocolServer getCommandResponse() deleteContainerCommand is not handled, so deleteContainerCommand is not sent to Datanode.

 

The deletecontainercommand request is sent for over replicated containers, so this over replication is currently broken because of this.

 

Because of this we see below error:

 
{code:java}
java.lang.IllegalArgumentException: Not implemented
 at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.getCommandResponse(SCMDatanodeProtocolServer.java:345)
 at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.sendHeartbeat(SCMDatanodeProtocolServer.java:272)
 at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolServerSideTranslatorPB.sendHeartbeat(StorageContainerDatanodeProtocolServerSideTranslatorPB.java:88)
 at org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos$StorageContainerDatanodeProtocolService$2.callBlockingMethod(StorageContainerDatanodeProtocolProtos.java:27753)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}
 ",SCM,[],HDDS,Bug,Major,2019-01-28 21:50:39,2
13212235,Update the base image of krb5 container for the secure ozone cluster ,"The krb5 image based on a base image which is no longer available from dockerhub

See: hadoop-ozone/dist/src/main/compose/ozonesecure/docker-image/docker-krb5/Dockerfile-krb5

Dockerfile:
FROM frolvlad/alpine-oraclejdk8:slim

https://hub.docker.com/r/frolvlad/alpine-oraclejdk8

I propose to switch to openjdk:8u191-jdk-alpine3.8 but it should be tested with the acceptance test suite (A java base image is required as the issuer application creates keystores/truststores on the fly with the help of java keytool.",newbie,[],HDDS,Bug,Major,2019-01-28 13:14:59,4
13212232,Use distributed tracing to indentify performance problems in Ozone,"In the recent months multiple performance issues are resolved in OM/SCM and datanode sides. To identify the remaining problems a distributed tracing framework would help a lot.

In HADOOP-15566 there is an ongoing discussion to remove the discontinued HTrace and use something else instead. Until now without any conclusion, but

 1). There is one existing poc in the jira which uses opentracing
 2). It was suggested to ""evaluating all the options"" before a final decision 

As an evaluation step we would like to investigate the performance of ozone components with opentracing. This patch can help us to find the performance problem but can be reverted when we will have a final solution in HADOOP-15566 about the common tracing library.

To make it lightweight we can use the ozone message level tracing identifier for context propagation instead of modifying the existing hadoop rpc framework. 

",pull-request-available,[],HDDS,Sub-task,Major,2019-01-28 12:51:22,1
13210490,MultipartUpload: S3API for list parts of a object,"https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html
",pull-request-available,[],HDDS,Sub-task,Major,2019-01-18 22:52:40,2
13208957,Exclude dependency-reduced-pom.xml from ozone rat check,"As of now we have one (false positive) rat violation:

 
{code:java}
hadoop-ozone/ozonefs/target/rat.txt: !????? /home/elek/projects/hadoop/hadoop-ozone/ozonefs/dependency-reduced-pom.xml
{code}
As it's generated during the build, it could be safely ignored.",pull-request-available,[],HDDS,Bug,Trivial,2019-01-11 09:32:59,1
13208874,Add getServiceAddress method to ServiceInfo and use it in TestOzoneShell,"This jira has been filed based on [~ajayydv]'s [review comment |https://issues.apache.org/jira/browse/HDDS-960?focusedCommentId=16739807&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16739807]on HDDS-960

1. Add a method getServiceAddress(ServicePort port) in ServiceInfo
2. Use this method in TestOzoneShell in place of following snippet:

{code:java}
String omHostName = services.stream().filter(
        a -> a.getNodeType().equals(HddsProtos.NodeType.OM))
        .collect(Collectors.toList()).get(0).getHostname();
{code}
",newbie,[],HDDS,Improvement,Major,2019-01-10 22:12:25,0
13207970,Ozone: checkstyle improvements and code quality scripts,Experimental scripts to test github pr capabilities after the github url move. The provided scripts are easier to use locally and provides more strict/focused checks then the existing pre-commit scripts. But this is not a replacements of the existing yetus build as it adds additional (more strict) checks. ,pull-request-available,[],HDDS,New Feature,Major,2019-01-07 08:36:16,1
13207339,MultipartUpload: List Parts for a Multipart upload key,"This Jira is to implement backend to support API in S3 for list parts for an object.

https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html",pull-request-available,[],HDDS,Sub-task,Major,2019-01-02 19:54:24,2
13204046,Create isolated classloder to use ozonefs with any older hadoop versions,"As of now we create a shaded ozonefs artifact which includes all the required class files to use ozonefs (Hadoop compatible file system for Ozone)

But the shading process of this artifact is very easy, it includes all the class files but no relocation rules (package name renaming) are configured. With this approach ozonefs can be used from the compatible hadoop version (this is hadoop 3.1 only, I guess) but can't be used with any older hadoop version as it requires the newer version of hadoop-common.

I tried to configure a full shading (with relocation) but it's not a simple task. For example a pure (non-relocated) Configuration is required by the ozonefs itself, but an other, newer Configuration class is required by the ozone client code which is a dependency of OzoneFileSystem So we need a relocated and a non-relocated class in the same time.

I tried out a different approach: I moved out all of the ozone specific classes from the OzoneFileSystem to an adapter class (OzoneClientAdapter). In case of an older hadoop version the adapter class itself can be loaded with an isolated classloader. The isolated classloader can load all the required classes from the jar file from a specific path. It doesn't require any specific package relocation as the default class loader doesn't load these classes. 

The OzoneFileSystem (in case of older hadoop version) can load the adapter with the isolated classloader and only a few classes should be shared between the normal and isolated classloader (the interface of the adapter and the types in the method signatures). All of the other ozone classes and the newer hadoop dependencies will be hidden by the isolated classloader.

This patch is more like a proof of concept, I would like to start a discussion about this approach. I successfully used the generated artifact to use ozonefs from spark 2.4 default distribution (which includes hadoop 2.7). 

For a final patch I would add some check to use the ozonefs without any classpath separation by default. (could be configured or chosen by automatically)


For using spark (+ hadoop 2.7 + kubernetes scheduler) together with ozone, you can check this screencast: https://www.youtube.com/watch?v=cpRJcSHIEdM&t=8s
",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2018-12-12 17:05:11,1
13203964,Enable prometheus endpoint for Ozone S3 gateway,"HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.

Unfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone S3G daemon in the default web server. ",newbie,[],HDDS,Bug,Major,2018-12-12 10:57:28,5
13203962,Enable prometheus endpoints for Ozone datanodes,"HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.

Unfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone/HDDS datanodes on the web server of the HDDS Rest endpoint. ",pull-request-available,[],HDDS,Bug,Major,2018-12-12 10:56:19,1
13202780,Display the ozone version on SCM/OM web ui instead of Hadoop version,"SCM and OM web uis (http://localhost:9876 and http://localhost:9874) display the actual version but the displayed version is the version of the hadoop dependencies:

This is provided by the org.apache.hadoop.hdds.server.ServiceRuntimeInfoImpl which is a default implementation of ServiceRuntimeInfo. (Both OzoneManager and StorageContainerManager extend this class).

We need to use OzoneVersionInfo and HddsVersionInfo classes to display the actual version instead of org.apache.hadoop.util.VersionInfo.

",newbie,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2018-12-06 11:32:46,0
13202769,Create informative landing page for Ozone S3 gateway ,"{color:#FF0000}{color}

As of now the main s3g endpoint (such as [http://localhost:9878|http://localhost:9878/]) returns with HTTP 500 if it's opened from the browser.

The main endpoint is used to get all the available buckets, but amazon returns with a redirect IF the Authorization header is missing:
{code:java}
 curl -v s3.us-east-2.amazonaws.com
*   Trying 52.219.88.59...
* TCP_NODELAY set
* Connected to s3.us-east-2.amazonaws.com (52.219.88.59) port 80 (#0)
> GET / HTTP/1.1
> Host: s3.us-east-2.amazonaws.com
> User-Agent: curl/7.62.0
> Accept: */*
> 
< HTTP/1.1 307 Temporary Redirect
< x-amz-id-2: fq8RXJdSlVo8PqidHaP8XXczMfLSEAt5Tm4JP98atilWRjalMvqtPa6mwq6rEIXz4cCPrPqJkO4=
< x-amz-request-id: 5C6ACE6D6FC273B9
< Date: Thu, 06 Dec 2018 11:16:36 GMT
< Location: https://aws.amazon.com/s3/
< Content-Length: 0
{code}
I propose to do the same for Ozone:

1.) If the authorization header is missing on the root URL, redirect to an internal page.
 2.) Create an internal landing page at [http://localhost:9878/_ozone] with the following content:
 a) A very short introduction to use the endpoint (with aws client)
 b) The actual documentation of ozone (which is also included in the scm/om ui)

Note: we need an url schema which is not conflicting with the real REST requests. As the bucket and volume names should not contain underscore in ozone, we can use it to prefix all the urls:
 * [http://localhost:9878/_ozone] --> landing page
 * [http://localhost:9878/_ozone/(css]|js) --> required resources
 * [http://localhost:9878/_ozone/docs] --> Documentation with the required resources.",newbie pull-request-available,['S3'],HDDS,Sub-task,Major,2018-12-06 11:23:20,1
13199715,Fix NPE ServerUtils#getOzoneMetaDirPath,"This can be reproed with ""mvn test"" under hadoop-ozone project but not with individual test run under IntelliJ.

 
{code:java}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.33 s <<< FAILURE! - in org.apache.hadoop.ozone.TestOmUtils

testNoOmDbDirConfigured(org.apache.hadoop.ozone.TestOmUtils)  Time elapsed: 0.028 s  <<< FAILURE!

java.lang.AssertionError:

 

Expected: an instance of java.lang.IllegalArgumentException

     but: <java.lang.NullPointerException> is a java.lang.NullPointerException

Stacktrace was: java.lang.NullPointerException

        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)

        at org.apache.hadoop.hdds.server.ServerUtils.getOzoneMetaDirPath(ServerUtils.java:130)

        at org.apache.hadoop.ozone.OmUtils.getOmDbDir(OmUtils.java:141)

        at org.apache.hadoop.ozone.TestOmUtils.testNoOmDbDirConfigured(TestOmUtils.java:89)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:498)

        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)

        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)

        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)

        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

 

{code}",test-badlands,[],HDDS,Bug,Minor,2018-11-20 23:47:39,4
13199305,Provide official apache docker image for Ozone,"Similar to the apache/hadoop:2 and apache/hadoop:3 images I propose to provide apache/ozone docker images which includes the voted release binaries.

The image can follow all the conventions from HADOOP-14898

1. BRANCHING

I propose to create new docker branches:

docker-ozone-0.3.0-alpha
docker-ozone-latest

And ask INFRA to register docker-ozone-(.*) in the dockerhub to create apache/ozone: images

2. RUNNING

I propose to create a default runner script which starts om + scm + datanode + s3g all together. With this approach you can start a full ozone cluster as easy as

{code}
docker run -p 9878:9878 -p 9876:9876 -p 9874:9874 -d apache/ozone
{code}

That's all. This is an all-in-one docker image which is ready to try out.

3. RUNNING with compose

I propose to include a default docker-compose + config file in the image. To start a multi-node pseudo cluster it will be enough to execute:

{code}
docker run apache/ozone cat docker-compose.yaml > docker-compose.yaml
docker run apache/ozone cat docker-config > docker-config
docker-compose up -d
{code}

That's all, and you have a multi-(pseudo)node ozone cluster which could be scaled up and down with ozone.

4. k8s

Later we can also provide k8s resource files with the same approach:

{code}
docker run apache/ozone cat k8s.yaml | kubectl apply -f -
{code}",Triaged,[],HDDS,Bug,Major,2018-11-19 11:47:52,1
13195865,Support custom key/value annotations on volume/bucket/key,"I propose to add a custom Map<String,String> annotation field to objects/buckets and keys in Ozone Manager.

It would enable to build any extended functionality on top of the OM's generic interface. For example:

 * Support tags in Ozone S3 gateway (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html)
 * Support md5 based ETags in s3g
 * Store s3 related authorization data (ACLs, policies) together with the parent objects

As an optional feature (could be implemented later) the client can defined the exposed annotations. For example s3g can defined which annotations should be read from rocksdb on OM side and sent the the client (s3g)",pull-request-available,['Ozone Manager'],HDDS,New Feature,Major,2018-11-02 10:22:55,1
13195193,Run S3 smoke tests with replication STANDARD.,"This Jira is created from the comment from [~elek]

1. I think sooner or later we need to run ozone tests with real replication. We can add a 'scale up' to the hadoop-ozone/dist/src/main/smoketest/test.sh
{code:java}
docker-compose -f ""$COMPOSE_FILE"" down
docker-compose -f ""$COMPOSE_FILE"" up -d
docker-compose -f ""$COMPOSE_FILE"" scale datanode=3
{code}
And with this modification we don't need the '--storage-class REDUCED_REDUNDANCY'. (But we can do it in separated jira)",newbie pull-request-available,['test'],HDDS,Sub-task,Major,2018-10-30 18:25:46,1
13195020,Create S3 subcommand to run S3 related operations,"This Jira is added to create S3 subcommand, which will be used for all S3 related operations.
Under this jira, move the command ozone sh bucket <<bucketname>> to ozone s3 bucket <<bucketname>>",pull-request-available,[],HDDS,Sub-task,Major,2018-10-30 03:21:28,6
13194160,Removing REST protocol support from OzoneClient,"Since we have functional {{S3Gateway}} for Ozone which works on REST protocol, having REST protocol support in OzoneClient feels redundant and it will take a lot of effort to maintain it up to date.
As S3Gateway is in a functional state now, I propose to remove REST protocol support from OzoneClient.

Once we remove REST support from OzoneClient, the following will be the interface to access Ozone cluster
 * OzoneClient (RPC Protocol)
 * OzoneFS (RPC Protocol)
 * S3Gateway (REST Protocol)",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2018-10-25 14:46:06,1
13192396,Incorrect creation time for files created by o3fs.,Files created by o3fs show creation timestamp as unix epoch.,app-compat,['Ozone Filesystem'],HDDS,Improvement,Blocker,2018-10-17 23:13:45,7
13192046,Fix OzoneFS directory rename," 

Renaming a directory within the same parent directory fails with the exception:
{code:java}
Unable to move: o3://bucket2.volume2/testo3/.hive-staging_hive_2018-10-16_21-09-35_130_1001829123585250245-1/_tmp.-ext-10000 to: o3://bucket2.volume2/testo3/.hive-staging_hive_2018-10-16_21-09-35_130_1001829123585250245-1/_tmp.-ext-10000.moved
{code}
Detailed exception in comment below.",app-compat,[],HDDS,Bug,Blocker,2018-10-16 21:19:36,7
13191737,Creating hive table on Ozone fails,"Modified HIVE_AUX_JARS_PATH to include Ozone jars. Tried creating Hive external table on Ozone. It fails with ""Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)""
{code:java}
-bash-4.2$ beeline
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
Enter username for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:
Enter password for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:
18/10/15 21:36:55 [main]: INFO jdbc.HiveConnection: Connected to ctr-e138-1518143905142-510793-01-000004.hwx.site:10000
Connected to: Apache Hive (version 3.1.0.3.0.3.0-63)
Driver: Hive JDBC (version 3.1.0.3.0.3.0-63)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.0.3.0.3.0-63 by Apache Hive
0: jdbc:hive2://ctr-e138-1518143905142-510793> create external table testo3 ( i int, s string, d float) location ""o3://bucket2.volume2/testo3"";
Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)
0: jdbc:hive2://ctr-e138-1518143905142-510793> {code}
 ",Triaged app-compat,['documentation'],HDDS,Bug,Major,2018-10-15 21:50:45,7
13191261,Rename dfs.container.ratis.num.container.op.threads,"public static final String DFS_CONTAINER_RATIS_NUM_CONTAINER_OP_EXECUTORS_KEY
 = ""dfs.container.ratis.num.container.op.threads"";

This should be changed to dfs.container.ratis.num.container.op.executors

 

HDDS-550 has added this in OzoneConfigKeys.java, but they have named differently in ozone-default.xml and ScmConfigKeys.java

 

Because of this TestOzoneConfigurationFields.java is failing

[https://builds.apache.org/job/PreCommit-HDDS-Build/1378/testReport/]

 ",newbie,[],HDDS,Bug,Minor,2018-10-12 17:35:27,2
13190786,"On SCM UI, Node Manager info is empty","Fields like below are empty

Node Manager: Minimum chill mode nodes 
Node Manager: Out-of-node chill mode 
Node Manager: Chill mode status 
Node Manager: Manual chill mode

Please see attached screenshot !Screen Shot 2018-10-10 at 4.19.59 PM.png!",pull-request-available,[],HDDS,Bug,Major,2018-10-10 23:22:45,1
13190304,Mapreduce example fails with java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character,"Set up a hadoop cluster where ozone is also installed. Ozone can be referenced via o3://xx.xx.xx.xx:9889
{code:java}
[root@ctr-e138-1518143905142-510793-01-000002 ~]# ozone sh bucket list o3://xx.xx.xx.xx:9889/volume1/
2018-10-09 07:21:24,624 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[ {
""volumeName"" : ""volume1"",
""bucketName"" : ""bucket1"",
""createdOn"" : ""Tue, 09 Oct 2018 06:48:02 GMT"",
""acls"" : [ {
""type"" : ""USER"",
""name"" : ""root"",
""rights"" : ""READ_WRITE""
}, {
""type"" : ""GROUP"",
""name"" : ""root"",
""rights"" : ""READ_WRITE""
} ],
""versioning"" : ""DISABLED"",
""storageType"" : ""DISK""
} ]
[root@ctr-e138-1518143905142-510793-01-000002 ~]# ozone sh key list o3://xx.xx.xx.xx:9889/volume1/bucket1
2018-10-09 07:21:54,500 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[ {
""version"" : 0,
""md5hash"" : null,
""createdOn"" : ""Tue, 09 Oct 2018 06:58:32 GMT"",
""modifiedOn"" : ""Tue, 09 Oct 2018 06:58:32 GMT"",
""size"" : 0,
""keyName"" : ""mr_job_dir""
} ]
[root@ctr-e138-1518143905142-510793-01-000002 ~]#{code}
Hdfs is also set fine as below
{code:java}
[root@ctr-e138-1518143905142-510793-01-000002 ~]# hdfs dfs -ls /tmp/mr_jobs/input/
Found 1 items
-rw-r--r-- 3 root hdfs 215755 2018-10-09 06:37 /tmp/mr_jobs/input/wordcount_input_1.txt
[root@ctr-e138-1518143905142-510793-01-000002 ~]#{code}
Now try to run Mapreduce example job against ozone o3:
{code:java}
[root@ctr-e138-1518143905142-510793-01-000002 ~]# /usr/hdp/current/hadoop-client/bin/hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /tmp/mr_jobs/input/ o3://xx.xx.xx.xx:9889/volume1/bucket1/mr_job_dir/output
18/10/09 07:15:38 INFO conf.Configuration: Removed undeclared tags:
java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : :
at org.apache.hadoop.hdds.scm.client.HddsClientUtils.verifyResourceName(HddsClientUtils.java:143)
at org.apache.hadoop.ozone.client.rpc.RpcClient.getVolumeDetails(RpcClient.java:231)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)
at com.sun.proxy.$Proxy16.getVolumeDetails(Unknown Source)
at org.apache.hadoop.ozone.client.ObjectStore.getVolume(ObjectStore.java:92)
at org.apache.hadoop.fs.ozone.OzoneFileSystem.initialize(OzoneFileSystem.java:121)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3354)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(FileOutputFormat.java:178)
at org.apache.hadoop.examples.WordCount.main(WordCount.java:85)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
18/10/09 07:15:39 INFO conf.Configuration: Removed undeclared tags:
[root@ctr-e138-1518143905142-510793-01-000002 ~]#
{code}",app-compat test-badlands,['documentation'],HDDS,Bug,Blocker,2018-10-09 07:25:25,7
13189703,ContainerStateMachine should fail subsequent transactions per container in case one fails,"ContainerStateMachine will keep of track of the last successfully applied transaction index and on restart inform Ratis the index, so that the subsequent transactions can be reapplied from here.

Moreover, in case one transaction fails, all the subsequent transactions on the container should fail in the containerStateMachine and a container close action to SCM needs to be initiated to close the container.",recovery,[],HDDS,Bug,Major,2018-10-05 12:32:59,3
13189331,Add proto changes required for CopyKey support in ozone,"This Jira is the starter Jira to make changes required for copy key request in S3 to support copy key across the bucket. In ozone world, this is just a metadata change. This Jira is created to just change .proto file for copy key request support.

 

This Jira is created to provide a copy object API in RpcClient to copy an object which already exists in ozone to same/another bucket.

[https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html]",S3,[],HDDS,New Feature,Major,2018-10-04 00:05:14,2
13188594,Remove volume name parsing from VirtualHostStyleFilter,"UPDATE:  Since the creation of this jira the volume name is removed from the url. So the original goal is simplified: the only task is to remove the volume from the VirtualHostStyleFilter parsing part.

 ORIGINAL description:

In VirtualHost Style filter parse bucketname from the HttpHeader Host if it is virtual host style request. Remove the volume parsing code.

This Jira is created from [~elek] comments on HDDS-525 jira.",newbie,['S3'],HDDS,Sub-task,Major,2018-10-01 17:19:31,2
13187717,Update the acceptance test location mentioned in ozone document,"This is found during the release verification of 0.2.1. In the ""Building from Souces"" page:

"" please follow the instructions in the *README.md* in the 

{{$hadoop_src/hadoop-ozone/acceptance-test""}}

 

{{The correct location should be }}

{{""$hadoop_src/}}hadoop-ozone/dist/target/$hdds_version/smoketest""",newbie,[],HDDS,Bug,Major,2018-09-26 20:28:49,4
13186260,Implement DeleteObject REST endpoint,"Simple delete Object call.

Implemented by HDDS-444 without the acceptance tests.

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:44:43,1
13186258,Implement PutBucket REST endpoint,"The create bucket creates a bucket using createS3Bucket which has been added as part of HDDS-577.

[https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html]

Stub implementation is created as part of HDDS-444. Need to finalize, check the missing headers, add acceptance tests.",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:38:32,2
13186257,Implement DeleteBucket REST endpoint,"The delete bucket will do the opposite of the create bucket call. It will locate the volume via the username in the delete call.

Reference is here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html

This is implemented as part of HDDS-444 but we need the double check the headers and add acceptance tests.",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:35:16,2
13186256,Implement HeadBucket REST endpoint,"This operation is useful to determine if a bucket exists and you have permission to access it. The operation returns a 200 OK if the bucket exists and you have permission to access it. Otherwise, the operation might return responses such as 404 Not Found and 403 Forbidden.  

See the reference here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:34:05,2
13186250,Implement CopyObject REST endpoint,"The Copy object is a simple call to Ozone Manager.  This API can only be done after the PUT OBJECT Call.

This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.

If the Put Object call has this header, then Put Object call will issue a rename. 

Work Items or JIRAs
Detect the presence of the extra header - x-amz-copy-source
Make sure that destination bucket exists.

The AWS reference is here:

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html

(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody would be interested, I can be more specific, explain what we need or help).",newbie,['S3'],HDDS,Sub-task,Major,2018-09-20 07:24:10,2
13186248,Implement GetObject REST endpoint,"The goal is implement the GetObject endpoint for the S3 Gateway:

There reference doc is here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html

This is mostly just a simple stream copy between the OzoneClient and the output, but we also need to:

1. support rest-response-* headers
2. Support Range headers (Jetty may support it out-of the box)

(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody interested I can be more specific).",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:21:40,1
13185918,OzoneManager HA,"OzoneManager can be a single point of failure in an Ozone cluster. We propose an HA implementation for OM using Ratis (Raft protocol).

Attached the design document for the proposed implementation.",Triaged,"['OM HA', 'Ozone Manager']",HDDS,New Feature,Major,2018-09-18 22:08:32,7
13184931,OM and SCM should use picocli to parse arguments,"SCM and OM can use the picocli to parse command-line arguments.

Suggested in HDDS-415 by [~anu].",alpha2 newbie,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2018-09-13 14:52:09,5
13184885,"PutKey failed due to error ""Rejecting write chunk request. Chunk overwrite without explicit request""","steps taken :

------------------
 # Ran Put Key command to write 50GB data. Put Key client operation failed after 17 mins.

error seen  ozone.log :

------------------------------------

 
{code}
2018-09-13 12:11:53,734 [ForkJoinPool.commonPool-worker-20] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_1 chunk stage:COMMIT_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_1 tmp chunk file
2018-09-13 12:11:56,576 [pool-3-thread-60] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 chunk stage:WRITE_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 tmp chunk file
2018-09-13 12:11:56,739 [ForkJoinPool.commonPool-worker-20] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 chunk stage:COMMIT_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 tmp chunk file
2018-09-13 12:12:21,410 [Datanode State Machine Thread - 0] DEBUG (DatanodeStateMachine.java:148) - Executing cycle Number : 206
2018-09-13 12:12:51,411 [Datanode State Machine Thread - 0] DEBUG (DatanodeStateMachine.java:148) - Executing cycle Number : 207
2018-09-13 12:12:53,525 [BlockDeletingService#1] DEBUG (TopNOrderedContainerDeletionChoosingPolicy.java:79) - Stop looking for next container, there is no pending deletion block contained in remaining containers.
2018-09-13 12:12:55,048 [Datanode ReportManager Thread - 1] DEBUG (ContainerSet.java:191) - Starting container report iteration.
2018-09-13 12:13:02,626 [pool-3-thread-1] ERROR (ChunkUtils.java:244) - Rejecting write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216}
2018-09-13 12:13:03,035 [pool-3-thread-1] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 54834b29-603d-4ba9-9d68-0885215759d8 : Message: Rejecting write chunk request. OverWrite flag required.ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216} : Result: OVERWRITE_FLAG_REQUIRED
2018-09-13 12:13:03,037 [ForkJoinPool.commonPool-worker-11] ERROR (ChunkUtils.java:244) - Rejecting write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216}
2018-09-13 12:13:03,037 [ForkJoinPool.commonPool-worker-11] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 54834b29-603d-4ba9-9d68-0885215759d8 : Message: Rejecting write chunk request. OverWrite flag required.ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216} : Result: OVERWRITE_FLAG_REQUIRED
 
{code}
 ",alpha2,['Ozone Client'],HDDS,Bug,Blocker,2018-09-13 12:35:29,3
13184843,Add rest service to the s3gateway,"The next step is after HDDS-441 is to add a rest server to the s3gateway service.

For the http server the obvious choice is to use org.apache.hadoop.http.HttpServer2. We also have a org.apache.hadoop.hdds.server.BaseHttpServer which helps to create the HttpServer2.

In hadoop usually the jersey 1.19 is used. I prefer to exclude jersey dependency from the s3gateway and add the latest jersey2. Hopefully it also could be initialized easily, similar to HttpServer2.addJerseyResourcePackage

The trickiest part is the resource handling. By default the input parameter of the jersey is the JAX-RS resource class and jersey creates new instances from the specified resource classes.

But with this approach we can't inject other components (such as the OzoneClient) to the resource classes. In Hadoop usually a singleton is used or the reference object is injected to the ServletContext. Both of these are just workaround and make the testing harder.

I propose to use some lightweight managed dependency injection:
 # If we can use and JettyApi to instantiate the resource classes, that would be the easiest one.
 # Using a simple CDI framework like dagger, also would help. Dagger is very lightweight, it doesn't support request scoped objects just simple @Inject annotations, but hopefully we won't need fancy new features.
 # The most complex solution would be to use CDI or Guice. CDI seems to be more nature choice for the JAX-RS. It can be checked how easy is to integrate Weld to the Jetty + Jersey combo.

The expected end result of this task is a new HttpServer subcomponent inside the s3gateway which could be started/stopped. We need an example simple service (for exampe a /health endpoint which returns with an 'OK' string) which can demonstrate how our own utilitites (such as OzoneClient) could be injected to the REST resources.",newbie,[],HDDS,Sub-task,Major,2018-09-13 09:22:42,1
13184803,Create new s3gateway daemon,"The first element what we need is a new command line application to start the s3 gateway.

1. A new project should be introduced: hadoop-ozone/s3-gateway

2. A new command line application (eg. org.apache.hadoop.ozone.s3.Gateway should be added with a simple main and start/stop method which just prints out a starting/stopping log message

3. hadoop-ozone/common/src/main/bin/ozone should be modified to manage the new service (eg. ozone s3g start, ozone s3g stop)

4. to make it easier to test a new docker-compose based test cluster should be added to the hadoop-dist/src/main/compose (the normal ./ozone could be copied but we need to add the new s3g component)",newbie,[],HDDS,Sub-task,Major,2018-09-13 06:56:32,1
13184761,Datanode loops forever if it cannot create directories,"Datanode starts but runs in a tight loop forever if it cannot create the DataNode ID directory e.g. due to permissions issues. I encountered this by having a typo in my ozone-site.xml for {{ozone.scm.datanode.id}}.

In just a few minutes the DataNode had generated over 20GB of log+out files with the following exception:
{code:java}
2018-09-12 17:28:20,649 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 2
63:
java.io.IOException: Unable to create datanode ID directories.
at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
2018-09-12 17:28:20,648 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Execution exception when running task in Datanode State Mach
ine Thread - 160
2018-09-12 17:28:20,650 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 1
60:
java.io.IOException: Unable to create datanode ID directories.
at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){code}

We should just exit since this is a fatal issue.",newbie,['Ozone Datanode'],HDDS,Bug,Blocker,2018-09-13 00:32:35,2
13181326,Remove dependencies between hdds/ozone and hdfs proto files,"It would be great to make the hdds/ozone proto files independent from hdfs proto files. It would help as to start ozone with multiple version of hadoop version.

Also helps to make artifacts from the hdds protos:  HDDS-220

 Currently we have a few unused ""hdfs.proto"" import in the proto files and we use the StorageTypeProto from hdfs:

{code}
cd hadoop-hdds
grep -r ""hdfs"" --include=""*.proto""
common/src/main/proto/ScmBlockLocationProtocol.proto:import ""hdfs.proto"";
common/src/main/proto/StorageContainerLocationProtocol.proto:import ""hdfs.proto"";

 cd ../hadoop-ozone
grep -r ""hdfs"" --include=""*.proto""
common/src/main/proto/OzoneManagerProtocol.proto:import ""hdfs.proto"";
common/src/main/proto/OzoneManagerProtocol.proto:    required hadoop.hdfs.StorageTypeProto storageType = 5 [default = DISK];
common/src/main/proto/OzoneManagerProtocol.proto:    optional hadoop.hdfs.StorageTypeProto storageType = 6;
{code}

I propose to 

1.) remove the hdfs import statements from the proto files
2.) Copy the StorageTypeProto and create a Hdds version from it (without PROVIDED)",newbie,[],HDDS,Improvement,Major,2018-08-27 12:25:59,1
13180787,Add RetriableException class in Ozone,"Certain Exception thrown by a server can be because server is in a state
where request cannot be processed temporarily.
 Ozone Client may retry the request. If the service is up, the server may be able to
 process a retried request. This Jira aims to introduce notion of RetriableException in Ozone.",TriagePending,['Ozone Client'],HDDS,Bug,Major,2018-08-23 17:58:28,3
13179303,Separate install and testing phases in acceptance tests.,"In the current acceptance tests (hadoop-ozone/acceptance-test) the robot files contain two kind of commands:

1) starting and stopping clusters
2) testing the basic behaviour with client calls

It would be great to separate the two functionality and include only the testing part in the robot files.

1. Ideally the tests could be executed in any environment. After a kubernetes install I would like to do a smoke test. It could be a different environment but I would like to execute most of the tests (check ozone cli, rest api, etc.)

2. There could be multiple ozone environment (standlaone ozone cluster, hdfs + ozone cluster, etc.). We need to test all of them with all the tests.

3. With this approach we can collect the docker-compose files just in one place (hadoop-dist project). After a docker-compose up there should be a way to execute the tests with an existing cluster. Something like this:

{code}
docker run -it apache/hadoop-runner -v ./acceptance-test:/opt/acceptance-test -e SCM_URL=http://scm:9876 --network=composenetwork start-all-tests.sh
{code}

4. It also means that we need to execute the tests from a separated container instance. We need a configuration parameter to define the cluster topology. Ideally it could be just one environment variables with the url of the scm and the scm could be used to discovery all of the required components + download the configuration files from there.

5. Until now we used the log output of the docker-compose files to do some readiness probes. They should be converted to poll the jmx endpoints and check if the cluster is up and running. If we need the log files for additional testing we can create multiple implementations for different type of environments (docker-compose/kubernetes) and include the right set of functions based on an external parameters.

6. Still we need a generic script under the ozone-acceptance test project to run all the tests (starting the docker-compose clusters, execute tests in a different container, stop the cluster) 
",test,[],HDDS,Improvement,Major,2018-08-16 08:57:13,1
13175286,Add InterfaceAudience/InterfaceStability annotations,This Jira is to add IntefaceAudience for datanode code in the container-service module.,Triaged,[],HDDS,Improvement,Major,2018-07-27 22:58:09,2
13173770,Recon: Add container size distribution visualization,"It would be good if we have some metric/histogram in OzoneManager UI indicating the different container size range and corresponding percentages for the same created in the cluster.

For example :

0-2 GB           10%

2-4 GB .         20%

4-5 GB           70%

5+ GB            0%

 ",TriagePending,['Ozone Recon'],HDDS,Improvement,Minor,2018-07-23 10:42:26,6
13171796,Eliminate the datanode ID file,"This Jira is to remove the datanodeID file. After ContainerIO  work (HDDS-48 branch) is merged, we have a version file in each Volume which stores datanodeUuid and some additional fields in that file.

And also if this disk containing datanodeId path is removed, that DN will now be unusable with current code.",alpha2,[],HDDS,Improvement,Major,2018-07-12 18:25:06,2
13170081,Remove hdfs command line from ozone distribution.,"As the ozone release artifact doesn't contain a stable namenode/datanode code the hdfs command should be removed from the ozone artifact.

ozone-dist-layout-stitching also could be simplified to copy only the required jar files (we don't need to copy the namenode/datanode server side jars, just the common artifacts",newbie,[],HDDS,Sub-task,Major,2018-07-04 13:54:13,1
13170075,add existing docker-compose files to the ozone release artifact,"Currently we use docker-compose files to run ozone pseudo cluster locally. After a full build, they can be found under hadoop-dist/target/compose.

As they are very useful, I propose to make them part of the ozone release to make it easier to try out ozone locally. 

I propose to create a new folder (docker/) in the ozone.tar.gz which contains all the docker-compose subdirectories + some basic README how they could be used.

We should explain in the README that the docker-compose files are not for production just for local experiments.",newbie,[],HDDS,Sub-task,Minor,2018-07-04 13:43:23,1
13167767,Improve shell error message for unrecognized option,"The error message with an unrecognized option is unfriendly. E.g.
{code}
$ ozone oz -badOption
Unrecognized option: -badOptionERROR: null
{code}",newbie,[],HDDS,Improvement,Blocker,2018-06-22 22:52:00,1
13163311,createVolume verbose warning with non-existent user,"When createVolume is invoked for a non-existent user, it logs a verbose warning for {{PartialGroupNameException}}.
{code:java}
    hadoop@9a70d9aa6bf9:~$ ozone oz volume create --user=nosuchuser vol4
    2018-05-31 20:40:17 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user nosuchuser
    PartialGroupNameException The user name 'nosuchuser' is not found. id: ‘nosuchuser’: no such user
    id: ‘nosuchuser’: no such user

      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)
      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)
      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)
      at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)
      at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:384)
      at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:319)
      at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:269)
      at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
      at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
      at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
      at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
      at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
      at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
      at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)
      at org.apache.hadoop.security.Groups.getGroups(Groups.java:227)
      at org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1545)
      at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1533)
      at org.apache.hadoop.ozone.client.rpc.RpcClient.createVolume(RpcClient.java:190)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      at java.lang.reflect.Method.invoke(Method.java:498)
      at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)
      at com.sun.proxy.$Proxy11.createVolume(Unknown Source)
      at org.apache.hadoop.ozone.client.ObjectStore.createVolume(ObjectStore.java:77)
      at org.apache.hadoop.ozone.web.ozShell.volume.CreateVolumeHandler.execute(CreateVolumeHandler.java:98)
      at org.apache.hadoop.ozone.web.ozShell.Shell.dispatch(Shell.java:395)
      at org.apache.hadoop.ozone.web.ozShell.Shell.run(Shell.java:135)
      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
      at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:114)
    2018-05-31 20:40:17 INFO  RpcClient:210 - Creating Volume: vol4, with nosuchuser as owner and quota set to 1152921504606846976 bytes.
{code}
However the volume is created:
{code}
$ ozone oz volume list --user=nosuchuser
[ {
  ""owner"" : {
    ""name"" : ""nosuchuser""
  },
  ""quota"" : {
    ""unit"" : ""TB"",
    ""size"" : 1048576
  },
  ""volumeName"" : ""vol4"",
  ""createdOn"" : ""Thu, 31 May 2018 20:40:17 GMT"",
  ""createdBy"" : ""nosuchuser""
} ]
{code}",newbie usability,[],HDDS,Bug,Blocker,2018-05-31 23:39:00,3
13161589,Implement VolumeSet to manage disk volumes,"VolumeSet would be responsible for managing volumes in the Datanode. Some of its functions are:
 # Initialize volumes on startup
 # Provide APIs to add/ remove volumes
 # Choose and return volume to calling service based on the volume choosing policy (currently implemented Round Robin choosing policy)",ContainerIO,[],HDDS,Sub-task,Major,2018-05-23 21:09:05,7
13159933,Merge ContainerData and ContainerStatus classes,"According to refactoring of containerIO, ContainerData has common fields for different kinds of containerTypes, and each Container will extend ConatinerData to add its fields. So, for this merging ContainerStatus fields to ConatinerData.",reviewed,[],HDDS,Sub-task,Major,2018-05-17 05:56:09,2
13152822,Fix Ozone related doc links in hadoop-project/src/site/site.xml,"Because Hdds profile https://issues.apache.org/jira/browse/HDDS-61#is off by default, the links in the generated site will be invalid without specifying maven profile -Phdds. 

 

 ",newbie,[],HDDS,Bug,Major,2018-04-16 20:49:54,4
13096955,Ozone: C/C++ implementation of ozone client using curl,"This Jira is introduced for implementation of ozone client in C/C++ using curl library.

All these calls will make use of HTTP protocol and would require libcurl. The libcurl API are referenced from here:
https://curl.haxx.se/libcurl/

Additional details would be posted along with the patches.",OzonePostMerge,['Native'],HDDS,Bug,Major,2017-08-23 08:35:12,3
13157736,Fix Ozone Unit Test Failures,This is an umbrellas JIRA to fix unit test failures related or unrelated HDDS-1.,alpha2,[],HDDS,Test,Major,2018-05-07 23:08:29,4
13151737,Send NodeReport and ContainerReport when datanodes register,"From chillmode Deisgn Notes:

As part of this Jira, will update register to send NodeReport and ContaineReport.

Current Datanodes, send one heartbeat per 30 seconds. That means that even if the datanode is ready it will take around a 1 min or longer before the SCM sees the datanode container reports. We can address this partially by making sure that Register call contains both NodeReport and ContainerReport.

 

 ",reviewed,"['Ozone Datanode', 'SCM']",HDDS,Task,Major,2018-04-11 19:04:30,2
