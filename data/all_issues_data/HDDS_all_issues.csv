id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13391987,New k3s version breaks kubernetes test,"New k3s version changes the permissions of volume mounts to root only:

[https://github.com/k3s-io/k3s/issues/3704]

I've created a PR to revert k3s to the older version until we decide what to do.",pull-request-available pull_request_available,[],HDDS,Bug,Minor,2021-07-26 23:41:45,0
13391832,Reduce retry in Kubernetes test,"_kubernetes_ tests wait for cluster startup, checking some conditions with retry.  In worst case all conditions are checked 100 times with 3 seconds delay, so the test may take 15 minutes to fail.",pull-request-available,['test'],HDDS,Improvement,Minor,2021-07-26 08:29:14,1
13391786,Disable failing kubernetes test,"_kubernetes_ check is failing in CI at {{ozone}}.  It is probably related to persistent volumes.  I propose to disable the failing test temporarily, until it can be fixed.

Last successful run: https://github.com/apache/ozone/runs/3137129344
First failed run: https://github.com/apache/ozone/runs/3140001398

SCM init container fails to start due to:

{code}
Unable to create directory /data/metadata specified in configuration setting ozone.metadata.dirs
{code}",pull-request-available,"['kubernetes', 'test']",HDDS,Task,Major,2021-07-26 05:18:12,1
13391581,Install OS-specific flekszible,"{{kubernetes.sh}} installs {{flekszible}} for Linux, even on other platforms.",pull-request-available,['kubernetes'],HDDS,Bug,Major,2021-07-23 14:14:25,1
13391348,Intermittent failure in TestReplicationManager#testMovePrerequisites,"On multiple runs, testMovePrerequisites sometimes fails with an AssertionError:
{code:java}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:87)
	at org.junit.Assert.assertTrue(Assert.java:42)
	at org.junit.Assert.assertTrue(Assert.java:53)
	at org.apache.hadoop.hdds.scm.container.TestReplicationManager.testMovePrerequisites
(TestReplicationManager.java:1331)
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2021-07-22 12:30:46,2
13391278,Fix stream() and link() method in ContainerStateMachine,"The link() method is missing `Override` annotation.

The stream() method has a bug: the parsing of request.message is inconsistent with the ContainerStateMachine#startTransaction(), which will be called during the close of the stream. This caused the stream closing issue in our POC.

 

Here is the details about the issue in stream() method, by diving deep into the Ratis streaming implementation: 

 

When initializing the stream, we need pass a headerMessage to DataStreamApi#stream().
{code:java}
DataStreamOutput stream(ByteBuffer headerMessage, RoutingTable routingTable); 
{code}
This message will be used again when calling the DataStreamOutput#close, by the primary DataNode, in DataStreamManageMent#startTransaction.

It is wrapped in a Raft FORWARD request, then unwrapped in the RaftServerImpl#SubmitClientRequestAsync. The filter here is doing the unwrap job.
{code:java}
TransactionContext context = stateMachine.startTransaction(filterDataStreamRaftClientRequest(request));{code}
Then it hands the headerMessage to ContainerStateMachine#startTransaction.

 

In ContainerStateMachine#startTransaction, it's expecting the request.message being a ContainerCommandRequestMessage.

However, in ContainerStateMachine#stream, it's expecting the same request.message being a ContainerCommandRequestProto.

So we have to change the ContainerStateMachine#stream to match ContainerStateMachine#startTransaction.",pull-request-available,[],HDDS,Sub-task,Critical,2021-07-22 07:58:32,3
13391268,Client and server should support stream setup.,"Before we add new BlockDataStreamOutput, we need to support stream setup. In this way, we can get the DataStreamOutput instance through xceiverClient.

",pull-request-available,[],HDDS,Sub-task,Major,2021-07-22 07:17:35,4
13391241,s3g bucket list failed when there is non-english key name,"Caused by: java.nio.BufferOverflowException
        at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:189)
        at java.nio.ByteBuffer.put(ByteBuffer.java:859)
        at org.apache.hadoop.ozone.s3.util.ContinueToken.encodeToString(ContinueToken.java:77)
        at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.list(BucketEndpoint.java:200)
        at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
        at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
        at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
        at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
        at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)",pull-request-available,[],HDDS,Bug,Major,2021-07-22 03:48:49,5
13391111,Bump jetty version to 9.4.41.v20210516,Please see: https://github.com/apache/ozone/pull/2449,pull-request-available,[],HDDS,Improvement,Major,2021-07-21 10:53:14,6
13391108,Old versions of location in OmKeyLocationInfoGroup causes OOM of OM,"Currently the keyLocation of different versions is stored redundantly in OmKeyInfo. 
 * OmKeyInfo:
 ** keyLocationVersions: List<OmKeyLocationInfoGroup>  stores different versions of information in a list.
 * OmKeyLocationInfoGroup:
 **  locationVersionMap: Map<Long, List<OmKeyLocationInfo>> stores different versions of location in a map.

If the versions are large, the redundent location is causing the large GC overhead for OM, in our cluster, the OM even crashes because of OOM.

This ticket is to remove the redundant location information to keep OM healthy.

Attached the OmKeyInfo of a key with 549 versions.",pull-request-available,[],HDDS,Improvement,Major,2021-07-21 10:17:43,7
13391053,EC: Add padding and generate parity if the last stripe is not full,"When last stripe is not full, we need to pad with zeros and generate the parity as we generate parity at stripe level.",pull-request-available,[],HDDS,Sub-task,Major,2021-07-21 05:05:04,8
13390913,Bump Apache Ratis version to 2.1,Please see: https://github.com/apache/ozone/pull/2443,pull-request-available,[],HDDS,Improvement,Major,2021-07-20 11:35:39,6
13390903,Avoid long sleep in TestPeriodicVolumeChecker,TestPeriodicVolumeChecker requires at least 130 seconds due to two long sleeps.,pull-request-available,['test'],HDDS,Bug,Major,2021-07-20 10:52:55,1
13390816,Refactor BlockOutputStream,"We propose to make BlockOutputStream abstract and move out the Ratis specific code out.  Then, other implementations such as ECBlockOutputStream can extend it and reuse the common parts.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2021-07-20 03:57:55,9
13390689,Multi-raft style placement with permutations for offline data generator,Please see: https://github.com/apache/ozone/pull/2434,pull-request-available,[],HDDS,Improvement,Major,2021-07-19 12:06:29,6
13390688,Move old objects to delete table on overwrite,"HDDS-5243 was a patch for omitting key locations for clients on reading. But the same warning of large response size observed in our cluster for putting data. This is harmful in terms of retry storm, as hadoop-rpc handles this large-response-exception as retry-able exception. Thus, RetryInvocationHandler retries, despite it cannot be recovered by retry, for 15 times, receiving large response message exceeding default limit of RPC message size 128MB as follows.

{quote}
2021-06-21 19:23:10,717 [IPC Server handler 65 on default port 9862] WARN org.apache.hadoop.ipc.Server: Large response size 134538349 for call Call#2037538 Retry#15 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.192.17.172:34070
2021-06-21 19:23:10,722 [IPC Server handler 65 on default port 9862] WARN org.apache.hadoop.ipc.Server: IPC Server handler 65 on default port 9862, call Call#2037538 Retry#15 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.192.17.172:34070: output error
2021-06-21 19:23:10,722 [IPC Server handler 65 on default port 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 65 on default port 9862 caught an exception
java.nio.channels.AsynchronousCloseException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:478)
        at org.apache.hadoop.ipc.Server.channelIO(Server.java:3642)
        at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
        at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
        at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
        at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
{quote}

Suggestion in HDDS-5393 was wrong and it shall be fixed by making old blocks eligible for deletion service, moving to deletion table. It is only needed for normal object-put, while not needed for MultipartUpload objects, if I understand correctly. 

Keeping old blocks and key locations after overwrite might be intended for supporting [object versioning API|https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectVersions.html], but IMO current design will not scale more than, say, thousands of objects. The order of the size of value in key table will be in O( n(version) * n(blocks) ), which might easily exceed current limit of RPC message (128MB by default) or intended value size in RocksDB. Although current implementation is effective for concurrency control, object versioning should be implemented in some different way.",pull-request-available,['OM'],HDDS,Improvement,Major,2021-07-19 11:53:34,10
13390597,ReplicationConfig#getDefault is hardcoded with RatisReplicationConfig,"ReplicationConfig#getDefault is currently hardcoded with RatisReplicationConfig.

Instead of that, it should load the configurations and create respective replication config accordingly.",pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2021-07-19 04:02:48,8
13390559,Replication Manager should process containers synchronously for tests,"The method ReplicationManager.processContainersNow() only wakes up the thread, and returns before the containers have been processed.

This results in all RM tests having a sleep(100) after all calls to this method.

With a small refactor to RM, we can avoid this sleep. After this change all tests run about 100ms faster, and the code in the tests is slightly better.",pull-request-available,[],HDDS,Improvement,Major,2021-07-18 21:47:33,11
13390174,s3_compatbility_check.sh/aws compatibility issues,"*Summary*:
 The smoketest/s3/s3_compatbility_check.sh script was incomplete. I added the rest of the robot scripts and got 4 failures.

*Details*:

I added the following smoketest/s3 robot files to the s3 compatibility test script:
 awss3.robot
 bucketcreate.robot
 bucketdelete.robot
 buckethead.robot
 bucketlist.robot

The only s3 robot files I didn't include are:
 __init__.robot
 commonawslib.robot
 boto3.robot

I get the following failures from the new files. All except #4 fail because our error messages differ from aws.

 

 

*1. ""Create bucket with invalid bucket name""* 
 It expects: ""InvalidBucketName"" but gets:
 _""An error occurred (BucketAlreadyExists) when calling the CreateBucket operation: The requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again.""_

It currently uses ""bucket_1"" as the bad bucket name. ""Changing that name to BadBucketName_1"", causes it to pass.

It seems even though the ""bucket_1"" name is invalid, it collides with some existing name first, and generates a different error.

I'm thinking the bad bucket name should be randomized.

 

 

*2. ""Delete non-existent bucket""* 
 It expects: ""NoSuchBucket"" but gets:
 _""An error occurred (AccessDenied) when calling the DeleteBucket operation: Access Denied""_

So the error message here is different than the one returned by our s3gateway. Do we want to fix the gateway or change the test?

 

 

*3. ""Head Bucket not existent""*
 It is expecting a 404 exit and a ""Not Found"" message. Instead it gets a 400 exit code and this message:
 _""An error occurred (400) when calling the HeadBucket operation: Bad Request""_

Again, do we fix the gateway or change the test?

 

 

*4. ""Test Multipart Upload Put With Copy and range with IfModifiedSince""*

Without any of my changes the current s3 script fails on this test, where a file is uploaded and then tested with ""IfModifiedSince"".

The original file hasn't been modified, so the upload is expected to fail. But on AWS, the upload succeeds.  The problem is that the test sets the ""IfModifiedSince"" into  the future.

In that case AWS ignores the Precondition and does the upload even though ""IfModifiedSince"" is false.

This is a known issue with how the API works on AWS: [https://forums.aws.amazon.com/thread.jspa?threadID=88985]

Currently the test sets the ""IfModifiedSince"" time to a full day in the future. To fix it, we could modify the test to set the ""IfModifiedSince"" time to be a few seconds after the creation time, and pause till that time has passed.",newbie,"['S3', 'test']",HDDS,Bug,Minor,2021-07-16 21:25:33,0
13390168,"services.s3g.environment.OZONE-SITE.XML_ozone.om.enable.filesystem.paths must be a string, number or null","Encountered error when I launch a docker compose cluster with compose/ozone. Just updated Docker for Mac to the latest (3.5.2).

{code}
ozone-1.2.0-SNAPSHOT/compose/ozone
▶ docker compose up -d --scale datanode=3
WARN[0000] The OZONE_OM_METADATA_LAYOUT variable is not set. Defaulting to a blank string.
WARN[0000] The OZONE_OM_ENABLE_FILESYSTEM_PATHS variable is not set. Defaulting to a blank string.
WARN[0000] The OZONE_REPLICATION_FACTOR variable is not set. Defaulting to a blank string.
WARN[0000] The OZONE_SAFEMODE_MIN_DATANODES variable is not set. Defaulting to a blank string.
services.s3g.environment.OZONE-SITE.XML_ozone.om.enable.filesystem.paths must be a string, number or null
{code}
",pull-request-available,['test'],HDDS,Bug,Major,2021-07-16 20:47:28,12
13390069,Inject a Clock into Replication Manager to allow timeouts to be tested,"Replication Manager current uses Time.monotonicNow() in a few places to check if a pending command should be timed out. Currently, this is difficult to test without adding sleeps in the tests. We should inject a Clock object to use rather than making calls to SystemTime to allow tests more flexibility.",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-07-16 14:38:11,11
13390009,OM shutdown when creating key with malformed characters,"In OzoneManagerStateMachine.runCommand, if there is a Runtime exception, OM will be terminated.

In our case, the user would like to create an object with malformed characters in keyName, and the following error happened and throws an InvalidPathException, which is a Runtime Exception causing the OM's shutdown.

Normally this wouldn't happen, since a user can not easily create an invalid keyName with UTF-8. It happened in our environment because our OM started with the property of ""file.encoding"" and ""sun.jnu.encoding"" set to ""ANSI_X3.4-1968"", and it's easy for the user to write a invalid keyName.

This issue can be avoided when running in a good environment, but it's still not reasonable that a client create key operation causing the shutdown of OM.

This ticket is to check the keyName if it fits the current encoding of OM, if not just return the OMException of ""INVALID_KEY_NAME"".",pull-request-available,[],HDDS,Bug,Major,2021-07-16 09:51:48,7
13389995,Add metrics in SCM block deleting service,"This task is to add ""block command send"" and ""block command complete"" two metrics in SCMBlockDeletingService to measure the block deletion speed. ",pull-request-available,[],HDDS,Improvement,Major,2021-07-16 08:59:21,5
13389991,Add link() method to ContainerStateMachine,Add link() method to ContainerStateMachine for Ratis streaming.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2021-07-16 08:45:22,3
13389981,Incorrect PENDINGDELETEBLOCKCOUNT caused by resent command,"

Here are default column family data of two different container replicas，

""#BCSID"" -> 1354765
""#BLOCKCOUNT"" -> -21
""#BYTESUSED"" -> 0
""#PENDINGDELETEBLOCKCOUNT"" -> 78
""#delTX"" -> 1141106

""#BCSID"" -> 1895040
""#BLOCKCOUNT"" -> -5
""#BYTESUSED"" -> 0
""#PENDINGDELETEBLOCKCOUNT"" -> 106
""#delTX"" -> 1146817
",pull-request-available,[],HDDS,Bug,Major,2021-07-16 07:45:01,5
13389978,Avoid refresh pipeline for S3 headObject,"S3 head uses OM API lookup key which refreshes pipeline info by contacting SCM.

For S3 head we donot require any pipeline info, we need very basic details like length, type, etag and last modification time. 

By removing pipeline info which is not required for HEAD object, HEAD API performance can be improved.

This is identified during looking up graphs from [~kerneltime] testing",pull-request-available,"['OM', 'S3']",HDDS,Improvement,Major,2021-07-16 07:25:11,13
13389741,Wrong formatting of comments,"Apache license header is in dangling Javadoc comment (starts with {{/\*\*}}) in several Java source files.  Since it does not belong to any documentable code construct, it should be in regular multiline comment ({{/\*}}).",pull-request-available,[],HDDS,Improvement,Trivial,2021-07-15 03:03:06,14
13389667,s3 bucketcreate.robot inaccurate test and side effect,"When running the s3 smoketests, the bucketcreate.robot test tries to create what is supposed to be an already existing bucket without verifying that the bucket already exists. If the bucket doesn't exist, the test will create the bucket and pass as though the test scenario of bucket create on an existing bucket succeeded. Subsequent tests will now be executed with this existing bucket after this test. Typically the bucket is expected to exist, as those are the test instructions, but the test should insulate itself, verify that the bucket exists or use a randomized bucket name for a repeated create operation.",pull-request-available,"['S3', 'test']",HDDS,Bug,Minor,2021-07-14 16:24:37,15
13389655,Datanode HTTP server fails to start in ozonesecure due to wrong keytab name,"{noformat}
[main] ERROR ozone.HddsDatanodeService: HttpServer failed to start.
...
Caused by: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/datanode.keytab
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2021-07-14 15:41:43,1
13389649,Disallow same set of DNs to be part of multiple pipelines,"Currently, with multi raft feature ON, a data node can participate in multiple pipelines. But, same set of dns can be part of multiple pipelines . This leads to 2 problems:

1) In case of let's say 5 datanodes, as and when 3 datanodes register initially with multi-raft feature ON, those 3 datanodes will pair among themselves to create two pipelines thereby satisfying the default pipeline limit of 2 per dn. Next datanodes even after registering cannot take part in writes bcoz it needs a 6th datanode to come up to form the nest tri-set , thereby remain unutilized.

2) With same set of Datanodes being part of multiple plpelines, if one datanode fails, the source for container re-replication will be only the other set of 2 nodes and can beocme a bottleneck.

3) This uneven distribution of pipelines among dns will lead to load distribution getting uneven as well.

 

 ",pull-request-available,['SCM'],HDDS,Bug,Major,2021-07-14 14:55:59,16
13389511,Move dead DN out of topology ,It's task is to move the DN out of topology when DN is dead， and add it back again when it's changed from dead to stale or alive.,pull-request-available,[],HDDS,Improvement,Major,2021-07-14 03:15:51,2
13389417,Add acceptance test for Hadoop 3.3,"Hadoop 3.3 support was recently added (HDDS-3292) in Ozone.  We should run Hadoop/MapReduce acceptance tests for this version, too.",pull-request-available,['test'],HDDS,Task,Major,2021-07-13 15:25:57,1
13389400,Reduce usage of OmKeyLocationInfoGroup.getLocationList(),"Follow-up of HDDS-5384: check usages of {{OmKeyLocationInfoGroup.getLocationList()}} and {{getLocationListCount()}}, and replace with cheaper calls if possible.",performance,['OM'],HDDS,Improvement,Major,2021-07-13 14:05:32,1
13389334,Fix potential InterruptedException in Ozone,Handle InterruptedException properly.,pull-request-available,[],HDDS,Bug,Major,2021-07-13 06:31:41,17
13389307,rename hadooprpc proto compilation in pom to refer to proto2,rename hadooprpc proto compilation in pom to refer to proto2,pull-request-available,['Tools'],HDDS,Sub-task,Major,2021-07-13 04:25:16,18
13388863,Fix potential BigDecimal precision in StorageUnit.java,"Because of floating point imprecision, you’re unlikely to get the value you expect from the *BigDecimal(double)* constructor.

This should be
{code:java}
BigDecimal val = BigDecimal.valueOf(value);
BigDecimal bDivisor = BigDecimal.valueOf(divisor);
{code}
{code:java}
BigDecimal firstVal = BigDecimal.valueOf(first);
BigDecimal secondVal = BigDecimal.valueOf(second);
{code}
Instead of
{code:java}
BigDecimal val = new BigDecimal(value);
BigDecimal bDivisor = new BigDecimal(divisor);
{code}
{code:java}
BigDecimal firstVal = new BigDecimal(first);
BigDecimal secondVal = new BigDecimal(second);
{code}",pull-request-available,[],HDDS,Improvement,Major,2021-07-11 08:07:16,17
13388859,"Either re-interrupt this method or rethrow the ""InterruptedException"" that can be caught here","as follows：

[https://github.com/apache/ozone/blob/77c83c0af28976fe1a2e29551ae80ddc2de20957/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L221]

[https://github.com/apache/ozone/blob/77c83c0af28976fe1a2e29551ae80ddc2de20957/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L262]

[https://github.com/apache/ozone/blob/77c83c0af28976fe1a2e29551ae80ddc2de20957/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L272]",pull-request-available,[],HDDS,Bug,Major,2021-07-11 05:29:02,17
13388858,Remove unused method parameter in XceiverClientGrpc.java,"There is an unused method parameter ""encodedToken"" in line 541 of XceiverClientGrpc.java.
 ([https://github.com/apache/ozone/blob/77c83c0af28976fe1a2e29551ae80ddc2de20957/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L541])",pull-request-available,[],HDDS,Bug,Minor,2021-07-11 04:58:57,17
13388624,Pipeline list command's output should be in local timezone,"Pipeline list command prints the creation time in UTC format, this should be in the system's local timezone.",pull-request-available,['SCM'],HDDS,Bug,Major,2021-07-09 12:44:50,19
13388612,Robot test should print the entire exception stack trace.,"Noticed exception stack trace getting truncated with a note like "" [ Message content over the limit has been removed. ]"".

For example :-

{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/google/protobuf/ServiceException
	at org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransport.submitRequest(Hadoop3OmTransport.java:95)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:222)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1099)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:192)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:246)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:229)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:92)
	at org.apache.hadoop.ozone.shell.OzoneAddress.createRpcClientFromHostPort(OzoneAddress.java:107)
	at org.apache.hadoop.ozone.shell.OzoneAddress.createClient(OzoneAddress.java:148)
	at org.apache.hadoop.ozone.shell.Handler.createClient(Handler.java:104)
    [ Message content over the limit has been removed. ]
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:2465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)
	at org.apache.hadoop.ozone.shell.OzoneShell.lambda$execute$0(OzoneShell.java:55)
	at org.apache.hadoop.hdds.tracing.TracingUtil.executeInNewSpan(TracingUtil.java:159)
	at org.apache.hadoop.ozone.shell.OzoneShell.execute(OzoneShell.java:53)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)
	at org.apache.hadoop.ozone.shell.OzoneShell.main(OzoneShell.java:47)
Caused by: java.lang.ClassNotFoundException: com.google.protobuf.ServiceException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 26 more' does not contain 'Client cannot authenticate via'
{code}",pull-request-available,[],HDDS,Bug,Major,2021-07-09 11:50:31,18
13388403,Avoid eager string formatting in preconditions,"Some calls to {{Preconditions.check...}} eagerly format the error message, resulting in unnecessary allocations.",pull-request-available,[],HDDS,Improvement,Minor,2021-07-08 13:27:42,1
13388233,SCM throws NPE during JMX call,"Whenever there is a JMX call to SCM, there is an NPE in SCM log file.

JMX call invokes _StorageContainerManager#getScmRatisRoles_
{code:java}
  @Override
  public String getScmRatisRoles() throws IOException {
    return HddsUtils.format(getScmHAManager().getRatisServer().getRatisRoles());
  }
{code}
When in NON-HA mode the below call is returning null, causing NPE during JMX call.
{code:java}
SCMHAManager#getRatisServer(){code}",pull-request-available,['SCM'],HDDS,Bug,Major,2021-07-07 18:31:24,20
13388119,Recon doesn't update datanode's hostname ,"Currently the ""DatanodeDetails"" are stored in Recon's local DB. Once a datanode has registered the content of DatanodeDetails won't be changed.

We met a scenario that datanodes' hostname is changed after running a while in Ozone cluster, the hostname in Recon's UI will still show the old hostnames.

This ticket proposes to send ""reregisterCommand"" to datanodes if the information of the datanode is changed.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2021-07-07 08:23:18,7
13388080,Fix json parse stackoverflow for cmd ozone debug container list,"When I run the command `ozone debug container list`, I saw the output:
{code:java}
[ozoneadmin@0dc124237e09 ozone-1.2.0-SNAPSHOT]$ ./bin/ozone debug container list
2021-07-07 03:59:48,075 [main] INFO volume.HddsVolume: Creating HddsVolume: /data1/hdds of storage type : DISK capacity : 47341600768
2021-07-07 03:59:48,076 [main] INFO volume.MutableVolumeSet: Added Volume : /data1/hdds to VolumeSet
2021-07-07 03:59:48,077 [main] INFO volume.HddsVolume: Creating HddsVolume: /data2/hdds of storage type : DISK capacity : 47341600768
2021-07-07 03:59:48,078 [main] INFO volume.MutableVolumeSet: Added Volume : /data2/hdds to VolumeSet
2021-07-07 03:59:48,078 [main] INFO volume.HddsVolume: Creating HddsVolume: /data3/hdds of storage type : DISK capacity : 47341600768
2021-07-07 03:59:48,079 [main] INFO volume.MutableVolumeSet: Added Volume : /data3/hdds to VolumeSet
2021-07-07 03:59:48,153 [main] INFO container.ContainerCommands: Starting the read all the container metadata
2021-07-07 03:59:48,153 [main] INFO container.ContainerCommands: Loading container metadata from volume /data1/hdds
2021-07-07 03:59:48,154 [main] INFO ozoneimpl.ContainerReader: Running in upgrade mode:true
2021-07-07 03:59:48,155 [main] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data1/hdds
2021-07-07 03:59:48,568 [main] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data1/hdds
2021-07-07 03:59:48,568 [main] INFO container.ContainerCommands: Loading container metadata from volume /data2/hdds
2021-07-07 03:59:48,568 [main] INFO ozoneimpl.ContainerReader: Running in upgrade mode:true
2021-07-07 03:59:48,568 [main] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data2/hdds
2021-07-07 03:59:48,750 [main] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data2/hdds
2021-07-07 03:59:48,751 [main] INFO container.ContainerCommands: Loading container metadata from volume /data3/hdds
2021-07-07 03:59:48,751 [main] INFO ozoneimpl.ContainerReader: Running in upgrade mode:true
2021-07-07 03:59:48,751 [main] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data3/hdds
2021-07-07 03:59:48,887 [main] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data3/hdds
2021-07-07 03:59:48,887 [main] INFO container.ContainerCommands: All the container metadata is loaded.
Infinite recursion (StackOverflowError) (through reference chain: org.apache.hadoop.ozone.container.common.volume.HddsVolume[""volumeSet""]->org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet[""volumeMap""]->com.google.common.collect.RegularImmutableMap[""/data1/hdds""]->org.apache.hadoop.ozone.container.common.volume.HddsVolume[""volumeSet""]->org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet[""volumeMap""]->com.google.common.collect.RegularImmutableMap[""/data1/hdds""]->org.apache.hadoop.ozone.container.common.volume.HddsVolume[""volumeSet""]->org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet[""volumeMap""]->com.google.common.collect.RegularImmutableMap[""/data1/hdds""]->org.apache.hadoop.ozone.container.common.volume.HddsVolume[""volumeSet""]->org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet[""volumeMap""]->com.google.common.collect.RegularImmutableMap[""/data1/hdds""]->org.apache.hadoop.ozone.container.common.volume.HddsVolume[""volumeSet""]->
...{code}
This is due to the json parser will try recursively to parse each field of ContainerData, and when it meets `HddsVolume`, it enters a loop 'HddsVolume->VolumeSet->VolumeMap->HddsVolume'.

We could break the loop by letting json parser ignore the HddsVolume in ContainerData, we already have location information like chunksPath, so detailed volume info in HddsVolume is not necessary for container info for debug.",pull-request-available,[],HDDS,Bug,Minor,2021-07-07 05:12:54,21
13388000,Intermittent test failure in TestSCMPipelineManager#testPipelineReload,"Failure first noticed in this CI run on upgrade branch: https://github.com/apache/ozone/runs/2969789739

Was able to reproduce failure on master after about 100 runs of the test locally.",pull-request-available,[],HDDS,Bug,Major,2021-07-06 18:56:16,1
13387986,Fix TestSCMNodeManager after merge of master at 1d8f972 into upgrade branch,After HDDS-5269 datanodes used for testing must report having space for metadata otherwise pipeline creation will fail.,pull-request-available,[],HDDS,Sub-task,Major,2021-07-06 17:40:45,22
13387882,Data buffers incorrectly filtered for Ozone Insight,"{{ozone insight logs -v datanode.dispatcher}} prints binary data ({{buffers: ""4w7u1Zp8@{""}})

{noformat}
[TRACE|org.apache.hadoop.ozone.container.common.impl.HddsDispatcher|OzoneProtocolMessageDispatcher] [service=DatanodeClient] [type=ReadChunk] request is processed. Response:
cmdType: ReadChunk
traceID: """"
result: SUCCESS
readChunk {
  blockID {
    containerID: 1
    localID: 107544261427200009
    blockCommitSequenceId: 34
  }
  chunkData {
    chunkName: ""107544261427200009_chunk_1""
    offset: 0
    len: 10
    checksumData {
      type: CRC32
      bytesPerChecksum: 1048576
      checksums: ""354d261247""
    }
  }
  dataBuffers {
    buffers: ""4w7u1Zp8@{""
    buffers: ""<redacted>""
  }
}
{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-07-06 09:28:40,1
13387856,Limit num of containers to process per round for ReplicationManager.,"For now, ReplicationManager process all containers at once, this will potentially bring healthy load to datanodes if there are a lot of containers to be replicated/deleted/closed.

So it is nice to have a bound for each round, HDFS has similar settings, and this issue tries to implement sth like 'dfs.block.misreplication.processing.limit: 10000' in HDFS.

 

This is just a limit on the number of containers to be processed, note that ReplicationManager count each container as processed no mater it is under replicated or over replicated or good. And all the cmds are queued and will be sent with heartbeats.

So this limit does not directly limit the data to be replicated, just to have a basic throttling for ReplicationManager in-memory processing, so 10000 should be good for ozone as well.

And we should add more throttling limits in the future.",pull-request-available,[],HDDS,Improvement,Major,2021-07-06 07:44:34,21
13387060,add proto version to all the proto files,This jira adds proto2 as the version for all the existing proto files,pull-request-available,[],HDDS,Sub-task,Major,2021-07-01 13:39:00,18
13387059,Refactor pom files for HadoopRpc and Grpc/Ratis compilation properties,This jira will introduce hadooprpc and grpc specific protobuf compilation flags,pull-request-available,[],HDDS,Sub-task,Major,2021-07-01 13:38:18,18
13386986,Support list node based on NodeOperationalState and NodeState options in datanode list CLI,"Currently, datanode list CLI display all DN information as output.
This task is to add NodeOperationalState and NodeState as the CLI options to the command, and display the DN accordingly. ",pull-request-available,[],HDDS,Improvement,Major,2021-07-01 08:03:41,23
13386985,Support list node based on NodeOperationalState and NodeState options in printTopology CLI,"Currently,  printTopology CLI display all DN information as output.  
This task is to add NodeOperationalState and NodeState as the CLI options to the command, and display the DN accordingly. ",pull-request-available,[],HDDS,Improvement,Major,2021-07-01 08:01:21,23
13386966,Add more metrics to ReplicationManager to help monitor replication progress,"For now SCM ReplicationManager only has 2 metrics: inflightReplication and inflightDeletion.

We could add more metrics to help better monitor the replication progress(via prometheus e.g.).

Then we could also estimate the time needed to complete the whole replication.

Some proposed metrics:
 * number of replicate/delete cmds sent
 * number of replicate/delete cmds completed
 * number of replicate/delete cmds timeout

These metrics will be refreshed for each replication round(300s by default). So we could calculate how many replicate/delete are completed between 2 successive rounds and how many are undergoing, thus we could estimate how much more time it needs.

Two more metrics to help more accurate estimation since closed containers could be in different sizes:
 * number of replicate bytes total
 * number of replicate bytes completed",pull-request-available,[],HDDS,Improvement,Major,2021-07-01 06:44:34,21
13386556,Avoid object creation in ReplicationManger debug log statements,"There are a few debug log entries in replication manager like this:

{code}
    LOG.debug(""Handling under-replicated container: {}"",
        container.getContainerID());

      LOG.debug(""Deleting empty container {} replicas,"", container.containerID());
{code}

Especially the latter one, will always allocate a new ContainerID object on each call, even if debug is not enabled.

If we just pass ""container"" then it will use the container.toString() method inside the logger, only if debug is enabled. The ContainerInfo toString looks like:

{code}
  @Override
  public String toString() {
    return ""ContainerInfo{""
        + ""id="" + containerID
        + "", state="" + state
        + "", pipelineID="" + pipelineID
        + "", stateEnterTime="" + stateEnterTime
        + "", owner="" + owner
        + '}';
  }
{code}

It contains some extra information, but some of that may be useful if debugging a problem.",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-06-29 16:02:55,11
13386231,Fix negligence issue conditional expressions in MockCRLStore.java,"The derivative problem comes from [HDDS-5383|https://github.com/apache/ozone/pull/2372].


{code:java}
if (crlInfos.isEmpty()) {
  log.debug(""CRL[0]: {}"", crlInfos.get(0));
}
{code}
",pull-request-available,[],HDDS,Improvement,Major,2021-06-28 09:16:06,17
13386168,Fix skipped volume check due to disk.check.min.gap,"After HDDS-5268, datanode data volumes and ratis volumes are checked in a single periodic volume checker together.

But actually, data volumes and ratis volumes are checked in 2 separated `checkAllVolumes` calls, the `checkAllVolumes` will check whether 2 successive calls are executed within a time gap controlled by 'disk.check.min.gap', then ratis volumes are always skipped.

To fix it we could put the check in `checkAllVolumeSets` which check volume sets in a single pass one by one.

And there is a another problem, there are 2 volume checkers implemented in datanode:
 * Periodic Volume Checker
 * On-demand Volume Checker(HDDS-5089)

The periodic volume checker is scheduled at fixed rate, 15 mins by default, but 'disk.check.min.gap' is also 15 mins by default and it also controls the time gap of 2 successive checks for a single volume. So within the 15 mins between 2 periodic checks, no on-demand check could happen.

To fix it we could make the 'periodic.disk.check.interval.minutes' longer, such as 1 hour, since we have the on-demand disk checker, this should be fine.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-28 06:22:44,21
13386148,Return latest version of key location for client on createKey/createFile,"HDDS-5243 was a patch for omitting unnecessary key locations for clients on reading. But the same warning of large response size observed in our cluster for putting data. The patch can also be ported for putting data, as long as until object versioning is supported.

My hypothesis is: The large message was originally, and possibly maybe due to this warning and sudden connection close from client side on reading large message in Hadoop IPC layer, from Ozone Manager - which causes hopeless 15 retries from RetryInvocationHandler. The retries create another entry in OpenKeyTable but they never moved to KeyTable because the key never gets commited.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2021-06-28 03:36:14,13
13386025,Avoid catching Error while creating Ozone client,"{{OzoneClientProducer}} catches any {{Throwable}} while creating the Ozone client and logs it at debug level.  I think this should be avoided for OOME at least, possibly other {{Error}} types.",pull-request-available,['S3'],HDDS,Bug,Major,2021-06-26 19:52:08,1
13385793,reconPipelineReportHandler should not retry when pipeline not found,"if recon receives a pipelineReport , the reconPipelineReportHandler will confirm it with scm. if scm does not find this pipeline , it will throw a pipelineNotFoundExecption. today, if this happen, reconPipelineReportHandler will try again and again. this behavior is not expected. the right behivor is that if this happen, reconPipelineReportHandler should just ignore this report and go ahead. ",pull-request-available,[],HDDS,Improvement,Major,2021-06-25 08:20:02,2
13385765,Include ozoneserviceid in fs.defaultFS when configuring o3fs,"Incorrect fs.defaultFS value on this page: https://hadoop.apache.org/ozone/docs/1.1.0/interface/o3fs.html

 

Incorrect section:

<property> 
<name>fs.defaultFS</name> 
<value>o3fs://bucket.volume</value>
</property>


Recommended correction

<property> 
<name>fs.defaultFS</name> 
<value>o3fs://bucket.volume.OzoneServiceId</value>
</property>

",pull-request-available,['documentation'],HDDS,Bug,Major,2021-06-25 04:32:09,17
13385739,ProfileServlet to move the default output location to an ozone specific directory,"The ProfileServlet is forked and used by several other projects: HBase, and Hive. They all defaults the output location to /tmp/prof-output.

https://github.com/apache/ozone/blob/8d57331ccf975391bd1324ccbc517bf9c694a0cf/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/ProfileServlet.java#L132

Sharing the output directory causes permission error, because typically they are run by separate users.

We should use an output directory specific to Ozone to avoid the permission issue. For example, /tmp/prof-output-ozone",newbie pull-request-available,[],HDDS,Improvement,Trivial,2021-06-25 01:39:37,17
13385710,Added a NSSummaryTask to write NSSummary info into RDB,"FileSizeCountTask counts the number of bins inside each buckets.

Try to build on FileSizeCountTask to count what we need for NSSummary as well.

[6.28] Added an independent Recon task to process namespace summary info per objectID in the attempt to decouple with other task modules.\

[7.7] 

New NSSummary schema:
{code:java}
objectId -> int numOfFiles, long sizeOfFiles, int[] fileDistribution, List<objectId> childDir{code}
where childDir is a list of objectIds for the next-level non-key objects.

Modify the NSSummaryCodec.

Modify NSSummaryTask to write the new field into RocksDB

CC [~avijayan]",pull-request-available,[],HDDS,Sub-task,Major,2021-06-24 20:29:47,24
13385657,OM refreshPipeline should not invoke the expensive OmKeyLocationInfoGroup.getLocationList(),"The OM's refreshPipeline (used by liststatus) implementation iterates over OmKeyLocationInfoGroup.getLocationList(), which is a very expensive call. It iterate over a collection of list of objects, allocates a new list, perform operations on each of them. In short, it's an O( n ) method in terms of space and time complexity.

There are many places in the Ozone code that uses this method. Most usages iterates over the generated list, without modifying the list. We should instead return the collection of lists, which is O( 1 ).

I have a client that issues many listStatus calls to examine the effect. Before the change, refreshPipeline costs 8.65% of heap usage. After: 1.95%.
CPU cost: before: 8.18% after: 4.22%

We should refrain from invoking getLocationList() as much as possible. But given the wide usage in the code, I elect not to remove the usage completely to avoid destabilizing it. Instead, I changed the usage in refreshPipeline to demonstrate its impact.",pull-request-available,['OM'],HDDS,Improvement,Major,2021-06-24 15:14:17,25
13385640,Eliminate expensive string creation in debug log messages,"Use IntelliJ and search with regex pattern: LOG\.debug.*toString

I found a number of bad log message practice where string objects are created or concatenated regardless log level.

Parameters passed to slf4j debug log are supposed to be O(1).

For example, 
{code:title=KeyManagerImpl.refreshPipeline}
} catch (IOException ioEx) {
  LOG.debug(""Get containerPipeline failed for {}"",
      containerIDs.toString(), ioEx);
{code}",pull-request-available,[],HDDS,Improvement,Trivial,2021-06-24 13:22:34,17
13385601,Increase default container report interval to 60 mins,"During scale testing of ozone with 350k+ containers and nearly 1 million replica reports it was observed that, there is a sudden burst in SCM heap usage . In HDFS, the full block report interval is 6 hours by default and in between, there are incremental block reports. Similarly, there are incremental reports in SCM . Setting the full container report interval to 1 hour make things quite stable as determined from tests and 60s for full report seems very aggressive.

1.Increase default container report interval to 60 mins from 60 sec currently .

2. Increase pipeline report interval",pull-request-available,['SCM'],HDDS,Bug,Major,2021-06-24 11:06:47,26
13385599,SCM terminated with exit status 1: null,"steps taken :
Running ozone fault injection tests which involve activating/deactivating pipelines.
In one of the tests, while activating pipeline, SCM terminated.SCM log:
{code:java}
2021-06-23 23:57:03,166 WARN org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline PipelineID=661b906a-fd6b-4c14-b6b5-8c0a12c67ff7 is not found in the pipeline Map. Pipeline may have been deleted already.
2021-06-23 23:57:03,168 ERROR org.apache.ratis.statemachine.StateMachine: Terminating with exit status 1: null
java.lang.IllegalMonitorStateException
	at java.base/java.util.concurrent.locks.ReentrantReadWriteLock$Sync.tryRelease(ReentrantReadWriteLock.java:372)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1302)
	at java.base/java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.unlock(ReentrantReadWriteLock.java:1147)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.updatePipelineState(PipelineStateManagerV2Impl.java:270)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:168)
	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:139)
	at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1690)
	at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:234)
	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:179)
	at java.base/java.lang.Thread.run(Thread.java:834)
2021-06-23 23:57:03,174 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
{code}",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-06-24 11:04:29,16
13385575,Get more accurate space info for DedicatedDiskSpaceUsage,"Java has 2 apis to get available space info for a mounted fs: `getFreeSpace()` and `getUsableSpace()`,  the latter is more accurate(see its java doc).

Let's do a experiment:
{code:java}
// java

import java.io.File;

public class FsSpace {
  public static void main(String args[]) {
    File file = new File(args[0]);
    System.out.println(""FreeSpace: "" + file.getFreeSpace());
    System.out.println(""UsableSpace: "" + file.getUsableSpace());
  }
}{code}
 compile it and run against my root /, let's compare with linux command 'df'.
{code:java}
// linux command line
$ java FsSpace /
FreeSpace: 49653022720           <-- A
UsableSpace: 45126942720         <-- B

$ df / -B1
Filesystem        1B-blocks        Used   Available Use% Mounted on
overlay        105553784832 55900872704 45126832128  56% /
                                             ^
                                             |
                                             C{code}
So the B and C is more close to each other, while A is larger.

Here we have about a 4GB diff on a 100GB disk for the 2 java apis.

I think that `getFreeSpace()` must only account for all unallocated space but does not take the space reserved for system use into consideration while `getUsableSpace()` do.

So we should be more conservative to prevent over consumed disk space.

P.S.

HDFS has a implemtation that use the 2 apis for different purposes, see:

hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DF.java",pull-request-available,[],HDDS,Sub-task,Minor,2021-06-24 07:50:59,21
13385481,Add APIs to retrieve Namespace Summary from Recon,Add new API(s) (likely REST APIs) exposed by Recon to be used by CLI and Web UI later.,pull-request-available,[],HDDS,Sub-task,Major,2021-06-23 18:44:33,24
13385457,EC: Extend PipelineManager.createPipeline API to support excluded nodes,"The WritableECContainerProvider needs to be able to create pipelines with excluded nodes. Therefore we should extend the pipelineManager interface to allow for passing the excluded nodes. After making that change, the WritableECContainerProvider should be modified to pass the excluded nodes to the PipelineManager.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-23 16:12:17,11
13385312,Cleanup unused configuration related to SCM HA,"SCM HA uses config from SCMHAConfiguration, remove unnused config related to SCM HA Ratis config in SCMConfigKeys.java
{code:java}
  public static final String OZONE_SCM_RATIS_RPC_TYPE_KEY
      = ""ozone.scm.ratis.rpc.type"";
  public static final String OZONE_SCM_RATIS_RPC_TYPE_DEFAULT
      = ""GRPC"";

  // SCM Ratis Log configurations
  public static final String OZONE_SCM_RATIS_STORAGE_DIR
      = ""ozone.scm.ratis.storage.dir"";
  public static final String OZONE_SCM_RATIS_SEGMENT_SIZE_KEY
      = ""ozone.scm.ratis.segment.size"";
  public static final String OZONE_SCM_RATIS_SEGMENT_SIZE_DEFAULT
      = ""16KB"";
  public static final String OZONE_SCM_RATIS_SEGMENT_PREALLOCATED_SIZE_KEY
      = ""ozone.scm.ratis.segment.preallocated.size"";
  public static final String OZONE_SCM_RATIS_SEGMENT_PREALLOCATED_SIZE_DEFAULT
      = ""16KB"";

  // SCM Ratis Log Appender configurations
  public static final String
      OZONE_SCM_RATIS_LOG_APPENDER_QUEUE_NUM_ELEMENTS =
      ""ozone.scm.ratis.log.appender.queue.num-elements"";
  public static final int
      OZONE_SCM_RATIS_LOG_APPENDER_QUEUE_NUM_ELEMENTS_DEFAULT = 1024;
  public static final String OZONE_SCM_RATIS_LOG_APPENDER_QUEUE_BYTE_LIMIT =
      ""ozone.scm.ratis.log.appender.queue.byte-limit"";
  public static final String
      OZONE_SCM_RATIS_LOG_APPENDER_QUEUE_BYTE_LIMIT_DEFAULT = ""32MB"";
  public static final String OZONE_SCM_RATIS_LOG_PURGE_GAP =
      ""ozone.scm.ratis.log.purge.gap"";
  public static final int OZONE_SCM_RATIS_LOG_PURGE_GAP_DEFAULT = 1000000;

  // SCM Ratis server configurations
  public static final String OZONE_SCM_RATIS_SERVER_REQUEST_TIMEOUT_KEY
      = ""ozone.scm.ratis.server.request.timeout"";
  public static final TimeDuration
      OZONE_SCM_RATIS_SERVER_REQUEST_TIMEOUT_DEFAULT
      = TimeDuration.valueOf(3000, TimeUnit.MILLISECONDS);
  public static final String
      OZONE_SCM_RATIS_SERVER_RETRY_CACHE_TIMEOUT_KEY
      = ""ozone.scm.ratis.server.retry.cache.timeout"";
  public static final TimeDuration
      OZONE_SCM_RATIS_SERVER_RETRY_CACHE_TIMEOUT_DEFAULT
      = TimeDuration.valueOf(600000, TimeUnit.MILLISECONDS);
  public static final String OZONE_SCM_RATIS_MINIMUM_TIMEOUT_KEY
      = ""ozone.scm.ratis.minimum.timeout"";
  public static final TimeDuration OZONE_SCM_RATIS_MINIMUM_TIMEOUT_DEFAULT
      = TimeDuration.valueOf(1, TimeUnit.SECONDS);


  // SCM Ratis Leader Election configurations
  public static final String
      OZONE_SCM_LEADER_ELECTION_MINIMUM_TIMEOUT_DURATION_KEY =
      ""ozone.scm.ratis.leader.election.minimum.timeout.duration"";
  public static final TimeDuration
      OZONE_SCM_LEADER_ELECTION_MINIMUM_TIMEOUT_DURATION_DEFAULT =
      TimeDuration.valueOf(1, TimeUnit.SECONDS);
  public static final String OZONE_SCM_RATIS_SERVER_FAILURE_TIMEOUT_DURATION_KEY
      = ""ozone.scm.ratis.server.failure.timeout.duration"";
  public static final TimeDuration
      OZONE_SCM_RATIS_SERVER_FAILURE_TIMEOUT_DURATION_DEFAULT
      = TimeDuration.valueOf(120, TimeUnit.SECONDS);

  // SCM Leader server role check interval
  public static final String OZONE_SCM_RATIS_SERVER_ROLE_CHECK_INTERVAL_KEY
      = ""ozone.scm.ratis.server.role.check.interval"";
  public static final TimeDuration
      OZONE_SCM_RATIS_SERVER_ROLE_CHECK_INTERVAL_DEFAULT
      = TimeDuration.valueOf(15, TimeUnit.SECONDS);
{code}",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-23 05:54:24,27
13384826,Implement stream method to ContainerStateMachine,Use RATIS-Streaming to make ContainerStateMachine support writing streaming to the local.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2021-06-21 09:23:47,4
13384801,EC: Adopt EC related utility from Hadoop source repository,"It seems only a few key classes are required from Apache Hadoop to be reused. The goal here is investigating the build process and check how they can be re-used. 

One option is just copy the files with some minor updates (e.g. using ConfigurationSource interface instead of o.a.hadoop.Configuration) which would help to reduce dependencies.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-21 07:57:02,6
13384800,Datanode shutdown due to too many bad volumes in CI,"_Acceptance (secure)_ check is frequently failing, usually at S3 tests.  The root cause is that datanodes are shut down due to too many ""bad"" volumes.

{noformat:title=S3 Gateway log}
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
{noformat}

{noformat:title=SCM log}
Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
{noformat}

{noformat:title=Datanode log}
datanode_2  | 2021-06-19 13:26:08,010 [Periodic HDDS volume checker] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2021-06-19 13:36:08,013 [Periodic HDDS volume checker] WARN volume.StorageVolumeChecker: checkAllVolumes timed out after 600000 ms
datanode_2  | 2021-06-19 13:36:08,014 [Periodic HDDS volume checker] WARN volume.MutableVolumeSet: checkAllVolumes got 1 failed volumes - [/data/hdds/hdds]
datanode_2  | 2021-06-19 13:36:08,016 [Periodic HDDS volume checker] INFO volume.MutableVolumeSet: Moving Volume : /data/hdds/hdds to failed Volumes
datanode_2  | 2021-06-19 13:36:08,016 [Periodic HDDS volume checker] ERROR statemachine.DatanodeStateMachine: DatanodeStateMachine Shutdown due to too many bad volumes, check hdds.datanode.failed.data.volumes.tolerated and hdds.datanode.failed.metadata.volumes.tolerated
{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2021-06-21 07:55:10,1
13384798,[FSO] Support bucket layout in OM,"We are introducing 3 kind of bucket layouts as of now in Ozone:

FSO (File System Optimized):- For this bucket, we need the configuration “ozone.om.enable.filesystem.paths” to be set to true.
 Behavior:
 - Supports _atomic rename and delete_ operations.
 - Files written to this bucket can be read via S3.
 - Keys written via S3 with a delimiter “/” *will* create intermediate directories.
 - This bucket *will* allow interoperability between S3 and FS APIs.

OBS (Object Store):- For this bucket, we need the configuration “ozone.om.enable.filesystem.paths” to be set to false.
 Behavior:
 - Keys written to this bucket will behave as a pure object-store.
 - Keys written via S3 with a delimiter “/” *will not* create intermediate dirs.
 - This bucket *will not* allow interoperability between S3 and FS APIs.

LEGACY:- This bucket layout has been introduced to handle the case of upgrading an existing cluster with pre-created buckets. Users can’t create a LEGACY bucket explicitly via the Ozone client.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-06-21 07:48:26,27
13384781,Suppress logging of ServerNotLeaderException ,"When client tries to figure out leader, when it contacts the first node if it is not a leader, we see this kind of exception in the log.


{code:java}
2021-06-02 07:08:31,760 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from 172.27.161.194:45505
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:38e91e94-b4fe-4307-b3b0-f8c1e7e2d4d7 is not the leader. Suggested leader is Server:quasar-vudtrs-8.quasar-vudtrs.root.hwx.site:9961.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:191)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:92)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:15124)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:533)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887)
{code}
",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-21 06:20:16,13
13384220,Move SCMUpdateProtocol to hdds interface-server package,This is intended for OM/SCM and server only. ,pull-request-available,[],HDDS,Sub-task,Major,2021-06-16 18:56:30,28
13384151,[SCM-HA] SCM start failed with PipelineNotFoundException,"{code:java}
 2021-06-16 03:02:14,478 ERROR org.apache.ratis.statemachine.StateMachine: Terminating with exit status 1: PipelineID=fe572097-8689-42ae-83aa-ba86439c5a0e not found
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=fe572097-8689-42ae-83aa-ba86439c5a0e not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:121)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.getPipeline(PipelineStateManagerV2Impl.java:125)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.updatePipelineState(PipelineStateManagerV2Impl.java:250)
        at jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:168)
        at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:139)
        at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1690)
        at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:234)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:179)
        at java.base/java.lang.Thread.run(Thread.java:834)
2021-06-16 03:02:14,701 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:{code}",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-06-16 11:48:12,16
13384148,"In ContainerStateMachine, share the executor threads between the containers","{code:java}
//ContainerStateMachine
  private ExecutorService getCommandExecutor(
      ContainerCommandRequestProto requestProto) {
    int executorId = (int)(requestProto.getContainerID() % executors.length);
    return executors[executorId];
  }
{code}
In the code above, different containers having the same remainder (mod executors.length ) will use the same executor even if some other executors are idle.  Ideally, different containers should use a different executor if there are executors available.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2021-06-16 11:30:20,9
13384111,Allow to restrict available ReplicationConfig ,"Administrators/vendors may require restricting available replication configs. For example, to disable STANDALONE replication or restrict certain EC scheme.

This patch creates a simple validator which can enforce this validation rule.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-16 08:53:24,6
13384033,Avoid unnecessary executeBatch call in insertAudits,"If batchSize = 1000, and the auditEntries size also 1000.

Now the executeBatch will execute twice.",pull-request-available,[],HDDS,Improvement,Trivial,2021-06-16 02:55:20,29
13384022,java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN,"Steps:
 # place a file under ozone from a secure cluster with kerberos and tls, say under ozone at: /s3v/blog-test1/1.txt
 # do a hdfs dfs cat of the file:

hdfs dfs -cat ofs://ozone1/s3v/blog-test/1.txt
{code}
21/06/11 2100 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
 21/06/11 2100 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
 21/06/11 2100 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
 Jun 11, 2021 901 PM [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider
 INFO: Java 9 ALPN API unavailable (this may be normal)
 Jun 11, 2021 901 PM [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider
 INFO: netty-tcnative unavailable (this may be normal)
 java.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_linux_x86_64_fedora, netty_tcnative_linux_x86_64, netty_tcnative_x86_64, netty_tcnative]
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:104)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:592)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:136)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.defaultSslProvider(GrpcSslContexts.java:225)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:145)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.forClient(GrpcSslContexts.java:94)
 at org.apache.hadoop.hdds.scm.XceiverClientGrpc.connectToDatanode(XceiverClientGrpc.java:181)
 :
 :
 Jun 11, 2021 901 PM [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider
 INFO: Jetty ALPN unavailable (this may be normal)
 java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.JettyTlsUtil.isJettyAlpnConfigured(JettyTlsUtil.java:64)
 at [org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.findJdkProvider(GrpcSslContexts.java:249)
 :
 :
 cat: Exception getting XceiverClient: [org.apache.hadoop.ozone.shaded.com|http://org.apache.hadoop.ozone.shaded.com/].google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Could not find TLS ALPN provider; no working netty-tcnative, Conscrypt, or Jetty NPN/ALPN available
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-06-15 22:32:14,30
13383993,Add allocate block support in MockOmTransport,"Currently MockOmTransport is not handling allocateBlock API. This Jira to add allocateBlock API support.

Note: This should be a small change. It can go into master directly and back merge to EC branch.",pull-request-available,['test'],HDDS,Sub-task,Major,2021-06-15 17:11:46,8
13383918,Avoid usage of lock in listStatus,"    // We don't take a lock in this path, since we walk the
    // underlying table using an iterator. That automatically creates a
    // snapshot of the data, so we don't need these locks at a higher level
    // when we iterate.

We have above snippet in listKeys, in listStatus also we use underlying table iterator, when iterating, we might skip the lock",ScaleTest pull-request-available,['OM'],HDDS,Improvement,Major,2021-06-15 10:44:38,13
13383881,Pipeline creator may miss one-shot run,"If {{BackgroundPipelineCreatorV2}} tries to trigger one-shot run while pipeline creation is in progress (before entering {{wait()}}), then its {{notifyAll()}} call is lost.  This can result in unnecessary wait, as it will only check if it needs to run after the wait is over.

I think this may contribute to tests timing out waiting for SCM safemode exit.",pull-request-available,['SCM'],HDDS,Bug,Major,2021-06-15 07:50:04,1
13383845,Wrong cache key for integration tests,"Cache key for integration tests is incomplete, hash of pom.xml files is missing.

{code:title=https://github.com/apache/ozone/runs/2818911609#step:4:1}
Run actions/cache@v2
  with:
    path: ~/.m2/repository
    key: maven-repo--8-
    restore-keys: maven-repo--8
  maven-repo-
  maven-repo-
...
Cache restored from key: maven-repo--8-
{code}",pull-request-available,['build'],HDDS,Bug,Major,2021-06-15 05:01:06,1
13383783,Adding debug log for block token verification,This can help test verification of block token usage. ,pull-request-available,[],HDDS,Improvement,Minor,2021-06-14 18:42:55,28
13383745,EC: Add ECReplicationConfig into KeyInfo proto,This is a small Jira to add ECReplicationConfig into KeyInfo proto structure,pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2021-06-14 14:36:23,8
13383732,HTML report missing from acceptance results,"HTML report files are missing from acceptance test results:

{code}
cp: cannot stat '/mnt/ozone/hadoop-ozone/dev-support/checks/../../../target/acceptance/log.html': No such file or directory
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-06-14 13:32:56,1
13383699,Container report processing is single threaded,The container report handler is single thread in the SCM Event queue and the process is synchronized per datanode as well. Need to explore if it can be made multithreaded.,ScaleTest pull-request-available,['SCM'],HDDS,Improvement,Major,2021-06-14 10:00:38,13
13383597,Put github PR template of ozone-go,"This jira is to addressed the following scene :
In the Apache Ozone repo, the jira number in commit history bar is linked to jira,
but in the Apache Ozone-go repo is not.

First I went to the [asf yaml|https://cwiki.apache.org/confluence/display/INFRA/Git+-+.asf.yaml+features] and not found something related. (Feel free to correct me if I miss anything), then I went through many Apache Project and I found those repo with this feature having pull_request_template.md.

So I guess it is the reason, let's try it.",pull-request-available,[],HDDS,Improvement,Minor,2021-06-12 20:51:53,31
13383417,Apply container space check to Ratis factor one pipelines,"HDDS-5252 added a check that datanodes have enough space for Ratis factor three pipelines. We should have this check for Ratis factor one pipelines as well, which may otherwise be auto created based on configuration.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-11 13:05:10,22
13383415,Fix datanode capacity related race condition,"After merging master into the upgrade branch in HDDS-5321, an intermittent failure was noticed in TestSCMNodeManager#testLayoutOnHeartbeat: https://github.com/apache/ozone/runs/2787582345

The issue occurs in SCMNodeManager#register, where the node is added to the nodeStateManager firing the NEW_NODE event, before the node report containing storage information for the new node is processed. The event triggers a one shot run on the background pipeline creator which will read the node's storage information to determine if it can hold a pipeline. If the storage report has not yet been processed when this happens, no pipeline will be created to use the new node when it is registered, because the node still appears to have no free space.

Relevant log lines from the test failure:

{code}
2021-06-09 21:04:44,087 [Listener at 0.0.0.0/34005] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/b06583c0-2c53-452b-83e4-398ff0104f72
2021-06-09 21:04:44,087 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(151)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
2021-06-09 21:04:44,088 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreatorV2.java:notifyEventTriggered(282)) - trigger a one-shot run on RatisPipelineUtilsThread.
2021-06-09 21:04:44,088 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(170)) - Sending CreatePipelineCommand for pipeline:PipelineID=8bfba789-d337-4fed-9eb6-b1debd3d19e8 to datanode:b06583c0-2c53-452b-83e4-398ff0104f72
2021-06-09 21:04:44,089 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManager (PipelineStateManagerV2Impl.java:addPipeline(101)) - Created pipeline Pipeline[ Id: 8bfba789-d337-4fed-9eb6-b1debd3d19e8, Nodes: b06583c0-2c53-452b-83e4-398ff0104f72{ip: 187.106.219.59, host: localhost-187.106.219.59, ports: [STANDALONE=0, RATIS=0, REST=0, REPLICATION=0, RATIS_ADMIN=0, RATIS_SERVER=0], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-06-09T21:04:44.088Z].
2021-06-09 21:04:44,089 [RatisPipelineUtilsThread - 0] INFO  ha.SCMHAInvocationHandler (SCMHAInvocationHandler.java:invokeRatis(113)) - Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5bf60155, cost 655.117us
2021-06-09 21:04:44,091 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(170)) - Pipeline creation failed due to no sufficient healthy datanodes with enough space for even a single container. Required 3. Found 2. Container size 5368709120.
2021-06-09 21:04:44,092 [Listener at 0.0.0.0/34005] INFO  node.SCMNodeManager (SCMNodeManager.java:register(386)) - Registered Data node : b06583c0-2c53-452b-83e4-398ff0104f72{ip: 187.106.219.59, host: localhost-187.106.219.59, ports: [STANDALONE=0, RATIS=0, REST=0, REPLICATION=0, RATIS_ADMIN=0, RATIS_SERVER=0], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2021-06-09 21:04:44,093 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 3 pipelines of type RATIS and factor ONE.
2021-06-09 21:04:44,093 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:45,094 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:46,094 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:47,095 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:48,096 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:49,096 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:50,097 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:51,097 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:52,098 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:53,098 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
2021-06-09 21:04:54,099 [Listener at 0.0.0.0/34005] INFO  node.TestSCMNodeManager (TestSCMNodeManager.java:lambda$assertPipelines$10(463)) - Found 0 pipelines of type RATIS and factor THREE.
{code}

Note that the new node is the third node registered, so we would expect a Ratis factor three pipeline to be created after this event. Factor one pipeline creation succeeds for this new node due to HDDS-5337, although this is not related to this test failure.
",pull-request-available,[],HDDS,Sub-task,Major,2021-06-11 12:51:39,22
13383398,Method not found: allocateBlock - when tracing is enabled,"Jaeger tracing is broken for key creation requests.

{code:title=steps}
cd hadoop-ozone/dist/target/ozone-*/compose/ozone
export COMPOSE_FILE=docker-compose.yaml:monitoring.yaml
# console 1
OZONE_REPLICATION_FACTOR=3 ./run.sh
# console 2
docker-compose exec -T scm ozone freon ockg -n1 -t1
{code}

{noformat:title=log}
om_1          | java.lang.NoSuchMethodException: Method not found: allocateBlock
om_1          | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:65)
om_1          | 	at com.sun.proxy.$Proxy36.allocateBlock(Unknown Source)
om_1          | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.allocateBlock(OMKeyRequest.java:130)
om_1          | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.preExecute(OMKeyCreateRequest.java:151)
om_1          | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:139)
om_1          | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om_1          | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122)
{noformat}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-06-11 11:24:54,1
13383350,Remove unncessary log added durig HDDS-5263,"        LOG.info(""bharat starting from sm"");",pull-request-available,[],HDDS,Bug,Minor,2021-06-11 08:19:55,13
13383275,Remove getRequestType method from OM request classes.,Follow up from HDDS-5244. The method is no longer needed.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-06-10 21:11:37,30
13383258,Add a new column family and a service provider in Recon DB for Namespace Summaries,"Table schema:
{code:java}
objectId --> class NSSummary
{code}
New class NSSummary should at least include these fields:

1. Total number of files *directly*[1] under this directory,
 2. Total size of files directly under this directory,
 3. File Size Buckets of files directly under this directory.

Note: [1] *directly* means not counting files under any subdirectories, for now. We could further optimize the speed later by trading more writes (propagating the statistics any layers deep upwards) with speed. This is for reducing potential write-amplification for now. The stats could be extended into arbitrary layers deep later.

[06/21] Renamed ReconContainerDBProvider to ReconDBProvider

[06/21] Expanded the scope of this issue by designing a service provider interface for namespace summary, which wraps up all operations on the new schema.

 

[06/23] Merge with HDDS-5379

Because we are intending to add a new CF to the Recon container DB, the existing ContainerDBServiceProvider must be refactored to resolve conflict with a new service provider sharing the same RDB as the current one directly operates on the DB not just the CF/tables.
 # Refactor ContainerDBServiceProvider into something not directly related to the underlying RDB
 # Use a new ReconDBStore instead of containerDbStore in ContainerDBServiceProviderImpl
 # Rename ContainerDBServiceProvider/ContainerDBServiceProviderImpl into sth more appropriate
 # Combined ReconRocksDB and ReconDBProvider into one class, which manages all db-level operations. (Retained ReconRocksDB, deleted ReconDBProvider, fixed injection dependency in ReconControllerModule)",pull-request-available,[],HDDS,Sub-task,Major,2021-06-10 18:00:35,24
13383141,remove lockmanager and synchronize on containerinfo in replication manager," ReplicationManager has a LockManager object that creates locks based on the ContainerInfo.getContainerID().However, this lockManager is not shared with other classes in SCM. It seems to be passed into only RM.When processing a container, RM locks using this.ICRs and FCRs can change the replicas and details stored in ContainerInfo and they lock a container using synchronized(containerInfo) while processing that Container in the report.As RM and ICR/FCR use different locks the locks do not protect against changes in each other.

this Jira aims to remove LockManager from RM and just make it synchronize on the containerInfo object it is processing. ",pull-request-available,[],HDDS,Improvement,Major,2021-06-10 09:30:23,2
13383022,Remove 'delete container' command," 

delete container command is not currently implemented in the SCM and doesn't currently look like it will be added.  

 

So we are removing the command from the client to keep from confusing users.",pull-request-available,['Ozone CLI'],HDDS,Task,Major,2021-06-09 18:38:01,0
13382839,Avoid unncessary report processing log messages in follower,"Avoid unncessary logging in SCM followe during report processing.
{code:java}
scm3_1      | 2021-06-09 05:10:34,386 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@74c0ee39, cost 799.6us
scm3_1      | 2021-06-09 05:10:34,388 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm3_1      |   id: ""c499d751-b5c7-4ddf-b3ab-b71c75128fca""
scm3_1      |   uuid128 {
scm3_1      |     mostSigBits: -4280153224896885281
scm3_1      |     leastSigBits: -5500101187051810870
scm3_1      |   }
scm3_1      | }
scm3_1      | isLeader: true
scm3_1      | bytesWritten: 0
scm3_1      |  from dn=74c73b20-b5c0-408d-ae95-2687d1835546{ip: 192.168.0.12, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}.
scm3_1      | org.apache.ratis.protocol.exceptions.NotLeaderException: Server 6d66a209-5955-4ddb-9fbd-8cc2f578a265@group-D6BFC1238401 is not the leader 68245269-1a37-4f90-a5e4-21c8c6b7e63d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)
scm3_1      | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)
scm3_1      | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
scm3_1      | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)
scm3_1      | 	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)
scm3_1      | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:214)
scm3_1      | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)
scm3_1      | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)
scm3_1      | 	at com.sun.proxy.$Proxy14.updatePipelineState(Unknown Source)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl.openPipeline(PipelineManagerV2Impl.java:271)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:124)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:91)
scm3_1      | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:50)
scm3_1      | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
scm3_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1      | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm2_1      | 2021-06-09 05:10:34,368 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2_1      |   id: ""c499d751-b5c7-4ddf-b3ab-b71c75128fca""
scm2_1      |   uuid128 {
scm2_1      |     mostSigBits: -4280153224896885281
scm2_1      |     leastSigBits: -5500101187051810870
scm2_1      |   }
scm2_1      | }
scm2_1      | isLeader: true
scm2_1      | bytesWritten: 0
scm2_1      |  from dn=74c73b20-b5c0-408d-ae95-2687d1835546{ip: 192.168.0.12, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}.
{code}
",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-09 05:13:02,13
13382724,Merge master branch at 12e2918 into upgrade branch,"Reverse merge of master into the upgrade branch to keep it up to date, while fixing any failures after the merge.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-08 14:18:43,22
13382650,Intermittent failure in TestOzoneManagerDoubleBufferWithOMResponse,"{code:title=https://github.com/elek/ozone-build-results/blob/9d152107f3e9deac65180cb23b7956433eb1c92a/2021/06/02/8212/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.txt#L5-L11}
testDoubleBufferWithMixOfTransactions(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 0.221 s  <<< FAILURE!
java.lang.AssertionError: expected:<16> but was:<1>
  ...
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBufferWithMixOfTransactions(TestOzoneManagerDoubleBufferWithOMResponse.java:197)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-06-08 08:09:55,1
13382634,BootStrapped SCM fails to bootstrap if it connects to another bootstrapped SCM first.,"GetSCMCertificate can happen non-leader SCM, as rootCA is only run on primary SCM.
So, when an SCM is bootstrapped, let's say it connects first to a bootstrapped SCM, we fail with a SCMSecurityResponse with status set to NOT_A_PRIMARY_SCM. As we return with a response, failOver will not happen.

*SCMSecurityProtocolClientSideTranslatorPB*
{code:java}
  private SCMSecurityResponse handleError(SCMSecurityResponse resp)
      throws SCMSecurityException {
    if (resp.getStatus() != SCMSecurityProtocolProtos.Status.OK) {
      throw new SCMSecurityException(resp.getMessage(),
          SCMSecurityException.ErrorCode.values()[resp.getStatus().ordinal()]);
    }
    return resp;
  }
{code}

To solve this issue, one possible solution is on server check if it is SCMSecurityException with errorCode NOT_A_PRIMARY_SCM return a RetriableWithFailOverException. In this way, FailOverProxyProvider performs failOver and Retry to the next SCM.

The exception message is available in comments.
",pull-request-available,"['SCM HA', 'Security']",HDDS,Bug,Blocker,2021-06-08 06:48:40,13
13382586,Fix datanode reserved space calculation,"Here we found a bad case with datanode reserved space:

Say we have 1TB volume and 500GB reserved space configed, e.g. /data1:500GB.

So we intend to reserve this 500GB to another app, e.g. yarn, then yarn consumed all 500GB.

Then we found that the available space of ozone is 0, which is not intended.

The root cause is the following piece:

 
{code:java}
// VolumeInfo.java
public long getAvailable() {
 return Math.max(usage.getAvailable() - reservedInBytes, 0);
}
...
// VolumeUsage.java
public long getAvailable() {
  long l = source.getCapacity() - source.getUsedSpace();         // 1TB - 500GB = 500GB
  return Math.max(Math.min(l, source.getAvailable()), 0);
}{code}
 

Here usage.getAvailable() will return 500GB, and it should not sub reservedInBytes there otherwise it produces a 0.

The reservedInBytes should only be sub at `getCapacity()`.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-08 01:58:06,21
13382582,Skip storing unwanted block tokens on OM DB,"When persisting the allocated blocks during writes to keys, skip writing the block token to the DB. 

I am testing a fix and will post a PR once testing is done.",pull-request-available,['OM'],HDDS,Bug,Major,2021-06-08 00:06:35,32
13382562,Show number of Open containers per Node in Recon UI,The Recon UI DataNode page only shows the total number of containers. This jira aims to show the open containers count on the UI.,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2021-06-07 20:04:45,24
13382535,ContainerInfo should use ReplicationConfig,"We introduced ReplicationConfig to most classes already, but we missed ContainerInfo.

This change will ensure that ContainerInfo uses ReplicationConfig rather than the legacy Type and Factor fields.",pull-request-available,[],HDDS,Sub-task,Major,2021-06-07 16:41:44,11
13382476,Intermittent failure in SCM Ratis integration test,"Some integration tests intermittently fail due to mini cluster not existing safe mode within 2 minutes timeout.  The problem is that pipeline creation interval is also 2 minutes.  It may happen that pipeline is created only while the cluster is being shut down due to timeout.

{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot-output.txt}
2021-06-02 03:21:03,005 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(151)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
...
2021-06-02 03:21:04,007 [Listener at 127.0.0.1/40677] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(224)) - Nodes are ready. Got 3 of 3 DN Heartbeats.
...
2021-06-02 03:22:59,107 [Listener at 127.0.0.1/40677] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(443)) - Shutting down the Mini Ozone Cluster
...
2021-06-02 03:23:03,031 [6d4e3dd1-e161-4c07-861b-817db46a0427@group-0D81E0660BF9-StateMachineUpdater] INFO  pipeline.PipelineStateManager (PipelineStateManagerV2Impl.java:addPipeline(101)) - Created pipeline Pipeline ... RATIS/THREE ...
{noformat}

{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt}
org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot  Time elapsed: 146.994 s  <<< ERROR!
java.util.concurrent.TimeoutException: 
  ...
  at org.apache.hadoop.ozone.MiniOzoneClusterImpl.waitForClusterToBeReady(MiniOzoneClusterImpl.java:217)
{noformat}

Related test failures:

{noformat}
2021/05/26/8113/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis.txt
2021/05/26/8118/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA.txt
2021/05/27/8142/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
2021/05/30/8164/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestOzoneManagerRestInterface.txt
2021/05/31/8166/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMSnapshot.txt
2021/05/31/8177/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestPipelineClose.txt
2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
2021/06/02/8193/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA.txt
2021/06/02/8211/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMSnapshot.txt
2021/06/02/8217/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
2021/06/07/8299/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt
{noformat}",pull-request-available,"['SCM HA', 'test']",HDDS,Bug,Major,2021-06-07 12:54:39,1
13382401,Enhance freon streaing generator to support multiple threads,Please see: https://github.com/apache/ozone/pull/2306,pull-request-available,[],HDDS,Improvement,Major,2021-06-07 08:00:30,6
13382225,"Remove checkAclRight method, duplicates checkAclRights","I found there are two functions performing the same purpose and the same logic. Would like to have a review if we could reduce to only one function.

*java source code:*
java/org/apache/hadoop/ozone/om/helpers/OzoneAclUtil.java

*class method:*
checkAclRight() and checkAclRights()",patch pull-request-available,['OM'],HDDS,Improvement,Trivial,2021-06-04 20:52:49,33
13382147,InterSCM protocol should be server-only,"InterSCM protocol is defined in client interface, but is concerned only with server-to-server communication.",pull-request-available,['SCM HA'],HDDS,Improvement,Major,2021-06-04 13:18:55,1
13382124,OzoneDelegationTokenSecretManager breaks the interface contract of S3SecretManager,"S3SecretManager is a generic interface which supposed to return with the secret key for one specific AWS access key id.

It's a generic interface which may have multiple implementation.

Unfortunately, it's not possible to use any implementation as OzoneDelegationTokenSecretManager does an explicit cast to retrieve the MetadataManager.

Instead of breaking the abstract contract of interface it seems to be better to directly inject the required MetadataManager to the  OzoneDelegationTokenSecretManager which makes it possible to use an implementation.",pull-request-available,[],HDDS,Bug,Trivial,2021-06-04 10:39:31,6
13381954,Use netty-bom to ensure consistent Netty version,There are multiple Netty versions pulled in through transitive dependencies. netty-bom can simplify the dependencyManagement section of the top level pom. ,pull-request-available,[],HDDS,Task,Minor,2021-06-03 16:54:04,34
13381892,Fix bugs in the scale test data generator (HDDS-4395),"HDDS-4395 implemented the scale test data generator. Found a few bugs
(1) OM's key name is not computed correctly such that the key names get overwritten over and over again.
(2) the generator for DataNode always got stuck at 33% progress.
(3) if the container id offset is > 1, the intermediate directory may be skipped and OM namespace gets corrupted partially.",pull-request-available,[],HDDS,Bug,Major,2021-06-03 11:59:36,25
13381838,testCRLStatusReportPublisher fails to create CRLInfo,"{code:title=https://github.com/apache/ozone/runs/2732634889#step:4:852}
[INFO] Running org.apache.hadoop.ozone.container.common.report.TestReportPublisher
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.665 s <<< FAILURE! - in org.apache.hadoop.ozone.container.common.report.TestReportPublisher
[ERROR] testCRLStatusReportPublisher(org.apache.hadoop.ozone.container.common.report.TestReportPublisher)  Time elapsed: 1.518 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.hadoop.hdds.security.x509.crl.CRLInfo.<init>(CRLInfo.java:47)
	at org.apache.hadoop.hdds.security.x509.crl.CRLInfo.<init>(CRLInfo.java:38)
	at org.apache.hadoop.hdds.security.x509.crl.CRLInfo$Builder.build(CRLInfo.java:219)
	at org.apache.hadoop.ozone.container.common.report.TestReportPublisher.testCRLStatusReportPublisher(TestReportPublisher.java:188)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-06-03 06:34:22,1
13381740,Introduce the WritableContainerInterface to SCM,"When implementing EC, we need to allocate blocks to containers on EC pipelines as well as Ratis pipelines (the current code path). The class `BlockManager.allocateBlock(...)` currently does this work.

To allow EC implementation to progress more smoothly, I would like to have BlockManager user a new interface to obtain a container for a block, namely the WritableContainerProvider interface.

This change is just a refactor to add the interface. Existing functionality is unchanged.",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-06-02 16:39:30,11
13381681,Handle SIGTERM to ensure clean shutdown of OM/DN/SCM,"Handle SIGTERM 15 with shutdown hook to properly/clean shutdown OM/DN/SCM.

In this way in OM HA/DN/SCM HA snapshot will be called and pending transactions will be flushed to DB.

",pull-request-available,"['Ozone Datanode', 'Ozone Manager']",HDDS,Improvement,Major,2021-06-02 11:45:39,13
13381669,Handle SIGTERM to ensure clean shutdown of SCM,"Handle SIGTERM 15 with shutdown hook to properly/clean shutdown SCM.

In this way in SCM HA, the snapshot will be called and pending transactions will be flushed to DB.

",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-06-02 10:22:49,13
13381647,Update container's deleteTransactionId on creation of the transaction in SCM,cc ~ [~glengeng],pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-06-02 09:10:09,35
13381629,Recon UI cannot sort DN by operational state, !image-2021-06-02-15-22-47-256.png! ,pull-request-available,[],HDDS,Bug,Major,2021-06-02 07:23:16,17
13381468,Document SCM HA classes,Add class-level javadoc for {{SCMDBCheckpointProvider}} and {{SCMHANodeDetails}}.,pull-request-available,['SCM'],HDDS,Task,Major,2021-06-01 14:43:10,17
13381467,Document block deleting service classes,Add class-level javadoc for {{DeletedBlockLogStateManager}} and {{DeletedBlockLogStateManagerImpl}}.,pull-request-available,['SCM'],HDDS,Task,Major,2021-06-01 14:42:00,17
13381420,[SCM-HA] SCM start failed with PipelineNotFoundException,"{code:java}
scm.log 
2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:028fed4a-0087-4b70-b6e3-11f18d739094
2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:a4b76016-dc24-47f2-a3ff-03c309fdcf9b
2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:ed9d4872-166d-41c6-96ab-437a44e4168b
2021-05-27 09:55:42,199 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-05-27T09:55:42.189Z].
2021-05-27 09:55:54,426 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:028fed4a-0087-4b70-b6e3-11f18d739094, CreationTimestamp2021-05-27T09:55:42.189Z] moved to OPEN state
2021-05-27 10:06:45,920 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=cd4a2a77-9715-4437-8d1d-3618a2c93103, PipelineID=ca6100b9-b42c-4b77-bef5-35a9b1e725f2, PipelineID=875b2073-4034-4374-bba6-39011294a280]
2021-05-27 10:06:45,932 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:DORMANT, leaderId:a4b76016-dc24-47f2-a3ff-03c309fdcf9b, CreationTimestamp2021-05-27T09:55:42.189Z] moved to CLOSED state
2021-05-27 10:06:57,921 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=cd4a2a77-9715-4437-8d1d-3618a2c93103, PipelineID875b2073-4034-4374-bba6-39011294a280, PipelineID=2878c722-84dc-40f9-b1c1-46ed0f8bcdd7]
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Scrubbing pipeline: id: PipelineID=875b2073-4034-4374-bba6-39011294a280 since it stays at CLOSED stage.
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode 028fed4a-0087-4b70-b6e3-11f18d739094
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode a4b76016-dc24-47f2-a3ff-03c309fdcf9b
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode ed9d4872-166d-41c6-96ab-437a44e4168b
2021-05-27 10:07:41,075 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:a4b76016-dc24-47f2-a3ff-03c309fdcf9b, CreationTimestamp2021-05-27T09:55:42.189Z] removed.
{code}
The logs indicate that, a pipeline got created, moved to open state, and then one of the datanodes went stale, thereby the pipeline moved to closed state. The pipeline got scrubbed by the pipeline scrubber and got deleted. 
{code:java}
2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Scrubbing pipeline: id: PipelineID=875b2073-4034-4374-bba6-39011294a280 since it stays at CLOSED stage.{code}
Next update for the pipeline to be moved to close state as a part of  report from other datanodes in the pipeline will fail as the pipeline is removed from scm memory/db and hence scm terminates.

The solution would be to ignore PipelineNotFoundException in PipelineStateManagerV2Impl#updatePipelineState.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-06-01 10:00:08,16
13381374,getStorageSize cast to int can cause issue,"XceiverServerRatis.java:438
{code:java}
 final int pendingRequestsByteLimit = (int)conf.getStorageSize(
        OzoneConfigKeys.DFS_CONTAINER_RATIS_LEADER_PENDING_BYTES_LIMIT,
        OzoneConfigKeys.DFS_CONTAINER_RATIS_LEADER_PENDING_BYTES_LIMIT_DEFAULT,
        StorageUnit.BYTES);
{code}
because of int limitation, the limit can not be set beyond 2 GB.

 ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-06-01 05:14:03,26
13381270,Relocate classes copied from Hadoop,"Ozone has some test classes partially copied from Hadoop, located in the original packages.  Relocate these to avoid classpath conflict.",pull-request-available,[],HDDS,Task,Major,2021-05-31 11:40:30,1
13381219,Add reinitialize() for SequenceIdGenerator.,"After installSnapshot, the bootstrapped SCM crashed in a short time while there is on-going write workload.

 

Clues from the core dump file, the new added SCM crashed in thread StateMachineUpdater, while accessing RocksDB. 
{code:java}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fcefbb5fc0f, pid=1406, tid=0x00007fceecbcb700
#
# JRE version: OpenJDK Runtime Environment (8.0_232) (build 1.8.0_232-86)
# Java VM: OpenJDK 64-Bit Server VM (25.232-b86 mixed mode, sharing linux-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni7209090472417999125.so+0x242c0f]  rocksdb_get_helper(JNIEnv_*, rocksdb::DB*, rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int, int)+0xcf
#
# Core dump written. Default location: /root/core or core.1406
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#---------------  T H R E A D  ---------------Current thread (0x00007fcf3ded2800):  JavaThread ""7a85dabc-3f8c-47e1-bf0a-de75abe92820@group-691FBC3A273C-StateMachineUpdater"" daemon [_thread_in_native, id=1559, stack(0x00007fceecacb000,0x00007fceecbcc000)]siginfo: si_signo: 11 (SIGSEGV), si_code: 128 (SI_KERNEL), si_addr: 0x0000000000000000
{code}
 
{code:java}
Stack: [0x00007fceecacb000,0x00007fceecbcc000],  sp=0x00007fceecbc96a0,  free space=1017k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [librocksdbjni7209090472417999125.so+0x242c0f]  rocksdb_get_helper(JNIEnv_*, rocksdb::DB*, rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int, int)+0xcf
C  [librocksdbjni7209090472417999125.so+0x242ea2]  Java_org_rocksdb_RocksDB_get__J_3BIIJ+0x62
j  org.rocksdb.RocksDB.get(J[BIIJ)[B+0
j  org.rocksdb.RocksDB.get(Lorg/rocksdb/ColumnFamilyHandle;[B)[B+13
j  org.apache.hadoop.hdds.utils.db.RDBTable.get([B)[B+9
j  org.apache.hadoop.hdds.utils.db.RDBTable.get(Ljava/lang/Object;)Ljava/lang/Object;+5
j  org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(Ljava/lang/Object;)Ljava/lang/Object;+14
j  org.apache.hadoop.hdds.utils.db.TypedTable.get(Ljava/lang/Object;)Ljava/lang/Object;+61
j  org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManagerImpl.lambda$allocateBatch$0(Ljava/lang/String;)Ljava/lang/Long;+5
j  org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManagerImpl$$Lambda$444.apply(Ljava/lang/Object;)Ljava/lang/Object;+8
J 3481 C1 java.util.concurrent.ConcurrentHashMap.computeIfAbsent(Ljava/lang/Object;Ljava/util/function/Function;)Ljava/lang/Object; (493 bytes) @ 0x00007fcf2daeb9e4 [0x00007fcf2daeb160+0x884]
j  org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManagerImpl.allocateBatch(Ljava/lang/String;Ljava/lang/Long;Ljava/lang/Long;)Ljava/lang/Boolean;+11
v  ~StubRoutines::call_stub
V  [libjvm.so+0x682be8]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1048
V  [libjvm.so+0x9a9b49]  Reflection::invoke(instanceKlassHandle, methodHandle, Handle, bool, objArrayHandle, BasicType, objArrayHandle, bool, Thread*)+0x599
V  [libjvm.so+0x9ad7ed]  Reflection::invoke_method(oopDesc*, Handle, objArrayHandle, Thread*)+0x14d
V  [libjvm.so+0x725a66]  JVM_InvokeMethod+0x1e6
J 2759  sun.reflect.NativeMethodAccessorImpl.invoke0(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (0 bytes) @ 0x00007fcf2d2a827d [0x00007fcf2d2a8180+0xfd]
J 2758 C1 sun.reflect.NativeMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (104 bytes) @ 0x00007fcf2d33f194 [0x00007fcf2d33dec0+0x12d4]
J 5190 C2 sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (10 bytes) @ 0x00007fcf2dff1968 [0x00007fcf2dff1920+0x48]
j  java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object;+56
j  org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(Lorg/apache/hadoop/hdds/scm/ha/SCMRatisRequest;)Lorg/apache/ratis/protocol/Message;+68
j  org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(Lorg/apache/ratis/statemachine/TransactionContext;)Ljava/util/concurrent/CompletableFuture;+27
j  org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(Lorg/apache/ratis/proto/RaftProtos$LogEntryProto;)Ljava/util/concurrent/CompletableFuture;+126
j  org.apache.ratis.server.impl.StateMachineUpdater.applyLog()Lorg/apache/ratis/util/MemoizedSupplier;+142
j  org.apache.ratis.server.impl.StateMachineUpdater.run()V+29
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.so+0x682be8]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1048
V  [libjvm.so+0x684127]  JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x2f7
V  [libjvm.so+0x684660]  JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x60
V  [libjvm.so+0x71c121]  thread_entry(JavaThread*, Thread*)+0x91
V  [libjvm.so+0xa8c671]  JavaThread::thread_main_inner()+0xf1
V  [libjvm.so+0x938f12]  java_start(Thread*)+0x132
C  [libpthread.so.0+0x7eb5]  start_thread+0xc5
{code}
 

The root cause is missing reinitialize() in SequenceIdGenerator, thereby after installing snapshot, SequenceIdGenerator holds a dangling reference to the old removed RocksDB.

 ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-31 07:04:01,36
13380905,Make XceiverClientManager creation when necessary in ContainerOperationClient,"ContainerOperation Client creates XceiverClientManager.

XceiverClientManager requires to getCA list.


{code:java}
      manager = new XceiverClientManager(conf,
          conf.getObject(XceiverClientManager.ScmClientConfig.class),
          caCertificates);
{code}

We can avoid listCA which is not required for most admin commands. It is required only for ChunkKeyHandler.

This will help when ACLS are configured for SCM security protocol where only admin/service principals can make calls to the SCMSecurityProtocol server, then we don't need to add all the users to them to make these commands work.

As for few of the commands like pipeline list, safe mode status we don't require admin privilege.
",pull-request-available,[],HDDS,Improvement,Major,2021-05-28 04:41:43,13
13380865,OFS mkdir -p does not work when Volume is not pre-created.,"{code}
ozone fs -mkdir -p ofs://ozone1/tgtvol/tgtbuck/passwd
{code}


{code}
21/05/27 17:57:12 DEBUG shell.Command: mkdir failure
VOLUME_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Volume tgtvol is not found
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:604)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getBucketInfo(OzoneManagerProtocolClientSideTranslatorPB.java:474)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.getBucketDetails(RpcClient.java:615)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getBucket(BasicRootedOzoneClientAdapterImpl.java:268)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getBucket(BasicRootedOzoneClientAdapterImpl.java:204)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.createDirectory(BasicRootedOzoneClientAdapterImpl.java:420)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.mkdir(BasicRootedOzoneFileSystem.java:711)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.mkdirs(BasicRootedOzoneFileSystem.java:722)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2376)
	at org.apache.hadoop.fs.shell.Mkdir.processNonexistentPath(Mkdir.java:87)
{code}
",pull-request-available,['OFS'],HDDS,Bug,Blocker,2021-05-27 21:22:27,12
13380627,"Recon shows operational status as ""DECOMMISSIONING"" for ""DECOMMISSIONED"" DNs","Decommissioned nodes have ""DECOMMISSIONING"" operational status in recon.


RCA
Recon relies on DN heartbeats to understand Node operational state. If the node goes down before it reports itself as DECOMMISSIONED, then there is a loss of information on Recon side. It is the SCM that moves a node form DECOMMISSIONING to DECOMMISSIONED state first, then the Datanode persists the state change locally, and then heartbeats with the new state to SCM & Recon subsequently.  If the DN is shutdown before it can heartbeat the state change to Recon, then Recon lives with the stale information.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2021-05-26 21:20:07,30
13380541,Use built-in cancel support for duplicates,GitHub Actions now provides support to cancel duplicate workflows.  This usage of the {{cancel-workflow-runs}} action can be replaced.,pull-request-available,['build'],HDDS,Task,Major,2021-05-26 12:58:31,1
13380423,Datanode Report Publisher publishes one extra report after DN shutdown,"There is one extra report that gets published by Datanodes even after shutdown. Ideally, this should be avoided and DN should not publish reports after shutdown. ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-05-25 23:48:29,37
13380387,Revert HDDS-5153,"After some discussion with [~pifta] and [~swagle] we believe that the change in HDDS-5153 should be reverted.

If a DN starts decommissioning or maintenance, but goes dead before it completes the process, then the node is moved back to a state of IN_SERVICE and DEAD by the decommission monitor when it notices it has become dead. This is because decommission should gracefully remove the node, but it goes dead first, we may not be able to replicate its containers. In this case decommission effectively fails.

In HDDS-5153, we decided that if a node is already dead and you decommission it, it should immediately move to DECOMMISSIONED. However that is not really consistent with the above behaviour.

Also, there is no real value in decommissioning a dead node - it does not do anything except adjust its state in SCM.

To keep things consistent, I propose we revert HDDS-5153 so starting decommission on a dead node will work the same as when a node goes dead part way through decommission. In both cases the node will end up as IN_SERVICE + DEAD.",pull-request-available,['SCM'],HDDS,Bug,Major,2021-05-25 16:39:51,11
13380375,Handle unsecure cluster convert to secure cluster for SCM,"In SCM sub-ca certs are setup during init, if a cluster is converted to secure later, in else part of the scmInit, we need to initialize security.

OM handled this scenario, SCM also needs a similar fix.
https://github.com/apache/ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L963",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-25 15:29:43,13
13380303,Make ozonefs.robot execution repeatable,"Most of our smoketests can be executed multiple times, which helps us to debug the tests locally.

Unfortunately it's not the case ozonefs/ozonefs.robot. This test uses hard-coded bucket/volume names instead of using randomized bucket names.

For example:

{code}
docker-compose up -d --scale datanode=3
docker-compose exec scm bash
robot smoketest/ozonefs/ozonefs.robot

robot smoketest/ozonefs/ozonefs.robot
{code}

The second execution fails which makes harder to debug test problems.

It seems to be better to follow existing pattern and adding a random prefix to the user bucket names.",pull-request-available,[],HDDS,Bug,Major,2021-05-25 10:13:08,6
13380085,Datandoe with low ratis log volume space should not be considered for new pipeline allocation,"Ratis volume report is sent to scm via datanodes. While allocating a new pipline, pipeline placement policy should ensure datanodes with low ratis volume space are not chosen for pipeline allocation.",pull-request-available,[],HDDS,Sub-task,Major,2021-05-24 11:56:30,21
13380083,Ensure disk checker also scans the ratis log disks periodically,"Ratis log disks if configured separately from Hdds disks, need to be scanned periodically like Hdds volumes and reported back to SCM. A datanode with no healthy ratis log disk should not participate in pipeline allocation.

Similarly, a datandoe with low ratis log volume space should not be considered for new pipeline allocation.",pull-request-available,[],HDDS,Sub-task,Major,2021-05-24 11:53:09,21
13380043,Misspelt words in S3MultipartUploadCommitPartRequest.java line 202,"Misspelt words in S3MultipartUploadCommitPartRequest.java line 202.
{code:java}
// S3MultipartUplodaCommitPartResponse before being added to
{code}
change S3MultipartUplodaCommitPartResponse to S3MultipartUploadCommitPartResponse",pull-request-available,[],HDDS,Bug,Trivial,2021-05-24 08:11:43,38
13380040,Leverage getSmallFile in ChunkInputStream,"When read data,  currently client first call GetBlock to retrived the Block with Chunk info，then call ReadChunk to get the data.   For small blocks(we can konw this from key's info), we can leverage getSmallFile  to save one RPC call. ",pull-request-available,[],HDDS,Improvement,Major,2021-05-24 08:00:19,5
13379897,SCM should send token for CloseContainer command,Close container command from SCM fails due to lack of token.,pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2021-05-22 07:52:43,1
13379836,SCM may stay in safe mode forever due to incorrect open pipeline count,"After an unclean shutdown, SCM may never come out of the safe mode.
Attached a document to explain the problem and the proposal.
More description with log info is available in the comments.
",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-05-21 18:40:08,13
13379732,Bump node to v16.2.0 for Recon,"Current node v12.14.1 has [CVEs|https://www.cvedetails.com/vulnerability-list/vendor_id-12113/Nodejs.html]. v12 is going [EoL|https://nodejs.org/en/about/releases/] in less than a year.

I see no reason not to upgrade it to the latest and greatest here. Compiled and ran locally in docker, Recon UI works just fine.

Also running tests on my [fork|https://github.com/smengcl/hadoop-ozone/commits/recon-node-v16] before posting a PR.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2021-05-21 10:17:59,12
13379484,Avoid SCM call to get CA certs in non-HA from OM.,"OM loads up its signed cert, rootCA cert and CA cert during startup.
So to get CA list in OM, we can use certClient and get them, in this way we can avoid unncessary network call to OM.",pull-request-available,[],HDDS,Improvement,Major,2021-05-20 10:24:47,13
13379450,Fix fall back of config in SCM HA Cluster,"If config is appended with serviceId and nodeId use that, else fall back to config appended without service id and node id. If that is also not defined, fallback to a default value.

*Example:*
ozone.scm.grpc.port = 9898 is set

We should use this port for SCM Grpc Service, as there is no port config with serviceid and nodeid defined.

Current code behavior is if port config with service id and node id not defined read from the default value.


The problematic code is

{code:java}
        String ratisPortKey = ConfUtils.addKeySuffixes(OZONE_SCM_RATIS_PORT_KEY,
            serviceId, nodeId);
int ratisPort = conf.getInt(ratisPortKey, OZONE_SCM_RATIS_PORT_DEFAULT);
{code}

In this way fix similarly for all RPC services.
",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-20 07:25:30,13
13379409,Pipeline placement policy filter datanodes that have not enough space for a single container,"Pipeline placement policy should filter out those datanodes that do not even have enough space for a single container, otherwise even if the pipeline creation succeeds, a datanode may quickly report that it fails to create a container and there will be successive failures because we will choose that node again and again.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2021-05-20 02:32:40,21
13379329,Build integration tests with Maven cache,"Currently only _integration_ check uses the {{ozone-builder}} Docker image.  The image is getting outdated and takes some effort to periodically update.  Investigate if _integration_ can be changed to build with Maven cache, like _unit_.

Also, it seems execution of Hugo (which is available in the image) is sometimes getting stuck recently:

* https://github.com/apache/ozone/runs/2607134035?check_suite_focus=true#step:4:1393
* https://github.com/apache/ozone/runs/2601041142?check_suite_focus=true#step:4:1436
* https://github.com/apache/ozone/runs/2600238841?check_suite_focus=true#step:4:1436
* https://github.com/adoroszlai/hadoop-ozone/runs/2601105245?check_suite_focus=true#step:4:1436
* https://github.com/adoroszlai/hadoop-ozone/runs/2621383226#step:4:1393",pull-request-available,['build'],HDDS,Improvement,Critical,2021-05-19 17:01:09,1
13379292,Race Condition between Full and Incremental Container Reports,"During testing we came across an issue with ICR and FCR handing.

The following log shows the issue:

{code}
2021-05-18 13:14:15,394 DEBUG org.apache.hadoop.hdds.scm.container.ContainerReportHandler: Processing replica of container #1 from datanode 945aa180-5cff-4298-a8ad-8197542e4562{ip: 172.27.108.136, host: quasar-nqdywv-7.quasar-nqdywv.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}


2021-05-18 13:14:15,394 DEBUG org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler: Processing replica of container #1001 from datanode 945aa180-5cff-4298-a8ad-8197542e4562{ip: 172.27.108.136, host: quasar-nqdywv-7.quasar-nqdywv.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}


2021-05-18 13:14:15,394 DEBUG org.apache.hadoop.hdds.scm.container.ContainerReportHandler: Processing replica of container #2 from datanode 945aa180-5cff-4298-a8ad-8197542e4562{ip: 172.27.108.136, host: quasar-nqdywv-7.quasar-nqdywv.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2021-05-18 13:14:15,394 DEBUG org.apache.hadoop.hdds.scm.container.ContainerReportHandler: Processing replica of container #3 from datanode 945aa180-5cff-4298-a8ad-8197542e4562{ip: 172.27.108.136, host: quasar-nqdywv-7.quasar-nqdywv.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2021-05-18 13:14:15,394 DEBUG org.apache.hadoop.hdds.scm.container.ContainerReportHandler: Processing replica of container #4 from datanode 945aa180-5cff-4298-a8ad-8197542e4562{ip: 172.27.108.136, host: quasar-nqdywv-7.quasar-nqdywv.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
...
{code}

In the above log, SCM is processing both an ICR and FCR for the same Datanode at the same time. The FCR does not container container #1001.

The FCR starts first, and it takes a snapshot of the containers on the node via NodeManager.

Then it starts processing the containers one by one.

The ICR then starts, and it added #1001 to the ContainerManager and to the NodeManager.

When the FCR completes, it replaces the list of containers in NodeManager with those in the FCR.

At this point, container #1001 is in the ContainerManager, but it is not listed against the node in NodeManager.

This would get fixed by the next FCR, but then the node goes dead. The dead node handler runs and uses the list of containers in NodeManager to remove all containers for the node. As #1001 is not listed, it is not removed by the DeadNodeManager. This means the container will never been seen as under replicated, as 3 copies will exist forever in the ContainerManager.
",pull-request-available,['SCM'],HDDS,Bug,Major,2021-05-19 12:58:20,11
13379263,SCM HA: Continuous PipelineNotFoundException seen in SCM log,"After running decommissioning tests and aborting few decommissioning command, seeing PipelineNotFoundException INFO logs continuously in scm log.
Although these are INFO logs, raising the issue as the exceptions are thrown in loop.


{code:java}
2021-05-16 20:06:53,421 INFO org.apache.hadoop.ipc.Server: IPC Server handler 53 on 9860, call Call#102 Retry#23 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.27.176.196:41982
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=b1431673-adf0-402f-968f-e9241ea37609 not found
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:121)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.getPipeline(PipelineStateManagerV2Impl.java:125)
	at jdk.internal.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeLocal(SCMHAInvocationHandler.java:83)
	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:68)
	at com.sun.proxy.$Proxy14.getPipeline(Unknown Source)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl.getPipeline(PipelineManagerV2Impl.java:198)
	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:529)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:497)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:153)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46400)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:533)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887)
2021-05-16 20:06:55,426 INFO org.apache.hadoop.ipc.Server: IPC Server handler 84 on 9860, call Call#102 Retry#24 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.27.176.196:41982
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=b1431673-adf0-402f-968f-e9241ea37609 not found
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:121)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.getPipeline(PipelineStateManagerV2Impl.java:125)
	at jdk.internal.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeLocal(SCMHAInvocationHandler.java:83)
	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:68)
	at com.sun.proxy.$Proxy14.getPipeline(Unknown Source)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl.getPipeline(PipelineManagerV2Impl.java:198)
	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:529)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:497)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:153)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46400)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:533)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887)
{code}
",pull-request-available,"['SCM', 'SCM HA']",HDDS,Sub-task,Major,2021-05-19 10:10:11,35
13379260,EC: Create ECReplicationConfig on client side based on input string,"HDDS-5073 improves the existing ""ozone sh"" client to support ReplicationConfig. The input string is parsed to ReplicationConfig by the constructors of the ReplicationConfig classes with string parameters.

After merging this improvement to the EC branch we need to implement the same constructor for ECReplicationConfig.

There are multiple options here:

 1. Create an enum with ALL the possible ECReplicationConfig
 2. Use meaningful programmatic validation rules.

During the EC sync we agreed that 2nd option can be more flexible as we may have very huge configuration matrix with all the EC parameters.
 ",pull-request-available,[],HDDS,Sub-task,Major,2021-05-19 09:55:47,6
13379239,Wait for ever to obtain CA list which is needed during OM/DN startup,"Right now during OM/DN startup we wait a total duration of 300seconds and wait in between each attempt for 10seconds to obtain CA list and check the recieved CA list size is as expected count.

So, in case DN's are started before and SCM's have not completed bootstrap after 300 seconds  DN/OM will fail to start. This Jira is to add support for retry forever and with fixed sleep, in this way, DN's can come up even if they are started before bootstrapping SCMs.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-19 08:47:36,13
13379159,Fix OzoneContainer TLS configuration,The TLS configuration is used by CreatePipelineCommandHandler on datanode to create pipeline across datanodes. It currently does not allow enable/disable like other TLS configurations and does not use mTLS when it is needed here. ,pull-request-available,[],HDDS,Bug,Major,2021-05-18 23:38:23,28
13379091,Allow multiple OM request versions to be supported at same layout version (HDDS-2939).,Remove use of instance factory in OM request handling switch. Added a new point cut in the OM aspect that makes sure new OM requests can be brought in through a layout version.,pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Major,2021-05-18 15:55:17,30
13379084,Return latest version of key location for client,"Currently, OmKeyInfo uses a `keyLocationVersions: List<OmKeyLocationInfoGroup>` to store all versions of KeyLocation information. If a key has lots of versions, OM needs to reply all the content of `keyLocationVersions` to the client, which may cause the following IPC warning, when the size exceeds the client IPC response size limit becomes an error.
{code:java}
2021-05-18 23:10:07,246 [IPC Server handler 97 on default port 9862] WARN org.apache.hadoop.ipc.Server: Large response size 20541123 for call Call#3 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.130.20.239:46306
{code}
This ticket is to add an config for client to only query the latest content in `keyLocationVersions`, which can reduce the IPC response size and improve OM's performance by excluding lots of unnecessary work with old key locations.",pull-request-available,[],HDDS,Improvement,Major,2021-05-18 15:14:11,7
13379078,Skip failing acceptance suite by default,"Acceptance tests are split into a few suites, which can be activated by setting {{OZONE_ACCEPTANCE_SUITE}}.  CI checks execute 3 splits for faster feedback:

 * {{secure}}
 * {{unsecure}}
 * {{misc}} + anything without {{suite}}

There are some other tests, tagged {{suite:failing}}, which exist only to test the behavior of the acceptance test runner when failures happen.

The goal of this task is to skip the {{failing}} suite by default, i.e. when acceptance tests are run without suite filter.  This would allow running all tests in sequence (e.g. in nightly job) while retaining the ability to run these special tests by setting {{OZONE_ACCEPTANCE_SUITE=failing}}.",pull-request-available,['test'],HDDS,Improvement,Major,2021-05-18 14:55:52,1
13379016,SCM UI should have leader/follower and Primordial SCM information,"In SCM HA setup, following informations need to be included in SCM UI.

1. SCM leader/follower information.
2. Primordial SCM / CA server information.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-05-18 11:51:04,26
13378925,Fix out of bound exception when loading auditparser,"{code:java}
ozone auditparser ~/audit.db load /logs/om-audit.log{code}
The command throw out of bound exception:

Index 6 out of bounds for length 6.

The audit log like this

!image-2021-05-18-10-37-41-770.png!

There is ""\n"" after ""partNumber: 1"" and ""partName***""

so, bReader.readLine() only read before the first ""\n""",pull-request-available,[],HDDS,Bug,Major,2021-05-18 02:24:33,29
13378806,Disable animal-sniffer maven plugin,Please see: https://github.com/apache/ozone/pull/2252,pull-request-available,[],HDDS,Improvement,Major,2021-05-17 12:30:31,6
13378799,Add SSL support to the Ozone streaming API,"HDDS-5142 will introduce a new streaming API for closed container replication / snapshot download and other data movement.

For server2server communication we need to support mTLS. We should configure pure mTLS on the netty server ",pull-request-available,[],HDDS,Improvement,Major,2021-05-17 12:08:35,6
13378785,Require block token for more operations,"Require block token in datanode for the following operations:

* CompactChunk (currently unsupported operation)
* DeleteBlock
* DeleteChunk
* GetCommittedBlockLength
* ListChunk (currently unsupported operation)

Require container token for ListBlock (currently unsupported operation).

Do not require container token for ListContainer (currently unsupported operation), as it does not have container ID in the request.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2021-05-17 10:59:41,1
13378780,Recon counts deleted containers to missing containers,"When a container is deleted, SCM has ReplicationManager to check if the container is empty, then set container's state to DELETING, then DELETED.

While Recon doesn't have ReplicationManager for this work, then ContainerHealthTask finds the replica of the deleted container is 0 and mark the container as MISSING.

The bug can be reproduced by creating some big keys with Freon and then delete them.

This ticket is for adding some prechecks for ContaienrHealthTask before setting a container to MISSING state.

 ",pull-request-available,[],HDDS,Bug,Major,2021-05-17 10:25:28,7
13378754,Change default grpc and ratis ports for scm ha,"Currently, by default scm ratis and scm grpc ports are defined to be 9865 and 9866 by default. These ports are also used for datanode https and datanode ports for hdfs by default hence may lead to port conflict issue in ozone deployment alongside hdfs .

987X series is used by OM and HDFS, 988X series is used by ozone datanode and freon by default,, proposal here is to move the ratis and grpc ports for scm ha to 989x series by default.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-17 09:02:11,26
13378748,SCM subsequent init failed when previous scm init failed,"The problem is SCM init because we use a new clusterID when the version writing failed.


{code:java}
Could not initialize SCM version file
java.io.IOException: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING
	at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
	at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
	at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:71)
	at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:354)
	at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:371)
	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.initialize(SCMRatisServerImpl.java:115)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:925)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:173)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at picocli.CommandLine.executeUserObject(CommandLine.java:1952)
	at picocli.CommandLine.access$1100(CommandLine.java:145)
	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2326)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2291)
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:2465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:57)
Caused by: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING
{code}
",pull-request-available,['SCM HA'],HDDS,Sub-task,Critical,2021-05-17 08:45:38,13
13378478,replace HADOOP_ with OZONE_ prefix in the ozone-runner docker image,Please see: https://github.com/apache/ozone-docker-runner/pull/7,pull-request-available,[],HDDS,Improvement,Major,2021-05-14 12:19:42,6
13378472,Update copyright year of NOTICE in all Ozone repositories,"As [~jmclean]  [reported on the mailing list |https://lists.apache.org/thread.html/rb2bbf79cf136b1c129ff7205e319223c2a780af49ed270031d676ce2%40%3Cdev.ozone.apache.org%3E]:

bq. While it may not matter in our lifetime, copyright can expire so it would be best to put the published year in the NOTICE file rather than ""Copyright 2006 and onwards"".

We should update all of our notice files to 2021:

{code}
Copyright 2021 The Apache Software Foundation
{code}",pull-request-available,[],HDDS,Bug,Major,2021-05-14 11:41:35,6
13378365,Do not fail SCM HA pre-finalize validation if SCM HA was already being used,"SCM HA has been merged in to the master branch before the upgrade framework. With the current implementation of SCM HA pre-finalize validation (which prevents the new feature from being used until the cluster is finalized) users who follow the Apache releases and those who incrementally pull from master will have different experiences when they get the upgrade framework:

For users following official Ozone releases, the upgrade framework and SCM HA will land in the same release. SCM HA  should not be allowed in pre-finalize so that these users can downgrade using the upgrade framework. This is currently how the validation action works.

For users who pull from master and are already using SCM HA, the current pre-finalize validation will fail to start the SCM until they turn off SCM HA. The cluster must be finalized, and then SCM HA can be re-enabled. This may be surprising and inconvenient.

To support both use cases, the SCM HA pre-finalize validation action should not fail if SCM HA was already being used before the upgrade.",pull-request-available,[],HDDS,Sub-task,Major,2021-05-13 20:16:19,22
13378098,EC: ReplicaIndex in Pipeline should be serialized and deserialized in the protobuf message,"ReplicaIndex and ReplicationConfig were added to the Pipeline class, but we missed added ReplicaIndex to the serialized message. This Jira will add it in.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-05-12 13:19:08,11
13378055,"when datanode delete the data,first delete metadata,then delete chunk files","when use datanode scanner scan the container，the checksum bases on Rocksdb metadata to find the chunk file on disk,but when datanode delete data，datanode should delete metadata first，the delete chunk file，in order to avoid when chunk file was deleted，but the metadata can not be modified，the datanode scanner scan the datanode will mark this container unhealthy.",pull-request-available,[],HDDS,Improvement,Major,2021-05-12 09:22:08,23
13377989,Limit number of bad volumes by dfs.datanode.failed.volumes.tolerated,"For now, datanode has config 'dfs.datanode.failed.volumes.tolerated', but it does not take effect as expected, we should use it to limit the number of bad volumes.",pull-request-available,[],HDDS,Sub-task,Minor,2021-05-12 02:52:46,21
13377890,ozone freon randomkeys failed after leader SCM node is down,"under .../compose/ozone-ha, create a HA cluster:

{code}
docker-compose up -d --scale datanode=3
{code}

Initial SCM roles as following:

{code}
bash-4.2$ ozone admin scm roles
[scm1:9865:FOLLOWER, scm2:9865:FOLLOWER, scm3:9865:LEADER]
{code}

Running freon random key generator as following:

{code}
ozone freon randomkeys --numOfVolumes=10 --numOfBuckets 50 --numOfKeys 50  --replicationType=RATIS --factor=THREE
{code}

While freon randomkeys was running, put all SCM nodes under blockade and stop leader SCM node:

blockade status:

{code}
NODE            CONTAINER ID    STATUS  IP              NETWORK    PARTITION  

                18f9c1e2d52f    UP      172.31.0.9      NORMAL                

ozone-ha_scm1_1                                                                

                25c74f0a9271    UP      172.31.0.6      NORMAL                

ozone-ha_scm2_1                                                                

                8808d10ccb3a    DOWN                    UNKNOWN               

ozone-ha_scm3_1
{code}
 

freon randomkeys failed with following error message:

Some test result msg as following:

{code}
6:00:30,131 [pool-2-thread-3] ERROR freon.RandomKeyGenerator: Exception while adding key: key-21-80493 in bucket: bucket-44-63818 of volume: vol-1-95998.
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No Route to Host from  om1/172.31.0.11 to scm3:9863 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  [http://wiki.apache.org/hadoop/NoRouteToHost]
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:604)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.openKey(OzoneManagerProtocolClientSideTranslatorPB.java:595)
at org.apache.hadoop.ozone.client.rpc.RpcClient.createKey(RpcClient.java:756)
at org.apache.hadoop.ozone.client.OzoneBucket.createKey(OzoneBucket.java:502)
at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:703)
at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:86)
at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:621)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at java.base/java.lang.Thread.run(Thread.java:834)
 44.10% |?????????????????????????????????????????????                                                        |  11024/25000 Time: 0:09:002021-05-11 06:00:37,231 [pool-2-thread-7] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-EA7B54107DBD->4c51bca8-cc0a-4c20-84dd-b5a7cb18c4ac
2021-05-11 06:00:37,231 [pool-2-thread-7] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
 100.00% |?????????????????????????????????????????????????????????????????????????????????????????????????????|  25000/25000 Time: 0:15:39
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No Route to Host from  om1/172.31.0.11 to scm:9863 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  [http://wiki.apache.org/hadoop/NoRouteToHost]
***************************************************
Status: Failed
Git Base Revision: 7a3bc90b05f257c8ace2f76d74264906f0f7a932
Number of Volumes created: 10
Number of Buckets created: 500
Number of Keys added: 24991
Ratis replication factor: THREE
Ratis replication type: RATIS
Average Time spent in volume creation: 00:00:00,114
Average Time spent in bucket creation: 00:00:01,263
Average Time spent in key creation: 00:02:48,698
Average Time spent in key write: 00:00:04,216
Total bytes written: 255907840
Total Execution time: 00:15:39,968
***************************************************
{code}

In this case, I'd expect the freon test would still finish successfully.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-11 15:25:50,13
13377849,Bump logical release name of Ozone 1.2,Please see: https://github.com/apache/ozone/pull/2238,pull-request-available,[],HDDS,Improvement,Trivial,2021-05-11 11:50:33,6
13377836,Create GRPC service definition and for existing Om protocol,"We need a hadoop-ozone/interface-server project (similar to hadoop-hdds/interface-server) to define the server2server communication on ozone level.

This project should depend on the existing interface-client which contains the OmRequest/OmResponse definition, and we need to define a sync GRPC service with these calls.

We should generate GRPC stub classes for later use.",pull-request-available,[],HDDS,Sub-task,Major,2021-05-11 11:24:50,39
13377741,Datanode hasEnoughSpace check should apply on volume instead of global DN,"During Container replication, Replication manager is responsible for choose the DN to place the new replica.  When a DN is choose, it's total remaning space is compared with the container used bytes.  This will cause write problem when the DN total remaning space is greater than container used bytes, while none of the volumes of DN has enough space to hold the new container replica. ",pull-request-available,[],HDDS,Sub-task,Major,2021-05-11 05:50:08,21
13377721,bump rocksdb to 6.20.3,"now , the rocksdb version used in ozone is 6.8.1, which is build on Apr 16 , 2020.

this Jira bump rockdsb to 6.20.3, which is build on May 5, 2021, and includes many new bug-fix",pull-request-available,[],HDDS,Improvement,Major,2021-05-11 02:56:19,2
13377689,Intermittent failure in TestRatisPipelineProvider#testCreatePipelineWithFactorThree,"{{TestRatisPipelineProvider#testCreatePipelineWithFactorThree}} fails intermittently due to same set of nodes being selected for two 3-node pipelines.

2021/02/01/5639/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/02/11/5875/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/02/24/6111/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/04/06/7045/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/04/07/7095/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
2021/04/09/7175/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt",pull-request-available,['test'],HDDS,Bug,Major,2021-05-10 21:08:44,1
13377675,Support revoking S3 secret,"Currently the CLI only support ""ozone s3 getsecret"", but has no user or admin command to revoke the secret, or to rotate the secret. Add a new command or two to do that.

I'm thinking ""ozone s3 revokesecret"" and ""ozone s3 rotatesecret"" for the user himself. For admin we can have something like ""ozone admin om s3secret revoke/rotate -u userA"", maybe in another jira.",pull-request-available,['S3'],HDDS,Sub-task,Major,2021-05-10 18:15:51,12
13377621,Make admin check work for SCM HA cluster,"By default, the user started principal is added to scmAdminUsernames.


{code:java}
    String scmUsername = UserGroupInformation.getCurrentUser().getUserName();
    if (!scmAdminUsernames.contains(scmUsername)) {
      scmAdminUsernames.add(scmUsername);
    }
{code}


In HA cluster, when kinit with scm2 principal when scm1 is leader, we get access denied as we check getUserName() and also when adding to adminlist we use getUserName.

In OM we don't have this kind of issue, as getShortUserName() is used.


{code:java}
  String omSPN = UserGroupInformation.getCurrentUser().getShortUserName();
    if (!ozAdmins.contains(omSPN)) {
      ozAdmins.add(omSPN);
    }
{code}

And during admin check it compares with both userName and shortUserName.


{code:java}
if (ozAdmins.contains(callerUgi.getShortUserName()) ||
        ozAdmins.contains(callerUgi.getUserName()) ||
{code}

",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-10 12:18:27,13
13377620,Missing type-level Javadoc comments,"Javadoc comments used to be required on all public types (classes, interfaces, etc.).  In Checkstyle 8.20

bq. functionality for validating missing javadocs was split into a new check MissingJavadocType.

So missing javadoc on new types introduced since 2020/Oct/20 (when HDDS-4306 upgraded Checkstyle from 8.19 to 8.29) were not caught, and now we have ~50 violations.

{noformat}
hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/TableCache.java
 114: Missing a Javadoc comment.
hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/NodeDetails.java
 25: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMCertStore.java
 304: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/balancer/ContainerBalancer.java
 34: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMDBCheckpointProvider.java
 34: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/MockSCMHADBTransactionBuffer.java
 28: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/HASecurityUtils.java
 70: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/LongCodec.java
 24: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/EnumCodec.java
 28: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/BooleanCodec.java
 24: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/ListCodec.java
 28: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/StringCodec.java
 24: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/Codec.java
 23: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/CodecFactory.java
 32: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/GeneratedMessageCodec.java
 27: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMContext.java
 180: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMHANodeDetails.java
 63: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeUsageInfo.java
 27: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogStateManagerImpl.java
 44: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogStateManager.java
 29: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMHAConfiguration.java
 45: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMServiceManager.java
 27: Missing a Javadoc comment.
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSequenceIDGenerator.java
 29: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBufferFactory.java
 42: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/utils/BufferUtils.java
 27: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBufferImpl.java
 23: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/ha/ConfUtils.java
 28: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/utils/ClientCommandsUtils.java
 23: Missing a Javadoc comment.
hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ha/RetriableWithNoFailoverException.java
 22: Missing a Javadoc comment.
hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/common/TestChecksumImplsComputeSameValues.java
 33: Missing a Javadoc comment.
hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMNodeInfo.java
 45: Missing a Javadoc comment.
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/statemachine/background/BlockDeletingService.java
 112: Missing a Javadoc comment.
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationSupervisor.java
 120: Missing a Javadoc comment.
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationServer.java
 126: Missing a Javadoc comment.
hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java
 130: Missing a Javadoc comment.
hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/replication/TestSimpleContainerDownloader.java
 41: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/containergenerator/BaseGenerator.java
 29: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkCRCBatch.java
 54: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkCRCStreaming.java
 71: Missing a Javadoc comment.
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/debug/ExportContainer.java
 61: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/signature/SignatureInfo.java
 110: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3Owner.java
 27: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3BucketAcl.java
 68: Missing a Javadoc comment.
 99: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3Acl.java
 38: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/signature/TestAuthorizationV4QueryParser.java
 30: Missing a Javadoc comment.
hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/TestEmptyContentTypeFilter.java
 27: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneHAClusterImpl.java
 659: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHAKeyDeletion.java
 29: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/read/TestInputStreamBase.java
 60: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMSnapshot.java
 40: Missing a Javadoc comment.
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemMissingParent.java
 42: Missing a Javadoc comment.
hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/ContainerHistory.java
 23: Missing a Javadoc comment.
hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/MultipartCryptoKeyInputStream.java
 34: Missing a Javadoc comment.
{noformat}",pull-request-available,[],HDDS,Bug,Major,2021-05-10 12:13:16,1
13377608,Allow suppressing deprecation warning for HADOOP_ variables,HDDS-4525 deprecated {{HADOOP_\*}} variables in Ozone in favor of corresponding {{OZONE_\*}} variables.  We should allow suppressing the warning for such variables to make upgrade easier.,pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2021-05-10 10:54:06,1
13377602,Use scm#checkLeader before processing client requests ,"Right now to check leader we use ScmContext#isLeader, this gets updated by notifyLeaderChanged.

But SCM server should start accepting requests when it is leader and isLeaderReady. 

We need isLeaderReady also because Statemachine should apply all the log committed transactions to start accepting requests.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-10 10:23:13,13
13377597,[FSO] S3MultiPart: Use existing ozone key format for MPU Info in DB,"This task is to simplify the prefix layout implementation for the S3MPU keys. As there is no ObjectStore API to perform *rename* or *delete* operation on MPU keys, the DB table key format can follow the existing structure for the MPU table entries.

This will help to reuse the existing {{ozoneBucket#listParts()}} and {{ozoneBucket#listMultipartUploads()}} and required very minimal code changes.",pull-request-available,[],HDDS,Sub-task,Major,2021-05-10 10:05:41,40
13377594,Fix scm roles command if one of the host is unresolvable,"{code:java}
while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/172.19.0.7:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
com.google.protobuf.ServiceException: java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: ""scm1"":9860; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost, while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1:9860 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7fc05067-c144-4c14-880b-e47f1e40599b is not the leader. Suggested leader is Server:scm3:9860.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:191)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:144)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:43838)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/172.19.0.2:9860 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.net.UnknownHostException): scm1
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
	at java.base/java.net.InetAddress.getByName(InetAddress.java:1252)
	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.getRatisRoles(SCMRatisServerImpl.java:233)
	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getScmInfo(SCMClientProtocolServer.java:579)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getScmInfo(StorageContainerLocationProtocolServerSideTranslatorPB.java:506)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:149)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:43838)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/172.19.0.7:9860 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
{code}

If one of the host is unresolvable roles command keep on failing.
",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-10 09:51:53,13
13377580,Create docker image for Apache Ozone 1.1.0 release,Please see: https://github.com/apache/ozone-docker/pull/19,pull-request-available,[],HDDS,Improvement,Major,2021-05-10 08:35:29,6
13377573,Use https for JBoss repo,[Maven 3.8.1|https://maven.apache.org/docs/3.8.1/release-notes.html] blocks http repos by default.  Need to update JBoss repo definition in {{pom.xml}} to https.,pull-request-available,['build'],HDDS,Bug,Major,2021-05-10 07:07:54,1
13377543,Pass option variables to OZONE_OPTS before adding default GC opts,"I thought I carefully migrated variables in hadoop-env.sh to ozone-env.sh on upgrading Ozone 1.0 to 1.1, but it ended up empty GC setting and failed to restart. OZONE_*_OPTS in ozone-env.sh are ignored in startup script",pull-request-available,['dist'],HDDS,Bug,Major,2021-05-10 02:44:12,10
13377139,Permission Deny when using auth:TOKEN,"Hi I’m got stuck on the permission issue where I gonna write the data, a text file to a ozone path {{/vol1/bucket1/mykey}} * with {{auth:KERBEROS}} It be able to complete the task
{code:java}
2021-04-29 11:49:01,145 Socket Reader #1 for port 9862 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for pakapoj_tul@DEV.TAP (auth:KERBEROS) from ip.ip.ip.ip:40294 *  with auth:TOKEN It got stuck on this error despite the given permission to /vol1  /bucket1 see below
2021-04-29 11:49:08,327 Socket Reader #1 for port 9862 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for pakapoj_tul@DEV.TAP (auth:TOKEN) from ip.ip.ip.ip:40412
 2021-04-29 11:49:12,228 Socket Reader #1 for port 9862 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for pakapoj_tul@DEV.TAP (auth:TOKEN) from ip.ip.ip.ip:35266
 2021-04-29 11:49:14,671 [OM StateMachine ApplyTransaction Thread - 0] WARN org.apache.hadoop.ozone.om.OzoneManager: User pakapoj_tul@DEV.TAP doesn't have WRITE permission to access key /vol1/bucket1/mykey/_temporary/0/_temporary/attempt_202104290449105826106778232640855_0000_m_000000_0/part-00000-9f9c4fcc-5e96-43ee-b53e-913a06729109-c000.txt/106146807974133768
 2021-04-29 11:49:14,672 [OM StateMachine ApplyTransaction Thread - 0] ERROR org.apache.hadoop.ozone.om.request.key.OMKeyCommitRequest: Key commit failed. Volume:vol1, Bucket:bucket1, Key:mykey/_temporary/0/_temporary/attempt_202104290449105826106778232640855_0000_m_000000_0/part-00000-9f9c4fcc-5e96-43ee-b53e-913a06729109-c000.txt.
 PERMISSION_DENIED org.apache.hadoop.ozone.om.exceptions.OMException: User pakapoj_tul@DEV.TAP doesn't have WRITE permission to access key vol1 bucket1 mykey/_temporary/0/_temporary/attempt_202104290449105826106778232640855_0000_m_000000_0/part-00000-9f9c4fcc-5e96-43ee-b53e-913a06729109-c000.txt/106146807974133768
 at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1803)
 at org.apache.hadoop.ozone.om.request.OMClientRequest.checkAcls(OMClientRequest.java:207)
 at org.apache.hadoop.ozone.om.request.OMClientRequest.checkAcls(OMClientRequest.java:185)
 at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.checkKeyAcls(OMKeyRequest.java:437)
 at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.checkKeyAclsInOpenKeyTable(OMKeyRequest.java:485)
 at org.apache.hadoop.ozone.om.request.key.OMKeyCommitRequest.validateAndUpdateCache(OMKeyCommitRequest.java:139)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:415)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:240)
 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
{code}

 *Given Permission*
{code:java}
$ ozone sh vol getacl /vol1/
[ {
  ""type"" : ""USER"",
  ""name"" : ""pakapoj_tul"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""WRITE"", ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""pakapoj_tul@DEV.TAP"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""WRITE"", ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""ozone-admin@DEV.TAP"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""ozone-admin"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""ozone-users"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
} ]
$ ozone sh bucket getacl /vol1/bucket1/
[ {
  ""type"" : ""USER"",
  ""name"" : ""ozone-admin@DEV.TAP"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""ozone-admin"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""ozone-users"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""pakapoj_tul@DEV.TAP"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""WRITE"", ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""pakapoj_tul"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""WRITE"", ""ALL"" ]
} ]
$ ozone sh key getacl /vol1/bucket1/mykey/
[ {
  ""type"" : ""USER"",
  ""name"" : ""ozone-admin@DEV.TAP"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""ozone-admin"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""ozone-users"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""pakapoj_tul@DEV.TAP"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""WRITE"", ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""pakapoj_tul"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""WRITE"", ""ALL"" ]
} ]{code}
 
 The spark code was deployed in Kubernetes in spark cluster mode. Then, the error would happed on spark executors side when the do {{commitKey}} with {{auth:TOKEN}} , BTW the spark driver was using {{auth:KERBEROS}} .
  
 so I reproduce using ozone java client writing to ozone with {{OzoneClient}} using # Token by {{export HADOOP_TOKEN_FILE_LOCATION=credential/ozone.token}} before running the program
 # Keytab by running {{/usr/bin/kinit -kt credential/pakapoj_tul.keytab pakapoj_tul@DEV.TAP}} before running the program

the code, output for #1 and #2 (DEBUG) is in attach
  ",pull-request-available,['OM'],HDDS,Bug,Critical,2021-05-07 04:48:34,33
13377090,Intermittent failure in TestOzoneRpcClient due to volume name conflict,"{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/05/06/7723/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient.txt}
Tests run: 80, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 175.065 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient
testListVolume  Time elapsed: 0.086 s  <<< FAILURE!
java.lang.AssertionError: expected:<20> but was:<21>
  ...
  at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testListVolume(TestOzoneRpcClientAbstract.java:1867)
{noformat}

{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/05/06/7723/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient-output.txt}
2021-05-06 13:33:08,991 [Time-limited test] INFO  rpc.RpcClient (RpcClient.java:createVolume(320)) - Creating Volume: vol-71454, with jenkins1001 as owner and space quota set to -1 bytes, counts quota set to -1
...
2021-05-06 13:33:09,650 [Time-limited test] INFO  rpc.RpcClient (RpcClient.java:createVolume(320)) - Creating Volume: vol-714-a-0-78727, with jenkins1001 as owner and space quota set to -1 bytes, counts quota set to -1
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2021-05-06 20:05:33,1
13376983,Increase default pvc storage size,"Containers are 5Gb by default, however pvc storage is 2Gb now.

 

I should think we fill pvc with 5GB by default.",pull-request-available,['kubernetes'],HDDS,Task,Minor,2021-05-06 07:57:11,41
13376974,"datanode scanner detect the all container replica unhealthy,replication manager can not delete all","if we have three replica,when three replicas unhealthy,the replication manager will sendDeleteCommand to one replica,and check the container under replicated,if no container no healthy replica,will print the warn log and return,the rest of unhealthy replicas will not be deleted",pull-request-available,[],HDDS,Bug,Major,2021-05-06 07:28:02,23
13376628,PSA: hugo 0.83.0 / 0.83.1 broke hadoop-hdds/docs/dev-support/bin/generate-site.sh,"After upgrading my hugo to 0.83.1, a maven build would fail at hadoop-hdds-docs for me.

{code}
[INFO] --- exec-maven-plugin:1.3.1:exec (default) @ hadoop-hdds-docs ---
Error: Error building site: "".../hadoop-hdds/docs/content/interface/ReconApi.md:1:1"": [BUG] goldmark: runtime error: slice bounds out of range [3503:3501]: create an issue on GitHub attaching the file in: /var/folders/yp/sxzthlnd45vdwcwvdldf_12r0000gp/T/hugo_bugs/goldmark_267fdfa526d051bdccb7cf9a98e0772d.txt
Start building sites …
goroutine 60 [running]:
runtime/debug.Stack(0x5f70c80, 0x6b7c280, 0xc00117c310)
	runtime/debug/stack.go:24 +0x9f
github.com/gohugoio/hugo/markup/goldmark.(*goldmarkConverter).Convert.func1(0xc000a2e6c0, 0xc000d25500, 0x2b24, 0x3038, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
{code}

ReconApi.md itself hasn't changed since Nov 2020, it's also unlikely that other markdowns have caused this. This issue can be simply repro'ed with:

{code}
./hadoop-hdds/docs/dev-support/bin/generate-site.sh
{code}

on hugo 0.83.0 or 0.83.1.

After reverting hugo to 0.82.1 (homebrew build), the problem goes away for me.

{code}
$ hugo version
hugo v0.82.1+extended darwin/amd64 BuildDate=unknown
{code}

PSA: Do not upgrade hugo to 0.83.0 or 0.83.1 if you want to compile hadoop-hdds-docs, it is still broken right now.
",pull-request-available,['build'],HDDS,Task,Major,2021-05-04 19:58:31,6
13376578,Replace GRPC based closed-container replication with Netty based streaming,"Today the closed containers are copied between datanodes as one big tar(.gz) file. Each datanode runs a GrpcReplicationService (with a grpc server) and when the SCM asks the destination-datanode to replicate data, it connects to the source datanode and retrieves the data.

This protocol is based on GRPC and very simple. After the first request (download(containerid)) the full container is streamed as a tar file in smaller chunks.

However, this protocol doesn't have any back-pressure  / traffic control handling. After the first request the FULL 5g container is sent back. 

This approach can fill up the netty buffers very easy:

{code}
	Exception in thread ""grpc-default-executor-0"" org.apache.ratis.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 2097152 byte(s) of direct memory (used: 3651141911, max: 3652190208)
	at org.apache.ratis.thirdparty.io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:802)
	at org.apache.ratis.thirdparty.io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:731)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:632)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:607)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:202)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.tcacheAllocateNormal(PoolArena.java:186)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:136)
	at org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:126)
	at org.apache.ratis.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:395)
	at org.apache.ratis.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
	at org.apache.ratis.thirdparty.io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:123)
	at org.apache.ratis.thirdparty.io.grpc.netty.NettyWritableBufferAllocator.allocate(NettyWritableBufferAllocator.java:51)
	at org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writeKnownLengthUncompressed(MessageFramer.java:227)
	at org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writeUncompressed(MessageFramer.java:168)
	at org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writePayload(MessageFramer.java:141)
	at org.apache.ratis.thirdparty.io.grpc.internal.AbstractStream.writeMessage(AbstractStream.java:65)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl.sendMessageInternal(ServerCallImpl.java:167)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl.sendMessage(ServerCallImpl.java:149)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onNext(ServerCalls.java:365)
	at org.apache.hadoop.ozone.container.replication.GrpcOutputStream.flushBuffer(GrpcOutputStream.java:124)
	at org.apache.hadoop.ozone.container.replication.GrpcOutputStream.write(GrpcOutputStream.java:90)
	at org.apache.hadoop.ozone.freon.ContentGenerator.write(ContentGenerator.java:76)
	at org.apache.hadoop.ozone.freon.ClosedContainerStreamGenerator.copyData(ClosedContainerStreamGenerator.java:19)
	at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:56)
	at org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolServiceGrpc.java:219)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2021-05-04 16:37:47,996 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:37:48,998 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:37:49,998 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)

{code}

This can be reproduced locally with a simple freon test. (See the code here: https://github.com/elek/ozone/tree/grpc-push)

The new freon test starts a GrpcServer and client. On server side the source is replaced with a simple `ContainerReplicationSource` which generates random 5g datastream (instead of reading container data from disk).

On the client side the replicator just downloads the container to the tmp location, but it's not moved to the final location.

This test works well for one container, but the test clearly shows that the full container data is streamed at the very beginning:

(Duplicated lines are removed)

{code}
2021-05-04 16:21:04,281 INFO  replication.DownloadAndDiscardReplicator (DownloadAndDiscardReplicator.java:replicate(62)) - Starting replication of container 0 from [7369fd21-7ee9-4780-a54b-5831e951ca9c{ip: 127.0.0.1, host: localhost, ports: [REPLICATION=41379], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}]
2021-05-04 16:21:04,434 INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (0) to other datanode
2021-05-04 16:21:05,269 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:06,270 INFO  freon.ProgressBar 
...
2021-05-04 16:21:10,275 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:11,275 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:11,791 INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 5368709120 bytes for container 0
...
2021-05-04 16:21:33,434 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:33,737 INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(190)) - Container 0 is downloaded to /tmp/container-copy/container-0.tar.gz
2021-05-04 16:21:34,434 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)
2021-05-04 16:21:34,690 INFO  replication.DownloadAndDiscardReplicator (DownloadAndDiscardReplicator.java:replicate(71)) - Container is downloaded but deleted, as you wished /tmp/container-copy/container-0.tar.gz
{code}

As you can see the full 5G data is sent out at 16:21:11 (after 6 seconds), but the data copy is finished only at  16:21:33 (22 more seconds).

Between the two time the majority of the container is kept in the GRPC/netty buffers.

As an experiment we can make the grpc client slow (GrpcReplicationClient):

{code}
    @Override
    public void onNext(CopyContainerResponseProto chunk) {
      try {
        try {
          Thread.sleep(1_000);
        } catch (InterruptedException e) {
          e.printStackTrace();
        }
        chunk.getData().writeTo(stream);
      } catch (IOException e) {
        response.completeExceptionally(e);
      }
    }
{code}

With this method we download the beginning of the container very slowly, and this is enough to get the exception above.

{code}
Exception in thread ""grpc-default-executor-0"" org.apache.ratis.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 2097152 byte(s) of direct memory (used: 3651141911, max: 3652190208)
{code}

Temporary it can be fixed with increasing the netty memory: -Dorg.apache.ratis.thirdparty.io.netty.maxDirectMemory=16000000000  but it's not a good long-term solution.

So we need to refactor the protocol to do a request/response chunk by chunk.

But we also have another problem. GRPC is not optimal for fast streaming.

The previous log showed that we replicated the container (5G) under 30 seconds (without reading the original container and without doing tar file compression).

This is 5 / 30 = 170 Mb / sec. (I wrote to a tmpfs on the destination side, but even my nvme is significant faster).

Based on [this|https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html] article the best (!) results (with C client!) were 1 Gb/s with GRPC. (with the explained black magic it is doubled).

Ansh Khanna earlier did some low-level benchmarking (for ratis streaming) which showed 5x difference between pure netty and GRPC:

Flatbuffers over GRPC
%CPU in Buffer Copying/Allocations: >10%
Time(in seconds): 16.44
 
Protobuffers over GRPC:
%CPU in Buffer Copying/Allocations: ~10%
Time(in seconds): 11.66

Netty Based Streaming
%CPU in Buffer Copying/Allocations: 0%
Time(in seconds): 2.7

Pure netty also supports zero copy async stream.

Summary:
 1. The current implementation should be refactored to avoid pushing the data
 2. Netty seems to be better for long-term solution

--> As a results it seems to be easier to create a POC with netty support and check how does it work.

Earlier I made an attempt which can be found here: https://github.com/elek/ozone/tree/close-container-replication-refactor

It's a generic interface which may also be used in https://issues.apache.org/jira/browse/HDDS-5142 But at least it can be used to compare the netty vs GRPC performance in this situation.",pull-request-available,[],HDDS,Bug,Major,2021-05-04 14:40:04,6
13376550,Avoid Maven connection errors in CI,"Github Actions builds intermittently fail due to Maven transfer errors.

Example:

{code:title=https://github.com/apache/ozone/runs/2454639477#step:6:2576}
Error:  Plugin org.apache.maven.plugins:maven-shade-plugin:3.2.4 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-shade-plugin:jar:3.2.4: Could not transfer artifact org.apache.maven.plugins:maven-shade-plugin:pom:3.2.4 from/to central (https://repo.maven.apache.org/maven2): Transfer failed for https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-shade-plugin/3.2.4/maven-shade-plugin-3.2.4.pom: Connection timed out (Read failed)
{code}",pull-request-available,['build'],HDDS,Task,Major,2021-05-04 12:17:12,1
13376548,"If primordial SCM id is set, a non-HA cluster can not be initialized.","Due to the check [here|https://github.com/apache/ozone/blob/master/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java#L907].

SCMHAUtils.isPrimordialSCM evaluates to false if we are not in an SCM-HA enabled environment, but this check executes when the primordialSCM id string is not null in the configuration.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-05-04 11:46:12,18
13376514,Update commons-io to 2.8.0,"Similar to HADOOP-17683 we should update despite we don't use the vulnerable API.

https://nvd.nist.gov/vuln/detail/CVE-2021-29425

In Apache Commons IO before 2.7, When invoking the method FileNameUtils.normalize with an improper input string, like ""//../foo"", or ""\\..\foo"", the result would be the same value, thus possibly providing access to files in the parent directory, but not further above (thus ""limited"" path traversal), if the calling code would use the result to construct a path value.",pull-request-available,[],HDDS,Task,Major,2021-05-04 08:55:58,1
13376505,Use separate DB profile for Datanodes,Currently datanodes shares a common DB profile with other components like SCM and OM etc. The jira aims to create a different profile for datanodes which also makes sure that column family options are shared by containers. ,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-05-04 08:00:15,35
13376486,For AccessControlException do not perform failover,"For AccessControlException donot perform failOver, as there is no real need.
{code:java}
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1454)
        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.recommissionNodes(SCMClientProtocolServer.java:459)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.recommissionNodes(StorageContainerLocationProtocolServerSideTranslatorPB.java:646)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:317)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:155)
        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46954)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.8:9860 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
{code}
",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-05-04 06:43:04,13
13376413,Acceptance test may exit with 0 in case of error,"If any acceptance test fails, {{test-all.sh}} (and in turn {{acceptance.sh}}) should exit with error code (1).  But if a successful test is run after a failing one, it will now wrongly exit with success (0).

{code}
$ export OZONE_TEST_SELECTOR='failing1\|ozone-csi'
$ ./hadoop-ozone/dev-support/checks/acceptance.sh
...
$ echo $?
0
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-05-03 17:02:21,1
13376381,Generate metadata directories config for production environment,"The config of ""ozone.metadata.dirs"" is the fallback config if the metadata directory of each service is not set.

It took me a while to find out all the configs for all services, I think if _genconf_ can generate all the configs related to the metadata directory it will save users a lot of time.",pull-request-available,[],HDDS,Improvement,Major,2021-05-03 13:48:57,7
13376375,Fix some minor error on Ozone site,"1.The source link of the ozone website is [https://github.com/apache/ozone] instead of [https://github.com/apache/hadoop-ozone].
2.The description under ozone’s documentation website should be ""Documentation is also part *of* the binary release packages and available online from the running Storage Container Manager or Ozone Manager webui.""
3.Fix incorrect link in
{code:html}
<link rel=""canonical"" href=""https://hadoop.apache.org/ozone"">
{code}
",pull-request-available,['website'],HDDS,Improvement,Trivial,2021-05-03 12:38:35,17
13376363,Update project information of Contribution guideline,IMHO the link of [wiki pages|https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Contributor+Guide] in [Contribution guideline|https://github.com/apache/ozone/blob/master/CONTRIBUTING.md#what-can-i-contribute] should be update to [this one|https://cwiki.apache.org/confluence/display/OZONE/Contributing+to+Ozone] after Ozone being TLP.,pull-request-available,[],HDDS,Improvement,Major,2021-05-03 11:36:15,17
13376361,Update link of weekly calls,IMHO the link of [Weekly calls|https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Community+Calls] in [README|https://github.com/apache/ozone#contact] should be updated to [this one|https://cwiki.apache.org/confluence/display/OZONE/Ozone+Community+Calls].,pull-request-available,[],HDDS,Improvement,Major,2021-05-03 11:17:32,42
13376335,Increase default block cache capacity for Datanodes,Currently Ozone datanodes use a default block cache capacity of 64MB. We can increase the default block cache capacity to a higher value like 512MB..1GB.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-05-03 08:17:16,35
13376148,ozone fs trash service does not handle bucket -rm -R,"ozone fs shell rm -R at bucket level throws exception when trash service active eg.

for ofs:
{code:java}
$ bin/ozone fs -rm -R /vol1/bucket1{code}
 
{code:java}
-rm: Fatal internal error
java.lang.RuntimeException: Volume or bucket doesn't have trash root.
 at org.apache.hadoop.ozone.OFSPath.getTrashRoot(OFSPath.java:298)
 at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getTrashRoot(BasicRootedOzoneFileSystem.java:690)
 at org.apache.hadoop.fs.TrashPolicyDefault.moveToTrash(TrashPolicyDefault.java:134)
 at org.apache.hadoop.fs.Trash.moveToTrash(Trash.java:110)
 at org.apache.hadoop.fs.Trash.moveToAppropriateTrash(Trash.java:96)
 at org.apache.hadoop.fs.shell.Delete$Rm.moveToTrash(Delete.java:153)
 at org.apache.hadoop.fs.shell.Delete$Rm.processPath(Delete.java:118)
 at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)
 
{code}
and for o3fs:
{code:java}
$ bin/ozone fs -rm -R o3fs://bucket1.vol1{code}
{code:java}
-rm: Fatal internal error
java.lang.NullPointerException
 at org.apache.hadoop.fs.Path.mergePaths(Path.java:277)
 at org.apache.hadoop.fs.TrashPolicyDefault.makeTrashRelativePath(TrashPolicyDefault.java:113)
 at org.apache.hadoop.fs.TrashPolicyDefault.moveToTrash(TrashPolicyDefault.java:146)
 at org.apache.hadoop.fs.Trash.moveToTrash(Trash.java:110)
 at org.apache.hadoop.fs.Trash.moveToAppropriateTrash(Trash.java:96)
 at org.apache.hadoop.fs.shell.Delete$Rm.moveToTrash(Delete.java:153)
 at org.apache.hadoop.fs.shell.Delete$Rm.processPath(Delete.java:118)
 at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367){code}
 

Will handle cases as if -skipTrash option is applied.

 ",pull-request-available,[],HDDS,Bug,Minor,2021-04-30 21:07:43,39
13376016,EC: Allow EC blocks to be requests from OM,We need to change the allocateBlock calls from OM to SCM so the EC Replication Config is passed through the stack.,pull-request-available,"['OM', 'SCM']",HDDS,Sub-task,Major,2021-04-30 10:01:05,11
13375995,Divide snapshot related work into  notifyInstallSnapshotFromLeader and reinitialize for SCMStateMachine.,"As mentioned in https://issues.apache.org/jira/browse/RATIS-1370

During notifyInstallSnapshotFromLeader, StateMachineUpdater may call applyTransactions when StateMachine is in PAUSED state.

Just divide snapshot related work into notifyInstallSnapshotFromLeader and reinitialize for SCMStateMachine.

During notifyInstallSnapshotFromLeader, SCM just downloads snapshot but not modify StateMachine, since StateMachineUpdater is in RUNNING state, may call applyTransactions during this period.

During reinitialize, SCM can safely reload the StateMachine, such as rocksdb and in-memory state. During this period, StateMachineUpdater is in RELOAD state, thus there will be no contention between SCM and StateMachineUpdater.

 ",pull-request-available,[],HDDS,Sub-task,Major,2021-04-30 08:12:35,36
13375984,Make the report max limit configurable,"Now, when get all available reports, the max limit always Integer.MAX_VALUE, maybe we can make it configurable.
{code:java}
public List<GeneratedMessage> getAllAvailableReports(
    InetSocketAddress endpoint) {
  return getReports(endpoint, Integer.MAX_VALUE);
}{code}",pull-request-available,[],HDDS,Improvement,Major,2021-04-30 07:26:55,29
13375913,Race condition in NodestateManager#addNode allows datanodes with lower MLV to be used in pipelines,"HDDS-4946 Introduced a race condition in NodeStateManager#addNode that allows SCM's background pipeline creator or another thread to read a node with a lower MLV than SCM as healthy before it is moved to the healthy readonly state.

{code:java}

  public void addNode(DatanodeDetails datanodeDetails,
      LayoutVersionProto layoutInfo) throws NodeAlreadyExistsException {
    NodeStatus newNodeStatus = newNodeStatus(datanodeDetails);
    nodeStateMap.addNode(datanodeDetails, newNodeStatus, layoutInfo);
    UUID dnID = datanodeDetails.getUuid();
    try {
      updateLastKnownLayoutVersion(datanodeDetails, layoutInfo);
      DatanodeInfo dnInfo = nodeStateMap.getNodeInfo(dnID);
      NodeStatus status = nodeStateMap.getNodeStatus(dnID);

      // State machine starts nodes as HEALTHY. If there is a layout
      // mismatch, this node should be moved to HEALTHY_READONLY.
      updateNodeLayoutVersionState(dnInfo, layoutMisMatchCondition, status,
          NodeLifeCycleEvent.LAYOUT_MISMATCH);
    } catch (NodeNotFoundException ex) {
      LOG.error(""Inconsistent NodeStateMap! Datanode with ID {} was "" +
          ""added but not found in  map: {}"", dnID, nodeStateMap);
    }
    eventPublisher.fireEvent(SCMEvents.NEW_NODE, datanodeDetails);
  }

{code}

The node is added to the node state map (where other threads can view it) before its layout version information is updated.

This manifests as an intermittent test failure in TestSCMNodeManager#testSCMLayoutOnRegister, which fails due to this condition after about 15-30 consecutive runs.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-29 21:39:38,22
13375816,Use ReplicationConfig in OmKeyArgs,"During the implementation of HDDS-5145 I realized that OmKeyArgs also uses factor/type, it seems to be easier to convert it to replicationConfig as it's an in-memory class not a protobuf which is required to be persisted.

Having a half-baked patch planning to upload it soon.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-29 11:34:36,6
13375766,Clean objects created by Freon randomkeys test,"When doing performance tests with Freon randomkeys, it will be quite annoying to delete the objects created. 

This feature is to add an option to let Freon help clean all the volumes, buckets, and keys created.",pull-request-available,[],HDDS,Improvement,Major,2021-04-29 07:17:46,7
13375754,Duplicate jdwp options when running bin/ozone freon and sh,"When trying to using remote debug with Freon, ozone prompt the following errors:

 
{code:java}
$  export OZONE_FREON_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005
$ bin/ozone freon randomkeys --num-of-volumes=1 --num-of-buckets 1 --num-of-keys 10  --replication-type=RATIS --factor=THREE 
ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.
Error occurred during initialization of VM
agent library failed to init: jdwp
{code}
The reason is that the value of `OZONE_FREON_OPTS` was added to `OZONE_OPTS` twice, need to remove the first assignment to `OZONE_OPTS`.

The same error happens to `/bin/ozone sh` command.

 ",pull-request-available,[],HDDS,Bug,Major,2021-04-29 06:53:30,7
13375739,OM DB checkpoint servlet not accessible in a secure cluster,"When security and ACL is enabled, but not spnego, the OMDBCheckpointServlet throws an error:
{code:java}
10:02:29.094 PM ERROR OMDBCheckpointServlet 
 Permission denied: User principal 'dr.who' does not have access to /dbCheckpoint.
 This can happen when Ozone Manager is started with a different user.
 Please append 'dr.who' to OM 'ozone.administrators' config and restart OM to grant current user access to this endpoint.{code}

When Spnego is disabled, permissions cannot be checked since HTTP request will not have an identity (kerberos principal) which is causing this error.

",pull-request-available,['OM'],HDDS,Bug,Major,2021-04-29 04:54:48,37
13375689,Improve client and server logging,"On the OM server side, NotLeaderException and LeaderNotReadyExceptions can be suppressed. Otherwise, OM1 log is flooded with NotLeaderExceptions as clients always try OM1 before moving to the next OM. Instead we can change these exception logs to DEBUG.

Some BlockOutputStream and BlockOutputStreamEntryPool logs should be DEBUG level instead of INFO.
Running a 20GB put key operation resulted in the following console log:
{code:java}
ozone sh key put o3://ozone1/vol2/buck2/20GB /tmp/20GB
 21/03/08 19:01:05 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
 21/03/08 19:01:05 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
 21/03/08 19:01:05 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
 21/03/08 19:01:06 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
 21/03/08 19:01:06 INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-30FCE96A6E8D->c2e9a19c-15f3-4eae-ba46-83c763d2ee8d
 21/03/08 19:01:06 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851570_chunk_43 blockID conID: 74 locID: 105855717519851570 bcsId: 7856 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 74 in CLOSING state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851570_chunk_44 blockID conID: 74 locID: 105855717519851570 bcsId: 7856 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 74 in CLOSED state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_1 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_2 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:39 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_3 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_4 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state
 21/03/08 19:02:41 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:43 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:45 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:47 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:48 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:50 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:52 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:54 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:56 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:02:57 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:02:59 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:01 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:02 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:04 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:06 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:07 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:10 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:11 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:13 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:15 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:17 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:18 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:20 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:22 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:23 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:28 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:30 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
21/03/08 19:03:31 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList
{datanodes = [], containerIds = [#74, #75], pipelineIds = []}
 21/03/08 19:03:33 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}
{code}",pull-request-available,[],HDDS,Improvement,Major,2021-04-28 21:09:17,43
13375560,Remove some Freon integration tests,"Some Freon integration tests are unnecessary, as it is already widely used in acceptance tests.",pull-request-available,[],HDDS,Test,Major,2021-04-28 09:45:26,1
13375556,Periodic disk check interval is fixed 15min and should be configurable,Periodic disk check interval is fixed 15min and should be configurable.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Minor,2021-04-28 09:19:42,21
13375364,Decommissioning a dead node should complete immediately,"If you run the decommission or ""enter maintenance"" command on a node which is already dead, then it should immediately go to the DECOMMISSIONED or IN_MAINTENANCE state. As the node is already dead, there is no way to replicate its containers in a controlled way, and hence the decommission process does not need to run.",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-04-27 14:47:47,11
13375289,Fix Suggested leader in Client,"Server returns suggested leader correctly, but on client when performing regex, extraction of suggested leader address is incorrect. The issue is due to the RegEx pattern to obtain the suggested leader.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-04-27 10:04:42,13
13375263,Support ByteBuffer read in OzoneInputStream,"Hadoop fs FSDataInputStream supports ByteBufferReadable interface, which is not available in Ozone now. 

This task is to implement the ByteBufferReadable interface in OzoneInputStream. 

",pull-request-available,[],HDDS,Improvement,Major,2021-04-27 08:36:27,5
13375198,"when source datanode download container tar from target datanode,but the target datanode container file missing,import error","2021-03-25 11:18:21,703 [grpc-default-executor-1879] INFO org.apache.hadoop.ozone.container.replication.GrpcReplicationService: Streaming container data (50383) to other datanode2021-03-25 11:18:21,703 [grpc-default-executor-1879] INFO org.apache.hadoop.ozone.container.replication.GrpcReplicationService: Streaming container data (50383) to other datanode2021-03-25 11:18:21,812 [grpc-default-executor-1879] INFO org.apache.hadoop.ozone.container.replication.GrpcOutputStream: Sent 119153 bytes for container 503832021-03-25 11:18:21,812 [grpc-default-executor-1879] ERROR org.apache.hadoop.ozone.container.replication.GrpcReplicationService: Error streaming container 50383java.nio.file.NoSuchFileException: /data3/hdds/hdds/326a5fe1-e63c-44b6-a57e-2f858fe4eaa7/current/containerDir98/50383/chunks at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427) at java.nio.file.Files.newDirectoryStream(Files.java:457) at java.nio.file.Files.list(Files.java:3451) at 

 

 

2021-03-25 11:18:21,695 [ContainerReplicationThread-4] INFO org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Starting replication of container 50383 from [d481556f-0451-4669-9aba-74360e0b26fd\{ip: 9.179.149.196, host: tdw-9-179-149-196, networkLocation: /rack2, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, bcdf3bd5-7b8e-435d-b3fa-b3e29f0eb307\{ip: 9.180.5.41, host: tdw-9-180-5-41, networkLocation: /rack10, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]2021-03-25 11:18:21,695 [ContainerReplicationThread-4] INFO org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Starting replication of container 50383 from [d481556f-0451-4669-9aba-74360e0b26fd\{ip: 9.179.149.196, host: tdw-9-179-149-196, networkLocation: /rack2, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, bcdf3bd5-7b8e-435d-b3fa-b3e29f0eb307\{ip: 9.180.5.41, host: tdw-9-180-5-41, networkLocation: /rack10, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]2021-03-25 11:18:21,723 [EndpointStateMachine task thread for /10.51.87.181:9891 - 0 ] INFO org.apache.hadoop.ipc.Client: Retrying connect to server: tdw-10-51-87-181/10.51.87.181:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)2021-03-25 11:18:21,807 [grpc-default-executor-1390] INFO org.apache.hadoop.ozone.container.replication.GrpcReplicationClient: Container 50383 is downloaded to /tmp/container-copy/container-50383.tar.gz2021-03-25 11:18:21,807 [ContainerReplicationThread-4] INFO org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Container 50383 is downloaded, starting to import.2021-03-25 11:18:21,810 [ContainerReplicationThread-4] ERROR org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Container 50383 replication was unsuccessful.java.io.IOException: Container descriptor is missing from the container archive. at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.unpackContainerDescriptor(TarContainerPacker.java:190) at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.importContainer(DownloadAndImportReplicator.java:76)",pull-request-available,[],HDDS,Bug,Major,2021-04-27 03:24:39,23
13375043,Bump ratis version to 2.1.0-ff8aa66-SNAPSHOT,"Bump ratis version to 2.1.0-ff8aa66-SNAPSHOT

Check https://issues.apache.org/jira/browse/RATIS-1369 for the reason.",pull-request-available,[],HDDS,Bug,Major,2021-04-26 11:10:11,36
13374747,Intermittent test failure in TestContainerDeletionChoosingPolicy#testRandomChoosingPolicy,"Observed in this CI run: [https://github.com/apache/ozone/pull/2179/checks?check_run_id=2423463857]

Bundle is attached to issue.",pull-request-available,[],HDDS,Bug,Minor,2021-04-23 23:06:26,6
13374647,Extend Pipline/ReplicationConfig refactor with ECReplicationConfig,"HDDS-5047 started to use ReplicationConfig in Pipeline and pipeline related SCM service on master. ReplicationConfig was introduced in HDDS-5011 but it has two versions one for master and one for the EC branch.

Merging master after HDDS-5047 requires small modification in the code to make HDDS-5047 compatible the ec version of HDDS-5011:

 * During the proto serialization / deserialization we should use optional ECReplicationConfig (if exists).",pull-request-available,[],HDDS,Sub-task,Major,2021-04-23 12:06:59,6
13374638,Create github check to alert when dependency tree is changed,Please see: https://github.com/apache/ozone/pull/2177,pull-request-available,[],HDDS,Improvement,Major,2021-04-23 11:44:46,6
13374545,"Make generic streaming client/service for container re-replication,  data read, scm/om snapshot download","Currently, grpc client/server end points are used for container re-replication, reading data, scm snapshot download for SCM HA etc but have individual client and server implementations. The idea her is to define a generic interface for client/servers and use it across all components for better maintainability, better code reuse and easier debugging.",pull-request-available,"['OM', 'Ozone Client', 'Ozone Datanode', 'SCM', 'SCM HA']",HDDS,Bug,Major,2021-04-23 04:46:54,6
13374362,Refactor HddsUtils and HddsServerUtils,This Jira is to cleanup getScmAddress and other methods in finding SCM client/block address.,pull-request-available,['SCM HA'],HDDS,Task,Major,2021-04-22 09:54:46,13
13374332,Upgrade related RPC calls should be allowed only for admins,"As far as I see any user can finalize upgrade (and I assume the same is true for preparation).

{code}
bash-4.2$ kinit -kt /etc/security/keytabs/testuser.keytab testuser/scm
bash-4.2$ ozone sh volume create /vol1
PERMISSION_DENIED User testuser/scm@EXAMPLE.COM doesn't have CREATE 
permission to access volume vol1 null null
{code}

Failed as I am not an admin, but:

{code}
bash-4.2$ ozone admin scm  finalizeupgrade
Upgrade has already been finalized.
Exiting...
bash-4.2$
{code}

Please confirm, but I think a quick isAdmin check is missing from all the related RPC endpoints.",pull-request-available,[],HDDS,Sub-task,Blocker,2021-04-22 08:54:38,22
13374331,Adjust LICENSE and NOTICE files for the non-rolling upgrade branch,"The branch introduced new dependencies:

{code}
> ./share/ozone/lib/aspectjrt.jar
> ./share/ozone/lib/aspectjweaver.jar
87c89
< ./share/ozone/lib/hk2-locator.jar
---
> ./share/ozone/lib/hk2-locator-b42.jar
108,109d109
< ./share/ozone/lib/jakarta.validation-api.jar
< ./share/ozone/lib/jakarta.ws.rs-api.jar
114a115
> ./share/ozone/lib/javax.inject-b42.jar
117a119
> ./share/ozone/lib/javax.ws.rs-api.jar
214a217
> ./share/ozone/lib/reflections.jar
229a233
> ./share/ozone/lib/validation-api.Final.jar
{code}

As far as I see the LICENCE and NOTICE files are not changed under hadoop-ozone/dist/src/main/license/bin

We should update the LICENSE file with new / modified maven artifact names.

Related LICENSE files should be included in hadoop-ozone/dist/src/main/license/bin/licenses (except pure Apache license)

In case of dependency is licensed under Apache, and IF there is a NOTICE file in the repository of the dependency, it should be copied to the NOTICE.txt (or referenced from NOTICE.txt)",pull-request-available,[],HDDS,Sub-task,Blocker,2021-04-22 08:47:21,21
13374188,Use timeout in github actions,"Default timeout for one GitHub action job is 6 hours. It turned out that in some unfortunate case the acceptance tests were pending at this time, for example in https://github.com/apache/ozone/runs/2053572861?check_suite_focus=true

We need to turn on stricter timeouts for each of our jobs  to avoid unnecessary build time usage.",pull-request-available,[],HDDS,Improvement,Major,2021-04-21 17:47:19,6
13374110,S3gateway didn't return correct error messages while using S3-sdk,"When connecting to S3 gateway using S3-sdk, users can get some unclear error messages from S3 gateway. For example, if the user didn't specify _accessId_ the error message would be as follows:
{code:java}
Server Error (Service: Amazon S3; Status Code: 500; Error Code: 500 Server Error; Request ID: null; S3 Extended Request ID: null; Proxy: null)
{code}
The user would assume it's a server-side exception and come to ask if the cluster got some error, after checking the logs, we found it's caused by user not specifying authenticate information. This information should be returned to users to save both the client and server side's time.

After checking the root cause, it's caused by weld-core put the _OS3Exception_ inside the _CreationException_ provided by the CDI annotation of _@produces_, then jersey wrapped the exception because it's not a _WebApplicationException_.

The fixed version of error message for the above scenario is as follows:
{code:java}
The authorization header you provided is invalid. (Service: Amazon S3; Status Code: 404; Error Code: AuthorizationHeaderMalformed; Request ID: 2102408b-f97b-477f-8510-07e7008792be; S3 Extended Request ID: null; Proxy: null){code}",pull-request-available,[],HDDS,Bug,Major,2021-04-21 10:55:37,7
13374077,HDDS-5127. Fix getServiceList when SCM HA is enabled,"steps taken :
1. SCM HA enabled in ozone cluster.
2. Ran ozone sh volume list command.

 

exception seen :

-------------------
{noformat}
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Does not contain a valid host:port authority: <SCM CLIENT ADDRESSES> at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:213) at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:164) at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:153) at org.apache.hadoop.hdds.HddsUtils.getScmAddressForClients(HddsUtils.java:112) at org.apache.hadoop.ozone.om.OzoneManager.getServiceList(OzoneManager.java:2658) at org.apache.hadoop.ozone.om.OzoneManager.getServiceInfo(OzoneManager.java:2678) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getServiceList(OzoneManagerRequestHandler.java:451) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:176) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:190) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:132) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887){noformat}
 

this issue is due to ""ozone.scm.client.address"" config set in SCM HA setup which contains multiple SCM host addresses.",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-04-21 09:06:41,13
13374064,Recon should check new containers of a container report with batch,"in my test environment,  400000 containers exist. when bootstrap a new recon, every container will be checked and added to recondb.

but , for now , recon check all the containers in a container report one by one , and each check will take a rpc call to scm. this is too slow and in my test environment , it leads to recon oom, because too many containers to be consumed are waitting in the message queue .

this Jira will use a batch method to handle this problem.",pull-request-available,[],HDDS,Bug,Major,2021-04-21 08:36:49,2
13373931,Only test ozonesecure with SCM Ratis disabled,"As [spotted|https://github.com/apache/ozone/pull/2052#discussion_r616570112] by [~elek], {{ozonesecure}} acceptance test is run twice: with SCM Ratis enabled and disabled.  This doubles execution time.  Now that {{ozonesecure-ha}} acceptance test also has SCM HA (implies Ratis enabled), I don't think it's necessary to run {{ozonesecure}} (single SCM) with Ratis enabled.",pull-request-available,['test'],HDDS,Improvement,Major,2021-04-20 19:17:29,1
13373826,"Use OzoneConsts.OZONE_TIME_ZONE instead of ""GMT""","The ""GMT"" already defined in OzoneConsts, so why not user it.",patch-available pull-request-available,[],HDDS,Improvement,Trivial,2021-04-20 10:43:50,29
13373805,Use the pre-created apache/ozone-testkrb5 image during secure acceptance tests,"Today ozonesecure compose clusters (and ozonesecure-ha and ozonesecure-mr) use an adhoc keytab issuer. The issuer is download during the image creation and uses a third party go lang application to create the keytabs on-demand.

As discussed earlier, it would be faster to use a dedicated, pre-built container image which includes the pre-created keytabs instead of issuing them on-the fly (keytab generation is slow + container creation is slow)

For each of the tagged images we can export to current keytabs to hadoop-ozone/dist/src/main/compose/ which can be mounted to to compose clusters.

It makes the overall acceptance test faster (instead of creating keytab, which is quite slow, we can start the cluster immediately). And we don't need to depend on an external utility app.

Pre-created keytabs are also more similar to production environment...

First test using the apache/ozone-testkrb5 from HDDS-4938

The time between starting test.sh script and first robot test:

master: 3:30 (01:43:08 --01:46:38)
this patch: 2:10 (12:59:29 13:02:39)

(note: there are some variances between different builds, and in general the patch build was a slower one. It can be even faster).

~",pull-request-available,[],HDDS,Improvement,Major,2021-04-20 09:04:41,6
13373785,SCM Reinitialization can end up leaking Ratis Segmented RaftLogWorker threads,"During SCM reinitialialisation, ratis server is spinned up to check if an existing ratis group exists or not, and closes the server without starting it. In ratis, the segmented raft log worker thraeds are started during init() itself but get closed during raftServer.close() only if the server transitions to RUNNING state which causes the issue.

 
{code:java}
Attaching to process ID 266710, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.232-b09
Deadlock Detection:No deadlocks found.Thread 266745: (state = BLOCKED)Locked ownable synchronizers:
    - NoneThread 266783: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=78, line=2078 (Compiled frame)
 - org.apache.ratis.util.DataBlockingQueue.poll(org.apache.ratis.util.TimeDuration) @bci=134, line=137 (Compiled frame)
 - org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run() @bci=16, line=292 (Interpreted frame)
 - org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$161.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)Locked ownable synchronizers:
    - NoneThread 266761: (state = BLOCKED)Locked ownable synchronizers:
    - NoneThread 266760: (state = BLOCKED)Locked ownable synchronizers:
    - NoneThread 266759: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.ref.ReferenceQueue.remove(long) @bci=59, line=144 (Compiled frame)
 - java.lang.ref.ReferenceQueue.remove() @bci=2, line=165 (Compiled frame)
 - java.lang.ref.Finalizer$FinalizerThread.run() @bci=36, line=216 (Interpreted frame)Locked ownable synchronizers:
    - NoneThread 266758: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Object.wait() @bci=2, line=502 (Compiled frame)
 - java.lang.ref.Reference.tryHandlePending(boolean) @bci=54, line=191 (Compiled frame)
 - java.lang.ref.Reference$ReferenceHandler.run() @bci=1, line=153 (Interpreted frame)Locked ownable synchronizers:
    - None
{code}",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-04-20 07:50:22,16
13373729,Release Ozone 1.1.0,Task for Ozone 1.1.0 release.,pull-request-available,[],HDDS,Task,Blocker,2021-04-20 00:43:25,44
13373675,Recover from failure during upgrade action,"Currently, if an error occurs while running an upgrade action and a component fails, restarting the component will result in an InconsistentStorageException. Restart requires manual modification of the version file. This case should be handled automatically by the upgrade framework.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-19 18:17:01,22
13373671,CRLInfo should include CRL Sequence ID,The CRLInfo class does not contain CRL Sequence ID and it would be good to include it while iterating the  CRLInfos as a list.,pull-request-available,['Security'],HDDS,Sub-task,Major,2021-04-19 17:50:31,37
13373626,Secure datanode/OM may exit if cannot connect to SCM,"Intermittent failure in secure acceptance tests indicates that datanode may fail to start up if SCM is not yet ready:

{noformat}
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
...
datanode_3  | 2021-04-19 08:20:29,030 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:dn@627dcb55b990
...
datanode_3  | 2021-04-19 08:20:57,660 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 627dcb55b990/172.26.0.4 to scm:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy18.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.26.0.10:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_3  | 2021-04-19 08:20:59,667 [main] ERROR ozone.HddsDatanodeService: Error while storing SCM signed certificate.
...
datanode_3  | 	at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.submitRequest(SCMSecurityProtocolClientSideTranslatorPB.java:104)
datanode_3  | 	at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.getDataNodeCertificateChain(SCMSecurityProtocolClientSideTranslatorPB.java:263)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.getSCMSignedCert(HddsDatanodeService.java:349)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:320)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:248)
datanode_3  | 	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:192)
...
datanode_3  | SHUTDOWN_MSG: Shutting down HddsDatanodeService at 627dcb55b990/172.26.0.4
{noformat}",pull-request-available,"['Ozone Datanode', 'SCM HA', 'Security']",HDDS,Bug,Critical,2021-04-19 13:54:05,13
13372668,DataNode should not always report full information in heartbeat,"when investigating SCM OOM,  I find that datanode will always report full information about containers，pipeline and node.

By default , ContainerReportPublisher thread runs periodically (HDDS_CONTAINER_REPORT_INTERVAL, default 60s) in Datanode , and The HeartbeatEndpointTask ，which runs periodically (hdds.heartbeat.interval)should only report information in incrementalReportQueue

I think the code below has a problem that getReports will always get full information ，because containerReports always holds full information.
{code:java}
// code placeholder
List<GeneratedMessage> getNonIncrementalReports() {
  List<GeneratedMessage> nonIncrementalReports = new LinkedList<>();
  GeneratedMessage report = containerReports.get();
  if (report != null) {
    nonIncrementalReports.add(report);
  }
  report = nodeReport.get();
  if (report != null) {
    nonIncrementalReports.add(report);
  }
  report = pipelineReports.get();
  if (report != null) {
    nonIncrementalReports.add(report);
  }
  return nonIncrementalReports;
}

/**
 * Returns available reports from the report queue with a max limit on
 * list size, or empty list if the queue is empty.
 *
 * @return List of reports
 */
public List<GeneratedMessage> getReports(InetSocketAddress endpoint,
                                         int maxLimit) {
  if (maxLimit < 0) {
    throw new IllegalArgumentException(""Illegal maxLimit value: "" + maxLimit);
  }
  List<GeneratedMessage> reports = getNonIncrementalReports();
  if (maxLimit <= reports.size()) {
    return reports.subList(0, maxLimit);
  } else {
    reports.addAll(getIncrementalReports(endpoint,
        maxLimit - reports.size()));
    return reports;
  }
}
{code}",pull-request-available,[],HDDS,Bug,Major,2021-04-15 14:58:36,2
13372469,TestStorageContainerManagerHttpServer fails in CI,"{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer
-------------------------------------------------------------------------------
Tests run: 3, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 302.762 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer
testHttpPolicy[0](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 60.956 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:110)

testHttpPolicy[1](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 180.331 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:116)

testHttpPolicy[2](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 60.223 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:110)
{noformat}",pull-request-available,['test'],HDDS,Bug,Critical,2021-04-14 18:13:48,1
13372446,Track OM prepare intermittent integration test failure,"During CI runs, an occasional timeout failure in TestOzoneManagerPrepare#testPrepareWithRestart has been noticed. This Jira is to investigate and fix the intermittent failure.

Failed runs:
https://github.com/apache/ozone/pull/2217/checks?check_run_id=2568316007
https://github.com/apache/ozone/runs/2335155142?check_suite_focus=true",pull-request-available,[],HDDS,Sub-task,Major,2021-04-14 16:09:06,22
13372371,Attempt to remove state from *UpgradeFinalizer classes.,Follow up from https://github.com/apache/ozone/pull/1998#discussion_r606369185.,pull-request-available,[],HDDS,Sub-task,Major,2021-04-14 14:46:20,30
13372283,Add trace for contaner report,"trace how much time does it take to process a single ContainerReport.

This can help debugging SCM OOM",pull-request-available,[],HDDS,Sub-task,Major,2021-04-14 11:46:07,2
13372267,[FSO] Avoid using V1 postfixes for prefix related classes,"I already commented in it HDDS-5097, but it seems that majority of the new functions are implemented in just copying existing classes with V1 postfix.

It's not clear what is V1 (or what is V0). And I already quoted the clead code recommendation:

It turned out that a lot of tests just copied with V1 prefix with small modification (or original test extended with V1 which means the methods of the old and new tests are executed) instead of improving the original test to cover both of the cases (simple/prefix-ed).

Also: the same functionality seems to be tested on multiple levels (FileSystemInterface, OMRequest, acceptance test...)

This copy can make the maintenance of the tests slightly harder, and the V1 prefix is quite meaningless and confusing.

From the legendary ""Clean code"" book:

bq. Programmers create problems for themselves when they write code solely to satisfy a compiler or interpreter. For example, because you can’t use the same name to refer to two different things in the same scope, you might be tempted to change one name in an arbitrary way. Sometimes this is done by misspelling one, leading to the surprising situation where correcting spelling errors leads to an inability to compile.2

bq.  [...] It is not sufficient to add number series or noise words, even though the compiler is satisfied. If names must be different, then they should also mean something different.

bq.  Number-series naming (a1, a2, .. aN) is the opposite of intentional naming. Such names are not disinformative—they are noninformative; they provide no clue to the author’s intention....

I totally agree with this section, I think we should avoid using V1 prefixes everywhere and have some more meaningful class names.

 ",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Sub-task,Blocker,2021-04-14 11:02:03,40
13372181,Fix Install Snapshot Mechanism in SCMStateMachine,"With https://issues.apache.org/jira/browse/RATIS-1326 now fixed and ozone being updated with latest ratis, the idea here is to fix the installSnapshot behaviour in SCM HA.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-04-14 05:04:20,16
13372152,Error with unit test for hdds.container-service TestSchemaOneBackwardsCompatibility,"The unit test _testDelete_ from _TestSchemaOneBackwardsCompatibility_ (_hadoop-hdds/container-service_) has an observed reproducible intermittent error.  This error is due to the test launching a background task (_BackgroundService.java_) that it expects to block to completion, however it does not.  The test, through a thread pool executor launches a task to delete blocks and expects to block until completion.   After blocked until completion it checks the worker result.  Since the current implementation does not block, at times the worker is not done before the test checks the condition, hence the intermittent error.

 

Observed error:
{code:java}
mvn -Dtest=TestSchemaOneBackwardsCompatibility test{code}
(see attachment err)

 

 With waiting until worker task is finished before checking condition: (see attachment showing test passed)

 

 

 ",pull-request-available,[],HDDS,Bug,Minor,2021-04-14 01:19:54,39
13372009,[FSO] Cleanup integration tests and reduce the execution time ,"Apache organization has a shared, organization-level resource limit for Github actions. All the projects should use the available minutes fairly to avoid blocking the build of other Apache projects.

This is discussed recently here: https://lists.apache.org/thread.html/r48d079eeff292254db22705c8ef8618f87ff7adc68d56c4e5d0b4105%40%3Cbuilds.apache.org%3E, but same mailing list has older threads, too.

HDDS-2939 branch build time has been increased significantly. The problem with acceptance tests are tracked with HDDS-5093. 

But integration tests are also increased from ~56 minutes to 94 minutes (~158%)

We need to avoid such increase unless we have very good reason to have it.

If we check the execution time of the tests on the branch and the master we can see the problem in more details:

{code}
 tmp  %  grep -e 'Tests run: .* in org' ""20"" | awk -F '[,:\-]' '{print $6, $8, $10, $12, $14, $15}' | sed 's/s   in //g' | sort -n -k5 -r | head
awk: warning: escape sequence `\-' treated as plain `-'
 104  0  0  0  478.951 org.apache.hadoop.fs.ozone.TestRootedOzoneFileSystem
 88  0  0  0  458.703 org.apache.hadoop.fs.ozone.TestOzoneFileSystem
 18  0  0  2  436.521 org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces
 6  0  0  0  152.808 org.apache.hadoop.fs.ozone.TestOzoneFSWithObjectStoreCreate
 4  0  0  0  108.191 org.apache.hadoop.hdds.scm.pipeline.TestPipelineClose
 1  0  0  0  88.905 org.apache.hadoop.hdds.scm.pipeline.TestNodeFailure
 2  0  0  0  74.192 org.apache.hadoop.fs.ozone.TestOzoneFsHAURLs
 19  0  0  0  58.2 org.apache.hadoop.fs.ozone.contract.rooted.ITestRootedOzoneContractGetFileStatus
 8  0  0  0  57.063 org.apache.hadoop.fs.ozone.contract.rooted.ITestRootedOzoneContractDistCp
 8  0  0  0  54.612 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
 tmp  %  grep -e 'Tests run: .* in org' ""18"" | awk -F '[,:\-]' '{print $6, $8, $10, $12, $14, $15}' | sed 's/s   in //g' | sort -n -k5 -r | head
awk: warning: escape sequence `\-' treated as plain `-'
 128  0  0  0  509.388 org.apache.hadoop.fs.ozone.TestRootedOzoneFileSystem
 132  0  0  0  508.585 org.apache.hadoop.fs.ozone.TestOzoneFileSystem
 140  0  0  8  479.578 org.apache.hadoop.fs.ozone.TestOzoneFileSystemV1
 18  0  0  2  475.686 org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces
 128  0  0  0  470.619 org.apache.hadoop.fs.ozone.TestRootedOzoneFileSystemV1
 18  0  0  6  329.812 org.apache.hadoop.fs.ozone.TestOzoneFileInterfacesV1
 5  0  0  0  226.093 org.apache.hadoop.fs.ozone.TestDirectoryDeletingServiceWithFSOBucket
 6  0  0  0  171.861 org.apache.hadoop.fs.ozone.TestOzoneFSWithObjectStoreCreate
 4  0  0  0  139.951 org.apache.hadoop.hdds.scm.pipeline.TestPipelineClose
 3  0  0  0  82.128 org.apache.hadoop.fs.ozone.TestOzoneFileSystemPrefixParser
{code}

It turned out that a lot of tests just copied with V1 prefix  with small modification  (or original test extended with V1 which means  the methods of the old and new tests are executed) instead of improving the original test to cover both of the cases (simple/prefix-ed).

Also: the same functionality seems to be tested on multiple levels (FileSystemInterface, OMRequest, acceptance test...) 

This copy can make the maintenance of the tests slightly harder, and the V1 prefix is quite meaningless and confusing.

From the legendary ""Clean code"" book:

>Programmers create problems for themselves when they write code solely to satisfy a compiler or interpreter. For example, because you can’t use the same name to refer to two different things in the same scope, you might be tempted to change one name in an arbitrary way. Sometimes this is done by misspelling one, leading to the surprising situation where correcting spelling errors leads to an inability to compile.2

> [...] It is not sufficient to add number series or noise words, even though the compiler is satisfied. If names must be different, then they should also mean something different.

> Number-series naming (a1, a2, .. aN) is the opposite of intentional naming. Such names are not disinformative—they are noninformative; they provide no clue to the author’s intention....

I totally agree with this section, I think we should avoid using V1 prefixes everywhere.


Suggestions:

 1. We should minimize the additional build time when possible.
 2. Similar to HDDS-5093 instead of creating a new dimension to the parametrized tests (which double the execution time) we should carefully select the meaningful sets of parameters. (When we have 3 parameters for a unit test with 2 values for each it's already 8 possible executions. Execution all of them with and without prefix may not be required, it seems to be better to select representative parameter sets and use only them).
 3. The V1 prefixes should be avoided. When possible we need to modify the original unit tests with adding additional parameters. It makes it easier to maintain the unit tests, and it requires less code and (together with the previous point) less time  
 4. We don't need to repeat ALL the tests IMHO. For example if we modified only the rename part of the OzoneFileSystem, I assume that it's enough to test that part with/without prefix (especially as the prefix handling is already tested with testing om requests...)

 

",pull-request-available,[],HDDS,Sub-task,Blocker,2021-04-13 10:25:14,40
13371977,Adjust download pages to use Apache Ozone (tlp) artifacts,Please see: https://github.com/apache/ozone-site/pull/4,pull-request-available,[],HDDS,Improvement,Major,2021-04-13 08:19:30,6
13371783,[FSO] Fail OM startup when turn on prefix layout with old buckets,"Steps to reproduce:

1. cd compose/upgrade && docker-compose up -d (if it fails, fix the permission of the dirs, and repeat the up command)

2. create /vol1/bucket1/dir1/dir2/key1

3. list the keys in /vol1/bucket1

4. stop the cluster

5. modify the docker-config by enabling prefix layout:
{code:java}
OZONE-SITE.XML_ozone.om.enable.filesystem.paths=true
OZONE-SITE.XML_ozone.om.metadata.layout=prefix
{code}
6. start the cluster

Expected behavior: Fail to start OM as the configured cluster layout and persisted bucket layout mismatches. Presently, there is no validation during OM startup.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-12 12:18:23,40
13371769,[FSO] Reducing time of ozonefs acceptance testmatrix,"Today we execute ozonefs/ozonefs.robot with compose/ozone/test.sh with multiple matrix parameters:

{code}
for scheme in ofs o3fs; do
    for bucket in link bucket; do
       #test ozonefs/ozonefs.robot
   done
done
{code}

HDDS-2939 doubles these 4 executions with introducing the layout parameter (simple vs prefix).  At the same time the execution time of acceptance (unsecure) tests are increased from 37 minutes to 57 minutes.

I would suggest suggesting to use only selected tests from this 2 x 2 x 2 matrix.

For example:

bucket / o3fs / prefix
link / ofs / simple
bucket / ofs / prefix

 ",pull-request-available,[],HDDS,Sub-task,Major,2021-04-12 11:43:05,6
13371202,Add project separation and first stable release to the HISTORY.md,Please see: https://github.com/apache/ozone/pull/2149,pull-request-available,[],HDDS,Improvement,Major,2021-04-12 08:21:43,6
13371185,make Decommission work under SCM HA.,"*The problem*

The decommission/maintenance info is saved in memory of SCM, and if SCM is restarted, it relearns this info during re-register of Datanode.

Only leader SCM handles the decommissionNodes(), recommissionNodes(), startMaintenanceNodes() request, and not replicate these info to follower SCM, thus when failover happens, the new leader SCM will lose this info, since they are saved in memory of previous leader SCM.

*Current status*
 If a SCM is restarted, then upon re-registration the datanode will already be in DECOMMISSIONING or ENTERING_MAINTENANCE or IN_MAINTENANCE state. In that case, it needs to be added back into the monitor to track its progress.

For a registered node, the information stored in SCM is the source of truth. If SCM finds that the opState or opStateExpiryEpoch is different from what it saves in memory, it will send SetNodeOperationalStateCommand to update the Datanode.

*The solution*

leader SCM --hb--> DN --hb--> follower SCM

1, Leader SCM updates PersistedOpState of Datanode via heartbeat. Datanode update OpState in follower SCM via heartbeat.

2, When follower SCM becomes leader, it calls continueAdminForNode for all datanode, so that the DECOMMISSIONING, ENTERING_MAINTENANCE, IN_MAINTENANCE datanode will be added back to the monitor.

*Disadvantage*

The same as now, if leader SCM records the info, notifies Datanode via heartbeat, but steps down before Datanode notifies follower SCM via heartbeat, that info will be lost in the new leader SCM.

As discussed with [~sodonnell], we can live with the rare event of a decommission starting and SCM failing over before the state has made it to the DNs.

 

For details: https://docs.google.com/document/d/1N5PsUuLBGgvkYFQgDumvRZujc-9RcDwoE0SubZcLUzY/edit?usp=sharing

 ",pull-request-available,[],HDDS,Sub-task,Major,2021-04-12 06:38:44,36
13371181,On-demand disk checker for hdds volume,"Currently the MutableVolumeSet has a periodic disk checker which runs every 15mins(fixed, not configurable), so it will be about serveral mins before datanode get the chance to detect bad disks.

As I found, datanode does not handle failure on the write path, we may have to introduce on-demand disk checks when io failure is hit(due to bad disks, etc).

By the way, hdfs has only a lazy on-demand disk checker:

[https://blog.cloudera.com/hdfs-datanode-scanners-and-disk-checker-explained/]",pull-request-available,[],HDDS,Sub-task,Major,2021-04-12 06:32:13,21
13370712,Ozone RPC client leaks KeyProvider instances,Ozone RPC Client currently create a key provider instance each time the getKeyProvider is invoked. The caller such as objectstore does not keep track of the returned KMS provider with a close(). This leads to leaks of resources associate with KMS provider (e.g. SSL). ,pull-request-available,[],HDDS,Bug,Major,2021-04-09 17:51:32,28
13370708,Add pre-finalize validation action for SCM HA,"Make sure that if a pre-finalized cluster (whose metadata layout version is behind SCM HA) is started with an SCM HA enabled configuration, SCM will fail to start with an error.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-09 17:27:46,22
13370664,Include HISTORY.md/SECURITY.md/CONTRIBUTING.md in the release artifacts.,Please see: https://github.com/apache/ozone/pull/2140,pull-request-available,[],HDDS,Improvement,Major,2021-04-09 13:28:33,6
13370653,Bump version of common-compress,Please see: https://github.com/apache/ozone/pull/2139,pull-request-available,[],HDDS,Improvement,Major,2021-04-09 13:08:37,6
13370650,Create unit test for OzoneClient,See: https://github.com/apache/ozone/pull/2138,pull-request-available,[],HDDS,Sub-task,Major,2021-04-09 13:02:25,6
13370629,Fix key put implementation,Ozone Go client's {{key put}} command always (tries to) upload local {{/tmp/asd}}.  It should take the input local file from command-line parameter.,pull-request-available,['Go Client'],HDDS,Sub-task,Major,2021-04-09 10:56:14,1
13370594,[SCM HA Security] Enable s3 test suite for ozone-secure-ha,Enable s3 test suite for ozone-secure-ha docker which starts SCM/OM HA in secure env.,pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-04-09 08:47:29,13
13370561,[FSO] Rename om metadata layout configuration value LEGACY,"Rename layout property value {{LEGACY}} to {{SIMPLE.}}

Thanks [~elek] for the suggestion.",pull-request-available,[],HDDS,Sub-task,Major,2021-04-09 06:30:24,40
13370471,Include dev-support in ozone source distribution,"This should include {{dev-support}} in the ozone-src dist.

CC [~ppogde]",pull-request-available,['dist'],HDDS,Task,Major,2021-04-08 19:24:01,12
13370353,[SCM HA Security] Remove code of not starting ozone services when Security is enabled on SCM HA cluster,"Now SCM HA Security is implemented, we can remove the code added as part of HDDS-4978
",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-04-08 10:58:00,13
13370344,Bump Guava version,Please see: https://github.com/apache/ozone/pull/2131,pull-request-available,[],HDDS,Improvement,Major,2021-04-08 10:10:55,6
13370342,Use ReplicationConfig on client side ,"HDDS-5011 introduced new ReplicationConfig with multiple implementations (Ratis/Standalone).

To make ozone sh client EC compatible we can use ReplicationConfig on the client side too: --replication parameter can be parsed from the string based on the ReplicationConfig. Today it can be THREE or ONE, but later ECReplicationConfig can support more sophisticated  schemes (like 3:2) ",pull-request-available,[],HDDS,Sub-task,Major,2021-04-08 10:07:36,6
13370253,[FSO] Addendum patch to fix compilation error,"After HDDS-4691 merge, it requires to change {{OMConfigKeys.OZONE_OM_LAYOUT_VERSION_V1}} to {{OMConfigKeys.OZONE_OM_METADATA_LAYOUT_PREFIX.}}

 

{code}
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestRecursiveAclWithFSOBucket.java:[259,21] cannot find symbol

[ERROR]   symbol:   variable OZONE_OM_LAYOUT_VERSION_V1

{code}",pull-request-available,[],HDDS,Sub-task,Major,2021-04-08 02:54:24,40
13369988,[SCM HA Security] Fix duration of sub-ca certs,"Right now sub-ca certs use hdds.x509.default.duration.
This Jira proposes to use hdds.x509.max.duration similar to selfsigned certs",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-04-07 07:59:52,13
13369895,Fix issues related to Ozone binary name change,- fix post-commit.yml to look for ozone binaries,pull-request-available,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Bug,Major,2021-04-06 22:37:47,44
13369879,build errors due to missing JAXB-runtime jar dependency for openjdk 11 se standalone,"Build errors occur when compiling ozone with _jaxb-api_ dependency on *openjdk 11 SE* standalone environments.  Problem observed with _openjdk 11.0.10 2021-01-19_
_OpenJDK (build 11.0.10+9-Ubuntu-0ubuntu1.18.04)_.

 

This error *is not* present compiling with Java 8.  See _jaxb_ docs for maven compile : [https://javaee.github.io/jaxb-v2/doc/user-guide/ch03.html#deployment-maven-coordinates]. In summary,
{code:java}
If client application needs to include the runtime, e.g. running standalone on JavaSE jaxb-runtime should be also included. {code}

 From this, build files, pom.xml, that currently have the jaxb-api dependency can be changed to :
{code:java}
<!-- API -->
 <dependency>
 <groupId>javax.xml.bind</groupId>
 <artifactId>jaxb-api</artifactId>
 <version>${jaxb.version}</version>
 </dependency>
<!-- Runtime -->
 <dependency>
 <groupId>org.glassfish.jaxb</groupId>
 <artifactId>jaxb-runtime</artifactId>
 <version>${jaxb.version}</version>
 </dependency>
{code}
 

Updating affected pom.xml files with the additional jaxb-runtime dependency resolves observed compilation issues. 

 

Problem originally seen by me and [~jwminton] when building unit tests for hadoop-ozone/tools :

 
{code:java}
hadoop-ozone/tools$ mvn test
[ERROR] testGenerateConfigurations(org.apache.hadoop.ozone.genconf.TestGenerateOzoneRequiredConfigurations) Time elapsed: 0.004 s <<< ERROR!
picocli.CommandLine$ExecutionException: 
Error while calling command (org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations@5562c41e): javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.
 - with linked exception:
[java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
 
{code}
 

Present affected build files: _pom.xml (base dir_), _hadoop-ozone/datanode/pom.xml_, _hadoop-ozone/insight/pom.xml_, _hadoop-ozone/s3gateway/pom.xml, haddop-ozone/tools/pom.xml_ ",pull-request-available,[],HDDS,Bug,Major,2021-04-06 20:57:44,39
13369784,Use fixed vesion from pnpm to build recon,"Recon build is broken since yesterday due to a new pnpm@6.0.0 release

{code}
[INFO] Running 'npx pnpm config set store-dir ~/.pnpm-store' in /home/elek/projects/ozone/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web
[INFO] npx: installed 1 in 1.057s
[INFO] ERROR: This version of pnpm requires at least Node.js v12.17
[INFO] The current version of Node.js is v12.14.1
[INFO] Visit https://r.pnpm.io/comp to see the list of past pnpm versions with respective Node.js version support.
{code}

This is because the frontend maven plugin uses [npx|https://www.npmjs.com/package/npx] which downloads the required tool (pnpm in our case) on-demand if it's not available locally.

This download uses the latest version (by default).  

I recommend using a fixed version from pnpm to avoid any unexpected error when external tools is updated.",pull-request-available,[],HDDS,Bug,Blocker,2021-04-06 12:09:11,6
13369734,Fix project name in NOTICE.txt,Please see: https://github.com/apache/ozone/pull/2112,pull-request-available,[],HDDS,Improvement,Blocker,2021-04-06 09:15:53,6
13369730,Remove hadoop- prefixes from release artifacts (release branch),"When we create a release artifact today it has the `hadoop-` prefix in the names. Would be better to remove to avoid confusion.

Note: full `hadoop-` prefix is removed by HDDS-4936 (thanks to [~sammichen]), this patch is a very small subset of that patch just to fix the tar file name on the release branch.",pull-request-available,[],HDDS,Bug,Major,2021-04-06 09:00:29,6
13369711,Add a config to bypass clusterId validation for bootstrapping SCM,"IN SCM HA, the primary node starts up the ratis server while other bootstrapping nodes will get added to the ratis group. Now, if all the bootstrapping SCM's get stopped, the primary node will now step down from leadership as it will loose majority. If the bootstrapping nodes are now bootstrapped again,  the bootsrapping node will try to first validate the cluster id from the leader SCM with the persisted cluster id , but as there is no leader existing, bootstrapping wil keep on failing and retrying until it shuts down. 

The issue can be very easily simulated in kubernetes deployments, where bootstrap and init cmds are run repeatedly on every restart.

The Jira aims to bypass the cluster id validation if a bootstrapping node already has a cluster id.",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-04-06 07:33:45,16
13369405,Improve block commit,This task is to improve the key location updation logic during commit block.,pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2021-04-03 12:12:06,40
13369167,[SCM HA Security] Make InterSCM grpc channel secure,"Currently, a bootstrapping SCM or a follower SCM can request a SCM rocks dn checkpoint from the leader via external grpc channel. The Jira here aims to ensure the channel is secured before any transfer takes place between SCMs.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Blocker,2021-04-01 16:51:27,13
13369156,Increase number of client retries/ failovers to OMs,"Currently in an OM HA setup, client by default does 15 failovers before giving up. If there are 3 OMs and a leader has not been elected yet, then the client will try each OM in a round robin way, wait for 2 secs and then try all the OMs again. This gives the client around 10 seconds before it exhausts all its failover attempts and fails. 

This Jira proposes to increase the failover attempts to 100.",pull-request-available,[],HDDS,Improvement,Major,2021-04-01 16:25:25,43
13369089,Make getScmInfo retry for a duration,"Previously during init of OM for getScmInfo we used to do RetryForEverWithFixedSleep, but during SCM HA we have removed this.

This Jira proposes to add a ceration duration to try getScmInfo, instead of retry forever with fixed sleep.

In a few docker tests CI run, we have seen this issue, after 15 retries Om init failed, as SCM is started later.


{code:java}
om1_1       | 2021-03-31 17:03:48,184 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2021-03-31 17:03:52,453 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:03:54,455 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:03:56,457 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:03:58,466 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:00,498 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:02,522 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:04,533 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:06,535 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:08,537 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:10,541 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:12,543 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:14,546 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:16,550 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:18,553 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2021-03-31 17:04:20,795 [main] ERROR om.OzoneManager: Could not initialize OM version file
om1_1       | org.apache.hadoop.ipc.RemoteException(org.apache.ratis.protocol.exceptions.NotLeaderException): Server 9cb7a7ae-4c40-401c-b1c6-55728c1f0907@group-C35E1BD0DE21 is not the leader
om1_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.triggerNotLeaderException(SCMRatisServerImpl.java:245)
om1_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:108)
om1_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13874)
om1_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
om1_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
om1_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
om1_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
om1_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
om1_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
om1_1       | 
{code}




",pull-request-available,[],HDDS,Bug,Major,2021-04-01 10:28:25,13
13369078,Remove backward direction dependency betweeon HDDS->Ozone,Please see: https://github.com/apache/ozone/pull/2106,pull-request-available,[],HDDS,Improvement,Major,2021-04-01 10:02:01,6
13368852,RocksDB block cache capacity is wrongly configured,"The DN process uses around 20 GB of RSS after startup and does not release the memory even though rocksdb is closed for every container. 
The capacity used by AbstractDatanodeStore is default capacity * MB. Since default capacity is 64MB therefore block cache is passed a capacity of 64TB. After the fix the DN memory usage drops to ~3GB.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-03-31 11:19:32,35
13368847,[SCM HA Security] Handle leader changes between SCMInfo and getSCMSigned Cert in OM,"This Jira is to handle leader change between getScmInfo and getScmSignedCert.

*Problem:*
*Leader is SCM1 - Returned SCMID is SCM1ID*
ScmInfo returns the leader SCMID

*Leader is SCM2 - SCM ID is SCM2ID*
getSCMSignedCert, during generate certificate it has a check compare the scmId passed in CSR, is same as current SCM scmID

In this case when the leader change between these 2 calls OM will fail to get a Certificate.

",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-31 10:53:11,13
13368824,Ensure failover to suggested leader if any for NotLeaderException,"Currently, ratis suggested leader info is not propagated to rpc clients for ratis request failures. Idea here is to propagate this info and perform failover accordingly if request fail with NotLeaderException.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-31 09:04:22,16
13368821,Add retry policy for ratis requests in SCM HA.,"For SCM HA, on certain exceptions , for example, LeaderNotReady, the requests must be retried on the same server. For exceptions, such as NotLeaderException, retry should happen along with a failover for rpc clients to the suggested leader if possible. The idea here is to address the retry policy requirements for SCM HA.",pull-request-available,[],HDDS,Bug,Major,2021-03-31 08:52:52,16
13368819,Add timeout support for ratis requests in SCM HA,"In SCM HA, the requests submitted to ratis leader can potentially hang. The idea here is to timeout the request if the response is not received after a certain threshold. ",pull-request-available,['SCM HA'],HDDS,Bug,Major,2021-03-31 08:51:28,16
13368671,Refactor Pipeline to use ReplicationConfig instead of factor/type,"HDDS-5011 introduces Java ReplicationConfig classes which can be used as a replacement of replicationType and replicationFactor.

First task is replacing type/factor with ReplicationConfig in Pipeline and related managers (PipelineManager BackgroundPipelineCreatorV2, PipelineStateManager...)

We can do it on the master without the EC related stuff... (later we will add the small part which is required for EC",pull-request-available,[],HDDS,Sub-task,Major,2021-03-30 15:00:58,6
13368669,Merge master with SCM HA changes into upgrade branch.,Merge master with SCM HA changes into HDDS-3698-nonrolling-upgrade branch.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-30 14:55:34,30
13368589,Create acceptance test for using rclone with s3 protocol,"HDDS-4506 adds query based authentication which was required by the s3 protocol implementation of rclone tool.

We can add rclone tool the ozone-runner image and create acceptance test to make sure the tool is always compatible with our s3 implementaion. ",newbie,[],HDDS,Improvement,Major,2021-03-30 08:46:31,32
13368447,[FSO] Improve KeyDeletingService to cleanup FSO files,"Follow-up Jira to HDDS-4495, where it adds the files to {{DeletedTable}} and expects to cleanup these files by KeyDeletingService thread.

Need to verify that, KeyDeletingService is able to cleanup these V1 files and do necessary changes to make it happen.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-29 18:11:48,26
13368407,Use getShortUserName in getTrashRoot(s),"Inspired by HDDS-5019, we should use getShortUserName() instead of getUserName() as Kerberos principal can differ on different nodes while we trust admin will map those Kerberos principal to the same short user names.

https://github.com/apache/ozone/blob/4128813ab495dfd3941c0252e61e41ca7d1cf1ce/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OFSPath.java#L301

https://github.com/apache/ozone/blob/8585fba44a6ffe22fa2c65cc651acec6b6872e5e/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java#L581

cc [~xyao]",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-03-29 15:24:47,12
13368375,"Append om1,om2,om3 to ozone.administrators config for secure OM HA docker config","With HDDS-4704, OM dbCheckpoint endpoint now requires ozone admin privilege to access.

In HA, one OM might access another OM's dbCheckpoint to download DB from another OM.

https://github.com/apache/ozone/blob/d09e4b000594494eff87a828e400fb1a1cdfb3c1/hadoop-ozone/dist/src/main/compose/ozonesecure-om-ha/docker-config#L53",newbie pull-request-available,['docker'],HDDS,Bug,Major,2021-03-29 13:36:47,12
13368320,Support command changing the logging level of a server without  restarting the server,"bin/ozone daemonlog -setlevel hostname:port classname loglevel 

bin/ozone daemonlog -getlevel hostname:port classname ",pull-request-available,[],HDDS,Improvement,Major,2021-03-29 08:02:23,23
13368295,Create and update ZH translation of SCM-HA.md and OM-HA.md in doc,[HDDS-4948|https://github.com/apache/ozone/pull/2050/files] added SCH-HA document.zh translation should be updated for changed /  new files.,doc pull-request-available translation-required-zh,['documentation'],HDDS,Task,Major,2021-03-29 06:51:38,17
13367983,Apply merge/notification settings to ozone-site repo,Apply merge and notification settings via {{.asf.yaml}} from other Ozone repos to {{ozone-site}} repo.,pull-request-available,['website'],HDDS,Improvement,Minor,2021-03-26 18:09:05,1
13367829,Config not loaded when using an Ozone assembled jar,"The issue happens when we were trying to do file operations in our own project with Hadoop-ozone-client assembled. Have attached the original config file.

The error is caused by the missing configs from hdds-common.  When we are building the fat-jar, there are different ""ozone-default-generated.xml"" files causing the conflict, and only one config file is kept. From the client-side, we can use the maven-shade-plugin to merge all ""ozone-default-generated.xml"" files as mentioned in HDDS-2250. But I think we'd better solve it from the server-side.

I have tried to add maven-shade-plugin in the module of hadoop-ozone-client and hadoop-ozone-common to only merge ""ozone-default-generated.xml"" files, but our own projects can not imply hadoop-hdds-common anymore.

I was thinking if we can change the logic of loading configs. For different modules to generate different config files, so that these files won't causing the conflict while still being able to load from different classpaths.

 ",pull-request-available,[],HDDS,Bug,Major,2021-03-26 10:57:04,7
13367789,Update wiki links in website,"The website has references to Hadoop wiki, from where pages were moved to new space.",pull-request-available,['website'],HDDS,Bug,Minor,2021-03-26 08:03:13,1
13367774,SCM may not be able to know full port list of Datanode after Datanode is started.,"Please check attachment.

After restart DN, the SCM may not know the full ports of that DN.

This issue can not be solved without restart SCM. The consequence is that Datanode can not participate any pipeline, and there will be continually NPE in DN.
{code:java}
2021-03-25 15:04:16,322 [Command processor thread] ERROR org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine: Critical Error : Command processor thread encountered an error. Thread: Thread[Command processor thread,5,main]
java.lang.NullPointerException
        at org.apache.hadoop.hdds.ratis.RatisHelper.toRaftPeerAddress(RatisHelper.java:99)
        at org.apache.hadoop.hdds.ratis.RatisHelper.raftPeerBuilderFor(RatisHelper.java:119)
        at org.apache.hadoop.hdds.ratis.RatisHelper.toRaftPeer(RatisHelper.java:111)
        at org.apache.hadoop.hdds.ratis.RatisHelper.newRaftGroup(RatisHelper.java:149)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:91)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:506)
        at java.lang.Thread.run(Thread.java:748)
2021-03-25 15:04:16,323 [Command processor thread] ERROR org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine: Critical Error : Command processor thread encountered an error. Thread: Thread[Command processor thread,5,main]
java.lang.NullPointerException
        at org.apache.hadoop.hdds.ratis.RatisHelper.toRaftPeerAddress(RatisHelper.java:99)
        at org.apache.hadoop.hdds.ratis.RatisHelper.raftPeerBuilderFor(RatisHelper.java:119)
        at org.apache.hadoop.hdds.ratis.RatisHelper.toRaftPeer(RatisHelper.java:111)
        at org.apache.hadoop.hdds.ratis.RatisHelper.newRaftGroup(RatisHelper.java:149)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:91)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:506)
        at java.lang.Thread.run(Thread.java:748)

{code}
 

After restart SCM, this issue gone.

The issue should be: SCMNodeManager just record the DatanodeDetails once during register.

But for DN, it won’t record the admin, server, client port into DatanodeDetails until its ratis server is up.

Thus there is contention here: if the register request is reported before ratis server is up, SCM won’t know full port list of that DN.

 

 *UPDATE*
{code:java}
public void start(String clusterId) throws IOException {
  if (!isStarted.compareAndSet(false, true)) {
    LOG.info(""Ignore. OzoneContainer already started."");
    return;
  }
  LOG.info(""Attempting to start container services."");
  startContainerScrub();

  replicationServer.start();
  datanodeDetails.setPort(Name.REPLICATION, replicationServer.getPort());

  writeChannel.start();
  readChannel.start();
  hddsDispatcher.init();
  hddsDispatcher.setClusterId(clusterId);
  blockDeletingService.start();
}
{code}
We are doing SCM HA test, which means the start will called multi times, and only the first SCM connection will succeed in the CAS. The second SCM connection will won't wait for writeChannel.start(); thus get a partial port list.

 

*UPDATE again*

It is the contention of connect to multi SCMs at DN side. We also needs add lock to DatanodeDetails.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-26 05:26:43,36
13367762,DN stopped to load containers on volume after a container load exception,"We have met two cases of container loading exceptions, one case is fixed by HDDS-4722 which throws out Runtime Exception, another case is I backuped a container dirctory using name ContainerID-Backup which triggers bad formated container directory name exception. 

The consequence of these two cases are the massive containers lefting on the same volume are not loaded. While DN is started and running healthly,  SCM treats all these container replicas as missing and starts to schedule many replica replication tasks. 

This task is to fix the issue. If there is specific container loading exception, LOG it, and go to load next container. 

Case 1:
2021-03-12 20:46:16,420 [Thread-8] ERROR org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader: Caught a Run time exception during reading container files from Volume /data3/hdds/hdds {}
java.lang.NumberFormatException: For input string: ""1823-raw""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:589)
        at java.lang.Long.parseLong(Long.java:631)
        at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.getContainerID(ContainerUtils.java:242)
        at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.getContainerFile(ContainerUtils.java:234)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:132)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:91)
        at java.lang.Thread.run(Thread.java:748)


Case2:
2021-03-25 10:15:47,502 [Thread-15] ERROR org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader: Caught a Run time exception during reading container files from Volume /data5/hdds/hdds {}
org.apache.hadoop.metrics2.MetricsException: Metrics source RDBMetrics already exists!
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
        at org.apache.hadoop.hdds.utils.db.RDBMetrics.create(RDBMetrics.java:47)
        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:152)
        at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:191)
        at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:128)
        at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:103)
        at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaOneImpl.<init>(DatanodeStoreSchemaOneImpl.java:40)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getUncachedDatanodeStore(BlockUtils.java:68)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getUncachedDatanodeStore(BlockUtils.java:93)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:195)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:181)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:158)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:136)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:91)
        at java.lang.Thread.run(Thread.java:748)
",pull-request-available,[],HDDS,Bug,Critical,2021-03-26 03:52:42,5
13367276,[SCM HA Security] Handle leader changes during bootstrap,https://github.com/apache/ozone/pull/2000#discussion_r599427607,pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-24 15:26:32,13
13367226,Cancel failing PR workflow runs,"Similar to HDDS-4988, we should also cancel PR builds that are already failing.  This is an improvement over HDDS-4933, where only concurrent builds of the same check could be cancelled.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-24 12:40:32,1
13367178,Upgrade Async Profiler,Async Profiler default output format is being changed in HDDS-5009 to one not supported by version 1.5.  We need to bump the version being installed in the image.,pull-request-available,['docker'],HDDS,Task,Minor,2021-03-24 08:16:50,1
13367131,Remove empty share/hadoop directory in dist package,"There is no jars under share/hadoop directory in the final dist package. This task is to remove the hadoop directory and clean up the package. 


{noformat}
share
|-- hadoop
|   |-- hdds
|   `-- ozone
`-- ozone
{noformat}
",pull-request-available,[],HDDS,Improvement,Major,2021-03-24 03:24:18,5
13367097,Datanodes should always use MLV 0 when no VERSION file is present,"Since older versions of the code did not have a VERSION file for datanodes, use the presence of the datanode.id file to determine whether the datanode is being started on a fresh install and should use the latest layout version, or being started as part of an upgrade and should use the initial layout version.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-23 20:58:41,22
13367046,SCM get roles command should provide Ratis Leader/Follower information.,*_ozone admin scm roles_*  provides only the list of SCM nodes part of the HA setup. It would be good to provide the Ratis role information like the OM role command does. ,pull-request-available,[],HDDS,Sub-task,Major,2021-03-23 16:13:42,45
13366982,Bump ratis version to 2.0.0,Please see: https://github.com/apache/ozone/pull/2076,pull-request-available,[],HDDS,Improvement,Blocker,2021-03-23 13:05:32,6
13366944,Inconsistent tmp bucket name on different nodes when Kerberos is enabled,"I setup yarn cluster on ozone, and kerberos is enable. NodeManager can't download resource package in ContainerLocalizer stage, show error like this

{code}
2021-03-22 19:56:42,004 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: \{ ofs://test1/tmp/hadoop-yarn/staging/$user/.staging/job_1616412423610_0008/job.splitmetainfo, 1616414199202, FILE, null } failed: hadoop-yarn/staging/$user/.staging/job_1616412423610_0008/job.splitmetainfo: No such file or directory!
java.io.FileNotFoundException: hadoop-yarn/staging/$user/.staging/job_1616412423610_0008/job.splitmetainfo: No such file or directory!
{code} 

The reason is that this wrong way to get user. For kerberos principal in different machine are not same, so UserGroupInformation.getCurrentUser().getUserName() are not same. Because kerberos principal's formate is ""user/host@HADOOP.COM""
 ",pull-request-available,[],HDDS,Bug,Critical,2021-03-23 10:08:16,46
13366918,[FSO] Add robot tests for new Filesystem layout ,Add robot tests that work with new FS Layout version V1.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-23 08:20:01,26
13366877,localId is not consistent across SCMs when setup a multi node SCM HA cluster.,"We set up the three node SCM HA cluster for test purpose.

From ozone dbug ldb tool, we found that the localIDs are not same between the three SCM. The reason is due to localID, which is initialized based on each machines own timestamp. 

The ldb result fetch from scm.db on 3 SCMs. 

*scm1*

17000 END 
 8000 END 
 105898712280731336 END

*scm2*

17000 END
 8000 END
 105898723592162080 END

*scm3*

17000 END
 8000 END
 105898724336720504 END",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-23 04:19:45,36
13366766,Upload upgrade design documentation to docs module.,"Documentation on upgrade design, how to perform an upgrade & upgrade framework developer primer.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-22 16:09:57,30
13366713,Introduce EC ReplicationConfig and Java based ReplicationConfig implementation,"SCM proto file should be extended to use ECReplicationConfig which can be de-serialized to a specific ReplicationConfiguration.

Note: this is the bare minimum version of HDDS-4882 which doesn't include the rafactor of the existing proto/persistent fields but de-/serialize them to the new java pojos.
 ",pull-request-available,[],HDDS,Sub-task,Major,2021-03-22 12:08:27,6
13366688,PrometheusMetricsSink TYPE is duplicated for multi-dimension metrics,"Currently, all the internal metrics exposed via the *PrometheusMetricsSink* has their own *# TYPE* definition even the metrics are repeatedly appearing in the endpoint due to different labels, e.g.

 
{code:java}
# TYPE hdds_dispatcher_counter counter
hdds_dispatcher_counter{type=""PutBlock"",hostname=""host1""} 0
...
# TYPE hdds_dispatcher_counter counter
hdds_dispatcher_counter{type=""ReadContainer"",hostname=""host1""} 0
{code}
 

Source of this:

[https://github.com/apache/ozone/blob/9fca2a788ca0ad39bbf8abb3af2a4f733cf8de16/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/PrometheusMetricsSink.java#L68]

 

The expected output should be categorized like this, # TYPE:

[https://prometheus.io/docs/guides/multi-target-exporter/#basic-querying-of-multi-target-exporters]

 
{code:java}
# TYPE hdds_dispatcher_counter counter
hdds_dispatcher_counter{type=""PutBlock"",hostname=""host1""} 0
hdds_dispatcher_counter{type=""ReadContainer"",hostname=""host1""} 0
...{code}
 

In case we are not categorizing the same metrics with different labels under the same # TYPE definition it could make decoders fail, like:

[https://github.com/prometheus/common/blob/90d71d7138448baa26007aa224ee917eea49f897/expfmt/text_parse.go#L511]
{code:java}
second TYPE line for metric name ""hdds_dispatcher_counter"", or TYPE reported after samples
{code}
Additionally, it's not advisable to use type as a label but this is not an issue from the decoder POV:

[https://prometheus.io/docs/instrumenting/writing_exporters/#labels]

 ",pull-request-available,[],HDDS,Bug,Major,2021-03-22 09:35:26,47
13366678,Set the default output format based on the newest version of async profiler,"Set the default output format based on the newest version of async profiler, and provide user version instructions in the comments. ([Version content replacement URL|https://github.com/jvm-profiling-tools/async-profiler/releases])",pull-request-available,[],HDDS,Improvement,Major,2021-03-22 08:40:59,17
13366617,Intellij run configuration for ozonefs shell,"As per #ozone slack channel discussion on intellij run configuration devised for executing ozonefs shell commands - patch to be made to add to
{code:java}
hadoop-ozone/dev-support/intellij/runConfigurations{code}
 

 ",pull-request-available,[],HDDS,New Feature,Minor,2021-03-22 02:31:11,39
13366575,Datanode didn't show detail error when Ratis storage directory is not defined,"Datanodes use ""ozone.metadata.dirs"" as default Ratis storage directory, when the directory fails to be created, Datanode will exit without any errors shown in log file. The output is as follows:

  !Screenshot 2021-03-21 at 10.04.52 PM.png|width=805,height=142!

The reason is that when Datanode got exception starting, the error messages will be printed in output_file, it will be hard for users to find the error message since we usually expect the messages to exist in log_file.",pull-request-available,[],HDDS,Improvement,Major,2021-03-21 13:58:46,7
13366400,Multipart Upload fails due to partName mismatch,"We're running the official Ozone 1.0.0 release and facing S3 Multipart Upload failures with large files. The error message looks similar to that is reported in HDDS-3554 but we'd like to report what we've found so far to help the further investigation of this issue.
h1. The error message recorded in OM log

Please find the following error message excerpted from our OM. Forgive us we redacted some sensitive information such as username and keyname which imply our project's topic.
{code:java}
2021-03-14 07:48:41,947 [IPC Server handler 88 on default port 9862] ERROR org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: <REDACTED_KEYNAME> in Volume/Bucket s3v/<BUCKETNAME>
INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: <BUCKETNAME> key: <REDACTED_KEYNAME>. Provided Part info is { /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282, 4}, whereas OM has partName /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406
        at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:199)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:224)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:145)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)
        at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915){code}
Anyway, OM thinks the partName for the partNumber 4 is /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406 but COMPLETE_MULTIPART_UPLOAD request think it must be /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282. This discrepancy is the immediate cause for this error.
h1. OM audit log says both are correct

Please find attached om-audit-HOSTNAME-2021-03-14-19-53-09-1.log.gz (also redacted, sorry), it contains filtered output of our OM audit log, the lines which include <REDACTED_KEYNAME> and multipartList entry are remain.

Interestingly, according to the OM audit log, there're two COMMIT_MULTIPART_UPLOAD_PARTKEY operations exist for partNumber=4 and both operations were succeeded:

 
{code:java}
% zgrep partNumber=4, om-audit-HOSTNAME-2021-03-14-19-53-09-1.log.gz
2021-03-14 07:16:04,992 | INFO  | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=8388608, replicationType=RATIS, replicationFactor=ONE, partNumber=4, partName=/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282} | ret=SUCCESS | 
2021-03-14 07:18:11,828 | INFO  | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=8388608, replicationType=RATIS, replicationFactor=ONE, partNumber=4, partName=/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406} | ret=SUCCESS | 
%
{code}
 

OM seemed to have accepted both partName ending with 105884795658268282 and  105884791629180406 for partNumber 4. And COMPLETE_MULTIPART_UPLOAD operation was called with the prior partName but OM believed it had the latter partName for partNumber 4.

 
{code:java}
2021-03-14 07:48:41,947 | ERROR | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMPLETE_MULTIPART_UPLOAD {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=0, replicationType=
RATIS, replicationFactor=ONE, multipartList=[partNumber: 1
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791631605244""
, partNumber: 2
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791631539707""
, partNumber: 3
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791628262900""
, partNumber: 4
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282""
, partNumber: 5
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629245944""
, partNumber: 6
partName: ""/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629245943""
{code}
We can also find there're multiple COMMIT_MULTIPART_UPLOAD_PARTKEY operations for several partNumbers, such as partNumber 4, 13, 20, 45, 57, 67, 73, ... some partNumbers like 172 have more than three COMMIT_MULTIPART_UPLOAD_PARTKEY operations they're all succeeded.

 
h1. How to solve this issue?

At first we thought this issue is caused by race condition, but noticed that there're enough time between each COMMIT_MULTIPART_UPLOAD_PARKEY operation. We're not sure but noticed that write operations to OmMetadataManager are isolated with omMetadataManager.getLock().acquireWriteLock(BUCKET_LOCK, volumeName, bucketName);

 
{code:java}
     multipartKey = omMetadataManager.getMultipartKey(volumeName,
         bucketName, keyName, uploadID);

     // TODO to support S3 ACL later.

     acquiredLock = omMetadataManager.getLock().acquireWriteLock(BUCKET_LOCK,
         volumeName, bucketName);

     validateBucketAndVolume(omMetadataManager, volumeName, bucketName);

     String ozoneKey = omMetadataManager.getOzoneKey(
         volumeName, bucketName, keyName);

     OmMultipartKeyInfo multipartKeyInfo = omMetadataManager
         .getMultipartInfoTable().get(multipartKey);
{code}
 

So our question is, is it normal to have multiple COMMIT_MULTIPART_UPLOAD_PARTKEY operations for a partNumber, with different partNames?
h1. Other findings

This issue occurs less frequently with aws configure set default.s3.multipart_chunksize 256MB. Almost always fails with multipart_chunksize 8MB, 1GB in our environment.

 

 ",pull-request-available,"['OM', 'S3']",HDDS,Bug,Major,2021-03-19 17:00:16,13
13366337,Upgrade Jersey2 dependency,Upgrade Jersey 2.27 to 2.32+.,pull-request-available,[],HDDS,Task,Major,2021-03-19 12:11:49,1
13366252,[SCM HA Security] Make storeValidCertificate method idempotent,This Jira is to make storeValidCertificate idempotent so that during replay it does not cause any issues.,pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-19 05:47:00,13
13366176,Upgrade kotlin-stdlib,"Upgrade {{kotlin-stdlib}}, transitive dependency via {{jaeger-client}}, to 1.4.21+.",pull-request-available,['build'],HDDS,Task,Major,2021-03-18 20:28:59,1
13366149,Skip CI for draft pull requests,"Github pull requests can be created as (or converted to) draft.  This is useful for gathering early feedback from other contributors about proof-of-concept code.  Such code may not even compile, and style issues are usually ignored.

Since further commits are generally expected in this case, I think it makes sense to skip CI build/test until the PR is marked as ""ready for review"".

If CI feedback is desired, the author should enable workflows in their fork so we can check push builds there.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-18 17:33:34,1
13365949,Add guardrail for reserved buffer size when DN reads a chunk,"https://github.com/apache/ozone/blob/db3c50d9fc4b1048cd83074343cc00444f0b24f7/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L148

{code}
    long len = info.getLen();
    long offset = info.getOffset();
    ByteBuffer data = ByteBuffer.allocate((int) len);
{code}

DN reserves a byte buffer for client's chunk read request based on whatever client specifies, without check. This is bad. Client can send a message forcing DN to allocate up to 2GB memory per request. (Fortunately the grpc handler captures the OOM exception so DN doesn't crash)

Propose: add a guardrail check. XceiverServerGrpc set max inbound message size as 32MB (OZONE_SCM_CHUNK_MAX_SIZE). We should make sure the requested length is less than this too.",pull-request-available,[],HDDS,Improvement,Major,2021-03-17 21:38:56,1
13365898,SCM should not use pipelines with HEALTHY_READONLY datanodes,"During upgrade testing for HDDS-4181, it was noticed that datanodes were incorrectly using the newest metadata layout version when pre-finalized. However, SCM still came out of safe mode counting these pipelines as healthy, even though the datanodes' MLV was ahead of SCM's. This Jira will make SCM not use pipelines involving HEATLHY_READONLY datanodes.",pull-request-available,[],HDDS,Sub-task,Critical,2021-03-17 17:27:47,22
13365652,Introduce First upgrade startup action and Pre-finalized state validation in Layout Feature.,"* Introduce 2 new phases of upgrade action hooks (per layout feature)
 *Prefinalized state validation* - A layout feature (version) can use this to validate that the component is not started up in a way to use it before finalization. For example, an SCM HA validation action can make sure HA is not enabled before finalization. 
*First Upgrade Start action* - Run exactly once when a component is started up after an upgrade with an unfinalized layout feature.
* Annotation based registration of layout actions to the layout features. After this change, an HDDS upgrade action be created like this.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-16 22:35:21,30
13365631,Decommission CLI should return details of nodes which fail,"With the current decommission / recommission / maintenance mode commands, you can pass a list of hosts to perform the operation on. If any of these hosts fail to enter the decommission / maintenance workflow, the command gives no feedback about the error. Some of the hosts can silently fail and the only way to know is to inspect the SCM log.

The most common way a host can fail, is if a node which is undergoing maintenance is instructed to go to decommission and vice versa as this is a transition which is not allowed.

This change will allow any failed nodes to feed back to the client. If the client detects that any of the nodes have failed, details will be written to stderr and the command exit code will be non-zero.

Note that even though the exit code is non-zero, the command may have partially worked.

Also note that the errors which are fed back are only around transitioning the node into the admin workflow - it is still possible for it to fail later for other reasons which will not be fed back to the client. This is because the client does not wait for the process to complete, but exits after confirmation the command has been processed by scm.",pull-request-available,"['SCM', 'SCM Client']",HDDS,Improvement,Major,2021-03-16 20:39:40,11
13365585,Cancel duplicate PR workflows,"CI runs for pull requests with multiple commits can take up too much resources.  We should cancel previous runs, as only the last one is considered for PR status.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-16 16:27:48,1
13365453,Import container should not delete container contents if container already exists,"KeyValueContainer # importContainerData


{code:java}
if (getContainerFile().exists()) {
        String errorMessage = String.format(
            ""Can't import container (cid=%d) data to a specific location""
                + "" as the container descriptor (%s) has already been exist."",
            getContainerData().getContainerID(),
            getContainerFile().getAbsolutePath());
        throw new IOException(errorMessage);
      }

....

catch (Exception ex) {
      //delete all the temporary data in case of any exception.
      try {
        FileUtils.deleteDirectory(new File(containerData.getMetadataPath()));
        FileUtils.deleteDirectory(new File(containerData.getChunksPath()));
        FileUtils.deleteDirectory(getContainerFile());
      } catch (Exception deleteex) {
        LOG.error(
            ""Can not cleanup destination directories after a container import""
                + "" error (cid"" +
                containerData.getContainerID() + "")"", deleteex);
      }
      throw ex;
{code}


That's one of the reason why DN report empty container directories during startup. 
2021-03-16 16:26:58,337 [Thread-9] ERROR org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader: Missing .container file for ContainerID: 112224
2021-03-16 16:26:58,433 [Thread-9] ERROR org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader: Missing .container file for ContainerID: 11913
2021-03-16 16:26:58,474 [Thread-9] ERROR org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader: Missing .container file for ContainerID: 11823
",pull-request-available,[],HDDS,Bug,Major,2021-03-16 09:10:22,5
13365431,Read failure because of unhealthy container,"Failed to read user data because of container which contains data is of unhealthy state. 

aws CLI response, 
get file 10.121.81.124/0ea3094efb41471c861f25d9c429d9f6.jfr
An error occurred (500) when calling the GetObject operation (reached max retries: 4): Internal Server Error


s3g LOG,
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: The container(1839) replica is unhealthy.
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:530)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.lambda$getValidatorList$0(ContainerProtocolCalls.java:537)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:374)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.lambda$sendCommandWithTraceIDAndRetry$0(XceiverClientGrpc.java:311)
        at org.apache.hadoop.hdds.tracing.TracingUtil.executeInSpan(TracingUtil.java:174)
        at org.apache.hadoop.hdds.tracing.TracingUtil.executeInNewSpan(TracingUtil.java:148)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:305)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:286)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:106)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:213)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:142)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:276)
        at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:199)
        at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:49)
        at java.io.InputStream.read(InputStream.java:101)
        at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2146)
        at org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)
        at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)
        at org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)
        at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.lambda$get$0(ObjectEndpoint.java:278)
        at org.glassfish.jersey.message.internal.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:79)
        at org.glassfish.jersey.message.internal.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:61)
        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.invokeWriteTo(WriterInterceptorExecutor.java:266)
        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.aroundWriteTo(WriterInterceptorExecutor.java:251)
        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:163)
        at org.glassfish.jersey.server.internal.JsonWithPaddingInterceptor.aroundWriteTo(JsonWithPaddingInterceptor.java:109)
        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:163)
        at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundWriteTo(MappableExceptionWrapperInterceptor.java:85)
        ... 62 more


For such containers,  it is in CLOSING state, 

bin/ozone admin container info 1839
Container id: 1839
Pipeline id: a9642446-33c9-42ae-9161-7c3a570cea93
Container State: CLOSING
Datanodes: [32e8d855-b702-438d-b829-ac43dc567afc/tdw-9-180-19-146,
b85962f2-6647-463b-9944-3c9b24e4e313/tdw-9-180-19-148,
07b24dc5-1d5e-4f6e-ad15-e3d89ea06c54/tdw-9-180-19-145]
",pull-request-available,[],HDDS,Bug,Critical,2021-03-16 07:23:07,5
13365428,"[SCM HA Security] When Ratis is enabled, SCM secure cluster is not working","
{code:java}
scm_1       | 2021-03-16 05:39:38,023 [a3c3a50a-98c8-4517-b619-4e0715dc1ef7@group-9A1DAB6BC185-StateMachineUpdater] ERROR statemachine.StateMachine: Terminating with exit status 1: org.apache.hadoop.hdds.scm.server.SCMCertStore.storeValidCertificate(java.math.BigInteger, sun.security.x509.X509CertImpl, org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType)
scm_1       | com.google.protobuf.InvalidProtocolBufferException: org.apache.hadoop.hdds.scm.server.SCMCertStore.storeValidCertificate(java.math.BigInteger, sun.security.x509.X509CertImpl, org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType)
scm_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:164)
scm_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:134)
scm_1       | 	at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1659)
scm_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:232)
scm_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:177)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
{code}
",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-16 07:11:48,13
13365388,Async profiler 2.0 no longer supports svg output,"HDDS-1116 added the servlet for async profiler which outputs a flame graph in svg format.

The async profiler (https://github.com/jvm-profiling-tools/async-profiler) 2.0 no longer supports svg. Instead, html only. Therefore, Ozone can't use the latest async profiler.

File this patch to make a small change to output html format.

Essentially, Ozone's async profiler servlet creates a process of ""/path/to/profiler.sh -e cpu -d 10 -o svg -f /path/to/output.svg pid"" We should make it ""-o html -f /path/to/output.html""",newbie pull-request-available,[],HDDS,Improvement,Major,2021-03-16 03:09:02,17
13365387,"Display key offset for each block in command ""key info""","sh key info ${keyName}

 {
    ""containerID"" : 988,
    ""localID"" : 104074850840994477,
    ""length"" : 5246566,
    ""offset"" : 0
  }

  ->

  {
    ""containerID"" : 988,
    ""localID"" : 104074850840994477,
    ""length"" : 5246566,
    ""offset"" : 0
    ""keyOffset"": 5246566
  }
",pull-request-available,[],HDDS,Improvement,Major,2021-03-16 03:03:19,23
13365383,Solve intellj warnings on DBTransactionBuffer.,There is IntelliJ warnings on DBTransactionBuffer. Check details in attached screenshots.,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-16 02:11:10,48
13365382,Remove SequenceIdGenerator#StateManagerImpl,"After [https://github.com/apache/ozone/pull/1981] is merged, **DBTransactionBuffer supports directly writes to rocksdb, thus SequenceIdGenerator#StateManagerImpl is not needed when ratis is not enabled.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-16 02:06:24,2
13365319,Bump jaeger-client to 1.6.0,"-Bumping jaeger-client to hopefully upgrade the libthrift version-

Apparently latest jaeger-client as of now 1.5.0 still doesn't include libthrift with the CVE fix:
{code}
[INFO] +- io.jaegertracing:jaeger-client:jar:1.2.0:compile
[INFO] |  +- io.jaegertracing:jaeger-thrift:jar:1.2.0:compile
[INFO] |  |  +- org.apache.thrift:libthrift:jar:0.13.0:compile
{code}
not until https://github.com/jaegertracing/jaeger-client-java/pull/768 is merged and a new version released.
",pull-request-available,[],HDDS,Task,Major,2021-03-15 18:28:33,12
13365220,[SCM HA Security] Ozone services should be disabled in SCM HA enabled and security enabled cluster,"SCM HA security work is still in progress.

[~elek] Brought up the point that until before merge of SCM HA branch we should add safeguard check to fail bringing up the cluster",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-15 11:32:13,13
13365124,"Add command in ""ozone debug"" to get the container replica information","It's difficult to find the container directory path when there is dozens of disks mounted on a datanode.  Following information is also very helpful. 


  ""state"" : ""CLOSED"",
  ""replicationFactor"" : ""THREE"",
  ""replicationType"" : ""RATIS"",
  ""usedBytes"" : -33644010,
  ""numberOfKeys"" : -29,
  ""lastUsed"" : ""2021-03-15T07:48:59.990Z"",
  ""stateEnterTime"" : ""1970-01-10T21:27:22.112Z"",
  ""owner"" : ""a46123a8-be63-4736-9478-ce4d8ac845cc"",
  ""containerID"" : 20,
  ""deleteTransactionId"" : 1027524,
  ""sequenceId"" : 284174
",pull-request-available,[],HDDS,Improvement,Major,2021-03-15 04:03:52,21
13365115,Refine missing delete block failure log,"Found a lot of block delete failure LOG in our cluster.  Cann't easily tell which container and block are failed from the LOG.


2021-03-12 21:53:25,844 [BlockDeletingService#5] WARN org.apache.hadoop.hdds.utils.BackgroundService: Background task execution failed
java.lang.NullPointerException: Block data cannot be null.
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:897)
        at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDispatcher.deleteChunks(ChunkManagerDispatcher.java:110)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteBlock(KeyValueHandler.java:1045)
        at org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService$BlockDeletingTask.deleteTransactions(BlockDeletingService.java:445)
        at org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService$BlockDeletingTask.deleteViaSchema2(BlockDeletingService.java:401)
        at org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService$BlockDeletingTask.call(BlockDeletingService.java:268)
        at org.apache.hadoop.hdds.utils.BackgroundService$PeriodicalTask.lambda$run$0(BackgroundService.java:112)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)


After patch, this is the log output,

2021-03-16 16:27:21,126 [BlockDeletingService#5] WARN org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService: Missing delete block(Container = 410375, Block = 105625340688072823
2021-03-16 16:27:21,126 [BlockDeletingService#7] WARN org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService: Missing delete block(Container = 410313, Block = 105625268260438073
",pull-request-available,[],HDDS,Improvement,Major,2021-03-15 03:04:04,5
13364694,[FSO] Missed to cleanup new FileTables in OMRequests,"This task is to cleanup New FileTables when the request is not successful in the following requests:
{code:java}
-> OMAllocateBlockRequestV1,
-> OMFileCreateRequestV1,
-> OMKeyCreateRequestV1,
-> OMKeyRenameRequestV1,
-> S3InitiateMultipartUploadRequestV1{code}
For example, [OMKeyCreateRequestV1.java#L216|https://github.com/apache/ozone/blob/HDDS-2939/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyCreateRequestV1.java#L216] is invoking OMKeyCreateResponse, where it is cleaning up OPEN_KEY_TABLE, KEY_TABLE tables. Actually, it should cleanup the new tables - DIRECTORY_TABLE, OPEN_FILE_TABLE and FILE_TABLE.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-12 16:58:48,40
13364055,Fix typo in SCMConnectionManager.java,"org/apache/hadoop/ozone/container/common/statemachine/SCMConnectionManager.java:132
org/apache/hadoop/ozone/container/common/statemachine/SCMConnectionManager.java:223
{code:java}
   * @param address - Address of the SCM machine to send heatbeat to.
{code}

heatbeat -> hea{color:red}r{color}tbeat",pull-request-available,[],HDDS,Improvement,Trivial,2021-03-12 08:37:04,17
13364043,Define Ozone specific trash-interval and checkpoint interval,Introduce ozone specific trash-interval and trash-checkpoint-interval. ,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-03-12 07:56:32,26
13363816,Extract check dependency installation from Github Actions workflow,"Add steps to check scripts for installing dependencies (eg. {{bats}} or {{spotbugs}}). This reduces complexity of Github-specific code in the workflow definition, and allows simpler prototyping/testing.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-11 10:55:58,1
13363815,Back-port HDDS-4911 (List container by container state) to ContainerManagerV2,"We need to back-port [HDDS-4911|https://github.com/apache/ozone/pull/2001] to ContainerManagerV2, before merging HDDS-2823 back to master. Otherwise CI will fail due to the test introduced by [HDDS-4911|https://github.com/apache/ozone/pull/2001].",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-11 10:51:49,2
13363648,NoSuchMethodException when wrapping RpcException on downgrade,"After downgrade from a pre-finalized cluster, the following exception appears multiple times in the datanode logs:
{code:java}
dn1_1    | 2021-03-01 23:17:58 WARN  NetUtils:836 - Unable to wrap exception of type class org.apache.hadoop.ipc.RpcException: it has no (String) constructor
dn1_1    | java.lang.NoSuchMethodException: org.apache.hadoop.ipc.RpcException.<init>(java.lang.String)
dn1_1    | 	at java.base/java.lang.Class.getConstructor0(Class.java:3349)
dn1_1    | 	at java.base/java.lang.Class.getConstructor(Class.java:2151)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:832)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:808)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:834)
{code}
It does not seem to cause adverse behavior in the cluster.",pull-request-available,[],HDDS,Sub-task,Minor,2021-03-10 20:38:33,17
13363613,Fix flaky test TestSCMInstallSnapshotWithHA#testInstallCorruptedCheckpointFailure,"The test assumes that a follower node must be behind the leader during the final validation assertions. Fix is to not start the follower server itself , so that it always lags behind the leader at any point of time.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 17:01:46,16
13363562,[SCM HA Security] Make CertStore DB updates for StoreValidateCertificate go via Ratis,"This Jira is to make DB updates of CertStore go via ratis, so that all SCM's can be in sync.

In this way, OM/DN/SCM Certs will be in sync across Ratis.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-10 14:12:15,13
13363560,[SCM HA Security] Implement listCAs and getRootCA API,"This Jira is to implement an API that lists all CA's of SCM nodes along with RootCA.

*Example output returned by this new API:*
SCM1 CA
SCM2 CA
SCM3 CA
SCM1 RootCA

And to implement getRootCA which returns the root CA which has signed the certificate for other SCMs.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-10 14:10:04,13
13363554,Return with exit code 0 in case of optional scm bootstrap/init ,"HDDS-4896 provides a very elegant way to initialize SCM HA in containerized environment:

Both `--init` and `--bootstrap` can be executed, but when `ozone.scm.primordial.node.id` is set, the init is ignored on non-primordial, the bootstrap ignored on primordial nodes.

To make it even easier to use in k8s environments I suggest to return with exit code 0 in these cases as init proces shouldn't be repeated in these cases ignoring the init/bootstrap is part of the expected workflow.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 13:37:22,6
13363550,Provide example k8s files to run full HA Ozone,We already have Kubernetes examples for Ozone cluster to show how HA can be supported in Kubernetes environment seems to be good idea to provide example for full HA Ozone cluster.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 13:28:44,6
13363540,NullPointerException during SCM init,"
{code:java}
scm1_1       | java.lang.NullPointerException
scm1_1       | 	at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.close(SCMStateMachine.java:297)
scm1_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.stop(StateMachineUpdater.java:130)
scm1_1       | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:184)
scm1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
{code}

This is due to during SCM init, RatisServer is started with StateMachine initialized false.
",pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 12:36:33,13
13363445,[Doc] Add SCM HA Setup Doc,we need to display how to set up SCM HA.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-10 07:41:29,6
13363371,Reuse compiled binaries in combined coverage calculation,"Combined coverage calculation consists of:

# download coverage files created by separate checks
# merge coverage files
# full build
# extract classes from jars
# produce jacoco report

Instead of the full build, we could reuse the binaries created in _compile_ check, to save some time.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-09 20:58:30,1
13363367,Do not wait one heartbeat to move newly registered datanodes that match SCM's MLV from HEALTHY_READONLY to HEALTHY,"Currently, when a datanode is registered with SCM, it is added as HEALTHY_READONLY, regardless of its metadata layout version. On the next heartbeat, the layout is checked against that of SCM's to determine if it matches and can be moved to HEALTHY. This Jira will add the check for layout match when the datanode is registered, as well as on each heartbeat, so nodes whose MLV matches SCM's on registration will move directly from HEALTHY_READONLY to HEALTHY without needing to wait for a heartbeat.",pull-request-available,[],HDDS,Sub-task,Major,2021-03-09 20:41:21,22
13363361,Multi-Tenant Support in Ozone,This Jira will be used to track a new feature for Multi-Tenant support in Ozone. Initially Multi-Tenant feature would be limited to ozone-users accessing Ozone over S3 interface.,pull-request-available,"['Ozone CLI', 'Ozone Datanode', 'Ozone Manager', 'S3', 'SCM', 'Security']",HDDS,New Feature,Major,2021-03-09 20:01:16,44
13363282,Provide testkrb5 image for faster ozonesecure tests,Please see: https://github.com/apache/ozone-docker-testkrb5/pull/1,pull-request-available,[],HDDS,Improvement,Major,2021-03-09 14:42:35,6
13363276,Enhance SCMServerProtocol with using ReplicationConfig,"In HDDS-4882 a new ReplicationConfig is introduced. This patch shows how can it be used between OM and SCM on the protocol.

This patch is not a full refactor of SCM it focuses on the SCM protocol side only. Pipeline manager can be improved in follow-up patches...",pull-request-available,[],HDDS,Sub-task,Major,2021-03-09 14:10:58,6
13363225,Make CI checks fail faster,"Currently most CI checks run to completion even if other checks fail.  This is useful if we want to find as many problems as possible in a single CI run.  However, it is a waste of resources when trying to get a clean run before merge.

I propose to make checks fail faster for PR runs.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-09 09:04:34,1
13363224,[FSO] Provide list subpaths function to perform recursive ACL check during delete and rename op,This task is to provide list subpaths interface to perform recursive ACL check during delete and rename operation.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-03-09 08:52:10,40
13363201,Select target datanodes and containers to move for Container Balancer,"Once container balancer determines the over and under utilised datanodes, it should select the containers which should be moved and schedule move for the same. It should also select the appropriate datanode where the selected container can be moved.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-03-09 07:53:35,49
13363198,Support container move in Replication Manager,Container balancer needs to move the containers from over-utilised to under-utilised datanodes. This requires support for container move in Replication Manager so that after a container is replicated to an under-utilised datanode it can be deleted from the over-utilised datanode.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-03-09 07:46:03,2
13363197,Determine over and under utilized datanodes in Container Balancer,Container Balancer needs to determine the over and under utilised datanodes in order to balance the containers. Jira would provide the corresponding implementation.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-03-09 07:42:10,49
13363196,Support start/stop for container balancer via command line,Admin should be able to start/stop the container balancer via command line. The jira aims to implement this support. In case of start via command line the configuration should be reloaded from the disk in order for configuration changes to take effect.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-03-09 07:36:25,2
13363194,Introduce ContainerBalancer in SCM with start/stop capabilities.,"Container Balancer would be a separate service inside SCM like SCMNodeManager, PipelineManager etc. The jira aims to add the service along with start/stop capabilities. It should not be started when SCM starts. The start/stop support via command line will be added in a separate jira.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-03-09 07:32:21,49
13363175,[FSO]S3Multipart: Implement OzoneBucket#listParts,"This task is to implement prefix based FSO for the {{OzoneBucket#listParts}} API.

*Note:* Internally, OM DB partName key format is ""{{<parentID>/partFileName + ClientID""}}. But OM needs to return {{fullKeyPartName}} in {{OzoneMultipartUploadPartListParts.PartInfo.getPartName().}}

For example, user give keyName is {{a/b/c/d/multipartKeyName}} and assume objectID for the parent {{a/b/c/d }} is 1027 and clientID is {{4445890}}.

Now, in DB it stores like 1027/{{multipartKeyName4445890.}}

User expects partName like, {{/volName/bucketName/a/b/c/d/multipartKeyName4445890}}",pull-request-available,[],HDDS,Sub-task,Major,2021-03-09 06:20:59,40
13363174,[FSO]S3Multipart: Run S3 acceptance test with prefix layout,"This task is to run s3 acceptance test with PREFIX layout.

Presently, s3 acceptance is only running against {{unnormalized}} way with _ozone.om.enable.filesystem.paths=false_ and it requires to add separate task to run it against {{normalized}} path _ozone.om.enable.filesystem.paths=true_ and _ozone.om.metadata.layout=PREFIX_",pull-request-available,[],HDDS,Sub-task,Major,2021-03-09 06:12:24,40
13363139,Replace the usage of deprecated Junit#timeout() in Ozone.,This ticket is opened to replace the deprecated usage of Timeout(int millis) to  Timeout.seconds(). ,pull-request-available,[],HDDS,Test,Major,2021-03-09 01:37:52,28
13363054,Rename Apache Hadoop Ozone to Apache Ozone in pom and markdown files,Please see: https://github.com/apache/ozone/pull/2005,pull-request-available,[],HDDS,Improvement,Major,2021-03-08 16:09:12,6
13362961,[FSO]Implement ACL requests for new layout,"Implement getAcl , AddAcl, setAcl and removeAcl for new layout V1",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-03-08 09:34:17,26
13362937,[SCM HA Security] Integrate CertClient,"*This Jira is to implement*
1. Use RootCertificate server to issue certs for SCM
2. Use scmCertificatServer to issue certs for DN/OM. (This cert server got certs from RootCertificate Server)
3. Start RootCertificate server only on primary SCM.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-08 07:49:30,13
13362702,Refine the native authorizer parent context right check,"Current we map CREATE/DELETE to parent WRITE. All the other are just 1:1 map from child to parent. 

This may not work, e.g., child WRITE_ACL does not equal to parent WRITE_ACL

Here is the proposed new mapping:
    // Refined the parent context
    // OP         |CHILD     |PARENT

    // CREATE      NONE         WRITE
    // DELETE      DELETE       WRITE
    // WRITE       WRITE        WRITE
    // WRITE_ACL   WRITE_ACL    WRITE     (V1 WRITE_ACL=>WRITE)

    // READ        READ         READ
    // LIST        LIST         READ      (V1 LIST=>READ)
    // READ_ACL    READ_ACL     READ      (V1 READ_ACL=>READ)",pull-request-available,[],HDDS,Sub-task,Major,2021-03-05 20:52:57,28
13362485,List container by container state,"bin/ozone admin container info 1823Container id: 1823
Pipeline id: fd18e250-753a-49a3-868f-1795d612225f
Container State: CLOSING
Datanodes: [32e8d855-b702-438d-b829-ac43dc567afc/tdw-9-180-19-146,
b85962f2-6647-463b-9944-3c9b24e4e313/tdw-9-180-19-148,
07b24dc5-1d5e-4f6e-ad15-e3d89ea06c54/tdw-9-180-19-145]",pull-request-available,[],HDDS,Improvement,Major,2021-03-05 03:57:58,5
13362471,Cp command not working on Ozone ofs mounted using hdfs-fuse or hcfsfuse.	,"The question is same to: HDDS-3492

Now,  we are able to support the cp operation of FUSE using o3fs via HDDS-3492. But ofs does not support, and so increase the PosixRootedOzoneFileSystem to make ofs support FUSE cp.",pull-request-available,['OFS'],HDDS,Bug,Major,2021-03-05 03:08:44,4
13362440,Layout version should be available in DB for an un-finalized OM to be finalized through a Ratis snapshot.,"Although OM Finalization is a Ratis request, it updates a persistent location outside the OM state machine maintained ""RocksDB"". Hence, in a rare case where an unfinalized OM needs a Ratis snapshot to finalize (without any logs), the VERSION file will not be updated, and hence the OM will be stuck in that state.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-03-05 00:25:00,30
13362405,Remove mention of CSI support,"The Ozone website prominently mentions CSI support:

[https://ozone.apache.org/docs/1.0.0/]

[https://ozone.apache.org/docs/1.0.0/interface/csi.html]

Our docs give a false impression to users that CSI is fully functional and supported for persistent storage inside containers.

This support uses goofys+S3 gateway, so it is not appropriate for any serious usage. A real CSI solution should use an approach like the cBlocks prototype by building directly on top of HDDS containers with a real device driver.

Until that time we should not claim CSI support. Alternatively we should be honest with our users that it is a prototype and not suitable for serious usage.",pull-request-available,[],HDDS,Improvement,Critical,2021-03-04 20:05:26,6
13362390,Add Layout version information to Recon datanode info API.,Simple change to add Layout version information to Recon datanode info API.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2021-03-04 18:54:07,30
13362268,ACL is not inherited on multiupload object,"Reproduce steps,
1. create a bucket, assign multiple ACLs on bucket
2. upload an object through multi-upload
3. check object's ACL. It only has ACL for the account who uploaded the object. ",pull-request-available,[],HDDS,Bug,Major,2021-03-04 10:04:51,5
13362079,[SCM HA Security]  Create SCM Cert Client and change DefaultCA to allow self signed and intermediary,"This Jira is to implement
1. Create CertClient, which generates a public key, private key, and generates CSR with ClusterID, SCMID. 
2. Modify DefaultCA Server to work in 2 modes, SELF_SIGNED_CA and INTERMEDIARY_CA.
3. Modify SCMStorageConfig to persist SCM cert serial ID.


",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-03-03 11:32:27,13
13362058,Need a tool to upgrade current non-HA SCM node to single node HA cluster,"Need a tool to upgrade current non-HA SCM node to single node HA cluster

cc [~shashikant]",pull-request-available,[],HDDS,Sub-task,Major,2021-03-03 09:25:35,16
13361993,Use PipelineManagerV2Impl in Recon and enable ignored Recon test cases.,"Use PipelineManagerV2Impl in Recon

Enable ignored Recon test cases.
{code:java}
hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/ContainerEndpoint.java
     // TODO: Fix ME, This has to be ReconContainerManager

hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/ClusterStateEndpoint.java
     // TODO: Fix ME, This has to be ReconContainerManager

hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/fsck/TestContainerHealthTask.java
    testRun

hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/scm/TestReconIncrementalContainerReportHandler.java
    testProcessICR
    testProcessICRStateMismatch

hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/recon/TestReconAsPassiveScm.java
    testDatanodeRegistrationAndReports
    testReconRestart
{code}
{{}}",pull-request-available,[],HDDS,Sub-task,Major,2021-03-03 02:48:52,36
13361940,EC: Implement basic EC pipeline provider,"Currently we have the interface for creating the pripelines provider, that is PipelineProvider
We also have implementation of RatisPipeLineProvider, SimplePipelineProvider (Standalone), ReconPipelineProvider.

So, EC we need another pipeline provider which should pick the require number of DNs based on the requested EC replication.

So, to start with I propose to create simple ECPipelineProvider to handle this logic.
",pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-03-02 19:15:56,11
13361860,Avoid latest/master in Github Actions,We should avoid versions {{latest}} (for runners) and {{master}} (for actions) in Github Actions workflows.,pull-request-available,['build'],HDDS,Improvement,Minor,2021-03-02 12:33:41,1
13361844,SCM Ratis enable/disable switch ,"With SCM HA coming in, SCM would be able to operate with/without ratis. The aim of the Jira is to ensure the existing tests work with both Ratis enabled and disabled on SCM.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-02 12:05:34,16
13361833,Add simple CI check for docs,"The goal of this task is to add a separate check for building {{hadoop-hdds/docs}} with Hugo.

Benefits:

# separation of concerns: Currently docs are implicitly built in _integration_ checks.  If there is a syntax error in the docs, all of these fail due to the same cause.  (Used to be built in _findbugs_, _unit_, too, but these no longer have Hugo available.)
# better error reporting: The above checks do not collect compile errors in their summaries.  Thus one needs to dig into build logs to find any such problem.
",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-02 10:50:39,1
13361821,Merge basic CI checks,"Some of the CI checks have code duplication, which could be reduced by using matrix build.",pull-request-available,['build'],HDDS,Improvement,Major,2021-03-02 10:21:58,1
13361688,Useless message about node schema location,"SCM (and Recon) log this message during startup:

{code}
INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@6aa3bfc
{code}",pull-request-available,['SCM'],HDDS,Bug,Minor,2021-03-01 20:12:54,1
13361656,Fix and enable TestEndpoints.java,Fix and enable TestEndpoints.java,pull-request-available,[],HDDS,Sub-task,Major,2021-03-01 17:43:14,18
13361645,ozone admin datanode list filter by UUID broken,"{{ozone admin datanode list}} filter by UUID is broken:

{code}
$ ozone admin datanode list
Datanode: 8b7a289e-3449-49c7-a9d9-0dae7751559d (/rack2/10.5.0.8/ozone-topology_datanode_5_1.ozone-topology_net/3 pipelines)
...
$ ozone admin datanode list --id 8b7a289e-3449-49c7-a9d9-0dae7751559d
$
{code}",pull-request-available,['Ozone CLI'],HDDS,Bug,Minor,2021-03-01 16:22:34,1
13361643,Fix and enable TestReconTasks,Fix and enable TestReconTasks,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-03-01 15:34:55,18
13361546,Pre-install awscli in ozone-runner,awscli is installed for S3 tests at runtime.  It could be pre-installed in the ozone-runner docker image to save some test execution time.,pull-request-available,['docker'],HDDS,Improvement,Minor,2021-03-01 08:37:37,1
13361526,Fix removing local SCM when submitting request to other SCM.,This Jira is to fix an issue in removing self ID when building proxy objects to submit requests.,pull-request-available,[],HDDS,Sub-task,Major,2021-03-01 06:58:33,13
13361474,Make SCM ratis server spin up time during initialization configurable,"Currently , during scm --init, the ratis server is instantiated and there is wait for it to elect a leader and be ready. This interval is hardcoded for now. The idea is to make it configurable.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-28 22:29:05,2
13361423,[SCM HA Security] Add failover proxy to SCM Security Server Protocol,"This Jira is to add support for FailOverProxyProvider for SCMSecurityServer which is used by OM and Datanode. (In further jira's when security work is implemented, this API will be used by SCM also)",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-02-28 09:04:12,13
13361159, [SCM HA Security] Implement listCertificates based on role,"This Jira is to implement list certificates based on role.
HDDS-4861 added scm Certs to scmCert Table. In this Jira, listCerts will be modified to list all SCM Certificates.

(Once for other roles like SCM/DN they moved to new tables, listCertiificates will work for all the roles. This is not planned in this Jira)

This API will be used to get all the SCM certificates for SCM HA nodes during bootStrap SCM nodes startup.",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-02-26 11:01:09,13
13361141,Re-replication failure throws NPE,"
If downloading the container from source fails, DN throws an NPE. 
{noformat}
1:02:16.055 AM	ERROR	DownloadAndImportReplicator	
Container 998373 replication was unsuccessful.
java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:116)
	at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:129)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}

We should handle it more gracefully. Perhaps retry one more time?",pull-request-available,[],HDDS,Improvement,Trivial,2021-02-26 09:15:38,17
13361108,Upgrade the UsageInfoSubcommand with options to show most and least used datanodes.,"This Jira aims to add new functionalities to the UsageInfoSubcommand:
 * Query the most used 'count' number of datanodes
 * Query the least used 'count' number of datanodes
 * Specify 'count' as an integer number of datanodes

Related to https://issues.apache.org/jira/browse/HDDS-4816#.",pull-request-available,"['Ozone CLI', 'Tools']",HDDS,New Feature,Major,2021-02-26 06:23:43,49
13361091,Start cluster in Intellj failed to bind port,"Try to start a cluster in intellij with runConfigurations installed, but failed with failed to bind errors like:

java.io.IOException: Failed to bindjava.io.IOException: Failed to bind at org.apache.ratis.thirdparty.io.grpc.netty.NettyServer.start(NettyServer.java:264) at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:183) at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:90) at org.apache.hadoop.ozone.container.replication.ReplicationServer.start(ReplicationServer.java:103) at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:258) at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:112) at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41) at java.util.concurrent.FutureTask.run(FutureTask.java:266)",pull-request-available,[],HDDS,Bug,Trivial,2021-02-26 03:57:04,21
13360935,Bump jetty version,Please see: https://github.com/apache/ozone/pull/1964,pull-request-available,[],HDDS,Improvement,Major,2021-02-25 10:52:45,6
13360933,Bump jackson version number,Please see: https://github.com/apache/ozone/pull/1963,pull-request-available,[],HDDS,Improvement,Major,2021-02-25 10:40:48,6
13360860,Ozone datanode initraftlog fail due to bad disk so can not communicate to SCM,"because of bad disk,the datanode initraftlog fail and throw exception:
java.io.IOException: java.lang.IllegalStateException
        at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
        at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
        at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:71)
        at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:301)
        at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:318)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:461)
        at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:242)
        at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:112)
        at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Major,2021-02-25 02:31:57,23
13360697,Ozone admin datanode list should report dead and stale nodes,"In ListInfoSubcommand, the logic explicitly only displays HEALTHY nodes:

{code}
  private List<DatanodeWithAttributes> getAllNodes(ScmClient scmClient)
      throws IOException {
    List<HddsProtos.Node> nodes = scmClient.queryNode(null,
        HddsProtos.NodeState.HEALTHY, HddsProtos.QueryScope.CLUSTER, """");
  ...
{code}

I believe we should include stale and dead nodes in the the output too.",pull-request-available,['SCM'],HDDS,Improvement,Major,2021-02-24 17:46:48,11
13360695,Datanode with scmID format should work with clusterID directory format,Datanode with scmID format should work with clusterID directory format,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-24 17:36:05,18
13360662,Add acceptance tests to certify Ozone with boto3 python client.,"* Validate boto3 python client with supported Ozone s3 APIs.
* Identify gaps if any and document them or create JIRAs for Todos.
* Add acceptance tests that test boto3 APIs.",pull-request-available,['S3'],HDDS,Task,Major,2021-02-24 15:31:31,45
13360548,[SCM HA Security] Implement generate SCM certificate,"This Jira is to implement generatePeerSCMCertificate which will be called during bootStrap and also during init by primary SCM. This is similar to how OM and DN gets certificate.

",pull-request-available,"['SCM HA', 'Security']",HDDS,Sub-task,Major,2021-02-24 08:14:33,13
13360341,Useless Maven cache cleanup,"The {{build-branch}} Github Actions workflow has a step to avoid caching Ozone artifacts:

{noformat:title=https://github.com/apache/ozone/blob/1cf90150410882cd1ef88cdef33a0de18674399e/.github/workflows/post-commit.yml#L221-L225}
      - name: Delete temporary build artifacts before caching
        run: |
          #Never cache local artifacts
          rm -rf ~/.m2/repository/org/apache/hadoop/hdds
          rm -rf ~/.m2/repository/org/apache/hadoop/ozone
{noformat}

There are 2 problems:

# the cache is in a different job
# there are no {{hdds}} and {{ozone}} subdirectories, only ones like {{hadoop-hdds-common}}, etc.",pull-request-available,['build'],HDDS,Bug,Minor,2021-02-23 13:54:26,1
13360108,Ruby S3 SDK never get authenticated by Ozone,"When the very first call by Ruby client against secure setup of Ozone, the server returns 400 no matter how valid the request is. See the attached ruby-sdk-patch.diff, which adds some tests on S3 auth header signature-to-sign generation. It consists of two test additions, the ""2"" is the one generated by boto3, the ""3"" is generated by aws-ruby-sdk. Both passes the additional tests, which are definitely valid.

However, when real HTTP request is sent by Ruby client, e.g. ozone-test.rb attached, it fails with 400. The header was like this (though the host names and domains are masked):

{quote}GET //ozone.example.com:9879/sandbox?list-type=2&max-keys=1 HTTP/1.1
Content-Type:
Accept-Encoding:
User-Agent: aws-sdk-ruby3/3.112.0 ruby/2.7.2 x86_64-linux aws-sdk-s3/1.88.1
Host: ozone.example.com:9879
X-Amz-Date: 20210222T110554Z
X-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
Authorization: AWS4-HMAC-SHA256 Credential=kota@EXAMPLE.COM/20210222/foobar/s3/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=0c9469f018f5
b3fd2cff6f8d4e4963f50aa71c6704def59527634404f5fc98a9
Content-Length: 0
Accept: */*{quote}

On the other hand, request headers made by boto3 was:

{quote}GET //ozone.example.com:9879/sandbox?list-type=2&encoding-type=url HTTP/1.1
Host: ozone.example.com:9879
Accept-Encoding: identity
User-Agent: Boto3/1.17.12 Python/3.9.1 Linux/5.10.14-arch1-1 Botocore/1.20.12
X-Amz-Date: 20210222T110829Z
X-Amz-Content-SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
Authorization: AWS4-HMAC-SHA256 Credential=kota@EXAMPLE.COM/20210222/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=94302f21cccac8832d3e
4fe25c5f6d8a0307188fb0e1b1983264339381d21dac{quote}

The difference of these requests are IMHO, ""Content-Type"" and ""Accept-Encoding"" are both empty in Ruby SDK. I'm afraid this error stems from partly Ruby SDK and partly from [Jetty Issue|https://github.com/eclipse/jetty.project/issues/2883]. The former sends empty header lines and the latter rejects them.

And the s3g debug log (only error'ish part) follows:
{quote}2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: chain=NoCacheFilter@5e600dd5==org.apache.hadoop.hdds.server.http.NoCacheFilter,inst=true,async=true-
>safety@63a12c68==org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter,inst=true,async=true->info-page-redirect@576d5deb==org.apache.hadoop.ozone.s3.RootPageDis
playFilter,inst=true,async=false->jaxrs@603a422==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=1,inst=true,async=false
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter NoCacheFilter@5e600dd5==org.apache.hadoop.hdds.server.http.NoCacheFilter,inst=true,async
=true
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter safety@63a12c68==org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter,inst=
true,async=true
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter info-page-redirect@576d5deb==org.apache.hadoop.ozone.s3.RootPageDisplayFilter,inst=true,
async=false
2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call servlet jaxrs@603a422==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=1,inst=true
,async=false
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: sendError HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=false se=false i=tru
e al=0}
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.session: Leaving scope org.eclipse.jetty.server.session.SessionHandler367746789==dftMaxIdleSec=-1 dispatch=REQUEST, a
sync=false, session=null, oldsession=null, oldsessionhandler=null
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.Server: handled=true async=false committed=true on HttpChannelOverHttp@769bb34b{s=HttpChannelState@4893b376{s=HANDLIN
G rs=BLOCKING os=OPEN is=IDLE awp=false se=true i=true al=0},r=1,c=false/false,a=HANDLING,uri=https://ozone.example.com:9879/sandbox?list-type=2&ma
x-keys=1,age=2}
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: unhandle HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=false se=true i=true
al=0}
2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: nextAction(false) SEND_ERROR HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=f
alse se=false i=false al=0}
{quote}",pull-request-available,['S3'],HDDS,Bug,Major,2021-02-22 12:49:55,6
13359832,libexec/entrypoint.sh might copy from wrong path,"Not quite sure, but I expect the first cp cmd should be:
    cp /opt/ozone/etc/hadoop/* ""$CONF_DESTINATION_DIR/"" > /dev/null 2>&1

instead of:
||libexec/entrypoint.sh||
|...
#Try to copy the defaults
set +e
if [[ -d ""/opt/ozone/etc/hadoop"" ]]; then
   cp /opt/hadoop/etc/hadoop/* ""$CONF_DESTINATION_DIR/"" > /dev/null 2>&1
elif [[ -d ""/opt/hadoop/etc/hadoop"" ]]; then
   cp /opt/hadoop/etc/hadoop/* ""$CONF_DESTINATION_DIR/"" > /dev/null 2>&1
fi
...|",pull-request-available,['OM'],HDDS,Bug,Minor,2021-02-20 20:56:12,17
13359617,More compatibility problem with DatanodeDetails.Port.Name.REPLICATION,"Some {{ozone fs}} operations still encounter the compatibility issue attempted to fix in HDDS-4731.

{code}
$ ozone fs -mkdir o3fs://bucket1.vol1/dir/
$ ozone fs -ls o3fs://bucket1.vol1/dir/
$ ozone fs -put /etc/passwd o3fs://bucket1.vol1/dir/
-put: No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION
$ ozone fs -ls o3fs://bucket1.vol1/dir/
-ls: No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION
{code}",pull-request-available,[],HDDS,Bug,Critical,2021-02-19 15:36:31,1
13359613,Intermittent failure in ozonesecure due to unable to allocate block,"{code}
scm_1       | 2021-02-19 02:23:28,815 [IPC Server handler 28 on default port 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 1 pipelines. Required 3. Found 0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:172)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:224)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:134)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:271)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:201)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:192)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:162)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:118)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13265)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
scm_1       | 2021-02-19 02:23:28,815 [IPC Server handler 28 on default port 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
{code}

https://github.com/elek/ozone-build-results/tree/master/2021/02/19/6000/acceptance-secure",pull-request-available,[],HDDS,Bug,Critical,2021-02-19 15:10:22,1
13359584,Improve Ozone admin shell decommission/recommission/maintenance commands user experience,"1. Currently, entering `ozone admin datanode decommission` command alone doesn't give any feedback.

w/ patch:
{code}
bash-4.2$ ozone admin datanode decommission
Incomplete command
Usage: ozone admin datanode decommission [-hV] [--scm=<scm>] [<hosts>...]
Decommission a datanode
      [<hosts>...]   List of fully qualified host names
  -h, --help         Show this help message and exit.
      --scm=<scm>    The destination scm (host:port)
  -V, --version      Print version information and exit.
{code}

2. When decommission command is executed successfully, it lacks feedback on the client (it does log on the server side though).

w/ patch:
{code}
bash-4.2$ ozone admin datanode decommission 172.18.0.7 172.18.0.2
Started decommissioning datanodes:
172.18.0.7
172.18.0.2
{code}

3. Improve decommission failure message due to host/port resolution.

w/ patch:
{code}
bash-4.2$ ozone admin datanode decommission 172.18.0.71
Host 172.18.0.71 (172.18.0.71) is not running any datanodes registered with SCM. Please check the host name.

bash-4.2$ ozone admin datanode decommission 172.18.0.7:9999
Host 172.18.0.7:9999 is running a datanode registered with SCM, but the port number doesn't match. Please check the port number.
{code}



Same for recommission and maintenance commands.",pull-request-available,['Ozone CLI'],HDDS,Sub-task,Major,2021-02-19 13:16:13,12
13359571,Commit key audit log has inaccurate factor and type,"2021-02-19 16:04:27,344 | INFO  | OMAudit | user=kona_prof_user | ip=10.51.87.* | op=ALLOCATE_KEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=konajdk-profiler, key=sammichen/ozone.tar.gz, dataSize=241797850, replicationType=RATIS, replicationFactor=THREE} | ret=SUCCESS |
2021-02-19 16:04:33,042 | INFO  | OMAudit | user=kona_prof_user | ip=10.51.87.*| op=COMMIT_KEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=konajdk-profiler, key=sammichen/ozone.tar.gz, dataSize=241797850, replicationType=RATIS, replicationFactor=ONE} | ret=SUCCESS |",pull-request-available,[],HDDS,Bug,Major,2021-02-19 11:26:59,5
13359520,Fix CSI typo,fix typo in ozone csi: IdentitiyService -> IdentityService,newbie pull-request-available typo,['Ozone Filesystem'],HDDS,Bug,Trivial,2021-02-19 06:49:10,50
13359495,Add line break when node has no pipelines for `ozone admin datanode list` command,"The line {{No related pipelines or the node is not in Healthy state.}} should have a new line after it.

Just a trivial formatting issue.

Before:
{code}
bash-4.2$ ozone admin datanode list
Datanode: e56040c8-5cfa-4558-966b-0851ccf5c6c5 (/default-rack/172.22.0.2/ozone_datanode_1.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
ece97f05-fbf4-49db-9959-58c49c479f9b/ONE/RATIS/OPEN/Leader
62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Follower

Datanode: f702a7cc-d887-4095-8f1a-7826669a5ddd (/default-rack/172.22.0.7/ozone_datanode_4.ozone_default/0 pipelines)
Operational State: DECOMMISSIONED
Related pipelines:
No related pipelines or the node is not in Healthy state.
Datanode: 1baca70e-69f5-48e0-ae31-7f6d8ed2fafc (/default-rack/172.22.0.3/ozone_datanode_2.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Follower
8a387971-315d-4fd2-b2a3-c993cd36e8bb/ONE/RATIS/OPEN/Leader

Datanode: 30815665-dcda-40e4-bce6-000a46ab6d3d (/default-rack/172.22.0.6/ozone_datanode_3.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Leader
a7ff21f5-6d91-4452-9532-0c8789b1d435/ONE/RATIS/OPEN/Leader
{code}

After:

{code}
bash-4.2$ ozone admin datanode list
Datanode: 8f572444-6134-4740-845f-12a8f454fad0 (/default-rack/172.22.0.2/ozone_datanode_4.ozone_default/0 pipelines)
Operational State: DECOMMISSIONING
Related pipelines:
No related pipelines or the node is not in Healthy state.

Datanode: ae9cab4e-d163-4983-b7bb-4d140fbd41b5 (/default-rack/172.22.0.7/ozone_datanode_1.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
7e7bd855-3cc4-4e95-b363-38740580915c/ONE/RATIS/OPEN/Leader
9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower

Datanode: d992655c-aa4f-44ec-8bca-631191a527ef (/default-rack/172.22.0.8/ozone_datanode_3.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower
6fa79344-0791-454f-8c4f-2edb0cb2ea34/ONE/RATIS/OPEN/Leader

Datanode: 327bc5f1-874b-42da-9951-f4cc1571bab9 (/default-rack/172.22.0.9/ozone_datanode_2.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
a28a6008-b3b8-45c8-a7f4-23ca680b3e46/ONE/RATIS/OPEN/Leader
9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower
{code}",pull-request-available,['Tools'],HDDS,Sub-task,Trivial,2021-02-19 03:18:48,12
13359493,Update NodeStatus OperationalState for Datanodes in Recon,"Possibly due to Recon ignoring {{setNodeOperationalStateCommand}} (HDDS-4766), {{NodeStatus}} isn't being updated for its {{operationalState}} and {{opStateExpiryEpochSeconds}} fields (but {{DatanodeInfo}}'s {{persistedOpState}} and {{persistedOpStateExpiryEpochSec}} are correct).

See the attached screenshot.

Found this during development of HDDS-4832.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2021-02-19 03:13:14,12
13359472,fault_injection_tool build failure on some platforms,"On some platforms cmake build for fault-injection-tool complains about not able to find the threads library. Eventually this leads to failure when doing the make.
{code:java}
root@ccycloud:~/ozone/tools/fault-injection-service/Build# ~/cmake-3.14.0-Linux-x86_64/bin/cmake ../
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
CMake Warning (dev) at /root/.local/lib/cmake/protobuf/protobuf-options.cmake:6 (option):
  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake
  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to
  set the policy and suppress this warning.


  For compatibility with older versions of CMake, option is clearing the
  normal variable 'protobuf_MODULE_COMPATIBLE'.
Call Stack (most recent call first):
  /root/.local/lib/cmake/protobuf/protobuf-config.cmake:2 (include)
  CMakeLists.txt:37 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.


-- Using protobuf 
-- Using gRPC 1.35.0
-- Configuring done
CMake Warning (dev) at CMakeLists.txt:85 (add_executable):
  Policy CMP0028 is not set: Double colon in target name means ALIAS or
  IMPORTED target.  Run ""cmake --help-policy CMP0028"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.


  Target ""failure_injector_svc_server"" links to target ""Threads::Threads"" but
  the target was not found.  Perhaps a find_package() call is missing for an
  IMPORTED target, or an ALIAS target is missing?
This warning is for project developers.  Use -Wno-dev to suppress it.


CMake Warning (dev) at CMakeLists.txt:85 (add_executable):
  Policy CMP0028 is not set: Double colon in target name means ALIAS or
  IMPORTED target.  Run ""cmake --help-policy CMP0028"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.


  Target ""failure_injector_svc_server"" links to target ""Threads::Threads"" but
  the target was not found.  Perhaps a find_package() call is missing for an
  IMPORTED target, or an ALIAS target is missing?
This warning is for project developers.  Use -Wno-dev to suppress it.


CMake Warning (dev) at CMakeLists.txt:85 (add_executable):
  Policy CMP0028 is not set: Double colon in target name means ALIAS or
  IMPORTED target.  Run ""cmake --help-policy CMP0028"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.


  Target ""failure_injector_svc_server"" links to target ""Threads::Threads"" but
  the target was not found.  Perhaps a find_package() call is missing for an
  IMPORTED target, or an ALIAS target is missing?
This warning is for project developers.  Use -Wno-dev to suppress it.


CMake Warning (dev) at CMakeLists.txt:96 (add_executable):
  Policy CMP0028 is not set: Double colon in target name means ALIAS or
  IMPORTED target.  Run ""cmake --help-policy CMP0028"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.


  Target ""failure_injector_svc_client"" links to target ""Threads::Threads"" but
  the target was not found.  Perhaps a find_package() call is missing for an
  IMPORTED target, or an ALIAS target is missing?
This warning is for project developers.  Use -Wno-dev to suppress it.


CMake Warning (dev) at CMakeLists.txt:96 (add_executable):
  Policy CMP0028 is not set: Double colon in target name means ALIAS or
  IMPORTED target.  Run ""cmake --help-policy CMP0028"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.


  Target ""failure_injector_svc_client"" links to target ""Threads::Threads"" but
  the target was not found.  Perhaps a find_package() call is missing for an
  IMPORTED target, or an ALIAS target is missing?
This warning is for project developers.  Use -Wno-dev to suppress it.


CMake Warning (dev) at CMakeLists.txt:96 (add_executable):
  Policy CMP0028 is not set: Double colon in target name means ALIAS or
  IMPORTED target.  Run ""cmake --help-policy CMP0028"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.


  Target ""failure_injector_svc_client"" links to target ""Threads::Threads"" but
  the target was not found.  Perhaps a find_package() call is missing for an
  IMPORTED target, or an ALIAS target is missing?
This warning is for project developers.  Use -Wno-dev to suppress it.


-- Generating done
-- Build files have been written to: /root/ozone/tools/fault-injection-service/Build
root@ccycloud:~/ozone/tools/fault-injection-service/Build# 
{code}",pull-request-available,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Bug,Major,2021-02-18 23:04:42,44
13359396,Add timestamp to Revoked Certs table in SCM DB,Revoked Certs table in SCM DB does not have a timestamp of revocation. We need to add this to keep track of when certificates got revoked.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-02-18 17:49:18,37
13359283,Make changes required for SCM admin commands to work with SCM HA,This Jira is to make changes required to make SCM commands works with SCM HA/non-HA,pull-request-available,[],HDDS,Sub-task,Major,2021-02-18 11:40:30,13
13359277,Ozone DN always use the spinning disk RocksDB profile,"The property hdds.db.profile (default is ""DBProfile.DISK"". The SSD profile is selectable via ""DBProfile.SSD"") is configurable and allows OM, SCM and Reconn to choose the SSD profile which optimizes for SSD.

It's not the case for DN. DN always use the spinning disk profile for rocksdb. We should make it possible to choose SSD profile.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2021-02-18 11:12:06,21
13359272,Use SCM service ID in finding SCM Datanode address.,Use SCM serviceID and nodeID to figure SCM Datanode address and port in HA from SCM address key and SCM Datanode port config or else fall back to ozone.scm.names,pull-request-available,[],HDDS,Sub-task,Major,2021-02-18 10:58:40,13
13359177,[FSO]S3Multipart: Implement UploadAbortRequest,This task is to implement prefix based FSO for the {{S3MultipartUploadAbortRequest}}.,pull-request-available,[],HDDS,Sub-task,Major,2021-02-18 02:33:44,40
13358937,Replication failure in secure environment,"The following error prevents closed container replication if {{ozone.security.enabled}} is {{true}}.

{code}
java.lang.IllegalArgumentException: File does not contain valid certificates: certificate.crt
	at org.apache.ratis.thirdparty.io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:345)
	at org.apache.ratis.thirdparty.io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:294)
	at org.apache.hadoop.ozone.container.replication.GrpcReplicationClient.<init>(GrpcReplicationClient.java:81)
{code}

CC [~sodonnell]",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2021-02-16 20:44:00,1
13358924,Show Datanode OperationalState (IN_SERVICE/DECOMMISSION/MAINTENANCE) in Recon,"There are 5 NodeOperationalState s defined at the moment:

{code}
  /**
   * Protobuf enum {@code hadoop.hdds.NodeOperationalState}
   */
  public enum NodeOperationalState
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>IN_SERVICE = 1;</code>
     */
    IN_SERVICE(0, 1),
    /**
     * <code>DECOMMISSIONING = 2;</code>
     */
    DECOMMISSIONING(1, 2),
    /**
     * <code>DECOMMISSIONED = 3;</code>
     */
    DECOMMISSIONED(2, 3),
    /**
     * <code>ENTERING_MAINTENANCE = 4;</code>
     */
    ENTERING_MAINTENANCE(3, 4),
    /**
     * <code>IN_MAINTENANCE = 5;</code>
     */
    IN_MAINTENANCE(4, 5),
    ;
{code}

We should show the Datanode OperationalState in Recon Datanode page as well as it provides valuable information. See the attached screenshots for the result.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2021-02-16 18:54:50,12
13358920,Attempting an SCM finalization after a failed / incomplete finalization.,"* Trigger failure cases in the middle of SCM finalization
* Attempt the SCM finalization after an SCM restart
* SCM finalization should go through successfully eventually.",SCM UpgrateTesting,[],HDDS,Sub-task,Major,2021-02-16 18:28:16,44
13358901,Support wildcard(*) in ozone.administrators in SCM admin check,"OM supports wildcard * during admin access check, but SCM does not honor this. This Jira is to fix this issue.",pull-request-available,[],HDDS,Bug,Major,2021-02-16 16:34:31,13
13358900,Rename MiniOzoneHACluster to MiniOzoneOMHACluster,Current MiniOzoneHACluster only handles OM HA and does not handles SCM HA. This jira renames the MiniOzoneHACluster to reflect the state.,pull-request-available,[],HDDS,Sub-task,Major,2021-02-16 16:32:05,18
13358878,"SCM should go into ""safe mode"" until there is at least 1 pipeline to work with after finalization.","On the docker cluster it was noticed that although SCM can finalize very quickly, it takes at least a heartbeat amount of time for DNs to finalize and only after that can an open pipeline be ready on the SCM. It would be a good experience for the SCM finalization to wait for at least 1 pipeline to be ready for use after finalization after declaring success.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-02-16 14:33:29,22
13358712,Trash checkpoints are not getting deleted from multiple buckets,"Trash checkpoints are not getting deleted from all buckets from which keys are deleted. Deletion occurs only in selective bucket/s.

In the below exception ,notice that the bucket and volume are different in the Exception stacktrace and the first message.
{code:java}
5:03:00.019 PMERRORTrashPolicyOzoneUnable to checkpoint:/vol-test1-1613316962/buck-test1-1613316962/.Trash/hrt_qa
 FILE_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Unable to get file status: volume: vol2 bucket: buck2 key: .Trash/hrt_qa/210214153900
 at org.apache.hadoop.ozone.om.KeyManagerImpl.getOzoneFileStatus(KeyManagerImpl.java:1834)
 at org.apache.hadoop.ozone.om.KeyManagerImpl.getFileStatus(KeyManagerImpl.java:1775)
 at org.apache.hadoop.ozone.om.KeyManagerImpl.getFileStatus(KeyManagerImpl.java:1753)
 at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.getFileStatus(TrashOzoneFileSystem.java:221)
 at org.apache.hadoop.ozone.om.TrashOzoneFileSystem$OzoneListingIterator.<init>(TrashOzoneFileSystem.java:298)
 at org.apache.hadoop.ozone.om.TrashOzoneFileSystem$DeleteIterator.<init>(TrashOzoneFileSystem.java:441)
 at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.delete(TrashOzoneFileSystem.java:163)
 at org.apache.hadoop.ozone.om.TrashPolicyOzone.deleteCheckpoint(TrashPolicyOzone.java:269)
 at org.apache.hadoop.ozone.om.TrashPolicyOzone.access$900(TrashPolicyOzone.java:55)
 at org.apache.hadoop.ozone.om.TrashPolicyOzone$Emptier.lambda$run$0(TrashPolicyOzone.java:177)
 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
 at java.base/java.lang.Thread.run(Thread.java:834){code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2021-02-15 18:36:28,26
13358675,StateContext set the same reports repeatedly,"In StateContext.addReport(GeneratedMessage report), the report was added to containerReports, nodeReports or pipelineReports for each endpoint loop, but the three reports keep the lastest full report for the whole context, there is no need to set the value in each endpoint loop. 

The original code is as follows:
{code:java}
for (InetSocketAddress endpoint : endpoints) {
  if (reportType.equals(CONTAINER_REPORTS_PROTO_NAME)) {
    containerReports.set(report);
  } else if (reportType.equals(NODE_REPORT_PROTO_NAME)) {
    nodeReport.set(report);
  } else if (reportType.equals(PIPELINE_REPORTS_PROTO_NAME)) {
    pipelineReports.set(report);
  } else if (ACCEPTED_INCREMENTAL_REPORT_TYPE_SET.contains(reportType)) {
    synchronized (incrementalReportsQueue) {
      incrementalReportsQueue.get(endpoint).add(report);
    }
  } else {
    throw new IllegalArgumentException(
        ""Unidentified report message type: "" + reportType);
  }
}
{code}",pull-request-available,[],HDDS,Improvement,Major,2021-02-15 14:58:16,7
13358635,Make SCM Generic config support HA Style,"To use a single config across the cluster, a few of the config keys in SCM need to support HA Style, and they need to be set to default config keys so that the conf can be reusable in other parts of code.

In this Jira, added the following config keys to support HA style config.

      OZONE_SCM_DATANODE_ADDRESS_KEY,
      OZONE_SCM_DATANODE_PORT_KEY,
      OZONE_SCM_DATANODE_BIND_HOST_KEY,
      OZONE_SCM_BLOCK_CLIENT_ADDRESS_KEY,
      OZONE_SCM_BLOCK_CLIENT_PORT_KEY,
      OZONE_SCM_BLOCK_CLIENT_BIND_HOST_KEY,
      OZONE_SCM_CLIENT_ADDRESS_KEY,
      OZONE_SCM_CLIENT_PORT_KEY,
      OZONE_SCM_CLIENT_BIND_HOST_KEY,
      OZONE_SCM_SECURITY_SERVICE_ADDRESS_KEY,
      OZONE_SCM_SECURITY_SERVICE_PORT_KEY,
      OZONE_SCM_SECURITY_SERVICE_BIND_HOST_KEY,
      OZONE_SCM_RATIS_PORT_KEY,
      OZONE_SCM_HTTP_BIND_HOST_KEY,
      OZONE_SCM_HTTPS_BIND_HOST_KEY,
      OZONE_SCM_HTTP_ADDRESS_KEY,
      OZONE_SCM_HTTPS_ADDRESS_KEY,
      OZONE_SCM_DB_DIRS,
      OZONE_SCM_ADDRESS_KEY",pull-request-available,[],HDDS,Sub-task,Major,2021-02-15 11:12:18,13
13358579,Implement scm --bootstrap command,"During SCM --bootstrap, the bootstrapping SCM node will connect to primary SCM node (already running) and get the cluster Id. Once security is implemented, it will also fetch the CSR root certificates from primary SCM during SCM --bootstrap phase. ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-15 05:04:15,16
13358539,Use SCM service ID in SCMBlockClient and SCM Client,"Use SCM service ID in SCMBlockClient and SCM Client

For existing installations, it is also important to fallback to use the ozone.scm.names as well

cc [~shashikant] [~nanda] [~bharat]",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-14 13:19:29,13
13358529,Add multiple SCM nodes to MiniOzoneCluster,This jira adds multiple SCMs to MiniOzoneCluster,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-14 10:49:51,16
13358000,Add read only command to get status of Finalization in OM & SCM.,"Currently, the finalization client exposes only the ""finalizeupgrade"" command that starts finalization, and sees it to completion. In an already finalized cluster or a cluster where finalization is going on, the only way to get finalization status by another client is to run finalizeupgrade again (with --takeover flag if finalization is in progress). Since this is not very user intuitive, we can add a new admin command in OM and SCM to get finalization status of the component through a read only command, like 

*_ozone admin om/scm finalization-status_*",pull-request-available,"['Ozone Filesystem', 'SCM']",HDDS,Sub-task,Major,2021-02-10 18:54:36,21
13357999,Fresh deploy of Ozone must use the highest layout version by default,"By default, a new cluster should use the latest layout version while being deployed (--init). This is also true for Datanodes. If there is a new DN when the cluster is an unfinalized state, SCM layout version check will not let it become part of a pipeline until finalization.",pull-request-available,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Sub-task,Major,2021-02-10 18:53:01,30
13357913,Add a tool to get the usage information of a datanode.,"Currently, Ozone does not have a command to get the usage information of a datanode.

The Jira aims to add an admin datanode subcommand called UsageInfoSubcommand to query usage info of a datanode. Information such as Capacity, SCMUsed and Remaining space is listed.",pull-request-available,"['Ozone Datanode', 'Tools']",HDDS,Bug,Major,2021-02-10 11:02:24,49
13357810,unbuffer caused connection leak,"HDDS-4320 implemented unbuffer API. However, I found it changed BlockInputStream.close(). The close() call is supposed to close the enclosed ChunkInputStreams, but that logic was moved to unbuffer() and close() never closes ChunkInputStreams, causing socket leak.

Running a small 100GB SparkSQL TPC-DS workloads on a 3-node cluster, the test set couldn't complete because the processes failed with ""java.net.BindException: Cannot assign requested address"" message. Further investigation found the process created tens of thousands of sockets (lsof -p). I was finally able to pinpoint the source of leak using btrace.",pull-request-available,['Ozone Client'],HDDS,Bug,Blocker,2021-02-09 17:55:29,25
13357776,Ozone couldn't be started with Java 15,"The magic -- which tries to provide sensible GC defaults -- fails on Java 15: 

{code}
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
WARNING: /opt/ozone/logs does not exist. Creating.
Unrecognized VM option 'UseConcMarkSweepGC'
{code}",pull-request-available,[],HDDS,Improvement,Critical,2021-02-09 15:04:47,7
13357772,[FSO]S3Multipart: Implement UploadCompleteRequest,This task is to implement prefix based FSO for the {{S3MultipartUploadCompleteRequest}}. ,pull-request-available,[],HDDS,Sub-task,Major,2021-02-09 14:53:42,40
13357716,Move Ratis group creation to scm --init phase,"Currently, the ratis group creation happens post start of scm service. The idea here is move the ratis group initialization to scm --init phase and use cluster id as the SCM HA group ID.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-09 09:07:31,16
13357545,Add comment to FilePerChunkStrategy for readability,"In FilePerChunkStrategy.readChunk(), the following code seems confused, as _finalChunkFile_ was added twice.
{code:java}
    List<File> possibleFiles = new ArrayList<>();
    possibleFiles.add(finalChunkFile);
    if (dispatcherContext != null && dispatcherContext.isReadFromTmpFile()) {
      possibleFiles.add(getTmpChunkFile(finalChunkFile, dispatcherContext));
      possibleFiles.add(finalChunkFile);
    }
{code}
After checking HDDS-2372, I found it's a simple solution to solve race condition between read and commit. This ticket is to add some comments to save the time for later code readers.",pull-request-available,[],HDDS,Improvement,Major,2021-02-08 13:32:29,7
13357517,Add Genesis benchmark for various CRC implementations,"As highlighted in HDDS-4138 Ozone appears to have a greater CRC overhead than Hadoop. In order to figure out where the problem is, we should add a benchmark to Genesis to test if one implementation is much better than the others.",pull-request-available,[],HDDS,New Feature,Major,2021-02-08 12:12:44,11
13357478,Add install checkpoint in SCMStateMachine,"Currently, https://issues.apache.org/jira/browse/HDDS-4773 adds functionality to download a Rocks db checkpoint from leader node. The idea here is to add functionality to install the downloaded checkpoint and reinitialise SCMStateMachine with latest downloaded checkpoint.",pull-request-available,[],HDDS,Sub-task,Major,2021-02-08 09:22:39,16
13357456,Fix misc acceptance test: List pipelines on unknown host,"For now, SCMContainerLocationFailoverProxyProvider first use ozone.scm.names to create rpcProxy, if ozone.scm.names is not set, it falls back to use 

ozone.scm.client.address. However, it breaks the master's behavior, which directly use ozone.scm.client.address to build the rpcProxy.

 

In ozone admin --verbose pipeline list --scm unknown-host, unknown-host will be set to ozone.scm.client.address, the created rpcProxy will trigger ""java.net.UnknownHostException: Invalid host name: ""

 

This Jira https://issues.apache.org/jira/browse/HDDS-4556 and PR [https://github.com/apache/ozone/pull/1671 |https://github.com/apache/ozone/pull/1671] adds a smoketest for {{ozonescripts}} environment, helps to find this bug.
{code:java}
------------------------------------------------------------------------------ 
 List pipelines on unknown host | FAIL | 
 1'Pipeline[ Id: abdee66c-4fce-4099-a7b6-ab0dab03f52f, Nodes: 8df277fe-9d11-4dce-8c64-898e2ad83901{ip: 172.26.0.2, host: ozonescripts_datanode_1.ozonescripts_default, ports: [REPLICATION=9886, RATIS=9858, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:8df277fe-9d11-4dce-8c64-898e2ad83901, CreationTimestamp2021-02-08T04:05:29.452Z] 
 Pipeline[ Id: d1ad94f4-bbe2-4676-8e4d-2f7261754b31, Nodes: 8df277fe-9d11-4dce-8c64-898e2ad83901{ip: 172.26.0.2, host: ozonescripts_datanode_1.ozonescripts_default, ports: [REPLICATION=9886, RATIS=9858, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:ONE, State:CLOSED, leaderId:, CreationTimestamp2021-02-08T04:05:53.496Z]' does not contain 'Invalid host name' 
 ------------------------------------------------------------------------------ 
 Pipeline :: Test ozone admin pipeline command | FAIL |{code}
 

 ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-08 06:34:30,36
13357443,[FSO]Fix findbugs issues after HDDS-2195,"After spotbugs is enabled with HDDS-2195, there are additional issues reported by findbugs. This task is to fix those cases in the feature branch.

Following are the cases reported and seen in the [PR_CHECK|https://github.com/apache/ozone/pull/1897/checks?check_run_id=1838392377].",pull-request-available,[],HDDS,Sub-task,Major,2021-02-08 05:01:04,40
13357432,Fix TestReconContainerManager after merge master to HDDS-2823,"TestReconContainerManager#testUpdateAndRemoveContainerReplica is failed after merge back master to HDDS-2823.

 
Error:  Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.251 s <<< FAILURE! - in org.apache.hadoop.ozone.recon.scm.TestReconContainerManager 
Error:  testUpdateAndRemoveContainerReplica(org.apache.hadoop.ozone.recon.scm.TestReconContainerManager) Time elapsed: 0.18 s <<< ERROR! 
org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: Container with id #1 not found.
 ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-08 03:01:29,36
13357398,Skip coverage check for PRs and in forks,"Currently _coverage_ CI check:

# calculates combined test coverage
# uploads it to Sonar only for Apache Ozone repo and only for builds on push/schedule
# stores combined coverage in GitHub Actions artifact

Thus for PR in Apache Ozone and for all builds in forks, it only stores coverage in the artifact.  These expire in 30 days and I don't think anybody really checks them manually.

I propose to completely skip _coverage_ check for PRs and in forks, instead of only skipping upload to Sonar.  This would save ~12 minutes for such builds.",pull-request-available,['build'],HDDS,Improvement,Major,2021-02-07 15:13:35,1
13357384,Fix TestContainerEndpoint after merging master to HDDS-2823.,TestContainerEndpoint is broken after merging master to HDDS-2823 at Feb.,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-07 12:35:05,36
13357271,Unexpected update of container's bcsID in BlockManagerImpl,"In BlockManagerImpl.putBlock, there is a special case that BlockData's BlockCommitSequenceId could be 0. The reason should be as the comment:
{code:java}
// default blockCommitSequenceId for any block is 0. It the putBlock
// request is not coming via Ratis(for test scenarios), it will be 0.
// In such cases, we should overwrite the block as well
{code}
But after the Block with bcsId as 0 is updated, the container's bcsId will also be updated to 0. If there is no such special case, the container's bscId should always increase, but with this case, the container's bcsId could decrease to 0.

This ticket is to add a check before updating the container's bcsId.",pull-request-available,[],HDDS,Bug,Major,2021-02-06 14:12:07,7
13357012,Fix findbugs issues after HDDS-2195,"After spotbugs is enabled with HDDS-2195, there are additional issues reported by findbugs. These issues have to fixed.",pull-request-available,[],HDDS,Sub-task,Major,2021-02-05 09:52:38,36
13356951,Separate source of Ozone container images to different repositories,"As discussed [here|https://lists.apache.org/thread.html/r222da5ce97d42f945f5bbd4a10bc8d5fab1f0a2324ad2de2ff19c860%40%3Cdev.ozone.apache.org%3E], the current repository (which includes source for apache/ozone-runer, apache/ozone-build, apache/ozone containers) can be separtaed to make it more straightforward to use.",pull-request-available,[],HDDS,Improvement,Major,2021-02-05 07:19:43,6
13356848,No appenders could be found for logger - in tests,"Some tests only output this:

{noformat}
log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdds.utils.db.RDBStore).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2021-02-04 20:35:29,1
13356841,Allow recon access /dbCheckpoint in ozonesecure-om-ha,"Fix the following permission issue in {{ozonesecure-om-ha}} environment:

{code}
om1_1        | ERROR om.OMDBCheckpointServlet: Permission denied: User principal 'recon/recon@EXAMPLE.COM' does not have access to /dbCheckpoint.
om1_1        | This can happen when Ozone Manager is started with a different user.
om1_1        | Please append 'recon/recon@EXAMPLE.COM' to OM 'ozone.administrators' config and restart OM to grant current user access to this endpoint.
{code}",pull-request-available,['docker'],HDDS,Improvement,Trivial,2021-02-04 19:43:23,1
13356798,Redundant state check in KeyValueHandler.handleDeleteChunk,"In KeyValueHandler.handleDeleteChunk, the state of container was checked twice as below:
{code:java}
try {
  checkContainerIsHealthy(kvContainer);
} catch (StorageContainerException sce) {
  return ContainerUtils.logAndReturnError(LOG, sce, request);
}

try {
  checkContainerOpen(kvContainer);
  ...
}{code}
The check of healthy can be removed.",pull-request-available,[],HDDS,Improvement,Major,2021-02-04 15:55:05,7
13356633,StateContext.getReports may return list with size larger the maxLimit,"The method of StateContext.getReports may return an incorrect size of result. The invoke of the method is as below:
{code:java}
public List<GeneratedMessage> getReports(InetSocketAddress endpoint, int maxLimit)
{code}
It will first try to get messages from _incrementalReportsQueue_, then add the mesages from _containerReports_, _nodeReport_ and _pipelineReports_. If _incrementalReportQueue_ returns result with the size of maxLimit, getReports will return result with size of (maxLimit + 3) at most.",pull-request-available,[],HDDS,Bug,Major,2021-02-04 03:36:30,7
13356525,[FSO]Add a tool to parse entries in the prefix format,Add a tool to parse entries in the prefix format,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-02-03 16:00:41,18
13356462,Improve usability of ozone s3 getsecret output,Please see: https://github.com/apache/ozone/pull/1889,pull-request-available,[],HDDS,Improvement,Major,2021-02-03 10:41:43,6
13356422,Enable mTLS for Ratis in OM HA,The goal of this task is to enable TLS for Ratis in OM HA based on {{SecurityConfig#isGrpcTlsEnabled}}.,pull-request-available,['OM HA'],HDDS,Improvement,Major,2021-02-03 08:50:00,1
13356353,Trash emptier fails to create checkpoints in a secure setup,"Since TrashOzoneFilesystem doesn't create rpc calls and calls the OM code internally during ACL checks we get NPE
{code:java}
java.lang.NullPointerException
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3612)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3585)
	at org.apache.hadoop.ozone.om.OzoneManager.listStatus(OzoneManager.java:2992)
	at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.listStatus(TrashOzoneFileSystem.java:167)
	at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.getTrashRoots(TrashOzoneFileSystem.java:252)
	at org.apache.hadoop.ozone.om.TrashPolicyOzone$Emptier.run(TrashPolicyOzone.java:167)
	at java.base/java.lang.Thread.run(Thread.java:834)
{code}
{code:java}
 
Couldn't perform fs operation fs.listStatus()/fs.exists()Couldn't perform fs operation fs.listStatus()/fs.exists()java.lang.NullPointerException at org.apache.ranger.authorization.ozone.authorizer.RangerOzoneAuthorizer.checkAccess(RangerOzoneAuthorizer.java:125) at org.apache.ranger.authorization.ozone.authorizer.RangerOzoneAuthorizer.checkAccess(RangerOzoneAuthorizer.java:90) at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1791) at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1701) at org.apache.hadoop.ozone.om.OzoneManager.listStatus(OzoneManager.java:3004) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.listStatus(TrashOzoneFileSystem.java:167) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.getTrashRoots(TrashOzoneFileSystem.java:252) at org.apache.hadoop.ozone.om.TrashPolicyOzone$Emptier.run(TrashPolicyOzone.java:167) at java.base/java.lang.Thread.run(Thread.java:834)
{code}



{code:java}
Terminating with exit status 1: Request cmdType: RenameKey
clientId: ""client-E6D89B84315B""
renameKeyRequest {
  keyArgs {
    volumeName: ""s3v""
    bucketName: ""buck1""
    keyName: "".Trash/hive/Current/""
  }
  toKeyName: "".Trash/hive/210202163300/""
}
failed with exception
java.lang.NullPointerException
	at org.apache.ranger.authorization.ozone.authorizer.RangerOzoneAuthorizer.checkAccess(RangerOzoneAuthorizer.java:125)
	at org.apache.ranger.authorization.ozone.authorizer.RangerOzoneAuthorizer.checkAccess(RangerOzoneAuthorizer.java:90)
	at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1791)
	at org.apache.hadoop.ozone.om.request.OMClientRequest.checkAcls(OMClientRequest.java:176)
	at org.apache.hadoop.ozone.om.request.OMClientRequest.checkAcls(OMClientRequest.java:154)
	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.checkKeyAcls(OMKeyRequest.java:437)
	at org.apache.hadoop.ozone.om.request.key.OMKeyRenameRequest.validateAndUpdateCache(OMKeyRenameRequest.java:143)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:415)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:240)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
{code}
",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2021-02-03 04:06:24,26
13356231,Document run configuration launch order,Document the order in which Ozone components should be started from IntelliJ.  See https://github.com/apache/ozone/pull/1872,pull-request-available,['documentation'],HDDS,Improvement,Major,2021-02-02 15:27:00,51
13356191,TestRootedOzoneFileSystem is not picking the configured 'Parameters',This is a follow-up Jira to update {{TestRootedOzoneFileSystem}} test class and run against all the configured parameters combinations.,pull-request-available,['test'],HDDS,Bug,Major,2021-02-02 12:25:51,40
13356170,Merge SCMRatisSnapshotInfo and OMRatisSnapshotInfo into a single class,SCMRatisSnapshotInfo and OMRatisSnapshotInfo seem to be duplicate of each other. The idea is to merge them into a single class and use as is.,pull-request-available,[],HDDS,Sub-task,Major,2021-02-02 10:43:19,16
13356167,[FSO]S3MultiPart: Implement create and commit upload part file,"This task is a follow-up task to HDDS-4771 jira to implement {{open}}, {{create}} and {{commit}} upload part file request.",pull-request-available,[],HDDS,Sub-task,Major,2021-02-02 10:38:30,40
13356162,Disable spotbugs for (the empty) hadoop-ozone-datanode project,Please see: https://github.com/apache/ozone/pull/1878,pull-request-available,[],HDDS,Improvement,Major,2021-02-02 10:21:14,6
13356010,Disable Prevote  in Ratis for Ozone by default,"Pre-vote and Leader Lease are new features in Ratis which may not be required/not tested enough with ozone currently. The idea here is to disable these by default in ozone.

Leader lease is yet be committed : https://issues.apache.org/jira/browse/RATIS-1273",pull-request-available,[],HDDS,Bug,Blocker,2021-02-01 17:32:50,16
13355966,Add functionality to transfer Rocks db checkpoint from leader to follower,"As a part of install Snapshot notification from leader to follower, the follower needs to get the latest Rocks db checkpoint from the follower first . It then needs to reload the stateMachine from the latest state , and then start participating in the ring. This jira aims to add the transfer rocks db checkpointing functionality for SCM HA.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-02-01 14:56:30,16
13355838,Upgrade Ratis Thirdparty to 0.6.0 for fixing ozone codebase build error ,"Upgrade Ratis Thirdparty to 0.6.0

Failure to find org.apache.ratis:ratis-thirdparty-misc:jar:0.6.0-SNAPSHOT in [https://repository.apache.org/content/repositories/snapshots] was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced -> [Help 1]",pull-request-available,[],HDDS,Improvement,Major,2021-02-01 07:28:13,52
13355801,[FSO]S3MultiPart: Implement InitiateMultiPartUpload ,"This task is the first one in the work pipeline to implement *S3MultipartUpload* requests, I will create follow-up jira tasks to cover {{commit/complete/list/abort}} multipart requests incrementally.

Here, the idea of this jira task is to implement {{Initiate multipart upload}} for a specified key.",pull-request-available,[],HDDS,Sub-task,Major,2021-02-01 06:06:04,40
13355711,Upgrade Ratis Thirdparty to 0.6.0,"{{ratis-thirdparty}} 0.6.0 was released recently, and now 0.6.0-SNAPSHOT is gone from the repos.",pull-request-available,['build'],HDDS,Task,Blocker,2021-01-31 08:29:35,1
13355708,Simplify the insert operation of ContainerAttribute,"Currently when inserting a containerId to a ContainerAttribute, if the containerId is already in the ContainerAttribute, it will first remove the containerId and then add it again, but the value of containerId is not changed at all.

The ticket is for simplifying the insert operation for better performance and readability.",pull-request-available,[],HDDS,Improvement,Major,2021-01-31 07:31:06,7
13355652,Intermittent failure in TestOzoneManagerDoubleBufferWithDummyResponse,"{noformat}
Error:  Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.986 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse
Error:  testDoubleBufferWithDummyResponse(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse)  Time elapsed: 0.912 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.testDoubleBufferWithDummyResponse(TestOzoneManagerDoubleBufferWithDummyResponse.java:124)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2021-01-30 12:47:35,1
13355484,Recon resets the Operational State of datanodes to IN_SERVICE,"When a datanode is decommission or put to maintenance, its new state is persisted into the datanode.yaml file. When running on a cluster with Recon enabled, we can see conflicting commands are received repeatedly on the Datanode, eg:

{code}
datanode_3  | 2021-01-29 16:26:20,009 [EndpointStateMachine task thread for scm/172.24.0.6:9861 - 0 ] INFO endpoint.HeartbeatEndpointTask: Received SCM set operational state command. State: DECOMMISSIONED Expiry: 0 id 3645344
datanode_3  | 2021-01-29 16:26:50,012 [EndpointStateMachine task thread for recon/172.24.0.3:9891 - 0 ] INFO commands.SetNodeOperationalStateCommand: Create a new command to set op state IN_SERVICE 0 id is 3675347
{code}

This is happening because Recon delegates processing the DN heartbeats received by ReconNodeManager to an instance of SCMNodeManager running inside Recon. SCMNodeManager checks the reported state of the datanode matches the SCM memory state, and if they don't match, it issues a command to the DN to update its state.

In this case, Recon always tries to set the DN state back to IN_SERVICE.

The fix here, is probably to update the Recon in memory state before delegating the heartbeat to SCMNodeManager.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2021-01-29 16:47:02,11
13355470,Update close-pending workflow for new repo,The Github Actions workflow that looks for pending PRs to be closed does not work currently because it is limited to the old {{hadoop-ozone}} repo.,pull-request-available,['CI'],HDDS,Task,Major,2021-01-29 16:15:49,1
13355453,Add HA-OM support for ozonefs-hadoop2,"Now Hadoop27RpcTransport is creating the connection directly to the OM specified by ""

ozone.om.address"", so it is not supporting HA-OM.

Checked the code of Hadoop-common of version 2.7, RetryProxy is already usable, so this ticket is for using RetryProxy for ozonefs-hadoop2 to enable HA-OM support.",pull-request-available,[],HDDS,Improvement,Major,2021-01-29 14:54:14,7
13355444,Owner field of S3AUTHINFO type delegation token should be validated,"Delegation token of Ozone has two flavor:
 1. delegation token (based public key infrastructure provided by SCM CA)
 2. s3 token

S3 token includes all the information which is required to validate a S3 HTTP request: aws access key id, string2sign, signature. OM can check the signature based on all this information which are stored in the OzoneTokenInfo.

When the request is authenticated the owner field is used for all the following authentication. But the content of the follower field is not validated. It's filled by the S3g but any client can create a custom request where the Owner field contains a custom string.

1. To reproduce start an ozonesecure cluster where testuser2 is not an admin. (The easiest way to achieve this is removing the hadoop.security.auth_to_local settings, as in our ozonesecure environment all users are mapped to local root which is admin)

To make the test easier, groups can also be turned off:

{code}
 <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.NullGroupsMapping</value>
  </property>
{code}

2. Check if testuser2 is not an admin:

{code}
kinit -kt /etc/security/keytabs/testuser2.keytab testuser2/scm

klist
Ticket cache: FILE:/tmp/krb5cc_1000
Default principal: testuser2/scm@EXAMPLE.COM

Valid starting     Expires            Service principal
01/29/21 13:27:03  01/30/21 13:27:03  krbtgt/EXAMPLE.COM@EXAMPLE.COM
	renew until 02/05/21 13:27:03


ozone sh volume create /vol3
PERMISSION_DENIED User testuser2/scm@EXAMPLE.COM doesn't have CREATE permission to access volume vol3 null null
{code}

3. To create a s3 type delegation token we need valid string2sign and signature strings.

{code}
ozone s3 getsecret
{code}

Set the environment variables:

{code}
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=....
{code}

Try to create a bucket (will be denied) with --debug flag:

{code}
aws s3api --debug --endpoint=http://localhost:9878 create-bucket --bucket=bucket1
{code}

Copy the signature and string2sign from the output:

{code}
2021-01-29 15:03:52,269 - MainThread - botocore.auth - DEBUG - StringToSign:
AWS4-HMAC-SHA256
20210129T140352Z
20210129/us-west-1/s3/aws4_request
ff6c0c767b0292cf3459d02ae1199d4c7786f3cca2f383a46b442f19d964d996
2021-01-29 15:03:52,269 - MainThread - botocore.auth - DEBUG - Signature:
9830423f18ac1f90ec658d1b5c47bdd7765d67fdc0dc67393c162627bfa45789
{code}

And execute a java app:

{code}
  public static void main(String[] args) throws Exception {
    OzoneConfiguration conf = new OzoneConfiguration();
    conf.set(""ozone.om.address"", ""192.168.32.6"");

    String awsAccessId = ""testuser2/scm@EXAMPLE.COM"";

    UserGroupInformation.setConfiguration(conf);

    UserGroupInformation remoteUser =
        UserGroupInformation.createRemoteUser(awsAccessId, AuthMethod.TOKEN);

    final Text omService = SecurityUtil.buildTokenService(OmUtils.
        getOmAddressForClients(conf));
    OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();
    identifier.setTokenType(S3AUTHINFO);
    identifier.setStrToSign(""AWS4-HMAC-SHA256\n""
        + ""20210129T133557Z\n""
        + ""20210129/us-west-1/s3/aws4_request\n""
        + ""8fc985d9c7442c33d6f146ab123de49b18c83c4c6ccdfd182f10fc78691bdd53"");
    identifier.setSignature(
        ""044cf03375ea10b3e454b16887a1f5ce6ebb14d45b506dd7ac5e02fd0179ba7b"");
    identifier.setAwsAccessId(awsAccessId);
    identifier.setOwner(new Text(""testuser/scm@EXAMPLE.COM""));
    Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),
        identifier.getSignature().getBytes(UTF_8),
        identifier.getKind(),
        omService);

    remoteUser.addToken(token);

    OzoneClient client = remoteUser.doAs(
        (PrivilegedExceptionAction<OzoneClient>) () -> OzoneClientFactory.getRpcClient(conf));
    client.getObjectStore().createVolume(""vol2"");

  }
{code}

As a result /vol2 is created even if testuser2 is not an admin. Note: testuser IS an admin and setOwner used testuser instead of testuser2. 

A quick fix is to validate the owner field. A proper, long-term fix is disabling the s3 auth token type for client2server communication which can be done with HDDS-4440.",pull-request-available,[],HDDS,Bug,Blocker,2021-01-29 14:06:59,6
13355443,Merge ozone-ha and ozone-om-ha-s3,"Both {{ozone-ha}} and {{ozone-om-ha-s3}} run tests in OM HA environment.  I think they can be merged, both to reduce the number of environments and to save some runtime in CI.",pull-request-available,['test'],HDDS,Improvement,Major,2021-01-29 13:52:43,1
13355329,Intermittent failure in ozone-ha acceptance test,"{{ozone-ha}} acceptance test is failing intermittently with:

{code}
'Couldn't create RpcClient protocol' does not contain 'VOLUME_NOT_FOUND'
{code}

Examples:
* https://github.com/elek/ozone-build-results/tree/master/2021/01/27/5516/acceptance-misc/
* https://github.com/elek/ozone-build-results/tree/master/2021/01/28/5536/acceptance-misc/
* https://github.com/elek/ozone-build-results/tree/master/2021/01/28/5550/acceptance-misc/",pull-request-available,['test'],HDDS,Bug,Major,2021-01-29 04:39:15,1
13355166,Adjust classpath of ozone version to include log4j,Please see: https://github.com/apache/ozone/pull/1850,pull-request-available,[],HDDS,Improvement,Major,2021-01-28 11:47:26,6
13355158,Unnecessary WARNING to set OZONE_CONF_DIR,See: https://github.com/apache/ozone/pull/1849,pull-request-available,[],HDDS,Bug,Major,2021-01-28 11:31:53,6
13355084,activatePipeline/deactivatePipeline in PipelineManagerV2Impl should acquire lock before calling StateManager#updatePipelineState.,"activatePipeline/deactivatePipeline in PipelineManagerV2Impl should acquire lock before calling StateManager#updatePipelineState.

 

This issue exist in master branch as well.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-01-28 05:34:29,48
13354715,TestOzoneFileSystem is not picking the configured 'Parameters',"In my local environment, I observed that [TestOzoneFileSystem#Init|https://github.com/apache/ozone/blob/master/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java#L119] function is called once during the lifecycle of the test. In effect, TestOzoneFileSystem all 4 parameterized iterations are running always with {{enabledFileSystemPaths=false}} and {{omRatisEnabled=false}}, which is basically due to the wrong init function using {{@BeforeClass}}. 

After searching I found discussions to add @AfterParam and @BeforeParam [Discussion_Thread|https://github.com/junit-team/junit4/issues/45] , which is again available only in 4.13 junit version [Junit_PR_With_NewParams|https://github.com/junit-team/junit4/pull/1435] but Ozone is using 4.11 junit version.",pull-request-available test,[],HDDS,Bug,Major,2021-01-26 16:36:29,40
13354714,TestOzoneFileSystem#testTrash failed when enabledFileSystemPaths and omRatisDisabled,"Presently TestOzoneFileSystem all 4 modes are running always with {{enabledFileSystemPaths=false}} and {{omRatisEnabled=false}}, which is basically due to the wrong init using {{@BeforeClass}}. I've modified the test to have proper init/teardown and noticed this failure [Build_Check|https://github.com/apache/ozone/pull/1841/checks?check_run_id=1763626625]. I will raise separate jira to correct TestOzoneFileSystem {{@BeforeClass}}.

Hits below exception during the run.

{code}

2021-01-26 21:40:00,016 [pool-25-thread-2] ERROR key.OMKeyDeleteRequest (OMKeyDeleteRequest.java:validateAndUpdateCache(188)) - Key delete failed. Volume:volume49162, Bucket:bucket42587, Key:.Trash/rakeshr/210126213900.2021-01-26 21:40:00,016 [pool-25-thread-2] ERROR key.OMKeyDeleteRequest (OMKeyDeleteRequest.java:validateAndUpdateCache(188)) - Key delete failed. Volume:volume49162, Bucket:bucket42587, Key:.Trash/rakeshr/210126213900.KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:132) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:246) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.submitRequest(TrashOzoneFileSystem.java:113) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.access$200(TrashOzoneFileSystem.java:65) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem$DeleteIterator.processKeyPath(TrashOzoneFileSystem.java:450) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem$OzoneListingIterator.iterate(TrashOzoneFileSystem.java:350) at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.delete(TrashOzoneFileSystem.java:163) at org.apache.hadoop.ozone.om.TrashPolicyOzone.deleteCheckpoint(TrashPolicyOzone.java:269) at org.apache.hadoop.ozone.om.TrashPolicyOzone.access$900(TrashPolicyOzone.java:55) at org.apache.hadoop.ozone.om.TrashPolicyOzone$Emptier.lambda$run$0(TrashPolicyOzone.java:177) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

{code}",pull-request-available,[],HDDS,Bug,Major,2021-01-26 16:21:39,26
13354660,Fix delete container occurs unknown command type.,"An exception ""Unknown command type: DeleteContainer"" occurs when we delete a container.  This is a bug that needs to be fixed.
 !screenshot-4.png! ",pull-request-available,[],HDDS,Bug,Major,2021-01-26 12:29:30,4
13354501,Make trash work with FS Optimised Buckets.,Integrate Ozone FS trash with HDDS-2939 .,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-01-25 17:37:44,26
13354439,Modularize upgrade test,"{{upgrade}} acceptance test verifies upgrade from Ozone 0.5.0 to ""snapshot"" (built from current sources).  In order to test upgrade from several earlier versions, it needs to be more modular.",pull-request-available,['test'],HDDS,Task,Minor,2021-01-25 13:32:12,1
13354390,Admin command should take effect on all SCM instance,"*Scope*

admin command includes rm start/stop and safe mode exit.

 

*Requirement*

1, When admin stops rm, rm in all SCM should stop, re-election should not trigger rm to start in the new leader.

2, When admin starts rm, only rm in leader and out of safe mode should take effect. Given leader is in safe mode, even if admin starts rm explicitly, it does not take effect.

3, This admin rm start/stop can not survive restart for a SCM instance. When admin decides to stop rm of the SCM cluster, he should pay attention if any of the SCM crashes.

 

*Status*

1, For now, admin rm start/stop will create/destroy the rm thread.

2, SCMContainerLocationFailoverProxyProvider has been proxied by FailoverProxyProvider, it will round robin SCMs in ozone.scm.names, until it is successfully handled. In ServerSide, whenever receiving a client request, it do isLeader check first, return nle to trigger fpp to failover to the next SCM.

3, SCMService decides the next iteration of rm to take effect or not by changing RUNNING and PAUSING.

 

*Solution:*

When receiving a rm stop/start request on the server side, SCM skip the isLeader check, just destroys/creates rm thread, client side fake an exception to trigger fpp to try the next SCM in a round robin way.

The Running and PAUSING status and rm start/stop can be treated separately. The admin operations and the raft status are requirements of two dimensions.

 

*We can achieve above requirements:*

1, When admin stops rm, rm in all SCM should stop, re-election should not trigger rm to start in the new leader.

Meet, admin rm start destroy rm thread in all SCM.

 

2, When admin starts rm, only rm in leader and out of safe mode should take effect. Given leader is in safe mode, even if admin starts it explicitly, rm does not take effect.

Meet, admin rm stop create rm thread in all SCM, but SCMStatus is decided by leader and safe mode.

 

3, This admin rm start/stop can not survive restart for a SCM instance. When admin decides to stop rm of the SCM cluster, he should pay attention if any of the SCM crashes.

Meet. The is actually a relax item. ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2021-01-25 11:34:43,36
13354372,Upgrade Ratis to 1.1.0-eb66796d-SNAPSHOT,Upgrade Ratis snapshot to {{1.1.0-eb66796d-SNAPSHOT}} as we need RATIS-1290 for HDDS-4730.,pull-request-available,[],HDDS,Task,Major,2021-01-25 10:41:34,1
13354191,Add ModifierOrder to checkstyle rules,"in the codebase, some places use `final static`, should use `static final`",pull-request-available,[],HDDS,Improvement,Major,2021-01-24 16:33:45,52
13354173,Intermittent failure in testExpiredCertificate,"In the test case of TestOzoneBlockTokenSecretManager.testExpiredCertificate(), there is a chance that the cert becomes not expired, which will cause the PR check failure as follows:

 
{code:java}
[INFO] Running org.apache.hadoop.ozone.security.TestOzoneBlockTokenSecretManager
Error:  Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.296 s <<< FAILURE! - in org.apache.hadoop.ozone.security.TestOzoneBlockTokenSecretManager
Error:  testExpiredCertificate(org.apache.hadoop.ozone.security.TestOzoneBlockTokenSecretManager)  Time elapsed: 0.13 s  <<< FAILURE!
java.lang.AssertionError: Expected test to throw (an instance of org.apache.hadoop.hdds.security.token.BlockTokenException and exception with message a string containing ""Block token can't be verified due to expired certificate"")
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.rules.ExpectedException.failDueToMissingException(ExpectedException.java:184)
	at org.junit.rules.ExpectedException.access$100(ExpectedException.java:85)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:170)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
{code}
In order to eliminate this failure, we can set the time of the token to a past time.

 ",pull-request-available,[],HDDS,Improvement,Major,2021-01-24 12:20:07,7
13354164,Fix typo in hdds.proto,"{code:java}
* LifeCycleState for SCM object creation state machine:
*    ->Allocated: allocated on SCM but clean has not started creating it yet.
*    ->Creating: allocated and assigned to client to create but not ack-ed yet.
{code}
The description of the state of Allocated has some typo error, the ""clean"" should be ""client"". And it could be very misleading for users.",pull-request-available,[],HDDS,Improvement,Major,2021-01-24 09:19:07,7
13354092,Remove leveldb from codebase,"LevelDB support was removed for OM, SCM, DBs. And users can no longer be able to use levedb through configuration options.

 

Related links:

https://issues.apache.org/jira/browse/HDDS-3914",pull-request-available,[],HDDS,Improvement,Major,2021-01-23 10:58:28,52
13353904,Fix compatibility issue caused by new add enum DatanodeDetails.Port.Name.REPLICATION,"In tencent, we monthly deploy latest master to our production environment.

When running an upgrade test from old ozone version (at Dec 14 2020) to new ozone version (at Jun 14 2021), testDFSIO using jars of old ozone can not write to the new ozone cluster, met the error that 

No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION

We consider it is related to HDDS-4496. Separate client and server2server GRPC services of datanode.

 ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2021-01-22 05:37:51,1
13353671,Use separate Ratis admin and client ports,Use separate client/server/admin ports being introduced in RATIS-1290.,pull-request-available,[],HDDS,Improvement,Major,2021-01-21 07:51:24,1
13353630,Add token support for container admin operations,"HDDS-2321 disabled token based authentication for container admin commands part of the DataNode admin protocol as that caused problems with requests that are not going through Ozone Manager, as token based auth support is present only there currently.

Within this feature, the followings to be added:
- a new SCM request to get a new kind of token issued by the SCM
- the token would be short living, without renewal or cancellation signed by SCM
- the token will be required for container admin commands inside DataNodes
- the token will be supplied to container admin requests from command line client, and for commands arriving via DN heartbeat responses
- the token is validated on the DN side for every container admin command, and in case a token is not supplied or invalid the DN should reject the request.

Also it is part of the development to revisit all DN API requests and add the appropriate (OM or SCM) token based auth where applicable.",pull-request-available,[],HDDS,New Feature,Major,2021-01-21 02:36:19,1
13353548,ozone debug chunkinfo is not working correctly,The ozone debug chunkinfo command fails with the error {{Missing required parameter: '<uri>'}},pull-request-available,[],HDDS,Bug,Major,2021-01-20 14:44:38,26
13353530,[FSO]OMConfig: Modify 'ozone.om.layout.version' config,"Improve config name {{""ozone.om.layout.version"" to enable/disable feature.}}",pull-request-available,[],HDDS,Sub-task,Major,2021-01-20 13:21:37,26
13353493,Debug utility to export container data,Please see: https://github.com/apache/ozone/pull/1825,pull-request-available,[],HDDS,Improvement,Major,2021-01-20 09:34:39,6
13353487,Change metrics unit from nanosecond to millisecond,"Some metrics unit is nanosecond, and some is millisecond. The metrics are following

{quote}

EventWatcherMetrics   ms

ProtocolMessageMetrics ns

CSMMetrics ns

ContainerMetrics ns

XceiverClientMetrics ns

OzoneManagerDoubleBufferMetrics ns

OzoneManagerSyncMetrics ns

ContainerCacheMetrics ms

{quote}

I see that the most metrics of OM, SCM, DN are the order of millisecond, so I think we should change unit from nanosecond to millisecond uniformly ",pull-request-available,[],HDDS,Improvement,Major,2021-01-20 09:07:30,53
13353486,Revert column family names of Datanode db to avoid compatibility issue.,"Tencent monthly deploy the latest master to our production environment.

During latest upgrade test, we found that new Ozone(version at Jan 14 2021) can not read the data written by old Ozone(version at Dec 14 2020).

We found following traces
{code:java}
2021-01-20 16:36:39,575 INFO org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet: Added Volume : /data4/hdds/hdds to VolumeSet
2021-01-20 16:36:39,582 INFO org.apache.hadoop.ozone.container.common.volume.ThrottledAsyncChecker: Scheduling a check for /data4/hdds/hdds
2021-01-20 16:36:39,594 INFO org.apache.hadoop.ozone.container.common.volume.HddsVolumeChecker: Scheduled health check for volume /data4/hdds/hdds
2021-01-20 16:36:39,595 INFO org.apache.hadoop.ozone.container.common.volume.ThrottledAsyncChecker: Scheduling a check for /data1/hdds/hdds
2021-01-20 16:36:39,595 INFO org.apache.hadoop.ozone.container.common.volume.HddsVolumeChecker: Scheduled health check for volume /data1/hdds/hdds
2021-01-20 16:36:39,595 INFO org.apache.hadoop.ozone.container.common.volume.ThrottledAsyncChecker: Scheduling a check for /data2/hdds/hdds
2021-01-20 16:36:39,596 INFO org.apache.hadoop.ozone.container.common.volume.HddsVolumeChecker: Scheduled health check for volume /data2/hdds/hdds
2021-01-20 16:36:39,596 INFO org.apache.hadoop.ozone.container.common.volume.ThrottledAsyncChecker: Scheduling a check for /data3/hdds/hdds
2021-01-20 16:36:39,596 INFO org.apache.hadoop.ozone.container.common.volume.HddsVolumeChecker: Scheduled health check for volume /data3/hdds/hdds
2021-01-20 16:36:39,793 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,794 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,794 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,797 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,916 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,916 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,916 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:39,927 INFO org.apache.hadoop.hdds.utils.db.RDBStore: Found the following extra column families in existing DB : [block_data, deleted_blocks]
2021-01-20 16:36:40,001 WARN org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil: Attempt to get an uncached RocksDB handle failed and an instance was retrieved from the cache. This should only happen in tests
2021-01-20 16:36:40,006 WARN org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil: Attempt to get an uncached RocksDB handle failed and an instance was retrieved from the cache. This should only happen in tests
2021-01-20 16:36:40,013 WARN org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil: Attempt to get an uncached RocksDB handle failed and an instance was retrieved from the cache. This should only happen in tests
2021-01-20 16:36:40,018 WARN org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil: Attempt to get an uncached RocksDB handle failed and an instance was retrieved from the cache. This should only happen in tests
{code}
The name changing of the datanode column families done in HDDS-4369 is actually not a backward compatible.

This PR aims to revert the name from deletedBlocks, deleteTxns, blockData to block_data, deleted_blocks, delete_txns.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-01-20 09:06:59,36
13353134, Creating RDBStore fails due to RDBMetrics instance race,"I am using Ozone APIs to create containers, and it occasionally aborts due to a data race in acessing the RBDMetric instance:
{noformat}
2021-01-09 02:39:36,944 [pool-1-thread-4] INFO keyvalue.KeyValueContainer: Container 318054 is closed with bcsId 0.
2021-01-09 02:39:36,988 [pool-1-thread-17] ERROR freon.BaseFreonGenerator: Error on executing task 318048
com.google.common.util.concurrent.UncheckedExecutionException: org.apache.hadoop.metrics2.MetricsException: Metrics source RDBMetrics already exists!
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3951)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
        at org.apache.hadoop.ozone.freon.ContainerGenerator.lambda$writeContainer$1(ContainerGenerator.java:489)
        at com.codahale.metrics.Timer.time(Timer.java:101)
        at org.apache.hadoop.ozone.freon.ContainerGenerator.writeContainer(ContainerGenerator.java:485)
        at org.apache.hadoop.ozone.freon.BaseFreonGenerator.tryNextTask(BaseFreonGenerator.java:189)
        at org.apache.hadoop.ozone.freon.BaseFreonGenerator.taskLoop(BaseFreonGenerator.java:169)
        at org.apache.hadoop.ozone.freon.BaseFreonGenerator.lambda$startTaskRunners$0(BaseFreonGenerator.java:152)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source RDBMetrics already exists!
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
        at org.apache.hadoop.hdds.utils.db.RDBMetrics.create(RDBMetrics.java:47)
        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:152)
        at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:191)
        at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:128)
        at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:103)
        at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaTwoImpl.<init>(DatanodeStoreSchemaTwoImpl.java:48)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.createContainerMetaData(KeyValueContainerUtil.java:112)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:133)
        at org.apache.hadoop.ozone.freon.ContainerGenerator.createContainer(ContainerGenerator.java:463)
        at org.apache.hadoop.ozone.freon.ContainerGenerator.access$100(ContainerGenerator.java:109)
        at org.apache.hadoop.ozone.freon.ContainerGenerator$ContainerCreator.load(ContainerGenerator.java:357)
        at org.apache.hadoop.ozone.freon.ContainerGenerator$ContainerCreator.load(ContainerGenerator.java:353)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
{noformat}

Looking at the code, I believe RDBMetrics#unRegister() should be made synchronized. Otherwise create and close RDBStore objects could lead to race of the RDBMetrics instance object.

After making RDBMetrics#unRegister() synchronized, the tool no longer aborts due to the race.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2021-01-18 20:53:00,25
13353044,RenameKey : add unit test to verify bucket#renameKey,"This task is to verify the functionality of {{ozonebucket#renameKey}} client API. Basically the existing {{o3fs#rename(path, path)}} logic is internally invoking bucket#renameKey.

Need to add UTs and fix functional gap, if any.",pull-request-available,[],HDDS,Sub-task,Major,2021-01-18 10:19:54,40
13353013,Fix TestOzoneFileSystemV1 and TestObjectStoreV1 cases,"Following test cases are failing.

 
{code:java}
Error:  Failures: 
Error:    TestObjectStoreV1.testCreateKey:143->verifyKeyInFileTable:374 Invalid Key: -9223372036854774526/file1 expected:<-922337203685477[0429/key01695]> but was:<-922337203685477[4526/file1]>
Error:    TestObjectStoreV1.testLookupKey:212->verifyKeyInFileTable:374 Invalid Key: -9223372036854770429/key01695 expected:<-92233720368547[68381/key05336]> but was:<-92233720368547[70429/key01695]>
{code}
{code:java}
Error:  testFileSystem[3](org.apache.hadoop.fs.ozone.TestOzoneFileSystemV1)  Time elapsed: 28.327 s  <<< FAILURE!
java.lang.AssertionError: Delete root failed! expected:<0> but was:<1>
{code}
[Reference-TestObjectStoreV1|https://github.com/apache/ozone/pull/1811/checks?check_run_id=1716517031]
 [Reference-TestOzoneFileSystemV1|https://github.com/apache/ozone/pull/1811/checks?check_run_id=1716517021]",pull-request-available,[],HDDS,Sub-task,Major,2021-01-18 08:36:10,40
13352955,OzoneQuota change BYTES to B,OzoneQuota change BYTES to B. It's easier to use.,pull-request-available,[],HDDS,Sub-task,Minor,2021-01-18 03:46:34,4
13352768,Add build-helper-maven-plugin to interface-client module,"In this module, sources files are generated by protobuf  plugin. But when other modules depend on this module, idea always prompts ""Cannot resolve symbol"" message.",pull-request-available,[],HDDS,Improvement,Major,2021-01-17 11:13:56,52
13352452,Pipeline: chooseDatanodes function  random pick not working,"I was trying to configure a four-node cluster to test SCM HA, after configuring the ozone.datanode.pipeline.limit to 3 and 5 separately, the pipeline existed only chose the first three data nodes, the fourth node was never chosen.(They are equal nodes)

The bug here is due to that the method *{color:#172b4d}+chooseDatanodes+{color}* in PipelinePlacementPolicy calls *+super.getResultSet+*, in which another method +{color:#172b4d}*chooseNode*{color}+ is called. This +*chooseNode*+ method is an abstract method in the superclass and is overridden in PipelinePlacementPolicy. The overridden version uses a simple logic instead of a randomly chosen policy which incurs my problem.

 

 
 * !image-2021-01-15-17-46-31-492.png!

 ",pull-request-available,[],HDDS,Bug,Major,2021-01-15 10:00:40,48
13352416,Optimization: update RetryCount less frequently (update once per ~100),"SCM maintains a DeleteBlockTransaction table [1]. For each transaction record in this table, there is a retry count [2]. This retry count increases every time when SCM retries the delete transaction and until it exceeds the maximum limit, then SCM stops retrying and admin can analyze why some blocks fail to delete.

Because the count is written into DB every time upon retries, I want to discuss whether it is worth an optimization that we can maintain the retry count as an in-memory state and we only write to DB when the retry count exceeds the limit (thus to leave for further analysis).

The reason for this idea is in SCM HA we are replicating DB changes over Ratis, and still persist retry count for every increase will have 3x cost compared to now. 

The drawback of only updating retrycount at the limit is, if SCM restart at a time, the retry count will be cleared and restart to count.


[1]: https://github.com/apache/ozone/blob/master/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/SCMMetadataStore.java#L70
[2]: https://github.com/apache/ozone/blob/master/hadoop-hdds/interface-server/src/main/proto/ScmServerDatanodeHeartbeatProtocol.proto#L331",pull-request-available,[],HDDS,Improvement,Major,2021-01-15 07:02:21,54
13352398,"Failed to generate reports using ""mvn site""","Failed to run ""mvn site"". Following error is reported:
{code}
 [INFO] --- maven-site-plugin:3.3:site (default-site) @ hadoop-main-ozone ---
[WARNING] Report plugin org.apache.maven.plugins:maven-project-info-reports-plugin has an empty version.
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[INFO] configuring report plugin org.apache.maven.plugins:maven-project-info-reports-plugin:3.1.1
[WARNING] Error injecting: org.apache.maven.report.projectinfo.CiManagementReport
java.lang.NoClassDefFoundError: org/apache/maven/doxia/siterenderer/DocumentContent
    at java.lang.Class.getDeclaredConstructors0 (Native Method)
    at java.lang.Class.privateGetDeclaredConstructors (Class.java:2671)
    at java.lang.Class.getDeclaredConstructors (Class.java:2020)
    at com.google.inject.spi.InjectionPoint.forConstructorOf (InjectionPoint.java:245)
    at com.google.inject.internal.ConstructorBindingImpl.create (ConstructorBindingImpl.java:115)
    at com.google.inject.internal.InjectorImpl.createUninitializedBinding (InjectorImpl.java:706)
    at com.google.inject.internal.InjectorImpl.createJustInTimeBinding (InjectorImpl.java:930)
    at com.google.inject.internal.InjectorImpl.createJustInTimeBindingRecursive (InjectorImpl.java:852)
    at com.google.inject.internal.InjectorImpl.getJustInTimeBinding (InjectorImpl.java:291)
    at com.google.inject.internal.InjectorImpl.getBindingOrThrow (InjectorImpl.java:222)
    at com.google.inject.internal.InjectorImpl.getProviderOrThrow (InjectorImpl.java:1040)
    at com.google.inject.internal.InjectorImpl.getProvider (InjectorImpl.java:1071)
    at com.google.inject.internal.InjectorImpl.getProvider (InjectorImpl.java:1034)
    at com.google.inject.internal.InjectorImpl.getInstance (InjectorImpl.java:1086)
    at org.eclipse.sisu.space.AbstractDeferredClass.get (AbstractDeferredClass.java:48)
    at com.google.inject.internal.ProviderInternalFactory.provision (ProviderInternalFactory.java:85){code}
According to CRUNCH-671, it's caused by the missing of maven-site-plugin.",pull-request-available,[],HDDS,Bug,Major,2021-01-15 04:00:01,7
13352346,Add permission check in OMDBCheckpointServlet,TBA,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2021-01-14 20:02:24,12
13352284,Add JUnit5 dependency for integration tests,JUnit5 makes it easier to [run tests multiple times|https://junit.org/junit5/docs/current/user-guide/#writing-tests-repeated-tests] via {{\@RepeatedTest(N)}} annotation.  Adding JUnit5 dependency for integration tests would allow upgrading flaky tests one by one for easier debug.,pull-request-available,['test'],HDDS,Task,Major,2021-01-14 15:18:28,1
13352283,Remove unused OzoneClientUtils,"{{OzoneClientUtils}} is unused, can be removed.",pull-request-available,['Ozone Client'],HDDS,Task,Major,2021-01-14 15:18:13,55
13352273,Upgrade Java for Sonar check,"bq. The version of Java installed in the scanner environment must be upgraded to at least Java 11 before 1 February 2021. Pre-11 versions of Java are already deprecated and scanners using them will stop functioning on that date. ([source|https://sonarcloud.io/documentation/appendices/end-of-support/]

https://github.com/apache/ozone/runs/1701483011#step:6:2695",pull-request-available,['build'],HDDS,Task,Major,2021-01-14 14:25:52,1
13352072,Handle CRLStatusReport got from DN heartbeats and persist them,We should store the state of CRLStatus of each DN in SCM. This can be done by handling the CRLStatusReport message in heartbeats received from DNs.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2021-01-13 18:30:11,37
13352054,[FSO]Authorizer: OM can do recursive ACL check for subpaths,This task is to make recursive invocation of IAccessAuthorizer#checkAccess() function in the OM code. Its basically to ensure recursive acl check from OM side and this logic can be removed once Native and Ranger implementations are done.,pull-request-available,[],HDDS,Sub-task,Major,2021-01-13 17:11:14,26
13352029,Update Hadoop version to 3.2.2,"Hadoop 3.2.2 contains not only bug fixes but (as [~kuenishi] showed me) HADOOP-16935 is included which fixes the annoying warning about reflection / Keberos API.

As it's a minor update (from Hadoop 3.2.1 to Hadoop 3.2.2) it should be safe to do.",newbie pull-request-available,[],HDDS,Improvement,Major,2021-01-13 14:51:17,7
13352025,Disable compression for closed-container replication,"During the measurement of closed container replication I found that the biggest bottleneck is the read side. 5 Gb container is replicated under ~3 minutes but ~2:30 was the downloading part.

Closed containers are replicated via GRPC. The source side creates an OutputStream on-the-fly (OnDemandContainerReplicationSource.java) and stream all the container content as a ""tar.gz"" archive to the client.

It turned out that the compression (the .gz part) is quite expensive:

I created a CLI tool to export containers to tar files (same logic as the replication but without streaming via GRPC, just saving to a file).

I have seen the 2:30 time to create the archive:

{code}
2021-01-13 05:51:25,302 [main] INFO debug.ExportContainer: Preparation is done
2021-01-13 05:53:53,472 [main] INFO debug.ExportContainer: Container is exported to /tmp/container-3.tar.gz
{code}

But when I removed the compression in TarContainerPacker.java, the speed was significant better (25 sec instead of the 150 sec)

{code}
2021-01-13 06:11:46,254 [main] INFO debug.ExportContainer: Preparation is done
2021-01-13 06:12:11,512 [main] INFO debug.ExportContainer: Container is exported to /tmp/container-3.tar
{code}

As a result I suggest turning off the compression for closed container replication.",pull-request-available,[],HDDS,Improvement,Critical,2021-01-13 14:32:33,6
13351914,[FSO]ListKeys: do lookup in dir and file tables,"This task is to perform #listKeys by searching the {{startKey}}, {{keyPrefix}} in Dir & File Tables.
{code:java}
ObjectStore API: ListKeys{code}",pull-request-available,[],HDDS,Sub-task,Major,2021-01-13 03:29:04,40
13351905,SCM webui display wrong Node counts,"After merge git 92369fcfaedfee29f212b8998ef33915a249d6fd , Node counts display wrong info

 

!image-2021-01-13-10-24-57-807.png!

 ",pull-request-available,[],HDDS,Bug,Major,2021-01-13 02:25:04,55
13351890,Change default OM Node ID from UUID to a constant,"If a nodeID is not set explicitly (for a single node OM cluster), then it defaults to the OM storage ID which is an UUID string. Proposing to change this default to a constant (such as ""om1"") instead. 
This would help when a cluster is upgraded to HA for example. It is not straightforward to change the nodeID after Ratis server has been instantiated (as the nodeID is used to generate the RaftPeerID). Hence, it would be good to have a more readable nodeID by default. ",pull-request-available,[],HDDS,Bug,Major,2021-01-13 00:02:29,43
13351866,Intermittent failure in TestSCMRestart,"[~shashikant], I think this is caused by enabling multi-raft and creating more pipelines:

{code:title=https://github.com/elek/ozone-build-results/blob/master/2021/01/12/5149/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.txt}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 31.976 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart
testPipelineWithScmRestart(org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart)  Time elapsed: 0.01 s  <<< FAILURE!
java.lang.AssertionError: expected:<PipelineID=84d01728-7484-4ac8-9900-94ffbef8c967> but was:<PipelineID=c057a116-1d16-4def-985c-1c39482078eb>
  ...
  at org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.testPipelineWithScmRestart(TestSCMRestart.java:127)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2021-01-12 21:20:17,16
13351790,Support scanning content of DN rocksdb instances with current scheme.,Please see: https://github.com/apache/ozone/pull/1786,pull-request-available,[],HDDS,Improvement,Major,2021-01-12 14:35:05,6
13351684,Bucket usedBytes inaccurate because of preallocated blocks,"Currently,  for every createKey request,  OM will preallocate a block for the key and returned in the createKey response.  When commit empty key, client will filter out the block location which doesn't have data, so OM doesn't have the chance to remove the preallocated block size from bucket's usedbytes statitics. ",pull-request-available,[],HDDS,Bug,Major,2021-01-12 08:32:26,5
13351683,NodeStateMap leaks internal representation of container sets,Please see: https://github.com/apache/ozone/pull/1782,pull-request-available,[],HDDS,Improvement,Major,2021-01-12 08:30:55,6
13351673,Add warning log when old volume and bucket set quota,"https://github.com/apache/ozone/blob/master/hadoop-hdds/docs/content/feature/Quota.md
 !image-2021-01-12-15-53-36-175.png! 

Currently we only advise users in the doc document not to enable quota for old volumes and buckets. Sometimes the user might not notice, so we added a warning on the client side.
 !image-2021-01-12-15-55-41-312.png! ",pull-request-available,[],HDDS,Sub-task,Major,2021-01-12 07:56:43,4
13351630,Use mvn to get ozone.version instead of  complex shell commands,"In acceptance.sh, blockade.sh & kubernetes.sh
I see 
{quote}
OZONE_VERSION=$(grep ""<ozone.version>"" ""pom.xml"" | sed 's/<[^>]*>//g'|  sed 's/^[ \t]*//')
{quote}

It seems that using mvn to get ozone.version is more clear.
{quote}
OZONE_VERSION=$(mvn help:evaluate -Dexpression=ozone.version -q -DforceStdout)
{quote}",pull-request-available,['build'],HDDS,Improvement,Major,2021-01-12 03:29:04,53
13351626,The ozone script should set ratis options for om and scm,"In the ozone script (hadoop-ozone/dist/src/shell/ozone/ozone), it sets ratis (netty) options for datanode.  It should also set the options for om and scm.",pull-request-available,"['OM', 'SCM']",HDDS,Improvement,Major,2021-01-12 02:54:07,9
13351586,TestOzoneFileSystem#testTrash() fails when OM Ratis is enabled,"TestOzoneFileSystem#testTrash() is failing when OM Ratis is enabled. This is happening because of the following error:
{noformat}
2021-01-11 11:02:25,451 [pool-29-thread-1] INFO  om.TrashPolicyOzone (TrashPolicyOzone.java:lambda$run$0(181)) - Unable to checkpoint
java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:128)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.createWriteRaftClientRequest(OzoneManagerRatisServer.java:149)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.submitRequest(OzoneManagerRatisServer.java:113)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:182)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:144)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122)
	at org.apache.hadoop.ozone.om.TrashOzoneFileSystem$RenameIterator.processKeyPath(TrashOzoneFileSystem.java:354)
	at org.apache.hadoop.ozone.om.TrashOzoneFileSystem$OzoneListingIterator.iterate(TrashOzoneFileSystem.java:314)

	at org.apache.hadoop.ozone.om.TrashOzoneFileSystem.rename(TrashOzoneFileSystem.java:124)Disconnected from the target VM, address: '127.0.0.1:57683', transport: 'socket'
	at org.apache.hadoop.ozone.om.TrashPolicyOzone.createCheckpoint(TrashPolicyOzone.java:230)
	at org.apache.hadoop.ozone.om.TrashPolicyOzone.access$1000(TrashPolicyOzone.java:55)
	at org.apache.hadoop.ozone.om.TrashPolicyOzone$Emptier.lambda$run$0(TrashPolicyOzone.java:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
	at java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
{noformat}
The Filesystem which is calling the rename is using the DUMMY_CLIENT_ID and hence the Precondition fails in {{OzoneManagerRatisServer.createWriteRaftClientRequest}}. 
To reproduce, set ozone.om.ratis.enable to true in {{TestOzoneFileSystem#setupOzoneFileSystem}}.",pull-request-available,[],HDDS,Sub-task,Blocker,2021-01-11 20:59:48,26
13351450,BlockInputStream should give up read retry if pipeline is not updated,"Found it during the usage of a data generator.

 1. I accidentally uploaded keys without checksum data.

  2. With this specific key, the client is moved to an endless loop instead of giving up after the first unexpected exceptions:

{code}
2021-01-11 13:01:50,031 INFO  storage.BlockInputStream (BlockInputStream.java:refreshPipeline(166)) - Unable to read information for block conID: 2 locID: 185 bcsId: 0 from pipeline PipelineID=206da15d-62f6-4e24-93d1-e2e805fc1376: Unexpected OzoneException: org.apache.hadoop.ozone.common.OzoneChecksumException: Original checksumData has no checksums
2021-01-11 13:01:50,047 ERROR scm.XceiverClientGrpc (XceiverClientGrpc.java:sendCommandWithRetry(408)) - Failed to execute command cmdType: ReadChunk
traceID: """"
containerID: 2
datanodeUuid: ""2c124e08-e8a5-4493-a41e-84797984e6a6""
readChunk {
  blockID {
    containerID: 2
    localID: 185
    blockCommitSequenceId: 0
  }
  chunkData {
    chunkName: ""chunk0""
    offset: 0
    len: 4194304
    checksumData {
      type: CRC32
      bytesPerChecksum: 1048576
    }
  }
}
 on the pipeline Pipeline[ Id: 7d5ed2da-7453-4113-b766-4100458dcc16, Nodes: 2c124e08-e8a5-4493-a41e-84797984e6a6{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:, CreationTimestamp2021-01-11T12:01:50.032Z].
2021-01-11 13:01:50,047 INFO  storage.BlockInputStream (BlockInputStream.java:refreshPipeline(166)) - Unable to read information for block conID: 2 locID: 185 bcsId: 0 from pipeline PipelineID=7d5ed2da-7453-4113-b766-4100458dcc16: Unexpected OzoneException: org.apache.hadoop.ozone.common.OzoneChecksumException: Original checksumData has no checksums
2021-01-11 13:01:50,062 ERROR scm.XceiverClientGrpc (XceiverClientGrpc.java:sendCommandWithRetry(408)) - Failed to execute command cmdType: ReadChunk
traceID: """"
containerID: 2
datanodeUuid: ""2c124e08-e8a5-4493-a41e-84797984e6a6""
readChunk {
  blockID {
    containerID: 2
    localID: 185
    blockCommitSequenceId: 0
  }
  chunkData {
    chunkName: ""chunk0""
    offset: 0
    len: 4194304
    checksumData {
      type: CRC32
      bytesPerChecksum: 1048576
    }
  }
}
 on the pipeline Pipeline[ Id: 3a4b5032-6b2f-4297-8c4b-89d715175bb1, Nodes: 2c124e08-e8a5-4493-a41e-84797984e6a6{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:, CreationTimestamp2021-01-11T12:01:50.048Z].
{code}

Please note that the two attempt happens in the same milliseconds.

The problematic part seems to be in the BlockInputStream:

{code}
      try {
        numBytesRead = current.read(b, off, numBytesToRead);
      } catch (IOException e) {
        handleReadError(e);
        continue;
      }
{code}

In case of system exceptions we should ""break"" from the loop instead of ""continue"".

(Normally it's not possible in a production cluster as the data is created with a bad client. But it has security implication: a malicious user can create similar keys which makes a DoS attack: all the clients will retry without sleep...)",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2021-01-11 12:14:34,1
13351375,Fix when update quota the usedBytes and namespace will become to 0,"When update quota the usedBytes and namespace will become to 0. This is a bug that needs to be fixed.
 !image-2021-01-11-14-51-04-460.png! ",pull-request-available,[],HDDS,Sub-task,Major,2021-01-11 06:49:47,4
13351314,"Missing ""Expose any volume"" content in S3.md (ZH Docs)","Missing `Expose any volume` content in ZH version of S3 docs.

 !image-2021-01-11-01-12-32-602.png! ",pull-request-available,['documentation'],HDDS,Improvement,Minor,2021-01-10 17:12:42,17
13351283,Fix typo in S3.md," 

It seems that there are some typos in the example script for ""Expose any volume""

[https://github.com/apache/ozone/blob/master/hadoop-hdds/docs/content/interface/S3.md#expose-any-volume]
{code:java}
ozone sh create volume /s3v
ozone sh create volume /vol1

ozone sh create bucket /vol1/bucket1
ozone sh bucket link /vol1/bucket1 /s3v/common-bucket
{code}
IIRC, it should be `ozone sh volume create` and `ozone sh bucket create` 

 ",pull-request-available,['documentation'],HDDS,Improvement,Minor,2021-01-10 06:43:35,56
13351159,Merge OMTransactionInfo with SCMTransactionInfo,"Requirements:
1. Rename OMTransactionInfo to TransactionInfo (use by both OM and SCM), same for the codec.
2. Add extra functions in https://github.com/apache/ozone/blob/HDDS-2823/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMTransactionInfo.java to OMTransactionInfo.

By doing so, we can use the TransactionInfo in HDDS-2823 and remove duplicate code.",pull-request-available,[],HDDS,Sub-task,Major,2021-01-08 20:15:40,16
13351097,No cleanup in TestOzoneFSWithObjectStoreCreate,TestOzoneFSWithObjectStoreCreate does not clean up mini clusters.,pull-request-available,['test'],HDDS,Bug,Major,2021-01-08 14:12:56,1
13351030,LookupKey: do lookup in dir and file tables,"This task is to perform look up of the user given key path in the directory, file and openFile tables.

{code:java}
ObjectStore API: LookupKey   
{code}",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2021-01-08 07:44:21,40
13350958,Support TDE for MPU Keys on Encrypted Buckets,This Jira is to support MPU on encrypted buckets.,pull-request-available,[],HDDS,New Feature,Major,2021-01-07 19:34:39,13
13350812,Implement Distributed Sequence ID Generator ," 
 * SCM allocates ids in a batch way. It maintains three fields: firstId, lastId and nextId. It saves the lastId in rocksDB, saves firstId and lastId in memory. The initial value of nextId is firstId.
 * When getNextId() is called, if nextId is less than lastId, SCM just returns nextId and increases it, otherwise it calls allocateBatch(expectedLastId, newLastId) to allocate a batch of ids, then serves the request.
 * In allocateBatch(expectedLastId, newLastId) , it works in CAS way:
 * it reads lastId from rocksDB.
 * if lastId equals expectedLastId, it saves newLastId into rocksDB, returns success, otherwise it rejects the allocation request.
 * It also provides a getLastId() to read lastId from rocksDB. The allocation works in a loop way:",pull-request-available,[],HDDS,Sub-task,Major,2021-01-07 06:49:27,36
13350807,Refactor ACL operation error handling ,"Currently,  ACL operation has three results, boolean success, boolean failure because of ACL not exists(remove ACL case) or ACL already exists(add ACL case), exception because of other reasons, such as no enough permission. 

The goal of this task is to use the exception to replace the boolean failure. 

",pull-request-available,[],HDDS,Improvement,Major,2021-01-07 06:16:25,17
13349245,Starting OM with the --upgrade flag should delete the prepare marker file.,"When an OM is put in prepare mode, a marker file is written to disk so that it remains in prepare mode upon restart. After an OM has been upgraded, it can be restarted with a --upgrade flag to automatically delete the marker file and take the OM out of prepare mode.",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2021-01-05 17:21:11,22
13349193,Block token verification failed: no READ permission for WriteChunk,"With HDDS-4558 committed, secure acceptance test logs increased considerably (over 1GB).

https://github.com/apache/ozone/actions/runs/462095579

I think the root cause is that {{WriteChunk}} request may need to also {{ReadChunk}}, but now it fails because it only has write access:

{code}
datanode_3  | 2021-01-05 10:41:23,067 [ChunkWriter-1-0] INFO impl.HddsDispatcher: Operation: ReadChunk , Trace ID:  , Message: Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission , Result: BLOCK_TOKEN_VERIFICATION_FAILED , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:214)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
datanode_3  | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:398)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:585)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$read$5(ContainerStateMachine.java:656)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.hdds.security.token.BlockTokenException: Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission
datanode_3  | 	at org.apache.hadoop.hdds.security.token.BlockTokenVerifier.verify(BlockTokenVerifier.java:131)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.validateBlockToken(HddsDispatcher.java:431)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:211)
datanode_3  | 	... 10 more
datanode_3  | 2021-01-05 10:41:23,083 [ChunkWriter-1-0] ERROR ratis.ContainerStateMachine: gid group-5BCDF056E270 : ReadStateMachine failed. cmd ReadChunk logIndex 4 msg : Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission Container Result: BLOCK_TOKEN_VERIFICATION_FAILED
{code}

CC [~elek] [~xyao]",pull-request-available,['Security'],HDDS,Bug,Critical,2021-01-05 12:30:19,1
13349013,OM handle expired certificate when verify token signature,Similar to HDDS-4571 but on OM side. ,pull-request-available,[],HDDS,Sub-task,Major,2021-01-04 22:35:41,28
13348935,Intermittent failure in MapReduce test due to existing output file,"MapReduce acceptance test may fail if the same random number is generated for both O3FS and OFS tests:

{noformat}
<msg timestamp=""20210104 11:37:50.503"" level=""INFO"">${random} = 58</msg>
...
<msg timestamp=""20210104 11:37:50.519"" level=""INFO"">${result} = o3fs://bucket1.volume1.om/wordcount-58.txt</msg>
...
<msg timestamp=""20210104 11:39:16.274"" level=""INFO"">${random} = 58</msg>
...
<msg timestamp=""20210104 11:39:16.295"" level=""INFO"">${result} = ofs://om/volume1/bucket1/wordcount-58.txt</msg>
...
FileAlreadyExistsException: Output directory ofs://om/volume1/bucket1/wordcount-58.txt already exists
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2021-01-04 12:29:12,1
13348906,Safemode wait may end without checking,"Intermittent failure in secure acceptance test:

{noformat:title=output}
Wait for safemode exit                                                | FAIL |
255 != 0
{noformat}

{noformat:title=log}
Running command 'ozone admin safemode wait -t 2 2>&1'.
${rc} = 255
${output} = WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Safe mode is not ended within the timeout period.
{noformat}

This is executed with safemode already OFF.  Also, notice that neither {{SCM is out of safe mode}}, nor {{SCM is in safe mode}} is shown.

It indicates that {{ozone admin safemode wait}} may conclude without actually checking status, if it takes more time to create SCM client than the given timeout:

{code:title=https://github.com/apache/ozone/blob/aacba62a1a1bdee45dad15e82ef64019ff67b767/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SafeModeWaitSubcommand.java#L60-L62}
    while (getRemainingTimeInSec() > 0) {
      try (ScmClient scmClient = scmOption.createScmClient()) {
        while (getRemainingTimeInSec() > 0) {
{code}

It is reproducible by adding {{sleep}} in {{createScmClient()}}.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2021-01-04 10:02:49,1
13348862,Move OFSPath to a common package to be used by the TrashFileSystem.,Move OFSPath to a common package so that it would be accessible on the server (OM) to be used by the TrashFileSystem.,pull-request-available,[],HDDS,Sub-task,Major,2021-01-04 05:48:00,26
13348432,Add term into SetNodeOperationalStateCommand.,"We need add term into SCMCommand to help DN distinguish SCMCommand from stale leader.

SetNodeOperationalStateCommand is a new added SCMCommand by Decommission, which misses the adding of the term.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-30 08:42:58,36
13348398,Fixed client set quota with 0,"Currently setting Quota is invalid if set to 0. It's actually going to be set to -1.
{code:java}
store.getVolume(volumeName).getBucket(bucketName).setQuota(
        OzoneQuota.parseQuota(""0GB"", 0));
    Assert.assertEquals(-1, bucket.getQuotaInBytes());
    Assert.assertEquals(-1, bucket.getQuotaInNamespace());
{code}
This behavior is actually incorrect, and In general setting quota to 0 isn't a valid entry even in HDFS. we can be consistent with HDFS on this part.：
{code:java}
java.lang.IllegalArgumentException: Invalid values for quota : 0
{code}

This problem occurs because the original client quota type is long. If the user does not specify quota, the default value for this field is 0.
A backend validation fails to verify that the user did not specify quota or set quoat to 0, since both actions are passed to the backend to be 0.
So it would be more appropriate to change the long to a string. If the user does not specify quota, it is null. It's easier to judge.",pull-request-available,[],HDDS,Sub-task,Major,2020-12-30 07:18:17,4
13348246,Solve deadlock triggered by PipelineActionHandler.,"This dead lock is found when trying to replace the MockRatisServer with single server SCMRatisServer in MiniOzoneCluster.

It can be reproduced by case TestContainerStateMachineFlushDelay#testContainerStateMachineFailures, when replacing the mock ratis server with the real one.

 

*The root cause is*

when close a pipeline, it will first close the open containers of this pipeline, then remove the pipeline. The contention here is:
 # ContainerManager has committed the log entry that containing updateContainerState, and the StateMachineUpdater is applying this method, waiting for the lock of PipelineManagerV2Impl. Since when a container transitions from open to un-open, it needs to call PipelineManager#removeContainerFromPipeline, thus need the lock of PipelineManagerV2Impl.
 # In PipelineActionHander, it has acquired the lock of PipelineManagerV2Impl during the call of PipelineManagerV2Impl#removePipeline(), and it is waiting for StateManager#removePipeline() to be committed by raft and applied by StateMachineUpdater.

thus, ContainerManager occupy StateMachineUpdater, and waiting for the lock of PipelineManager, PipelineActionHander acquire the lock of PipelineManager, and waiting for StateMachineUpdater to apply its raft client request.

 

*The solution is*

We have PipelineManager and PipelineStateManager, ContainerManager and ContainerStateManager, each has its own rw lock.

Let's discuss about PipelineManager and PipelineStateManager first.

 

PipelineStateManager contains the in-memory state and the rocksdb. It use a rw lock to ensure the consistency of the in-memory state and rocksdb. This is done in this PR: [https://github.com/apache/ozone/pull/1676]

The write request needs acquire the write lock before do modification, and the read request needs acquire the read lock before read. All the write request are from StateMachineUpdater, and the read requests are mainly from foreground request, which means all the modifications are done from ratis.

For the non-HA code, the rw lock in PipelineManager is the only protection for thread-safety, there is no lock in PipelineStateManager. But for HA code, we have to rely on the rw lock in PipelineStateManager to ensure the thread-safety.

 

Since currently most of the lock operations in PipelineManager and PipelineStateManager are duplicated, we can relax the lock in PipelineManager, just use it to ensure that there is at most one on-going ratis operation. Previous logic is acquiring the write lock of PipelineManager and doing raft client request, ratis client requests are serialized, we just follow this logic.

 

We have a small drawback, the read request handled by PipelineStateManager may not be able to see the latest update, since there might be one in-flight action. We accept it since:
 # it won't cause any safety problem.
 # current code has the issue as well.
 # This should be our future direction of performance optimization: allow batch and parallel raft client request.

 

*P.S.* 

The analysis is also applicable for ContainerManager and ContainerStateManager.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-29 07:40:08,36
13348232,min/max election timeout of SCMRatisServer is not set properly.,"min/max election timeout of SCMRatisServer is not properly set.

Previously, it is [ratisRequestTimeout - 1s, ratisRequestTimeout + 1s]

It should be  [ratisLeaderElectionTimeout, ratisLeaderElectionTimeout + 0.2s]",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-29 06:06:23,36
13348103,Disable coverage upload to codecov,Please see: https://github.com/apache/ozone/pull/1741,pull-request-available,[],HDDS,Improvement,Major,2020-12-28 10:04:17,6
13347906,Fix set configs in SCMHAConfigration ,"If running the following code:

{code:java}
    SCMHAConfiguration scmhaConfiguration = conf.getObject(
        SCMHAConfiguration.class);
    scmhaConfiguration.setRatisStorageDir(""scm-ratis"");
    conf.setFromObject(scmhaConfiguration);
{code}

Will get the following exception:

{code:java}
java.lang.IllegalArgumentException: Attempt to get double field ""org.apache.hadoop.hdds.scm.ha.SCMHAConfiguration.raftSegmentSize"" with illegal data type conversion to long

	at sun.reflect.UnsafeFieldAccessorImpl.newGetIllegalArgumentException(UnsafeFieldAccessorImpl.java:69)
	at sun.reflect.UnsafeFieldAccessorImpl.newGetLongIllegalArgumentException(UnsafeFieldAccessorImpl.java:136)
	at sun.reflect.UnsafeDoubleFieldAccessorImpl.getLong(UnsafeDoubleFieldAccessorImpl.java:60)
	at java.lang.reflect.Field.getLong(Field.java:611)
	at org.apache.hadoop.hdds.conf.ConfigurationReflectionUtil.updateConfigurationFromObject(ConfigurationReflectionUtil.java:247)
	at org.apache.hadoop.hdds.conf.ConfigurationReflectionUtil.updateConfiguration(ConfigurationReflectionUtil.java:199)
	at org.apache.hadoop.hdds.conf.ConfigurationTarget.setFromObject(ConfigurationTarget.java:55)
	at org.apache.hadoop.hdds.scm.TestUtils.getScmSimple(TestUtils.java:484)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.createSCM(MiniOzoneClusterImpl.java:634)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:522)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.startCluster(TestOzoneRpcClientAbstract.java:173)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis.init(TestOzoneRpcClientWithRatis.java:71)
{code}
",pull-request-available,[],HDDS,Sub-task,Major,2020-12-26 00:39:47,54
13347693,Use singe server raft cluster in MiniOzoneCluster.,"Use SCMHAManagerImpl and SCMRatisServerImpl instead of MockSCMHAManager and MockRatisServer in MiniOzoneCluster.

The MockRatisServer and MockSCMHAManager can not fully test the HA code.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-24 05:45:02,36
13347679,"Set, add and remove ACL have no audit logs",Only get ACL has audit log,pull-request-available,[],HDDS,Bug,Major,2020-12-24 03:24:19,5
13347100,Update bouncycastle to 1.67,"The latest bouncy castle contains important fixes. Ozone is not affected by the bcrypt issue, but it seems to be safer updating to the latest version.

(Thanks to [~aengineer] how reported this issue)",pull-request-available,[],HDDS,Improvement,Trivial,2020-12-21 08:17:42,6
13347038,Add Integration test for HDDS upgrade (happy path cases),Add end to end integration test to verify upgrade for both SCM and DataNodes.,pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2020-12-20 17:01:36,44
13346654,Create OMCancelPrepareRequest and Response to cancel the prepared state of an OM.,Implement an OM request and response to cancel the prepared state of an ozone manager. Client protocol is added to allow integration testing. This will be connected to a client CLI command and acceptance tested in HDDS-4611.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-17 19:27:57,22
13346653,"Add an admin command to cancel ""preparation"" of an OM quorum.",Implement the command ozone om admin cancelprepare to cancel preparation of OMs.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-17 19:26:44,22
13346650,Fix issues in 'prepare' operation with one OM down.,"On a 3 node OM HA setup, when a prepare is done with 1 OM down, it leads to a state where the leader and follower are fully prepared (Snapshot at last index and logs purged). When the 3rd node rejoins the quorum, it leads to an infinite installSnapshot loop between the leader and the 3rd node, and the system goes into a bad state until a restart is done.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-17 19:13:07,30
13346409,Add a unit test to identify missing/extra tables in OMDBDefinition,"Add a unit test to identify missing/extra tables in OMDBDefinition

One example of such a case is the s3Table",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-12-16 19:03:47,18
13346355,Enable Multi Raft by default in Ozone,"Currently in ozone cluster, when the no of datanodes is not multiple of 3, the only way to make use of all datanodes for read/write is to enable multiraft. The idea here is to enable this feature by default in ozone.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2020-12-16 12:55:04,16
13346295,Writing to a new key failed when integrating with other components(HBase etc.) in SpringBoot project,"# in my biz project, it contain multi components, like: Apache Ozone(1.1.0), Apache HDFS(2.7.2), Apache HBase(1.2.1), Apache Elastic(7.6.0) etc.. we uploading & coping files with ofs api or native java api.
 # these have different version of google guava, Apache Ozone dependency the guava with version 28.2, but other component, like Apache HBase which is dependency the guava with lower version(like 16.0.1). -- *it has different guava version in Apache Ozone and others(like Apache HBase**)*
 # in module hadoop-hdds-client(dependency guava version: *28.2*), the  org.apache.hadoop.hdds.scm.XceiverClientRatis#sendRequestAsync(request) method contain param type com.google.common.base.Supplier in TracingUtil.executeInNewSpan() method which implement java.util.function.Supplier.  !image-2020-12-16-16-37-31-321.png|width=431,height=298!
 !image-2020-12-16-16-38-53-607.png|width=421,height=114!
 # in my biz project(dependency guava version: *16.0.1*), the *com.google.common.base.Supplier not implement java.util.function.Supplier, acturally, it's not impl* *java.util.function.Supplier neither when guava version less than 21.0.*
 # when uploading or coping file to a new key, it throw exception like: {color:#de350b}*org.apache.hadoop.hdds.scm.{color}{color:#ff0000}XceiverClien{color}tRatis does not implement the requested interface java.util.function.Supplier*.
 # if I reimport com.google.common.base.Supplier to java.util.function.Supplier, it works well for me  !image-2020-12-16-16-40-16-840.png|width=482,height=159!

 

 

 ",pull-request-available,"['OFS', 'Ozone Client', 'Ozone Filesystem']",HDDS,Bug,Major,2020-12-16 08:12:30,57
13346287,Ozone shell failed with Kerberos enabled,"When running a ozone cluster with kerberos, the ozone shell failed. We have the following error logs: (CMD: bin/ozone sh volume create vol0)
{code:java}
// code placeholder
2020-12-16 12:24:41,745 [main] ERROR OzoneClientFactory:249 - Couldn't create RpcClient protocol exception: 
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy16.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy16.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransport.submitRequest(Hadoop3OmTransport.java:80)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:218)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1066)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:241)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:246)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:229)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:149)
	at org.apache.hadoop.ozone.shell.OzoneAddress.createRpcClient(OzoneAddress.java:95)
	at org.apache.hadoop.ozone.shell.OzoneAddress.createClient(OzoneAddress.java:164)
	at org.apache.hadoop.ozone.shell.Handler.createClient(Handler.java:104)
	at org.apache.hadoop.ozone.shell.Handler.call(Handler.java:92)
	at org.apache.hadoop.ozone.shell.Handler.call(Handler.java:42)
	at picocli.CommandLine.executeUserObject(CommandLine.java:1933)
	at picocli.CommandLine.access$1100(CommandLine.java:145)
	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2326)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2291)
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:2465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)
	at org.apache.hadoop.ozone.shell.OzoneShell.lambda$execute$0(OzoneShell.java:55)
	at org.apache.hadoop.hdds.tracing.TracingUtil.executeInNewSpan(TracingUtil.java:159)
	at org.apache.hadoop.ozone.shell.OzoneShell.execute(OzoneShell.java:53)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)
	at org.apache.hadoop.ozone.shell.OzoneShell.main(OzoneShell.java:47)
{code}
The error shows that reading or setting secure configure  failed.

I guess we call  UserGroupInformation#getCurrentUser before UserGroupInformation#setConfiguration.

I will try to fix this.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-12-16 07:52:35,58
13346284,envtoconf broken for .conf and few other formats,"{{envtoconf}} does not work for some output formats:

 * {{.env}}
 * {{.sh}}
 * {{.cfg}}
 * {{.conf}}

To reproduce:

{code:title=docker run -it --rm -e DUMMY.CONF_key=value apache/ozone:1.0.0}
Traceback (most recent call last):
  File ""/opt/hadoop/libexec/envtoconf.py"", line 117, in <module>
    Simple(sys.argv[1:]).main()
  File ""/opt/hadoop/libexec/envtoconf.py"", line 108, in main
    self.transform()
  File ""/opt/hadoop/libexec/envtoconf.py"", line 96, in transform
    content = transformer_func(content)
  File ""/opt/hadoop/libexec/transformation.py"", line 121, in to_conf
    for key, val in props:
ValueError: too many values to unpack
{code}",pull-request-available,['docker'],HDDS,Bug,Major,2020-12-16 07:43:48,1
13346272,Use OM style Configuration to initialize SCM HA,Design doc: https://docs.google.com/document/d/1Q_ofDnvYYUGMQfZqdiUs5BMGQAy0YDuaM5AYuG8Ub6o/edit?usp=sharing,pull-request-available,[],HDDS,Sub-task,Major,2020-12-16 06:56:37,54
13346260,Uniform naming conventions of keytab file name,"I create a pull request to uniform those difference. 

And I wonder, Is it necessary to create a JIRA to handle these things ？
 naming conventions for variable, classname, and so on. ",pull-request-available,"['documentation', 'Ozone Filesystem']",HDDS,Improvement,Major,2020-12-16 05:48:59,58
13346251,"OM Terminates when adding acls to ""S3v"" volume","
{code:java}
2020-12-14 10:04:58,612 [OM StateMachine ApplyTransaction Thread - 0] ERROR ratis.OzoneManagerStateMachine: Terminating with exit status 1: Request cmdType: AddAcl
traceID: """"
clientId: ""client-CAE43266CDF2""
userInfo {
  userName: ""om/om-server@DEV.COM""
  remoteAddress: ""10.101.211.49""
  hostName: ""om-server""
}
addAclRequest {
  obj {
    resType: VOLUME
    storeType: OZONE
    path: ""/s3v""
  }
  acl {
    type: USER
    name: ""airflow@DEV.TAP""
    rights: ""\200""
    aclScope: ACCESS
  }
}
failed with exception
java.lang.IllegalArgumentException: Trying to set updateID to 213 which is not greater than the current value of 36028797018963967 for OMVolumeArgs{volume='s3v', admin='hadoop', owner='hadoop', creationTime='1606808208748', quota='1152921504606846976'}
       at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142)
...
       at java.base/java.lang.Thread.run(Thread.java:834)
2020-12-14 10:04:58,635 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG:
{code}
",pull-request-available,[],HDDS,Bug,Blocker,2020-12-16 05:20:18,13
13346247,"Directory table, fileTable and openFile Table is missing from the OM DB Definition","Directory table, fileTable and openFile Table is missing from the OM DB Definition",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-16 04:39:51,18
13346245,RDBScanner should print table name if it is not present in the db definition,"RDBScanner should print table name if it is not present in the db definition

{code}
➜  ~/code/apache/ozone/oz2 git:(HDDS-2939) ✗ hadoop-ozone/dist/target/ozone-1.1.0-SNAPSHOT/bin/ozone debug ldb --db=hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-62a36acd-f4a6-4025-95b9-adf73bb35048/ozone-meta/om.db/ scan --column_family=directoryTable
2020-12-16 00:22:23,625 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
2020-12-16 00:22:23,630 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
2020-12-16 00:22:23,630 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
Table with specified name does not exist%
{code}",pull-request-available,['Tools'],HDDS,Bug,Major,2020-12-16 04:28:41,18
13346171,Refine IAccessAuthorizer interface to do recursive ACL check on a path,"OM server is presently doing ACL check access for each path separately. This will become too costly RPC calls assume if user performs recursive delete on a path where there are millions of sub-directories under this path.

The idea/proposal is to refine #checkAccess interface by giving ""recursiveDelete"" hint and make only one single call and the implementation classes(native or external entity) can do recursive logic in an optimized way.

 
{code:java}
IAccessAuthorizer#checkAccess(IOzoneObj ozoneObject, RequestContext context)

public class RequestContext {
     …...
     private final boolean recursiveDelete; // add new attribute
}
{code}",pull-request-available,[],HDDS,Improvement,Major,2020-12-15 18:04:20,40
13346094,BasicOzoneFileSystem reports StorageUnit class not found,"

20/12/15 17:31:03 WARN ozone.BasicOzoneFileSystem: delete: Path does not exist: o3fs://ozone.s3v/test2/io_control
java.lang.NoClassDefFoundError: org/apache/hadoop/conf/StorageUnit
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.getDefaultBlockSize(BasicOzoneFileSystem.java:706)
	at org.apache.hadoop.fs.FileSystem.getDefaultBlockSize(FileSystem.java:2112)
	at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1085)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:320)
	at org.apache.hadoop.fs.TestDFSIO.createControlFile(TestDFSIO.java:312)
	at org.apache.hadoop.fs.TestDFSIO.run(TestDFSIO.java:816)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
	at org.apache.hadoop.fs.TestDFSIO.main(TestDFSIO.java:713)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:130)
	at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.StorageUnit
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)


The root cause is there is no org/apache/hadoop/conf/StorageUnit in hadoop-ozone-filesystem-hadoop2-1.1.0-SNAPSHOT.jar.  We should use org/apache/hadoop/hdds/conf/StorageUnit instead. ",pull-request-available,[],HDDS,Bug,Major,2020-12-15 12:14:39,5
13346036,Duplicate dependency hadoop-hdds-hadoop-dependency-server in datanode,"{code}
Warning:  Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-datanode:jar:1.1.0-SNAPSHOT
Warning:  'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdds-hadoop-dependency-server:jar -> duplicate declaration of version (?) @ line 41, column 17
Warning:  
Warning:  It is highly recommended to fix these problems because they threaten the stability of your build.
Warning:  
Warning:  For this reason, future Maven versions might no longer support building such malformed projects.
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-12-15 05:48:16,1
13346029,Handle potential data loss during ReplicationManager.handleOverReplicatedContainer(),"h4. Problem: 

ReplicationManager maintains the in-flight replication and deletion in-memory, which is not replicated using Ratis. So, theoretically it’s possible that we might run into issues if we immediately start ReplicationManager after a failover.

Scenario: There are 6 replicas of the container C1 namely CR1, CR2, CR3, CR4, CR5 and CR6. The container is over replicated, so the current SCM S1 decides to delete the excess replicas. SCM S1 picks CR1, CR2 and CR3 for deletion, this information is updated in the in-flight deletion list and deletion commands are sent to the datanodes. If there is a failover at this point and SCM S2 becomes leader, it doesn’t have the in-flight deletion list from SCM S1 and it finds the container C1 to be over replicated. Theoretically it’s possible that SCM S2 picks CR4, CR5 and CR6 for deletion. If this happens, we will end up in data loss.

To address this issue we will make the logic to select a replica for deletion deterministic. This will make sure that the new leader after failover will pick the same replica for deletion which was picked by the old leader. 
h4. Approach: 

Sort the candidate replicas. Delete excess replicas from small to large. There will not be any DeleteContainerCommand for the largest 3 Replica sent by any SCM.
h4. Example:

Assume there are 6 replicas of the container C1, the factor of C1 is 3, the names of the replicas are CR1, CR2, CR3, CR4, CR5 and CR6. There will not be any DeleteContainerCommand for CR4, CR5, CR6 sent by any SCM:

If SCM sees less than or equal to 3 replicas, it won’t send any DeleteContainerCommand.

If SCM sees 4 replicas, It deletes the smallest replica, which means it won’t send DeleteContainerCommand for CR4 CR5, CR6, otherwise there will be a contradiction.

If SCM sees 5 replicas, It deletes the smallest 2 replicas, which means it won’t send DeleteContainerCommand for CR4 CR5, CR6, otherwise there will be a contradiction.

If SCM sees 6 replicas, It deletes the smallest 3 replicas, which means it won’t send DeleteContainerCommand for CR4 CR5, CR6, otherwise there will be a contradiction.
h4. P.S.:

Since this issue exists in master as well, e.g., a quickly restart of SCM, we decide to fix this problem in master. 

 ",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-15 04:08:12,36
13346019,If volume's quota is enabled then bucket's quota cannot be cleared,If volume's quota is enabled then bucket's quota cannot be cleared. We need to prompt the user to clear volume quota first,pull-request-available,[],HDDS,Sub-task,Major,2020-12-15 02:46:03,4
13345920,Merge master into HDDS-3698-upgrade branch.,"Seeing some conflicts on attempted merge.

{code}
CONFLICT (content): Merge conflict in hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestStorageContainerManager.java
Removing hadoop-ozone/dist/src/main/smoketest/freon/freon.robot
Auto-merging hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/OMConfigKeys.java
CONFLICT (content): Merge conflict in hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/OMConfigKeys.java
Auto-merging hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/scm/node/TestSCMNodeMetrics.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/scm/node/TestSCMNodeMetrics.java
Auto-merging hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/container/testutils/ReplicationNodeManagerMock.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/container/testutils/ReplicationNodeManagerMock.java
Auto-merging hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestSCMNodeManager.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestSCMNodeManager.java
Auto-merging hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestDeadNodeHandler.java
Auto-merging hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java
Auto-merging hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/MockNodeManager.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeProtocolServer.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/NodeStateMap.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/NodeStateMap.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeMetrics.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeMetrics.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeStateManager.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeStateManager.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeManager.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeInfo.java
CONFLICT (content): Merge conflict in hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeInfo.java
Auto-merging hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/events/SCMEvents.java
Auto-merging hadoop-hdds/interface-server/src/main/proto/ScmServerDatanodeHeartbeatProtocol.proto
CONFLICT (content): Merge conflict in hadoop-hdds/interface-server/src/main/proto/ScmServerDatanodeHeartbeatProtocol.proto
Auto-merging hadoop-hdds/interface-client/src/main/proto/hdds.proto
CONFLICT (content): Merge conflict in hadoop-hdds/interface-client/src/main/proto/hdds.proto
Auto-merging hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java
CONFLICT (content): Merge conflict in hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java
Auto-merging hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java
CONFLICT (content): Merge conflict in hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java
Automatic merge failed; fix conflicts and then commit the result.
{code}",pull-request-available,[],HDDS,Sub-task,Major,2020-12-14 18:10:01,44
13345883,Mismatch in prefix of DN and SCM http.auth.type config key,"{code:title=https://github.com/apache/ozone/blob/327c148582a793860ad97d46bf55997cc1f15029/hadoop-hdds/docs/content/security/SecuringOzoneHTTP.md#enable-simple-authentication-for-scm-http}
ozone.scm.http.auth.type | simple
ozone.scm.http.auth.simple.anonymous_allowed | false
...
ozone.datanode.http.auth.type | simple
ozone.datanode.http.auth.simple.anonymous_allowed | false
{code}

versus actual config keys:

{code:title=https://github.com/apache/ozone/blob/327c148582a793860ad97d46bf55997cc1f15029/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMHTTPServerConfig.java#L32-L33}
@ConfigGroup(prefix = ""hdds.scm.http.auth"")
public class SCMHTTPServerConfig {
{code}

and

{code:title=https://github.com/apache/ozone/blob/327c148582a793860ad97d46bf55997cc1f15029/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsConfigKeys.java#L245-L246}
  public static final String OZONE_DATANODE_HTTP_AUTH_CONFIG_PREFIX =
      ""hdds.datanode.http.auth."";
{code}",pull-request-available,['documentation'],HDDS,Bug,Minor,2020-12-14 15:16:09,55
13345744,Coverage not updated since TLP,"Ozone's GitHub Actions CI workflow references the old {{hadoop-ozone}} repository, so coverage data has not been uploaded to Sonar and Codecov for two months.",pull-request-available,['build'],HDDS,Bug,Major,2020-12-13 17:34:52,1
13345620,TableCache Refactor to fix issues in cleanup never policy,"Right now we have 2 clean up policies.
1. Never
2. Manual

Never = Full Table Cache
Manual = Partial Table Cache

In OM, the main purpose of Table cache is for correctness. (Because OM return response after adding to cache, does not wait for double buffer flush to complete)

The current implementation has few problems.
1. Cleanup Policy Never uses ConcurrentSkipListMap, and its computeIfPresent is not atomic, so there can be a race condition between cleanup and requests adding to cache. (This might cause cleaning up entries which are not flushed to DB, and this can cause correctness issue)
2. Cleanup for override entries for full cache, never removes epoch entries.

*Proposal:*
1. Make TableCache based on cache type and have separate implementation for full cache and partial cache.
2. Fix FullCache issue, using the lock.
3. Fix evict cache logic for full cache to cleanup epoch entries for override entries.

",pull-request-available,[],HDDS,Improvement,Major,2020-12-12 01:38:20,13
13345599,Update QUOTA_IN_COUNTS to quota in namespace,quota in count seems not very readable. We may rename all of it to quota in namespace.,pull-request-available,[],HDDS,Sub-task,Major,2020-12-11 21:15:20,54
13345593,Cleanup usage of volumeArgs in KeyRequests,"This Jira proposes to remove volumeArgs usage in the KeyRequests, after HDDS-4308, KeyRequests does not update volume used.

So, request/response classes do not require volumeArgs during bytes calculation.

This Jira proposes to cleanup usage of volumeArgs.",pull-request-available,[],HDDS,Bug,Major,2020-12-11 20:44:58,13
13345541,Datanode can be stuck in leader not ready state after restart,"On restart the transactions are reapplied for an existing ratis pipeline. ContainerStateMachine#applyTransaction while processing future can throw NullPointerException leading to the future being completed exceptionally. 

{code:java}
      future.thenApply(r -> {
        if (trx.getServerRole() == RaftPeerRole.LEADER) {
          long startTime = (long) trx.getStateMachineContext();
          metrics.incPipelineLatency(cmdType,
              Time.monotonicNowNanos() - startTime);
        }
{code}

In the above code snippet trx.getStateMachineContext() will be null during restart and this fails the future itself without updating the applyTransactionCompletionMap. As a result the lastAppliedIndex is not updated for the server and server is stuck in leader not ready state.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-12-11 14:54:49,35
13345523,Add acceptance test for Ozone Client Key Validator,Add acceptance test for Freon's Ozone Client Key Validator.,pull-request-available,['test'],HDDS,Improvement,Major,2020-12-11 13:30:53,1
13345363,Intermittent failure in TestOzoneClientRetriesOnException#testMaxRetriesByOzoneClient,"TestOzoneClientRetriesOnException#testMaxRetriesByOzoneClient intermittently fails with:

{code:title=https://github.com/elek/ozone-build-results/blob/master/2020/12/08/4407/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException.txt}
testMaxRetriesByOzoneClient(org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException)  Time elapsed: 42.789 s  <<< FAILURE!
java.lang.AssertionError: Expected exception not thrown
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException.testMaxRetriesByOzoneClient(TestOzoneClientRetriesOnException.java:235)
{code}

The problem happens if the number of distinct containers used for the key is less than 4.  In this case max retries (of 3) is never reached.

Normal execution:

{code}
block: conID: 1 locID: 105357240184995843 bcsId: 0
block: conID: 2 locID: 105357240185061380 bcsId: 0
block: conID: 3 locID: 105357240185061381 bcsId: 0
block: conID: 4 locID: 105357240185061382 bcsId: 0
{code}

Failing execution:

{code}
block: conID: 1 locID: 105357233754603523 bcsId: 0
block: conID: 2 locID: 105357233754603524 bcsId: 0
block: conID: 1 locID: 105357233754669061 bcsId: 0
{code}",pull-request-available,['test'],HDDS,Bug,Critical,2020-12-10 19:10:52,1
13345293,TestDefaultCertificateClient misuses chars param of random(),"{{TestDefaultCertificateClient}} passes ""UTF-8"" to {{RandomStringUtils.random(int, String)}} with the intention to use UTF-8 encoding.  It was added in HDDS-1087 to fix (intermittent?) failure in CI.

However, the parameter is not for character encoding, rather:

{code}
     * @param chars  the String containing the set of characters to use,
     *  may be null, but must not be empty
{code}

So this results in values like: {{""--8F8T8U8T...""}}.",pull-request-available,['test'],HDDS,Bug,Minor,2020-12-10 12:52:16,1
13345261,Refactor SCMHAManager and SCMRatisServer with RaftServer.Division,Simplify SCMHAManager and SCMRatisServer with RaftServer.Division,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-10 10:29:30,36
13345216,Inconsistencies in case the parent of a open file is deleted/renamed.,"A test to depict the behaviour(if kept in TestRootedOzoneFileSystem)

 
{code:java}
public void testCloseFileWithMissingParent() throws Exception {
  // Test if the parent directory gets deleted before commit.
  Path parent = new Path(bucketPath, ""parent"");
  Path file = new Path(parent, ""file"");
  // Create file with missing parent, this would create parent directory.
  FSDataOutputStream stream = fs.create(file);
  stream.write(""data"".getBytes());
  // Delete the parent.
  fs.delete(parent, false);
  stream.close();
  assertTrue(fs.exists(file)); // File exists
  assertFalse(fs.exists(file.getParent())); // But the parent doesn't exist
  // (Breaks FileSystem semantics)

  //cleanup
  fs.delete(file,true);

  // Test if the parent directory gets renamed before commit.
  FSDataOutputStream stream1 = fs.create(file);
  stream1.write(""data"".getBytes());
  // Rename the parent.
  Path renamedPath = new Path(bucketPath, ""parent1"");
  assertTrue(fs.rename(parent, renamedPath));
  fs.exists(renamedPath);
  // Close should throw exception.
  stream1.close();
  assertTrue(fs.exists(file)); // File exists
  assertFalse(fs.exists(file.getParent())); // But the parent doesn't exist

  // Inconsistency induced in listing.
  FileStatus[] ls = fs.listStatus(bucketPath);
  assertTrue(ls.length == 2); // Only one elem should there that should be
  // the renamed path, but both renamed and original are shown. // Though only the renamed is accessible 
}
{code}
We should not allow committing a key, if the parent doesn't exist.

 ",pull-request-available,[],HDDS,Bug,Major,2020-12-10 07:58:48,59
13345189,"inconsistent path when the item ""ozone.recon.db.dir"" is not configured","There is an inconsistent path when the item ""ozone.recon.db.dir"" is not configured : 
1 : falling back to ozone.metadata.dirs instead, just same as ozone.scm.db.dir/ozone.om.db.dir
2 : the jdbcUrl variable in ReconSqlDbConfig class set default value (jdbc:derby:${ozone.recon.db.dir}/ozone_recon_derby.db)

The second points assume that ""ozone.recon.db.dir"" is must be configured, but it was not.

The result is that it will create a path named ""${ozone.recon.db.dir}"" in current directory, when
1. ""ozone.recon.db.dir"" is not configured
2. ReconSqlDbConfig.setJdbcUrl is not called.

That maybe a bug.",pull-request-available,['Ozone Recon'],HDDS,Bug,Minor,2020-12-10 03:40:42,55
13345181,Simplify inequality condition,"The following is not easy to be quickly understood
{code:java}
!(ContainerDataProto.State.INVALID == state); 
{code}",pull-request-available,[],HDDS,Improvement,Major,2020-12-10 02:01:16,52
13345123,Reduce memory footprint of OMUpdateEventBatch for Recon when processing OM update events,"We are experiencing OOM when Recon heap size is too small (1GB) and it looks like {{OMUpdateEventBatch#events}} is taking most of them.

Goal:
1. Reduce new object creation when Recon is consuming batches OM events ({{consumeOMEvents}})
2. Improve {{OMUpdateEventBatch#filter}} to reduce memory usage. e.g. avoid generating a new list?",pull-request-available,[],HDDS,Improvement,Major,2020-12-09 18:26:06,12
13345122,Add pre append gate and marker file to OM prepare state,"Add a gate to the pre append step of OzoneManagerStateMachine to block further write requests from coming in when an ozone manager is prepared. Also add a marker file that is written when the OM is prepared so that if it is restarted without a special flag, it automatically enters prepare mode.",pull-request-available,"['OM', 'Ozone Manager']",HDDS,Sub-task,Major,2020-12-09 18:12:34,22
13345000,SCMContext,"The design refers to

[https://docs.google.com/document/d/1h_3gpC4o2EpuBlcQiJC7MMoZz9JmaMX9CxObSxWU614/edit#heading=h.4f1y2k1ojlx8]",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-09 06:53:13,36
13344999,SCMContext Phase 1 - Raft Related Info," 

refer to

https://docs.google.com/document/d/1h_3gpC4o2EpuBlcQiJC7MMoZz9JmaMX9CxObSxWU614/edit#heading=h.4f1y2k1ojlx8",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-09 06:51:37,36
13344983,inconsistencies Port describtion in  Monitoring with Prometheus  document,"I found a inconsistencies describtion in  Monitoring with Prometheus  document with URL: [https://ci-hadoop.apache.org/view/Hadoop%20Ozone/job/ozone-doc-master/lastSuccessfulBui[…]ld/artifact/hadoop-hdds/docs/public/recipe/prometheus.html|https://ci-hadoop.apache.org/view/Hadoop%20Ozone/job/ozone-doc-master/lastSuccessfulBuild/artifact/hadoop-hdds/docs/public/recipe/prometheus.html]

step exaplains:
http://scm:9874/prom
http://ozoneManager:9876/prom

prometheus conf:
- ""scm:9876""
- ""ozoneManager:9874""

Maybe, we should consider to correct this.",pull-request-available,['documentation'],HDDS,Improvement,Trivial,2020-12-09 05:00:57,58
13344944,Prepare client should check every OM individually for the prepared check based on Txn Id.,"* After getting a successful response for the OMPrepareRequest, the prepare client should use the Txn ID from the response to check every OM's preparation completeness.
* This JIRA is partially dependent on HDDS-4569 which plans to introduce a marker file at the end of preparation. In a follow up JIRA, this will be refined to take up that logic.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-09 00:34:40,30
13344770,Increase default value for key and block deletion limit,Currently ozone.key.deleting.limit.per.task defaults to 1000 and hdds.scm.block.deletion.per-interval.max defaults to 10000. These config values can be increased to 20000 based on the latest block deletion results. ,pull-request-available,[],HDDS,Sub-task,Major,2020-12-08 09:49:44,35
13344766,Old bucket needs to be accessible after the cluster was upgraded to the Quota version.,"long quotaInBytes are new fields in the bucketArgs, this field will go to 0 by default in the old bucket when the old cluster is upgraded. At this point, the data writes are rejected.

This is similar to creating a bucket without specifying quotaInBytes, where quotaInBytes is set to -1 by default via getQuotaValue. We can use 0 as a special term.


 !screenshot-quota.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-12-08 09:42:18,4
13344760,We should support suggested Ratis Server Leader in OM HA.,"Now the OM HA not support suggested Leader, we should support it.",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2020-12-08 09:26:36,60
13344752,Add ReadWriteLock into PipelineStateManagerV2Impl to protect contentions between RaftServer and PipelineManager,"Add ReadWriteLock into PipelineStateManagerV2Impl to protect contentions between RaftServer and PipelineManager.

If current leader SCM (as leader of term N) steps down while there are on-going transactions, there might be read/write contentions that new leader SCM (as leader of term N+1) has replicated some raft log entries encapsulating method addPipeline()/removePipeline()/updatePipelineState() to the underlying RaftServer that are being applied by StateMachineUpdater, and at the same time PipelineManager is calling getPipelines().",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-08 09:01:20,36
13344694,Avoid using  hard coding uft-8 charset,Avoid using hard coding uft-8 charset，should use StandardCharsets.UTF_8,pull-request-available,[],HDDS,Improvement,Major,2020-12-08 02:06:54,52
13344653,Support Ozone block token with access mode check,Thanks [~elek] for reporting the issue. There is a TODO in the Ozone block token verifier to support this. Currently the block token is giving all access to the client without access mode check. This ticket is opened to set access mode properly for block token and validate them in the token verifier. ,pull-request-available,['Security'],HDDS,Bug,Major,2020-12-07 20:19:01,28
13344593,S3 and HCFS interoperability,"This jira will be used to track and finish the design discussion which is started in HDDS-4097.

This design doc defines the current behavior and defines a possible 3rd option which is not yet implemented. HDDS-4097 contains the implementation details of one specific option.  ",pull-request-available,[],HDDS,Bug,Major,2020-12-07 14:32:43,6
13344576,Add smoketest for ozonescripts environment,"{{start-ozone.sh}} and {{stop-ozone.sh}} are currently not exercised by any tests, so they can be inadvertently broken without CI feedback.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-12-07 13:26:31,1
13344529,Delete a duplicate character in ZH Docs homepage,Homepage text display error.,pull-request-available,[],HDDS,Bug,Trivial,2020-12-07 10:03:06,61
13344492,numKey metrics goes negative after intermediate directory deletion,numKey metrics goes negative after intermediate directory deletion,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-12-07 06:17:00,18
13344310,ChunkInputStream should release buffer as soon as last byte in the buffer is read,"We currently wait for reading up till the Chunk EOF before releasing the buffers in ChunkInputStream (or when the stream is closed). Let's say a client reads first 3 MB of a chunk  of size 4MB and does not close the stream immediately. This would lead to the 3MB of data being cached in the ChunkInputStream buffers till the stream is closed.

Once HDDS-4552 is resolved, a chunk read from DN would return the chunk data as an array of ByteBuffers. After each ByteBuffer is read, it can be released. This would greatly help with optimizing memory usage of ChunkInputStream.",pull-request-available,[],HDDS,Improvement,Major,2020-12-04 23:57:04,43
13344309,Read data from chunk into ByteBuffer[] instead of single ByteBuffer,"When a ReadChunk operation is performed, all the data to be read from one chunk is read into a single ByteBuffer. 
{code:java}
#ChunkUtils#readData()
public static void readData(File file, ByteBuffer buf,
    long offset, long len, VolumeIOStats volumeIOStats)
    throws StorageContainerException {
  .....
  try {
    bytesRead = processFileExclusively(path, () -> {
      try (FileChannel channel = open(path, READ_OPTIONS, NO_ATTRIBUTES);
           FileLock ignored = channel.lock(offset, len, true)) {

        return channel.read(buf, offset);
      } catch (IOException e) {
        throw new UncheckedIOException(e);
      }
    });
  } catch (UncheckedIOException e) {
    throw wrapInStorageContainerException(e.getCause());
  }
  .....
  .....{code}
This Jira proposes to read the data from the channel and put it into an array of ByteBuffers each with a set capacity. This capacity can be configurable. 

This would help with optimizing Ozone InputStreams in terms of cached memory. Currently, data in ChunkInputStream is cached till either the stream is closed or the chunk EOF is reached. This sometimes leads to upto 4MB (default ChunkSize) of data being cached in memory per ChunkInputStream.

After the proposed change, we can optimize ChunkInputStream to release a ByteBuffer as soon as that ByteBuffer is read instead of waiting to read the whole chunk (HDDS-4553). Read I/O performance will not be affected as the read from DN still returns the requested length of data at one go. Only difference would be that the data would be returned in an array of ByteBuffer instead of a single ByteBuffer.",pull-request-available,[],HDDS,Improvement,Major,2020-12-04 23:49:06,43
13344161,Remove checkLeader in PipelineManager.,"We do not need isLeader check in PipelineManager, just remove them.

By design, we just need do isLeader check when receiving rpc requests and firing datanode command.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-04 06:30:50,36
13344132,Fix typos in documents,Fix typos in readme document,pull-request-available,[],HDDS,Improvement,Major,2020-12-04 02:22:55,52
13344044,Upgrade aws-java-sdk to 1.11.901,"AWS Java SDK 1.11.678 or later (commit 06a2180e) is needed to support AWS IMDSv2. 

* https://stackoverflow.com/questions/62127953/how-to-use-imdsv2-from-aws-java-sdk
* Support for IMDSv2 was added in: 1.11.678 of aws-java-sdk (commit 06a2180e)

Currently Ozone is pulling in AWS Java SDK 1.11.615. It would be good to upgrade to a recent version - 1.11.901",pull-request-available,[],HDDS,Task,Major,2020-12-03 15:41:08,34
13344040,"Change ""ozone.datanode.pipeline.limit"" to 3 by default","Change ""ozone.datanode.pipeline.limit"" to 3 by default.

This will help in cases when the clusters are of sizes 8 nodes or 5 nodes, where 2 nodes are currently completely unused.",pull-request-available,[],HDDS,Bug,Major,2020-12-03 15:23:32,17
13343984,Failed to list keys when there are two bucket names with same prefix,"For example,  we have bucket named ""hadoop"" and another bucket named ""hadoop-ozone"".  It will return empty key list when we execute ""sh key list /s3v/hadoop"" command. 

 ""hadoop-ozone"" and ""hadoop"" share the same prefix ""hadoop"".  
There is a bug in listKeys implementation which failes to append ""/"" to seekKey when prefix is null.  Since ""-"" is ahead of ""/"" in order, so no key is returned in the listKeys.


",pull-request-available,[],HDDS,Bug,Major,2020-12-03 10:30:43,5
13343935,Need throw exception to trigger FailoverProxyProvider of SCM client to work,ScmBlockLocationProtocolServerSideTranslatorPB and StorageContainerLocationProtocolServerSideTranslatorPB need throw exception to trigger FailoverProxyProvider of SCM client to work.,pull-request-available,['SCM HA'],HDDS,Sub-task,Minor,2020-12-03 08:02:05,36
13343864,Add a new OM admin command to submit the OMPrepareRequest.,"* Introduce a new OM client operation to ""prepare"" the OM quorum.
* As a first pass, the client will just submit the request (HDDS-4480) and print out the response (Txn ID)
* In a follow up JIRA, the subsequent steps to probe every individual OM for preparation completeness will be added.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-02 21:00:58,30
13343666,Remove refreshPipeline in listKeys ,"In few usecases like delete which iterates and listKeys to get KeyNames to deleteKeys, we donot need to refreshPipeline, and also in S3 listKeys also we don't need Keys with PipelineInfo.

Doing this will help in improving the performance of delete/rename/listKeys API.

As RpcClient is returning only info like KeyName, length, replication type,  factor, modificationTime, creationTime. So we don't really need an extra param, as previously even though server sent pipelineInfo it is not used/returned to the client. So we don't need extra param, we can remove refresh pipeline in OM Server.",pull-request-available,[],HDDS,Improvement,Major,2020-12-02 00:38:45,13
13343615,Add more unit tests for OM layout version manager.,"* Add test to make sure an unsupported OM request is rejected.
* Add test to make sure all new OM requests have the getRequestType method defined.
* Minor code refactor.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-12-01 17:19:14,30
13343531,Use fixed thread pool for closed container replication ,"Number of threads for closed container replications can be adjusted by the settings  {{hdds.datanode.replication.streams.limit}}. But this number is ignored today due to the misuse of {{ThreadPoolExecutor}}:

{code}
new ThreadPoolExecutor(
        0, poolSize, 60, TimeUnit.SECONDS,
        new LinkedBlockingQueue<>(),
        new ThreadFactoryBuilder().setDaemon(true)
            .setNameFormat(""ContainerReplicationThread-%d"")
            .build())
{code}

Here the minimal number of threads is 0 and the maximum number of the threads is the configured value.  Threads in the thread pool supposed to be scaled up, but it doesn't.

[From the JDK docs|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#ThreadPoolExecutor(int,%20int,%20long,%20java.util.concurrent.TimeUnit,%20java.util.concurrent.BlockingQueue)]:

bq. A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize()) according to the bounds set by corePoolSize (see getCorePoolSize()) and maximumPoolSize (see getMaximumPoolSize()). When a new task is submitted in method execute(java.lang.Runnable), [...] [AND]  If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full.

So if queue is not full (and {{LinkedBlockgingQueue}} is unbounded by default) the threads will never be created.

For a quick fix we can switch to use static thread pool instead of dynamic and always keep the required number of threads.",pull-request-available,[],HDDS,Sub-task,Major,2020-12-01 11:52:04,6
13343466,Avoid rewriting pipeline information during PipelineStateManagerV2Impl initialization,{{PipelineStateManagerV2Impl}} adds the pipeline back to the pipeline table (RocksDB) during initialization which is unnecessary. This can be avoided.,newbie pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-12-01 05:16:00,54
13343465,Update pipeline db when pipeline state is changed,In the new {{PipelineManager}} implementation we should update the pipeline table (RocksDB) when there is a pipeline state change.,pull-request-available,[],HDDS,Sub-task,Major,2020-12-01 05:13:55,16
13343447,Add authorization check for SCM,"Currently a number of the SCM Client RPCs don't perform authorization check.

In my opinion, SCMClientProtocolServer stopReplicationManager, forceExitSafeMode, startReplicationManager, deactivatePipeline, closePipeline should all be admin-only.",pull-request-available,['SCM'],HDDS,Task,Major,2020-12-01 02:01:05,61
13343397,Upgrade snakeyaml to 1.26+,Ozone is pulling in snakeyaml 1.16. snakeyaml should be updated to 1.26+.,pull-request-available,[],HDDS,Task,Major,2020-11-30 19:49:14,34
13343396,Upgrade slf4j to 1.7.30,slf4j 1.7.25 is in use today and should upgrade to the latest 1.7.x - which is 1.7.30,pull-request-available,[],HDDS,Task,Minor,2020-11-30 19:43:18,34
13343327,Remove false-positive error logs from LeaseManager,"There is an error log which can be seen frequently in the log:

{code}
2020-11-30 15:35:58,539 [CommandWatcher-LeaseManager#LeaseMonitor] ERROR lease.LeaseManager (LeaseManager.java:run(238)) - Execution was interrupted 
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:234)
	at java.lang.Thread.run(Thread.java:748)
{code}

This log is introduced by HDDS-2561 to make Sonar happy, but it was not required as LeaseManager use the thread interrupt intentionally.

For a proper fix we can keep the logging on WARN level AND replace thread interrupts with thread notify/wait.
 ",pull-request-available,[],HDDS,Improvement,Trivial,2020-11-30 14:38:32,6
13343323,Replace Hadoop variables and functions in Ozone shell scripts with Ozone-specific ones,"Currently Ozone relies on {{HADOOP_\*}} environment variables (eg. {{HADOOP_HOME}}) for historical and practical reasons (code reuse).  This can lead to unexpected results if Hadoop and Ozone are both present on a node and they share their environment.  Eg. we had to implement a workaround for finding {{ozone-config.sh}} relative to the script being executed when {{HADOOP_HOME}} points to Hadoop, not Ozone (HDDS-1912 and HDDS-4450).

Another similar severe problem happens if we would like to access Ozone filesystem both via {{ozone}} and {{hadoop}} commands.  The latter needs shaded Ozone FS JAR in {{HADOOP_CLASSPATH}}.  The same {{HADOOP_CLASSPATH}} results in {{ClassNotFound}} for {{ozone}}.

The solution proposed in HDDS-1912:

bq. Long-term we may need to replace all the HADOOP_ environment variable with an OZONE_ environment variable, but that would be a significant bigger change.

This would allow using different classpaths, logging parameters, and more.

To be backward compatible, we should use existing {{HADOOP_}} variables as fallback for the corresponding {{OZONE_}} ones, but let {{OZONE_X}} take precedence over {{HADOOP_X}} if the former is defined.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Critical,2020-11-30 14:24:39,1
13343305,Create freon test to measure closed container replication,Create new freon test for container download,pull-request-available,[],HDDS,Sub-task,Major,2020-11-30 12:54:12,6
13342826,"Module [hadoop-ozone-filesystem-hadoop2] compilation failure, missing dependencies","The following error is reported when run the maven command ""mvn clean package -DskipTests=true""

 

[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-ozone-filesystem-hadoop2: Compilation failure: Compilation failure:
[ERROR] /Users/dev/github/hadoop-ozone/hadoop-ozone/ozonefs-hadoop2/src/main/java/org/apache/hadoop/fs/ozone/Hadoop27RpcTransport.java:[37,27] package com.google.protobuf does not exist
[ERROR] /Users/dev/github/hadoop-ozone/hadoop-ozone/ozonefs-hadoop2/src/main/java/org/apache/hadoop/fs/ozone/Hadoop27RpcTransport.java:[38,27] package com.google.protobuf does not exist
[ERROR] /Users/dev/github/hadoop-ozone/hadoop-ozone/ozonefs-hadoop2/src/main/java/org/apache/hadoop/fs/ozone/Hadoop27RpcTransport.java:[45,24] cannot find symbol
[ERROR] symbol: class RpcController
[ERROR] location: package org.apache.hadoop.fs.ozone.Hadoop27RpcTransport
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.

 

I think the module hadoop-ozone-filesystem-hadoop2 should add the dependency  com.google.protobuf package",pull-request-available,['build'],HDDS,Bug,Major,2020-11-26 11:55:04,62
13342778,Add initialization configuration to ozone java API doc,"Currently, the usage is described in Ozone API Doc, not how to initialize the configuration. Without this section, some users cannot run the sample code based on Doc. So we need to add to that.",pull-request-available,[],HDDS,Improvement,Major,2020-11-26 08:29:22,4
13342761,Return forbidden instead of interval server error from s3g when user doesn't have the resouce permission ,"[root@host183]# aws s3api --profile kona_prof_user  --endpoint-url http://host183:9878 list-objects-v2 --bucket=konajdk-profiler | grep Size | wc -l

An error occurred (500) when calling the ListObjectsV2 operation (reached max retries: 4): Internal Server Error
",pull-request-available,[],HDDS,Bug,Major,2020-11-26 06:47:38,5
13342725,[Doc] Add zh translation to Recon Architecture,"HDDS-4392 added documentation for Recon Architecture and HTTP endpoints.

We need zh translations for the same.",Translation doc newbie pull-request-available,['documentation'],HDDS,Task,Major,2020-11-25 23:37:23,17
13342722,Datanodes should be able to persist and load CRL,Ozone Datanodes should be able to persist or load Certificate Revocation List (CRL) ,pull-request-available,['Security'],HDDS,Sub-task,Major,2020-11-25 23:01:49,37
13342638,AllocateBlock : lookup and update open file table for the given path,This task is to use open file table for the allocate block operations. This has been identified as part of TPCDS benchmark test.,pull-request-available,['OM'],HDDS,Sub-task,Major,2020-11-25 15:37:30,40
13342637,[FSO]OzoneContract unit test case fixes,"Following are some of the gaps identified as part of OzoneContract test cases. This task is to fix those cases.
 # {{BasicOzoneFileSystem#renameV1}} throws OMException to the client instead of specific FileAlreadyExistsException, FileNotFoundException cases.",pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2020-11-25 15:33:24,26
13342622,Remove unused netty3 transitive dependency,"Ozone uses Netty either as direct dependency (ozone-csi) or from the ratis shaded dependency (for ratis gprc server). Both use Netty 4.x.

But netty 3 is also included in share/lib/ozone which is not required. The declared netty 3 version has security issues, we need to remove it to make it clear it's not used. (And make classpath safer)

It turned out that netty (and other dependencies) came with the test-jar dependencies used from Hadoop.

Based on the reference of Maven, compile time dependencies of a test dependency should be used as test dependency (https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html) but in this case it doesn't work:

{code}
cd hadoop-hdds/container-service
mvn dependency:tree

...
[INFO] +- org.apache.hadoop:hadoop-hdfs:test-jar:tests:3.2.1:test
[INFO] |  +- org.eclipse.jetty:jetty-server:jar:9.4.34.v20201102:test
[INFO] |  |  +- org.eclipse.jetty:jetty-http:jar:9.4.34.v20201102:test
[INFO] |  |  \- org.eclipse.jetty:jetty-io:jar:9.4.34.v20201102:test
[INFO] |  +- org.eclipse.jetty:jetty-util-ajax:jar:9.4.34.v20201102:test
[INFO] |  +- com.sun.jersey:jersey-core:jar:1.19:test
[INFO] |  |  \- javax.ws.rs:jsr311-api:jar:1.1.1:test
[INFO] |  +- com.sun.jersey:jersey-server:jar:1.19:test
[INFO] |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.11:compile
[INFO] |  +- commons-daemon:commons-daemon:jar:1.0.13:test
[INFO] |  +- javax.servlet:javax.servlet-api:jar:3.1.0:test
[INFO] |  +- io.netty:netty:jar:3.10.5.Final:compile
[INFO] |  +- org.apache.htrace:htrace-core4:jar:4.1.0-incubating:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-databind:jar:2.10.3:compile
[INFO] \- junit:junit:jar:4.11:test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
...
{code}

Here all the dependencies of the hadop-hdfs:test-jar suppposed to have test scope.

I didn't find the exact MVN issue, but found that there are multiple open issues related to transitive dependency resolution (can be the https://issues.apache.org/jira/browse/MNG-1378, but there are other open issues, too).

As a result, we should remain on the same side. I ssugest:

 1. Exclude ALL the TRANSITIVE test dependencies for hadoop test-jars. Hadoop test-jars can still be used, but if we need any other class, they should be requested with an explicit dependency

 2. hadoop-ozone-dependency-test should be used everywhere instead of using hadoop-hdfs or hadoop-common test jars (because it includes all the required excludes ;-) ) ",pull-request-available,[],HDDS,Bug,Major,2020-11-25 14:39:41,6
13342598,HDDS-4511: Avoiding StaleNodeHandler to take effect in TestDeleteWithSlowFollower.,"This improvement is inspired by the fixing of TestDeleteWithSlowFollower in the broken HDDS-2823.

 

In the test case TestDeleteWithSlowFollower, there is following trace appearing in the log
{code:java}
2020-11-24 19:32:13,551 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 132e6d1b-e472-449e-929e-5f42b87114c6{ip: 10.73.23.64, host: 10.73.23.64, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=6f0e173c-b5e2-4dc6-99e1-854aafdc8295, PipelineID=c78bc2fb-dca1-4e09-ba71-dd824e2d4e73]2020-11-24 19:32:13,552 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (PipelineManagerV2Impl.java:closePipeline(389)) - Pipeline Pipeline[ Id: 6f0e173c-b5e2-4dc6-99e1-854aafdc8295, Nodes: 132e6d1b-e472-449e-929e-5f42b87114c6{ip: 10.73.23.64, host: 10.73.23.64, networkLocation: /default-rack, certSerialId: null}46a77559-9d5c-4a1d-bad7-e7eb7b9c32da{ip: 10.73.23.64, host: 10.73.23.64, networkLocation: /default-rack, certSerialId: null}524fea63-ad85-4a3a-bcfb-ac40dfe3d5e7{ip: 10.73.23.64, host: 10.73.23.64, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:46a77559-9d5c-4a1d-bad7-e7eb7b9c32da, CreationTimestamp2020-11-24T11:30:23.805Z] moved to CLOSED state

{code}
 

 But by design of this case, the stale node handler should not take effect.
{code:java}
// Make the stale, dead and server failure timeout higher so that a dead
// node is not detecte at SCM as well as the pipeline close action
// never gets initiated early at Datanode in the test.{code}
 

This test case relies on ReplicationManager to close the OPEN container in SCM, so that SCM won't hold the delete blocks command. 

It can send out the close container command either because it is an OPEN container but under replicate or it is an OPEN container but it has CLOSED replica.

Since the default interval of RM is 5m, the test case actually relies the ""it is an OPEN container but under replicate"" to avoid trigger the stale node handler..

 

But the command disappears, since ReplicationManager#isContainerUnderReplicated does not consider OPEN container, it only take care of CLOSED and QUASI_CLOSED container.

 

After talked with [~Sammi], by design, it just needs to explicitly avoid replicating container in DELETING or DELETED state.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-11-25 12:33:14,36
13342593,SCM can avoid creating RetriableDatanodeEventWatcher for deletion command ACK,"As blockDeletingService is not using RETRIABLE_DATANODE_COMMAND for datanode deletion commands (HDDS-4487), SCM can avoid RetriableDatanodeEventWatcher which is currently created for watching over deletion command ACK.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-11-25 11:52:32,27
13342469,Add total container count to SCMContainerMetrics,"While working on a SCM issue with [~shashikant], we found that there is not a metric for ""total"" container count. Currently, there are OpenContainers, ClosingContainers, QuasiClosedContainers, ClosedContainers, DeletingContainers, DeletedContainers. So, a total container count would be the sum of all.",pull-request-available,['SCM'],HDDS,Task,Major,2020-11-24 21:11:14,17
13342352,Support query parameter based v4 auth in S3g,"AWS S3 accepts authorization information both from headers and query parameter.

Ozone s3g implementation parses only the headers.",pull-request-available,[],HDDS,Improvement,Major,2020-11-24 12:00:07,6
13342332,Increase default value for SCM heartbeat timeout to 5s,"On increasing the block deletion limit for SCM from 10000 blocks per interval to 100000 blocks, the heartbeats were timing out in the datanodes. Current rpc timeout configured for SCM heartbeat is 1 second. The jira proposes to increase the rpc timeout to 5 seconds. 
With 5 second configuration timeouts were not seen by the datanodes.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2020-11-24 10:27:36,35
13342326,Datanode deletion config should be based on number of blocks,"Currently datanode's deletion config is based on number of containers on which deletion should be performed(default value 10). The deletion is only performed on those containers. 

It should rather be based on number of blocks to delete per interval rather than number of containers to process. If a datanode has 100000 containers and every container has 10 blocks to delete then deletion of all these blocks can take a lot of time with the current configuration. It would take 100000/10 = 10000 intervals for deletion to complete.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-11-24 10:10:26,27
13342313,Provide info on block size via FileSystem,Provide information about Ozone's configured block size via Hadoop-compatible file system implementations: {{getDefaultBlockSize()}} and {{getDefaultBlockSize(Path)}} in {{FileSystem}}.,pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2020-11-24 09:21:07,1
13342255,[OFS] Listing volumes under root should return all volumes whenever possible,"Currently {{listStatusRoot()}} only lists volumes owned(ACL disabled) or accessible(ACL enabled) by current user.

This prevents HttpFS from listing all volumes under OFS root.

Since we probably can't provide an argument to allow passing in parameter {{--all}} due to conforming to HCFS, we should default to list ALL volumes whenever this is possible.

Note: {{ozone.om.volume.listall.allowed}} is an OM side argument that controls whether any user can list all volumes on an Ozone cluster, it defaults to {{true}}.",pull-request-available,[],HDDS,Sub-task,Major,2020-11-24 03:17:24,12
13342238,Reload OM State fail should terminate OM for any exceptions,"Right now OM is terminating only when IOException when reload terminate happens, OM should terminate in case of any exceptions.

In one of our customer issue we have seen reloadOMState failed with IllegalStateException due to a bug which is fixed by HDDS-3944 and caused below issue, as installSnapshot failed we donot update commitIndex, lastWrittenIndex in SegmentedRaftLogWorker.



{code:java}
2020-11-20 01:17:01,990 ERROR org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker: om3@group-0C953F6B62D0-SegmentedRaftLogWorker hit exception
java.lang.IllegalStateException: lastWrittenIndex == 134264, entry == term: 393
index: 134278
metadataEntry {
  commitIndex: 134277
}

        at org.apache.ratis.util.Preconditions.assertTrue(Preconditions.java:63)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$WriteLog.execute(SegmentedRaftLogWorker.java:507)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:302)
        at java.base/java.lang.Thread.run(Thread.java:834)
{code}

",pull-request-available,[],HDDS,Bug,Major,2020-11-24 01:33:10,13
13342190,Enable OM Ratis by default,"Currently, by default, Ratis is not enabled on an OM single node cluster. This Jira proposes to change the default single node OM to use Ratis server. 

OM Ratis has been tested extensively and is stable now. To convert a single node cluster to HA, the first step would be to upgrade the cluster to Ratis enabled cluster. 

A non-ratis single node cluster can be upgraded to  ratis-enabled single node cluster by just setting the \{{ozone.om.ratis.enable}} config to true and restarting the OM.",pull-request-available,['OM'],HDDS,Improvement,Major,2020-11-23 18:16:22,43
13342185,Recon File Size Count task throws SQL Exception.,"{code}
Caused by: org.jooq.exception.DataAccessException: SQL [insert into FILE_COUNT_BY_SIZE (volume, bucket, file_size, count) values (?, ?, ?, ?)]; [SQLITE_CONSTRAINT]  Abort due to constraint violation (UNIQUE constraint failed: FILE_COUNT_BY_SIZE.volume, FILE_COUNT_BY_SIZE.bucket, FILE_COUNT_BY_SIZE.file_size)
        at org.jooq_3.11.9.SQLITE.debug(Unknown Source)
        at org.jooq.impl.Tools.translate(Tools.java:2429)
        at org.jooq.impl.DefaultExecuteContext.sqlException(DefaultExecuteContext.java:832)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:364)
        at org.jooq.impl.TableRecordImpl.storeInsert0(TableRecordImpl.java:202)
        at org.jooq.impl.TableRecordImpl$1.operate(TableRecordImpl.java:173)
        at org.jooq.impl.RecordDelegate.operate(RecordDelegate.java:125)
        at org.jooq.impl.TableRecordImpl.storeInsert(TableRecordImpl.java:169)
        at org.jooq.impl.TableRecordImpl.insert(TableRecordImpl.java:157)
        at org.jooq.impl.TableRecordImpl.insert(TableRecordImpl.java:152)
        at org.jooq.impl.DAOImpl.insert(DAOImpl.java:175)
        at org.jooq.impl.DAOImpl.insert(DAOImpl.java:151)
        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.lambda$writeCountsToDB$0(FileSizeCountTask.java:209)
        at java.util.HashMap$KeySet.forEach(HashMap.java:933)
        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.writeCountsToDB(FileSizeCountTask.java:181)
        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.reprocess(FileSizeCountTask.java:100)
        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$reInitializeTasks$3(ReconTaskControllerImpl.java:175)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        ... 3 more
Caused by: org.sqlite.SQLiteException: [SQLITE_CONSTRAINT]  Abort due to constraint violation (UNIQUE constraint failed: FILE_COUNT_BY_SIZE.volume, FILE_COUNT_BY_SIZE.bucket, FILE_COUNT_BY_SIZE.file_size)
        at org.sqlite.core.DB.newSQLException(DB.java:941)
        at org.sqlite.core.DB.newSQLException(DB.java:953)
        at org.sqlite.core.DB.execute(DB.java:854)
        at org.sqlite.core.DB.executeUpdate(DB.java:895)
        at org.sqlite.jdbc3.JDBC3PreparedStatement.executeUpdate(JDBC3PreparedStatement.java:102)
        at org.jooq.tools.jdbc.DefaultPreparedStatement.executeUpdate(DefaultPreparedStatement.java:99)
        at org.jooq.impl.AbstractDMLQuery.execute(AbstractDMLQuery.java:629)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350)
        ... 17 more
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Critical,2020-11-23 17:45:19,30
13342121,Separate client and server2server GRPC services of datanode,"Today both the closed-container-replication service and datanode client service are exposed on the same datanode port which makes it impossible to use different network configuration (like mutual TLS). 

In this patch I propose to separated the two service and use two network port. ",incompatible pull-request-available,[],HDDS,Improvement,Critical,2020-11-23 12:41:32,6
13341983,Provide Guice module to inject configuration to annotated fields,"hadoop-hdds/config project provides a light-weight annotation based configuration interface. It supports to create an object with injection:


{code:java}
ReplicationConfig replicationConfig := ozoneConfig.getObject(replicationConfig); {code}
However, it seems to be hard to inject configuration to the service itself as usually we inject a lot of other dependencies to the constructor, not just the configuration.

 

One possible solution is using a Guice module. Guice is already used by recon, and this patch adds some optional modules to inject configuration variables to any service / instance if required.",pull-request-available,[],HDDS,Improvement,Major,2020-11-22 12:59:56,6
13341824,CLI flag --quota should default to 'spaceQuota' to preserve backward compatibility.,"In HDDS-3725, we introduced changes to support --spaceQuota and --countQuota by doing away with the generic --quota flag. This breaks 1.0.0 compatibility since existing systems using --quota would face issues. We have to alias --quota to the --spaceQuota since that was the behavior in 1.0.0.

cc. [~micahzhao] ",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-11-20 17:32:18,59
13341823,Deprecate raft.server.rpcslowness.timeout config key,"If and when a config is removed or replaced by a new config, the old config key must be deprecated. There should be deprecation warning message when the old config key is used.

For example, HDDS-4432 changed a Ratis config {{rpcslowness.timeout}} to {{rpc.slowness.timeout.}}

Refer to Recon's {{ConfigurationProvider}} for handling deprecated configs using {{DeprecationDelta}}.",pull-request-available,[],HDDS,Bug,Major,2020-11-20 17:30:19,61
13341787,[FSO]RenameAndDelete : make ofs#rename and ofs#delete an atomic operation,This task is to implement {{BasicRootedOzoneFileSystem#rename}} and {{BasicRootedOzoneFileSystem#delete}} an atomic operation.,pull-request-available,[],HDDS,Sub-task,Major,2020-11-20 12:40:54,26
13341627,Open RocksDB read only when loading containers,"When a datanode is started, it must read some metadata from all the Containers. Part of that metadata is stored in RocksDB, so the startup process involves opening each rocksDB and closing it again.

Testing on a dense node, with 45 high performance spinning disks and 200K containers, I saw about 75ms on average to open each RockDB. Further testing demonstrated that if we open RockDB read only, the average open time is about 35ms.

At startup time, the DBs are only read and never written, so opening read only is fine. HDDS-4427 already ensures these opened DBs are not cached.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2020-11-19 16:55:41,11
13341598,SCM can avoid using RETRIABLE_DATANODE_COMMAND for datanode deletion commands,RETRIABLE_DATANODE_COMMAND are used to retry deletion commands to datanode in case of timeout. SCM can avoid using RETRIABLE_DATANODE_COMMAND for deletion commands because the service would be sending any pending transactions in the next iteration.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-11-19 14:27:55,27
13341557,Feature Config : Make proper enableFSPaths and OFS optimized flag combinations,"Presently OM has four combinations, which leads to complexity as objectStore#rename can't be differentiated with fs#rename.
 # ofsOptimized=true && enableFSPaths=true
 # ofsOptimized=true && enableFSPaths=false
 # ofsOptimized=false && enableFSPaths=true
 # ofsOptimized=false && enableFSPaths=false

This jira proposal is to simplify the ofs optimization cases. OM can support ofs optimization only if both are TRUE({{ofsOptimized=true && enableFSPaths=true)}} and will write table key entries in NEW_FORMAT(prefix separated format using objectID). All the other cases, it will write table key entries in OLD_FORMAT(existing format).

[Reference-1|https://github.com/apache/ozone/pull/1557#discussion_r520962991]
[Reference-2|https://github.com/apache/ozone/pull/1473#discussion_r502350594]",pull-request-available,['OM'],HDDS,Sub-task,Major,2020-11-19 11:05:11,40
13341505,Modification of the operating introduction of ozone ranger.,There are few descriptions of Ranger in Ozone documents. I tried to install Ranger locally and verified the authentication strategy of Ozone. These need to be added to the existing documentation.,pull-request-available,[],HDDS,Improvement,Major,2020-11-19 06:22:23,4
13341464,Use RatisServerImpl isLeader instead of periodic leader update logic in OM and isLeaderReady for read requests,"Previously RatisServer which is network-partitoned/split-brain does not step down from leader state, so we require custom logic to determine leader(But that is also not completely correct, as the role state is updated based on server state not a quorum based information)

Now, in Ratis leader steps down after the leader election max time out, so we can use RaftServer Api isLeader check. (RATIS-981 fixed this behavior)

This also fixes when serving read requests it should check leaderReady, not just isLeader because when it is leader, it might be still applying pending commit transactions, so there is a chance that acked transactions of write, might not be visible until we wait for isLeaderReady.
",pull-request-available,[],HDDS,Improvement,Major,2020-11-19 00:45:22,13
13341404,Datanodes should send last processed CRL sequence ID in heartbeats,Datanodes should include their latest processed CRL sequence ID in heartbeats to keep SCM updated on the status of CRL.  ,pull-request-available,['Security'],HDDS,Sub-task,Major,2020-11-18 18:15:49,37
13341403,SCM should be able to persist CRL,Ozone SCM should be able to persist Certificate Revocation List (CRL). We should add a new table in SCM DB definition to persist CRL and sequence id.,pull-request-available,['Security'],HDDS,Sub-task,Major,2020-11-18 18:13:05,37
13341235,With HA OM can send deletion blocks to SCM multiple times,"Currently OM deletion service iterates through deleted keys table and sends blocks for deletion to SCM. I can see that blocks for same key are sent 10 times from OM to SCM. This would lead to creation of 10 transactions for the same blocks in SCM.
Apparently the db is not updated and OM keeps sending the same set of keys for about 10 minutes.

cc [~bharat] [~hanishakoneru]",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2020-11-18 06:46:40,13
13341153,Implement OM Prepare Request/Response,"Add the new OM request that will be used to prepare the OM for upgrade. In the applyTransaction step for this request:
 # Wait for double buffer flush to finish.
 # Take a snapshot.
 # purge the log.

The transaction log index of this request will be returned in the response to the caller.",pull-request-available,['OM'],HDDS,Sub-task,Major,2020-11-17 20:59:56,22
13341108,Large deletedKeyset slows down OM via listStatus,"During TPC-DS test runs with Hive backed by Ozone storage I noticed the following:

As part of data generation we create tables, and with auto partitioning turned on, some of the tables have a couple thousand partitions created.
This process involves a couple of renames of the different data files while MR containers are working on to create the data files, and move them into their final position.

After this, when I start to run queries, certain parts of the query run work on to split the work for mappers, and in order to do this, they do listStatus calls, as I tracked down and understood the code, there is a recursive listing is happening, that filters out files on the client side. This means that for a table where we have 3K partitions, Hive issues 3K listsSatus calls on the partition directories.

During this phase the query is stuck INITIALIZING containers and query phases for a long time, which gave me the possibility to take a couple of jstacks, and all of them has 5-10 threads with this particular stack trace:
{code}
""IPC Server handler 90 on 9862"" #147 daemon prio=5 os_prio=0 tid=0x00007fec32dd7800 nid=0x19ac2 runnable [0x00007febe13d9000]
   java.lang.Thread.State: RUNNABLE
        at java.util.TreeMap.put(TreeMap.java:575)
        at java.util.TreeSet.add(TreeSet.java:255)
        at org.apache.hadoop.ozone.om.KeyManagerImpl.listStatusFindKeyInTableCache(KeyManagerImpl.java:2041)
        at org.apache.hadoop.ozone.om.KeyManagerImpl.listStatus(KeyManagerImpl.java:2117)
        at org.apache.hadoop.ozone.om.OzoneManager.listStatus(OzoneManager.java:2938)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.listStatus(OzoneManagerRequestHandler.java:573)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:197)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:218)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:145)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB$$Lambda$88/1748961453.apply(Unknown Source)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)
        at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)
{code}

This is happening with a recent (maybe 1 week old tops) master build.

After I took heapdumps from the OM during query runs, at two different times, I saw the following concerning numbers:
From CacheKey instances we have 78k in the earlier one, and 80K in the older one
From CacheValue instance we have 78K in both.
Almost all of them has the OMMetadataManager as GC root, and belongs to the keytable cache.

During listStatusFindKeyInTableCache, we iterate through the cache entries via the cache iterator, and if the cacheKey is not equal to the keyArgs, and cacheValue is null, we add the key to the deletedKeySet.
Where this method is called, is KeyManager's listStatus method, which for every call creates a new deletedKeySet as a TreeSet. Then it goes over all keys in the bucket that are cached, and puts together the deletedKeySet. We seem to need this deletedKeySet in order to leave out keys that are already deleted in the cache but still present in the db from the listing which is a valid reason, but causes a significant slowdown (1-2 order of magnitude) compared to runtimes after OM restart.

In the heap dump the cache values are either an instance of Present, or Absent, and the majority seems to be Absent. Based on the epoch even present values and absent values are from various epochs, from 1 to 300K+.

Int the TableCacheImpl, we have an evictCache method that happens in an executor service, and which should run on DoubleBuffer flush.
It gets a list of epochs to evict, and it iterates through the epochEntries the cache has, and, as we talk about the KeyTable that has a MANUAL eviction policy, it will remove the entry from the cache if the entry's epoch is the current epoch in the iteration, and if the supplied epochs to evict contain the current epoch.

Epochs come from DoubleBuffer#flushTransactions.
It starts as a HashMap of Strings and List of Longs, and DoubleBuffer puts together the list of epochs for a table during flush, based on table names registered for Responses with the CleanupTableInfo annotation.

The only suspicious thing I found, is that OMFileCreateResponse does not define the key table as something to clean up, and it parents neither has it defined for cleanup, while the OMFileCreateRequest adds the parents of the created file to the key table cache [here|https://github.com/apache/ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileCreateRequest.java#L299-L301].


",pull-request-available,[],HDDS,Bug,Critical,2020-11-17 16:30:22,63
13341041,Delete txnId in SCMMetadataStoreImpl may drop to 0 after SCM restart.,"*Summary*

If SCM cleans up all the DeletedBlocksTransaction in deletedBlocksTable, then restart, the txID in SCMMetadataStoreImpl will drop to 0.

 

*Detail*

in SCMMetadataStoreImpl.java
{code:java}
public SCMMetadataStoreImpl(OzoneConfiguration config)
    throws IOException {
  this.configuration = config;
  start(this.configuration);
  this.txID = new AtomicLong(this.getLargestRecordedTXID());
}

private Long getLargestRecordedTXID() throws IOException {
  try (TableIterator<Long, ? extends KeyValue<Long, DeletedBlocksTransaction>>
      txIter = deletedBlocksTable.iterator()) {
    txIter.seekToLast();
    Long txid = txIter.key();
    if (txid != null) {
      return txid;
    }
  }
  return 0L;
}{code}
 

in DeletedBlockLogImpl.java
{code:java}
public void commitTransactions(
    List<DeleteBlockTransactionResult> transactionResults, UUID dnID) {
  ...
  scmMetadataStore.getDeletedBlocksTXTable().delete(txID);
  ...
}{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2020-11-17 12:07:40,35
13341030,Improve the ZH translation of the HA.md in doc,Fix the parent tag in ZH translation of the HA.md doc.,pull-request-available,[],HDDS,Improvement,Minor,2020-11-17 11:17:16,64
13341029,Extend DatanodeChunkGenerator to write all on all pipelines of a set of dns,"Currently, DatanodeChunkGenerator works against a single pipeline. The idea here is extend the tool to make it work against all pipelines constituted by a set of datanodes or a set of pipelines . This will be really useful to test/validate MultiRaft.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-11-17 11:11:04,26
13340978,GrpcOutputStream length can overflow,"DN says it sent negative bytes of container data to destination.

{noformat}
2020-11-16 22:07:26,445 INFO org.apache.hadoop.ozone.container.replication.GrpcReplicationService: Streaming container data (982134) to other datanode
2020-11-16 22:16:37,693 INFO org.apache.hadoop.ozone.container.replication.GrpcOutputStream: Sent -2033242913 bytes for container 982134
{noformat}

Looks like there was an overflow. The writtenBytes shouldn't be an int, because the container is as big as 5GB, easily overflow a signed int.

{code:title=GrpcOutputStream.java}
  private int writtenBytes;

...
      CopyContainerResponseProto response =
          CopyContainerResponseProto.newBuilder()
              .setContainerID(containerId)
              .setData(data)
              .setEof(eof)
              .setReadOffset(writtenBytes)
              .setLen(length)
              .build();
      responseObserver.onNext(response);
      writtenBytes += length;
{code}

Looking at the source code, I think we can change it to long without breaking serialization compatibility because setReadOffset() expects a long.

Looks like a minor issue to me. The other side doesn't use the read offset, so even though it can overflow, there's no real impact. Still, we should fix it.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-11-17 07:00:08,61
13340814,Update url in .asf.yaml to use TLP project,"Change URL on github page to the new TLP domain.

 

Thanks the report for [~cxorm]

https://github.com/apache/ozone/pull/1540#issuecomment-722217690

 ",pull-request-available,[],HDDS,Improvement,Trivial,2020-11-16 10:57:21,6
13340631,Upgrade httpclient version due to CVE-2020-13956,"According to  CVE-2020-13956 https://www.openwall.com/lists/oss-security/2020/10/08/4 ,
{quote}
Apache HttpClient versions prior to version 4.5.13 and 5.0.3 can
misinterpret malformed authority component in request URIs passed to
the library as java.net.URI object and pick the wrong target host for
request execution.  
{quote}
",pull-request-available,[],HDDS,Bug,Major,2020-11-14 10:34:33,9
13340571,Verify that OM/SCM start fails when Software Layout Version < Metadata Layout Version,Add tests for OzoneManager and StorageContainerManager classes that will verify an Exception is thrown on startup when the metadata layout version on disk is larger than the software version of the component. A similar test will be added for the datanode after the DN version file logic is refactored in a later non-rolling upgrade Jira.,pull-request-available,"['OM', 'Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2020-11-13 18:12:23,22
13340569,Add --frozen-lockfile to pnpm install to prevent ozone-recon-web/pnpm-lock.yaml from being updated automatically,"Update {{ozone-recon-web/pnpm-lock.yaml}} as Git is reporting not clean after a maven build.

CC [~vivekratnavel]",pull-request-available,['Ozone Recon'],HDDS,Improvement,Trivial,2020-11-13 17:56:12,12
13340496,Reuse compiled binaries in acceptance test,"Save Ozone binaries from {{compile}} check, and use these in {{acceptance}} and {{kubernetes}} checks, instead of each check performing its own full build.  Total execution time is similar, but the dependent checks are started later, so we save GitHub Actions cycles.",pull-request-available,['build'],HDDS,Improvement,Major,2020-11-13 11:08:36,1
13340490,Add metrics for closed container replication,Today it's hard to understand what's going on and what is the performance of the replication.,pull-request-available,[],HDDS,Sub-task,Major,2020-11-13 10:25:58,6
13340191,Fix typo in README.md doc,Fix typo in README.md. ,pull-request-available,['documentation'],HDDS,Improvement,Major,2020-11-12 11:04:48,52
13339996,Replicate closed container from random selected datanode ,"[~AlfredChang] reported a very particular case related to the data replication:

When a container is downloaded successfully but the content is invalid, the error the retry mechanism doesn't work very well. Downloader reports the download successfull therefore it won't be reported as failure, but import will fail again and again.

To make it more resilient we can select datanode randomly,",pull-request-available,[],HDDS,Sub-task,Major,2020-11-11 14:29:47,6
13339955,findbugs.sh couldn't be executed after a full build,"./hadoop-ozone/dev-support/checks/findbugs.sh -- which is a short-cut to execute the CI findbugs check locally -- couldn't be executed locally after a full build:

{code}
./hadoop-ozone/dev-support/checks/findbugs.sh
....
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3.451 s
[INFO] Finished at: 2020-11-11T11:42:40+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs (spotbugs) on project hadoop-hdds: Execution spotbugs of goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs failed: Java returned: 1 -> [Help 1]
[ERROR] 
{code}

The problem:

`target/classes` directory should be either empty/missing or it should contain java classes to make spotbugs work.

On github it works well as an empty checkout is tested. But locally it's possible that a dummy classpath file is created under `hadoop-hdds/target/classes` which breaks spotbug local execution.

The solution is easy: execute the classpath descriptor generation only if `src/main/java` dir exists.",pull-request-available,[],HDDS,Bug,Major,2020-11-11 10:44:44,6
13339951,Handle start & stop of Trash Emptier thread when node becomes leader/follower,Add handling for when to start/stop thread when OM switches from follower to leader or vice-versa.,pull-request-available,[],HDDS,Sub-task,Major,2020-11-11 10:03:15,26
13339912,Cannot run ozone if HADOOP_HOME points to Hadoop install,"Currently Ozone relies on {{HADOOP_\*}} environment variables (eg. {{HADOOP_HOME}}) for historical and practical reasons (code reuse).  If Hadoop and Ozone are both present on a node, and they share their environment, then {{ozone}} (and {{stop-ozone.sh}}) exits with error:

{code}
$ ozone help
ERROR: Cannot execute /usr/local/hadoop/libexec/ozone-config.sh.
{code}

The same problem was reported and fixed in HDDS-1912 for the {{start-ozone.sh}} script.  This task is limited to applying the changes from HDDS-1912 to the other two scripts.  Ideally Ozone should use its own environment variables, which is a much larger change.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-11-11 08:08:38,1
13339796,Rename config ozone.datanode.pipeline.limit,"I'm trying to enable multi-raft. But the naming of the config property ""ozone.datanode.pipeline.limit"" is confusing to me. I would have thought this is a DN side configuration because of the ""ozone.data"" prefix. Turns out it's a SCM config. Can we move it to ozone.scm.*?",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-11-10 16:55:38,61
13339736,Duplicate refreshPipeline in listStatus,"{{KeyManagerImpl#listStatus}} issues duplicate {{refreshPipeline}} for each file.

HDDS-3824 moved {{refreshPipeline}} outside the bucket lock.  But HDDS-3658 added it back, while keeping the one outside the lock, probably as a result of merge conflict resolution.

Also, [~weichiu] pointed out that {{refreshPipeline}} supports lists, too, so the calls can be batched to reduce number of RPC.

CC [~msingh]",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-11-10 12:19:19,1
13339679,SCMBlockDeletingService should handle ContainerNotFoundException,"SCMBlockDeletingService terminates task on receiving ContainerNotFoundException. As a result SCM stops deleting blocks.
{code:java}
2020-11-09 23:53:10,026 ERROR org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService: Failed to get block deletion transactions from delTX log
org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: Container with id #193 not found.
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:542)
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.getContainerInfo(ContainerStateMap.java:188)
        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.getContainer(ContainerStateManager.java:499)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainer(SCMContainerManager.java:212)
        at org.apache.hadoop.hdds.scm.block.DeletedBlockLogImpl.getTransactions(DeletedBlockLogImpl.java:364)
        at org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService$DeletedBlockTransactionScanner.call(SCMBlockDeletingService.java:126)
        at org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService$DeletedBlockTransactionScanner.call(SCMBlockDeletingService.java:106)
        at org.apache.hadoop.hdds.utils.BackgroundService$PeriodicalTask.lambda$run$0(BackgroundService.java:112)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}
",pull-request-available,['SCM'],HDDS,Bug,Major,2020-11-10 08:15:09,27
13339675,Update Jetty version to 9.4.34,We are on 9.4.26. The latest is 9.4.34. We should update.,pull-request-available,[],HDDS,Improvement,Major,2020-11-10 07:54:32,25
13339657,Owner info is not passed to authorizer for BUCKET/KEY create request,"HDDS-4088 add the owner info to the authorizer access check context. There is a bug in the getOwner info logic which is supposed to skip volume owner info only for volume create or volume root access. 

This ticket is opened to fix the issue above. ",pull-request-available,[],HDDS,Bug,Major,2020-11-10 05:02:15,28
13339653,Remove KeyManagerImpl#refreshPipeline because it is the same as refresh(),"{code:java}
/**
 * Refresh pipeline info in OM by asking SCM.
 * @param value OmKeyInfo
 */
@VisibleForTesting
protected void refreshPipeline(OmKeyInfo value) throws IOException {
  Preconditions.checkNotNull(value, ""OMKeyInfo cannot be null"");
  refreshPipeline(Arrays.asList(value));
} {code}

{code}
/**
   * Refresh the key block location information by get latest info from SCM.
   * @param key
   */
  public void refresh(OmKeyInfo key) throws IOException {
    Preconditions.checkNotNull(key, ""Key info can not be null"");
    refreshPipeline(Arrays.asList(key));
  }
{code}
Both methods are essentially the same. I suggeset remove refreshPipeline() since it's protected.",pull-request-available,['OM'],HDDS,Improvement,Minor,2020-11-10 03:58:26,55
13339651,Recon: Using Mysql database throws exception and fails startup,"Recon uses Derby as its SQL database by default. Switching it to Mariadb/Mysql throws an exception `Specified key was too long; max key length is 767 bytes`

Fix this issue by reducing the primary key length from VARCHAR(768) to VARCHAR(766) in ReconTaskSchemaDefinition.java. Also change the autocommit config to default to true since data is not persisted in MySql tables even after insert queries are executed with autocommit flag set to false by default.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-11-10 03:19:17,37
13339611,Add metrics for ACL related operations,"Looks like we are missing a few OM metrics, especially the ACL related ones:
* getAcl
* setAcl
* removeAcl
* addAcl
",pull-request-available,['OM'],HDDS,Improvement,Major,2020-11-09 22:36:34,59
13339507,Design Doc: Use per-request authentication and persistent connections between S3g and OM,"Design doc will be uploaded, soon.",pull-request-available,[],HDDS,Improvement,Major,2020-11-09 11:21:22,6
13339483,Avoid counting skipped transactions belonging to open containers,"HDDS-4366 also counts transactions belonging to open containers. Although SCM skips these transactions, it still counts them which leads to sending a lower number of transactions to the datanodes.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-11-09 10:04:36,35
13339368,Avoid unnecessary builder conversion in setting volume Quota/Owner request,"Thanks [~xyao] for careful [review|https://github.com/apache/ozone/pull/1301#issuecomment-718396168].

Here we have 2 OMRequest to fix, 
1. [OMVolumeSetOwnerRequest#preExecute()|https://github.com/apache/ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetOwnerRequest.java#L65#L76]

2. [OMVolumeSetQuotaRequest#preExecute()|https://github.com/apache/ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetQuotaRequest.java#L71#L82]

Feel free to discuss about it if I miss anything, thanks.",pull-request-available,[],HDDS,Improvement,Minor,2020-11-08 08:13:23,42
13339313,TestReplicationManager#testUnderReplicatedQuasiClosedContainerWithUnhealthyReplica fails intermittently ,"{noformat}
testUnderReplicatedQuasiClosedContainerWithUnhealthyReplica(org.apache.hadoop.hdds.scm.container.TestReplicationManager)  Time elapsed: 0.927 s  <<< FAILURE!testUnderReplicatedQuasiClosedContainerWithUnhealthyReplica(org.apache.hadoop.hdds.scm.container.TestReplicationManager)  Time elapsed: 0.927 s  <<< FAILURE!java.lang.AssertionError: expected:<1> but was:<0> at {noformat}

Ref : https://github.com/apache/ozone/pull/1556/checks?check_run_id=1367180111#step:3:3939",pull-request-available,[],HDDS,Bug,Major,2020-11-07 11:10:39,59
13339130,Fix typo in Admin.md,"In [this line|https://github.com/apache/ozone/blame/master/hadoop-hdds/docs/content/tools/Admin.md#L28] and [this line|https://github.com/apache/ozone/blame/master/hadoop-hdds/docs/content/tools/Admin.md#L31], it should be *replication* and *rack-awareness*.",pull-request-available,[],HDDS,Improvement,Trivial,2020-11-06 09:01:00,17
13338742,Update Ratis version to latest snapshot,"This Jira aims to update ozone with latest Ratis snapshot which has a critical fix for ""Bootstrap new OM Node"" feature - HDDS-4330.",pull-request-available,[],HDDS,Bug,Major,2020-11-03 23:18:52,43
13338737,Improve output message for key/bucket list command,"Currently when a user which doesn't have permission to list on a bucket, they get the following message on stderr:
{noformat}
$ ozone sh key list o3://ozone1/volume1/bucket1

PERMISSION_DENIED org.apache.hadoop.ozone.om.exceptions.OMException: User  <user> doesn't have LIST permission to access bucket{noformat}
We should hide the  *org.apache.hadoop.ozone.om.exceptions.OMException:* from the error message so that it is consistent with other PERMISSION_DENIED messages we get with other commands where we don't have this Exception class shown.

 ",pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2020-11-03 23:00:53,22
13338726,OM failover timeout is too short,"The current OM has one second failover timeout. This is too short as any network hiccup, system I/O or JVM GC pause could easily trigger a failover.

Example:
{noformat}
2020-10-29 09:02:46,557 WARN org.apache.ratis.server.impl.RaftServerImpl: om3@group-942F8267F22A-LeaderState: Lost leadership on term: 33. Election timeout: 1200ms. In charge for: 82665
0319ms. Conf: 32189729: [om1:rhelnn01.ozone.cisco.local:9872:0, om3:rhelnn03.ozone.cisco.local:9872:0, om2:rhelnn02.ozone.cisco.local:9872:0], old=null. Followers: [om3@group-942F8267F2
2A->om1(c34577386,m34577394,n34577395, attendVote=true, lastRpcSendTime=7, lastRpcResponseTime=0), om3@group-942F8267F22A->om2(c34577386,m34577261,n34577395, attendVote=true, lastRpcSen
dTime=7, lastRpcResponseTime=0)]
2020-10-29 09:02:46,558 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2236ms
No GCs detected
2020-10-29 09:02:46,562 INFO org.apache.ratis.server.impl.RaftServerImpl: om3@group-942F8267F22A: changes role from    LEADER to FOLLOWER at term 33 for stepDown
2020-10-29 09:02:46,563 INFO org.apache.ratis.server.impl.RoleInfo: om3: shutdown LeaderState
{noformat}

[~hanishakoneru] also thinks we should increase ratis leader election timeout too.

{noformat}
  <property>
    <name>ozone.om.ratis.minimum.timeout</name>
    <value>1s</value>
    <tag>OZONE, OM, RATIS, MANAGEMENT</tag>
    <description>The minimum timeout duration for OM's Ratis server rpc.
    </description>
  </property>

  <property>
    <name>ozone.om.leader.election.minimum.timeout.duration</name>
    <value>1s</value>
    <tag>OZONE, OM, RATIS, MANAGEMENT</tag>
    <description>The minimum timeout duration for OM ratis leader election.
      Default is 1s.
    </description
{noformat}",pull-request-available,['OM HA'],HDDS,Improvement,Critical,2020-11-03 21:16:16,43
13338676,Create unit test for SimpleContainerDownloader,"[~weichiu] reported that in a specific case the Datanode tried to download / replicate containers multiple times from the same datanode.

 

SimpleContainerDownload has a logic to try out all the available datanodes: this Jira creates a unit test t make sure the logic works well.",pull-request-available,[],HDDS,Sub-task,Major,2020-11-03 15:52:01,6
13338662,IT TestOzoneManagerHAMetadataOnly.testOMRetryCache is flaky," 

Failed twice recently 

[https://github.com/apache/ozone/pull/1538/checks?check_run_id=1336371166]

[https://github.com/apache/ozone/pull/1486/checks?check_run_id=1333307728]

 

 
{noformat}

Error:  Tests run: 10, Failures: 1, Errors: 0, Skipped: 2, Time elapsed: 198.367 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOzoneManagerHAMetadataOnly
3084Error:  testOMRetryCache(org.apache.hadoop.ozone.om.TestOzoneManagerHAMetadataOnly)  Time elapsed: 29.258 s  <<< FAILURE!
3085java.lang.AssertionError
3086	at org.junit.Assert.fail(Assert.java:86)
3087	at org.junit.Assert.assertTrue(Assert.java:41)
3088	at org.junit.Assert.assertFalse(Assert.java:64)
3089	at org.junit.Assert.assertFalse(Assert.java:74)
3090	at org.apache.hadoop.ozone.om.TestOzoneManagerHAMetadataOnly.testOMRetryCache(TestOzoneManagerHAMetadataOnly.java:422)
3091	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
3092	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
3093	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
3094	at java.lang.reflect.Method.invoke(Method.java:498)
3095	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
3096	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
3097	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
3098	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
3099	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
3100	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
3101	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
3102	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
3103 {noformat}",pull-request-available,[],HDDS,Bug,Major,2020-11-03 14:39:10,61
13338637,Avoid ContainerCache in ContainerReader at Datanode startup,"Testing on a dense datanode (200k containers, 45 disks) I see contention around the ContainerCache. Most of the time most threads are running in parallel, but we see slowdowns where most threads get blocked waiting on the ContainerCache lock.

Examining JStacks, we can see the runnable thread blocking others is typically evicting a RocksDB instance from the cache:

{code}
""Thread-37"" #131 prio=5 os_prio=0 tid=0x00007f8f49219800 nid=0x1c5e9 runnable [0x00007f86f7e78000]
   java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksDB.closeDatabase(Native Method)
        at org.rocksdb.RocksDB.close(RocksDB.java:468)
        at org.apache.hadoop.hdds.utils.RocksDBStore.close(RocksDBStore.java:389)
        at org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB.cleanup(ReferenceCountedDB.java:79)
        at org.apache.hadoop.ozone.container.common.utils.ContainerCache.removeLRU(ContainerCache.java:106)
        at org.apache.commons.collections.map.LRUMap.addMapping(LRUMap.java:242)
        at org.apache.commons.collections.map.AbstractHashedMap.put(AbstractHashedMap.java:284)
        at org.apache.hadoop.ozone.container.common.utils.ContainerCache.getDB(ContainerCache.java:167)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:63)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:165)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:183)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:160)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:137)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:91)
        at java.lang.Thread.run(Thread.java:748)
{code}

The slowness seems to be driven by the RocksDB close call. It is generally fast, but is often around 1ms. Eg, here are some timings from that call after adding instrumentation to the code:

{code}
grep -a ""metric: closing DB took"" ozone-datanode.log | cut -d "":"" -f 6 | sort -n | uniq -c
61940 0
128155 1
2786 2
236 3
53 4
42 5
17 6
10 7
8 8
15 9
{code}

The timer was only at ms precision, so that is why many are zero. Even at 1ms per close, we can only close 1000 per second and this point of the code is serialized.

At startup time, there is no value in caching the open containers. All containers on the node need to be read in parallel, therefore we should simply open and close each container without caching the instance.
",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2020-11-03 12:56:18,11
13338635,	SCM should create transactions using all blocks received from OM,Currently SCM creates a transaction per key deleted by OM. If OM is deleting 1000 keys then SCM should create transaction using all these keys i.e. segregate the blocks by containerID and then create transactions per containerID.,pull-request-available,[],HDDS,Sub-task,Major,2020-11-03 12:52:31,27
13338597,Update README with information how to report security issues,"security@ozone.apache.org is created, we can add in to the README.md and we can create the SECURITY.md file (Github naming convention)",pull-request-available,[],HDDS,Improvement,Major,2020-11-03 09:24:10,6
13338559,Fix TestMiniOzoneHACluster.testGetOMLeader(),"{code:java}
providers.put(ReplicationType.RATIS,
    new RatisPipelineProvider(nodeManager,
        (PipelineStateManager) stateManager, conf,
        eventPublisher));
{code}
It is because PipelineStateManager is deprecated and we have moved to PipelineStateManagerV2Impl ",pull-request-available,[],HDDS,Sub-task,Major,2020-11-03 06:38:07,54
13338494,Missing a container shouldn't log message at  ERROR level,"Missing a container replica should be a WARN level log at most.
{noformat}
2020-11-02 11:54:06,217 ERROR org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler: Exception while processing ICR for container 169498
org.apache.hadoop.hdds.scm.container.ContainerReplicaNotFoundException: Container #169498, replica: ContainerReplica{containerID=#169498, datanodeDetails=d2371c12-90b8-4ee8-a76f-c1e57a61509c{ip: 10.17.221.16, host: vc1506.halxg.cloudera.com, networkLocation: /default, certSerialId: null}, placeOfBirth=d2371c12-90b8-4ee8-a76f-c1e57a61509c, sequenceId=0, keyCount=100, bytesUsed=30720000}
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.removeContainerReplica(ContainerStateMap.java:256)
        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.removeContainerReplica(ContainerStateManager.java:538)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.removeContainerReplica(SCMContainerManager.java:565)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerReplica(AbstractContainerReportHandler.java:272)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:85)
        at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:80)
        at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}",pull-request-available,['SCM'],HDDS,Improvement,Trivial,2020-11-02 20:26:54,61
13338467,Misleading SCM web UI Safe mode status ," !Screen Shot 2020-11-01 at 11.46.40 PM.png! 

As an example, the above screenshot is for a SCM who was no container reported. However, the status message ""currentContainerThreshold 0.0 >= safeModeCutoff 0.99"" which is counterintuitive. We should make it easier to tell why SCM is still in safe mode.",pull-request-available,['SCM'],HDDS,Bug,Minor,2020-11-02 17:20:50,6
13338449,TestContainerMetrics is flaky,"TestContainerMetrics is flaky since HDDS-4359. Failed in following master builds:

{code}
2020/10/26/3569/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/27/3581/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/28/3591/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/29/3619/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/30/3628/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/30/3642/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/31/3650/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/31/3654/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
{code}

Some of the added assertions couldn't be guaranteed all the time:

{code}
      // ReadTime and WriteTime vary from run to run, only checking non-zero
      Assert.assertNotEquals(0L, getLongCounter(""ReadTime"", volumeIOMetrics));
      Assert.assertNotEquals(0L, getLongCounter(""WriteTime"", volumeIOMetrics));
{code}

In very lucky case the read/write time can be zero.",pull-request-available,['test'],HDDS,Bug,Major,2020-11-02 15:37:57,6
13338443,Simplify Ozone client code with configuration object,"In HDDS-4185 we agreed to introduce a new configuration for the Ozone client to adjust the behavior of incremental byte buffer.

When I started to add it I realized the code is already very complex as all the required configuration values are propagated manually to the Key/Block/ChunkOutputstream as induvidual constructor parameters.

In this patch I simplify the structure with using only one POJO with configuration annotations.",pull-request-available,[],HDDS,Improvement,Major,2020-11-02 15:02:41,6
13338373,Update README.md after TLP separation,"README.md can be updated with the new mailing lists and references to ""Hadoop subproject"" can be removed.",pull-request-available,[],HDDS,Improvement,Major,2020-11-02 09:16:31,6
13338112,Container replication should fail in case of import failure,"See the following log. There was an EOF exception when importing the container, however, the log message next line says successful.
{noformat}
2020-10-30 08:11:02,985 INFO org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Starting replication of container 740890 from [0ffe3494-8498-48b8-8877-e7986a151e
a4{ip: 10.12.1.72, host: rhel02.ozone.cisco.local, networkLocation: /default, certSerialId: null}, ab448e94-d2fd-48b0-bd57-8d4e88cbd893{ip: 10.12.1.83, host: rhel13.ozone.cisco.local, n
etworkLocation: /default, certSerialId: null}]
2020-10-30 08:11:08,548 INFO org.apache.hadoop.ozone.container.replication.GrpcReplicationClient: Container 740890 is downloaded to /tmp/container-copy/container-740890.tar.gz
2020-10-30 08:11:08,548 INFO org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Container 740890 is downloaded, starting to import.
2020-10-30 08:11:08,563 ERROR org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Can't import the downloaded container data id=740890
java.io.IOException: unexpected EOF with 7680 bytes unread. Occured at byte: 7447040
        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.read(TarArchiveInputStream.java:490)
        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.skip(TarArchiveInputStream.java:182)
        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:220)
        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextEntry(TarArchiveInputStream.java:435)
        at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.unpackContainerDescriptor(TarContainerPacker.java:182)
        at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.importContainer(DownloadAndImportReplicator.java:74)
        at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:121)
        at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-10-30 08:11:08,564 INFO org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator: Container 740890 is replicated successfully
2020-10-30 08:11:08,564 INFO org.apache.hadoop.ozone.container.replication.ReplicationSupervisor: Container 740890 is replicated.
{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Minor,2020-10-30 16:46:01,59
13337969,Cache DatanodeDetails#getUuidString(),"DatanodeDetails#getUuidString() is used a lot in OM. In our benchmark about 2% of time is spent getting this string. We should consider caching the string to avoid constructing the UUID string repeatedly.

",pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-10-30 00:45:16,59
13337835,Datanode State Machine Thread should keep alive during the whole lifetime of Datanode,"Datanode State Machine Thread should keep alive during the whole lifetime of Datanode, since it periodic generates heartbeat tasks which trigger DN to actively talk with DN. If this thread crashes, DN will become a zombie: although it is alive, heartbeats between itself and SCM are stopped.

 

In Tencent internal production environment, we got several dead DNs which can never come back without a restart.

We found that the thread ""Datanode State Machine Thread - 0"" does not exist in the output of jstack, thus no HeartbeatEndpointTask will be created,  this DN will soon become dead and can not recover unless being restarted.

 

After checked the .out log, we saw that OOM occurred in thread ""Datanode State Machine Thread"", which should be responsible for this issue:
{code:java}
114370.799: Total time for which application threads were stopped: 1.0883622 seconds, Stopping threads took: 0.0002926 seconds
Exception in thread ""Datanode State Machine Thread - 0"" java.lang.OutOfMemoryError: GC overhead limit exceeded 114370.810: Application time: 0.0115941 seconds {Heap before GC invocations=2946 (full 2680): PSYoungGen total 3170304K, used 2846720K [0x00000006eab00000, 0x00000007c0000000, 0x00000007c0000000) eden space 2846720K, 100% used [0x00000006eab00000,0x0000000798700000,0x0000000798700000) from space 323584K, 0% used [0x00000007ac400000,0x00000007ac400000,0x00000007c0000000) to space 324096K, 0% used [0x0000000798700000,0x0000000798700000,0x00000007ac380000) ParOldGen total 6990848K, used 6990627K [0x0000000540000000, 0x00000006eab00000, 0x00000006eab00000) object space 6990848K, 99% used [0x0000000540000000,0x00000006eaac8c90,0x00000006eab00000) Metaspace used 60721K, capacity 63446K, committed 64128K, reserved 1105920K class space used 6583K, capacity 7031K, committed 7296K, reserved 1048576K
{code}
 
{code:java}
300010.579: Total time for which application threads were stopped: 3.0848769 seconds, Stopping threads took: 0.0000943 seconds
Exception in thread ""Datanode State Machine Thread - 0"" java.lang.OutOfMemoryError: Java heap space
300010.579: Application time: 0.0001554 seconds
300010.580: Total time for which application threads were stopped: 0.0015600 seconds, Stopping threads took: 0.0002747 seconds
300010.581: Application time: 0.0004684 seconds
{Heap before GC invocations=13766 (full 11664):
 PSYoungGen total 3441664K, used 3388416K [0x000000072ab00000, 0x0000000800000000, 0x0000000800000000)
 eden space 3388416K, 100% used [0x000000072ab00000,0x00000007f9800000,0x00000007f9800000)
 from space 53248K, 0% used [0x00000007fcc00000,0x00000007fcc00000,0x0000000800000000)
 to space 53248K, 0% used [0x00000007f9800000,0x00000007f9800000,0x00000007fcc00000)
 ParOldGen total 6990848K, used 6990848K [0x0000000580000000, 0x000000072ab00000, 0x000000072ab00000)
 object space 6990848K, 100% used [0x0000000580000000,0x000000072ab00000,0x000000072ab00000)
 Metaspace used 55150K, capacity 57816K, committed 59224K, reserved 1101824K
 class space used 5922K, capacity 6372K, committed 6744K, reserved 1048576K{code}
 

BTW, after running DN for more than a week, we see a lot of ""java.lang.OutOfMemoryError: GC overhead limit exceeded"" in DN's log. Since we configured a dead Recon, we guess this could an evidence for HDDS-4404.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2020-10-29 09:11:20,36
13337792,Run MiniOzoneChaosCluster via github actions,"This jira proposes to run MiniOzoneChaosCluster via a cron job using githubActions.

The results of the run can be debugged later on.",pull-request-available,"['chaos', 'test']",HDDS,Bug,Major,2020-10-29 03:39:18,18
13337773,Proxy failover is logging with out trying all OMS,"{code:java}
[root@uma-1 ~]# sudo -u hdfs hdfs dfs -ls o3fs://bucket.volume.ozone1/
20/10/28 23:37:50 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:73)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)
, while invoking $Proxy10.submitRequest over {om1=nodeId=om1,nodeAddress=uma-1.uma.root.hwx.site:9862, om3=nodeId=om3,nodeAddress=uma-3.uma.root.hwx.site:9862, om2=nodeId=om2,nodeAddress=uma-2.uma.root.hwx.site:9862} after 1 failover attempts. Trying to failover immediately.{code}

This issue in the Apache Ozone main branch will be fixed once Hadoop version is updated. For vendors/users who backport fix to their Hadoop version and have ozone compiled with that version, this fix will help them not to see first failovers till it finds leader OM.",pull-request-available,[],HDDS,Bug,Major,2020-10-29 00:48:29,13
13337718,Datanode can go OOM when a Recon or SCM Server is very slow in processing reports.,"From [~nanda619]'s analysis.

ContainerReportPublisher thread runs periodically (default interval 60s) in Datanode and adds ContainerReport to StateContext (Queue).
Heartbeat thread runs periodically (default interval 30s), picks up the ContainerReport (if any) from StateContext.
For short time, the ContainerReport will be held in Datanode StateContext.
For Recon, a change was made in datanode such that the ContainerReport will be cached in Datanode StateContext separately for each endpoint (i.e. SCM and Recon). As I see, if Recon is configured in the Datanode and all the reports that are to be sent to Recon will be pending in the StateContextQueue (LinkedList)",pull-request-available,['Ozone Datanode'],HDDS,Task,Blocker,2020-10-28 17:54:07,12
13337674,Update the container replica history to the Recon DB lazily instead of for every report.,"Recon tracks the history for every container replica on the Ozone cluster in its SQL DB (By default, this is Derby). To track this, it keeps track of the last timestamp of a replica on a DN through reports. This becomes a SQL DB scan + write operation for every container report received.  Even though there is async hand off from the report to EventQueue, the event queue handler itself by default uses 1 thread per event type (report type). Hence, there is implicit blocking behavior here which is pushed down to DNs.

This has to be changed into a lazy update of DB to support better scalability. Details on how to achieve this will be added to the JIRA later.",pull-request-available,['Ozone Recon'],HDDS,Task,Critical,2020-10-28 13:52:19,12
13337670,Fix compilation issue in HDDS-3698-upgrade branch.,"{code}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-ozone-ozone-manager: Compilation failure: Compilation failure: 
Error:  /mnt/ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java:[691,6] not a statement
Error:  /mnt/ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java:[691,23] ';' expected
Error:  -> [Help 1]
{code}",pull-request-available,[],HDDS,Sub-task,Major,2020-10-28 13:35:54,30
13337648,Make raft log directory deletion configurable during pipeline remove,The idea here is to add a config to make raft log directory removal configurable during pipeline remove.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-10-28 11:27:50,16
13337644,Safe mode rule for piplelines should only consider open pipelines,"Currently, for safe mode we consider all pipelines existing in DB for safe mode exit criteria. It ma happen that, SCM has the pipelines craeted , but none of the participants datanodes ever created these datanodes. In such cases, SCM fails to come out of safemode as these pipelines are never reported back to SCM.

 

The idea here is to consider pipelines which are marked open during SCM startup.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-10-28 11:18:18,16
13337562,Ozone TLP - update documents,"Ozone is approved to become an apache TLP.  

There is need to update Ozone documents, changing from ""Apache Hadoop Ozone"" to ""Apache Ozone"".",pull-request-available,[],HDDS,Task,Major,2020-10-28 03:42:22,55
13337246,Ozone Data Generator for Fast Scale Test,"I've been working on this fun project and would like to share with the community.

 
h1. Synopsis

We want to prove Ozone runs well at scale, in terms of number of keys (billions of keys), as well as dense DataNodes where each DN has hundreds of TB or even PB-scale capacity.
h1. Challenge: Data generation

The challenge is to generate a huge data set fast so that we can benchmark the system quickly. No existing tool is capable at this scale. 

 
h1. Proposal:

The major bottleneck is OM’s key insertion performance. In addition, Ozone uses a single pipeline to write data, unless multi-raft is enabled.

 

Instead of using Ozone's client API to generate data, We should write directly to OM, SCM and DN’s rocksdb. RocksDB can support u[p to a million key|https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks] bulk load operations.

 

Similarly, we can skip the normal Ozone client write path; populate the container db and block files directly.

 

(more details in the design doc)",pull-request-available,['Tools'],HDDS,New Feature,Major,2020-10-26 16:37:33,6
13336987,Add Recon architecture to docs,Recon Architecture and details are missing in the docs - https://ci-hadoop.apache.org/view/Hadoop%20Ozone/job/ozone-doc-master/lastSuccessfulBuild/artifact/hadoop-hdds/docs/public/concept/ozonemanager.html,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-10-23 20:51:22,37
13336933,Disable Rocksdb caching when performing Trash specific operations,The idea here is to not cache keys that are in trash i.e after deletion since they are going to get deleted anyway ( except if user recovers ) and keeping it in cache will only populate it unnecessarily.,pull-request-available,[],HDDS,Sub-task,Major,2020-10-23 14:03:54,26
13336869,Make writeStateMachineTimeout retry count proportional to node failure timeout,"Currently, in ratis ""writeStateMachinecall"" gets retried indefinitely in event of a timeout. In case, where disks are slow/overloaded or number of chunk writer threads are not available for a period of 10s, writeStateMachine call times out in 10s. In cases like these, the same write chunk keeps on getting retried causing the same chunk of data to be overwritten. The idea here is to abort the request once the node failure timeout reaches.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-10-23 07:31:45,16
13336842,Each EndpointStateMachine uses its own thread pool to talk with SCM/Recon,"In Tencent production environment, after start Recon for a while, we got warnings that all DNs become stale/dead at SCM side. After kill recon, all DNs become healthy in a very short time.

 

*The root cause is:*

1) EndpointStateMachine for SCM and that for Recon share the thread pool created by DatanodeStateMachine, which is a fixed size thread pool:
{code:java}
executorService = Executors.newFixedThreadPool(
    getEndPointTaskThreadPoolSize(),
    new ThreadFactoryBuilder()
        .setNameFormat(""Datanode State Machine Task Thread - %d"").build());

private int getEndPointTaskThreadPoolSize() {
  // TODO(runzhiwang): current only support one recon, if support multiple
  //  recon in future reconServerCount should be the real number of recon
  int reconServerCount = 1;
  int totalServerCount = reconServerCount;

  try {
    totalServerCount += HddsUtils.getSCMAddresses(conf).size();
  } catch (Exception e) {
    LOG.error(""Fail to get scm addresses"", e);
  }

  return totalServerCount;
}
{code}
meanwhile, current Recon has some performance issue, after running for hours, it became slower and slower, and crashed due to OOM. 

2) The communication between DN and Recon will soon exhaust all the threads in DatanodeStateMachine.executorService, there will be no available threads for DN to talk SCM. 

3) all DNs become stale/dead at SCM side.

 

*The fix is quite straightforward:*

Each EndpointStateMachine uses its own thread pool to talk with SCM/Recon, a slow Recon won't interfere the communication between DN and SCM, or vice versa.

 

*P.S.*

The first edition for DatanodeStateMachine.executorService is a cached thread pool, if there exists a slow SCM/Recon, more and more threads will be created, and DN will OOM eventually, due to tens of thousands of threads are created.",pull-request-available,[],HDDS,Bug,Blocker,2020-10-23 04:09:58,36
13336816,OM changes the block length when receives truncate request,"When OM receives truncate(key, newLength),in the keyTable, OM deletes the blocks which are fully truncated, and updates the block length which is partially truncated, then return success to client.",pull-request-available,[],HDDS,Sub-task,Major,2020-10-23 01:09:42,65
13336689,Datanode deletion service can avoid storing deleted blocks,Currently BlockDeletingService in datanode processes the blocks to be deleted and then stores them in a deleted blocks table. We can avoid storing deleted blocks in the container rocksDB.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-10-22 10:35:56,27
13336688,Datanode should store the delete transaction as is in rocksDB,Currently datanode breaks down the transaction into individual blocks and then stores every block as separate entry in rocksDB. Since transaction can contain multiple blocks it is better to store the entire transaction in rocksDB.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-10-22 10:32:22,27
13336656,"Configuration for deletion service intervals should be different for OM, SCM and datanodes",Currently all background deletion services use the same intervals for deletion. It should ideally use different configs and default value can be same for all the configs.,pull-request-available,"['OM', 'Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2020-10-22 07:53:46,27
13336653,SCM deletion service should delete configured number of blocks every interval,"SCM service currently uses datanode's configuration to determine the number of blocks to delete every interval. It should have its own congifuration for maximum number of blocks to delete in every interval.
Further it currently scans the entire DB to fetch block deletion transactions. This can be avoided with this approach. With this patch service would always fetch configured number of blocks from the db.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-10-22 07:41:49,35
13336626,SCMBlockLocationFailoverProxyProvider should use ScmBlockLocationProtocolPB.class in RPC.setProtocolEngine,"in SCMBlockLocationFailoverProxyProvider,

currently it is
{code:java}
private ScmBlockLocationProtocolPB createSCMProxy(
    InetSocketAddress scmAddress) throws IOException {
  ...
  RPC.setProtocolEngine(hadoopConf, ScmBlockLocationProtocol.class,
      ProtobufRpcEngine.class);
  ...{code}
 it should be 
{code:java}
private ScmBlockLocationProtocolPB createSCMProxy(
    InetSocketAddress scmAddress) throws IOException {
  ...
  RPC.setProtocolEngine(hadoopConf, ScmBlockLocationProtocolPB.class,
      ProtobufRpcEngine.class);
  ...{code}
 

FYi, according to non-HA version
{code:java}
private static ScmBlockLocationProtocol getScmBlockClient(
    OzoneConfiguration conf) throws IOException {
  RPC.setProtocolEngine(conf, ScmBlockLocationProtocolPB.class,
      ProtobufRpcEngine.class);
  long scmVersion =
      RPC.getProtocolVersion(ScmBlockLocationProtocolPB.class);
  InetSocketAddress scmBlockAddress =
      getScmAddressForBlockClients(conf);
  ScmBlockLocationProtocolClientSideTranslatorPB scmBlockLocationClient =
      new ScmBlockLocationProtocolClientSideTranslatorPB(
          RPC.getProxy(ScmBlockLocationProtocolPB.class, scmVersion,
              scmBlockAddress, UserGroupInformation.getCurrentUser(), conf,
              NetUtils.getDefaultSocketFactory(conf),
              Client.getRpcTimeout(conf)));
  return TracingUtil
      .createProxy(scmBlockLocationClient, ScmBlockLocationProtocol.class,
          conf);
}
{code}",pull-request-available,['SCM'],HDDS,Sub-task,Minor,2020-10-22 03:31:11,36
13336538,Add metric to track the number of RocksDB open/close operations,"We are benchmarking Ozone performance, and realized RocksDB open/close operations have huge impact to performance. Each db open takes about 70ms on average and close takes about 1ms on average.

 

Having metrics on these operations will help understand DataNode performance problems.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2020-10-21 18:29:57,18
13336270,Expose VolumeIOStats in DN JMX,Expose VolumeIOStats in DN JMX web endpoint.,pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2020-10-20 10:50:33,12
13336201,Delete : make delete an atomic operation,This task handles directory and file deletions. This won't cleanup the garbage and orphans which will be implemented in another jira.,pull-request-available,[],HDDS,Sub-task,Major,2020-10-20 05:51:12,40
13336200,Rename : make rename an atomic ops by updating key path entry in dir/file table,"This task is to handle rename key path request and make it an atomic operation by updating the DirTable or FileTable.

Here in this jira, we consider only new Ozone FS client talking to new OM server. Later, I will raise separate Jira task to handle compatibilities across different client/server versions.",pull-request-available,[],HDDS,Sub-task,Major,2020-10-20 05:46:03,40
13336144,"SCM is flooded with useless ""Deleting blocks"" messages","Testing a 1.0.0 SCM.

I'm seeing these messages flooding SCM log file when a dead DN is detected.
{noformat}
2020-10-19 13:48:19,642 INFO org.apache.hadoop.hdds.scm.node.DeadNodeHandler: A dead datanode is detected. 9b27c38d-9104-491b-b76b-959dc9dd06a2{ip: 10.12.1.82, host: rhel12.ozone.local, networkLocation: /default, certSerialId: null}
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer: SCM is informed by OM to delete 1000 blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,894 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,895 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,895 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
2020-10-19 13:48:24,895 INFO org.apache.hadoop.hdds.scm.block.BlockManagerImpl: Deleting blocks
 {noformat}
SCM deletes 1000 blocks max at a time, and the ""Deleting blocks"" message repeats for 1000 times. Worse, it doesn't give any useful information (at the very least, it should print the block id)",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-10-19 21:19:50,35
13335721,Avoid spamming of logs because of deleted transactions,"The following two log entries are seen every regularly during MiniOzoneChaosTests
{code}
2020-10-15 17:21:25,467 [CommandWatcher-LeaseManager#LeaseMonitor] ERROR lease.LeaseManager (LeaseManager.java:run(238)) - Execution was interrupted
java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:234)
        at java.lang.Thread.run(Thread.java:748)
{code}

{code}
2020-10-15 17:21:58,487 [IPC Server handler 11 on default port 59106] WARN  block.DeletedBlockLogImpl (DeletedBlockLogImpl.java:incrementCount(135)) - Deleted TXID 27 not found.
2020-10-15 17:21:58,488 [IPC Server handler 11 on default port 59106] WARN  block.DeletedBlockLogImpl (DeletedBlockLogImpl.java:incrementCount(135)) - Deleted TXID 30 not found.
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2020-10-16 07:02:47,18
13335447,Make Ozone specific Trash remover multi threaded,number of thread for trash emptier would be configurable. Each trash emptier thread would work on a bucket granularity.,pull-request-available,[],HDDS,Sub-task,Major,2020-10-14 16:42:31,26
13335446,Ozone specific TrashPolicy,Extend TrashPolicy to write ozone specific Trash policy.,pull-request-available,[],HDDS,Sub-task,Major,2020-10-14 16:40:37,26
13335366,ReplicationManager.handleOverReplicatedContainer() does not handle unhealthyReplicas properly.,"{code:java}
      // If there are unhealthy replicas, then we should remove them even if it
      // makes the container violate the placement policy, as excess unhealthy
      // containers are not really useful. It will be corrected later as a
      // mis-replicated container will be seen as under-replicated.
      for (ContainerReplica r : unhealthyReplicas) {
        if (excess > 0) {
          sendDeleteCommand(container, r.getDatanodeDetails(), true);
          excess -= 1;
        }
        break;
      }
      // After removing all unhealthy replicas, if the container is still over
      // replicated then we need to check if it is already mis-replicated.
      // If it is, we do no harm by removing excess replicas. However, if it is
      // not mis-replicated, then we can only remove replicas if they don't
      // make the container become mis-replicated.
{code}

From the comment, it wants to remove all unhealthy replicas until excess reach 0 ? It should be
{code:java}
      for (ContainerReplica r : unhealthyReplicas) {
        if (excess > 0) {
          sendDeleteCommand(container, r.getDatanodeDetails(), true);
          excess -= 1;
        } else {
          break;
        }
      }
{code}",pull-request-available,['SCM'],HDDS,Bug,Minor,2020-10-14 08:35:57,36
13335308,Add DataNode state and transitions for a node going through upgrade,"A typical SCM sequence for driving datanodes through upgrade would be

Something like following :                                                                  

 - client sends Finalize                                                       

 - SCM moves to Finalizing states. If SCM crashes, and it comes up             

   it will always restart from this state.                                     

 - SCM disallows new pipeline creation, SCM in safe mode                       

    (SCM Freeze for new pipeline)                                              

 - SCM closes existing pieplines                                               

 - SCM updates MLV = SLV if not already so. Update on-disk MLV state.          

 - SCM moves all data nodes to HEALTHY_READONLY state. Please note that        

   initial state for all data node is HEALTHY_READONLY. For data nodes         

   to move from HEALTHY_READONLY -> HEALTHY, they need to send atleast         

   one heartbeat where DN.MLV == SCM.MLV                                       

 - SCM waits for few heartbeats                                                

 - SCM allows new pipeline creation (SCM thaw for new pipeline creation).      

   New Pipelines can be created if enough HEALTHY data nodes are found.        

 - If SCM comes across any data node heart beat with DN.MLV < SCM.MLV => SCM sends

   that data node finalize command 

======================================================================

 

As part of this, we would be introducing a new state HEALTHY-READONLY in DataNode state machine maintained in SCM .

This Jira will be used to make changes in the datanode state machine.                                                    ",pull-request-available,[],HDDS,Sub-task,Major,2020-10-14 01:44:03,44
13335218,Add Operational State to the datanode list command,"The existing CLI command `ozone admin datanode list` provides output like:

{code}
bash-4.2$ ozone admin datanode list
Datanode: f2b2452a-bf7b-4c6d-b2d6-a0d9d219b21a (/default-rack/172.20.0.8/ozone_datanode_1.ozone_default/2 pipelines) 
Related pipelines: 
16561bc4-746a-4c79-b6f8-1c275b31e37d/THREE/RATIS/OPEN/Leader
4e45ff9c-478b-4ab8-a66c-7bfa98c8c632/ONE/RATIS/OPEN/Leader

Datanode: 57c7fd5f-e32c-4de9-a04a-89d8d4273431 (/default-rack/172.20.0.6/ozone_datanode_3.ozone_default/2 pipelines) 
Related pipelines: 
4b24bc61-28cf-471a-893c-a05cac273856/ONE/RATIS/OPEN/Leader
16561bc4-746a-4c79-b6f8-1c275b31e37d/THREE/RATIS/OPEN/Follower

Datanode: 6699fc6d-5c2d-4110-8d88-5ffa5b99f326 (/default-rack/172.20.0.3/ozone_datanode_2.ozone_default/2 pipelines) 
Related pipelines: 
16561bc4-746a-4c79-b6f8-1c275b31e37d/THREE/RATIS/OPEN/Follower
5ce21cae-9a2d-486d-8b4b-f8ddf75efc61/ONE/RATIS/OPEN/Leader
{code}

We should extend this to show the ""Operational State"" of the node for decommission.",pull-request-available,['SCM Client'],HDDS,Sub-task,Major,2020-10-13 11:56:22,11
13335149,Ozone S3 gateway throws NPE with goofys,"Configured goofys and s3g on different hosts and Fiotest writes files on the goofys mount point. Export AWS secrets on the s3g host. See a bunch of NPE in s3g logs.
 # Looks like missing AWS auth header could cause NPE. Looks like AWSSignatureProcessor.init() doesn't handle header missing which causes NPE.
 # Why it's missing AWS auth header is also unknown.

Note that there are files that have been successfully written into Ozone via goofys, while not all of them are succeeded.  

 

2020-10-13 11:18:43,425 [qtp1686100174-1238] ERROR org.apache.hadoop.ozone.s3.OzoneClientProducer: Error: 
org.jboss.weld.exceptions.WeldException: WELD-000049: Unable to invoke public void org.apache.hadoop.ozone.s3.AWSSignatureProcessor.init() throws java.lang.Exception on org.apache.hadoop.ozone.s3.AWSSignatureProcessor@5535155b
 at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.invokeMethods(DefaultLifecycleCallbackInvoker.java:99)
 at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.postConstruct(DefaultLifecycleCallbackInvoker.java:80)
 at org.jboss.weld.injection.producer.BasicInjectionTarget.postConstruct(BasicInjectionTarget.java:122)
 at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.postConstruct(CdiComponentProvider.java:887)
 at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:162)
 at org.jboss.weld.context.AbstractContext.get(AbstractContext.java:96)
 at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
 at org.jboss.weld.bean.ContextualInstanceStrategy$CachingContextualInstanceStrategy.get(ContextualInstanceStrategy.java:177)
 at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
 at org.jboss.weld.bean.proxy.ContextBeanInstance.getInstance(ContextBeanInstance.java:99)
 at org.jboss.weld.bean.proxy.ProxyMethodHandler.getInstance(ProxyMethodHandler.java:125)
 at org.apache.hadoop.ozone.s3.AWSSignatureProcessor$Proxy$_$$_WeldClientProxy.getAwsAccessId(Unknown Source)
 at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:79)
 at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:68)
 at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)
 at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:78)
 at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:100)
 at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
 at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
 at org.jboss.weld.context.unbound.DependentContextImpl.get(DependentContextImpl.java:70)
 at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
 at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
 at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:785)
 at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:885)
 at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
 at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:358)
 at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:369)
 at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:70)
 at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
 at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:72)
 at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
 at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:873)
 at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:159)
 at org.jboss.weld.context.unbound.DependentContextImpl.get(DependentContextImpl.java:70)
 at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
 at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
 at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:785)
 at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:808)
 at org.jboss.weld.util.ForwardingBeanManager.getReference(ForwardingBeanManager.java:61)
 at org.jboss.weld.bean.builtin.BeanManagerProxy.getReference(BeanManagerProxy.java:85)
 at org.glassfish.jersey.ext.cdi1x.internal.CdiUtil.getBeanReference(CdiUtil.java:151)
 at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier$1.getInstance(AbstractCdiBeanSupplier.java:93)
 at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier._provide(AbstractCdiBeanSupplier.java:127)
 at org.glassfish.jersey.ext.cdi1x.internal.RequestScopedCdiBeanSupplier.get(RequestScopedCdiBeanSupplier.java:70)
 at org.glassfish.jersey.inject.hk2.InstanceSupplierFactoryBridge.provide(InstanceSupplierFactoryBridge.java:77)
 at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:153)
 at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:487)
 at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:70)
 at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2126)
 at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:777)
 at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:740)
 at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:710)
 at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:184)
 at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:54)
 at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:129)
 at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:284)
 at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:75)
 at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:110)
 at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:113)
 at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:113)
 at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:113)
 at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:93)
 at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:62)
 at org.glassfish.jersey.process.internal.Stages.process(Stages.java:197)
 at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:269)
 at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
 at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
 at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
 at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
 at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
 at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
 at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
 at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
 at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
 at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1666)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
 at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
 at org.eclipse.jetty.server.Server.handle(Server.java:500)
 at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
 at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
 at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
 at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.invokeMethods(DefaultLifecycleCallbackInvoker.java:97)
 ... 121 more
Caused by: java.lang.NullPointerException

 

!image-2020-10-13-15-23-49-864.png!",pull-request-available,[],HDDS,Bug,Blocker,2020-10-13 07:30:00,66
13335105,"SCM web UI banner shows ""HDFS SCM""","!Screen Shot 2020-10-12 at 6.42.31 PM.png!  Let's call it Ozone SCM, shall we?",newbie pull-request-available,[],HDDS,Bug,Trivial,2020-10-13 01:43:09,64
13335015,Implement RocksDB options cache for new datanode DB utilities,"HDDS-3869 switched datanodes from using the old database utilities found in the hdds.utils package to the new database utilities in the hdds.utils.db package. Since the datanode RocksDB options cache from HDDS-2283 was implemented in the old utilities package, it is no longer present on the datanodes after HDDS-3869 was merged. This issue aims to add the options cache performance improvement to the new datanode code.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2020-10-12 13:40:21,22
13335011,ContainerInfo does not persist BCSID leading to failed replicas reports,"If you create a container, and then close it, the BCSID is synced on the datanodes and then the value is updated in SCM via setting the ""sequenceID"" field on the containerInfo object for the container.

If you later restart just SCM, the sequenceID becomes zero, and then container reports for the replica fail with a stack trace like:

{code}
Exception in thread ""EventQueue-ContainerReportForContainerReportHandler"" java.lang.AssertionError
	at org.apache.hadoop.hdds.scm.container.ContainerInfo.updateSequenceId(ContainerInfo.java:176)
	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerStats(AbstractContainerReportHandler.java:108)
	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:83)
	at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:162)
	at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:130)
	at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)
	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

The assertion here is failing, as it does not allow for the sequenceID to be changed on a CLOSED container:

{code}
  public void updateSequenceId(long sequenceID) {
    assert (isOpen() || state == HddsProtos.LifeCycleState.QUASI_CLOSED);
    sequenceId = max(sequenceID, sequenceId);
  }
{code}

The issue seems to be caused by the serialisation and deserialisation of the containerInfo object to protobuf, as sequenceId never persisted or restored.

However, I am also confused about how this ever worked, as this is a pretty significant problem.

",pull-request-available,['SCM'],HDDS,Bug,Major,2020-10-12 13:24:33,11
13334777,ListFileStatus - do lookup in directory and file tables,"This task is to perform look up of the user given {{key}} path in the directory, file and openFile tables.

OzoneFileSystem APIs:
     - GetFileStatus
     - listStatus",pull-request-available,[],HDDS,Sub-task,Major,2020-10-10 08:52:34,40
13334734,Bootstrap new OM node,"In a ratis enabled OM cluster, add support to bootstrap a new OM node and add it to OM ratis ring. ",pull-request-available,[],HDDS,New Feature,Major,2020-10-09 23:02:47,43
13334702,Expose Ratis retry config cache in OM,"This Jira is to expose config Ratis retry cache duration in OM, and also choose a sensible default value.",pull-request-available,[],HDDS,Sub-task,Major,2020-10-09 18:52:34,13
13334482,Potential resource leakage using BatchOperation,"there are a number of places in the code where BatchOperation is used but not closed. As a best practice, better to close them explicitly.

I have a stress test code that uses BatchOperation to insert into OM rocksdb. Without closing BatchOperation explicitly, the process crashes after just a few minutes.",pull-request-available,[],HDDS,Bug,Blocker,2020-10-08 18:43:00,13
13334327,Incompatible return codes from Ozone getconf -confKey,"It seems that the return codes of ozone getconf -confKey are different in 1.0 and after 1.0.

Looking at the code:

in old code:

/** Method to be overridden by sub classes for specific behavior. */
int doWorkInternal(OzoneGetConf tool, String[] args) throws Exception {


{code:java}
 String value = tool.getConf().getTrimmed(key);
 if (value != null) {
 tool.printOut(value);
 return 0;
 }
 tool.printError(""Configuration "" + key + "" is missing."");
 return -1;
}
{code}

with 1.0 code:
@Override
  public Void call() throws Exception {
    String value = tool.getConf().getTrimmed(confKey);
    if (value != null) {
      tool.printOut(value);
    } else {
      tool.printError(""Configuration "" + confKey + "" is missing."");
    }
    return null;
  }

We are returning null irrespective of the cases.
Some applications/tests depending on this codes.

Thanks [~nmaheshwari] for helping on debug and finding the issue.
 ",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-10-08 01:36:36,1
13334276,DatanodeAdminMonitor no longers needs maintenance end time to be passed,"An earlier change moved the maintenance endtime into the NodeStatus object. However when adding a node to the decommission monitor the end time must still be passed. This value is never used.

This Jira will remove the endInHours field from the interface:

{code}
public interface DatanodeAdminMonitor extends Runnable {

  void startMonitoring(DatanodeDetails dn, int endInHours);
  void stopMonitoring(DatanodeDetails dn);

}
{code}",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-10-07 20:18:10,11
13334273,Add integration tests for putting nodes into maintenance and fix any issues uncovered in the tests,Add a series of intergration tests to prove nodes can enter and leave maintenance correctly and address any issues in the code when adding the tests,pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-10-07 20:12:30,11
13334272,Add integration tests for Decommission and resolve issues detected by the tests,"Add a series of integration tests to prove decommission work, and that decommission can survive a restart of SCM.

As part of adding these tests, some issues were discover that were fixed in the process of debugging the tests.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-10-07 20:08:34,11
13334251,Fix compilation errors : merge HDDS-4308 and HDDS-4473 changes into the branch,"This is tracking Jira to revisit the quota implementation once HDDS-4308  task is committed.

Need to revisit all the newly created ""{{*RequestV1.java}}"" and ""{{*ResponseV1.java}}"" set of classes and incorporate these changes into it.

OMDirectoryCreateRequestV1.java
 OMFileCreateRequestV1.java
 OMAllocateBlockRequestV1.java
 OMKeyCommitRequestV1.java
 OMKeyDeleteRequestV1.java

 

Following compilation error has to be fixed, which occurred after rebasing the branch to latest master code.
  
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-ozone-ozone-manager: Compilation failure: Compilation failure: 
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileCreateRequestV1.java:[191,7] cannot find symbol
[ERROR]  symbol:  method checkVolumeQuotaInBytes(org.apache.hadoop.ozone.om.helpers.OmVolumeArgs,long)
[ERROR]  location: class org.apache.hadoop.ozone.om.request.file.OMFileCreateRequestV1
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileCreateRequestV1.java:[207,19] cannot find symbol
[ERROR]  symbol:  method getUsedBytes()
[ERROR]  location: variable omVolumeArgs of type org.apache.hadoop.ozone.om.helpers.OmVolumeArgs
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileCreateRequestV1.java:[208,34] long cannot be dereferenced
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java:[1893,11] cannot find symbol
[ERROR]  symbol:  method sortDatanodeInPipeline(org.apache.hadoop.ozone.om.helpers.OmKeyInfo,java.lang.String)
[ERROR]  location: class org.apache.hadoop.ozone.om.KeyManagerImpl
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java:[2443,11] cannot find symbol
[ERROR]  symbol:  method sortDatanodeInPipeline(org.apache.hadoop.ozone.om.helpers.OmKeyInfo,java.lang.String)
[ERROR]  location: class org.apache.hadoop.ozone.om.KeyManagerImpl
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyCommitRequestV1.java:[172,19] cannot find symbol
[ERROR]  symbol:  method getUsedBytes()
[ERROR]  location: variable omVolumeArgs of type org.apache.hadoop.ozone.om.helpers.OmVolumeArgs
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyCommitRequestV1.java:[173,34] long cannot be dereferenced
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyDeleteRequestV1.java:[153,19] cannot find symbol
[ERROR]  symbol:  method getUsedBytes()
[ERROR]  location: variable omVolumeArgs of type org.apache.hadoop.ozone.om.helpers.OmVolumeArgs
[ERROR] /Users/rakeshr/Desktop/ozone/fork_m7/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyDeleteRequestV1.java:[154,34] long cannot be dereferenced
[ERROR] -> [Help 1]
{code}
 ",pull-request-available,[],HDDS,Sub-task,Major,2020-10-07 18:00:35,40
13334230,Compile error with Java 11,"{code:title=https://github.com/apache/hadoop-ozone/runs/1217093596#step:6:5632}
Error:  Failed to execute goal org.codehaus.mojo:aspectj-maven-plugin:1.10:compile (default) on project hadoop-ozone-ozone-manager: Execution default of goal org.codehaus.mojo:aspectj-maven-plugin:1.10:compile failed: Plugin org.codehaus.mojo:aspectj-maven-plugin:1.10 or one of its dependencies could not be resolved: Could not find artifact com.sun:tools:jar:11.0.8 at specified path /opt/hostedtoolcache/jdk/11.0.8/x64/../lib/tools.jar -> [Help 1]
{code}

https://github.com/mojohaus/aspectj-maven-plugin/issues/24#issuecomment-419077658",pull-request-available,['build'],HDDS,Bug,Major,2020-10-07 15:55:28,1
13334172,Disable single node pipeline creation by default in Ozone,"Currently, single node pipeline creation is ON by default in ozone, though its not used by default in Ozone write path. It would be good to disable this by turning off the config ""ozone.scm.pipeline.creation.auto.factor.one"" by default. It may lead to some unit test failures and for those tests , this config needs to b explicitly set to true.",pull-request-available,[],HDDS,Bug,Major,2020-10-07 11:52:11,27
13334093,Upgrade to angular 1.8.0 due to CVE-2020-7676,"Angular versions < 1.8.0 are vulnerable to cross-site scripting

[https://nvd.nist.gov/vuln/detail/CVE-2020-7676]",pull-request-available,[],HDDS,Task,Major,2020-10-07 00:00:05,37
13334083,Ensure ObjectIDs are unique across restarts,"In a non-Ratis OM, the transaction index used to generate ObjectID is reset on OM restart. This can lead to duplicate ObjectIDs when the OM is restarted. ObjectIDs should be unique. 
HDDS-2939 and NFS are some of the features which depend on ObjectIds being unique.

To ensure that objectIDs are unique across restarts in non-ratis OM cluster, the transaction index should be updated in DB on every flush to DB. This can be done in a similar fashion to what is being done for ratis enabled cluster today. TransactionInfo table is updated with transaction index as part of every batch write operation to DB.

Also, and epoch number is introduced to ensure that objectIDs do not clash with older clusters in which this fix does not exist. From the 64 bits of ObjectID (long variable), 2 bits are reserved for epoch and 8 bits for recursive directory creation, if required. The most significant 2 bits of objectIDs is set to epoch. For clusters before HDDS-4315 there is no epoch as such. But it can be safely assumed that the most significant 2 bits of the objectID will be 00 (as it unlikely to reach trxn index > 2^62 in an existing cluster). From HDDS-4315 onwards, the Epoch for non-ratis OM clusters will be binary 01 (= decimal 1) and for ratis enabled OM cluster will be binary 10 (= decimal 2).

 ",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-10-06 22:35:48,43
13334077,OM Layout Version Manager init throws silent CNF error in integration tests.,"{code}
org.reflections.ReflectionsException: could not get type for name mockit.MockUp
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.reflections.Reflections.<init>(Reflections.java:182)
	at org.reflections.Reflections.<init>(Reflections.java:155)
	at org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.registerOzoneManagerRequests(OMLayoutVersionManagerImpl.java:122)
	at org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.init(OMLayoutVersionManagerImpl.java:100)
	at org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.initialize(OMLayoutVersionManagerImpl.java:83)
	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:363)
	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:930)
	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.createOMService(MiniOzoneHAClusterImpl.java:379)
	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.build(MiniOzoneHAClusterImpl.java:294)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.lang.ClassNotFoundException: mockit.MockUp
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 23 more
{code}",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-10-06 22:04:18,30
13333937,findbugs check succeeds despite compile error,"Findbugs check has been silently failing but reporting success for some time now.  The problem is that {{findbugs.sh}} determines exit code based on the number of findbugs failures.  If {{compile}} step fails, exit code is 0, ie. success.

{code:title=https://github.com/apache/hadoop-ozone/runs/1210535433#step:3:866}
2020-10-02T18:37:57.0699502Z [ERROR] Failed to execute goal on project hadoop-hdds-client: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-client:jar:1.1.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-common:jar:tests:1.1.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
{code}",pull-request-available,['build'],HDDS,Bug,Major,2020-10-06 13:53:57,1
13333931,Type-safe config design doc points to OM HA,"Abstract and links for http://hadoop.apache.org/ozone/docs/1.0.0/design/typesafeconfig.html are wrong, reference OM HA design doc.",pull-request-available,['documentation'],HDDS,Bug,Minor,2020-10-06 13:27:21,1
13333780,"Fix inconsistent Recon config keys that start with ""recon.om.""","{code}
hadoop-hdds/common/src/main/resources/ozone-default.xml
2318:    <name>recon.om.connection.request.timeout</name>
2327:    <name>recon.om.connection.timeout</name>
2336:    <name>recon.om.socket.timeout</name>
2345:    <name>recon.om.snapshot.task.initial.delay</name>
2353:    <name>recon.om.snapshot.task.interval.delay</name>
2361:    <name>recon.om.snapshot.task.flush.param</name>
{code}

These need to be deprecated and changed to ""ozone.recon.om.<>"".",newbie pull-request-available,['Ozone Recon'],HDDS,Bug,Minor,2020-10-05 18:29:15,67
13333773,Fix issue with quota update,"Currently volumeArgs using getCacheValue and put the same object in doubleBuffer, this might cause issue.

Let's take the below scenario:

InitialVolumeArgs quotaBytes -> 10000
1. T1 -> Update VolumeArgs, and subtracting 1000 and put this updated volumeArgs to DoubleBuffer.
2. T2-> Update VolumeArgs, and subtracting 2000 and has not still updated to double buffer.

*Now at the end of flushing these transactions, our DB should have 7000 as bytes used.*

Now T1 is picked by double Buffer and when it commits, and as it uses cached Object put into doubleBuffer, it flushes to DB with the updated value from T2(As it is a cache object) and update DB with bytesUsed as 7000.

And now OM has restarted, and only DB has transactions till T1. (We get this info from TransactionInfo Table(https://issues.apache.org/jira/browse/HDDS-3685)

Now T2 is again replayed, as it is not committed to DB, now DB will be again subtracted with 2000, and now DB will have 5000.

But after T2, the value should be 7000, so we have DB in an incorrect state.

Issue here:
1. As we use a cached object and put the same cached object into double buffer this can cause this kind of issue. ",pull-request-available,[],HDDS,Bug,Blocker,2020-10-05 17:27:04,4
13330930,Ozone checkstyle rule can't be imported to IntelliJ,"CheckStyle: Move LineLength Check parent from TreeWalker to Checker, otherwise fail to import to latest IntelliJ

Similar issue has been reported here and I've verified the fix locally that the IntelliJ can import checkstyle rule after the fix. 
https://github.com/checkstyle/checkstyle/issues/2116
",pull-request-available,[],HDDS,Improvement,Major,2020-10-04 02:44:05,28
13330591,Close Container event can fail if pipeline is removed,"If you call `pipelineManager.finalizeAndDestroyPipeline()` with onTimeout=false, then the finalizePipeline call will result in a closeContainer event to be fired for every container on the pipeline. These are handled asynchronously.

However, immediately after that, the `destroyPipeline(...)` call is made. This will remove the pipeline details from the various maps / stores.

Then the closeContainer events get processed, and they attempt to remove the container from the pipeline. However as the pipeline has already been destroyed, this throws an exception and the close container events never get sent to the DNs:

{code}
2020-10-01 15:44:18,838 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2
2020-10-01 15:44:18,842 [EventQueue-CloseContainerForCloseContainerEventHandler] ERROR container.CloseContainerEventHandler: Failed to close the container #2.
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=59e5ae16-f1fe-45ff-9044-dd237b0e91c6 not found
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:372)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:111)
	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:413)
	at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:352)
	at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:331)
	at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:66)
	at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.Onmessage(CloseContainerEventHandler.java:45)
	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor
{code}

The simple solution is to catch the exception and ignore it.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-10-02 15:38:01,11
13330458,Shade the org.apache.common.lang3 package as this is coming from other hadoop packages as well.,"In one of our duplicate classes tests, we noticed the duplicate classes because of commons-lang3. To avoid class collisions, it's good to shade the common-lang3 package as well.

java.lang.Exception: Duplicate class 'org.apache.commons.lang3.arch.Processor$Arch.class' detected in '/Users/umagangumalla/Work/repos/Gerrit/xxx/xxx/target/xxx-client-x2.3-dependencies/hadoop-ozone-filesystem-hadoop3-xxxx.jar', class is already present in '/Users/umagangumalla/Work/repos/Gerrit/xxx/xxx/target/xxx-client-xxx-dependencies/commons-lang3-3.9.jar'",pull-request-available,[],HDDS,Bug,Major,2020-10-01 22:24:11,8
13330452,SCM CA certificate does not encode KeyUsage extension properly,"This could be problematic with strict security provider such as FIPS. The default non-FIPS provider such as SunJCE and BC provider work fine though. This ticket is opened to fix it. 


{code:java}
2020-09-30 12:01:52,962 ERROR org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer: Unable to initialize CertificateServer.
org.apache.hadoop.hdds.security.exception.SCMSecurityException: java.security.cert.CertificateParsingException: cannot construct KeyUsage: java.lang.IllegalArgumentException: illegal object in getInstance: com.safelogic.cryptocomply.asn1.DEROctetString
        at org.apache.hadoop.hdds.security.x509.certificate.utils.CertificateCodec.getPEMEncodedString(CertificateCodec.java:105)
        at org.apache.hadoop.hdds.security.x509.certificate.utils.CertificateCodec.writeCertificate(CertificateCodec.java:182)
        at org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer.generateRootCertificate(DefaultCAServer.java:495)
        at org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer.generateSelfSignedCA(DefaultCAServer.java:303)
  
{code}

",pull-request-available,['Security'],HDDS,Improvement,Major,2020-10-01 21:56:49,28
13330341,Remove no longer needed class DatanodeAdminNodeDetails,"DatanodeAdminNodeDetails was added earlier in the decommission branch, to track metrics and, the decommission state and maintenance end time. 

After enhancing NodeStatus to old the Maintenance Expiry time, this class is no longer needed and it also duplicates information which is stored in other existing places.

This change removes it and then metrics etc can be added later in a different way.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-10-01 11:19:07,11
13330158,Use an interface in Ozone client instead of XceiverClientManager,"XceiverClientManager is used everywhere in the ozone client (Key/Block Input/OutputStream) to get a client when required.

To make it easier to create genesis/real unit tests, it would be better to use a generic interface instead of XceiverClientManager which can make it easy to replace the manager with a mock implementation.",pull-request-available,[],HDDS,Improvement,Major,2020-09-30 11:59:46,6
13330110,Allow multiple transactions per container to be sent for deletion by SCM,Currently SCM Block Deleting Service allows only one transaction per container to be sent for deletion to the datanode. This can slow down deletion if there are multiple delete transactions for a container.,pull-request-available,[],HDDS,Sub-task,Major,2020-09-30 07:30:42,35
13330076,SCM ServiceManager ,"SCM ServiceManager is going to control all the SCM background service so that they are only serving as the leader. 

ServiceManager also would bootstrap all the background services and protocol servers. 

It also needs to do validation steps when the SCM is up as the leader.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-09-30 03:11:11,36
13330062,Ozone Client not working with Hadoop Version  < 3.2,"HDDS-3560 created new ProxyInfo object in case of IllegalAccessError exception. But, it does not return the new instance and causes NPE in Hadoop versions < 3.2


{code:java}
20/09/29 23:10:22 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:20/09/29 23:10:22 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:java.lang.NullPointerException at org.apache.hadoop.io.retry.RetryInvocationHandler.isRpcInvocation(RetryInvocationHandler.java:435) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:354) at com.sun.proxy.$Proxy10.submitRequest(Unknown Source) at org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransport.submitRequest(Hadoop3OmTransport.java:89) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:213) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1030) at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:175) at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:242) at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:113) at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:149) at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:51) at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:94) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:161) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3288) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3337) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3305) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361) at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:352) at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:250) at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233) at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103) at org.apache.hadoop.fs.shell.Command.run(Command.java:177) at org.apache.hadoop.fs.FsShell.run(FsShell.java:326) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) at org.apache.hadoop.fs.FsShell.main(FsShell.java:389)ls: Couldn't create RpcClient protocol
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-09-29 23:58:50,13
13329897,Throw exception from hadoop2 filesystem jar in HA environment,"Thanks for Tamas Pleszkan for reporting this problem.

ozone-filesystem-hadoop2 doesn't support OM-HA (today) as the used Hadoop3OmTransport uses FailoverProxyProvider which is not available in hadoop2.

Long-term we need a custom failover mechanism, but this jira suggests to improve the error handling. `Hadoop27OmTransportFactory` should throw an exception if HA is used.

Used command:

{code}
spark-submit --master yarn --deploy-mode client --executor-memory 1g --conf ""spark.yarn.access.hadoopFileSystems=o3fs://bucket.hdfs.ozone1/"" --jars ""/opt/cloudera/parcels/CDH-7.1.3-1.cdh7.1.3.p0.4992530/jars/hadoop-ozone-filesystem-hadoop2-0.5.0.7.1.3.0-100.jar"" SparkWordCount.py o3fs://bucket.hdfs.ozone1/words 2
{code}

Current exception:

{code}
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om1.
{code}

Expected exception: Unsupported operation exception with meaningful hint to use hadoop3 filesystem jar.",newbie pull-request-available,['OM HA'],HDDS,Bug,Major,2020-09-29 09:28:25,7
13329886,the icon of hadoop-ozone is bigger than ever,It could be a by-product of the introduction of the issue： https://issues.apache.org/jira/browse/HDDS-4166,pull-request-available,['documentation'],HDDS,Bug,Trivial,2020-09-29 08:26:14,6
13329821,Exclude protobuff classes from ozone-filesystem-hadoop3 jars,"Currently Ozone-filesystem-hadoop3 jar including protobuff classes. We are already keeping the dependency on hadoop jars a prerequisite condition. And hadoop will get the protobuf classes along with it's jars. So, getting protobuff jars again with Ozone-filesystem-hadoop3 jar would be just duplication. So, we can exclude that prootobuff classes.",pull-request-available,[],HDDS,Bug,Major,2020-09-29 03:08:06,8
13329759,Read is slow due to frequent calls to UGI.getCurrentUser() and getTokens(),"Ozone read operation turned out to be slow mainly because we do a new UGI.getCurrentUser for block token for each of the calls.

We need to cache the block token / UGI.getCurrentUserCall() to make it faster.

 !image-2020-09-28-16-19-17-581.png! 

To reproduce:

Checkout: https://github.com/elek/hadoop-ozone/tree/mocked-read

{code}
cd hadoop-ozone/client

export MAVEN_OPTS=-agentpath:/home/elek/prog/async-profiler/build/libasyncProfiler.so=start,file=/tmp/profile-%t-%p.svg

mvn compile exec:java -Dexec.mainClass=org.apache.hadoop.ozone.client.io.TestKeyOutputStreamUnit -Dexec.classpathScope=test
{code}",pull-request-available,[],HDDS,Improvement,Major,2020-09-28 14:19:20,1
13329614,Improve the emptyDir syntax,"the usage of emptyDir should be {}

 
{code:java}
emptyDir: null

emptyDir: {}{code}",pull-request-available,[],HDDS,Improvement,Minor,2020-09-27 10:09:02,52
13329539,Document notable configurations for Recon ,"In [Reon doc link|https://hadoop.apache.org/ozone/docs/1.0.0/feature/recon.html], there is no helpful description about how to quickly setup the Recon server. As Recon is one major feature in Ozone 1.0 version, we need to completed this document.",pull-request-available,['Ozone Recon'],HDDS,Improvement,Minor,2020-09-26 14:58:26,68
13329372,Change the log level of the SCM Delete block to improve performance.,"There are scenarios in which the DELETE operation is very frequent. Now that info level logs in SCM are affecting performance, we should change this to Debug",pull-request-available,[],HDDS,Improvement,Major,2020-09-25 09:47:20,4
13328985,Avoid logging chunk content in Ozone Insight,"HDDS-2660 added an insight point for the datanode dispatcher.  At trace level it logs all chunk content, which can be huge and contain control characters, so I think we should avoid it.",pull-request-available,['Tools'],HDDS,Improvement,Major,2020-09-23 12:13:43,1
13328981,Add more reusable byteman scripts to debug ofs/o3fs performance,"I am using https://byteman.jboss.org to debug the performance of spark + terage with different scripts. Some byteman scripts are already shared by HDDS-4095 or HDDS-342 but it seems to be a good practice to share the newer scripts to make it possible to reproduce performance problems.

For using byteman with Ozone, see this video:
https://www.youtube.com/watch?v=_4eYsH8F50E&list=PLCaV-jpCBO8U_WqyySszmbmnL-dhlzF6o&index=5",pull-request-available,[],HDDS,Improvement,Major,2020-09-23 12:01:11,6
13328885,Ozone DataNode thinks a volume is failed if an unexpected file is in the HDDS root directory,"Took me some time to debug a trivial bug.

DataNode crashes after this mysterious error and no explanation:
{noformat}
10:11:44.382 PM	INFO	MutableVolumeSet	Moving Volume : /var/lib/hadoop-ozone/fake_datanode/data/hdds to failed Volumes
10:11:46.287 PM	ERROR	StateContext	Critical error occurred in StateMachine, setting shutDownMachine
10:11:46.287 PM	ERROR	DatanodeStateMachine	DatanodeStateMachine Shutdown due to an critical error
{noformat}
Turns out that if there are unexpected files under the hdds directory ($hdds.datanode.dir/hdds), DN thinks the volume is bad and move it to failed volume list, without an error explanation. I was editing the VERSION file and vim created a temp file under the directory. This is impossible to debug without reading the code.
{code:java|title=HddsVolumeUtil#checkVolume()}
} else if(hddsFiles.length == 2) {
      // The files should be Version and SCM directory
      if (scmDir.exists()) {
        return true;
      } else {
        logger.error(""Volume {} is in Inconsistent state, expected scm "" +
                ""directory {} does not exist"", volumeRoot, scmDir
            .getAbsolutePath());
        return false;
      }
    } else {
      // The hdds root dir should always have 2 files. One is Version file
      // and other is SCM directory.
      <---- HERE!
      return false;
    }
{code}",newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-09-22 23:12:57,55
13328819,Prepare for Upgrade step should purge the log after waiting for the last txn to be applied.,"This is a follow up task from HDDS-4227 in which the prepare upgrade/downgrade task should purge the Raft log immediately after waiting for the last txn to be applied. This is to make sure that we dont ""apply"" transactions in different versions of the code across the quorum. A lagging follower will use a Ratis snapshot to bootstrap itself on restart.",pull-request-available,[],HDDS,Sub-task,Major,2020-09-22 16:23:52,30
13328756,CreateFile : store parent dir entries into DirTable and file entry into separate FileTable,This task is to handle #createFile ofs client request. Here the idea is to store all the missing parents in the {{keyname}} into 'DirTable' and file into 'FileTable'.,pull-request-available,[],HDDS,Sub-task,Major,2020-09-22 11:12:41,40
13328705,Uniform naming conventions of Ozone Shell Options.,"Current Shell command of Ozone, some use hump connection, some use '-' connection. We need to unify the naming conventions.
See the usage [documentation of Picocli|https://picocli.info/#command-methods], which use '-' connection. So I'm going to unify the naming conventions here.
 !image-2020-09-22-14-51-18-968.png! ",pull-request-available,[],HDDS,Improvement,Major,2020-09-22 07:01:38,4
13328475,ReplicatiomManager shouldn't consider origin node Id for CLOSED containers.,"ReplicationManager now retain one healthy replica for per origin node id, so if there are 5 replicas and all of them are birth in different node, replicatiomManager won't reduce the replica for this container.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-09-21 07:45:59,69
13328324,Use ClientID and CallID from Rpc Client to detect retry requests,"Use clientID and callID to uniquely identify the requests.
This will help in case when the request is retried for write requests, when the previous one is already processed, the previous result can be returned from the cache.",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2020-09-18 23:14:23,13
13328280,Invalid ozone CLI command shows wrong warning for expected syntax for URI,"When running invalid commands , we see the expected syntax not mentioning about om service id when HA is enabled. 

 
{code:java}
[root@c1265-node2 ~]# ozone freon ddsg -n 1 -r o3fs://ozonesvc
 
0:00:0020/09/18 16:27:23 WARN fs.FileSystem: Failed to initialize fileystem o3fs://ozonesvc: java.lang.IllegalArgumentException: Ozone file system URL should be one of the following formats: o3fs://bucket.volume/key  OR o3fs://bucket.volume.om-host.example.com/key  OR o3fs://bucket.volume.om-host.example.com:5678/key
Ozone file system URL should be one of the following formats: o3fs://bucket.volume/key  OR o3fs://bucket.volume.om-host.example.com/key  OR o3fs://bucket.volume.om-host.example.com:5678/key
{code}
 

We might need to revisit below uri syntax description for below one.
{code:java}
[root@c1265-node2 ~]# ozone sh volume create --help
Usage: ozone sh volume create [-hV] [-q=<quota>] [-u=<ownerName>] <value>
Creates a volume for the specified user
      <value>              URI of the volume.
                           Ozone URI could start with o3:// or without prefix. URI
                             may contain the host/serviceId  and port of the OM
                             server. Both are optional. If they are not specified it
                             will be identified from the config files.
  -h, --help               Show this help message and exit.
{code}",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-09-18 16:32:15,49
13328245,Set GDPR to a Security submenu in EN and CN document.,"Base on [~xyao] comment on HDDS-4156.
https://github.com/apache/hadoop-ozone/pull/1368#issuecomment-694532324

Set GDPR to a Security submenu in EN and CN document.",newbie pull-request-available,[],HDDS,Improvement,Minor,2020-09-18 13:01:21,67
13327947,Bucket space: add usedBytes and update it when create and delete key,"In addition, the current Quota setting does not take effect. HDDS-541 gives all the work needed to perfect Quota.
This PR is a subtask of HDDS-541.
First, we increase usedBytes of Bucket. Later, we will judge whether the Bucket can be written based on this when we write the key.(Volume has implemented this, and this PR is based on HDDS-4053)",pull-request-available,[],HDDS,Sub-task,Major,2020-09-17 02:35:34,4
13327946,SCM changes to process Layout Info in register request/response,Add LayoutVersion request/response for DN registration.,pull-request-available,[],HDDS,Sub-task,Major,2020-09-17 02:29:20,44
13327935,Update Ratis version to latest snapshot,This Jira aims to update ozone with latest Ratis snapshot which has a critical fix for OM HA - RATIS-1025.,pull-request-available,[],HDDS,Bug,Major,2020-09-16 23:35:17,43
13327852,Fix wrong logger name,"Fix wrong logger name, the logger name doesn't match the class name.

example
{code:java}
public class OMBucketSetAclRequest extends OMBucketAclRequest {
  private static final Logger LOG =
      LoggerFactory.getLogger(OMBucketAddAclRequest.class);
{code}",pull-request-available,['build'],HDDS,Improvement,Major,2020-09-16 11:24:59,52
13327540,Get API not working from S3A filesystem with Ozone S3,"TroubleShooting S3A mentions S3 compatible servers that donot support Etags will see this server

Refer [link|https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html] and look for below section content.
Using a third-party S3 implementation that doesn’t support eTags might result in the following error.

org.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://my-bucket/test/file.txt':
 Change detection policy requires ETag
  at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:153)
  at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:200)
  at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:346)
  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$2(Invoker.java:195)
  at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
  at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:193)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:215)
  at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:339)
  at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:372)


{code:java}
org.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processNewRevision(ChangeTracker.java:275)
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processMetadata(ChangeTracker.java:261)
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:195)
	at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:208)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:359)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$3(Invoker.java:223)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$5(Invoker.java:347)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:343)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:221)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:265)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:351)
	at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:464)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129)
	at org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:494)
	at org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:416)
	at org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:351)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:286)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:271)
	at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
	at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:266)
	at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)
	at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:237)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
get: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag
{code}
",OzoneS3 S3A,['S3'],HDDS,Bug,Major,2020-09-14 23:27:05,13
13327474,Support HADOOP_TOKEN_FILE_LOCATION for Ozone token CLI,"Currently, Ozone token CLI produce token in base64 encode format. This is not compatible with HADOOP_TOKEN_FILE_LOCATION and can't be used directly for Ozone/Hadoop CLI to authenticate. This ticket is opened to persist ozone token in a format that is compatible with HADOOP_TOKEN_FILE_LOCATION along with tests. ",pull-request-available,['Security'],HDDS,Improvement,Major,2020-09-14 15:46:44,28
13327321,Ozone support truncate operation,Design: https://docs.google.com/document/d/1Ju9WeuFuf_D8gElRCJH1-as0OyC6TOtHPHErycL43XQ/edit#,pull-request-available,[],HDDS,New Feature,Major,2020-09-14 01:42:24,65
13327234,"Move ""Om*Codec.java"" to new project hadoop-ozone/interface-storage",This is the first step to separate storage and RPC proto files. ,pull-request-available,[],HDDS,Sub-task,Major,2020-09-12 18:07:27,54
13327164,Ozone client FS path validation is not present in OFS.,"HDDS-3969 added a validation step in the BasicOzoneFileSystem which makes sure there are no keys with trailing "".."" created through the o3fs API. This needs to go into the OFS code path as well.",pull-request-available,['Ozone Client'],HDDS,Bug,Blocker,2020-09-11 22:35:12,59
13326836,SCMBlockLocationFailoverProxyProvider should handle LeaderNotReadyException,"It is an enhancement for HDDS-3188.

Like OMFailoverProxyProvider, SCMBlockLocationFailoverProxyProvider should also handle LeaderNotReadyException.

If SCM client (like OzoneManager) has touched leader SCM, meanwhile leader SCM is stuck in replaying raft log entries(e.g., that SCM restarts and becomes leader, it needs time to recover its state machine by replaying all raft log entries), SCM client should not round robin to the next SCM, It should wait and retry the same SCM later.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-09-10 10:31:05,54
13326779,add field 'num' to ALLOCATE_BLOCK of scm audit log.," 

The scm audit log for ALLOCATE_BLOCK is as follows:
{code:java}
2020-09-10 03:42:08,196 | INFO | SCMAudit | user=root | ip=172.16.90.221 | op=ALLOCATE_BLOCK {owner=7da0b4c4-d053-4fa0-8648-44ff0b8ba1bf, size=268435456, type=RATIS, factor=THREE} | ret=SUCCESS |{code}
 

One might be interested about the num of blocks allocated, better add field 'num' to  ALLOCATE_BLOCK of scm audit log.",pull-request-available pull-requests-available,[],HDDS,Improvement,Minor,2020-09-10 03:51:47,36
13326735,"Implement a ""prepareForUpgrade"" step that applies all committed transactions onto the OM state machine.","*Why is this needed?*
Through HDDS-4143, we have a generic factory to handle multiple versions of apply transaction implementations based on layout version. Hence, this factory can be used to handle versioned requests across layout versions, whenever both the versions need to exist in the code (Let's say for HDDS-2939). 

However, it has been noticed that the OM ratis requests are still undergoing lot of minor changes (HDDS-4007, HDDS-4007, HDDS-3903), and in these cases it will become hard to maintain 2 versions of the code just to support clean upgrades. 

Hence, the plan is to build a pre-upgrade utility (client API) that makes sure that an OM instance has no ""un-applied"" transactions in this Raft log. Invoking this client API makes sure that the upgrade starts with a clean state. Of course, this would be needed only in a HA setup. In a non HA setup, this can either be skipped, or when invoked will be a No-Op (Non Ratis) or cause no harm (Single node Ratis).

*How does it work?*
Before updating the software bits, our goal is to get OMs to get to the  latest state with respect to apply transaction. The reason we want this is to make sure that the same version of the code executes the AT step in all the 3 OMs. In a high level, the flow will be as follows.

* Before upgrade, *stop* the OMs.
* Start OMs with a special flag --prepareUpgrade (This is something like --init,  which is a special state which stops the ephemeral OM instance after doing some work)
* When OM is started with the --prepareUpgrade flag, it does not start the RPC server, so no new requests can get in.
* In this state, we give every OM time to apply txn until the last txn.
* We know that at least 2 OMs would have gotten the last client request transaction committed into their log. Hence, those 2 OMs are expected to apply transaction to that index faster.
* At every OM, the Raft log will be purged after this wait period (so that the replay does not happen), and a Ratis snapshot taken at last txn.
* Even if there is a lagger OM which is unable to get to last applied txn index, its logs will be purged after the wait time expires.
* Now when OMs are started with newer version, all the OMs will start using the new code.
* The lagger OM will get the new Ratis snapshot since there are no logs to replay from.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-09-09 20:38:38,30
13326610,[OzoneFS optimization] Provide a mechanism for efficient path lookup,With the new file system HDDS-2939 like semantics design it requires multiple DB lookups to traverse the path component in top-down fashion. This task to discuss use cases and proposals to reduce the performance penalties during path lookups.,pull-request-available,[],HDDS,New Feature,Major,2020-09-09 07:01:59,40
13326553,BlockManagerImpl#getBlockByID does unnecessary serialization,"After HDDS-3869, tables in the datanode handle coding/decoding objects to/from RocksDB, and the caller no longer has to do this manually. As a result, the BlockManagerImpl#getBlockByID method should now return a BlockData type, instead of a byte array. In the current implementation, this method converts the block data into a byte array and returns it to the caller, who then converts the byte array back to block data in order to use it.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2020-09-08 20:51:57,22
13326512,Revisit 'static' nature of OM Layout Version Manager.,"Investigate whether we can programmatically instantiate the OM Aspect so that we can move away from static nature of OM Layout Version Manager. Moving away from static behavior will help out with easy unit testing. 

cc [~pifta]",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-09-08 15:16:52,30
13326454,Remove test TestRatisManager,Delete this test as RatisManager is no longer present and this test has been disabled for a long time,pull-request-available,[],HDDS,Bug,Major,2020-09-08 12:02:17,26
13326453,Remove test TestOzoneContainerRatis,Delete TestOzoneContainerRatis as it has been disabled for a long time and is no longer relevant.,pull-request-available,[],HDDS,Bug,Major,2020-09-08 12:01:24,26
13326291,update freon doc.,"At present, the link to the Freon introduction document is 0.4.0, and now 1.0 has been released and the URL needs to be updated to 1.0",pull-request-available,[],HDDS,Improvement,Major,2020-09-07 10:55:15,4
13326113,[OFS] Better owner and group display for listing Ozone volumes and buckets,"Improve volumes' and buckets' owner and group display when listing in OFS.

1. Display short name instead of full Kerberos principal.
2. For volumes, get actual group of the owner (currently it is the volume admin name which is incorrect)
3. For buckets, display the owner and group of its parent volume.",pull-request-available,[],HDDS,Bug,Major,2020-09-04 20:19:44,12
13326110,ResolveBucket during checkAcls fails,"In HA, in validateAndUpdateCache when resolveBucket, it checks the permission using checkAcls. But it will have not any RpcContext and it will fail with NPE in checkAcls when getting hostName.

For this same reason, we added the required information to check ACLs into OMRequest.


{code:java}
java.lang.NullPointerException
	at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1604)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3497)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3465)
	at org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3452)
	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.resolveBucketLink(OMKeyRequest.java:96)
	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:215)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:246)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-09-04 19:30:45,13
13326018,S3A Filesystem does not work with Ozone S3 in file system compat mode,"When *ozone.om.enable.filesystem.paths* is enabled

 

hdfs dfs -mkdir -p s3a://b12345/d11/d12 -> Success

hdfs dfs -put /tmp/file1 s3a://b12345/d11/d12/file1 -> fails with below error

 
{code:java}
2020-09-04 03:53:51,377 ERROR org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:b1234, Keyd11/d12/file1._COPYING_. Exception:{}
NOT_A_FILE org.apache.hadoop.ozone.om.exceptions.OMException: Can not create file: cp/k1._COPYING_ as there is already file in the given path
 at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:256)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:246)
 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748){code}
*Reason for this*
 S3A filesystem when create directory creates an empty file

*Now entries in Ozone KeyTable after create directory*
 d11/
 d11/d12

Because of this in OMFileRequest.VerifyInFilesPath fails with FILE_EXISTS_IN_GIVEN_PATH because d11/d12 is considered as file not a directory. (As in ozone currently, directories end with trailing ""/"")

So, when d11/d12/file is created, we check parent exists, now d11/d12 is considered as file and fails with NOT_A_FILE

When disabled it works fine, as when disabled during key create we do not check any filesystem semantics and also does not create intermediate directories.
{code:java}
[root@bvoz-1 ~]# hdfs dfs -mkdir -p s3a://b12345/d11/d12
[root@bvoz-1 ~]# hdfs dfs -put /etc/hadoop/conf/ozone-site.xml s3a://b12345/d11/d12/k1
[root@bvoz-1 ~]# hdfs dfs -ls s3a://b12345/d11/d12
Found 1 items
-rw-rw-rw-   1 systest systest       2373 2020-09-04 04:45 s3a://b12345/d11/d12/k1
{code}
 ",OzoneS3 S3A pull-request-available,[],HDDS,Sub-task,Blocker,2020-09-04 04:55:41,13
13325986,Fix table rendering and logo display in docs,"The Docs page does not render the table correctly in OM and SCM architecture pages.

Also, the ozone logo in the header is not visible.",pull-request-available,['documentation'],HDDS,Bug,Major,2020-09-03 21:18:14,37
13325938,upgrade docker environment does not work with KEEP_RUNNING=true,"Ozone {{upgrade}} Docker Compose environment fails if run with {{KEEP_RUNNING=true}}.  The variable is applied to both runs (pre- and post-upgrade), but pre-upgrade containers should be stopped anyway, since they will be replaced by the new ones.

{code}
$ cd hadoop-ozone/dist/target/ozone-1.1.0-SNAPSHOT/compose/upgrade
$ KEEP_RUNNING=true ./test.sh
...
Failed: IO error: While lock file: scm.db/LOCK: Resource temporarily unavailable
{code}",pull-request-available,"['docker', 'test']",HDDS,Bug,Minor,2020-09-03 14:22:07,1
13325887,Publish docker image for ozone 1.0.0,Docker image is based on the voted and approved artifacts which is availabe. We can create the image.,pull-request-available,[],HDDS,Improvement,Major,2020-09-03 09:40:07,6
13325827,Fix failed UT: TestOMAllocateBlockRequest#testValidateAndUpdateCache, !screenshot-1.png! ,pull-request-available,[],HDDS,Bug,Major,2020-09-03 01:29:51,65
13325812,Compile Ozone with multiple Java versions,Add matrix build in GitHub Actions for compiling Ozone with both Java 8 and 11.,pull-request-available,['build'],HDDS,Improvement,Major,2020-09-02 21:25:20,1
13325809,Failed to load existing service definition files: ...SubcommandWithParent,"{code}
[INFO] Apache Hadoop HDDS Tools ........................... FAILURE
...
[ERROR] Failed to load existing service definition files: java.nio.file.NoSuchFileException: hadoop-hdds/tools/target/classes/META-INF/services/org.apache.hadoop.hdds.cli.SubcommandWithParent
{code}",jdk11,['build'],HDDS,Bug,Major,2020-09-02 20:27:32,1
13325801,Add an endpoint in Recon to query Prometheus,Recon should have an endpoint to proxy requests to the configured prometheus instance.,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-09-02 19:06:52,37
13325796,Fix Recon after HDDS-4133,"HDDS-4133 breaks Recon, this jira is for fixing and make Recon work with SCM HA.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-09-02 18:50:17,19
13325776,Create a script to check AWS S3 compatibility,"Ozone S3G implements the REST interface of AWS S3 protocol. Our robot test based scripts check if it's possible to use Ozone S3 with the AWS client tool.

But occasionally we should check if our robot test definitions are valid: robot tests should be executed with using real AWS endpoint and bucket(s) and all the test cases should be passed.

This patch provides a simple shell script to make this cross-check easier.  ",pull-request-available,[],HDDS,Improvement,Major,2020-09-02 16:43:46,6
13325774,Range used by S3 MultipartUpload copy-from-source should be inclusive,"S3 API provides a feature to copy a specific range from an existing key.

Based on the documentation, this range definitions is inclusive:

https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part-copy.html

{quote}
-copy-source-range (string)

    The range of bytes to copy from the source object. The range value must use the form bytes=first-last, where the first and last are the zero-based byte offsets to copy. For example, bytes=0-9 indicates that you want to copy the first 10 bytes of the source. You can copy a range only if the source object is greater than 5 MB.
{quote}

But as it's visible from our [robot test|http://example.com], in our case we use exclusive range:

{code}
upload-part-copy ... --copy-source-range bytes=0-10485758
upload-part-copy ... --copy-source-range bytes=10485758-10485760
{code}

Based on this AWS documentation it will return with a (10485758 + 1) + 3 bytes long key,  which is impossible if our original source key is just 10485760.

I think the right usage to get the original key is the following:

{code}
upload-part-copy ... --copy-source-range bytes=0-10485757
upload-part-copy ... --copy-source-range bytes=10485758-10485759
{code}

(Note, this bug is found with the script in HDDS-4194, which showed that AWS S3 is working in different way).",pull-request-available,[],HDDS,Bug,Blocker,2020-09-02 16:40:40,6
13325697,enable SCM Raft Group based on config ozone.scm.names," 

Say ozone.scm.names is ""ip1,ip2,ip3"", scm with ip1 identifies its RaftPeerId as scm1,  scm with ip2 identifies its RaftPeerId as scm2, scm with ip3 identifies its RaftPeerId as scm3. They will automatically become a raft group.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-09-02 08:53:10,36
13325692,Add failover proxy for SCM container client,Take advantage of failover proxy in HDDS-3188 and have failover proxy for SCM container client as well,pull-request-available,[],HDDS,Sub-task,Major,2020-09-02 08:27:28,66
13325494,Adjust RetryPolicy of SCMConnectionManager for SCM/Recon,"*The problem is:*

If setup one Recon and one SCM, then shutdown the Recon server, all Datanodes will be stale/dead very soon at SCM side.

 

*The root cause is:*

Current RetryPolicy of Datanode for SCM is retryForeverWithFixedSleep:
{code:java}
RetryPolicy retryPolicy =
 RetryPolicies.retryForeverWithFixedSleep(
 1000, TimeUnit.MILLISECONDS);

StorageContainerDatanodeProtocolPB rpcProxy = RPC.getProtocolProxy(
    StorageContainerDatanodeProtocolPB.class, version,
    address, UserGroupInformation.getCurrentUser(), hadoopConfig,
    NetUtils.getDefaultSocketFactory(hadoopConfig), getRpcTimeout(),
    retryPolicy).getProxy();{code}
 that for Recon is retryUpToMaximumCountWithFixedSleep:
{code:java}
RetryPolicy retryPolicy =
    RetryPolicies.retryUpToMaximumCountWithFixedSleep(10,
        60000, TimeUnit.MILLISECONDS);

ReconDatanodeProtocolPB rpcProxy = RPC.getProtocolProxy(
    ReconDatanodeProtocolPB.class, version,
    address, UserGroupInformation.getCurrentUser(), hadoopConfig,
    NetUtils.getDefaultSocketFactory(hadoopConfig), getRpcTimeout(),
    retryPolicy).getProxy();
{code}
 

The executorService in DatanodeStateMachine is Executors.newFixedThreadPool(...), whose default pool size is 2, one for Recon, another for SCM.

 

When encounter rpc failure, call() of RegisterEndpointTask, VersionEndpointTask, HeartbeatEndpointTask will retry while holding the rpcEndpoint.lock(). For example:
{code:java}
public EndpointStateMachine.EndPointStates call() throws Exception {
  rpcEndpoint.lock();

  try {

    ....
    SCMHeartbeatResponseProto reponse = rpcEndpoint.getEndPoint()
        .sendHeartbeat(request);
    ....

  } finally {
    rpcEndpoint.unlock();
  }

  return rpcEndpoint.getState();
}
{code}
 

If Recon is down, the thread running Recon task will retry due to rpc failure, meanwhile holds the lock of EndpointStateMachine for Recon. When DatanodeStateMachine schedule the next round of SCM/Recon task, the only left thread will be assigned to run Recon task, and blocked at waiting for the lock of EndpointStateMachine for Recon.
{code:java}
public EndpointStateMachine.EndPointStates call() throws Exception {
  rpcEndpoint.lock();
  ...{code}
 

*The solution is:*

Since DatanodeStateMachine will periodically schedule SCM/Recon tasks, we may adjust RetryPolicy so that won't retry for longer that 1min. 

 

*The change has no side effect:*

1) VersionEndpointTask.call() is fine

2) RegisterEndpointTask.call() will query containerReport, nodeReport, pipelineReports from OzoneContainer, which is fine.

3) HeartbeatEndpointTask.call() will putBackReports(), which is fine.

 ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2020-09-01 09:49:24,36
13325479,Disable IncrementalByteBuffer by default in Ozone Client,"During the teragen test it was identified that the IncrementalByteBuffer is one of the biggest bottlenecks. 

In the PR of HDDS-4119 a long conversation has been started if it can be removed or we need other solution to optimize.

This jira is opened to continue the discussion and either remove or optimize the IncrementalByteByffer.",pull-request-available,[],HDDS,Improvement,Major,2020-09-01 08:36:00,6
13325460,Add Features menu for Chinese document.,"In English document, there is a *Features* menu, and *GDPR* is *Feature's* submenu.
 So we can add *Features* menu and change *GDPR* to *Features* submenu in Chinese document.",newbie pull-request-available,['documentation'],HDDS,Improvement,Minor,2020-09-01 06:25:17,55
13325440,Onboard HDDS-3869 into Layout version management,"In HDDS-3869 (Use different column families for datanode block and metadata),  there was a backward compatible change made in the Ozone datanode RocksDB. This JIRA tracks the effort to use a ""Layout Version"" to track this change such that it is NOT used before finalizing the cluster.

cc [~erose], [~hanishakoneru]",pull-request-available,[],HDDS,Sub-task,Major,2020-09-01 03:38:27,22
13325439,"Add acceptance tests for upgrade, finalization and downgrade","Start up Ozone in a specific old version from docker registry. Write some data into the cluster. Upgrade to latest version, finalize if needed and run some data validation tests. We can create a generic test harness that takes in a source and target version and runs the set of tests that are specified after the upgrade.",pull-request-available,[],HDDS,Sub-task,Major,2020-09-01 03:36:20,22
13325436,SCM Finalize client command implementation.,"* RPC endpoint implementation
* Ratis request to persist MLV, Trigger DN Finalize, Pipeline close. (WHEN MLV changes)",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-09-01 03:32:25,44
13325421,Fix failed UT: test2WayCommitForTimeoutException,"org.apache.ratis.protocol.GroupMismatchException: 6f2b1ee5-bc2b-491c-bff4-ab0f4ce64709: group-2D066F5AFBD0 not found.

        at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:127)
        at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:274)
        at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:283)
        at org.apache.hadoop.ozone.container.ContainerTestHelper.getRaftServerImpl(ContainerTestHelper.java:593)
        at org.apache.hadoop.ozone.container.ContainerTestHelper.isRatisFollower(ContainerTestHelper.java:608)
        at org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.test2WayCommitForTimeoutException(TestWatchForCommit.java:302)",pull-request-available,['test'],HDDS,Bug,Major,2020-09-01 01:37:42,65
13325418,Implement Datanode Finalization,"* Create FinalizeCommand in SCM and Datanode protocol.
* Create FinalizeCommand Handler in Datanode.
* Datanode Finalization should FAIL if there are open containers on it.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-09-01 00:44:47,44
13325417,Add current HDDS layout version to Datanode heartbeat and registration.,Add the layout version as a field to proto.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-09-01 00:39:43,44
13325416,Implement HDDS Version management using the LayoutVersionManager interface.,"* Create HDDS Layout Feature Catalog similar to the OM Layout Feature Catalog.
* Any layout change to SCM and Datanode needs to be recorded here as a Layout Feature.
* This includes new SCM HA requests, new container layouts in DN etc.
* Create a HDDSLayoutVersionManager similar to OMLayoutVersionManager.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2020-09-01 00:38:24,44
13325413,Implement Finalize command in Ozone Manager server.,"Using changes from HDDS-4141 and HDDS-3829, we can finish the OM finalization logic by implementing the Ratis request to Finalize.

On the server side, this finalize command should update the internal Upgrade state to ""Finalized"". This operation can be a No-Op if there are no layout changes across an upgrade.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-09-01 00:20:37,63
13325293,Fix typo in method description.,"[In this line|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/utils/CertificateCodec.java#L288], the word _X509Cer{color:#ff0000}i{color}tificate_ is misspelled, it should be ""X509Certificate"".",newbie pull-request-available,[],HDDS,Improvement,Trivial,2020-08-31 08:12:19,70
13325283,Fix some minor errors in StorageContainerManager.md,"This [document|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/concept/StorageContainerManager.md] contains two small errors:

1. typo: SCM's Cer{color:#ff0000}i{color}tificate authority
 2. duplicate title bar: Notable configuration",pull-request-available,['documentation'],HDDS,Improvement,Trivial,2020-08-31 07:13:38,71
13325264,Remove reference to Skaffold in the README in dist/,"I got sidetracked when I ran into the README in the dist folder because it referenced Skaffold which hasn't been used in a while.

So that others don't get confused as I did,  I created a PR  to fix the wayward reference:
 * [https://github.com/apache/hadoop-ozone/pull/1360]

This issue is merely to track the PR.",pull-request-available,['documentation'],HDDS,Improvement,Trivial,2020-08-31 04:29:27,72
13325118,Acceptance test logs missing if SCM fails to exit safe mode,"Acceptance test sometimes fails due to SCM not coming out of safe mode.  If this happens, the cluster is stopped without running Robot tests.  {{rebot}} command to process test results fails due to missing input, and acceptance check is abruptly stopped without fetching docker logs or running tests in other environments.",pull-request-available,['test'],HDDS,Bug,Major,2020-08-29 06:42:19,1
13325070,GitHub Actions cache does not work outside of workspace,"Ozone source is checked out for _acceptance_ and _kubernetes_ checks to {{/mnt}}, outside of {{GITHUB_WORKSPACE}}, and only after _Cache_ steps.  Therefore no files are found for which hash would be computed to be included in cache keys.

{code:title=https://github.com/apache/hadoop-ozone/blob/44acf78aec6c3a4e1c5fea3a43971144c6da9a4c/.github/workflows/post-commit.yml#L167-L171}
      - name: Cache for maven dependencies
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: maven-repo-${{ hashFiles('**/pom.xml') }}
{code}

Cache key is always {{maven-repo-}}:

{code:title=https://github.com/apache/hadoop-ozone/runs/1042358389#step:2:10}
Cache restored from key: maven-repo-
{code}

The same old cache is used for all builds, even if dependencies are changed in {{pom.xml}}, gradually resulting in more and more downloads during builds:

{code:title=https://github.com/apache/hadoop-ozone/runs/1036271227#step:9:680}
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/info/picocli/picocli/4.4.0/picocli-4.4.0.jar (389 kB at 2.4 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ratis/ratis-server/1.0.0/ratis-server-1.0.0.jar (380 kB at 2.4 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-api/2.13.3/log4j-api-2.13.3.jar (292 kB at 1.9 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ratis/ratis-proto/1.0.0/ratis-proto-1.0.0.jar (1.2 MB at 6.6 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-core/2.13.3/log4j-core-2.13.3.jar (1.7 MB at 8.5 MB/s)
{code}",pull-request-available,['build'],HDDS,Bug,Minor,2020-08-28 19:05:30,1
13324918,Use BeforeClass to init test cluster in TestOzoneFileSystem,"Quote [~ayushsaxena]:

> Initialising cluster can be though done in BeforeClass and the tests can be made independent, preventing failure of one affecting subsequent tests.

Previous discussion here: https://github.com/apache/hadoop-ozone/pull/1316#discussion_r478634376


We could init the cluster in @BeforeClass, then tear it down in @AfterClass . In theory the performance would be the same with the current solution ({{testFileSystem}}).

And we might need to tweak the clean up of some existing tests a bit.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-08-27 21:23:01,71
13324890,Set fs.defaultFS in docker compose cluster config to OFS,"Set fs.defaultFS in docker compose cluster config to OFS by default.

This shouldn't impact any existing tests. This makes life easier when developer manually logs into the cluster and use hadoop fs commands. e.g. {{ozone fs -ls /}} can easily list all volumes in the test cluster with OFS.

Previous discussion at https://github.com/apache/hadoop-ozone/pull/1352#discussion_r478388391",pull-request-available,[],HDDS,Improvement,Minor,2020-08-27 18:25:06,12
13324737,add hierarchical layout to Chinese doc,"English doc is updated a lot in https://issues.apache.org/jira/browse/HDDS-4042, and its flat structure becomes more hierarchical, to keep the consistency, we need to update the Chinese doc too.",pull-request-available,[],HDDS,Sub-task,Major,2020-08-27 01:18:30,55
13324720,Directory and filename can end up with same name in a path,"Scenario:

Create Key via S3, and Create Directory through Fs.
 # open key -> /a/b/c
 # CreateDirectory -> /a/b/c
 # CommitKey -> /a/b/c

So, now in Ozone we will have directory and file with name ""c""

When created through Fs interface.
 # create file -> /a/b/c
 # CreateDirectory -> /a/b/c
 # CommitKey -> /a/b/c

So, now in Ozone we will have directory and file with name ""c""

 
 # InitiateMPU /a/b/c
 # Create Part1 /a/b/c
 # Commit Part1 /a/b/c
 # Create Directory /a/b/c
 # Complete MPU /a/b/c

So, now in Ozone, we will have directory and file with name ""c"".  In MPU this is one example scenario.

 

Few proposals/ideas to solve this:
 # Check during commit whether a directory already exists with same name. But disadvantage is after user uploads the entire data during last stage we fail.  (File system with create in progress acts similarly. Scenario: 1. vi t1 2. mkdir t1 3. Save t1: (Fail:""t1"" is a directory)
 # During create directory check are there any open key creation with same name and fail.

 

Any of the above approaches are not final, this Jira is opened to discuss this issue and come up with solution.",pull-request-available,[],HDDS,Bug,Major,2020-08-26 21:50:14,13
13324671,Increase default timeout in kubernetes tests,"Kubernetes tests are timing out sometimes. (eg. here: https://github.com/elek/ozone-build-results/tree/master/2020/08/26/2562/kubernetes)

Based on the log, SCM couldn't move out from safe mode. It's either a real issue or github environment is slow sometimes.

To make it clear what is the problem I propose to increase the default timeout from 90 sec to 300 sec (5 min).",pull-request-available,[],HDDS,Improvement,Major,2020-08-26 15:49:06,6
13324646,Archive container logs for kubernetes check,"_kubernetes_ check archives only Robot results.  It should also include logs from all pods, similar to compose-based acceptance tests.",pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-08-26 12:23:55,1
13324644,recon.api.TestEndpoints is flaky,"
Failed on the PR:
https://github.com/apache/hadoop-ozone/pull/1349

And on the master:

https://github.com/elek/ozone-build-results/blob/master/2020/08/25/2533/unit/hadoop-ozone/recon/org.apache.hadoop.ozone.recon.api.TestEndpoints.txt

and here:

https://github.com/elek/ozone-build-results/blob/master/2020/08/22/2499/unit/hadoop-ozone/recon/org.apache.hadoop.ozone.recon.api.TestEndpoints.txt",pull-request-available,[],HDDS,Bug,Blocker,2020-08-26 11:55:03,37
13324618,Implement OzoneFileStatus#toString,{{OzoneFileStatus}} should implement {{toString}} for debug purposes.,pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Minor,2020-08-26 09:33:32,1
13324571,Add OFS to FileSystem META-INF,"So that {{fs.ofs.impl}} won't have to be explicitly set in core-site.xml.

Derived from HDDS-3805.",pull-request-available,[],HDDS,Improvement,Major,2020-08-25 23:11:21,12
13324467,Bump version to 1.1.0-SNAPSHOT on master,s/0.6.0-SNAPSHOT/1.1.0-SNAPSHOT/g,pull-request-available,[],HDDS,Improvement,Major,2020-08-25 12:43:33,6
13324327,Implement a factory for OM Requests that returns an instance based on layout version.,"* Add the current layout version (MLV) to the OM Ratis request. If there is no layout version   present, we can default to '0'.
* Implement Generic factory which stores different instances of Type 'T' sharded by a key & version. A single key can be associated with different versions of 'T'. This is to support a typical use case during upgrade to have multiple versions of a class / method / object and chose them based on current layout version at runtime. Before finalizing, an older version is typically needed, and after finalize, a newer version is needed.
* Using the generic factory, we scan all the different OM ""write"" requests and associate them with versions.
* Layout feature code refactoring. Added more comments and tests.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-08-24 18:44:22,30
13324325,Expose upgrade related state through JMX,Expose the metadata layout version and software layout version through JMX.,pull-request-available,[],HDDS,Sub-task,Major,2020-08-24 18:38:10,22
13324323,Implement Finalize command in Ozone Manager client.,"* On the client side, add a new command to finalize OM through CLI.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-08-24 18:34:21,63
13324306,Auto-close /pending pull requests after 21 days of inactivity,"Earlier we introduced a way to mark the inactive pull requests with ""pending"" label (with the help of /pending comment).

This pull requests introduce a new scheduled build which closes the ""pending"" pull requests after 21 days of inactivity.

IMPORTANT: Only the pull requests  which are pending on the author will be closed.

We should NEVER close a pull requests which are waiting for the attention of a committer.",pull-request-available,['build'],HDDS,Improvement,Major,2020-08-24 16:26:31,6
13324290,Update version number in upgrade tests,"Ozone 0.6.0 release is renamed to Ozone 1.0.0, but there are a few leftover references to 0.6.0, mostly in {{upgrade}} acceptance test.",pull-request-available,['test'],HDDS,Task,Minor,2020-08-24 14:53:26,1
13324257,Improve crc efficiency by using Java.util.zip.CRC when available,"HADOOP has implemented several method to calculate crc: https://issues.apache.org/jira/browse/HADOOP-15033
We should choose the method with high efficiency.

This flame graph is from [~elek]
 !screenshot-1.png! ",pull-request-available,[],HDDS,Task,Major,2020-08-24 11:52:50,11
13324194,"In ContainerStateManagerV2, modification of RocksDB should be consistent with that of memory state."," 

Fix a bug in https://issues.apache.org/jira/browse/HDDS-3895 

In ContainerStateManagerV2, both disk state (column families in RocksDB) and memory state (container maps in memory) are protected by raft, and should keep their consistency upon each modification.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-08-24 06:51:01,36
13323738,Use new ContainerManager in SCM,Implement the pending methods in {{ContainerManager}} and integrate {{ContainerManager}} to {{StorageContainerManager}}.,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-08-20 12:52:52,19
13323585,Container report should update container key count and bytes used if they differ in SCM,"In HDDS-4037 it was noted that when blocks are deleted from closed containers, the bytesUsed and Key Count metrics on the SCM container are not updated correctly.

These stats should be updated via the container reports issued by the DNs to SCM periodically. However, in `AbstractContainerReportHandler#updateContainerStats`, the code assumes the values are always increasing and it will not update them if they are decreasing:

{code}
  private void updateContainerStats(final ContainerID containerId,
                                    final ContainerReplicaProto replicaProto)
      throws ContainerNotFoundException {
    if (isHealthy(replicaProto::getState)) {
      final ContainerInfo containerInfo = containerManager
          .getContainer(containerId);

      if (containerInfo.getSequenceId() <
          replicaProto.getBlockCommitSequenceId()) {
        containerInfo.updateSequenceId(
            replicaProto.getBlockCommitSequenceId());
      }
      if (containerInfo.getUsedBytes() < replicaProto.getUsed()) {
        containerInfo.setUsedBytes(replicaProto.getUsed());
      }
      if (containerInfo.getNumberOfKeys() < replicaProto.getKeyCount()) {
        containerInfo.setNumberOfKeys(replicaProto.getKeyCount());
      }
    }
  }
{code}

In HDDS-4037 a change was made to the Replication Manager, so it updates the stats. However I don't believe that is the correct place to perform this check, and the issue is caused by the logic shared above.

In this Jira, I have removed the changes to Replication Manager in HDDS-4037 (but retained the other changes), ensuring the problem statistics are only updated via the containers reports if they are different in SCM from what is reported.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-08-19 16:04:13,11
13323522,remove the 1st edition of RatisServer of SCM HA which is copied from OM HA,"The 1st edition of RatisServer of SCM HA is copied from OM HA.

This version is abandoned, since we finally choose the RatisServer that is implemented by InvocationHandler.

This Jira is used to remove the 1st edition RatisServer.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-08-19 09:04:00,36
13323497,Change MAX_QUOTA_IN_BYTES to Long.MAX_VALUE,"Currently MAX_QUOTA_IN_BYTES is 1EB, which is not very exact. Later we need use it to determine whether quota is enabled or not.
We need to change it to long.max_value to be consistent with HDFS.",pull-request-available,[],HDDS,Improvement,Major,2020-08-19 07:07:47,4
13323156,RATIS ONE Pipeline is closed but not removed when a datanode goes stale,"Since the Scheduler in SCMPipelineManager that used to destroyPipeline is removed,
{code:java}
scheduler.schedule(() -> destroyPipeline(pipeline),
    pipelineDestroyTimeoutInMillis, TimeUnit.MILLISECONDS, LOG,
    String.format(""Destroy pipeline failed for pipeline:%s"", pipeline));{code}
meanwhile the PipelineManagerV2Impl::scrubPipeline only handles and remove RATIS THREE pipeline,
{code:java}
public void scrubPipeline(ReplicationType type, ReplicationFactor factor)
    throws IOException {
  checkLeader();
  if (type != ReplicationType.RATIS || factor != ReplicationFactor.THREE) {
    // Only srub pipeline for RATIS THREE pipeline
    return;
  }
{code}
 

RATIS ONE Pipeline is closed but not removed when a datanode goes stale. The solution is let scrubPipeline handle all kinds of pipelines.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-08-18 05:45:52,36
13323056,Components with web interface should depend on hdds-docs,"[~sammichen] reported the problem that the ozone-0.6.0 branch couldn't be compiled after changing the version to 1.0.0.

{code}
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.0.2:unpack (copy-common-html) on project hadoop-hdds-container-service: Unable to find/resolve artifact.: Could not find artifact org.apache.hadoop:hadoop-hdds-docs:jar:1.0.0 in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
[ERROR]
{code}

Web components include the compiled docs (in case of hugo is on the path)

{code}
  <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-dependency-plugin</artifactId>
        <executions>
          <execution>
            <id>copy-common-html</id>
            <phase>prepare-package</phase>
            <goals>
              <goal>unpack</goal>
            </goals>
            <configuration>
              <artifactItems>
                <artifactItem>
                  <groupId>org.apache.hadoop</groupId>
                  <artifactId>hadoop-hdds-server-framework</artifactId>
                  <outputDirectory>${project.build.outputDirectory}
                  </outputDirectory>
                  <includes>webapps/static/**/*.*</includes>
                </artifactItem>
                <artifactItem>
                  <groupId>org.apache.hadoop</groupId>
                  <artifactId>hadoop-hdds-docs</artifactId>
                  <outputDirectory>${project.build.outputDirectory}/webapps/ozoneManager</outputDirectory>
                  <includes>docs/**/*.*</includes>
                </artifactItem>
              </artifactItems>
              <overWriteSnapshots>true</overWriteSnapshots>
            </configuration>
          </execution>
        </executions>
      </plugin>
{code}

But the explicit dependency between the container-service and hdds-docs accidentally missing. With adding a provided dependency, it can be fixed (maven will compile hdds-docs first)",pull-request-available,[],HDDS,Bug,Blocker,2020-08-17 13:53:48,6
13322949,Pipeline is not removed when a datanode goes stale,"When a node goes stale the pipelines in that node have to be closed and removed from {{PipelineManager}}. Currently, the pipeline is only closed and left in {{PipelineManager}}.

 

*Root Cause Analysis* 

Since the Scheduler in SCMPipelineManager that used to destroyPipeline is removed,
{code:java}
scheduler.schedule(() -> destroyPipeline(pipeline),
    pipelineDestroyTimeoutInMillis, TimeUnit.MILLISECONDS, LOG,
    String.format(""Destroy pipeline failed for pipeline:%s"", pipeline));{code}
meanwhile the PipelineManagerV2Impl::scrubPipeline only handles and remove RATIS THREE pipeline,
{code:java}
public void scrubPipeline(ReplicationType type, ReplicationFactor factor)
    throws IOException {
  checkLeader();
  if (type != ReplicationType.RATIS || factor != ReplicationFactor.THREE) {
    // Only srub pipeline for RATIS THREE pipeline
    return;
  }
{code}
 RATIS ONE Pipeline is closed but not removed when a datanode goes stale. The solution is let scrubPipeline handle all kinds of pipelines.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-08-16 18:31:55,36
13322817,Integrate OM Open Key Cleanup Service Into Existing Code,"Implement the `OpenKeyCleanupService` class, and start and stop the service in `KeyManagerImpl`. The following configurations will be added to specify the service's behavior:
 # ozone.open.key.cleanup.service.interval: Frequency the service should run.
 # ozone.open.key.expire.threshold: Time from creation after which an open key is deemed expired.
 # ozone.open.key.cleanup.limit.per.task: Maximum number of keys the service can mark for deletion on each run.

Default values for these configurations will be chosen from HDFS data.

 ",pull-request-available,['OM HA'],HDDS,Sub-task,Minor,2020-08-14 21:25:47,22
13322816,Implement OM Delete Expired Open Key Request and Response,"Create an OM request and response that allows moving open keys from the open key table to the deleted table in OM HA. The request portion of this operation, which updates the open key table cache, will use a bucket lock.

 ",pull-request-available,['OM HA'],HDDS,Sub-task,Major,2020-08-14 21:24:52,22
13322815,Implement OmMetadataMangerImpl#getExpiredOpenKeys,"Implement the getExpiredOpenKeys method in OmMetadataMangerImpl to return keys in the open key table that are older than a configurable time interval. The method will be modified to take a parameter limiting how many keys are returned. This value will be configurable with the existing ozone.open.key.expire.threshold setting, which currently has a default value of 1 day.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-08-14 21:23:38,22
13322745,Improve performance of the BufferPool management of Ozone client,"Teragen reported to be slow with low number of mappers compared to HDFS.

In my test (one pipeline, 3 yarn nodes) 10 g teragen with HDFS was ~3 mins but with Ozone it was 6 mins. It could be fixed with using more mappers, but when I investigated the execution I found a few problems reagrding to the BufferPool management.

 1. IncrementalChunkBuffer is slow and it might not be required as BufferPool itself is incremental
 2. For each write operation the bufferPool.allocateBufferIfNeeded is called which can be a slow operation (positions should be calculated).
 3. There is no explicit support for write(byte) operations

In the flamegraph it's clearly visible that with low number of mappers the client is busy with buffer operations. After the patch the rpc call and the checksum calculation give the majority of the time. ",pull-request-available,[],HDDS,Improvement,Blocker,2020-08-14 12:09:55,6
13322723,Allow running Kubernetes example tests on k3d,"Kubernetes example tests currently only run on local cluster.  With a minor change we can make the tests work on k3d, too.

bq. [k3d|https://k3d.io/] makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes.",pull-request-available,['test'],HDDS,Improvement,Major,2020-08-14 09:39:40,1
13322617, Normalize Keypath for listKeys.,"When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.

When listKeys uses given keyName(not normalized key path) as prefix and Starkey the list-keys will return empty result.

Similar to HDDS-4102, we should normalize startKey and keyPrefix.


",pull-request-available,[],HDDS,Sub-task,Major,2020-08-13 21:58:47,13
13322494,Bump log4j2 version,"There are bunch of bugfixes and improvements since the used 2.10:

https://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3",pull-request-available,[],HDDS,Improvement,Major,2020-08-13 08:47:18,6
13322283,Improve SCM webui page performance,"The current scm page now send two jmx request, get the same result.
One is jmx?qry=Hadoop:service=*,name=*,component=ServerRuntime
The other is jmx?qry=Hadoop:service=StorageContainerManager,name=StorageContainerManagerInfo,component=ServerRuntime

Now, i suppose to remove the second one, using ctrl.overview to reference the result.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-08-12 14:57:41,69
13322214,Tests in TestOzoneFileSystem should use the existing MiniOzoneCluster,"In HDDS-2833, [~adoroszlai] made a change that greatly reduces the run time of the test suite {{TestOzoneFileSystem}} by sharing one {{MiniOzoneCluster}} among the tests.

But 4 new tests have been added since and are not sharing that {{MiniOzoneCluster}}.

I am able to cut down the run time of {{TestOzoneFileSystem}} from 3m18s to 1m2s on my Mac. It would only save more run time on GitHub Workflow.",pull-request-available,['test'],HDDS,Improvement,Trivial,2020-08-12 08:19:05,12
13322194,replace scmID with clusterID for container and volume at Datanode side,"The disk layout per volume is as follows:
{code:java}
../hdds/VERSION
../hdds/<<scmUuid>>/current/<<containerDir>>/<<containerID>>/metadata
../hdds/<<scmUuid>>/current/<<containerDir>>/<<containerID>>/<<dataDir>>{code}
However, after SCM-HA is enabled, a typical SCM group will consists of 3 SCMs, each of the SCMs has its own scmUuid, meanwhile share the same clusterID.

Since federation is not supported yet, only one cluster is supported now, this Jira will change scmID to clusterID for container and volume at Datanode side.

The disk layout after the change will be as follows:
{code:java}
../hdds/VERSION
../hdds/<<clusterID>>/current/<<containerDir>>/<<containerID>>/metadata
../hdds/<<clusterID>>/current/<<containerDir>>/<<containerID>>/<<dataDir>>{code}",backward-incompatible pull-request-available upgrade,['SCM'],HDDS,Sub-task,Major,2020-08-12 06:16:13,36
13322153,Provide a way to get the default value and key of java-based-configuration easily,"- getDefaultValue
- getKeyName",pull-request-available,[],HDDS,New Feature,Minor,2020-08-11 23:30:32,69
13322090,Normalize Keypath for lookupKey,"When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.

Now when user tries to read the file from S3 using the keyName which user has used to create the Key, it will return error KEY_NOT_FOUND

The issue is, lookupKey need to normalize path, when ozone.om.enable.filesystem.paths is enabled. This is common API used by S3/FS. 
",pull-request-available,[],HDDS,Sub-task,Major,2020-08-11 18:02:07,13
13321990,Improve om admin getserviceroles error message,"Steps to reproduce:

# Start sample docker cluster
# Run {{ozone admin om getserviceroles}} with unknown service ID

{code:title=repro}
$ cd hadoop-ozone/dist/target/ozone-*/compose/ozone
$ docker-compose up -d
$ docker-compose exec scm bash
bash-4.2$ ozone admin om getserviceroles --service-id=om
Error: This command works only on OzoneManager HA cluster. Service ID specified does not match with ozone.om.service.ids defined in the configuration. Configured ozone.om.service.ids are[]bash-4.2$
{code}

* The message should include a space before {{[]}}, and a newline at the end (prompt should appear in next line).
* Wording of the message could also be improved.",newbie pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2020-08-11 11:30:32,73
13321868,S3/Ozone Filesystem inter-op,"This Jira is to implement changes required to use Ozone buckets when data is ingested via S3 and use the bucket/volume via OzoneFileSystem. Initial implementation for this is done as part of HDDS-3955. There are few API's which have missed the changes during the implementation of HDDS-3955. 

Attached design document which discusses each API,  and what changes are required.

Excel sheet has information about each API, from what all interfaces the OM API is used, and what changes are required for the API to support inter-operability.

Note: The proposal for delete/rename is still under discussion, not yet finalized. 
",pull-request-available,[],HDDS,New Feature,Major,2020-08-10 19:53:31,13
13321749,update RATIS version from 1.0.0 to 1.1.0-85281b2-SNAPSHOT,"1.1.0-85281b2-SNAPSHOT is the latest version of Ratis master that supports following two features which are needed by SCM-HA

 

Step-down stale leader in case of split-brain

[https://issues.apache.org/jira/projects/RATIS/issues/RATIS-981?filter=allissues]

 

add currentTerm to LeaderInfoProto for supporting SCM-HA.

[https://issues.apache.org/jira/projects/RATIS/issues/RATIS-1001?filter=allissues]

 

 ",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-08-10 08:29:03,36
13321725,Writing delta to Ozone hangs when creating the _delta_log json,"I am testing writing delta, OSS not databricks, data to Ozone FS since my company is looking to replace Hadoop if feasible. However, whenever I write delta table, the parquet files are writing, the delta log directory is created, but the json is never writing. 

I am using the spark operator to submit a batch test job to write about 5mb of data.

Neither on the driver nor on the executor is there an error. The driver never finishes since the creation of the json hangs.

 

Code I used for testing spark operator and then I ran the pieces in the shell for testing. In the save path, update bucket and volume info for your data store.
{code:java}
package app.OzoneTest

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{BinaryType, StringType}

object CreateData {

  def main(args: Array[String]): Unit = {

    val spark: SparkSession = SparkSession
      .builder()
      .appName(s""Create Ozone Mock Data"")
      .enableHiveSupport()
      .getOrCreate()

    import spark.implicits._

    val df: DataFrame = Seq.fill(100000)
    {(randomID, randomLat, randomLong, randomDates, randomHour)}
      .toDF(""msisdn"", ""latitude"", ""longitude"", ""par_day"", ""par_hour"")
      .withColumn(""msisdn"", $""msisdn"".cast(StringType))
      .withColumn(""msisdn"", sha1($""msisdn"".cast(BinaryType)))
      .select(""msisdn"", ""latitude"", ""longitude"", ""par_day"", ""par_hour"")

    df
      .repartition(3, $""msisdn"")
      .sortWithinPartitions(""latitude"", ""longitude"")
      .write
      .partitionBy(""par_day"", ""par_hour"")
      .format(""delta"")
      .save(""o3fs://your_bucker.your_volume/location_data"")

  }

  def randomID: Int = scala.util.Random.nextInt(10) + 1

  def randomDates: Int = 20200101 + scala.util.Random.nextInt((20200131 - 20200101) + 1)

  def randomHour: Int = scala.util.Random.nextInt(24)

  def randomLat: Double = 13.5 + scala.util.Random.nextFloat()

  def randomLong: Double = 100 + scala.util.Random.nextFloat()
}
{code}",delta filesystem scala spark,['Ozone Filesystem'],HDDS,Bug,Major,2020-08-10 04:28:42,6
13321567,Adding Owner info for Authorizer plugin to honor owner access rights,Currently external authorizer does not have the owner info of the volume bucket keys when authorizing requests. Explicit rules/policies must be set before volume/bucket/key creation is allowed even for owner themselves. ,pull-request-available,[],HDDS,Improvement,Major,2020-08-07 17:26:41,28
13321535,Create ZH translation of Ofs/O3fs.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,translation-zh,[],HDDS,Bug,Major,2020-08-07 14:14:12,64
13321534,Create ZH translation of Cli.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,translation-zh,[],HDDS,Bug,Major,2020-08-07 14:13:31,64
13321533,Create ZH translation of CSI.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,translation-zh,[],HDDS,Bug,Major,2020-08-07 14:13:11,64
13321532,Create ZH translation of Topology.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,translation-zh,[],HDDS,Bug,Major,2020-08-07 14:12:23,64
13321531,Create ZH translation of Recon.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,pull-request-available translation-zh,[],HDDS,Bug,Major,2020-08-07 14:12:00,17
13321530,Create ZH translation of HA.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,pull-request-available translation-zh,[],HDDS,Bug,Major,2020-08-07 14:11:30,64
13321529,Create ZH translation of StorageContainerManager.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,pull-request-available translation-zh,[],HDDS,Bug,Major,2020-08-07 14:10:49,64
13321528,Create ZH translation of OzoneManager.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,pull-request-available translation-zh,[],HDDS,Bug,Major,2020-08-07 14:08:44,64
13321527,Create ZH translation of Containers.md in doc,HDDS-4042 modified the documentation page. zh translation should be updated for changed / new files.,pull-request-available translation-zh,[],HDDS,Bug,Major,2020-08-07 14:08:31,64
13321518,Incomplete OzoneFileSystem statistics,OzoneFileSystem does not record some of the operations that are defined in [Statistic|https://github.com/apache/hadoop-ozone/blob/d7ea4966656cfdb0b53a368eac52d71adb717104/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/Statistic.java#L44-L75].,pull-request-available,['Ozone Filesystem'],HDDS,Bug,Minor,2020-08-07 13:40:29,1
13321378,Retry request on different OM on AccessControlException,"If a client attempts a request on an OM which has not caught up with leader OM and hence does have the delegation token, the request could fail with AccessControlException without trying it on other OMs.
On AccessControlException, all OMs must be tried once before the request is failed.",pull-request-available,['OM HA'],HDDS,Bug,Major,2020-08-06 20:16:09,43
13321348,[OFS] Implement AbstractFileSystem for RootedOzoneFileSystem,"Extracted from HDDS-3805: introduce an implementation of {{AbstractFileSystem}}, similar to {{OzFs}}, for {{RootedOzoneFileSystem}}.",pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Major,2020-08-06 17:41:34,12
13321332,Remove leftover robot.robot,"An unused Robot test ({{robot.robot}}) was accidentally added in HDDS-3612.  It was refactored to separate {{string_tests.robot}} and {{fs_tests.robot}}, but the original file was not removed.",pull-request-available,['test'],HDDS,Task,Trivial,2020-08-06 16:13:18,1
13321197,Client should not retry same OM on network connection failure,"Right now retry logic on client to OM is, it will try connect to OM1, if it is leader fine, else try with next OM and so on. If OM1, is down, client retries for 50 times when ipc.client.connect.max.retries is set to 50 and ipc.client.connect.retry.interval default to 1sec, so a total of 50seconds is spent in retry and then move to next OM. 
I think here client -> OM should have its own retry policy, in this way if the first OM is down, to complete request, the user does not need to wait for 50sec.

As ipc.client.connect.retry.interval and ipc.client.connect.max.retries  are common configurations for RPC, creating a new default retry policy with smaller values would be nice. 



{code:java}
20/08/06 00:21:29 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:30 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:31 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:32 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:33 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
20/08/06 00:21:34 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
{code}
",pull-request-available,"['OM HA', 'Ozone Client']",HDDS,Bug,Major,2020-08-06 00:25:49,43
13321193,Implement toString for OMTransactionInfo,"During debug
We see the logs

{code:java}
23:30:36,175 INFO org.apache.hadoop.ozone.om.OzoneManager: Installing checkpoint with OMTransactionInfo org.apache.hadoop.ozone.om.ratis.OMTransactionInfo@e19e
{code}

It would be helpful to print actual transaction info. For this toString need to be implemented for OMTransactionInfo.
",newbie pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-08-05 23:50:08,54
13320980,Fix InstallSnapshot in OM HA,OzoneManagerStateMachine#notifyInstallSnapshotFromLeader() checks the incoming roleInfoProto and proceeds with install snapshot request only if the role is Leader. This check is wrong and the roleInfoProto will contain the self node ID and not the leaders.,pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-08-05 00:26:44,43
13320904,Non rack aware pipelines should not be created if multiple racks are alive,"If we have a scenario where one rack has more nodes that others, it is possible for all hosts in the cluster to have reached their pipeline limit, while 3 nodes on the larger rack have not. 

The current fallback logic will then allow a pipeline to be created which uses only the 3 nodes on the same rack, violating the rack placement policy.

There may be other ways this could happen with cluster load too, were the pipeline capacity has reached its limit on some nodes but not others.

The proposal here, is that if the cluster has multiple racks AND there are healthy nodes covering at least 2 racks, where healthy is defined as a node which is registered and not stale or dead, then we should not allow ""fallback"" (pipelines which span only 1 rack) pipelines to be created.

This means if you have a badly configured cluster - eg Rack 1 = 10 nodes; Rack 2 = 1 node, the pipeline limit will be constrained by the capacity of that 1 node on rack 2. Even a setup like Rack 1 = 10 nodes, Rack 2 = 5 would be constrained by this.

This constraint is better than creating non rack aware pipelines, and the rule above will handle the case when the cluster degrades to 1 rack, as the healthy node definition will notice only 1 rack is alive.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-08-04 16:43:00,11
13320872,Pending delete blocks are not always included in #BLOCKCOUNT metadata,"Within the code that handles containers, there is currently some disagreement as to whether blocks marked for deletion with the #deleting# prefix should be included in the #BLOCKCOUNT metadata value. The BlockDeletingService includes pending delete blocks in the block count, since after deleting the blocks, BlockDeletingService#call calls KeyValueContainerData#updateAndCommitDBCounters, where the pending deletion blocks (now deleted) are subtracted from the block count metadata. However, KeyValueContainerUtil#initializeUsedBytesAndBlockCount filters away all blocks that have prefixes when setting the block count. This means pending delete blocks with the #deleting# prefix are excluded from the block count.

This fix will alter KeyValueContainerUtil#initializeUsedBytesAndBlockCount to include #deleting# blocks when setting the #BLOCKCOUNT and #BYTESUSED metadata values. It will also adjust the TestContainerReader unit tests to account for this change.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-08-04 13:59:16,22
13320773,SCMStateMachine::applyTransaction() should not invoke TransactionContext.getClientRequest(),"applyTransaction should not call trx.getClientRequest(), since client request will not be replicated from leader to follower, follower will not be able to update its state machine.
{code:java}
SCMStateMachine
  applyTransaction()
    final SCMRatisRequest request = SCMRatisRequest.decode(
        trx.getClientRequest().getMessage());{code}
 

Instead, we should call trx.getStateMachineLogEntry()
{code:java}
SCMStateMachine
  applyTransaction()
    final SCMRatisRequest request = SCMRatisRequest.decode(
        Message.valueOf(trx.getStateMachineLogEntry().getLogData()));{code}
 

content of client request will be injected to StateMachineEntryProto at leader,
{code:java}
static StateMachineLogEntryProto toStateMachineLogEntryProto(
    RaftClientRequest request, ByteString logData, ByteString stateMachineData) {
  if (logData == null) {
    logData = request.getMessage().getContent();
  }
  return toStateMachineLogEntryProto(request.getClientId(), request.getCallId(), logData, stateMachineData);
}
{code}
 

and extracted from log entry at follower.
{code:java}
/**
 * Construct a {@link TransactionContext} from a {@link LogEntryProto}.
 * Used by followers for applying committed entries to the state machine.
 * @param logEntry the log entry to be applied
 */
public TransactionContextImpl(RaftPeerRole serverRole, StateMachine stateMachine, LogEntryProto logEntry) {
  this(serverRole, stateMachine);
  this.logEntry = logEntry;
  this.smLogEntryProto = logEntry.getStateMachineLogEntry();
}
{code}
 ",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-08-04 03:31:43,36
13320596,Wrong use of AtomicBoolean in HddsDatanodeService,"* {{AtomicBoolean isStopped}} should be {{final}}, not {{volatile}}, since the reference is not being changed
* {{stop()}} should use atomic {{getAndSet()}} instead of {{get()}} followed by {{set()}}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-08-03 05:59:25,1
13320552,Failed acceptance test missing from bundle,"Acceptance test run is stopped after any failed test, and its logs are missing from the bundle.

{code:title=misc suite}
hadoop27
hadoop31
hadoop32
ozone-csi
ozone-mr
ozone-om-ha-s3
ozone-topology
ozones3-haproxy
ozonesecure-mr
ozonesecure-om-ha
upgrade
{code}

* {{ozone-topology}} failed in [this run|https://github.com/apache/hadoop-ozone/runs/927545620], {{ozones3-haproxy}} and subsequent tests were skipped, {{ozone-topology}} robot log is missing from [bundle|https://github.com/apache/hadoop-ozone/suites/984510888/artifacts/12629978]
* {{ozone-secure}} failed in [this run|https://github.com/adoroszlai/hadoop-ozone/runs/933098576], it is the only test in {{secure}} suite, so {{acceptance-secure.zip}} was not even created",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2020-08-02 16:57:19,1
13320400,Cleanup GitHub workflow,"Cleanup GitHub workflow definition:

# Provide name for all steps
# Apply HDDS-4038 to new {{bats}} and {{kubernetes}} checks
# Apply HDDS-3877 to new {{bats}} check
# Fix mixed use of 2, 3, 4 spaces indentation
# Remove unnecessary job names (where job name and ID are the same)",pull-request-available,['build'],HDDS,Improvement,Major,2020-07-31 12:41:03,1
13320344,Remove duplicate judgments in allocateBlockInKey,"In HDDS-1666 we have [changed the size of the key size|https://github.com/apache/hadoop/pull/943/files#diff-2d061b57a9838854d07da9e0eca64f31R439]. So this size must be >0.
{code:java}
final long size = args.getDataSize() > 0 ?
 args.getDataSize() : scmBlockSize;
{code}
We [pass this size when we call allocateBlockInKey|https://github.com/apache/hadoop/pull/943/files#diff-2d061b57a9838854d07da9e0eca64f31R480]. The key must be greater than 0, so the ""if (size > 0){}"" in allocateBlockInKey is redundant and should be removed.",pull-request-available,[],HDDS,Improvement,Major,2020-07-31 08:21:47,4
13320213,OzoneManager met NPE exception while getServiceList," !image-2020-07-30-22-46-53-040.png! 

2020-07-30 22:28:03,895 [IPC Server handler 77 on default port 9862] WARN org.apache.hadoop.ipc.Server: IPC Server handler 77 on default port 9862, call Call#61137 Retry#13 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.179.142.158:48848
java.lang.NullPointerException
        at org.apache.hadoop.ozone.om.OzoneManager.getServiceList(OzoneManager.java:2505)
        at org.apache.hadoop.ozone.om.OzoneManager.getServiceInfo(OzoneManager.java:2578)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getServiceList(OzoneManagerRequestHandler.java:451)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:176)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:218)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:145)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)
        at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2020-07-30 14:50:01,69
13319987,Add more ignore rules to the RAT ignore list,"We have separated rat ignore list for Hdds and Ozone projects but some files (which are ignored by .gitignore) not added to there. It's not a problem on github as rat is executed on a clean repo, but locally it can make the execution easier (exclude files which are already ignored).

Rats can read ignore list from VCS ignore file, unfortunately only from directory of the current project not from the root of the git repository.",pull-request-available,[],HDDS,Improvement,Trivial,2020-07-29 14:40:45,6
13319822,Deprecate ozone.s3g.volume.name,"HDDS-3612 introduced bucket links.
After this feature now we don't need this parameter, any volume/bucket can be exposed to S3 via using bucket links.

ozone bucket link srcvol/srcbucket destvol/destbucket

So now to expose any ozone bucket to S3G

For example, the user wants to expose a bucket named bucket1 under volume1 to S3G, they can run below command

{code:java}
ozone bucket link volume1/bucket1 s3v/bucket2
{code}

Now, the user can access all the keys in volume/bucket1 using s3v/bucket2 and also ingest data to the volume/bucket1 using using s3v/bucket2

This Jira is opened to remove the config from ozone-default.xml
And also log a warning message to use bucket links, when it does not have default value s3v.
",pull-request-available,['S3'],HDDS,Bug,Blocker,2020-07-28 18:54:07,13
13319781,allow deletion from Trash directory without -skipTrash option,"""skipTrash"" option is mandatory while deleting  from ""Trash"".

Deletion from Trash should be allowed even when skipTrash option is not used.

 

ozone fs -rm -r o3fs://bucket3.s3v.ozone1/.Trash
20/07/28 14:50:46 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
rm: Failed to move to trash: o3fs://bucket3.s3v.ozone1/.Trash: rename from o3fs://bucket3.s3v.ozone1/.Trash to /.Trash/hrt_qa/Current/.Trash failed.. Consider using -skipTrash option

 

ozone fs -rm -r -skipTrash o3fs://bucket3.s3v.ozone1/.Trash
Deleted o3fs://bucket3.s3v.ozone1/.Trash",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Minor,2020-07-28 14:55:50,39
13319704,Update documentation for the GA release,"HDDS-3413 is opened to add OM HA related documentation to the Ozone docs but it turned out that it contains additional out-of-date (and missing) information.

This issue is opened to track a big documentation update.",pull-request-available,['documentation'],HDDS,Task,Blocker,2020-07-28 08:33:37,6
13319693,Ozone /conf endpoint triggers kerberos replay error when SPNEGO is enabled ,"{code}
curl  -k --negotiate -X GET -u : ""https://quasar-jsajkc-8.quasar-jsajkc.root.hwx.site:9877/conf""
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 403 GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</title>
</head>
<body><h2>HTTP ERROR 403 GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</h2>
<table>
<tr><th>URI:</th><td>/conf</td></tr>
<tr><th>STATUS:</th><td>403</td></tr>
<tr><th>MESSAGE:</th><td>GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</td></tr>
<tr><th>SERVLET:</th><td>conf</td></tr>
</table>

</body>
</html>
{code}",pull-request-available,[],HDDS,Bug,Major,2020-07-28 07:46:55,28
13319640,[OFS] BasicRootedOzoneFileSystem to support batchDelete,"This Jira is to use deleteObjects in OFS delete now that HDDS-3286 is committed.

Currently when ozone.om.enable.filesystem.paths is enabled it normalizes the path, so using deleteKey for delete directory will fail.

According to [~bharat] this should be a blocker for 0.6.0.
",pull-request-available,[],HDDS,Bug,Blocker,2020-07-27 23:30:59,12
13319609,Reduce the number of fields in hdds.proto to improve performance,"HDDS-3989 introduced revision and buildDate fields to hdds.proto file. These fields are required only for Recon UI and don't have to be part of hdds.proto.

Also, version and setupTime are other two fields which can be removed and added only to the SCM registration type as per [~elek] ([https://github.com/apache/hadoop-ozone/pull/1226|https://github.com/apache/hadoop-ozone/pull/1226#issuecomment-663416483])",pull-request-available,['Ozone Datanode'],HDDS,Task,Major,2020-07-27 19:55:37,74
13319606,Eliminate GitHub check warnings,"GitHub Actions warns that 

bq. The ""master"" branch is no longer the default branch name for actions/upload-artifact

and similarly for actions/checkout.

So we should use specific version instead of master.",pull-request-available,['build'],HDDS,Improvement,Trivial,2020-07-27 19:42:44,1
13319552,Incorrect container numberOfKeys and usedBytes in SCM after key deletetion,"bin/ozone admin container list -s=3 -c=1
{
  ""state"" : ""CLOSED"",
  ""replicationFactor"" : ""THREE"",
  ""replicationType"" : ""RATIS"",
  ""usedBytes"" : 2153526137,
  ""numberOfKeys"" : 57,
  ""lastUsed"" : ""2020-07-27T13:37:16.305Z"",
  ""stateEnterTime"" : ""1970-01-10T08:12:26.645Z"",
  ""owner"" : ""a46123a8-be63-4736-9478-ce4d8ac845cc"",
  ""containerID"" : 4,
  ""deleteTransactionId"" : 799549,
  ""sequenceId"" : 0,
  ""open"" : false
}

`-- 4
|       |-- chunks
|       `-- metadata
|           |-- 4-dn-container.db
|           |   |-- 000052.sst
|           |   |-- 000121.log
|           |   |-- CURRENT
|           |   |-- IDENTITY
|           |   |-- LOCK
|           |   |-- LOG
|           |   |-- LOG.old.1574862506326174
|           |   |-- LOG.old.1575885724469410
|           |   |-- LOG.old.1579433296221409
|           |   |-- LOG.old.1583661885934875
|           |   |-- LOG.old.1583669077602829",pull-request-available,[],HDDS,Bug,Critical,2020-07-27 13:38:07,5
13319524,Update logs of HadoopDirGenerator,"1. Added logs to HadoopNestedDirGenerator.

2. Edit logs of HadoopDirTreeGenerator. 

3. Use BaseFreonGenerator printReport() for shell and command-line execution of freon commands like ddsg, dtsg, etc.",pull-request-available,[],HDDS,Improvement,Minor,2020-07-27 11:05:20,27
13319523,Add Unit Test for HadoopNestedDirGenerator,Unit test - It checks the span and depth of nested directories created by the HadoopNestedDirGenerator Tool.,https://github.com/apache/hadoop-ozone/pull/1266 pull-request-available,[],HDDS,Test,Major,2020-07-27 10:53:40,27
13319511,Make the acceptance test reports hierarchical,"Acceptance test reports of today  uses a generated name for each of the executed robot tests.

Instead of using a flat structure with generated name it seems to be better to use a hierarchical structure which represents the directory structure.",pull-request-available,[],HDDS,Improvement,Major,2020-07-27 09:44:20,6
13319495,Run author check without docker,"{{author.sh}} can be run in CI without docker container, since it's mostly just a recursive grep.",pull-request-available,['build'],HDDS,Improvement,Minor,2020-07-27 08:03:52,1
13319473,Run shell tests in CI,Shell scripts can be tested using [bats|https://github.com/bats-core/bats-core].  Such tests should be run as part of CI.,pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-07-27 06:35:28,1
13319330,Remember the selected columns and make the X-axis scrollable in recon datanodes UI,"Currently, the Datanodes tab can't remember the selected columns, when the page is auto reloaded, the columns will be reset to the default columns.

Meanwhile, when several columns are selected, we can't see some columns in the right side.

This issue will fix those.",pull-request-available,['Ozone Recon'],HDDS,Improvement,Minor,2020-07-26 06:42:08,74
13319221,Recon unable to add a new container which is in CLOSED state.,"{code}
2020-07-24 19:56:11,777 INFO org.apache.hadoop.ozone.recon.scm.ReconContainerManager: Exception while adding container #1 .
java.io.IOException: Pipeline PipelineID=ccfb3a54-848c-4ed2-91bf-a174267e3435 not found. Cannot add container #1
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.addNewContainer(ReconContainerManager.java:119)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.checkAndAddNewContainer(ReconContainerManager.java:92)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:62)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-07-24 19:56:11,777 ERROR org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler: Exception while checking and adding new container.
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=06bf6a83-9afb-4477-b18d-de4c6556ce4b not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:372)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:111)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:350)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.addNewContainer(ReconContainerManager.java:126)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.checkAndAddNewContainer(ReconContainerManager.java:92)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:62)
        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Blocker,2020-07-24 19:56:33,30
13319194,Suppress ERROR message when SCM attempt to create additional pipelines,"This gives false negative errors and can flood SCM log. 


{code:java}
scm_1       | 2020-07-24 16:39:51,756 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-07-24 16:39:51,757 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
{code}
",pull-request-available,[],HDDS,Improvement,Major,2020-07-24 16:43:47,28
13319173,Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true,"Sets ozone.om.enable.filesystem.paths=true, then starts the Ozone cluster.
{code:java}
[root~]$ ozone fs -mkdir o3fs://bucket2.vol2.ozone1/subdir2
[root~]$ ozone fs -mv o3fs://bucket2.vol2.ozone1/subdir2 o3fs://bucket2.vol2.ozone1/subdir2-renamedmv: Key not found /vol2/bucket2/subdir2
{code}
 ",pull-request-available,[],HDDS,Bug,Blocker,2020-07-24 13:24:20,13
13319135,Add test for creating encrypted key,Add acceptance test to create a key in encrypted bucket.,pull-request-available,['Ozone Manager'],HDDS,Test,Major,2020-07-24 09:52:22,1
13319085,Delete closed container after all blocks have been deleted,"One of our use case is customers delete old objects and files regularly.  Once the old files are deleted, there are many containers with no user data.  

The goal of this task is delete all these containers to reduce the metadata footprint of both dn and scm. 


Here is the container state machine involved, 

{noformat}
                   Container key count &size 0                    Container replica count 0
CLOSED       ---------------------------------> DELETING  --------------------------------> DELETED
                   DELETE event                                           CLEANUP event

{noformat}

Currently when Container is in DELETED state, it still exists in SCM memory and DB. 

",pull-request-available,[],HDDS,Improvement,Major,2020-07-24 03:39:53,5
13319052,Ozone s3 API return 400 Bad Request for head-bucket for non existing bucket,"Ozone s3 API returns 400 Bad Request for head-bucket for non-existing bucket.

hrt_qa$ aws s3api  --ca-bundle=/usr/local/share/ca-certificates/ca.crt --endpoint https://s3g:9879/  head-bucket --bucket fsdghj

An error occurred (400) when calling the HeadBucket operation: Bad Request

It should return 404 as per AWS documentation:
https://docs.aws.amazon.com/cli/latest/reference/s3api/head-bucket.html

A client error (404) occurred when calling the HeadBucket operation: Not Found ",pull-request-available,['S3'],HDDS,Bug,Blocker,2020-07-23 22:16:00,13
13319046,Organize Recon DBs into a 'DBDefinition'.,"* ReconNodeManager uses node db in an old format which is not part of ReconDBDefinition. Move the definition to ReconDBDefinition.
* Create DB Definition for Recon Container DB.
* Modify DBScanner tool to allow it to read Recon DBs. ",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-07-23 21:35:06,30
13318998,ACL commands like getacl and setacl should return a response only when Native Authorizer is enabled,"Currently, the getacl and setacl commands return wrong information when an external authorizer such as Ranger is enabled. There should be a check to verify if Native Authorizer is enabled before returning any response for these two commands.

If an external authorizer is enabled, it should show a nice message about managing acls in external authorizer.  ",pull-request-available,"['Ozone CLI', 'Ozone Manager']",HDDS,Task,Major,2020-07-23 16:49:15,13
13318936,Show the storageDir while need init om or scm,"When you accident to use a wrong ozone-site.xml or wrong configure key ozone.metadata.dirs to a new dir, the scm or om cannot start and shown that 'ozone om --init' or 'ozone scm --init' is needed, but you don't know the root cause is you are using a wrong config file or gave a wrong value of ozone.metadata.dirs. 

So we can show the current storageDir to user, so that user can aware the real problem.",pull-request-available,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2020-07-23 12:47:20,69
13318928,Datanode log spammed by NPE,"{code}
datanode_1  | 2020-07-22 13:11:47,845 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 2 seconds.
datanode_1  | 2020-07-22 13:11:47,846 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,847 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,848 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,848 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-07-22 13:11:47,851 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
...
thread in pool for past 22 seconds.
datanode_1  | 2020-07-22 13:11:47,854 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_1  | java.lang.NullPointerException
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)
datanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)
datanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)
datanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)
{code}

This increases acceptance test logs to several hundred MBs.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2020-07-23 12:29:55,1
13318926,Acceptance check may run against wrong commit,"For push builds, acceptance check may build and test a different commit than the one that was pushed.

The check for [HDDS-3991|https://github.com/apache/hadoop-ozone/commit/404ec6d0725cfe9c80aa912f150c6474037b10bb] built [HDDS-3933|https://github.com/apache/hadoop-ozone/commit/ff7b5a3367eccc0969bfd92a2cafe48899a2aaa5]:

{code:title=https://github.com/apache/hadoop-ozone/runs/898449998#step:4:30}
HEAD is now at ff7b5a336 HDDS-3933. Fix memory leak because of too many Datanode State Machine Thread (#1185)
{code}",pull-request-available,['build'],HDDS,Bug,Major,2020-07-23 12:26:42,1
13318841,AbortMultipartUpload's audit should be changed to ABORT_MULTIPART_UPLOAD,"Currently, the audit log of AbortMultipartUpload is COMPLETE_MULTIPART_UPLOAD, which cannot be expressed clearly. We need to change it to ABORT_MULTIPART_UPLOAD.",pull-request-available,[],HDDS,Improvement,Major,2020-07-23 06:04:05,4
13318805,FLAKY-UT: TestWatchForCommit#testWatchForCommitForGroupMismatchException,"[INFO] Running org.apache.hadoop.ozone.client.rpc.TestWatchForCommit
[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 211.911 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestWatchForCommit
[ERROR] testWatchForCommitForGroupMismatchException(org.apache.hadoop.ozone.client.rpc.TestWatchForCommit)  Time elapsed: 38.862 s  <<< ERROR!
java.io.IOException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:740)
	at org.apache.hadoop.ozone.container.TestHelper.waitForPipelineClose(TestHelper.java:220)
	at org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForGroupMismatchException(TestWatchForCommit.java:344)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.ratis.protocol.GroupMismatchException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.
	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:414)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:372)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:355)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:738)
	... 29 more",pull-request-available,['test'],HDDS,Sub-task,Major,2020-07-23 01:53:13,1
13318795,Update S3 related documentation,"HDDS-3993 created volume required for S3G during the OM startup.
So, remove the step that s3v volume needs to be created.",pull-request-available,[],HDDS,Bug,Major,2020-07-22 23:34:57,13
13318794,S3G startup fails when multiple service ids are configured.,"This Jira is to fix this TODO.

OzoneServiceProvider.java L59:
{code:java}
      // HA cluster.
      //For now if multiple service id's are configured we throw exception.
      // As if multiple service id's are configured, S3Gateway will not be
      // knowing which one to talk to. In future, if OM federation is supported
      // we can resolve this by having another property like
      // ozone.om.internal.service.id.
      // TODO: Revisit this later.
      if (serviceIdList.size() > 1) {
        throw new IllegalArgumentException(""Multiple serviceIds are "" +
            ""configured. "" + Arrays.toString(serviceIdList.toArray()));
{code}

      ",pull-request-available,[],HDDS,Bug,Major,2020-07-22 23:26:30,13
13318790,"Recon Overview page: The volume, bucket and key counts are not accurate","The counts shown in the overview page are not accurate due to the usage of ""rocksdb.estimate-num-keys"" to get the counts. Instead, keep track of accurate counts by updating the counter in a global table every time an event is triggered via FileSizeCount Task in Recon.  ",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-07-22 23:03:45,37
13318770,Recon should fallback to ozone.om.service.ids when the internal service id is not defined.,"Recon connects to OM via RPC using the ""ozone.om.internal.service.id"" to get updates. If the above config is not defined, but the ozone.om.service.ids is defined, Recon should use the latter as a fallback. Currently, a single Recon instance supports only 1 OM HA cluster at a time. Hence, if multiple ids are defined, Recon will pick the first.

Thanks to [~vivekratnavel] for reporting the issue.",pull-request-available,['Ozone Recon'],HDDS,Bug,Blocker,2020-07-22 19:42:15,30
13318768,Generate encryption info for the bucket outside bucket lock,"This Jira is to generate FileEncryption for a key outside the bucket lock.
As right now, we hold the lock when making a network call to KMS to obtain encryption info.",pull-request-available,"['Ozone Manager', 'Security']",HDDS,Improvement,Major,2020-07-22 19:26:38,13
13318759,Disallow MPU on encrypted buckets.,"With HDDS-3612 buckets created via ozone are also accessible via S3.
This has caused a problem when the bucket is encrypted, the keys are not encrypted on disk.

*2 Issues:*
1. On OM, for each part a new encryption info is generated. During complete Multipart upload, the encryption info is not stored in KeyInfo.
2. On the client, for part upload, the encryption info is silently ignored.

If we don't throw an error, on an encrypted bucket, key data is not encrypted on disks.
For 0.6.0 release, we can mark this as not supported, and this will be fixed in next release by HDDS-4005


",pull-request-available,[],HDDS,Bug,Blocker,2020-07-22 18:22:03,13
13318672,Delete the redundant word of the description,The [description|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/shell/volume/CreateVolumeHandler.java#L40] of ownerName has a redundant word.,newbie pull-request-available,[],HDDS,Improvement,Trivial,2020-07-22 11:01:19,75
13318619,Split acceptance tests to reduce CI feedback time,"CI checks run integration tests in parallel using GitHub Actions matrix build.  I propose to use the same approach for acceptance tests, which are currently run in a single check, taking about 1.5 hours.

Also, some of the integration test splits could be combined to reduce the number of checks, as we have a limit on parallel actions.",pull-request-available,['test'],HDDS,Improvement,Major,2020-07-22 06:07:54,1
13318617,OM NPE and shutdown while S3MultipartUploadCommitPartResponse#checkAndUpdateDB,"The related bad code.

 !screenshot-1.png! 


The following is the part log of OM. If you want to see the full log, you can download the attachment.

2020-07-20 16:28:56,395 ERROR ratis.OzoneManagerDoubleBuffer (ExitUtils.java:terminate(133)) - Terminating with exit status 2: OMDoubleBuffer flush threadOMDoubleBufferFlushThreadencountered Throwable error
java.lang.NullPointerException
	at org.apache.hadoop.ozone.om.response.s3.multipart.S3MultipartUploadCommitPartResponse.checkAndUpdateDB(S3MultipartUploadCommitPartResponse.java:99)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.lambda$null$0(OzoneManagerDoubleBuffer.java:256)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.addToBatchWithTrace(OzoneManagerDoubleBuffer.java:201)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.lambda$flushTransactions$1(OzoneManagerDoubleBuffer.java:254)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:250)
	at java.lang.Thread.run(Thread.java:748)
2020-07-20 16:28:56,426 INFO  om.OzoneManagerStarter (StringUtils.java:lambda$startupShutdownMessage$0(124)) - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down OzoneManager at BAOLOONGMAO-MB0/10.78.32.49
************************************************************/


",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-07-22 06:01:50,69
13318602,Shorten Ozone FS Hadoop compatibility module names,"The name of {{hadoop-ozone-filesystem-hadoopX}} modules is so long that Maven output looks ugly:

{code}
...
[INFO] Apache Hadoop Ozone Insight Tool ................... SUCCESS [  3.940 s]
[INFO] Apache Hadoop Ozone FileSystem Shaded .............. SUCCESS [02:28 min]
[INFO] Apache Hadoop Ozone FileSystem Hadoop 2.x compatibility SUCCESS [ 14.397 s]
[INFO] Apache Hadoop Ozone FileSystem Hadoop 3.x compatibility SUCCESS [ 13.095 s]
[INFO] Apache Hadoop Ozone Distribution ................... SUCCESS [ 12.380 s]
[INFO] Apache Hadoop Ozone Fault Injection Tests .......... SUCCESS [  0.725 s]
{code}",pull-request-available,['build'],HDDS,Improvement,Trivial,2020-07-22 04:17:39,1
13318585,Ozone certificate needs additional flags and SAN extension for GRPC TLS.,"Current Ozone certificate are good for sign/verify tokens but can't do SSL handshake. 

Here are a few missing pieces: 
1. Caused by: java.security.cert.CertificateException: No subject alternative names present
        at java.base/sun.security.util.HostnameChecker.matchIP(HostnameChecker.java:137)

2. Caused by: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
        at java.base/sun.security.validator.EndEntityChecker.checkTLSServer(EndEntityChecker.java:278)
",pull-request-available,[],HDDS,Improvement,Major,2020-07-22 00:17:51,28
13318582,Missing TLS client configurations to allow ozone.grpc.tls.enabled.,"As a result, when ozone.grpc.tls.enabled, RATIS THREE pipeline will not work as DN failed in SSL handshaking without the TLS configuration. 




",pull-request-available,[],HDDS,Improvement,Major,2020-07-22 00:14:19,28
13318379,s3g met NPE exception while write file by multiPartUpload,"*strong text*The following is the exception

 

2020-07-20 17:26:17,692 [java.util.concurrent.ThreadPoolExecutor$Worker@2b70f372[State = -1, empty queue]] ERROR org.apache.hadoop.hdds.scm.storage.BlockOutputStream: writing chunk failed 104545451305649559_chunk_1 blockID conID: 284442 locID: 104545451305649559 bcsId: 0 with exception org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-96F1E0530C66->RAFT is closed.
2020-07-20 17:26:17,700 [java.util.concurrent.ThreadPoolExecutor$Worker@2b70f372[State = -1, empty queue]] ERROR org.apache.hadoop.hdds.scm.storage.BlockOutputStream: writing chunk failed 104545451305649559_chunk_2 blockID conID: 284442 locID: 104545451305649559 bcsId: 0 with exception org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-96F1E0530C66->RAFT is closed.
2020-07-20 17:26:17,700 [java.util.concurrent.ThreadPoolExecutor$Worker@2b70f372[State = -1, empty queue]] ERROR org.apache.hadoop.hdds.scm.storage.BlockOutputStream: writing chunk failed 104545451305649559_chunk_3 blockID conID: 284442 locID: 104545451305649559 bcsId: 0 with exception org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-96F1E0530C66->RAFT is closed.
2020-07-20 17:26:17,726 [qtp2131952342-276] WARN org.apache.hadoop.ozone.client.io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-96F1E0530C66->RAFT is closed. on the pipeline Pipeline[ Id: 58e4eef0-d2b1-4246-a152-83a28012848f, Nodes: 38103930-07b9-4434-a657-621276c65683\{ip: 9.180.20.47, host: host-9-180-20-47, networkLocation: /rack1, certSerialId: null}207b98d9-ad64-45a8-940f-504b514feff5\{ip: 9.180.21.88, host: host-9-180-21-88, networkLocation: /rack2, certSerialId: null}d3336357-8920-4a4e-a12f-e57da1640c4d\{ip: 9.180.20.94, host: host-9-180-20-94, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:207b98d9-ad64-45a8-940f-504b514feff5, CreationTimestamp2020-07-07T15:11:10.887Z]. The last committed block length is 0, uncommitted data length is 10485760 retry count 0
2020-07-20 17:26:17,726 [qtp2131952342-276] INFO org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool: Allocating block with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = [PipelineID=58e4eef0-d2b1-4246-a152-83a28012848f]}
2020-07-20 17:26:18,238 [qtp2131952342-276] WARN org.eclipse.jetty.server.HttpChannel: /bucketmbl/114.dat
javax.servlet.ServletException: javax.servlet.ServletException: java.lang.NullPointerException
 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
 at org.eclipse.jetty.server.Server.handle(Server.java:500)
 at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
 at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
 at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
 at java.lang.Thread.run(Thread.java:748)
Caused by: javax.servlet.ServletException: java.lang.NullPointerException
 at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)
 at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
 at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)

at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
 at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1645)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
 at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
 ... 17 more
Caused by: java.lang.NullPointerException
 at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.createMultipartKey(ObjectEndpoint.java:565)
 at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:136)
 at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
 at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
 at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
 at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
 at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
 at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
 at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
 at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
 at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
 at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
 at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
 at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
 at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
 at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
 at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
 ... 45 more


After switch the log level to TRACE, i see the following error


2020-07-21 15:31:03,212 [qtp1150963491-534] ERROR exception.S3ErrorTable: <?xml version=""1.0"" encoding=""UTF-8""?>
<Error>
  <Code>NoSuchUpload</Code>
  <Message>The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed.</Message>
  <Resource>828e5250-d3e5-42dd-a273-e2c687293e2d-104550574711057985</Resource>
  <RequestId/>
</Error>
",pull-request-available s3g,['S3'],HDDS,Bug,Major,2020-07-21 02:40:48,69
13318378,Write object when met exception can be slower than before,"After HDDS-3350 , the retry policy changed, and the client write performance getting lower than before.

 

With HDDS-3350, I restore the method RatisHelper#createRetryPolicy to the previous commit, it works well.

 

The previous is 

 
{code:java}
static RetryPolicy createRetryPolicy(ConfigurationSource conf) {
    int maxRetryCount =
        conf.getInt(OzoneConfigKeys.DFS_RATIS_CLIENT_REQUEST_MAX_RETRIES_KEY,
            OzoneConfigKeys.
                DFS_RATIS_CLIENT_REQUEST_MAX_RETRIES_DEFAULT);
    long retryInterval = conf.getTimeDuration(OzoneConfigKeys.
        DFS_RATIS_CLIENT_REQUEST_RETRY_INTERVAL_KEY, OzoneConfigKeys.
        DFS_RATIS_CLIENT_REQUEST_RETRY_INTERVAL_DEFAULT
        .toIntExact(TimeUnit.MILLISECONDS), TimeUnit.MILLISECONDS);
    TimeDuration sleepDuration =
        TimeDuration.valueOf(retryInterval, TimeUnit.MILLISECONDS);
    RetryPolicy retryPolicy = RetryPolicies
        .retryUpToMaximumCountWithFixedSleep(maxRetryCount, sleepDuration);
    return retryPolicy;
  }

{code}

When I switch logLevel to TRACE level, i see the following log While using HDDS-3350


2020-07-21 12:56:11,822 [grpc-default-executor-5] TRACE impl.OrderedAsync: client-6F623ADF656D: Failed* RaftClientRequest:client-6F623ADF656D->207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F, cid=2876, seq=1*, Watch(0), null
java.util.concurrent.CompletionException: org.apache.ratis.protocol.LeaderNotReadyException: 207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F is in LEADER state but not ready yet.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.completeReplyExceptionally(GrpcClientProtocolClient.java:358)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.access$000(GrpcClientProtocolClient.java:264)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:283)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:269)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:436)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInternal(ClientCallImpl.java:658)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:643)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.protocol.LeaderNotReadyException: 207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F is in LEADER state but not ready yet.
        at org.apache.ratis.client.impl.ClientProtoUtils.toRaftClientReply(ClientProtoUtils.java:281)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:274)
        ... 9 more
2020-07-21 12:56:11,822 [grpc-default-executor-5] DEBUG impl.OrderedAsync: schedule* attempt #1 with sleep 608ms and policy RequestTypeDependentRetryPolicy{WRITE->org.apache.ratis.retry.ExceptionDependentRetry@5685c509, WATCH->org.apache.ratis.retry.ExceptionDependentRetry@5572f5cd} for RaftClientRequest:client-6F623ADF656D->207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F, cid=2876, seq=1*, Watch(0), null


2020-07-21 12:56:18,273 [java.util.concurrent.ThreadPoolExecutor$Worker@157a0c99[State = -1, empty queue]] DEBUG impl.OrderedAsync: schedule* attempt #35 with sleep 4606ms and policy RequestTypeDependentRetryPolicy{WRITE->org.apache.ratis.retry.ExceptionDependentRetry@19a0a59, WATCH->org.apache.ratis.retry.ExceptionDependentRetry@41afac0b} for RaftClientRequest:client-F768C93F1755->207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F, cid=2034, seq=1*, Watch(0), null
2020-07-21 12:56:18,820 [java.util.concurrent.ThreadPoolExecutor$Worker@64953e21[State = -1, empty queue]] DEBUG impl.OrderedAsync: client-6F623ADF656D: send* RaftClientRequest:client-6F623ADF656D->207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F, cid=2876, seq=1*, Watch(0), null
2020-07-21 12:56:18,821 [java.util.concurrent.ThreadPoolExecutor$Worker@64953e21[State = -1, empty queue]] TRACE impl.OrderedAsync: client-6F623ADF656D: Failed* RaftClientRequest:client-6F623ADF656D->207b98d9-ad64-45a8-940f-504b514feff5@group-83A28012848F, cid=2876, seq=1*, Watch(0), null
java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: client-6F623ADF656D->207b98d9-ad64-45a8-940f-504b514feff5 is closed.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:614)
        at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1983)
        at org.apache.ratis.client.impl.OrderedAsync.sendRequest(OrderedAsync.java:234)
        at org.apache.ratis.client.impl.OrderedAsync.sendRequestWithRetry(OrderedAsync.java:187)
        at org.apache.ratis.util.SlidingWindow$Client.sendOrDelayRequest(SlidingWindow.java:278)
        at org.apache.ratis.util.SlidingWindow$Client.retry(SlidingWindow.java:294)
        at org.apache.ratis.client.impl.OrderedAsync.lambda$scheduleWithTimeout$7(OrderedAsync.java:220)
        at org.apache.ratis.util.TimeoutScheduler.lambda$onTimeout$0(TimeoutScheduler.java:141)
        at org.apache.ratis.util.TimeoutScheduler.lambda$onTimeout$1(TimeoutScheduler.java:155)
        at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
        at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.protocol.AlreadyClosedException: client-6F623ADF656D->207b98d9-ad64-45a8-940f-504b514feff5 is closed.
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.onNext(GrpcClientProtocolClient.java:313)
        at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequestAsync(GrpcClientRpc.java:68)
        at org.apache.ratis.client.impl.OrderedAsync.sendRequest(OrderedAsync.java:233)
        ... 15 more",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-07-21 02:35:11,69
13318363,Create volume required for S3G during OM startup,Create volume required for S3G operations during OM startup,pull-request-available,[],HDDS,Bug,Major,2020-07-21 00:10:14,13
13318225,Ignore protobuf lock files,"HDDS-3595 introduced a new build time check to make sure protobuf files are changed only with backward compatible way. A lock file which contains the structure of the proto files are committed to the repository and during the build the new state of the proto files are compared with the committed version.

Unfortunately the plugin always generate a new lock file, even if it should be updated only after the releases. To make it more safe (and less confusing) I suggest putting the lock files to the git ignore list",pull-request-available,[],HDDS,Improvement,Major,2020-07-20 10:36:44,6
13318220,Test Kubernetes examples with acceptance tests,"hadoop-ozone/dist/src/main/k8s/example directory contains example kubernetes resources to start Ozone in kubernetes environment. To make sure those resources are working and up-to-date I propose to test them during standard build.

K3s project provides a lightweight Kubernetes distribution which can be installed easily in Github Actions environment and kubernetes based clusters can be tested.",pull-request-available,['kubernetes'],HDDS,Improvement,Major,2020-07-20 10:20:40,6
13318210,Display revision and build date of DN in recon UI,"Display the revision and build date in the recon UI.

So we can get more detailed version info in UI.",pull-request-available,['Ozone Recon'],HDDS,Improvement,Minor,2020-07-20 09:28:03,74
13318204,DN can distinguish SCMCommand from stale leader SCM,"As part of SCMCommand SCM will also send its current term, which will be used in Datanode to identify if the command was sent by the latest leader SCM.

 
Datanode will maintain the highest term that it has seen and compare it with the term that is received as part of SCMCommand.
 * If the term in the Datanode and SCMCommand are same, the command is added to the command queue for processing.

 * If the term in the Datanode is less than the term received in SCMCommand, Datanode will update its term and add the command to the command queue for processing.

 * If the term in the Datanode is greater than the term received in SCMCommand, Datanode will ignore the command.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-07-20 09:10:35,36
13318171,Encrypted bucket creation failed with INVALID_REQUEST Encryption cannot be set for bucket links,"Bucket creation with encrypted key fails.

Steps:

# Created encryption key
# Created volume
# Tried to create bucket with encryption key

Result:

{code}
INVALID_REQUEST Encryption cannot be set for bucket links
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-07-20 06:22:27,1
13318156,Frequent failure in TestCommitWatcher#testReleaseBuffersOnException,"{code}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 84.876 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestCommitWatcher
testReleaseBuffersOnException(org.apache.hadoop.ozone.client.rpc.TestCommitWatcher)  Time elapsed: 44.888 s  <<< FAILURE!
java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at org.apache.hadoop.ozone.client.rpc.TestCommitWatcher.testReleaseBuffersOnException(TestCommitWatcher.java:320)
{code}

https://github.com/elek/ozone-build-results/tree/master/2020/07/20/1837/it-client
https://github.com/elek/ozone-build-results/tree/master/2020/07/19/1830/it-client
https://github.com/apache/hadoop-ozone/runs/885336971?check_suite_focus=true
https://github.com/elek/ozone-build-results/tree/master/2020/07/17/1811/it-client
https://github.com/apache/hadoop-ozone/runs/880734025?check_suite_focus=true",pull-request-available,['test'],HDDS,Bug,Major,2020-07-20 05:01:55,1
13317592,Update proto.lock files,HDDS-3807 and HDDS-3612 introduced new additions to proto files but failed to update proto.lock files.  ,pull-request-available,"['Ozone Datanode', 'Ozone Manager']",HDDS,Task,Major,2020-07-19 05:03:33,13
13317554,Support filter and search the columns in recon UI,Add the functions of filter and search in recon UI. People can filter/search the columns.,pull-request-available,['Ozone Recon'],HDDS,Improvement,Major,2020-07-18 14:55:16,74
13317472,Ozone RocksDB Iterator wrapper should not expose key() and value() API.,"While investigating HDDS-3965 with [~nanda619], it was found that there is discrepancy in the implementation of the next(), key() and value() methods in the RDBStoreIterator wrapper class. 

*next()* returns the current rocksdb entry and moves ahead to the next entry. 
*key()* returns current rocksdb entry's key.
*value()* returns current rocksdb entry's value.

This means that during iteration next() returns the current value, and subsequent calls to key() / value() after next() will return the next value. To solve this, we can remove those 2 APIs from the iterator class, and have the usages follow this pattern.

{code}
Iterator iter = rdbTable.iterator();
while (iter.haxNext()) {
   Entry<Key,Value> entry = iter.next();
   // Use only entry.getKey(), entry.getValue().
}
{code}",pull-request-available,[],HDDS,Bug,Major,2020-07-17 18:48:55,76
13317463,Disable moveToTrash in o3fs and ofs temporarily,"A proper server-side trash cleanup solution (HDDS-3915) might not land any time soon.

This jira aims to completely disable ""move to trash"" when a client is deleting files and {{fs.trash.interval > 0}} by intercepting the deprecated {{fs.rename(src, dst, options)}} call used by {{TrashPolicyDefault#moveToTrash}}.

This should be reverted when trash cleanup is implemented.

CC [~arp] and [~msingh]",pull-request-available,['Ozone Client'],HDDS,Task,Major,2020-07-17 17:43:36,12
13317242,KeyValueBlockIterator#nextBlock skips valid blocks,"HDDS-3854 fixed a bug in KeyValueBlockIterator#hasNext, but introduced another one in KeyValueBlockIterator#nextBlock, which depends on the behavior of that method. When the first key encountered does not pass the filter, the internal nextBlock field is never intialized. Then a call to nextBlock() results in call to hasNext() which returns true, which recursively calls nextBlock(), again calling hasNext(), etc until the end of the set is reached and an exception is thrown. This skips all valid keys that may occur past the first invalid key.

Additionally, the current implementation of KeyValueBlockIterator#seekLast depends on the internal RocksDB iterators seekLast() method, which will skip to the last key in the DB regardless of whether it matches the filter or not. This could be different from last key according to the filter.

This bug was identified while working on HDDS-3869, which adds a strong typing layer before objects are serialized into RocksDB for datanode. Due to RocksDB internals, this changes the database layout so that all prefixed keys are returned at the beginning of the key set, instead of in the end. Since the original layout returned all prefixed keys at the end of the key set, this bug was not evident in any of the original unit tests, since the behavior described above could not occur.",pull-request-available,[],HDDS,Bug,Major,2020-07-16 19:30:59,22
13317177,Use Duration for time in RatisClientConfig,Change parameter and return type of time-related config methods in {{RatisClientConfig}} to {{Duration}}.  This results in more readable parameter values and type safety.,pull-request-available,[],HDDS,Improvement,Major,2020-07-16 14:22:52,1
13317147,StateContext#addPipelineActionIfAbsent does not work as expected,"{code:java}
/**
 * Add PipelineAction to PipelineAction queue if it's not present.
 *
 * @param pipelineAction PipelineAction to be added
 */
public void addPipelineActionIfAbsent(PipelineAction pipelineAction) {
  synchronized (pipelineActions) {
    /**
     * If pipelineAction queue already contains entry for the pipeline id
     * with same action, we should just return.
     * Note: We should not use pipelineActions.contains(pipelineAction) here
     * as, pipelineAction has a msg string. So even if two msgs differ though
     * action remains same on the given pipeline, it will end up adding it
     * multiple times here.
     */
    for (InetSocketAddress endpoint : endpoints) {
      Queue<PipelineAction> actionsForEndpoint =
          this.pipelineActions.get(endpoint);
      for (PipelineAction pipelineActionIter : actionsForEndpoint) {
        if (pipelineActionIter.getAction() == pipelineAction.getAction()
            && pipelineActionIter.hasClosePipeline() && pipelineAction
            .hasClosePipeline()
            && pipelineActionIter.getClosePipeline().getPipelineID()
            .equals(pipelineAction.getClosePipeline().getPipelineID())) {
          break;
        }
      }
      actionsForEndpoint.add(pipelineAction);
    }
  }
}
{code}
no matter absent or not, pipeline action will be added.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2020-07-16 11:18:40,12
13317126,Add option to limit number of items while displaying through ldb tool.,This Jira aims to add an option in the ldb tool  to limit the number of displayed items.,pull-request-available,['Tools'],HDDS,Sub-task,Minor,2020-07-16 09:17:59,26
13317055,Validate KeyNames created in FileSystem requests.,"This jira is to validate KeyNames which are created with OzoneFileSystem.
Similar to how hdfs handles using DFSUtil. isValidName()",pull-request-available,[],HDDS,Bug,Major,2020-07-16 01:21:44,13
13317036,LDB scan fails to read from transactionInfoTable,"*[root@bv-cml-1 ~]# ozone debug ldb --db=/var/lib/hadoop-ozone/om/data/om.db/ scan --column_family=transactionInfoTable
Table with specified name does not exist*

This is because DBDefinition is missing transactionInfo table.",pull-request-available,[],HDDS,Bug,Major,2020-07-15 22:14:13,13
13316926,Remove leftover debug setting,HDDS-3821 [accidentally|https://github.com/apache/hadoop-ozone/pull/1101#issuecomment-658750232] set some [log level to DEBUG|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/resources/log4j.properties#L23-L24] for integration tests.,pull-request-available,['test'],HDDS,Bug,Minor,2020-07-15 13:03:35,1
13316921,Intermittent crash in TestOMRatisSnapshots,"TestOMRatisSnapshots was recently enabled and is crashing intermittently:

https://github.com/elek/ozone-build-results/tree/master/2020/07/14/1690/it-hdds-om
https://github.com/elek/ozone-build-results/tree/master/2020/07/14/1710/it-hdds-om
https://github.com/elek/ozone-build-results/tree/master/2020/07/15/1713/it-hdds-om",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-07-15 12:45:48,27
13316909,SCM failed to start up for duplicated pipeline detected,"SCM LOG：
{code}
2020-07-15 19:25:09,768 [main] ERROR org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SCM start failed with exception
java.io.IOException: Duplicate pipeline ID PipelineID=db5966ec-140f-48d8-b0d6-e6f2ff777a77 detected.
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.addPipeline(PipelineStateMap.java:89)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(PipelineStateManager.java:53)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.initializePipelineState(SCMPipelineManager.java:165)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.<init>(SCMPipelineManager.java:100)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:410)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:281)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:213)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:624)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:144)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:119)


RocksDB dump, string,
rocksdb_ldb --db=scm.db scan --column_family=pipelines

$db5966ec-140f-48d8-b0d6-e6f2ff777a77ؑ????٬??????޹? : 
?
$02d3c9b4-7972-4471-a520-fff108b8d32e
                                     10.73.33.62
                                                10.73.33.62""

RATIS?M""

/default-rack???Ƕ?????Ő???? *?71-a520-fff108b8d32e:
$db5966ec-140f-48d8-b0d6-e6f2ff777a77ؑ????٬??????޹?2
?Yf?Hذ????wzw : 
?
$02d3c9b4-7972-4471-a520-fff108b8d32e
                                     10.73.33.62
                                                10.73.33.62""

RATIS?M""

HEX:
0x0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB001 : 0x0AAA010A2430326433633962342D373937322D343437312D613532302D666666313038623864333265120B31302E37332E33332E36321A0B31302E37332E33332E3632220A0A05524154495310824D220F0A0A5354414E44414C4F4E4510834D322430326433633962342D373937322D343437312D613532302D6666663130386238643332653A0D2F64656661756C742D7261636BA2061508F188C9CBC7B6F2E90210AEA6E3C590FEBF90A5011001180120012A3F0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB00132004085A7C1E5B42E
0xDB5966EC140F48D8B0D6E6F2FF777A77 : 0x0AAC010A2430326433633962342D373937322D343437312D613532302D666666313038623864333265120B31302E37332E33332E36321A0B31302E37332E33332E3632220A0A05524154495310824D220F0A0A5354414E44414C4F4E4510834D322430326433633962342D373937322D343437312D613532302D6666663130386238643332653A0D2F64656661756C742D7261636B4800A2061508F188C9CBC7B6F2E90210AEA6E3C590FEBF90A5011001180120012A3F0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB0013200409DFCAF8BB52E
{code}",pull-request-available upgrade-p0,[],HDDS,Bug,Blocker,2020-07-15 12:19:33,30
13316901,Ratis config key mismatch,"Some of the Ratis configurations in integration tests are not applied due to mismatch in config keys.
 # [Ratis|https://github.com/apache/incubator-ratis/blob/master/ratis-client/src/main/java/org/apache/ratis/client/RaftClientConfigKeys.java#L41-L53]: {{raft.client.rpc.watch.request.timeout}}
 [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java#L119-L122]: {{raft.client.watch.request.timeout}}
 # [Ratis|https://github.com/apache/incubator-ratis/blob/4db4f804aa90f9900cda08c79b54a45f80f4213b/ratis-server/src/main/java/org/apache/ratis/server/RaftServerConfigKeys.java#L470-L473]: {{raft.server.notification.no-leader.timeout}}
 [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/conf/DatanodeRatisServerConfig.java#L42]: {{raft.server.Notification.no-leader.timeout}}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-07-15 11:39:47,1
13316826,Improve efficiency by batching refreshPipeline calls to save RPC traffic,"This task to reduce the number of #refreshPipeline SCM rpc calls by batching together. This would be useful when many #getFileStatus or #listStatus user calls.

Code reference: [refreshPipeline|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java#L686]",performance,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2020-07-15 04:49:43,40
13316718,Avoid HddsProtos.PipelineID#toString,"{{PipelineID}} was recently changed to have integer-based ID in addition to the string ID.  Now log messages including {{PipelineID}} span multiple lines:

{code:title=https://github.com/elek/ozone-build-results/blob/92d31c9b58065b37a371c71c97b346f99163318d/2020/07/11/1626/acceptance/docker-ozone-ozone-freon-scm.log#L218-L223}
datanode_1  | 2020-07-11 13:07:00,540 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: ""8101dcbf-1a28-4f20-863a-0616b4e4bc4b""
datanode_1  | uuid128 {
datanode_1  |   mostSigBits: -9150790254504423648
datanode_1  |   leastSigBits: -8774694229384053685
datanode_1  | }
datanode_1  | .
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-07-14 14:46:23,1
13316677,Intermittent failure in Recon acceptance test due to mixed stdout and stderr,"Recon acceptance test failed with:

{code}
Check if Recon picks up OM data                                       | FAIL |
...
...,{""volume"":""vol-0-40306"",""bucket"":""bucket-0-15468"",""fileSize"":204
100  4177  100  4177    0     0   343k      0 --:--:-- --:--:-- --:--:--  370k
* Connection #0 to host recon left intact
8,""count"":10},...' does not contain '""fileSize"":2048,""count"":10'
{code}

It seems stdout and stderr was mixed, breaking {{""fileSize"":2048}} into two parts, thus search string was not found in the output.",pull-request-available,['test'],HDDS,Bug,Major,2020-07-14 11:19:17,1
13316504,Unable to list intermediate paths on keys created using S3G.,"Keys created via the S3 Gateway currently use the createKey OM API to create the ozone key. Hence, when using a hdfs client to list intermediate directories in the key, OM returns key not found error. This was encountered while using fluentd to write Hive logs to Ozone via the s3 gateway.
cc [~bharat]",pull-request-available,['Ozone Manager'],HDDS,New Feature,Blocker,2020-07-13 17:01:18,13
13316059,Rename the num.write.chunk.thread key,dfs.container.ratis.num.write.chunk.thread -> dfs.container.ratis.num.write.chunk.thread.per.disk ,pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2020-07-10 05:39:07,69
13315889,Sort DNs for client when the key is a file for #getFileStatus #listStatus APIs,"Similar to {{OzoneManagerFS#lookupFile(OmKeyArgs args, String clientAddress)}}, it would be good to sort the DNs for the given client address when the key is a file.

Thanks [~xyao] for your comments in [HDDS-3824-PR|https://github.com/apache/hadoop-ozone/pull/1164].",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2020-07-09 12:53:07,40
13315871,OM StateMachine unpause fails with NPE,"Noticed this NPE in OM logs for OM HA [acceptance test|https://github.com/apache/hadoop-ozone/pull/1173/checks?check_run_id=847204159]:

{code}
2020-07-07 20:54:23 WARN  RaftServerImpl:1247 - om2@group-D66704EFC61C: Failed to notify StateMachine to InstallSnapshot. Exception: java.lang.NullPointerException: When ratis is enabled indexToTerm should not be null
{code}",pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-07-09 11:38:20,1
13315733,Update proto.lock files,HDDS-426 introduced new additions to proto files but failed to update proto.lock files.  ,pull-request-available,"['Ozone CLI', 'Ozone Datanode']",HDDS,Task,Major,2020-07-08 21:31:02,37
13315673,Change latest snapshot log to debug,"In OM HA, followers log latest snapshot information twice per second:

{code}
om1_1       | 2020-07-08 15:46:47,097 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om2_1       | 2020-07-08 15:46:47,097 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om1_1       | 2020-07-08 15:46:47,604 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om2_1       | 2020-07-08 15:46:47,604 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om1_1       | 2020-07-08 15:46:48,110 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
om2_1       | 2020-07-08 15:46:48,110 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1
{code}

I think this should be debug-level message.",pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-07-08 16:09:20,1
13315367,Fix memory leak because of too many Datanode State Machine Thread,"When create 22345th  Datanode State Machine Thread, OOM happened.
!screenshot-1.png! 
 !screenshot-2.png! 
 !screenshot-3.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-07-07 10:21:24,65
13315327,Maven warning due to deprecated expression pom.artifactId,"{code:title=mvn clean}
[INFO] Scanning for projects...
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-interface-client:jar:0.6.0-SNAPSHOT
[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-common:jar:0.6.0-SNAPSHOT
[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.
...
{code}

Same warning in {{hadoop-hdds/pom.xml}} was fixed during review of HDDS-3875, but the one in {{hadoop-ozone/pom.xml}} was left.",pull-request-available,['build'],HDDS,Bug,Trivial,2020-07-07 07:27:07,1
13315266,Fix OMKeyDeletesRequest,"Cache of key should be updated in ValidateAndUpdateCache, as we return response once after adding to cache, and before DoubleBuffer flushes to disk using OmClientResponse#addToDBBatch.",pull-request-available,[],HDDS,Bug,Blocker,2020-07-07 00:36:14,13
13315249,Prettify OMDeleteRequest error log,"{noformat}
2020-07-06 21:57:04,266 ERROR org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest: Key delete failed. Volume:weichiu-test, Bucket:weichiu-bucket, Keytable_dir/_impala_insert_staging/8f49c4cce657919b_e77ca44900000000. Exception:{}
KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
        at 
...
{noformat}

The ""Key"" and ""table_dir"" should be separted. Also, the exception message doesn't require parameterization. ",pull-request-available,[],HDDS,Improvement,Trivial,2020-07-06 22:09:03,25
13315225,Fix endpoint display in S3 Gateway webpage,Fix the way in which S3g endpoint is being displayed in S3 gateway webpage.,pull-request-available,['S3'],HDDS,Bug,Major,2020-07-06 19:45:11,37
13315179,"Rename Ozone OM,DN,SCM runtime options to conform to naming conventions","Similar to {{HDFS_NAMENODE_OPTS}}, {{HDFS_DATANODE_OPTS}}, etc., we should have {{OZONE_MANAGER_OPTS}}, {{OZONE_DATANODE_OPTS}} to allow adding JVM args for GC tuning and debugging.

Update 1:

[~bharat] mentioned we already have some equivalents for OM and Ozone DNs:
- [HDFS_OM_OPTS|https://github.com/apache/hadoop-ozone/blob/bc7786a2fafb2d36923506f8de6c25fcfd26d55b/hadoop-ozone/dist/src/shell/ozone/ozone#L157] for Ozone OM. This looks like a typo, should begin with HDDS
- [HDDS_DN_OPTS|https://github.com/apache/hadoop-ozone/blob/bc7786a2fafb2d36923506f8de6c25fcfd26d55b/hadoop-ozone/dist/src/shell/ozone/ozone#L108] for Ozone DNs

Update 2:
- HDFS_OM_OPTS -> OZONE_OM_OPTS
- HDDS_DN_OPTS -> OZONE_DATANODE_OPTS
- HDFS_STORAGECONTAINERMANAGER_OPTS -> OZONE_SCM_OPTS

The new names conforms to {{hadoop_subcommand_opts}}. Thanks [~elek] for pointing this out.

Objective:

Rename the environment variables to be in accordance with the convention, and keep the compatibility by deprecating the old variable names.",pull-request-available,[],HDDS,Improvement,Minor,2020-07-06 15:26:33,12
13315165,Ozone Manager Token Identifier table should use in-house serialization rather than rely on proto serialization for key.,"Relying on Protobuf serialization for exact match is unreliable according to the docs. Hence, we have to move away from using proto.toByteArray() for on disk RocksDB keys. For more details, check parent JIRA.

In the case of the Ozone token identifier, it can be uniquely identified using the following fields.
* Issue Date
* Master Key ID
* Sequence Number

We can provide a simplified serialization method using the above 3 fields (in that order) to be used in the Token Identifier codec.

cc [~xyao]",pull-request-available upgrade-p0,['SCM'],HDDS,Sub-task,Major,2020-07-06 13:53:52,44
13315164,SCM Pipeline DB should directly use UUID bytes for key rather than rely on proto serialization for key.,"Relying on Protobuf serialization for exact match is unreliable according to the docs. Hence, we have to move away from using proto.toByteArray() for on disk RocksDB keys. For more details, check parent JIRA.

cc [~nanda619]",pull-request-available upgrade-p0,['SCM'],HDDS,Sub-task,Major,2020-07-06 13:53:20,63
13315079,Display the safemode status on scm page,"With this feature, we can clearly know the reason why scm stay in the safemode

!image-2020-07-06-14-44-36-479.png!",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-07-06 06:36:32,69
13314982,IllegalArgumentException triggered in SCMContainerPlacementRackAware.chooseDatanodes,"The root cause is existing replicas plus replicas in replicationInFlight meet the crossing rack requriement, for this case, we should check 
if misReplicated again. 

2020-07-04 16:25:01,496 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy: currentRackCount 1, replicas 3, requiredRacks 2, numRacks 4
2020-07-04 16:25:01,496 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: misReplicated = true, replicas size = 1
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: Handling underreplicated container: 250499
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: replicas of container {}#250499
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.142.198 CLOSED
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: deletionInFlight of container {}#250499
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: replicationInFlight of container {}#250499
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.19.147
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 100.76.61.31
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: source of container {}#250499
2020-07-04 16:25:01,496 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.142.198
2020-07-04 16:25:01,496 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Container #250499 expected replica count 3, but found 3, delta 0.
2020-07-04 16:25:01,496 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy: currentRackCount 2, replicas 3, requiredRacks 2, numRacks 4
2020-07-04 16:25:01,496 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.container.ReplicationManager: Process container #250499 error:
java.lang.IllegalArgumentException
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:128)
        at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware.chooseDatanodes(SCMContainerPlacementRackAware.java:101)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.handleUnderReplicatedContainer(ReplicationManager.java:578)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.processContainer(ReplicationManager.java:331)
        at java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649)
        at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1082)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.run(ReplicationManager.java:238)
        at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Major,2020-07-04 09:12:25,5
13314900,Too many redudant replications due to fail to get node's ancestor in ReplicationManager,"In our production cluster, we turn on the network topology configuraiton.  Due to fail to get the node's ancestor(the datanode object used doesn't have parent corrently set)  in ReplicationManager during the under-replicate and over-replicate check, ReplicationManager think the replicas of the container doean't meet the acrossing more than one rack requirement, then treat the container as under-replicate although it already has many replicas, and send command to datanodes to replicate the container again and again.  


2020-07-03 16:26:45,200 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Container #105228 is over replicated. Expected replica count is 3, but found 31.


2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: Handling underreplicated container: 210413
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: deletionInFlight of container {}#210413
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: replicationInFlight of container {}#210413
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.20.43
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: source of container {}#210413
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.5.41
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.142.251
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.8.85
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.142.250
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.8.35
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.8.67
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.142.135
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.144.104
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.20.58
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.179.142.198
2020-07-03 10:48:00,161 [ReplicationMonitor] DEBUG org.apache.hadoop.hdds.scm.container.ReplicationManager: 9.180.20.222
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.container.ReplicationManager: Process container #210413 error:
java.lang.IllegalArgumentException
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:128)
        at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware.chooseDatanodes(SCMContainerPlacementRackAware.java:101)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.handleUnderReplicatedContainer(ReplicationManager.java:568)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.processContainer(ReplicationManager.java:331)
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.NetUtils: Fail to get ancestor generation 1 of node :f8d9ccf6-20c6-4dfa-8a49-012f43a1b27e{ip: 9.179.142.251, host: host251, networkLocation: /rack3, certSerialId: null}
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.NetUtils: Fail to get ancestor generation 1 of node :826dda09-1259-4c5c-9a80-56b985665dc4{ip: 9.180.6.157, host: host-9-180-6-157, networkLocation: /rack10, certSerialId: null}
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.NetUtils: Fail to get ancestor generation 1 of node :b85962f2-6647-463b-9944-3c9b24e4e313{ip: 9.180.19.148, host: host-9-180-19-148, networkLocation: /rack3, certSerialId: null}
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.NetUtils: Fail to get ancestor generation 1 of node :039cb21e-4e2e-47e2-bf3e-b025319ee856{ip: 9.179.142.158, host: host158, networkLocation: /rack1, certSerialId: null}
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack1/33b49c34-caa2-4b4f-894e-dce7db4f97b9, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack3/b1e555d4-7114-4b80-b425-93086b0f2036, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack1/55148789-0cdb-4631-a3b3-c1da774523aa, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack3/32e8d855-b702-438d-b829-ac43dc567afc, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack2/2e1b2fdd-f8fb-4252-bfc1-31d5339681be, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack3/db854037-4846-4093-89de-e492e0f14239, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack3/f8d9ccf6-20c6-4dfa-8a49-012f43a1b27e, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack10/826dda09-1259-4c5c-9a80-56b985665dc4, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack3/b85962f2-6647-463b-9944-3c9b24e4e313, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] WARN org.apache.hadoop.hdds.scm.net.InnerNodeImpl: Ancestor not found, node: /rack1/039cb21e-4e2e-47e2-bf3e-b025319ee856, generation to exclude: 1, generation to return: 1
2020-07-03 10:48:00,161 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Container: #210419. The container is mis-replicated as it is on 1 racks but should be on 2 racks.
2020-07-03 10:48:00,161 [ReplicationMonitor] INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Sending replicate container command for container #210419 to datanode 5cb315e9-7326-4592-8dd6-21f4342b09c1{ip: 9.180.8.85, host: host-9-180-8-85, networkLocation: /rack10, certSerialId: null}


",pull-request-available,[],HDDS,Bug,Blocker,2020-07-03 14:09:52,5
13314840,ConcurrentModificationException in ContainerReportHandler.onMessage,"2020-07-03 14:51:45,489 [EventQueue-ContainerReportForContainerReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@8f6e7cb
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
        at java.util.HashMap$KeyIterator.next(HashMap.java:1469)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:343)
        at java.util.HashSet.<init>(HashSet.java:120)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:127)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-07-03 14:51:45,648 [EventQueue-ContainerReportForContainerReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@49d2b84b
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
        at java.util.HashMap$KeyIterator.next(HashMap.java:1469)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:343)
        at java.util.HashSet.<init>(HashSet.java:120)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:127)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Major,2020-07-03 07:05:07,28
13314838,Add recon to no_proxy of docker-config for acceptance test,"For internal acceptance test, which have configured proxy for http, test cases for recon will be affected.",pull-request-available,['Ozone Recon'],HDDS,Test,Minor,2020-07-03 07:01:45,36
13314557,Simple trash emptier on OM,"Implementation something similar to HDFS's {{NameNode#startTrashEmptier}} in OzoneManager to enable automatic trash clean up.

Some thoughts:
1. Ozone doesn't support efficient directory renaming (not until HDDS-2939 is fully merged).
- Possible solution: Override {{TrashPolicyDefault}} by setting {{fs.trash.classname}} in {{core-site.xml}}. So we can move files under {{/.Trash/<timestamp_of_delete>/}} instead of {{/.Trash/Current/}} to avoid folder renaming during checkpointing.
  - But this {{fs.trash.classname}} might affect ALL other FileSystems if configured in {{core-site.xml}}.
    - If there a way to only apply the config to o3fs and ofs.


Update:

In the design doc, #4 is the one we are looking at. But that approach is blocked by HDDS-3620 if we need an elegant way to implement batch rename on server side.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2020-07-01 21:21:32,12
13314556,Remove LevelDB configuration option for DN Metastore,"LevelDB support was removed for OM and SCM DBs but DN Metastore can still be configured to use LevelDB or RocksDB. 
This Jira proposes to remove LevelDB configuration option for DN Metastore (ozone.metastore.impl) and use RocksDB only.",pull-request-available,['Ozone Datanode'],HDDS,Task,Blocker,2020-07-01 21:14:15,43
13314484,Compile error in acceptance test on HDDS-2823,"{code}
[INFO] --- hadoop-maven-plugins:3.2.1:protoc (compile-protoc) @ hadoop-hdds-server-scm ---
[WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program ""protoc"": error=2, No such file or directory
[ERROR] stdout: []
{code}

https://github.com/apache/hadoop-ozone/runs/814218639",pull-request-available,['SCM HA'],HDDS,Bug,Blocker,2020-07-01 13:57:14,1
13314457,Duplicate dot in Prometheus endpoint config name,"{{HddsPrometheusConfig}} includes {{.}} at the end of the prefix, thus generated config name has double dot: {{hdds.prometheus..endpoint.token}}.",pull-request-available,[],HDDS,Bug,Minor,2020-07-01 11:27:05,1
13314308,OzoneRpcClient support batch rename keys.,"Currently rename folder is to get all the keys, and then rename them one by one. This makes for poor performance.

HDDS-2939 can able to optimize this part, but at present the HDDS-2939 is slow and still a long way to go. So we optimized the batch operation based on the current interface. We were able to get better performance with this PR before the HDDS-2939 came in.

This patch is a subtask of Batch Rename and first makes OzoneRpcClient Support Batch Rename Keys.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-06-30 14:39:46,4
13314017,Update default value of 'ozone.om.ratis.segment.size' and 'preallocated.size' to improve OM write perf,"Based on the *OM* performance tests on HDDs - write heavy workload {{Synthetic NNLoadGenerator}} in single node HA, the default 16KB ratis segment size becomes the bottleneck which affects the OM performance.

Below is the IOSTAT with 16KB segment.size and 16KB segment.preallocated.size, which causes high w_await time and very minimal batching <5 occurred in OM (most of the time during the run).
{code:java}
sdd: RATIS DISK
Device:  rrqm/s   wrqm/s  r/s   w/s    rMB/s  wMB/s  avgrq-sz  avgqu-sz   await   r_await     w_await    svctm    %util
sdd      0.00     0.00   0.00  138.00  0.00   1.27   18.88     21.99      65.25    0.00       65.25      6.88     94.90
sdd      0.00     0.00   0.00  103.00  0.00   1.07   21.23     40.36      918.25   0.00       918.25     9.72     100.10
sdd      0.00     0.00   0.00  104.00  0.00   1.04   20.55     30.08      1388.23  0.00       1388.23    9.62     100.10
sdd      0.00     0.00   0.00  396.00  0.00   1.55   8.00      136.50     285.30   0.00       285.30     2.40      94.90
{code}
 
 Below is the IOSTAT with 16MB segment.size and 16MB segment.preallocated.size. which minimizes the {{w_await}} time. This gives good performance improvement in traditional HDDs by doing more sync batching.
{code:java}
sdd: RATIS DISK
Device:  rrqm/s  wrqm/s  r/s    w/s      rMB/s   wMB/s   avgrq-sz  avgqu-sz  await  r_await   w_await   svctm   %util
sdd      0.00    0.00    0.00   125.74   0.00    19.85   323.34    3.05      24.28  0.00      24.28     7.17    90.10
sdd      0.00    0.00    0.00   128.00   0.00    19.76   316.12    3.31      25.91  0.00      25.91     7.14    91.40
sdd      0.00    0.00    0.00   115.00   0.00    4.59    81.81     0.93      8.10   0.00      8.10      8.04    92.50
sdd      0.00    0.00    0.00   111.00   0.00    4.53    83.57     0.90      8.12   0.00      8.12      8.14    90.30
sdd      0.00    0.00    0.00   115.00   0.00    4.64    82.64     0.93      8.08   0.00      8.08      8.10    93.20
{code}
 

Below is the IOSTAT with 4MB segment.size and 4MB segment.preallocated.size. which also minimizes the {{w_await}} time.
{code:java}
Device:  rrqm/s   wrqm/s  r/s    w/s       rMB/s   wMB/s  avgrq-sz  avgqu-sz  await   r_await  w_await   svctm  %util
sdd      0.00     0.00    0.00  115.00     0.00    6.08   108.34     0.99     8.57    0.00     8.57      8.10   93.20
sdd      0.00     0.00    0.00  122.00     0.00    7.81   131.13     1.48     12.15   0.00     12.15     7.80   95.10
sdd      0.00     0.00    0.00  115.00     0.00    7.81   139.04     1.05     9.09    0.00     9.09      8.10   93.20
sdd      0.00     0.00    0.00  115.00     0.00    7.85   139.78     1.04     8.95    0.00     8.95      8.03   92.30
sdd      0.00     0.00    0.00  114.00     0.00    5.83   104.70     0.97     8.57    0.00     8.57      7.97   90.90
sdd      0.00     0.00    0.00  115.00     0.00    7.80   138.92     1.05     9.10    0.00     9.10      8.11   93.30
sdd      0.00     0.00    0.00  119.00     0.00    7.93   136.47     1.72     14.41   0.00     14.41     7.81   92.90
{code}
 

Recommended config could be a value in MBs,  probably a value *higher than > 2MB or >4MB*",om-perf performance pull-request-available,[],HDDS,Bug,Major,2020-06-29 11:56:54,40
13313919,Implement container related operations in ContainerManagerImpl,SCM HA uses the new {{ContainerManagerImpl}}. This Jira tracks the implementation of container related operations in {{ContainerManagerImpl}}.,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-06-29 05:30:56,19
13313902,Datanode initialization is too slow when there are thousands of containers per volume,"It takes hours for datanode to finish the volume and container initailization.  Worst case is after 12hours it is still running volume reader on one datanode, which has 11 datanode volumes and each volume has 8000+ containers. 

Before the patch applied,  most of the volume reader threads are waiting for the ContainerCache lock.  With the patch applied, the worst case datanode cost 17m to finish the volume and container verify process. 

BTW: why there are so many containers are still under investigation.  It might the result of pipeline close.  So in long term, I think we should consider reuse healthy but not full containers which are closed because of their pipelines are closed. 


",pull-request-available,[],HDDS,Improvement,Blocker,2020-06-29 02:52:21,5
13313864,Add the usage of ofs in doc.,"HDDS-2665 has been merged into the master. Currently, ozone supports ofs, so some ofs  usage needs to be added to Doc.",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2020-06-28 15:18:39,4
13313784,Fix typo in pom.xml,ratis.thirdpary.version -> ratis.thirdparty.version,pull-request-available,[],HDDS,Bug,Minor,2020-06-27 18:39:15,13
13313738,Update doc and remove useless conf of o3fs.,"The latest changes have been made to all documents in HDDS-3704. I found in my validation that some of the other configurations were useless, so I removed some of the configurations based on the HDDS-3704 modifications.",pull-request-available,[],HDDS,Improvement,Major,2020-06-27 09:54:23,4
13313729,Update modification time when updating volume/bucket/key ACLs,"Thanks [~xyao] for the [notification|https://github.com/apache/hadoop-ozone/pull/164#pullrequestreview-377003954].

We should update field of modification time when we update volume/bucket/key ACLs.",pull-request-available,[],HDDS,Improvement,Major,2020-06-27 07:57:37,31
13313673,Improve OM HA Robot tests,"This Jira aims to address the following:
1. Add robot test for Install Snapshot feature 
2. Fix the flakiness in OM HA robot tests (HDDS-3313)",pull-request-available,[],HDDS,Bug,Major,2020-06-26 19:21:01,43
13313670,Introduce SCM layout version 'v0'.,"The first layout version for SCM will be '0' which will be written to the version file. Until a future Ozone release with Upgrade & Finalize support, this will just be a dummy number, to support backward compatibility. ",pull-request-available upgrade-p0,[],HDDS,Sub-task,Major,2020-06-26 19:02:40,11
13313592,Make OMHA serviceID optional if one (but only one) is defined in the config ,"om.serviceId is required on case of OM.HA in all the client parameters even if there is only one om.serviceId and it can be chosen.

My goal is:

 1. Provide better usability
 2. Simplify the documentation task ;-)

With using the om.serviceId from the config if 

 1. config is available
 2. om ha is configured 
 3. only one service is configured

It also makes easier to run the same tests with/without HA",pull-request-available,[],HDDS,Improvement,Major,2020-06-26 11:52:08,6
13313585,Do not fail CI check for log upload failure,"This workflow failed only due to temporary problem during artifact upload after successful tests: https://github.com/apache/hadoop-ozone/runs/809777550

To save time and resources, checks should not fail in this case.",pull-request-available,['CI'],HDDS,Improvement,Minor,2020-06-26 11:09:56,1
13313583,Display summary of failures as a separate job step,"Most CI checks print a summary (failed tests, checkstyle/rat violations, etc.) to stdout at the end of the test run, as well as into {{summary.txt}}.  Currently we have the following ways to view this output:

* drill down to the test step, scroll past lots of output
* download raw log, scroll past lots of output
* download artifact, unzip, open {{summary.txt}}

I propose displaying contents of {{summary.txt}} as a separate step.",pull-request-available,['CI'],HDDS,Improvement,Minor,2020-06-26 10:50:39,1
13313580,Package classpath files to the jar files instead of uploading them as artifacts,"During the earlier release we realized that Apache Nexus couldn't handle the classpath files well.

Classpath files are generated during the build and copied to the final distribution to use separated classpath definition for each of the services.

It turned out that the Apache Nexus couldn't handle it well (INFRA-18344)  and HDDS-1510 was an attempt to fix this problem. But during 0.5.0 release we have seen the same problem.

To make the 0.6.0 release more seamless I propose to update the logic and instead of copy the classpath file via an artifact I would pack/unpack them to/from the final artifact jar.",pull-request-available,['0.6.0'],HDDS,Bug,Critical,2020-06-26 10:38:27,6
13313517,Add resource core-site during loading of ozoneconfiguration,"If users add the properties of ozone to core-site, then during loading of OzoneConfiguration, it addsResource ozone-default.xml. This overrides the properties of ozone which are added to core-site.

To avoid this kind of override issue, we can addResource core-site.xml after ozone-default.xml",pull-request-available,[],HDDS,Bug,Major,2020-06-25 22:51:15,13
13313507,Use different column families for datanode block and metadata,"Currently datanodes place all of their data under the default column family in RocksDB. This differs from OM and SCM which organize their data into different column families based on its type. This feature will first move the datanode code off of the database utilities in the hadoop.hdds.utils package (which has no column family support), and move them to the newer utilities used by OM and SCM in the hadoop.hdds.utils.db package (which has column family support). The datanode will divide its data into three column families:
 # block_data: String keys (block id with optional prefix) map to BlockData objects
 # metadata: String keys (name of metadata field) map to Long objects.
 # deleted_blocks: String keys (block id with optional prefix) map to the ChunkInfo lists (lists of chunks corresponding to the block that was deleted).

A new field, called 'schemaVersion' will be added to container files to indicate whether the container was created using the original schema version 1, where everything was in the default column family, or this new schema version 2. Code should be able to process older schema versions for backwards compatibility.",pull-request-available upgrade,['Ozone Datanode'],HDDS,Improvement,Critical,2020-06-25 20:34:53,22
13313493,Implement getTrashRoot and getTrashRoots in o3fs,"Override the default Hadoop FS impl, which moves trash under {{o3fs://bucketName.volumeName.om/user/userName/.Trash/Current/...}}

New trash location will be: {{o3fs://bucketName.volumeName.om/.Trash/userName/Current/...}}

This change also unifies the trash root with OFS.

CC [~arp] [~msingh] [~shashikant] [~sadanand_shenoy]",pull-request-available,['Ozone Filesystem'],HDDS,Task,Blocker,2020-06-25 18:37:10,12
13313467,Extend the chunkinfo tool to display information from all nodes in the pipeline.,Currently the chunk-info tool ([HDDS-3134|https://issues.apache.org/jira/browse/HDDS-3134])  inside ozone debug displays information (chunk/block files that constitute the key) only from the first node of the pipeline. The plan here is to extend it  for the replicas.,pull-request-available,['Tools'],HDDS,Improvement,Minor,2020-06-25 15:04:06,26
13313265,Support multi-part-upload with Freon S3 key generator,ozone freon s3kg is a key generator which uses the s3 interface. It uses simple put objects but it turned out that s3 mpu implementation has some specific problem. I would improve the key generator to make it possible to test mpu upload with AWS Java SDK.,pull-request-available,"['freon', 'S3']",HDDS,Improvement,Major,2020-06-24 14:19:32,6
13313206,Fix handlePipelineFailure throw exception if role is follower, !screenshot-1.png! ,pull-request-available,[],HDDS,Bug,Major,2020-06-24 08:51:03,65
13313095,Add ldb to ozone-runner docker image,Upgrade from Ozone 0.5.0 to 0.6.0 (snapshot) requires some workaround steps (at least for HDDS-3499).  Smoke test for upgrade requires a docker image with the tools necessary for the workaround.,pull-request-available,['docker'],HDDS,Task,Major,2020-06-23 18:20:19,1
13313075,Remove support to start Ozone and HDFS datanodes in the same JVM,"With a few thousands issues ago Ozone was integral part of Hadoop/HDFS project. At that time there were two options to start datanode:

 1. Start in a separated JVM

 2. Start in the same JVM with the HDFS

Today only 1 is the standard way, this is tested and working. 2nd is not working any more but still documented.

I propose to drop the support of this use case as I can't see any benefit to support it anymore:

 1. I think 100% of the users will use Ozone as a separated process not as a HDFS Datanode plugin
 2. Fixing the classpath issues is significant effort as the classpath of HDFS and Ozone are diverged.",pull-request-available,[],HDDS,Improvement,Major,2020-06-23 15:39:37,6
13313074,Datanode in compose/ozonescripts can't be started ,Datanode in compose/ozonscripts can't be started due to an old configuration (OzoneHddsDatanodeService is removed in HDDS-738),pull-request-available,[],HDDS,Bug,Major,2020-06-23 15:31:31,6
13312930,Modify the admin document to let user know how to show the status of all rules.,"As HDDS-3825 bring a useful option `--verbose` to SafeModeCheckSubcommand, the documents should be updated also to let user know this feature.",pull-request-available,['documentation'],HDDS,Improvement,Major,2020-06-23 00:49:14,69
13312922,Add ratis.thirdparty.version in main pom.xml,"Introduce ratis.thirdparty.version in main pom.xml
This will help to override ratis.version if ozone is required to compile with a different version.",pull-request-available,[],HDDS,Bug,Major,2020-06-22 23:09:20,13
13312899,Change OMNotLeaderException logging to DEBUG,"In a OM HA setup, client tries the OM's in a round robin fashion to find the leader OM. It is not required to log the retry information for every client call. It creates noise on the console. Instead, we should just log the retry attempts only when all the retries fail. 


{code:java}
20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om1 is not the leader. Suggested leader is OM:om3.20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om1 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)
20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om2 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882){code}",pull-request-available,[],HDDS,Bug,Major,2020-06-22 20:06:39,43
13312767,NPE in s3g log output OzoneClientProducer Error ,"I run ls /goofys_mountpoint. There are lots of NPE in the s3g, but it can works.

{code}
2020-06-22 13:01:23,033 [qtp1800031768-348715] ERROR org.apache.hadoop.ozone.s3.OzoneClientProducer: Error:
org.jboss.weld.exceptions.WeldException: WELD-000049: Unable to invoke public void org.apache.hadoop.ozone.s3.AWSV4SignatureProcessor
.init() throws java.lang.Exception on org.apache.hadoop.ozone.s3.AWSV4SignatureProcessor@78f69f97
        at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.invokeMethods(DefaultLifecycleCallbackInvoker.java:99)
        at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.postConstruct(DefaultLifecycleCallbackInvoker.java:80)
        at org.jboss.weld.injection.producer.BasicInjectionTarget.postConstruct(BasicInjectionTarget.java:122)
        .........
        .........
        at org.glassfish.jersey.process.internal.Stages.process(Stages.java:197)
at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.invokeMethods(DefaultLifecycleCallbackInvoker.java:97)
        ... 121 more
Caused by: java.lang.NullPointerException
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:878)
        at org.apache.hadoop.ozone.s3.header.AuthorizationHeaderV4.<init>(AuthorizationHeaderV4.java:68)
        at org.apache.hadoop.ozone.s3.AWSV4SignatureProcessor.init(AWSV4SignatureProcessor.java:117)
        ... 125 more
{code}

after debug, i see the header from goofys doesn't contains Authorization

 !screenshot-1.png! 


For compare purpose, i use awscli to list a folder and get the header correctly.

authorization -> AWS4-HMAC-SHA256 Credential=testuser/scm@EXAMPLE.COM/20200623/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=a8ac23339f3b2265e8e9782d0526ddbf1d63a3d6eea4fde0e9b86f9d7019f0b3

 !screenshot-2.png! ",pull-request-available,['S3'],HDDS,Bug,Major,2020-06-22 06:51:43,69
13312401,Add isLeader check for SCM state updates,"We only allow leader to make decisions to update map, DB and fire events to DN",pull-request-available,[],HDDS,Sub-task,Major,2020-06-19 07:50:17,77
13312350,Modify ContainerPlacementPolicyFactory JavaDoc,"now ContainerPlacementPolicyFactory‘s javadoc is
{code:java}
property ozone.scm.container.placement.classname.{code}
but this config name has been changed to 
{code:java}
ozone.scm.container.placement.impl{code}
so change it to
{code:java}
property {@link ScmConfigKeys#OZONE_SCM_CONTAINER_PLACEMENT_IMPL_KEY}.
{code}",pull-request-available,['SCM'],HDDS,Improvement,Minor,2020-06-19 03:36:21,78
13312338,We need a edge(master) docs website updated by nightly build,Reference https://docs.alluxio.io/os/user/edge/en/Overview.html,pull-request-available,['documentation'],HDDS,New Feature,Major,2020-06-19 02:19:39,69
13312331,Use Pipeline choose policy to choose pipeline from exist pipeline list,"With this policy driven mode, we can develop various pipeline choosing policy to satisfy complex production environment.",pull-request-available,[],HDDS,New Feature,Major,2020-06-19 01:21:07,69
13312249,Enhance Recon ContainerEndPoint to report on difference unhealty container states,"HDDS-3082 changed the unhealthyContainers job to store the counts of under replicated, over replicated, missing and mis-replicated containers.

We should enhance ContainerEndPoint.java to provide API access to those states and perhaps a summary status too, eg:

under-replicated-count: 10
over-replication-count: 0
mis-replicated-count: 5
...",pull-request-available,['Ozone Recon'],HDDS,Improvement,Major,2020-06-18 14:31:19,11
13312242,Introduce OM layout version 'v0'.,"The first layout version for OzoneManager will be '0' which will be written to the version file. Until a future Ozone release with Upgrade & Finalize support, this will just be a dummy number, to support backward compatibility. ",upgrade-p0,[],HDDS,Sub-task,Major,2020-06-18 14:01:03,11
13312241,Introduce Layout Feature interface in Ozone,"* Implement the concept of a 'Layout Feature' in Ozone (with sample usage in Ozone Manager), which defines a specific change in on-disk layout in Ozone.
* Every feature is associated with a layout version, and an API corresponding to the feature cannot be invoked (throws NOT_SUPPORTED_OPERATION) before finalization.
* Created an annotation based 'aspect' for ""guarding"" new APIs that are introduced by Layout Features. Check out TestOMLayoutFeatureAspect#testCheckLayoutFeature.
* Added sample features and tests for ease of review (To be removed before commit).
* Created an abstract VersionManager and an inherited OM Version manager to initialize features, check if feature is allowed, check need to finalize, do finalization.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-06-18 14:00:06,30
13312203,Configuration parsing of ozone insight should be based on fields,"Ozone insight command can print out configuration related to a specific ozone component with parsing the @Config object.

 

But the usage of config annotations has been changed since HDDS-2661. We should check the annotated fields not methods.",pull-request-available,[],HDDS,Bug,Major,2020-06-18 11:57:01,6
13312195,Intermittent failure in TestKeyManagerUnit#listMultipartUploads,"{code}
Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.637 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestKeyManagerUnit
listMultipartUploads(org.apache.hadoop.ozone.om.TestKeyManagerUnit)  Time elapsed: 0.102 s  <<< FAILURE!
java.lang.AssertionError: Creation date is too old
  ...
  at org.apache.hadoop.ozone.om.TestKeyManagerUnit.listMultipartUploads(TestKeyManagerUnit.java:148)
{code}

https://github.com/elek/ozone-build-results/blob/master/2020/06/14/1034/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.TestKeyManagerUnit.txt",pull-request-available,['test'],HDDS,Bug,Major,2020-06-18 11:24:58,1
13312190,Split Ozone FS acceptance tests,"Ozone FS acceptance test is currently a monolithic one, a single test case for each of the filesystems (ofs and o3fs).  Finding out which assertion failed is harder than necessary.",pull-request-available,['test'],HDDS,Improvement,Major,2020-06-18 10:46:30,1
13312183,Show rule status of scm safemode,"Now, after start a cluster, we cannot write data to the Ozone cluster until the safemode exit.

We know there are some rule don't passed now, we know there are some rules, but don't know well about the status of each rule.

So, i think it would be better if we return more information about the safemode rule status when get status from Ozone cluster.",pull-request-available,[],HDDS,New Feature,Major,2020-06-18 09:47:34,69
13312182,OM read requests should make SCM#refreshPipeline outside the BUCKET_LOCK ,"Refresh pipeline info does a call to SCM and it can be moved outside the {{BUCKET_LOCK}}, this would help to improve the performance of read/write mix workloads.

*KeyManagerImpl.java*
{code:java}
    metadataManager.getLock().acquireReadLock(BUCKET_LOCK, volumeName,
        bucketName);
    try {
.....
.....
        // No need to check if a key is deleted or not here, this is handled
        // when adding entries to cacheKeyMap from DB.
        if (args.getRefreshPipeline()) {
          refreshPipeline(entry.getValue().getKeyInfo());
        }
.....
.....
    } finally {
      metadataManager.getLock().releaseReadLock(BUCKET_LOCK, volumeName,
          bucketName);
    }

{code}
 

Code Reference:  [KeyManagerImpl.java#L2071|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java#L2071]",om-perf perfomance pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-06-18 09:47:34,40
13312158,Hadoop3 artifact should depend on the ozonefs-shaded,"ozonefs-hadoop3 is an all-in-one ozonefs client which can be used as a single jar file.

Unfortunately it uses wrong dependency (ozonefs-common instead of ozonefs-common-shaded) which means that it downloads additional dependencies (netty-all, ...) if it's used from maven.",pull-request-available,[],HDDS,Sub-task,Blocker,2020-06-18 08:36:51,6
13312097,Eliminate duplicated GitHub Actions workflow,"GitHub Actions workflows for Ozone CI have two flavors: post-commit and pull-request.  The only difference between the two is that Sonar is not updated for PRs:

{code}
-       - uses: ./.github/buildenv
-         if: github.repository == 'apache/hadoop-ozone'
-         with:
-           args: ./hadoop-ozone/dev-support/checks/sonar.sh
-         env:
-           SONAR_TOKEN: ${{ secrets.SONARCLOUD_TOKEN }}
-           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
{code}

This issue proposes to keep only the post-commit definition and execute {{sonar.sh}} conditionally.",pull-request-available,['build'],HDDS,Improvement,Minor,2020-06-18 02:52:50,1
13312079,Disable Ozone SPNEGO should not fall back to hadoop.http.authentication configuration ,"HDDS-3282 adds hdds.[compoment].http.auth.[type] for scm/dn and ozone.[component].http.auth.[type] for om/s3g/recon. The type can be simple and kerberos. When the type is kerberos, it will setup the spnego authentication filter for the servlets. 

However, when the filter prefix was not setup properly when the type is simple. As a result, it will fallback to configuration from hadoop.http.authentication.*.

This ticket is opened to fix the authentication filter prefix so that the simple configuration can be honored when SPNEGO is disabled. ",pull-request-available,[],HDDS,Bug,Major,2020-06-17 23:31:19,28
13312024,OzoneManager#listVolumeByUser ignores userName parameter when ACL is enabled,"When {{ozone.acl.enabled}} is set to {{true}}, the [ACL check logic in OzoneManager#listVolumeByUser|https://github.com/apache/hadoop-ozone/blob/aa04ac0a894e15c98b05b1acef110c6e26bb01dc/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L1845-L1857] ignored the provided {{userName}}.

This bug is introduced by my commit HDDS-3056, unfortunately.

h3. Impact
e.g. {{userA}} won't be able to use {{ozone sh volume list --user userB}} to list {{userB}}'s volumes when ACL is enabled.

h3. Solution
Use {{userName}} rather than {{ProtobufRpcEngine.Server.getRemoteUser()}} for ACL check.",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2020-06-17 18:44:02,12
13311991,Schedule daily 2 builds from master branch build,"Mukul suggested to schedule cron based build to have more frequent data points to identify flaky tests.

We can start with two additional daily build which can be independent from the commit frequency (today we build master only after the commits).",build,['build'],HDDS,Improvement,Major,2020-06-17 15:44:55,6
13311808,Erasure Coding,"We propose to implement Erasure Coding in Apache Ozone to provide efficient storage. With EC in place, Ozone can provide same or better tolerance by giving 50% or more  storage space savings. 
In HDFS project, we already have native codecs(ISAL) and Java codecs implemented, we can leverage the same or similar codec design.

However, the critical part of EC data layout design is in-progress, we will post the design doc soon.

Also see HDDS-5351, which has a bunch of pre-requisites for the EC feature committed directly to the {{master}} branch.",EC,"['OM', 'Ozone Client', 'Ozone Datanode', 'SCM']",HDDS,New Feature,Major,2020-06-16 19:59:49,8
13311767,Avoid buffer copy in ContainerCommandRequestProto,"Avoid buffer copy because of

{code}
 final int i = 4 + bytes.asReadOnlyByteBuffer().getInt();
{code}

 bytes.asReadOnlyByteBuffer() causes the entire buffer to be copied.",performance pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-06-16 16:28:05,18
13311656,Disable netty resource leak detector in datanode,"By default netty enables resource leak detector. However, this ends up throwing exception everytime to track the location.  

 

For every write and for every bytebuf allocation in netty, it ends up going via this path which can hurt during concurrency and for perf.

[https://github.com/netty/netty/blob/b82258b72fdf4c733d9c4a641c6b8725b036485f/common/src/main/java/io/netty/util/ResourceLeakDetector.java#L106]

[https://github.com/netty/netty/blob/b82258b72fdf4c733d9c4a641c6b8725b036485f/common/src/main/java/io/netty/util/ResourceLeakDetector.java#L591]

!image-2020-06-08-07-46-24-733.png!",pull-request-available,[],HDDS,Bug,Major,2020-06-16 08:10:44,35
13311637,Add the logic to distribute open containers among the pipelines of a datanode,A datanode can participate in multiple pipelines based on no of raft log disks as well the disk type. SCM should make the distribution of open containers among these set of pipelines evenly.,pull-request-available,[],HDDS,Sub-task,Major,2020-06-16 05:50:23,16
13311636,Make number of open containers on a datanode a function of no of volumes reported by it,The no of open containers on a datanode is to be driven by factor of no of data disks available multiplied by no of open containers per disk. The aim here is to add the logic here and verify it.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-06-16 05:48:35,16
13311634,Propagate raft log disks info to SCM from datanode,No of pipelines to be created on a datanode is to be driven by the no of raft log disks configured on datanode. The Jira here is to add support for propagation of raft log volume info to SCM.,pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2020-06-16 05:43:18,16
13311628,Support recognize aws v2 Authorization header,"too much v4 authentication parser failure logs in s3g log file when client using v2 authentication header.

2020-06-16 12:41:13,195 [qtp576936864-367] ERROR org.apache.hadoop.ozone.s3.exception.S3ErrorTable: <?xml version=""1.0"" encoding=""UTF-8""?>
<Error>
  <Code>AuthorizationHeaderMalformed</Code>
  <Message>The authorization header you provided is invalid.</Message>
  <Resource>AWS root:ixWQAgWvJDuqLUqgDG9o4b2HF7c=</Resource>
  <RequestId/>
</Error>


2020-06-16 12:41:13,196 [qtp576936864-367] ERROR org.apache.hadoop.ozone.s3.OzoneClientProducer: Error:
org.jboss.weld.exceptions.WeldException: WELD-000049: Unable to invoke public void org.apache.hadoop.ozone.s3.AWSV4SignatureProcessor.init() throws java.lang.Exception on org.apache.hadoop.ozone.s3.AWSV4SignatureProcessor@da5f2ac
        at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.invokeMethods(DefaultLifecycleCallbackInvoker.java:99)
        at org.jboss.weld.injection.producer.DefaultLifecycleCallbackInvoker.postConstruct(DefaultLifecycleCallbackInvoker.java:80)
        at org.jboss.weld.injection.producer.BasicInjectionTarget.postConstruct(BasicInjectionTarget.java:122)
        at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.postConstruct(CdiComponentProvider.java:887)
        at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:162)
        at org.jboss.weld.context.AbstractContext.get(AbstractContext.java:96)
        at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
        at org.jboss.weld.bean.ContextualInstanceStrategy$CachingContextualInstanceStrategy.get(ContextualInstanceStrategy.java:177)
        at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
        at org.jboss.weld.bean.proxy.ContextBeanInstance.getInstance(ContextBeanInstance.java:99)
        at org.jboss.weld.bean.proxy.ProxyMethodHandler.getInstance(ProxyMethodHandler.java:125)
        at org.apache.hadoop.ozone.s3.AWSV4SignatureProcessor$Proxy$_$$_WeldClientProxy.getAwsAccessId(Unknown Source)
        at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:79)
        at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:68)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)
        at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:78)
        at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:100)
        at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
        at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
        at org.jboss.weld.context.unbound.DependentContextImpl.get(DependentContextImpl.java:70)
        at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
        at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
        at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:785)
        at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:885)
        at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
        at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:358)
        at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:369)
        at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:70)
        at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
        at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:72)
        at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
        at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:873)
        at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:159)
        at org.jboss.weld.context.unbound.DependentContextImpl.get(DependentContextImpl.java:70)
        at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
        at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
",pull-request-available,[],HDDS,Improvement,Major,2020-06-16 04:47:16,5
13311582,[OFS] Remove usage of OzoneClientAdapter interface,"Use ClientProtocol (proxy) directly instead of OzoneClient / ObjectStore in BasicRootedOzoneClientAdapterImpl and BasicRootedOzoneFileSystem as [~elek] have suggested.

This is part of the OFS refactoring effort.",pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Minor,2020-06-15 23:42:14,12
13311563,Recon start fails with SQL exception with MySQL DB.,"{code}
org.jooq.exception.DataAccessException: SQL [create table if not exists `CONTAINER_HISTORY`(`container_id` bigint null, `datanode_host` varchar(1024) null, `first_report_timestamp` bigint null, `last_report_timestamp` bigint null, constraint `pk_container_id_datanode_host` primary key (`container_id`, `datanode_host`))]; Specified key was too long; max key length is 3072 bytes
        at org.jooq_3.11.9.MYSQL_5_7.debug(Unknown Source)
        at org.jooq.impl.Tools.translate(Tools.java:2429)
        at org.jooq.impl.DefaultExecuteContext.sqlException(DefaultExecuteContext.java:832)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:364)
        at org.hadoop.ozone.recon.schema.ContainerSchemaDefinition.createContainerHistoryTable(ContainerSchemaDefinition.java:90)
        at org.hadoop.ozone.recon.schema.ContainerSchemaDefinition.initializeSchema(ContainerSchemaDefinition.java:72)
        at org.apache.hadoop.ozone.recon.ReconSchemaManager.lambda$createReconSchema$0(ReconSchemaManager.java:50)
        at java.lang.Iterable.forEach(Iterable.java:75)
        at org.apache.hadoop.ozone.recon.ReconSchemaManager.createReconSchema(ReconSchemaManager.java:48)
        at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:102)
        at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:53)
        at picocli.CommandLine.execute(CommandLine.java:1173)
        at picocli.CommandLine.access$800(CommandLine.java:141)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
        at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
        at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
        at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
        at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:75)
        at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:66)
        at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:67)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 3072 bytes
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
        at com.mysql.jdbc.Util.getInstance(Util.java:408)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:943)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2487)
        at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)
        at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1197)
        at com.jolbox.bonecp.PreparedStatementHandle.execute(PreparedStatementHandle.java:140)
        at org.jooq.tools.jdbc.DefaultPreparedStatement.execute(DefaultPreparedStatement.java:209)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:432)
        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-06-15 21:23:06,55
13311491,[OFS] Add User Guide,"Need to add a user guide markdown for OFS. Especially the usage for {{/tmp}}.

Thanks [~umamaheswararao] and [~xyao] for the reminder.

{{hadoop-hdds/docs/content/design/ofs.md}}",pull-request-available,['documentation'],HDDS,Sub-task,Blocker,2020-06-15 13:07:15,12
13311481,Incorrect data returned by reading a FILE_PER_CHUNK block,"A summary of s3 big file download result with Apri 22th master branch code,
1. download with aws s3 sdk, md5 sum is different
2. download with ""ozone fs -get o3fs://"",  md5 sum is different
3. download with ""ozone sh key get"", md5 sum is the same as the local file
So it seems the issue is from the read part.  And the md5sum result of step 1. and step 2. are also different from each other. (edited) 

The difference behaviors are caused by different read buffer size of different interface. If the read buffer size equals to chunk size, then fine. If the read buffer size is smaller than chunk size, then content returned is incorrent, because datanode side read ignore the offset in request, use 0 as offset to read the data.


FilePerChunkStrategy#readChunk 


{code:java}
// use offset only if file written by old datanode
        long offset;
        if (file.exists() && file.length() == info.getOffset() + len) {
          offset = info.getOffset();
        } else {
          offset = 0;   ---> this line causes the trouble. 
        }
{code}
",pull-request-available,[],HDDS,Bug,Critical,2020-06-15 12:21:14,5
13311455,Config being reloaded with hdfs as 'fs.defaultFS',"Noticed performance degradation while running {{SynteticLoadGenerator}} benchmark test with {{fs.defaultFS}}=HDFS *Vs* {{fs.defaultFS}}=O3FS.
{code:java}
Running LoadGenerator against fileSystem: hdfs://vb0929.halxg.cloudera.com:8020
                       *Vs*
Running LoadGenerator against fileSystem: o3fs://bucket2.vol2/
{code}
 

Command to run SyntheticLoadGenerator:-
{code:java}
yarn jar /opt/cloudera/parcels/CDH-7.2.0-1.cdh7.2.0.p0.3738720/jars/hadoop-mapreduce-client-jobclient-3.1.1.7.2.0.0-236-tests.jar NNloadGenerator -writeProbability 1 -readProbability 0.00 -elapsedTime 120 -numOfThreads 300 -root o3fs://bucket2.vol2/fsperf-Jun-11-2020/
{code}
 

With HDFS as 'fs.defaultFS', I could see that configuration is reloading resources always for each FileContext#create API call and is causing delay in submitting requests to OM server.
{code:java}
""Thread-304"" #327 prio=5 os_prio=0 tid=0x00007f1609d42000 nid=0x2990b waiting for monitor entry [0x00007f15cb990000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.misc.URLClassPath.getNextLoader(URLClassPath.java:479)
        - waiting to lock <0x00000000f01097e8> (a sun.misc.URLClassPath)
        at sun.misc.URLClassPath.findResource(URLClassPath.java:224)
        at java.net.URLClassLoader$2.run(URLClassLoader.java:572)
        at java.net.URLClassLoader$2.run(URLClassLoader.java:570)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findResource(URLClassLoader.java:569)
        at java.lang.ClassLoader.getResource(ClassLoader.java:1089)
        at java.lang.ClassLoader.getResource(ClassLoader.java:1084)
        at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2803)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3059)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2991)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2871)
        - locked <0x00000000f1da5f10> (a org.apache.hadoop.hdds.conf.OzoneConfiguration)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1223)
        at org.apache.hadoop.ozone.OmUtils.isServiceIdsDefined(OmUtils.java:175)
        at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:125)
        at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:51)
        at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:167)
        at org.apache.hadoop.fs.DelegateToFileSystem.<init>(DelegateToFileSystem.java:54)
        at org.apache.hadoop.fs.ozone.OzFs.<init>(OzFs.java:41)
        at sun.reflect.GeneratedConstructorAccessor4.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:142)
        at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:180)
        at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:265)
        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:341)
        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:338)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:338)
        at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:330)
        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:85)
        at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.genFile(LoadGenerator.java:330)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.write(LoadGenerator.java:304)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.nextOp(LoadGenerator.java:271)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.run(LoadGenerator.java:236)
{code}
 

With O3FS as 'fs.defaultFS', there is no config#loading.
{code:java}
""Thread-400"" #424 prio=5 os_prio=0 tid=0x00007f1988fe6800 nid=0x58441 in Object.wait() [0x00007f1942080000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1541)
        - locked <0x00000000f60c3ab8> (a org.apache.hadoop.ipc.Client$Call)
        at org.apache.hadoop.ipc.Client.call(Client.java:1499)
        at org.apache.hadoop.ipc.Client.call(Client.java:1396)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
        at com.sun.proxy.$Proxy10.submitRequest(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
        - locked <0x00000000f60a1690> (a org.apache.hadoop.io.retry.RetryInvocationHandler$Call)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
        at com.sun.proxy.$Proxy10.submitRequest(Unknown Source)
        at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:424)
        at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createFile(OzoneManagerProtocolClientSideTranslatorPB.java:1632)
        at org.apache.hadoop.ozone.client.rpc.RpcClient.createFile(RpcClient.java:1088)
        at org.apache.hadoop.ozone.client.OzoneBucket.createFile(OzoneBucket.java:538)
        at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.createFile(BasicOzoneClientAdapterImpl.java:227)
        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.createOutputStream(BasicOzoneFileSystem.java:270)
        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.create(BasicOzoneFileSystem.java:250)
        at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1263)
        at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)
        at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:615)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)
        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
        at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.genFile(LoadGenerator.java:330)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.write(LoadGenerator.java:304)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.nextOp(LoadGenerator.java:271)
        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.run(LoadGenerator.java:236)
{code}",om-perf performance,[],HDDS,Bug,Major,2020-06-15 10:11:06,6
13311327,Display version and setupTime of DN in recon web,"!image-2020-06-14-18-53-11-656.png|width=735,height=191!  

Here is the Datanode info currently presented in Recon Web.

If show more info about Datanodes, like the start time and version etc. We can get the cluster status more conveniently.

For example, we can list how many different DN versions in cluster, and how long have DN been running, and so on.",pull-request-available,[],HDDS,Improvement,Minor,2020-06-14 10:51:38,74
13311318,Fix the mismatched dependency versions in submodule hadoop-ozone-filesystem-hadoop2,"when I compiled the whole project, I found the following error
{code:java}
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hadoop-ozone-filesystem-hadoop2 ---
[INFO] Compiling 5 source files to /Users/huangtao6/IDEA/apache/hadoop-ozone/hadoop-ozone/ozonefs-hadoop2/target/classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] cannot access org.apache.hadoop.classification.InterfaceAudience.Public
  class file for org.apache.hadoop.classification.InterfaceAudience$Public not found
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3.077 s
[INFO] Finished at: 2020-06-14T14:37:22+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-ozone-filesystem-hadoop2: Compilation failure
[ERROR] cannot access org.apache.hadoop.classification.InterfaceAudience.Public
[ERROR]   class file for org.apache.hadoop.classification.InterfaceAudience$Public not found
{code}
I can reproduce that with `mvn -pl ""hadoop-ozone/ozonefs-hadoop2"" clean install`",build pull-request-available,['CI'],HDDS,Improvement,Minor,2020-06-14 06:47:00,74
13311236,Allow running coverage locally,Currently {{coverage-report.sh}} only works in GitHub Actions environment.,pull-request-available,['build'],HDDS,Improvement,Minor,2020-06-13 05:02:58,1
13311233,No coverage reported for Ozone FS,"Ozone FS classes are ignored by JaCoCo agent, hence [reported coverage is 0%|https://sonarcloud.io/component_measures?id=hadoop-ozone&metric=coverage&selected=hadoop-ozone%3Ahadoop-ozone%2Fozonefs-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fozone&view=list].",pull-request-available,['build'],HDDS,Bug,Major,2020-06-13 04:38:33,1
13311158,Topology Aware read does not work correctly in XceiverClientGrpc,"In XceiverClientGrpc.java, the calls to read a block or chunks for a Datanode end up in the private method sendCommandWithRetry(). In this method it decides which datanode it should send the request to. To do that, it checks if there is a cached DN connection for the given block and if so it uses that. If there is no cached connection, it should take network topology into account or shuffle the nodes:

{code}
   List<DatanodeDetails> datanodeList = null;

    DatanodeBlockID blockID = null;
    if (request.getCmdType() == ContainerProtos.Type.ReadChunk) {
      blockID = request.getReadChunk().getBlockID();
    } else if (request.getCmdType() == ContainerProtos.Type.GetSmallFile) {
      blockID = request.getGetSmallFile().getBlock().getBlockID();
    }

    if (blockID != null) {
      LOG.info(""blockid is not null"");
      // Check if the DN to which the GetBlock command was sent has been cached.
      DatanodeDetails cachedDN = getBlockDNcache.get(blockID);
      if (cachedDN != null) {
        LOG.info(""Cached DN is not null"");
        datanodeList = pipeline.getNodes();
        int getBlockDNCacheIndex = datanodeList.indexOf(cachedDN);
        if (getBlockDNCacheIndex > 0) {
          LOG.info(""pulling cached dn to top of list"");
          // Pull the Cached DN to the top of the DN list
          Collections.swap(datanodeList, 0, getBlockDNCacheIndex);
        }
      } else if (topologyAwareRead) {
        LOG.info(""topology aware - order DNs"");
        datanodeList = pipeline.getNodesInOrder();
      }
    }
    if (datanodeList == null) {
      LOG.info(""List is null - shuffling"");
      datanodeList = pipeline.getNodes();
      // Shuffle datanode list so that clients do not read in the same order
      // every time.
      Collections.shuffle(datanodeList);
    }
    <call to DN after here>
{code}

The normal flow for the client is to first make a getBlock() call to the DN and then a readChunk() call.

Due to the logic at the top of the block above, blockID is always going to be null for the getBlock() call, then it never checks the topologyAwareRead section and shuffles the node.

Then for readChunk, it will find the blockID, find a cached DN, which was the result of the shuffle, and then it reuses that DN.

Therefore the topologyAwareRead does not work as expected.",pull-request-available,[],HDDS,Bug,Major,2020-06-12 15:35:47,11
13311130,Use Hadoop 2.7.2 for ozone-mr/hadoop27 acceptance tests,"Today 2.7.7 is used but to be compatible with older Hadoop (including 2.7.4 which is included in spark), it's easier to test with an older version",pull-request-available,[],HDDS,Sub-task,Major,2020-06-12 12:18:18,1
13311128,Separate client proto files of Ozone to separated subprojects,"Similar to the previous patch, we need to move the Ozone client interfaces to a separated project. It would help us to reuse it from different projects and monitor to backward compatibility.",pull-request-available,[],HDDS,Sub-task,Major,2020-06-12 12:15:48,6
13311086,Fix TestOzoneRpcClientAbstract#testDeletedKeyForGDPR,"{code:java}
[ERROR] Tests run: 67, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 36.615 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis
3053[ERROR] testDeletedKeyForGDPR(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.165 s  <<< ERROR!
3054java.lang.NullPointerException
3055	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testDeletedKeyForGDPR(TestOzoneRpcClientAbstract.java:2730)
{code}",pull-request-available,[],HDDS,Sub-task,Major,2020-06-12 08:40:40,35
13311046,Make CSI command configurable.,"It doesn't have to use goofys as the mountable tools to access Ozone

BTW. It would be better if there are a CSI in component list",pull-request-available,['kubernetes'],HDDS,Sub-task,Major,2020-06-12 04:04:30,69
13310999,Update topology.aware.read parameter in ozone-topology compose config,"HDDS-1865 updated the parameter dfs.network.topology.aware.read.enable to ozone.network.topology.aware.read, but in the docker-compose config for ozone-topology, the old parameter is still used.

This Jira is to update it to the new value.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-06-11 21:20:43,11
13310916,Use matrix build for integration test,Integration test profiles can be simplified with matrix builds in GitHub Actions.,pull-request-available,['build'],HDDS,Improvement,Major,2020-06-11 12:56:22,1
13310885,Upload coverage even if tests failed,"PRs get code coverage feedback from Codecov thanks to changes implemented in HDDS-3726.  Codecov needs coverage data for each new revision on master.  However, _coverage_ check is currently [skipped|https://github.com/apache/hadoop-ozone/runs/759894317?check_suite_focus=true] if any tests fail.",pull-request-available,['build'],HDDS,Improvement,Major,2020-06-11 09:47:14,1
13310836,Remove podAntiAffinity from datanode-statefulset ,"Now, i think the yaml files under examples folder are trial purpose, so we should not add the strict limitation to prevent people easy to taste the example, if who want reference this example and use it in production env, then they will do a lot of modification.

For this ticket, it suppose to remove the podAntiAffinity check to let on node kubernetes run the ozone cluster on kubernetes successfully.",pull-request-available,['kubernetes'],HDDS,Sub-task,Major,2020-06-11 07:49:07,69
13310832,Update the apiVersion to the newer one,"For some apiVersion has been removed many years ago, so it is not forward-compatible, and i can use it in the newer version kubernetes cluster. For example, v1beta1, v1beta2, v1beta3 are removed in 2015.",pull-request-available,['kubernetes'],HDDS,Sub-task,Major,2020-06-11 07:30:58,69
13310828,Replace the imagePullPolicy from always to IfNotPresent,"With the always imagePullPolicy, i cannot use the self-build local image.",pull-request-available,['kubernetes'],HDDS,Sub-task,Major,2020-06-11 07:02:46,69
13310820,Block distribution in a pipeline among open containers is not uniform,"Currently, with concurrent allocate block calls, the block allocation among the open containers of a pipeline is not uniform as with concurrent logic, block allocation logic with last used notion does not hold up. The idea here is to address this.",pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-06-11 06:23:02,16
13310817,Reduce recon UI build time,"Hi [~vivekratnavel], [~elek],

The change in HDDS-3682 increased Recon build time by ~10 minutes, eg.:

{code:title=before}
[INFO] Apache Hadoop Ozone Recon .......................... SUCCESS [04:04 min]
{code}

{code:title=after}
[INFO] Apache Hadoop Ozone Recon .......................... SUCCESS [16:34 min]
{code}

Recon UI is built in _compile_, _acceptance_ and _coverage_ checks.  Since _coverage_ runs after all other checks, the increment affects overall CI response time twice.  (Effect was 3x before HDDS-3726, thanks for the fix in {{integration.sh}}.)

Is there any way to speed this up, eg. by installing dependencies in the ozone-build docker image?

before: https://github.com/apache/hadoop-ozone/actions/runs/130155633
pre-HDDS-3726: https://github.com/apache/hadoop-ozone/actions/runs/130301719
current: https://github.com/apache/hadoop-ozone/actions/runs/131217499",pull-request-available,['build'],HDDS,Bug,Critical,2020-06-11 06:07:17,37
13310801,Upgrading RocksDB version to avoid java heap issue,"Currently we have rocksdb 6.6.4 as major version and there are some jvm issues in tests (happened in [https://github.com/apache/hadoop-ozone/pull/1019]) related to rocksdb core dump. We may upgrade to 6.8.1 to avoid this issue.

{{JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni2954960755376440018.jnilib+0x602b8]  rocksdb::GetColumnFamilyID(rocksdb::ColumnFamilyHandle*)+0x8

See full dump at [https://the-asf.slack.com/files/U0159PV5Z6U/F0152UAJF0S/hs_err_pid90655.log?origin_team=T4S1WH2J3&origin_channel=D014L2URB6E](url)}}",pull-request-available,['upgrade'],HDDS,Bug,Major,2020-06-11 02:38:50,6
13310674,Add OMDBDefinition to define structure of om.db,The rocksdb tool to display data  from a db file uses  implementations of DBDefinition class which specifically describes the structure and type of the db file. To support the tool for om.db this class is defined.,pull-request-available,['Tools'],HDDS,Sub-task,Minor,2020-06-10 14:01:06,26
13310517,Improve getPipelines performance, !screenshot-1.png! ,pull-request-available,[],HDDS,Sub-task,Major,2020-06-10 01:54:00,65
13310429,[OFS] Address merge conflicts after HDDS-3627,"HDDS-3627 removed isolated class loader and moved classes around.

I will address the merge conflicts after rebasing OFS feature branch to master branch which includes HDDS-3627 in this jira.",pull-request-available,[],HDDS,Sub-task,Blocker,2020-06-09 16:03:56,12
13310419,Fluentd writing to secure Ozone S3 API fails with 500 Error.,"Error on fluentd side.
{code}
opened
starting SSL for host1:9879...
SSL established
<- ""PUT /logs-bucket-1/logs/mytag/2020/06/05//202006052222_190411.gz HTTP/1.1\r\nContent-Type: application/x-gzip\r\nAccept-Encoding: \r\nUser-Agent: aws-sdk-ruby3/3.94.0 ruby/2.4.10 x86_64-l
inux aws-sdk-s3/1.63.0\r\nX-Amz-Storage-Class: STANDARD\r\nExpect: 100-continue\r\nContent-Md5: zGKVGGaD/U5WUK3cdWQiSA==\r\nHost: host1:9879\r\nX-Amz-Content-Sha256:
 277fe97f57a1127ee1a0765979ffd3270a6c18c6f75ff6a0f843e7163a338de2\r\nContent-Length: 44726\r\nX-Amz-Date: 20200608T190412Z\r\nAuthorization: AWS4-HMAC-SHA256 Credential=hdfs@ROOT.HWX.SITE/202
00608/us-east-1/s3/aws4_request, SignedHeaders=content-md5;content-type;expect;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-storage-class, Signature=11c1d0a43325d3f7b9d25dbd02023cef2
69b66f6a93fa4e1c935b52e3e372f70\r\nAccept: */*\r\n\r\n""
-> ""HTTP/1.1 100 Continue\r\n""
-> ""\r\n""
-> ""HTTP/1.1 500 Server Error\r\n""
-> ""Pragma: no-cache\r\n""
-> ""X-Content-Type-Options: nosniff\r\n""
-> ""X-FRAME-OPTIONS: SAMEORIGIN\r\n""
-> ""X-XSS-Protection: 1; mode=block\r\n""
-> ""Connection: close\r\n""
-> ""\r\n""
reading all...
-> """"
{code}
",pull-request-available,[],HDDS,Bug,Blocker,2020-06-09 15:07:47,30
13310385,Intermittent failure in TestDeleteWithSlowFollower,"TestDeleteWithSlowFollower failed soon after it was re-enabled in HDDS-3330.

{code:title=https://github.com/apache/hadoop-ozone/runs/753363338}
[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.647 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 0.163 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.junit.Assert.assertNotNull(Assert.java:631)
  at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)
{code}

CC [~shashikant] [~elek]",pull-request-available,['test'],HDDS,Sub-task,Major,2020-06-09 12:25:07,1
13310309,Avoid UUID#toString call in PipelineID#getProtobuf.,"
 !screenshot-1.png! 
 !screenshot-2.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-09 07:05:00,65
13310169,Add test coverage of the acceptance tests to overall test coverage ,Acceptance test coverage should be added to the generic coverage numbers. We have a lot of important tests there...,pull-request-available,[],HDDS,Improvement,Major,2020-06-08 15:29:43,6
13310141,Storage-class support for Ozone,"Use a storage-class as an abstraction which combines replication configuration, container states and transitions. 

See this thread for the detailed design doc:

 

[https://lists.apache.org/thread.html/r1e2a5d5581abe9dd09834305ca65a6807f37bd229a07b8b31bda32ad%40%3Cozone-dev.hadoop.apache.org%3E]
",pull-request-available,[],HDDS,Improvement,Major,2020-06-08 13:42:26,6
13310133,Rename framework to common-server,"I started to document the project hierarchy of Ozone and found that to subproject name is very confusing:

 

 1. framework: it supposed to be similar to the common but share the classes only between server side projects (om/scm/...). I propose to call it server-common

 2. container-service: While we have historical reasons why it's a separated project, but today we can call it datanode and remove hadoop-ozone/datanode.

 

While I can learn both the specific names, for new contributors it can help if we use more straightforward names.


In this patch I would like to fix the first item (framework --> common-server)",TriagePending pull-request-available,['build'],HDDS,Improvement,Major,2020-06-08 12:58:32,6
13310083,SCMPipelineManager's createPipeline method has redundant synchronization modifiers,I think it's safe to remove the synchronized qualifier from the createPipeline method since it already uses lock for critical zone protection inside createPipeline.,pull-request-available,['SCM'],HDDS,Improvement,Major,2020-06-08 10:29:52,79
13310068,"ozone ""fs -ls"" cann't list ""o3fs://bucket.volume"" without tail ""/""","ozone fs -ls o3fs://hive.tdw                        
ls: `o3fs://hive.tdw': No such file or directory",pull-request-available,[],HDDS,Bug,Major,2020-06-08 09:25:53,39
13310052,Improve SCM performance with 3.2% by avoid stream.collect,"I start a ozone cluster with 1000 datanodes and 10 s3gateway, and run two weeks with heavy workload, and perf scm.
 !screenshot-1.png! 
 !screenshot-2.png! 
 !screenshot-3.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-08 08:16:44,65
13310038,Improve OM performance with 3.7% by avoid stream.collect,"I start a ozone cluster with 1000 datanodes and 10 s3gateway, and run two weeks with heavy workload, and perf om.
 !screenshot-1.png! 
 !screenshot-2.png! 
 !screenshot-3.png! 
 !screenshot-4.png! 
 !screenshot-5.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-08 07:11:34,65
13309828,Improve OM and SCM performance with 64% by avoid call getServiceInfo in s3g,"*What's the problem ?*
I start a ozone cluster with 1000 datanodes and 10 s3gateway, and run two weeks with heavy workload, and perf om and scm.
1. From om perf, getServiceList cost 63.75% cpu.
 !screenshot-1.png! 
2. From scm perf, queryNode come from om::getServiceList cost 33.20% cpu
 !screenshot-2.png! 

*What's the reason ?*
Now s3g create a client for each request. when create each RpcClient, s3g will call ServiceInfoEx serviceInfoEx = ozoneManagerClient.getServiceInfo(), getServiceInfo will call getServiceList. Then om and scm are busy with getServiceList.
But s3g does not use the List<ServiceInfo> which got from getServiceList at all. 

 ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-06 04:19:05,65
13309820,Avoid NetUtils.normalize when get DatanodeDetails from proto,"I start a ozone cluster with 1000 datanodes, and run two weeks with heavy workload, and perf om.
You can find NetUtils.normalize cost 0.17% cpu, because it needs to do Matcher.replaceAll. It happen in the critical path, i.e. lookUpKey when read. we need not to do this when getFromProtoBuf, So we can avoid normalize by check the last character.
 !screenshot-3.png! 
 !screenshot-4.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-06 02:20:50,65
13309817,Improve OM performance with 5.29% by avoid stream.collect,"I start a ozone cluster with 1000 datanodes, and run two weeks with heavy workload, and perf om.
stream().collect(Collectors.toList())) in getProtoBuf cost 5.29% cpu.

 !screenshot-1.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-06 01:34:30,65
13309797,Reload old OM state if Install Snapshot from Leader fails,"Follower OM issues a pause on its services before installing new checkpoint from Leader OM (Install Snapshot). If this installation fails for some reason, the OM stays in paused state. It should be unpaused and the old state should be reloaded.",pull-request-available,['OM HA'],HDDS,Sub-task,Critical,2020-06-05 21:13:49,43
13309760,PipelineStateManagerV2Impl#removePipeline will remove pipeline from db in case of failure,"When {{PipelineStateManagerV2Impl#removePipeline}} is called, we are removing the pipeline from db and then making the call to {{pipelineStateMap#removePipeline}}.
{{pipelineStateMap#removePipeline}} can throw an exception if it's not able to remove the pipeline, this will leave the db in inconsistent state.",pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-06-05 15:47:35,2
13309694,Improve the performance of SCM with 0.78% by decrease the times of lock and unlock ,"I start a ozone cluster with 1000 datanodes, and run two weeks with heavy workload, and perf scm.
lock and unlock in NodeStateMap.getNodeInfo cost 0.78% cpu because of so many datanodes.

 !screenshot-1.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-05 11:02:41,65
13309678,Improve the performance of SCM with 3.86% by avoid TreeSet.addAll,"TreeSet.addAll cost 3.86% cpu.

we need not TreeSet.addAll which cost 3.86% cpu, because the DatanodeDetails  which got from NodeStateMap::ConcurrentHashMap<NodeState, Set<UUID>> stateMap, the value type of stateMap is a set, so there is no duplicate DatanodeDetails. we can replicate TreeSet.addAll to list.addAll

 
!screenshot-1.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-05 09:35:01,65
13309672,Avoid UUID#toString call in DatanodeDetails#getUuidString.,"I start a ozone cluster with 1000 datanodes, and run two weeks with heavy workload, and perf scm.
UUID.toString in DatanodeDetails cost 7.23% cpu because of so many datanodes.
 !screenshot-2.png! ",pull-request-available,[],HDDS,Sub-task,Major,2020-06-05 09:18:19,65
13309606,Upload code coverage to Codecov and enable checks in PR workflow of Github Actions,HDDS-3170 aggregates code coverage across all components. We need to upload the reports to codecov to be able to keep track of coverage and coverage diffs to be able to tell if a PR does not do a good job on writing unit tests.,pull-request-available,['build'],HDDS,Bug,Major,2020-06-05 05:27:20,37
13309515,Implement getContentSummary to provide replicated size properly to dfs -du command,"Currently when you run hdfs dfs -du command against a path on Ozone, it uses the default implementation from FileSystem class in the Hadoop project, and that does not care to calculate with replication factor by default. In DistributedFileSystem and in a couple of FileSystem implementation there is an override to calculate the full replicated size properly.

Currently the output is something like this for a folder which has file with replication factor of 3:
{code}
hdfs dfs -du -s -h o3fs://perfbucket.volume.ozone1/terasort/datagen
931.3 G  931.3 G  o3fs://perfbucket.volume.ozone1/terasort/datagen
{code}

The command in Ozone's case as well should report the replicated size az the second number so something around 2.7TB in this case.
In order to do so, we should implement getContentSummary and calculate the replicated size in the response properly in order to get there.",Triaged,[],HDDS,Improvement,Major,2020-06-04 18:21:57,63
13309478,Datanode configuration object has wrong values,"Regardless of values configured for {{hdds.datanode.replication.streams.limit}} and {{hdds.datanode.container.delete.threads.max}}, these config items always use 10 and 2, respectively.

Also, no warning is logged for invalid values ({{< 1}}).",Triaged pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-06-04 15:42:18,1
13309462,Datanode may fail to stop,"TestOzoneManagerListVolumes timed out after 2 minutes while trying to stop the mini cluster.  It seems one of the datanodes was stuck in an infinite loop trying to execute a task on a terminated executor:

{code:title=}
2020-06-03 15:28:19,475 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(221)) - Unable to finish the execution.
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@73c36b6c rejected from org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor@5c021707[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:181)
	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.execute(RunningDatanodeState.java:143)
	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:411)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:208)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:375)
	at java.lang.Thread.run(Thread.java:748)
{code}

{code:title=grep -c 'Unable to finish the execution' hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestOzoneManagerListVolumes-output.txt}
2087250
{code}

Test output eventually grew to 2.6GB.

https://github.com/apache/hadoop-ozone/runs/735169623",Triaged,['Ozone Datanode'],HDDS,Bug,Critical,2020-06-04 13:59:43,1
13309461,improve OmKeyLocationInfoGroup internal data structure,"Currently, OmKeyLocationInfoGroup stores all OmKeyLocationInfo instances with different versions into a list however many use-cases are just interested in OmKeyLocationInfo of a specific version, so I think storing OmKeyLocationInfos into more structured Map would provide better performance in trivial use-cases.",Triaged pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-06-04 13:58:46,80
13309450,CSI smoketest fails if socket file is not created on time,"HDDS-3461 introduced a new CSI smoketest (to be sure that the CSI daemon can be started).

It was reverted because a failure on the master and commited after an additional check is added to wait until the CSI socket is created.

Unfortunately this check is bad. In some cases it can fail:

For example in here:

[https://github.com/jsoft88/hadoop-ozone/runs/734147343?check_suite_focus=true]

 
{code:java}
connection error: desc = ""transport: Error while dialing dial unix /tmp/csi.sock: connect: no such file or directory"" {code}
Thanks to [~jsoft88] , who reported this problem.",pull-request-available,['test'],HDDS,Bug,Major,2020-06-04 13:17:43,6
13309425,Compile of Ozone fails with JDK 11+,"@PostConstruct annotation is removed from JDK (it's Java EE) in the recent JDKs.

It's used by the Configuration annotation but we don't need to use Java EE annotations:
{code:java}
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-hdds-config: Compilation failure: Compilation failure: 
[ERROR] /Users/sbanerjee/ozone_fork/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationReflectionUtil.java:[20,24] cannot find symbol
[ERROR]  symbol:  class PostConstruct
[ERROR]  location: package javax.annotation
[ERROR] /Users/sbanerjee/ozone_fork/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationReflectionUtil.java:[139,38] cannot find symbol
[ERROR]  symbol:  class PostConstruct
{code}
 

Thanks  Shashikant for the bug report.",pull-request-available,['build'],HDDS,Bug,Major,2020-06-04 11:32:29,6
13309396,Improvement for OzoneFS client to work with Hadoop 2.7.3,"The background is the hadoop production clusters we used internally is based on Hadoop 2.7.3.  Currenlty we maintain an internal OzoneFS client for Hadoop 2.7.3.  With HDDS-3627 merged,  it is the right time to use the community version instead of an internal version. 

The improvement are some Hadoop2.7.7 newly added functions which are not available in 2.7.3.   So refact the code to use an older version with same functionality. 
",pull-request-available,[],HDDS,Sub-task,Major,2020-06-04 08:58:33,5
13309337,Handle inner classes in SCMRatisRequest and SCMRatisResponse,Currently creating class object for inner class fails with {{ClassNotFoundException}}. We have to handle inner class instantiation.,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-06-04 03:35:36,19
13309175,Merge archived jacoco coverage results,"HDDS-3635 started to archive the jacoco coverage data for each of the unit and integration tests (unit, it-*).

 

This patch introduces a new build step to combine all of them together and archive the coverage report in HTML as a build artifact.

Notes:

 1. acceptance test coverage is not yet included

 2. I decided to do it only for master (branch) builds as it requires a new build which adds ~15 minutes to the full build. As the coverage data is not (yet) used for PR we don't need to enable it (yet)

 3. We can further improve it to upload the merged data to somewhere (sonar?) Can be done in the next Jira",pull-request-available,['build'],HDDS,Bug,Major,2020-06-03 11:44:10,6
13309065,Rebase OFS branch - 3. Adapt to HDDS-3501,"Due to HDDS-3501, I need to change OFS AdapterImpl as well to adapt to OzoneFileStatus class change.",pull-request-available,[],HDDS,Sub-task,Major,2020-06-02 23:13:04,12
13309040,Typo in GDPR doc,"[https://hadoop.apache.org/ozone/docs/0.5.0-beta/gdpr.html]

 

Once you create a GDPR compliant bucket, any key created in that bucket will automatically *{color:#de350b}by{color}* GDPR compliant.

 

by -> be",newbie pull-request-available,['documentation'],HDDS,Bug,Major,2020-06-02 20:35:07,79
13308980,UUID can be non unique for a huge samples,"Now, we have used UUID as id for many places, for example, DataNodeId, pipelineId. I believe that it should be pretty less chance to met collision, but, if met the collision, we are in trouble.",Triaged,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Bug,Minor,2020-06-02 15:33:31,69
13308970,[OFS] Implement getTrashRoots for trash cleanup,"We need to override {{getTrashRoots()}} as well in order to allow for easier future OM trash cleanup impl.

This jira doesn't directly implement the trash cleanup feature itself, but a prerequisite for this feature.

This is a follow-up jira to HDDS-3574: https://github.com/apache/hadoop-ozone/pull/941#discussion_r428212741
",pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Blocker,2020-06-02 14:54:58,12
13308940,Update all the documentation to use ozonefs-hadoop2/3  instead of legacy/current,Using legacy jar is legacy. We need to updated all the docs.,pull-request-available,[],HDDS,Sub-task,Blocker,2020-06-02 12:18:40,6
13308939,Consider avoiding file lookup calls in writeChunk hotpath,"In getChunkFile internally, it invokes ""verifyChunkDirExists"". This causes file existence checks for the directory and throws IO exception accordingly. If the file is anyways going to be written, it is better to handle it later and throw the same exception. This could avoid file checks for every ""writechunk""

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L106]

 

File channels are cached anyways in ""OpenFiles"". So if we can avoid ""file.getAbsolutePath()"", this could save memory and resolving paths. 

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L118]

 

Also ""validateChunkForOverwrite"" can be optimised, as ""isOverWritePermitted"" would be false most of the times.

 

!Screenshot 2020-06-02 at 5.28.59 PM.png|width=835,height=510!

 ",Triaged performance,[],HDDS,Improvement,Blocker,2020-06-02 12:07:03,1
13308877,Change Close container exception logging to debug in ContainerUtils#logAndReturnError,"The logging in ContainerUtils should be changed to debug in case of close container exception
{code:java}
2020-06-01 20:39:19,705 INFO org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler: Operation: PutBlock , Trace ID:  , Message: Requested operation not allowed as ContainerState is CLOSED , Result: CLOSED_CONTAINER_IO , StorageContainerException Occurred.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Requested operation not allowed as ContainerState is CLOSED
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.checkContainerOpen(KeyValueHandler.java:902)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:417)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:179)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:155)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:297)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:164)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:395)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:405)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction$6(ContainerStateMachine.java:746)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",Performance Triaged pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-06-02 06:34:35,55
13308874,Number of open containers per pipeline should be tuned as per the number of disks on datanode,"Currently, ""ozone.scm.pipeline.owner.container.count"" is configured by default to 3. The default should ideally be a function of the no of disks on a datanode. A static value may lead to uneven utilisation during active IO .",Performance,['Ozone Datanode'],HDDS,Bug,Major,2020-06-02 06:26:20,16
13308787,Ozone Non-Rolling upgrades,Support for Non-rolling upgrades in Ozone.,pull-request-available,[],HDDS,New Feature,Major,2020-06-01 19:23:37,30
13308697,Reduce dn-audit log,"Do we really need such fine grained audit log? This ends up creating too many entries for chunks.
{noformat}
2020-05-31 23:31:48,477 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267324230275483 bcsId: 93943} | ret=SUCCESS |
2020-05-31 23:31:48,482 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267323565871437 bcsId: 93940} | ret=SUCCESS |
2020-05-31 23:31:48,487 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267324230275483 bcsId: 93943} | ret=SUCCESS |
2020-05-31 23:31:48,497 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 166 locID: 104267324172472725 bcsId: 93934} | ret=SUCCESS |
2020-05-31 23:31:48,501 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267323675906396 bcsId: 93958} | ret=SUCCESS |
2020-05-31 23:31:48,504 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267324230275483 bcsId: 93943} | ret=SUCCESS |
2020-05-31 23:31:48,509 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 166 locID: 104267323685343583 bcsId: 93974} | ret=SUCCESS |
2020-05-31 23:31:48,512 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 166 locID: 104267324172472725 bcsId: 93934} | ret=SUCCESS |
2020-05-31 23:31:48,516 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267324332380586 bcsId: 0} | ret=SUCCESS |
2020-05-31 23:31:48,726 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 166 locID: 104267324232634780 bcsId: 93964} | ret=SUCCESS |
2020-05-31 23:31:48,733 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 166 locID: 104267323976323460 bcsId: 93967} | ret=SUCCESS |
2020-05-31 23:31:48,740 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267324131512723 bcsId: 93952} | ret=SUCCESS |
2020-05-31 23:31:48,752 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267324230275483 bcsId: 93943} | ret=SUCCESS |
2020-05-31 23:31:48,760 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 165 locID: 104267323675906396 bcsId: 93958} | ret=SUCCESS |
2020-05-31 23:31:48,772 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 166 locID: 104267323685343583 bcsId: 93974} | ret=SUCCESS |
2020-05-31 23:31:48,780 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 164 locID: 104267324304724389 bcsId: 0} | ret=SUCCESS |
2020-05-31 23:31:48,787 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 164 locID: 104267323991724421 bcsId: 93970} | ret=SUCCESS |
2020-05-31 23:31:48,794 | INFO  | DNAudit | user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 164 locID: 104267323725189479 bcsId: 93963} | ret=SUCCESS |

 {noformat}
And ends up choking disk utilization with lesser write/mb/sec.

Refer to 100+ writes being written with 0.52 MB/sec and choking entire disk utilization.

!write_to_dn_audit_causing_high_disk_util.png|width=726,height=300!

 

Also, the username and IP are currently set as null. This needs to be replaced by using details from grpc",Triaged performance pull-request-available,[],HDDS,Improvement,Critical,2020-06-01 10:37:42,81
13308639,Consider avoiding stream/map/sum in write hotpath,"[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BufferPool.java#L113]

 
{noformat}
public long computeBufferData() {
 return bufferList.stream().mapToInt(ChunkBuffer::position)
 .sum();
 }
  {noformat}
[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java#L256]

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java#L264]

Getting back to regular iteration should fix the issue. This causes 3x CPU expensive when compared to HDFS flow.

 

!image-2020-06-01-11-08-08-711.png|width=777,height=564!

 

 ",Triaged performance pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Critical,2020-06-01 05:39:06,82
13308587,Update gRPC to 1.29.0,This is to update the grpc compile version to 1.29.0,pull-request-available,['build'],HDDS,Bug,Major,2020-05-31 18:11:21,18
13308549,Fix a typo in BlockManagerImpl#allocateBlock,There is a typo in BlockManagerImpl#allocateBlock.,pull-request-available,[],HDDS,Bug,Trivial,2020-05-31 10:12:42,83
13308542,Add various profiles to MiniOzoneChaosCluster to run different modes,"Add various profiles to MiniOzoneChaosCluster to run different modes. This will help in running different modes easily from MiniOzoneChaosCluster shell script
",pull-request-available,['test'],HDDS,Bug,Major,2020-05-31 06:56:49,18
13308225,Remove replay logic from actual request logic,"HDDS-3476 used the transaction info persisted in OM DB during double buffer flush when OM is restarted. This transaction info log index and the term are used as a snapshot index. So, we can remove the replay logic from actual request logic. (As now we shall never have the transaction which is applied to OM DB will never be again replayed to DB)",pull-request-available,['OM HA'],HDDS,Sub-task,Blocker,2020-05-29 04:45:58,13
13308166,Recon UI: Add interactive visualization for file size counts,Include a histogram to interactively view file size counts across each volume/bucket,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-05-28 21:40:20,37
13308164,Recon: Add support to store file size counts in each volume/bucket,Update file size count task to keep track of file size counts across each volume/bucket. Make the necessary changes to the underlying schema.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-05-28 21:38:31,37
13308021,Remove usage of DFSUtil.addPBProtocol method,"Hadoop 3.3.0 upgraded protocol buffers to 3.7.1 and RPC code have been changed. This change will cause compile failure in Ozone.

Vinayakumar is fixing this in Hadoop-side (HADOOP-17046) but it would be better for Ozone to avoid the usage of Hadoop {{@Private}} classes to make Ozone a separate project from Hadoop.",Triaged pull-request-available,['build'],HDDS,Bug,Major,2020-05-28 12:19:11,1
13308018,Display datanode uuid into the printTopology command output,"Sorry to the previous changes HDDS-3606, it is useful, but i have to do some other efforts to distinguish the datanodes in same node, display the uuid of each datanode can make more sense.",Triaged pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-05-28 12:00:57,69
13307959,add a ozone-style.xml that can be imported into IDEA.,"Add  a ozone-style.xml, Based on the current requirements of ozone checkstyle. 
 After importing into IDEA, you can format the code as required by ozone. Avoiding the checkstyle problem in CI, this can improve our development efficiency.",pull-request-available,['Tools'],HDDS,Improvement,Major,2020-05-28 08:40:23,4
13307910,Ozone fs failed to list intermediate directory,"{code}
bin/ozone sh volume create /hadoop
bin/ozone sh bucket create /hadoop/ozone
bin/ozone sh key put /hadoop/ozone/benchmarks/TestDFSIO/dir1/test README.md
bin/ozone fs -ls o3fs://ozone.hadoop/
{code}

{code}
2020-05-28 11:46:32,028 INFO protocolPB.Hadoop3OmTransport: RetryProxy: java.lang.NullPointerException
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$KeyInfo$Builder.setFactor(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getProtobuf(OmKeyInfo.java:392)
	at org.apache.hadoop.ozone.om.helpers.OzoneFileStatus.getProtobuf(OzoneFileStatus.java:103)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.listStatus(OzoneManagerRequestHandler.java:572)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:196)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:208)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:135)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:103)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

2020-05-28 11:46:32,029 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$KeyInfo$Builder.setFactor(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getProtobuf(OmKeyInfo.java:392)
	at org.apache.hadoop.ozone.om.helpers.OzoneFileStatus.getProtobuf(OzoneFileStatus.java:103)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.listStatus(OzoneManagerRequestHandler.java:572)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:196)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:208)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:135)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:103)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
, while invoking $Proxy13.submitRequest over nodeId=null,nodeAddress=10.120.110.183:9862. Trying to failover after sleeping for 2000ms.
2020-05-28 11:46:34,033 INFO protocolPB.Hadoop3OmTransport: RetryProxy: java.lang.NullPointerException
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$KeyInfo$Builder.setFactor(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getProtobuf(OmKeyInfo.java:392)
	at org.apache.hadoop.ozone.om.helpers.OzoneFileStatus.getProtobuf(OzoneFileStatus.java:103)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.listStatus(OzoneManagerRequestHandler.java:572)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:196)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:208)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:135)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:103)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
{code}

",pull-request-available,[],HDDS,Bug,Major,2020-05-28 03:50:17,5
13307897,Introduce SCMStateMachineHandler marker interface,Introduce {{SCMStateMachineHandler}} marker interface which can be used to identify the StateMachine handlers in SCM,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-05-28 02:56:09,19
13307743,Remove property ozone.scm.container.creation.lease.timeout,Property {{ozone.scm.container.creation.lease.timeout}} is not used anymore and can be removed.,newbie pull-request-available,['SCM'],HDDS,Improvement,Major,2020-05-27 13:42:25,71
13307669,SCM Infinite loop in BlockManagerImpl.allocateBlock,"The following step can reproduce this issue

- A new ozone cluster with only a factor three pipeline
- put a big file(1G) into cluster, during the put process,  we kill the leader datanode of this pipeline.

The put command will hang, the following log will fill the scm log file.
2020-05-27 17:32:46,988 [IPC Server handler 23 on default port 9863] WARN org.apache.hadoop.hdds.scm.container.SCMContainerManager: Container allocation failed for pipeline=Pipeline[ Id: bf7dd356-2d97-4b2a-8a81-e2ddd25bc5a1, Nodes: e859cad9-c7f6-451a-a039-af06103aa978{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null}1cd2bf20-a791-42a0-b4cd-b26d995cb8eb{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null}0827f3bb-0d94-435a-a157-4db2c84cdedf{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:3, State:OPEN, leaderId:0827f3bb-0d94-435a-a157-4db2c84cdedf, CreationTimestamp2020-05-27T08:05:36.590Z] requiredSize=268435456 {}
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=bf7dd356-2d97-4b2a-8a81-e2ddd25bc5a1 not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getContainers(PipelineStateMap.java:301)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getContainers(PipelineStateManager.java:95)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getContainersInPipeline(SCMPipelineManager.java:360)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:507)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:428)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:230)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:190)
        at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:167)
        at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:119)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)
        at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13303)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)


",Triaged,['SCM'],HDDS,Bug,Major,2020-05-27 09:33:27,69
13307486,OzoneManager start fails with RocksDB error on downgrade to older version.,"OM start fails with RocksDB error when downgrading to older version that does not have all the column families that may have been created  in the newer version.

{code}
java.io.IOException: Failed init RocksDB, db path : /tmp/ozone/data/metadata/om.db, exception :org.rocksdb.RocksDBE
xception You have to open all column families. Column families not opened: transactionInfoTable; status : InvalidAr
gument; message : You have to open all column families. Column families not opened: transactionInfoTable
        at org.apache.hadoop.hdds.utils.db.RDBStore.toIOException(RDBStore.java:159)
        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:141)
        at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:181)
        at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.start(OmMetadataManagerImpl.java:267)
        at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.<init>(OmMetadataManagerImpl.java:164)
        at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:478)
        at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:416)
        at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:884)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.start(OzoneManagerStarter.java:123)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOm(OzoneManagerStarter.java:78)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:66)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:37)
        at picocli.CommandLine.execute(CommandLine.java:1173)
        at picocli.CommandLine.access$800(CommandLine.java:141)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
        at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
        at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
        at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
        at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:75)
        at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:66)
        at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:50)
Caused by: org.rocksdb.RocksDBException: You have to open all column families. Column families not opened: transact
ionInfoTable
        at org.rocksdb.RocksDB.open(Native Method)
        at org.rocksdb.RocksDB.open(RocksDB.java:290)
        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:97)
        ... 20 more
{code}

Thanks to [~bharat] for reporting this issue.",Triaged pull-request-available upgrade,['Ozone Manager'],HDDS,Bug,Critical,2020-05-26 17:07:01,30
13307446,If we gracefully stop datanode it would be better to notify scm and recon ,"if you execute `bin/ozone --daemon stop datanode`, datanode would be better if it handle the signal and the stop dn request to scm, then scm update datanode state to dead.

It would be better if you provide a admin cli tool to support remove datanode manually.


{code:bash}
ozone admin datanode remove <UUID>
{code}
 ",pull-request-available,"['Ozone Datanode', 'Ozone Recon', 'SCM']",HDDS,New Feature,Minor,2020-05-26 13:50:17,83
13307439,Delete container failed,"
{code:bash}
bin/ozone admin container delete 1
Loaded properties from hadoop-metrics2.properties
Scheduled Metric snapshot period at 10 second(s).
XceiverClientMetrics metrics system started
Unknown command type: DeleteContainer
{code}

",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-05-26 13:37:38,69
13307436,Container info command should print uuid of datanode,"bin/ozone admin container info 1
Container id: 1
Pipeline id: f47e37ff-7f7e-453b-9bcc-a58e8117b66f
Container State: CLOSED
Datanodes: [c4dcebcf-3997-4ebd-a501-8d7c3053de61/localhost,
5d7d5f77-57c6-4c90-8edf-eb0f872a0166/localhost,
aa01cb2c-6742-4938-a719-1dc75f76e88f/localhost]
",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2020-05-26 13:25:46,69
13307419,Display uuid in recon datanode page to distinguish each datanode," !screenshot-1.png! 

I don't know who is who",pull-request-available,['Ozone Recon'],HDDS,Improvement,Major,2020-05-26 12:01:46,69
13307395,decouple finalize and destroy pipeline,"We have to decouple finalize and destroy pipeline. We should have two separate calls, closePipeline and destroyPipeline.

Close pipeline should only update the pipeline state, it’s the job of the caller to issue close container commands to all the containers in the pipeline.

Destroy pipeline should be called from pipeline scrubber, once a pipeline has spent enough time in closed state the pipeline scrubber should call destroy pipeline.",pull-request-available,[],HDDS,Sub-task,Major,2020-05-26 10:06:54,77
13307361,Overwrite an existed key with different factor or type won't update the OmKeyInfo,"Here is the reproduce process. Put three replication, then with one replication and/or  STAND_ALONE type , the replicationFactor of the output is 3 and type is still RATIS. Vice versa, put one replication first, the replicationFactor is always going to be 1.  
 # bin/ozone sh volume create myvol
 # bin/ozone sh bucket create /myvol/mybucket
 # bin/ozone sh key put -r THREE /myvol/mybucket/NOTICE.txt NOTICE.txt
 # bin/ozone sh key info /myvol/mybucket/NOTICE.txt
{code:json}
{
  ""volumeName"" : ""myvol"",
  ""bucketName"" : ""mybucket"",
  ""name"" : ""NOTICE.txt"",
  ""dataSize"" : 17540,
  ""creationTime"" : ""2020-08-09T00:36:25.593Z"",
  ""modificationTime"" : ""2020-08-09T00:36:28.149Z"",
  ""replicationType"" : ""RATIS"",
  ""replicationFactor"" : 3,
  ""ozoneKeyLocations"" : [ {
    ""containerID"" : 1,
    ""localID"" : 104656626357960709,
    ""length"" : 17540,
    ""offset"" : 0
  } ],
  ""metadata"" : { },
  ""fileEncryptionInfo"" : null
}{code}
 # bin/ozone sh key put -r ONE -t STAND_ALONE /myvol/mybucket/NOTICE.txt NOTICE.txt
 # bin/ozone sh key info /myvol/mybucket/NOTICE.txt
{code:json}
 {
  ""volumeName"" : ""myvol"",
  ""bucketName"" : ""mybucket"",
  ""name"" : ""NOTICE.txt"",
  ""dataSize"" : 17540,
  ""creationTime"" : ""2020-08-09T00:36:25.593Z"",
  ""modificationTime"" : ""2020-08-09T00:38:05.381Z"",
  ""replicationType"" : ""RATIS"",
  ""replicationFactor"" : 3,
  ""ozoneKeyLocations"" : [ {
    ""containerID"" : 5,
    ""localID"" : 104656632737693702,
    ""length"" : 17540,
    ""offset"" : 0
  } ],
  ""metadata"" : { },
  ""fileEncryptionInfo"" : null
}{code}",Triaged,['Ozone Manager'],HDDS,Bug,Critical,2020-05-26 08:04:30,74
13307351,Stop to persist container related pipeline info of each key into OM DB to reduce DB size,"An investigation result of serilized key size, RATIS with three replica.  Following examples are quoted from the output of the ""ozone sh key info"" command which doesn't show related pipeline information for each key location element. 

1.  empty key,  serilized size 113 bytes
hadoop/bucket/user/root/terasort/10G-input-7/_SUCCESS
{
  ""volumeName"" : ""hadoop"",
  ""bucketName"" : ""bucket"",
  ""name"" : ""user/root/terasort/10G-input-7/_SUCCESS"",
  ""dataSize"" : 0,
  ""creationTime"" : ""2019-11-21T13:53:11.330Z"",
  ""modificationTime"" : ""2019-11-21T13:53:11.361Z"",
  ""replicationType"" : ""RATIS"",
  ""replicationFactor"" : 3,
  ""ozoneKeyLocations"" : [ ],
  ""metadata"" : { },
  ""fileEncryptionInfo"" : null
}

2.  key with one chunk data, serilized size 661 bytes
hadoop/bucket/user/root/terasort/10G-input-6/part-m-00037
{
  ""volumeName"" : ""hadoop"",
  ""bucketName"" : ""bucket"",
  ""name"" : ""user/root/terasort/10G-input-6/part-m-00037"",
  ""dataSize"" : 223696200,
  ""creationTime"" : ""2019-11-18T07:47:58.254Z"",
  ""modificationTime"" : ""2019-11-18T07:53:52.066Z"",
  ""replicationType"" : ""RATIS"",
  ""replicationFactor"" : 3,
  ""ozoneKeyLocations"" : [ {
    ""containerID"" : 7,
    ""localID"" : 103157811003588713,
    ""length"" : 223696200,
    ""offset"" : 0
  } ],
  ""metadata"" : { },
  ""fileEncryptionInfo"" : null
}

3. key with two chunk data, serilized size 1205 bytes,
ozone sh key info hadoop/bucket/user/root/terasort/10G-input-7/part-m-00027
{
  ""volumeName"" : ""hadoop"",
  ""bucketName"" : ""bucket"",
  ""name"" : ""user/root/terasort/10G-input-7/part-m-00027"",
  ""dataSize"" : 223696200,
  ""creationTime"" : ""2019-11-21T13:47:07.653Z"",
  ""modificationTime"" : ""2019-11-21T13:53:07.964Z"",
  ""replicationType"" : ""RATIS"",
  ""replicationFactor"" : 3,
  ""ozoneKeyLocations"" : [ {
    ""containerID"" : 221,
    ""localID"" : 103176210196201501,
    ""length"" : 134217728,
    ""offset"" : 0
  }, {
    ""containerID"" : 222,
    ""localID"" : 103176231767375926,
    ""length"" : 89478472,
    ""offset"" : 0
  } ],
  ""metadata"" : { },
  ""fileEncryptionInfo"" : null
}

When client reads a key, there is ""refreshPipeline"" option to control whether to get the up-to-date container location infofrom SCM. 
Currently, this option is always set to true, which makes  saved container location info in OM DB useless. 

Another motivation is when using Nanda's tool for the OM performance test,  with 1000 millions(1Billion) keys, each key with 1 replica, 2 chunk meta data, the total rocks DB directory size is 65.5GB.  One of our customer cluster has the requirement to save 10 Billion objects.  In this case ,the DB size is approximately （65.5GB * 10 * /2 * 3 ）~ 1TB. 

The goal of this task is going to discard the container location info when persist key to OM DB to save the DB space.




",pull-request-available,[],HDDS,Improvement,Major,2020-05-26 07:05:29,5
13307163,Add documentation for Copy key command,"HDDS-3646 introduced a copy command to copy a key to another one with a bucket.

The following documents need to be updated to describe the new command usage.
~/hadoop-ozone/hadoop-hdds/docs/content/shell/KeyCommands.md
~/hadoop-ozone/hadoop-hdds/docs/content/shell/KeyCommands.zh.md",Triaged pull-request-available,['Ozone CLI'],HDDS,New Feature,Major,2020-05-25 07:43:10,69
13307149,Add unit test for SCMRatisResponse,Jira to track the unit test coverage of SCMRatisResponse.,pull-request-available,"['SCM HA', 'test']",HDDS,Sub-task,Major,2020-05-25 06:12:23,77
13307148,Add unit test for SCMRatisRequest,Jira to track the unit test coverage of SCMRatisRequest,pull-request-available,"['SCM HA', 'test']",HDDS,Sub-task,Major,2020-05-25 06:11:19,77
13307135,Add documentation for Cat Key Command,"HDDS-3638 introduced a cat command to view the contents of a key.

KeyCommands.md and KeyCommands.zh.md files need to be updated to describe this new command and usage.",pull-request-available,['documentation'],HDDS,Bug,Major,2020-05-25 05:36:58,69
13306929,NPE while open datanode page since a pipeline no leader,"If there is a pipeline have no leader on a datanode, it will through NPE. STAND_ALONE pipeline have no leader, and ratis pipeline have no leader some time.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-05-23 07:09:59,69
13306900,Add a copy command to copy key to a new one within a same bucket. ,"It should support specify the replication factor and type. 

bin/ozone sh key cp ozone sh key cp <bucket URI> <fromKey> <toKey>

if i execute bin/ozone sh key cp /myvol/mybucket/ key1 key2 will have the same content with key1.

The documents will be updated, tracked by https://issues.apache.org/jira/browse/HDDS-3653",pull-request-available,[],HDDS,Sub-task,Major,2020-05-23 01:08:40,69
13306899,Add a replication type option for putkey command,"If we can put a key into Ozone by different type {RATIS, STAND_ALONE}, but i have to change the ozone-site.xml file, so, add a option can be better.",pull-request-available,['Ozone CLI'],HDDS,Sub-task,Major,2020-05-23 01:01:15,69
13306668,Failed to delete chunk file due to chunk size mismatch,"LOGs

{noformat}
2020-05-19 13:45:30,493 [BlockDeletingService#8] WARN org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy: Chunk file doe not exist. chunk info :ChunkInfo{chunkName='104079328540607246_chunk_1, offset=0, len=4194304}
2020-05-19 13:45:30,493 [BlockDeletingService#8] ERROR org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}
2020-05-19 13:45:30,494 [BlockDeletingService#8] ERROR org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService: Failed to delete files for block #deleting#104079328540607246
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}
        at org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy.deleteChunks(FilePerChunkStrategy.java:286)
        at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDispatcher.deleteChunks(ChunkManagerDispatcher.java:111)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteBlock(KeyValueHandler.java:1043)
        at org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService$BlockDeletingTask.lambda$call$0(BlockDeletingService.java:286)
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}
{noformat}


chunk_1 is 4MB and chunk_2 is 1MB in block info.  
chunk_1 doesn't exit(might been deleted successfully)  and chunk_2 is 5MB on disk. ",Triaged pull-request-available,[],HDDS,Bug,Major,2020-05-22 04:09:50,1
13306566,Stop/Pause Background services while replacing OM DB with checkpoint from Leader,"When a follower OM needs to replace its DB with a checkpoint from Leader (to catch up on the transactions), it should pause or stop services which read/ write to the DB. 



During OM HA testing, found that OM could crash with JVM error on RocksDB. This happened because KeyDeletingService was trying to access a memory which is already freed up.
{code:java}
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f19de835af0, pid=1389, tid=1712
#
# JRE version: OpenJDK Runtime Environment (11.0.6+10) (build 11.0.6+10-LTS)
# Java VM: OpenJDK 64-Bit Server VM (11.0.6+10-LTS, mixed mode, sharing, tiered, compressed oops, concurrent mark sweep gc, linux-amd64)
# Problematic frame:
# C  [librocksdbjni10001996641283911793.so+0x1aeaf0]  Java_org_rocksdb_RocksIterator_seekToFirst0+0x0
#
# Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /opt/core.1389)
#
# An error report file with more information is saved as:
# /opt/hs_err_pid1389.log

{code}
From the hs_error log file:
{code:java}
---------------  T H R E A D  ---------------Current thread (0x00000000011a4000):  JavaThread ""KeyDeletingService#1"" daemon [_thread_in_native, id=1712, stack(0x00007f19d2443000,0x00007f19d2544000)]Stack: [0x00007f19d2443000,0x00007f19d2544000],  sp=0x00007f19d2541e78,  free space=1019k
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [librocksdbjni10001996641283911793.so+0x1aeaf0]  Java_org_rocksdb_RocksIterator_seekToFirst0+0x0
j  org.rocksdb.AbstractRocksIterator.seekToFirst()V+26
j  org.apache.hadoop.hdds.utils.db.RDBStoreIterator.<init>(Lorg/rocksdb/RocksIterator;)V+13
j  org.apache.hadoop.hdds.utils.db.RDBTable.iterator()Lorg/apache/hadoop/hdds/utils/db/TableIterator;+30
j  org.apache.hadoop.hdds.utils.db.TypedTable.iterator()Lorg/apache/hadoop/hdds/utils/db/TableIterator;+4
j  org.apache.hadoop.ozone.om.OmMetadataManagerImpl.getPendingDeletionKeys(I)Ljava/util/List;+8
j  org.apache.hadoop.ozone.om.KeyManagerImpl.getPendingDeletionKeys(I)Ljava/util/List;+5
j  org.apache.hadoop.ozone.om.KeyDeletingService$KeyDeletingTask.call()Lorg/apache/hadoop/hdds/utils/BackgroundTaskResult;+39
j  org.apache.hadoop.ozone.om.KeyDeletingService$KeyDeletingTask.call()Ljava/lang/Object;+1
J 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]
J 4802 c1 java.util.concurrent.Executors$RunnableAdapter.call()Ljava/lang/Object; java.base@11.0.6 (14 bytes) @ 0x00007f19f0c87214 [0x00007f19f0c870e0+0x0000000000000134]
J 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]
J 4802 c1 java.util.concurrent.Executors$RunnableAdapter.call()Ljava/lang/Object; java.base@11.0.6 (14 bytes) @ 0x00007f19f0c87214 [0x00007f19f0c870e0+0x0000000000000134]
J 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]
J 4954 c1 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run()V java.base@11.0.6 (57 bytes) @ 0x00007f19f0cfe10c [0x00007f19f0cfde40+0x00000000000002cc]

{code}",pull-request-available,['OM HA'],HDDS,Sub-task,Critical,2020-05-21 18:37:17,43
13306559,Maintain FileHandle Information in OMMetadataManager,Maintain FileHandle Information in OMMetadataManager.,pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Major,2020-05-21 18:11:55,44
13306437,Archive jacoco coverage files for unit/integration tests,"Earlier we configured jacoco maven plugin to calculate the code coverage. But the jacoco.exec files (which contain the coverage information) are not stored in the artifacts created by the github actions.

We can add it easily.",pull-request-available,[],HDDS,Improvement,Major,2020-05-21 09:38:19,6
13306427,There is a typo on the documentation website,"[https://hadoop.apache.org/ozone/docs/0.5.0-beta/concept/datanodes.html]


""Once the client is able to locate the contianer, that is, understand which data nodes contain this container""


I think this sentence has a typo, should change the contianer to container",pull-request-available,['documentation'],HDDS,Bug,Minor,2020-05-21 09:30:16,62
13306383,Improve UniqueId.next() efficiency,"Currently,  UniqueId.next leverages Calendar.getInstance(UTC_ZONE).getTimeInMillis() to get a unique key.  Calendar.getInstance(UTC_ZONE) will create an object every time which is not efficient. Use System.currentTimeMillis() will be more efficient.  ",pull-request-available,[],HDDS,Improvement,Major,2020-05-21 07:02:31,5
13306373,HddsDatanodeService cannot be started if HDFS datanode running in same machine with same user.,"since the service names are same and they both referring to same location for pid files, we can not start both services at once.

Tweak is to export HADOOP_PID_DIR to different location after starting one service and start other one.

It would be better to have different pid file names.

 

 
{noformat}
Umas-MacBook-Pro ozone-0.5.0-beta % bin/ozone --daemon start datanode
datanode is running as process 25167.  Stop it first.
{noformat}
 ",Triaged newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2020-05-21 06:09:49,6
13306367,Proto changes and KeyInfo related changes to support fileHandle,Make Proto changes as well KeyInfo related changes to add FileHandle support.,pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Major,2020-05-21 05:42:33,44
13306173,Fix TestBlockOutputStreamWithFailures#test2DatanodesFailure,"[https://github.com/apache/hadoop-ozone/pull/767/checks?check_run_id=691722296] 
{code:java}
3083 at org.junit.Assert.fail(Assert.java:86) 


3084 at org.junit.Assert.assertTrue(Assert.java:41) 


3085 at org.junit.Assert.assertTrue(Assert.java:52) 


3086 at org.apache.hadoop.ozone.client.rpc.TestBlockOutputStreamWithFailures.test2DatanodesFailure(TestBlockOutputStreamWithFailures.java:335)

{code}",pull-request-available,[],HDDS,Sub-task,Major,2020-05-20 11:45:05,16
13306164,Remove FilteredClassloader and replace with maven based hadoop2/hadoop3 ozonefs generation,"As described in the parent issue, the final step is to create a Hadoop independent shaded client and hadoop2/hadoop3 related separated client jars.",pull-request-available,[],HDDS,Sub-task,Blocker,2020-05-20 11:10:22,6
13306022,Improve error message when GC parameters are not set,"Currently when GC parameters or any -XX are not set, it logs 

""No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS

It would be nice to improve this message with settings that are being set.",pull-request-available,[],HDDS,Bug,Major,2020-05-19 22:51:55,13
13306006,Implement getReadCopy in Table.,"Introduce a getReadCopy in table method.

As right now, get when a value exists in the cache it returns the cloned copy, so that when it used during double-buffer flush, if other threads modify the object during the flush time we will see some exceptions which are mentioned in HDDS-2344 and HDDS-2322. To avoid this, all the get() values returned are cloned copy if it exists in the cache.

But for a few of the requests like OMBucketCreateRequest, we need Volume info (OmVolumeArgs, but we don't use this info during double buffer flush, so we can safely get a cached copy without doing a clone.  In this Jira, fixed only Bucket requests. I Will file a new Jira to see where all this new API can be safely used.

",pull-request-available,[],HDDS,New Feature,Major,2020-05-19 20:55:15,13
13305973,Implement rocksdb tool to parse scm db ,This tool parses content from scm.db file and displays specified table contents.,pull-request-available,['Tools'],HDDS,Sub-task,Major,2020-05-19 17:45:22,26
13305925,Separate client/server/admin proto files of HDDS to separated subprojects ,"As described in the parent Jira admin/client/server protocols need different compatibility guarantees. It's better to separate them to separated maven project to make it easy to follow the changes.

I propose to create 3 new projects

hadoop-hdds/interface-client
hadoop-hdds/interface-server
hadoop-hdds/interface-admin

I called it -interface instead of -proto because we can include some basic Java classes not just proto files (for example utilities to serialize/deserialize proto files)

This first patch will include only the move without refactoring any RPC. While the new proto file names represent the proposed naming convention (Datanode/Scm + Server/Client/Admin + additional postfix) the name of the generated classes and the name of the RPC interfaces (Client/Server translator) not yet renamed to make the patch small.

Also: some methods should be moved between admin/server but it can be done in separated issue to make it easier to follow the change.",pull-request-available,[],HDDS,Sub-task,Major,2020-05-19 13:15:52,6
13305782,Call cleanup on tables only when double buffer has transactions related to tables.,"For volume/bucket table currently it is full cache, and we need to cleanup entries only when they are marked for delete.

So, it is unnecessary to call cleanup and waste the CPU resources on OM.

Similarly for other tables, when the double buffer has transaction entries that touch those tables, then only call cleanup.",Triaged pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-05-18 23:44:41,13
13305781,Remove S3Table from OmMetadataManager,"Remove the S3 table, after HDDS-3385, we don't have any use case for S3Table. We can remove this table from OmMetadataManager.",Triaged pull-request-available,[],HDDS,Bug,Major,2020-05-18 23:34:44,13
13305772,Fix JVMPause monitor start in OzoneManager,"Fix JVMPause monitor logic, right now it is started only in restart.

This should be started during OM start, and stopped during OM Stop. In restart() we can start this again.",pull-request-available,[],HDDS,Bug,Major,2020-05-18 22:06:29,13
13305747,Allow mounting bucket under other volume,"Step 2 from S3 [volume mapping design doc|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/design/ozone-volume-management.md#solving-the-mapping-problem-2-4-from-the-problem-listing]:

Implement a bind mount mechanic which makes it possible to mount any volume/buckets to the specific ""s3"" volume.",Triaged pull-request-available,['Ozone Manager'],HDDS,New Feature,Blocker,2020-05-18 19:28:24,1
13305722,Ozone client should not consider closed container error as failure,"ContainerNotOpen exception exception is thrown by datanode when client is writing to a non open container. Currently ozone client sees this as failure and would increment the retry count. If client reaches a configured retry count it fails the write. Map reduce jobs were seen failing due to this error with default retry count of 5.

Idea is to not consider errors due to closed container in retry count. This would make sure that ozone client writes do not fail due to closed container exceptions.
{code:java}
2020-05-15 02:20:28,375 ERROR [main] org.apache.hadoop.ozone.client.io.KeyOutputStream: Retry request failed. retries get failed due to exceeded maximum allowed retries number: 5
java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server e2eec12f-02c5-46e2-9c23-14d6445db219@group-A3BF3ABDC307: Container 15 in CLOSED state
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.setIoException(BlockOutputStream.java:551)
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$3(BlockOutputStream.java:638)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
        at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
        at org.apache.ratis.client.impl.OrderedAsync$PendingOrderedRequest.setReply(OrderedAsync.java:99)
        at org.apache.ratis.client.impl.OrderedAsync$PendingOrderedRequest.setReply(OrderedAsync.java:60)
        at org.apache.ratis.util.SlidingWindow$RequestMap.setReply(SlidingWindow.java:143)
        at org.apache.ratis.util.SlidingWindow$Client.receiveReply(SlidingWindow.java:314)
        at org.apache.ratis.client.impl.OrderedAsync.lambda$sendRequest$9(OrderedAsync.java:242)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.lambda$onNext$0(GrpcClientProtocolClient.java:284)
        at java.util.Optional.ifPresent(Optional.java:159)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.handleReplyFuture(GrpcClientProtocolClient.java:340)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.access$100(GrpcClientProtocolClient.java:264)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:284)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:267)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:436)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInternal(ClientCallImpl.java:658)
...{code}",TriagePending pull-request-available,['Ozone Client'],HDDS,Bug,Critical,2020-05-18 17:24:27,35
13305671,Avoid to use Hadoop3.x IOUtils in Ozone Client ,To support Hadoop 2.x it's better to avoid to use Hadoop 3.x specific utility methods from IOUtils,pull-request-available,[],HDDS,Sub-task,Major,2020-05-18 12:58:44,6
13305660,NPE while process a pipeline report when PipelineQuery absent in query2OpenPipelines,"2020-05-18 19:21:11,171 [EventQueue-PipelineReportForPipelineReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$PipelineReportFromDatanode@501c46e8
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.updatePipelineState(PipelineStateMap.java:380)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.openPipeline(PipelineStateManager.java:132)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.openPipeline(SCMPipelineManager.java:375)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:115)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:83)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:46)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-05-18 11:52:33,69
13305658,Lot of warnings at DN startup,"During DataNode startup, when we replay the edits to the StateMachine, a lot of warnings are emitted to the log depending on the amount of replayed transactions, one warning for each. The warning states the edit is ignored at this point in the org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl class:

{code}
WARN org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl: blockCommitSequenceId 1128422 in the Container Db is greater than the supplied value 1120656. Ignoring it
{code}

I think as this does not show any problem, and there can be an awful lot from this message just spamming the log at startup, we should move this to DEBUG level instead of WARN.
Attaching a PR with the change.",pull-request-available,[],HDDS,Improvement,Minor,2020-05-18 11:46:21,63
13305597,Add datanode port into the printTopology command output,"Now, i start 3 datanode in a node, the 3 datanode use differents port, but i cannot distinguish which one is HEALTHY, which one is not. A port print is necessary.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-05-18 06:36:43,69
13305520,Generate 2.5.0 protobuf classes using protobuf-maven-plugin,"Generate the protobuf 2.5.0 classe using  protobuf-maven plugin, which dynamically downloads the required protobuf executable, than depending on locally installed protoc.",pull-request-available,[],HDDS,Improvement,Major,2020-05-17 17:05:17,84
13305514,Fix KeyInputStream by adding a timeout exception,Fix KeyInputStream by adding a timeout exception,pull-request-available,['test'],HDDS,Bug,Major,2020-05-17 16:23:52,18
13305509,Refactor TestOzoneManagerHA.java into multiple tests to avoid frequent timeout issues,Refactor TestOzoneManagerHA.java into multiple tests to avoid frequent timeout issues,TriagePending pull-request-available,['test'],HDDS,Improvement,Major,2020-05-17 16:03:08,18
13305291,[OFS] Add contract test for HA,"Add contract tests for HA as well.

Since adding HA contract tests will be another ~10 new classes. [~xyao] and I decided to put HA OFS contract tests in another jira.",Triaged pull-request-available,['test'],HDDS,Sub-task,Blocker,2020-05-15 21:44:57,12
13305267,using protobuf maven plugin instead of the legacy protoc executable file ,"Now the first building of a new contributor of Ozone will always failed for missing protoc installed, so he have to install it, it is inconvenient.

As the project now has been using the protobuf maven plugin, but some legacy module using the protoc executable file directly, so i plan to united to using protobuf maven plugin, so we don't need to install protoc manually, it would make new contributor easy to build the repository successfully.",pull-request-available,['build'],HDDS,Improvement,Major,2020-05-15 18:50:37,69
13305266,Clean up unused code after HDDS-2940 and HDDS-2942,"It seems some snippets of code should be removed as HDDS-2940 is committed. Update: Pending HDDS-2942 commit before this can be committed.

For example [this|https://github.com/apache/hadoop-ozone/blob/ffb340e32460ccaa2eae557f0bb71fb90d7ebc7a/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneFileSystem.java#L495-L499]:
{code:java|title=BasicOzoneFileSystem#delete}
    if (result) {
      // If this delete operation removes all files/directories from the
      // parent directory, then an empty parent directory must be created.
      createFakeParentDirectory(f);
    }
{code}

(Found at https://github.com/apache/hadoop-ozone/pull/906#discussion_r424873030)",Triaged pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Blocker,2020-05-15 18:42:49,12
13305176,Add a maven proto file backward compatibility checker into Ozone.,"The plugin proto-backwards-compat-maven-plugin flags incompatible protobuf changes.

cc [~arp] [~avijayan] [~elek]

https://github.com/salesforce/proto-backwards-compat-maven-plugin
and 
https://mvnrepository.com/artifact/com.salesforce.servicelibs/proto-backwards-compatibility

To test, I modified the following code in proto
{code}

diff --git a/hadoop-ozone/csi/src/main/proto/csi.proto b/hadoop-ozone/csi/src/main/proto/csi.proto
index 3bd53a075..b8a984de2 100644
--- a/hadoop-ozone/csi/src/main/proto/csi.proto
+++ b/hadoop-ozone/csi/src/main/proto/csi.proto
@@ -119,7 +119,7 @@ message GetPluginInfoResponse {
   // characters or less, beginning and ending with an alphanumeric
   // character ([a-z0-9A-Z]) with dashes (-), dots (.), and
   // alphanumerics between. This field is REQUIRED.
-  string name = 1;
+  string newName = 1;
{code}

and this flagged the following change
{code}
[ERROR] CONFLICT: ""GetPluginInfoResponse"" field: ""name"" has been removed, but is not reserved [csi.proto]
[ERROR] CONFLICT: ""GetPluginInfoResponse"" field: ""newName"" ID: 1 has an updated name, previously ""name"" [csi.proto]
{code}
",pull-request-available,[],HDDS,Improvement,Major,2020-05-15 11:08:45,18
13305064,Recon UI: Add visualization for file size distribution ,Recon has an API endpoint to get file size distribution in Ozone. Add visualization in Recon UI for this using histograms.,TriagePending,['Ozone Recon'],HDDS,Task,Major,2020-05-14 22:19:35,37
13304880,OM HA can be started with 3 isolated LEADER instead of one OM ring,"Steps to reproduce:

Imagine that I have 3 different om with the following DNS names:

{code}
ozone-om-0.ozone-om
ozone-om-1.ozone-om
ozone-om-2.ozone-om
{code}

I configured the three hosts as the following:

{code}
  OZONE-SITE.XML_ozone.om.nodes.omservice: om1,om2,om3
  OZONE-SITE.XML_ozone.om.address.omservice.om1: ozone-om-0
  OZONE-SITE.XML_ozone.om.address.omservice.om2: ozone-om-1
  OZONE-SITE.XML_ozone.om.address.omservice.om3: ozone-om-2
  OZONE-SITE.XML_ozone.om.ratis.enable: ""true""
{code}

But unfortunately the DNS is not reliable. All the hosts can resolve only the LOCAL hostname.

OMHANodeDetails.java ignores ALL the configuration which are not resolvable:

{code}
 if (!addr.isUnresolved()) {
          if (!isPeer && OmUtils.isAddressLocal(addr)) {
            localRpcAddress = addr;
            localOMServiceId = serviceId;
            localOMNodeId = nodeId;
            localRatisPort = ratisPort;
            found++;
          } else {
            // This OMNode belongs to same OM service as the current OMNode.
            // Add it to peerNodes list.
            // This OMNode belongs to same OM service as the current OMNode.
            // Add it to peerNodes list.
            peerNodesList.add(getHAOMNodeDetails(conf, serviceId,
                nodeId, addr, ratisPort));
          }
        }
{code}

As a result I will have 3 running server but each has 1 one-node Ratis ring (peerNodesList is empty as only the local hostname can be resolved).

Group ID is the same for all. But they have separated database and they work as separated OM which is VERY dangerous.

 1. Option one: we can accept any unresolved address and retry with connection create if it couldn't be connected

2. Option two: at least the error handling should be fixed. When I configured 3 om, there supposed to be 3 om.",Triaged pull-request-available,[],HDDS,Improvement,Critical,2020-05-14 08:46:53,43
13304836,Make replicationfactor can be set as a number,"Now replicationfactor is an enum, which has only two option ONE and THREE, i aim to refactor it into a number, and keep compatible with the earlier client version.

This step is a beginning and make it possible to config any number for replication",pull-request-available,[],HDDS,Sub-task,Major,2020-05-14 05:01:18,69
13304455,Implement ofs://: Override getTrashRoot,"[~pifta] found if we delete file with Hadoop shell, namely {{hadoop fs -rm}}, without {{-skipTrash}} option, the operation would fail in OFS due to the client is renaming the file to {{/user/<username>/.Trash/}} because renaming across different buckets is not allowed in Ozone. (Unless the file happens to be under {{/user/<username>/}}, apparently.)

We could override {{getTrashRoot()}} in {{BasicOzoneFileSystem}} to a dir under the same bucket to mitigate the problem. Thanks [~umamaheswararao] for the suggestion.

This raises one more problem though: We need to implement trash clean up on OM. Opened HDDS-3575 for this.

CC [~arp] [~bharat]",Triaged pull-request-available,[],HDDS,Sub-task,Major,2020-05-12 20:53:40,12
13304364,Make sure ozone. administrators have access to all keys when acl enabled. ,"After our cluster is acl-enabled, the original user cannot continue to access his or her key. Because the acl of the old key (created by s3g) has been set to start the user for s3g.

For access all the keys, we need to increase the user's access to all the keys. Usually there are a lot of keys in a cluster, so it's very difficult to add permissions to all keys. 

For now, we're trying to use Administrators, but I found that the current ozone Administrators cannot access all keys. Administrators of ozone are also checked for permissions. In HDFS, dfs.cluster.administrators can able to access all files. ",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2020-05-12 13:34:19,4
13304224,Recon: Display leader count in Datanodes page,"Datanodes page should have ""Leader Count"" column that displays the leader count for how many Ratis pipelines the given datanode is elected as a leader.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-05-11 22:28:21,37
13304193,Add Recon UI lint checks and unit tests to Github Actions CI workflow,"Recon UI unit tests and linter checks should be added to Github actions CI. 

Optimization: Try to add a workflow in such a way that these checks are run only if there is a change related to Recon UI files. ",TriagePending,['Ozone Recon'],HDDS,Task,Major,2020-05-11 19:38:35,37
13304015,Update Ozone to latest Ratis Snapshot(0.6.0-1720b1f-SNAPSHOT),Update Ozone to Latest Ratis snapshot(0.6.0-1720b1f-SNAPSHOT),Triaged pull-request-available,[],HDDS,Bug,Blocker,2020-05-11 05:58:10,18
13303985,Make /s3v volume configurable  instead of constant.,"HDDS-3385 has a big change to s3g for creating buckets. At present, the volume is fixed as s3v. This should be user-configurable to better accommodate the older cluster (0.5).

Our production cluster will encounter the problem that the old s3g volume could not find after rebase master. So we are very eager to solve this problem.",pull-request-available,[],HDDS,Improvement,Major,2020-05-11 02:39:21,4
13303924,Datanodes should send ICR when a container replica deletion is successful,"Whenever a datanode executes the delete container command and deletes the container replica, it has to immediately send an ICR to update the container replica state in SCM.",Triaged pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2020-05-10 10:24:06,19
13303590,HDDS-3560 OMFailoverProxyProvider throws IllegalAccessError when assigning proxy with Hadoop version < 3.2 ,"The method createOMProxyIfNeeded of OMFailoverProxyProvider will throw IllegalAccessError when work with hadoop version lower than 3.2. 

The following is the problem code, it want assign a proxy to `proxyInfo.proxy`.
{code:java}
  private void createOMProxyIfNeeded(ProxyInfo proxyInfo,
      String nodeId) {
    if (proxyInfo.proxy == null) {
      InetSocketAddress address = omProxyInfos.get(nodeId).getAddress();
      try {
        proxyInfo.proxy = createOMProxy(address);
      } catch (IOException ioe) {
        LOG.error(""{} Failed to create RPC proxy to OM at {}"",
            this.getClass().getSimpleName(), address, ioe);
        throw new RuntimeException(ioe);
      }
    }
  }
{code}

But,  hadoop lower than 3.2, the field proxy of proxyInfo is marked as final. 

{code:java}
public static final class ProxyInfo<T> {
    public final T proxy;
    /*
     * The information (e.g., the IP address) of the current proxy object. It
     * provides information for debugging purposes.
     */
    public final String proxyInfo;
    ...
}
{code}

So, if we want to work around, we should  catch the exception and new a ProxyInfo instance instead.
",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Bug,Major,2020-05-08 09:54:31,69
13303461,Recon UI: Add strict linter rules to improve code quality,Add a linter like ([xojs|https://github.com/xojs/xo]) and fix all linter errors and warnings.,pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-05-07 18:12:37,37
13303379,Provide generic introduction / deep-dive slides as part of the documentation,"I think it's good to have a generic overview  of Ozonein presentation format and shared with the community. It can be used anytime by anybody to share details of Ozone on any meetup / conferences.

I am not sure what is the best place for this but documentation seems to be a natural choice:

 * This is version dependent (can be updated with new releases)
 * Topics and diagrams can be shared between documentations",pull-request-available,['documentation'],HDDS,Improvement,Major,2020-05-07 12:18:10,6
13303366,Refactor configuration in SCMRatisServer to Java-based configuration,[https://cwiki.apache.org/confluence/display/HADOOP/Java-based+configuration+API],pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-05-07 11:27:03,77
13303249,Multipart Upload Failed because partName mismatch, !screenshot-1.png! ,TriagePending,['S3'],HDDS,Bug,Major,2020-05-07 02:13:59,65
13303248,Remove unnecessary hard-code configuration in S3Gateway,"{code:java}
public Void call() throws Exception {
 OzoneConfiguration ozoneConfiguration = createOzoneConfiguration();
 TracingUtil.initTracing(""S3gateway"", ozoneConfiguration);
 OzoneConfigurationHolder.setConfiguration(ozoneConfiguration);
 // remove this hard-code line.
 ozoneConfiguration.set(""hadoop.http.authentication.type"", ""simple"");
 httpServer = new S3GatewayHttpServer(ozoneConfiguration, ""s3gateway"");
 start();
 return null;
}{code}",pull-request-available,[],HDDS,Improvement,Minor,2020-05-07 02:04:56,85
13303128,OzoneFS is slow compared to HDFS using Spark job,"Reported by ""Andrey Mindrin"" on the the-asf Slack:

{quote}
We have made a few tests to compare OZONE (0.4 and 0.5 on Cloudera Runtime 7.0.3 with 3 nodes) performance with HDFS and OZONE is slower in most cases. For example, Spark application with 18 containers that copies 6 Gb parquet file is about 50% slower on OzoneFS. The only one case shows the same performance - Hive queries over partitioned tables.

 simple spark code we used:

{code}
val file = spark.read.format(format).load(path_input)
file.write.format(format).save(path_output)
{code}

Tested on CSV file with 800 million records, 2 columns and parquet file converted from CSV above. Just copied file from HDFS to HDFS and from Ozone to Ozone. Application time is 1m 14s on HDFS and  1m 51s (+50%) on Ozone (parquet file). Ozone has default settings. (edited) 
{quote}",performance,[],HDDS,Improvement,Major,2020-05-06 14:31:45,6
13303106,Fix TestReadRetries,"{code:java}
-----------------------------------------------------------------Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 55.385 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestReadRetriestestPutKeyAndGetKeyThreeNodes(org.apache.hadoop.ozone.client.rpc.TestReadRetries)  Time elapsed: 10.086 s  <<< FAILURE!java.lang.AssertionError: expected same:<OPEN> was not:<CLOSING> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotSame(Assert.java:737) at org.junit.Assert.assertSame(Assert.java:680) at org.junit.Assert.assertSame(Assert.java:691) at org.apache.hadoop.ozone.client.rpc.TestReadRetries.testPutKeyAndGetKeyThreeNodes(TestReadRetries.java:181)
{code}",pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2020-05-06 12:57:02,16
13303066,TestKeyInputStream#testSeek fails intermittently,The failure happens due to mismatch in metric count. The metric compared is wrong. It should use a count metric but it used pendingOpsCount metric.,pull-request-available,['test'],HDDS,Bug,Major,2020-05-06 09:19:41,35
13303042,The default config items of ScmClientConfig don't appear in the ozone-filesystem-xxx-jar,"for example `scm.container.client.idle.threshold` should in the default config file of ozone, Without it, the following logic won't work well while put a key.


{code:java}
    long staleThresholdMs = clientConf.getStaleThreshold(MILLISECONDS);
    this.clientCache = CacheBuilder.newBuilder()
        .expireAfterAccess(staleThresholdMs, MILLISECONDS)
        .maximumSize(clientConf.getMaxSize())
{code}

because the staleThresholdMs is 0, if we don't config it, it should return the default value.
",Triaged pull-request-available,"['Ozone Filesystem', 'SCM Client']",HDDS,Bug,Major,2020-05-06 07:37:15,85
13302964,Remove unused joda-time,Joda-time is defined in the pom.xml but it's not used anywhere. It should be easy to remove it without problems.,Triaged,[],HDDS,Task,Major,2020-05-05 20:21:49,71
13302789,ozone chunkinfo CLI cannot connect to OM when run from NON OM leader client node,"When ozone debug chunkinfo command is run from a client node where OM leader is not present, the operation fails.
 When run from the client node where OM leader is present, it works fine
{quote}/opt/cloudera/parcels/CDH/bin/ozone debug chunkinfo o3://ozone1/vol1/buck1/file1 20/04/21 11:04:02 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 20/04/21 11:04:03 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 20/04/21 11:04:04 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 20/04/21 11:04:05 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) 20/04/21 11:04:06 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)
{quote}",TriagePending pull-request-available,[],HDDS,Bug,Major,2020-05-05 05:58:05,26
13302619,Add a freon generator to create directory tree and create files in each directory,"This Jira proposes to add a functionality to freon tool to create directories in tree structure. 

*Proposed structure:*
{code:java}
  /*
      Nested directories will be created like this,
      suppose you pass depth=3, span=3 and number of tests=2

      Directory Structure:-
                            |-- Dir111
                            |
                |-- Dir11 --|-- Dir112
                |           |
                |           |-- Dir113
                |
                |
                |           |-- Dir121
                |           |
       Dir1   --|-- Dir12 --|-- Dir122
                |           |
                |           |-- Dir123
                |
                |
                |           |-- Dir131
                |           |
                |-- Dir13 --|-- Dir132
                            |
                            |-- Dir133

     In each directory 'c' number of files to be written in each directory with file size 'g' will be created.
   */
{code}
This is basically useful to determine the performance evaluation of ozone, when creating defined number of nested directories and also create given number of files into each directory.",Triaged pull-request-available,['Tools'],HDDS,Improvement,Major,2020-05-04 11:31:02,40
13302613,Add a directory based Ozone Manager LoadGenerator,Most of the LoadGenerators in MiniOzoneLoadGenerator are all based on the read/write path. The purpose of this LoadGenerator is to add a directory-based load generator which stresses the IN.,pull-request-available,['test'],HDDS,Bug,Major,2020-05-04 10:37:39,18
13302149,Ensure consistent OM token service field in HA environment,"Currently OMFailoverProxyProvider#computeDelegationTokenService calculate the canonical token service name based on the enumeration order of the configured OM instances. An example service field can be like TS1: ""om1addr:port,om2addr:port,om3addr:port""

This could be problematic
1) clients have different omId to omRpcAddresses mappings
2) configuration enumeration orders are different among clients

Depend on the client configuration and enumeration order, the client may got its canonnical token service in different order like TS2: ""om2addr:port,om1addr:port,om3:addr:port""

MR/Yarn/Spark on Yarn relies on token service as key to check the UGI credential when building token cache map. When client got TS2 even though it has an OM token with TS1, client will try to collect OM token again. This will not work in YARN container (e.g., Spark on Yarn cluster mode) which may not have the kerberos ticket to fetch the token.

The proposed fix it to provide a consistent canonical token service for all OM clients in order.

 
",pull-request-available,['Security'],HDDS,Bug,Major,2020-04-30 16:37:55,28
13301996,Fix Memory leak of RaftServerImpl,"This depends on [RATIS-845|https://issues.apache.org/jira/browse/RATIS-845], find the details in RATIS-845.",Triaged pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-04-30 06:18:42,65
13301988,Add OzoneConfiguration to UGI when startup S3Gateway,"OzoneConfiguration missing load ozone-site.xml by default, this may cause some issues when setup a secure ozone cluster.

When we start a S3Gateway in secure mode (Kerberos), when start S3Gateway http server, it will use UserGroupInformation to check whether in security mode, which will load Configuration to check whether ""hadoop.security.authentication"" set to ""KERBEROS"". but unfortunately, default configuration will only load ""core-site.xml, core-default.xml, hdfs-site.xml, hdfs-default.xml ozone-default.xml"" and missing *{color:#ff0000}ozone-site.xml, {color} it*{color:#ff0000} {color:#172b4d}means we have to configure ""hadoop.security.authentication"" in one of default 5 config files if we want to start a secure S3Gateway.{color}{color}

It's better to add ozone-site.xml into OzoneConfiguration by default, so we don't need to make one same configuration in different part.",pull-request-available pull-requests-available,['S3'],HDDS,Improvement,Minor,2020-04-30 05:29:30,85
13301976,s3g multi-upload saved content incorrect when client uses aws java sdk 1.11.* version,"The default multi-part size  is 5MB, which is 5242880 byte, while all the chunks saved by s3g is 5246566 byte which is greater than 5MB.

By looking into the ObjectEndpoint.java, it seems the chunk size is retrieved from the ""Content-Length"" header. 

 ",TriagePending pull-request-available,['S3'],HDDS,Bug,Blocker,2020-04-30 04:45:16,5
13301886,Fix definition of DelegationTokenTable in OmMetadataManagerImpl,"The definition of [dTokenTable|https://github.com/apache/hadoop-ozone/blob/876bec0130094b24472a7017fdb1fd81a65023bc/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OmMetadataManagerImpl.java#L115] should be fixed.

And IMHO it could be OzoneTokenID -> renew_time.",newbie pull-request-available,[],HDDS,Improvement,Major,2020-04-29 18:10:05,42
13301792,Upgrade Ratis to 0.6.0-2816ea6-SNAPSHOT,Upgrade Ratis to {{0.6.0-2816ea6-SNAPSHOT}} to get the fix for RATIS-840.,pull-request-available,[],HDDS,Task,Critical,2020-04-29 13:02:44,1
13301707,Disable Jaeger tracing by default.,"Jaeger tracing is enabled by default, lets disable it.",pull-request-available,[],HDDS,Bug,Major,2020-04-29 05:15:43,16
13301577,TestOzoneFileInterfaces is flaky,"TestOzoneFileInterfaces.testOzoneManagerLocatedFileStatusBlockOffsetsWithMultiBlockFile is flaky and failed multiple times on master:

{code}
./2020/04/24/822/it-filesystem/hadoop-ozone/integration-test/org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.txt
./2020/04/24/822/it-filesystem/hadoop-ozone/integration-test/TEST-org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.xml
./2020/04/24/822/it-filesystem/output.log
./2020/04/27/830/it-filesystem/hadoop-ozone/integration-test/org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.txt
./2020/04/27/830/it-filesystem/hadoop-ozone/integration-test/TEST-org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.xml
./2020/04/27/830/it-filesystem/output.log
./2020/04/28/831/it-filesystem/hadoop-ozone/integration-test/org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.txt
./2020/04/28/831/it-filesystem/hadoop-ozone/integration-test/TEST-org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.xml
./2020/04/28/831/it-filesystem/output.log
./2020/04/28/833/it-filesystem/hadoop-ozone/integration-test/org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.txt
./2020/04/28/833/it-filesystem/hadoop-ozone/integration-test/TEST-org.apache.hadoop.fs.ozone.TestOzoneFileInterfaces.xml
./2020/04/28/833/it-filesystem/output.log
{code}

I am disabling it until the fix",TriagePending flaky-test ozone-flaky-test,['test'],HDDS,Sub-task,Critical,2020-04-28 13:53:44,63
13301573,Topology related acceptance test is intermittent,"It's failed multiple times on master and PRs. See the archive of the builds results at: https://github.com/elek/ozone-build-results 

I am disabling it to avoid flaky runs, but we need to fix and re-enable it.

{code}
./find-first.sh ""Test timeout 5 minutes exceeded."" --include=""robot-ozone-topology-ozone-topology-basic-scm.xml""
2020/04/17/789/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<msg timestamp=""20200417 18:06:32.549"" level=""FAIL"">Test timeout 5 minutes exceeded.</msg>
2020/04/17/789/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<status status=""FAIL"" starttime=""20200417 18:01:32.547"" endtime=""20200417 18:06:32.550"" critical=""yes"">Test timeout 5 minutes exceeded.</status>
2020/04/17/791/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<msg timestamp=""20200417 18:55:52.135"" level=""FAIL"">Test timeout 5 minutes exceeded.</msg>
2020/04/17/791/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<status status=""FAIL"" starttime=""20200417 18:50:52.134"" endtime=""20200417 18:55:52.135"" critical=""yes"">Test timeout 5 minutes exceeded.</status>
2020/04/20/792/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<msg timestamp=""20200420 12:16:47.410"" level=""FAIL"">Test timeout 5 minutes exceeded.</msg>
2020/04/20/792/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<status status=""FAIL"" starttime=""20200420 12:11:47.409"" endtime=""20200420 12:16:47.411"" critical=""yes"">Test timeout 5 minutes exceeded.</status>
2020/04/21/803/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<msg timestamp=""20200422 00:10:32.971"" level=""FAIL"">Test timeout 5 minutes exceeded.</msg>
2020/04/21/803/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<status status=""FAIL"" starttime=""20200422 00:05:32.970"" endtime=""20200422 00:10:32.972"" critical=""yes"">Test timeout 5 minutes exceeded.</status>
2020/04/27/830/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<msg timestamp=""20200427 22:05:52.823"" level=""FAIL"">Test timeout 5 minutes exceeded.</msg>
2020/04/27/830/acceptance/robot-ozone-topology-ozone-topology-basic-scm.xml:<status status=""FAIL"" starttime=""20200427 22:00:52.822"" endtime=""20200427 22:05:52.824"" critical=""yes"">Test timeout 5 minutes exceeded.</status>
First failed commit: 3bb5838196536f2ea4ac1ab4dcd0bc53ae97f7e0
{code}
",pull-request-available,[],HDDS,Bug,Critical,2020-04-28 13:48:07,11
13301548,Remove dependence on commons-lang,"Few parts of Ozone still use {{commons-lang}}, while most are already on {{commons-lang3}}.  Let's update those remaining usages and remove the unnecessary dependency.",pull-request-available,[],HDDS,Improvement,Minor,2020-04-28 12:07:51,1
13301545,OzoneFileStatus should not extend FileStatus,"FileStatus is a Hadoop specific class. The return type of getFileStatus OM call should be Hadoop independent and a simple POJO can be used.

OzoneFileSystem can create the appropriate FileStatus implementation based on the information in this simple pojo.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-04-28 11:50:43,6
13301543,Hide OMFailoverProxyProvider usage behind an interface,"OzoneManagerProtocolClientSideTranslatorPB uses OmFailoverProxyProvider to access OM HA, but this class is not supported in Hadoop 2.x environment.

It would be better to 
 1. separated ProtocolClientSideTranslator from the transport layer logic
 2. Remove implementation specific method from the OzoneManagerProtocol (getOMFailoverProxyProvider should be removed)
 3. Use a simple OMTransport interface to handle all the connection logic in one, isolated place",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-04-28 11:49:09,6
13301487,Address compatibility issue by SCM DB instances change,"After https://issues.apache.org/jira/browse/HDDS-3172, SCM now has one single rocksdb instance instead of multiple db instances. 

For running Ozone cluster, we need to address compatibility issues. One possible way is to have a side-way tool to migrate old metadata from multiple dbs to current single db.",Triaged,['SCM'],HDDS,Bug,Blocker,2020-04-28 08:39:19,6
13301389,Implement ofs://: Support volume and bucket deletion,"Question:
- Do we need to support deleting volumes and buckets in OFS? In OFS the first level directories are volumes (with the exception of tmp mount), the second level directories are buckets.
  - If the answer is yes, do we need to add this before merging to master branch?
Thanks!

Related: HDDS-2969 (Add contract test)

CC [~stevel@apache.org] [~arp]",pull-request-available,[],HDDS,Sub-task,Major,2020-04-27 21:12:04,12
13301286,Refactor Failures in MiniOzoneChaosCluster into pluggable model.,"Refactor Failures in MiniOzoneChaosCluster into pluggable model, so that more failures can be added later.",pull-request-available,['test'],HDDS,Bug,Major,2020-04-27 14:00:49,18
13301261,cp command not working on Ozone mounted using hdfs-fuse.,"'cp' (copy) command doesn’t work with hdfs-fuse file system. It works by passing 'cp' command two times for the same file. It mainly happens when destination file doesn’t exists. It happens because when 'cp' command is performed for the first time it creates the file at the destination path, but as per ozone file system protocols any file has no access till its committed. So, 'getattr' function is not able to locate the file(though it’s present at the destination path). Hence, 'cp' command fails in the first go. Second time when 'cp' command is executed, file is already present in the destination path so 'cp' command simply override the data in the file.",pull-request-available,[],HDDS,Bug,Major,2020-04-27 12:50:21,27
13301221,Remove avro as a dependency in Ozone,Remove avro as a depenency in Ozone,pull-request-available,[],HDDS,Bug,Major,2020-04-27 10:27:00,18
13301167,Remove unnecessary jackson dependencies from Ozone,Remove unnecessary jackson dependencies from Ozone,pull-request-available,[],HDDS,Bug,Major,2020-04-27 06:26:17,18
13300907,Ozone start fails with NullPointerException in TLS enabled cluster,"{code}
	
SCM start failed with exception
java.lang.NullPointerException
	at org.apache.hadoop.hdds.conf.ConfigurationSource.getPassword(ConfigurationSource.java:112)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.getPassword(BaseHttpServer.java:348)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.loadSslConfToHttpServerBuilder(BaseHttpServer.java:311)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.newHttpServer2BuilderForOzone(BaseHttpServer.java:179)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.<init>(BaseHttpServer.java:95)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerHttpServer.<init>(StorageContainerManagerHttpServer.java:33)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:334)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:215)
{code}",pull-request-available,[],HDDS,Bug,Critical,2020-04-24 23:36:02,30
13300904,Recon cannot track missing containers that were created and went missing while it is down.,"Since Recon relies on Container reports from DNs to track replicas, if a new container was created and went missing when Recon was down, it will not be able to tag it as a missing container. ",TriagePending,['Ozone Recon'],HDDS,Bug,Major,2020-04-24 23:14:10,44
13300509,Handle unhealthy replica state of an open container,"It was assumed that when a container is in OPEN state SCM doesn't have to take any action on it and the Ratis pipeline in datanode should handle the consistency of that container.

But, there are cases where a replica of a container which is in OPEN state can be marked as UNHEALTHY in datanode. This should be handled in SCM.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-04-23 09:35:09,19
13300496,"Impl getUriDefaultPort to BasicOzFsOzFs, return -1 forever to pass the checkPath","I test if yarn can use o3fs as its shared filesystem.
core-site.xml

{code:xml}
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>o3fs://mybucket.myvol/</value>
  </property>
<property>
  <name>fs.o3fs.impl</name>
  <value>org.apache.hadoop.fs.ozone.BasicOzoneFileSystem</value>
</property>
<property>
  <name>fs.AbstractFileSystem.o3fs.impl</name>
  <value>org.apache.hadoop.fs.ozone.BasicOzFs</value>
</property>
</configuration>
{code}

execute the following command 
{code:bash}
bin/yarn jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 1000 /input13
{code}

The following is error log

2020-04-23 15:15:04,544 [main] INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1587619434776_0007
org.apache.hadoop.fs.InvalidPathException: Invalid path name Wrong FS: o3fs://mybucket.myvol/tmp/hadoop-yarn/staging/root/.staging/job_1587619434776_0007, expected: o3fs://mybucket.myvol/
        at org.apache.hadoop.fs.AbstractFileSystem.checkPath(AbstractFileSystem.java:390)
        at org.apache.hadoop.fs.AbstractFileSystem.resolvePath(AbstractFileSystem.java:466)
        at org.apache.hadoop.mapred.YARNRunner.createApplicationSubmissionContext(YARNRunner.java:348)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:285)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:240)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
        at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:301)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:305)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

The following code logic don't make sense, but i think we can impl the method `getDefaultPort` and let it return -1 forever to work around. Maybe there are some better approach.

 !screenshot-1.png! 

",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-04-23 08:43:18,69
13300480,SCM ask too many datanodes to replicate the same container,"*What's the problem ?*
As the image shows,  scm ask 31 datanodes to replicate container 2037 every 10 minutes from 2020-04-17 23:38:51.  And at 2020-04-18 08:58:52 scm find the replicate num of container 2037 is 12, then it ask 11 datanodes to delete container 2037. 
 !screenshot-1.png! 
 !screenshot-2.png! 
*What's the reason ?*

scm check whether  (container replicates num + inflightReplication.get(containerId).size() - inflightDeletion.get(containerId).size()) is less than 3. If less than 3, it will ask some datanode to replicate the container, and add the action into inflightReplication.get(containerId). The replicate action time out is 10 minutes, if action timeout, scm will delete the action from inflightReplication.get(containerId) as the image shows. Then (container replicates num + inflightReplication.get(containerId).size() - inflightDeletion.get(containerId).size()) is less than 3 again, and scm ask another datanode to replicate the container.
Because replicate container cost a long time,  sometimes it cannot finish in 10 minutes, thus 31 datanodes has to replicate the container every 10 minutes.  19 of 31 datanodes replicate container from the same source datanode,  it will also cause big pressure on the source datanode and replicate container become slower. Actually it cost 4 hours to finish the first replicate. 
 !screenshot-4.png! ",Triaged pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-04-23 08:07:02,65
13300468,TestSCMNodeMetrics is flaky,"It failed without any reason during a a master build

[https://github.com/elek/ozone-build-results/blob/master/2020/04/22/808/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestSCMNodeMetrics.txt]

Checking it closely, I can't see any reason to have a MiniOzoneCluster there as it can be a simple unit test. The easiest fix seems to be a conversion to real unit test.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-04-23 07:27:46,6
13300173,Disable partial chunk write during flush() call in ozone client by default,"Currently, Ozone client flushes the partial chunks as well during flush() call by default.

[https://github.com/apache/hadoop-ozone/pull/716] proposes to add a configuration to disallow partial chunk flush during flush() call. This Jira aims to enable the config on by default to mimic the default hdfs flush() behaviour and fix any failing unit tests associated with the change.",Triaged pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-04-22 06:06:38,4
13300115,Use persisted transaction info during OM startup in OM StateMachine,HDDS-3475 persisted transaction info into DB. This Jira is to use transactionInfo persisted to DB during OM startup. ,Triaged pull-request-available,['OM HA'],HDDS,Sub-task,Critical,2020-04-21 23:34:54,13
13300063,Use transactionInfo table to persist transaction information,"This Jira is to flush the transaction information of the term and last transaction index applied during double buffer flush to OM DB.
",pull-request-available,[],HDDS,Sub-task,Major,2020-04-21 19:04:02,13
13300046,Create transactionInfo Table in OmMetadataManager,"This Jira is to create a transaction info table which stores the current term and last transaction index applied to DB.
*In this Jira following will be done:*
1. introduce a new transaction info table which stores transactionInfo.
Key = TRANSACTIONINFO
value = currentTerm-transactionIndex
2. Add new UT's for this table.
3. Provide utility/helper methods to parse the transaction info table value",pull-request-available,[],HDDS,Sub-task,Major,2020-04-21 18:00:55,13
13300030,ozone chunkinfo CLI should display block file path info,"sample chunkinfo output :

{
 ""container Id :12blockId :104035271344128004"": {
 ""chunks"": [
 ""/hadoop-ozone/datanode/data/hdds/3470c13f-f220-4d1c-9630-91d6aa4d526b/current/containerDir0/12/chunks/104035271344128004_chunk_1""
 ],
 ""chunkDataNodeDetails"": [

{ ""ipAddress"": ""<ip_1>"", ""hostName"": ""<replica_1>"" }

,

{ ""ipAddress"": ""<ip_2>"", ""hostName"": ""<replica_2>"" }

,

{ ""ipAddress"": ""<ip_3>"", ""hostName"": ""<replica_3>"" }

],
 ""pipelineID"": ""ee66cbad-3cc4-4d93-ae6c-b66a08c6cb2b""
 }
 }

 

Here, it displays chunk info path which actually does not exist.

The CLI should also display the block info path.

i.e, 
{noformat}
ls -l /hadoop-ozone/datanode/data/hdds/3470c13f-f220-4d1c-9630-91d6aa4d526b/current/containerDir0/12/chunks/
total 8
-rw-r--r-- 1 hdfs hdfs 5092 Apr 21 06:57 104035271344128004.block{noformat}
 

Other trivial issue is :

There should be space or newline between container Id value and blockId
{noformat}
""container Id :12blockId :104035271344128004"":{noformat}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-04-21 16:53:24,26
13299973,Update to latest Ratis Snapshot 0.6.0-490b689-SNAPSHOT,Update ozone to latest ratis snapshot.,pull-request-available,['build'],HDDS,Bug,Major,2020-04-21 11:44:03,16
13299906,Add third party jar versions as properties in pom.xml,"Currently, many third library dependencies are hardcoded in the dependency tag in the pom file. Idea is to re-organize the structure a bit by defining the jar version as a property in the pom file and reuse the defined version in the dependency tag

as done in https://issues.apache.org/jira/browse/HDDS-3468",pull-request-available,[],HDDS,Bug,Major,2020-04-21 08:03:55,1
13299898,Organize log4j dependency in pom.xml,"Currently, dependency of log4j in ozone is added as following:
{code:java}
<dependency>
  <groupId>log4j</groupId>
  <artifactId>log4j</artifactId>
  <version>1.2.17</version>
{code}
Idea here is to add log4j.version as a property in pom.xml and reuse the same while defining the dependency.",pull-request-available,['build'],HDDS,Bug,Major,2020-04-21 07:39:24,16
13299897,OM Client RPC failover retries happening more than configured. ,"Currently OM client is retrying more than configured.

Example if we configure MaxFailover times 2, it will try 3 times. Following log shows that.

{quote}{{2020-04-21 00:12:13,908 [Thread-0] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: ""21637.local/192.168.0.12""; destination host is: ""localhost"":12944; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking $Proxy43.submitRequest over nodeId=omNode-3,nodeAddress=127.0.0.1:12944. Trying to failover immediately.
2020-04-21 00:12:13,909 [Thread-0] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: ""21637.local/192.168.0.12""; destination host is: ""localhost"":12932; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking $Proxy43.submitRequest over nodeId=omNode-1,nodeAddress=127.0.0.1:12932 after 1 failover attempts. Trying to failover immediately.
2020-04-21 00:12:13,910 [Thread-0] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: ""21637.local/192.168.0.12""; destination host is: ""localhost"":12938; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking $Proxy43.submitRequest over nodeId=omNode-2,nodeAddress=127.0.0.1:12938 after 2 failover attempts. Trying to failover immediately.}}{quote}",pull-request-available,['Ozone Client'],HDDS,Bug,Minor,2020-04-21 07:33:00,8
13299829,OM Failover retry happens too quickly when new leader suggested and retrying on same OM,"When OM throws No leader exception with suggested leader.

Client side failover happens too quickly.

Incremental timeouts does not kick in this flow as we don't update lastOM/currentOM ids in this flow.

 

 ",pull-request-available,['OM HA'],HDDS,Bug,Blocker,2020-04-20 23:19:53,8
13299774,Use dedicated build partition for acceptance tests in github actions environment,Even with HDDS-3456 it seems to be a good idea to run acceptance test from /mnt in the github actions environment where we have dedicated 14GB space.,pull-request-available,[],HDDS,Improvement,Major,2020-04-20 18:16:17,6
13299773,Refactor OFSPath to adapt to master branch,See PR description: https://github.com/apache/hadoop-ozone/pull/847,pull-request-available,[],HDDS,Sub-task,Major,2020-04-20 18:03:46,12
13299687,Add acceptance test to smoketest CSI service startup,"Ozone CSI service implement Container Storage Interface specification to provide volumes for container orchestrators such as Yarn and Kubernetes.

We don't have any acceptance test which makes easy to break the classpath / functionality.

As a first step I would create a simple smoketest to check if the csi service can be started and the identitiy service can be called.",pull-request-available,[],HDDS,Improvement,Major,2020-04-20 12:56:41,6
13299599,Support Hadoop 2.x with build-time classpath separation instead of isolated classloader,"Apache Hadoop Ozone is a Hadoop subproject. It depends on the released Hadoop 3.2. But as Hadoop 3.2 is very rare in production, older versions should be supported to make it possible to work together with Spark, Hive, HBase and older clusters.

Our current approach is using classloader based separation (ozonefs ""legacy"" jar), which has multiple problems:

 1. It's quite complex and hard to debug
 2. It couldn't work together with security

The issue proposes to use a different approach
 1. Reduce the dependency on Hadoop (including the replacement of hadoop metrics and cleanup of the usage of configuration)
 2. Create multiple version from ozonefs-client with different compile time dependency. ",Triaged,['build'],HDDS,Improvement,Blocker,2020-04-20 07:47:20,6
13299239,Use UrlConnectionFactory to handle HTTP Client SPNEGO for ozone components,"Some of the places that need to be fixed, otherwise those http client won't be able to access the endpoints when SPNEGO is enabled on the server side. 

ReconUtils#makeHttpCall
OzoneManagerSnapshotProvider#getOzoneManagerDBSnapshot

The right API to use should be URLConnectionFactory
public URLConnection openConnection(URL url, boolean isSpnego)

The isSpnego should be based on OzoneSecurityUtil.isHttpSecurityEnabled()",pull-request-available,[],HDDS,Bug,Major,2020-04-17 22:09:58,28
13299185,Add timeout to XceiverServerRatis#submitRequest call,"There are cases where {{org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis#submitRequest}} call waits forever blocking the thread which calls it.

This is causing a problem with the CommandHanler thread in Datanode. The CommandHandler thread is blocked on {{XceiverServerRatis#submitRequest}} because of which all the subsequent commands sent by SCM are not getting processed in datanode.

We should add a timeout for this call and throw {{TimeoutException}}.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-04-17 17:38:34,19
13299096,Enable TestSCMPipelineMetrics test cases,Fix and enable TestSCMPipelineMetrics test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:25:01,71
13299092,Enable TestOzoneManagerRestart test cases,Fix and enable TestOzoneManagerRestart test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:23:36,44
13299089,Enable TestOmMetrics#testBucketOps,Fix and enable TestOmMetrics#testBucketOps,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:21:37,71
13299087,Enable TestKeyManagerImpl test cases,Fix and enable TestKeyManagerImpl test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:20:47,27
13299086,Enable TestFreonWithPipelineDestroy test cases,Fix and enable TestFreonWithPipelineDestroy test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:20:16,71
13299077,Enable TestBlockDeletion test cases,Fix and enable TestBlockDeletion test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:15:57,35
13299076,Enable TestContainerReplication test cases,Fix and enable TestContainerReplication test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:15:29,26
13299075,Enable TestWatchForCommit test cases,Fix and enable TestWatchForCommit test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:15:01,16
13299073,Enable TestOzoneRpcClientWithRatis test cases,Fix and enable TestOzoneRpcClientWithRatis test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:12:38,44
13299063,Enable TestStorageContainerManager test cases,Fix and enable TestStorageContainerManager test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 10:02:33,44
13299058,Enable TestNodeFailure test cases,Fix and enable TestNodeFailure test cases,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 09:59:59,19
13299055,Enable TestSCMNodeManager#testScmNodeReportUpdate,Fix and enable TestSCMNodeManager#testScmNodeReportUpdate,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 09:58:25,71
13299051,Enable TestSCMNodeManager#testScmStatsFromNodeReport,Fix and enable TestSCMNodeManager#testScmStatsFromNodeReport,pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-17 09:57:25,71
13298990,Ozone documentation to be revised for OM HA Support,"As OM HA is now supported in current version , We might need to update all documentation pages wherever service id is applicable and any new parameters that we might need to configure in core-site.xml for service id access for remote clusters etc. And all volume/bucket/key CLI syntaxes including service id for OM HA enabled clusters should be included in documentation",TriagePending,['documentation'],HDDS,Bug,Major,2020-04-17 07:03:55,6
13298940,Switch Recon SQL DB to Derby.,"Recon currently uses Sqlite as its defacto SQL DB with an option to configure other JDBC compatible databases. However, on some platforms like the IBM power pc, this causes problems from compile time since it does not have the sqlite native driver. This task aims to change the default SQL DB used by Recon to Derby, but retains the out of the box support (no need to supply the driver) for Sqlite as well. ",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-04-17 03:09:59,30
13298909,Update download links,The download links for signatures/checksums/KEYS should be updated from dist.apache.org to https://downloads.apache.org/hadoop/ozone/.,newbie,['website'],HDDS,Improvement,Major,2020-04-16 22:03:11,73
13298880,Remove RetryInvocation INFO logging from ozone CLI output,"In OM HA failover proxy provider, RetryInvocationHandler logs error stack trace when client tries contacting non-leader OM. Instead we can just log a message that the failover will happen and not include the stack trace.
{code:java}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:174) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:110) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682), while invoking $Proxy16.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
{code}",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Improvement,Major,2020-04-16 18:06:42,43
13298844,Tool for Listing keys from the OpenKeyTable,This tool lists keys present in the OpenKeyTable .The tool can be used to debug when keys  don't show up on OzoneManager after writing them .There is a chance that the key has not gotten committed and will show up in the OpenKeyTable through the tool.,pull-request-available,['Tools'],HDDS,New Feature,Minor,2020-04-16 15:01:00,26
13298741,Generate ozone specific version from type in FSProto.proto,"FSProtos.proto is copied from the Hadoop and used during the proto file generation **BUT** the types defined in FSProtos.proto are generated by Hadoop subproject.

This makes it hard to use Ozone with older Hadoop versions as the types of FSProtos are available only from Hadoop 3.x. 

An easy fix is to generate our own version based on the existing FSProtos.proto",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2020-04-16 08:57:35,6
13298705,Use proper acls for sub directories created during CreateDirectory operation,"Use proper ACLS for subdirectories created during create directory operation.

All subdirectories/missing directories should inherit the ACLS from the bucket if ancestors are not present in key table. If present should inherit the ACLS from its ancestor.",TriagePending,['Ozone Manager'],HDDS,Bug,Blocker,2020-04-16 05:40:34,40
13298704,Ozone audit entries could be consistent among volume creation with quota and update quota,"2020-04-14 09:44:55,089 | INFO  | OMAudit | user=root | ip=172.25.40.156 | op=CREATE_VOLUME

{admin=root, owner=hdfs, volume=hive2, creationTime=1586857495055, *quotaInBytes=1099511627776*, objectID=1792, updateID=7}

| ret=SUCCESS |

2020-04-14 09:58:09,634 | INFO  | OMAudit | user=root | ip=172.25.40.156 | op=SET_QUOTA

{volume=hive, *quota=536870912000*}

| ret=SUCCESS |

 

OMVolumeSetQuotaRequest.java -> auditMap.put(OzoneConsts.QUOTA,     String.valueOf(setVolumePropertyRequest.getQuotaInBytes()));

 

We can use OzoneConsts.QUOTA_IN_BYTES instead of OzoneConsts.QUOTA",pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-04-16 05:34:23,13
13298662,Extract test utilities to separate module,"TimedOutTestsListener cannot be added globally because it is in hadoop-hdds-common, which is not accessible in hadoop-hdds-config (since the latter is a dependency of the former).  The listener and related classes (GenericTestUtils, etc.) should be extracted into a separate module to be used by all others.",pull-request-available,"['build', 'test']",HDDS,Task,Major,2020-04-15 21:29:47,1
13298612,Update JaegerTracing,We currently use JaegerTracing 0.34.0. The latest is 1.2.0. We are several versions behind and should update. Note this update requires the latest version fo OpenTracing and has several breaking changes.,pull-request-available,[],HDDS,Task,Blocker,2020-04-15 16:34:37,6
13298610,Update Ratis/gRPC/Netty in Ozone ,Ozone currently use grpc-netty 1.17.1. The latest version is 1.28.1. We are several versions behind and should update.,pull-request-available,[],HDDS,Task,Major,2020-04-15 16:32:17,18
13298601,Make jmh jar dependencies optional,"jmh dependencies used by `ozone genesis` are licensed by GPL + classpath exception

It's better to make it optional and download it on demand (and exclude it from the release package).",pull-request-available,[],HDDS,Improvement,Major,2020-04-15 15:31:20,6
13298461,Skip generation of encryptionkey for directory create operation,While reading code observed that we are generating encryption info for create directory operation. This jira is to skip the generation of encryption info.,pull-request-available,[],HDDS,Bug,Major,2020-04-15 05:04:28,13
13298435,Recon throws NPE in clusterState endpoint,"Recon throws NPE while trying to get number of volumes from omMetadataManager instance (omMetadataManager.getVolumeTable().getEstimatedKeyCount()).

This happens before Recon gets its first snapshot db from OM, because only after first snapshot db, omMetadataManager initializes its tables.
{code:java}
Unable to get Volumes count in ClusterStateResponse.
java.lang.NullPointerException
	at org.apache.hadoop.ozone.recon.api.ClusterStateEndpoint.getClusterState(ClusterStateEndpoint.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
	at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)
	at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)
	at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)
	at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1615)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:500)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
	at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-04-15 02:44:51,37
13298366,OM create key/file should not generate different data encryption key during validateAndUpdateCache,"The problem with generate different Data encryption key for the same file across different OM instances are that when the OM leader changes, the client may not be able to read the data correctly. ",pull-request-available,[],HDDS,Bug,Blocker,2020-04-14 18:28:55,13
13298364,Delegate admin ACL checks to Ozone authorizer plugin,"Currently, the admin operation check are not sent to authorizer plugin. As a result, the audit log are not shown up in plug-in like ranger authorizer. ",pull-request-available,[],HDDS,Improvement,Major,2020-04-14 18:20:24,28
13298358,Rebase OFS branch - 2. Adapt OFS classes to HDDS-3101,"HDDS-3353 broke OFSPath since the latter uses {{org.apache.commons.codec.digest.DigestUtils}} but the dependency is removed.

HDDS-3359 broke OFSPath because the latter also uses {{org.apache.yetus.audience}}, but this is a simple fix - just replace it with {{org.apache.hadoop.hdds.annotation}}.",pull-request-available,[],HDDS,Sub-task,Major,2020-04-14 18:09:34,12
13298356,Add response to SetVolumePropertyResponse proto,"https://github.com/apache/hadoop-ozone/pull/806#discussion_r408279098

1. Add {{optional bool response = 1;}} in the [message|https://github.com/apache/hadoop-ozone/blob/15db251f16236228c6596253dab6946494fc8f5b/hadoop-ozone/common/src/main/proto/OzoneManagerProtocol.proto#L425-L427].
2. Handle the response on the client. e.g. in setOwner, if the response is false, we can print message: The specified user is already the owner of the volume.

Will start the work after PR #821 is merged.",pull-request-available,[],HDDS,Improvement,Major,2020-04-14 17:57:13,12
13298346,Add bucket encryption key info to bucket create audit log,The current audit log does not include the bucket encryption key information. This ticket is opened to add that. ,Triaged pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-04-14 16:56:48,28
13298306,Fix ContainerOperationClient#createContainer,"In the following snippet of code , it seems to acquire the xceiverclient instance twice and but release once. It should be acquired only once.
{code:java}
  @Override
  public ContainerWithPipeline createContainer(HddsProtos.ReplicationType type,
      HddsProtos.ReplicationFactor factor, String owner) throws IOException {
    XceiverClientSpi client = null;
    try {
      // allocate container on SCM.
      ContainerWithPipeline containerWithPipeline =
          storageContainerLocationClient.allocateContainer(type, factor,
              owner);
      Pipeline pipeline = containerWithPipeline.getPipeline();
     client = xceiverClientManager.acquireClient(pipeline);

      // connect to pipeline leader and allocate container on leader datanode.
      client = xceiverClientManager.acquireClient(pipeline);
      createContainer(client,
{code}",pull-request-available,[],HDDS,Bug,Major,2020-04-14 14:55:29,26
13298260,Simplify S3 -> Ozone volume mapping,"See the design doc for more details: 

https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/design/ozone-volume-management.md",backward-incompatible imcompatible pull-request-available,[],HDDS,Improvement,Critical,2020-04-14 12:34:34,1
13298148,Update SpringFramework to 5.1.14,"We are on SpringFramework 5.1.3. We should update to newer versions (5.1.14 or 5.2.x)

Also,
{code:java|title=hadoop-ozone/recon-codegen/pom.xml}
    <dependency>
      <groupId>org.springframework</groupId>
      <artifactId>spring-jdbc</artifactId>
      <version>5.1.3.RELEASE</version>
    </dependency>
{code}
It should specify the version with ${{{spring.version}}}",pull-request-available,[],HDDS,Task,Major,2020-04-13 22:15:24,86
13297884,OzoneManager starts 2 OzoneManagerDoubleBuffer for HA clusters,"OzoneManager starts 2 OzoneManagerDoubleBuffer for HA clusters. In the following example for 3 OM HA instances, 6 OzoneManagerDoubleBuffer instances were created.
{code}
➜  chaos-2020-04-12-20-21-11-IST grep canFlush stack1
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)
{code}",MiniOzoneChaosCluster Triaged pull-request-available,"['Ozone Manager', 'test']",HDDS,Bug,Major,2020-04-12 15:02:24,13
13297861,MiniOzoneHAClusterImpl#initOMRatisConf will reset the configs and causes for test failures,"While I was debugging some code paths using miniOzoneCluster:

For example in TestOzoneHAManager:

it plans to trigger snapshots at threshold 50 and same was configured and passed to MiniOzoneHACluster. But inside MiniOzoneHAClusterImpl#initOMRatisConf, it will silently reset to 100L. So, test will expect snapshot to trigger after 50 transactions, but it will not.

 

It will keep wait even after rolling at 50:
{quote}GenericTestUtils.waitFor(() -> {
 if (ozoneManager.getRatisSnapshotIndex() > 0) {
 return true;
 }
 return false;
}, 1000, 100000);
{quote}
 
{quote}2020-04-12 03:54:21,296 [omNode-1@group-523986131536-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(583)) - omNode-1@group-523986131536-SegmentedRaftLogWorker: created new log segment /Users/ugangumalla/Work/repos/hadoop-ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-fce544cd-3a80-4b0b-ac92-463cf391975c/omNode-1/ratis/c9bc4cf4-3bc3-3c60-a66b-523986131536/current/log_inprogress_49
{quote}
 

So, respecting user passed configurations will fix the issue. I will post the patch later in some time.",pull-request-available,"['OM HA', 'test']",HDDS,Bug,Minor,2020-04-12 10:50:08,8
13297772,OzoneManager group init failed because of incorrect snapshot directory location,"OzoneManager group init failed because of incorrect snapshot directory location

{code}
2020-04-11 20:07:57,180 [pool-59-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(44)) - raft.server.storage.dir = [/tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis] (custom)
2020-04-11 20:07:57,180 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(191)) - omNode-3: found a subdirectory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/snapshot
2020-04-11 20:07:57,181 [pool-59-thread-1] WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(197)) - omNode-3: Failed to initialize the group directory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/snapshot.  Ignoring it
java.lang.IllegalArgumentException: Invalid UUID string: snapshot
        at java.util.UUID.fromString(UUID.java:194)
        at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$0(RaftServerProxy.java:192)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at org.apache.ratis.server.impl.RaftServerProxy.lambda$initGroups$1(RaftServerProxy.java:189)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
        at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)
        at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
        at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
        at org.apache.ratis.server.impl.RaftServerProxy.initGroups(RaftServerProxy.java:186)
        at org.apache.ratis.server.impl.ServerImplUtils.newRaftServer(ServerImplUtils.java:41)
        at org.apache.ratis.server.RaftServer$Builder.build(RaftServer.java:76)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.<init>(OzoneManagerRatisServer.java:277)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.newOMRatisServer(OzoneManagerRatisServer.java:328)
        at org.apache.hadoop.ozone.om.OzoneManager.initializeRatisServer(OzoneManager.java:1249)
        at org.apache.hadoop.ozone.om.OzoneManager.restart(OzoneManager.java:1190)
        at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl.restartOzoneManager(MiniOzoneHAClusterImpl.java:229)
        at org.apache.hadoop.ozone.failure.Failures$OzoneManagerRestartFailure.lambda$fail$0(Failures.java:112)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
        at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)
        at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
        at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
        at org.apache.hadoop.ozone.failure.Failures$OzoneManagerRestartFailure.fail(Failures.java:109)
        at org.apache.hadoop.ozone.failure.FailureManager.fail(FailureManager.java:58)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-04-11 20:07:57,182 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(191)) - omNode-3: found a subdirectory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/b870c9eb-edfb-36b5-b758-d62218d261de
2020-04-11 20:07:57,183 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - omNode-3: addNew group-D62218D261DE:[omNode-3:localhost:12408, omNode-1:localhost:12396, omNode-2:localhost:12402] returns group-D62218D261DE:java.util.concurrent.CompletableFuture@2fc3d657[Not completed]
2020-04-11 20:07:57,183 [pool-1382-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(97)) - omNode-3: new RaftServerImpl for group-D62218D261DE:[omNode-3:localhost:12408, omNode-1:localhost:12396, omNode-2:localhost:12402] with OzoneManagerStateMachine:uninitialized
{code}",MiniOzoneChaosCluster pull-request-available,"['Ozone Manager', 'test']",HDDS,Bug,Major,2020-04-11 14:48:55,13
13297666,Remove guava 26.0-android jar,"I missed this during HDDS-3000

guava-26.0-android is not used but if it's in the classpath (copied explicitly in pom file), it could potentially load this one and cause runtime error.

{noformat}
$ find . -name guava*
./hadoop-ozone/ozonefs-lib-legacy/target/classes/libs/META-INF/maven/com.google.guava/guava
./hadoop-ozone/dist/target/ozone-0.4.0.7.1.1.0-SNAPSHOT/share/ozone/lib/guava-26.0-android.jar
./hadoop-ozone/dist/target/ozone-0.4.0.7.1.1.0-SNAPSHOT/share/ozone/lib/guava-28.2-jre.jar
{noformat}",pull-request-available,[],HDDS,Task,Major,2020-04-10 18:50:40,25
13297648,S3A failing complete multipart upload with Ozone S3,"
{code:java}
javax.xml.bind.UnmarshalException: unexpected element (uri:"""", local:""CompleteMultipartUpload""). Expected elements are <{http://s3.amazonaws.com/doc/2006-03-01/}CompleteMultipartUpload>,<{http://s3.amazonaws.com/doc/2006-03-01/}Part>
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleEvent(UnmarshallingContext.java:744)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:262)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:257)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportUnexpectedChildElement(Loader.java:124)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext$DefaultRootLoader.childElement(UnmarshallingContext.java:1149)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:574)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:556)
        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:168)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:509)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:374)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl$NSContentDriver.scanRootElementHook(XMLNSDocumentScannerImpl.java:613)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3132)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:852)

{code}


It seems http://s3.amazonaws.com/doc/2006-03-01/ is expected in the element.
But in class CompleteMultipartUploadRequest,  namespace http://s3.amazonaws.com/doc/2006-03-01/ is not defined here.

Reported by [~sammichen]





",pull-request-available,[],HDDS,Bug,Major,2020-04-10 17:04:28,13
13297603,OMVolumeSetOwnerRequest doesn't check if user is already the owner,"OMVolumeSetOwnerRequest doesn't seem to check if the user is already the owner.
If the user is already the owner, it shouldn't proceed to the update logic, otherwise the resulting volume list for that user in {{UserVolumeInfo}} would have duplicate volume entry. As demonstrated in the test case.

-It also doesn't seem to remove the volume from the UserVolumeInfo from the previous owner.- Checked [here|https://github.com/apache/hadoop-ozone/blob/80e9f0a7238953e41b06d22f0419f04ab31d4212/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetOwnerRequest.java#L152-L153].

[~bharat]",pull-request-available,[],HDDS,Bug,Major,2020-04-10 11:45:42,12
13297558,Intermittent failure in TestDnRatisLogParser and TestOMRatisLogParser,"{code:title=https://github.com/apache/hadoop-ozone/pull/783/checks?check_run_id=576054872}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 18.987 s <<< FAILURE! - in org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser
[ERROR] testRatisLogParsing(org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser)  Time elapsed: 18.882 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser.testRatisLogParsing(TestDnRatisLogParser.java:75)
{code}

{code:title=https://github.com/apache/hadoop-ozone/pull/846/checks?check_run_id=608153672}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.606 s <<< FAILURE! - in org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser
[ERROR] testRatisLogParsing(org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser)  Time elapsed: 30.476 s  <<< FAILURE!
java.lang.AssertionError
  at org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser.testRatisLogParsing(TestOMRatisLogParser.java:112)
{code}

CC [~msingh]",pull-request-available,['test'],HDDS,Bug,Major,2020-04-10 07:40:40,1
13297549,Delete HISTORY.txt,"During HDDS-2294, the old file was not deleted.

This Jira aims to remove the old file.",pull-request-available,[],HDDS,Bug,Major,2020-04-10 07:06:06,81
13297513,Ozone filesystem jar should not include webapps folder,"hadoop-ozone-filesystem-lib-current jar includes webapps folder of hdds datanode. 

This should not be included in the filesystem jar.",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-04-10 00:15:12,37
13297252,Ensure OzoneConfiguration is initialized in OzoneClientFactory#getOzoneClient,"HDDS-3319 added a new method to get RPCClient based on the omserviceID in OzoneToken. However, some om.service.id compare logic is based on a Hadoop Configuration object. As a result, the om configuration may not be loaded if the Configuration is a Hadoop Configuration only (e.g., RM). ",pull-request-available,[],HDDS,Bug,Major,2020-04-09 03:17:12,28
13297219,Increase test timeout for ozonesecure-security robot tests,"In few CI runs, ozone-security robot tests are timing out at 5 minutes. 
[https://github.com/apache/hadoop-ozone/pull/784/checks?check_run_id=569548444]

We should increase the timeout to avoid this.",pull-request-available,[],HDDS,Bug,Major,2020-04-08 21:03:41,43
13297212,Intermittent failure in testContainerImportExport,"{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/571992849}
2020-04-08T20:30:49.0510599Z [ERROR] Tests run: 22, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.669 s <<< FAILURE! - in org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer
2020-04-08T20:30:49.0535678Z [ERROR] testContainerImportExport[1](org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.079 s  <<< ERROR!
2020-04-08T20:30:49.0552584Z java.io.IOException: request to write '4096' bytes exceeds size in header of '19906' bytes for entry 'db/LOG'
2020-04-08T20:30:49.0572746Z 	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.write(TarArchiveOutputStream.java:385)
2020-04-08T20:30:49.0572897Z 	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2147)
2020-04-08T20:30:49.0582579Z 	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)
2020-04-08T20:30:49.0593659Z 	at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)
2020-04-08T20:30:49.0603340Z 	at org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)
2020-04-08T20:30:49.0613502Z 	at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includeFile(TarContainerPacker.java:225)
2020-04-08T20:30:49.0631425Z 	at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includePath(TarContainerPacker.java:215)
2020-04-08T20:30:49.0637525Z 	at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.pack(TarContainerPacker.java:155)
2020-04-08T20:30:49.0648504Z 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:549)
2020-04-08T20:30:49.0659852Z 	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testContainerImportExport(TestKeyValueContainer.java:233)
{code}",pull-request-available,['test'],HDDS,Sub-task,Major,2020-04-08 20:40:37,1
13297134,Change replication logic to use PersistedOpState,"In an earlier change, we decided that the Datanode would change the state of decommissioning / maintenance replicas and report the new state in its container report. Using that assumption, the logic in replication manager was changed to expect the ContainerReplica to contain the decom / maintenance state.

However in HDDS-2592 it was decided that the DN would simply store the operational state and report only the operation state in the heartbeat. This state is reported back to SCM and stored in the datanodeDetails instances.

This means that earlier assumption (ContainerReplica will contain the decom state) is no longer valid.

This Jira is to change the logic so replication decisions are based on the ""persistedOpState"" in the datanode details object instead.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-04-08 17:00:55,11
13297069,Remove Yetus helper code from ozone,Yetus related helper code is not currently being used. It can be removed from the repo.,pull-request-available,['build'],HDDS,Bug,Major,2020-04-08 13:10:00,18
13297060,Intermittent test failure related to a race conditon during PipelineManager close,"The test which is failed:

TestSCMNodeManager

The end of the log is:

{code}
2020-04-08 10:49:44,544 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(84)) - Error on execution message 19844615-0d70-4172-8c34-96e5b7295ef2{ip: 196.189.243.187, host: localhost-196.189.243.187, networkLocation: /default-rack, certSerialId: null}
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.finalizeAndDestroyPipeline(SCMPipelineManager.java:380)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:63)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-04-08 10:49:44,544 INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=fd1f9e92-2f90-43e7-8406-94ba6ac356b0, PipelineID=8d380e3c-b632-4bda-aa7a-554774fba09d]
2020-04-08 10:49:44,544 INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(373)) - Destroying pipeline:Pipeline[ Id: fd1f9e92-2f90-43e7-8406-94ba6ac356b0, Nodes: 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-08T10:49:37.441Z]
2020-04-08 10:49:44,544 INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(120)) - Pipeline Pipeline[ Id: fd1f9e92-2f90-43e7-8406-94ba6ac356b0, Nodes: 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:null, CreationTimestamp2020-04-08T10:49:37.441Z] moved to CLOSED state
2020-04-08 10:49:44,544 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(84)) - Error on execution message 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.finalizeAndDestroyPipeline(SCMPipelineManager.java:380)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:63)
        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:38)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2020-04-08 10:49:44,544 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(208)) - Send pipeline:PipelineID=e0e155c6-9fbe-46a7-b742-e805ea9baacf close command to datanode 30a24b04-1289-4c30-a28a-034edfe29e3d
2020-04-08 10:49:44,545 WARN  events.EventQueue (EventQueue.java:fireEvent(151)) - Processing of TypedEvent{payloadType=CommandForDatanode, name='Datanode_Command'} is skipped, EventQueue is not running
2020-04-08 10:49:44,544 INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 59bdd26b-05da-47d1-8c3f-8350d55d7299{ip: 248.147.58.17, host: localhost-248.147.58.17, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=17b032b7-b9c4-41eb-bba6-50106881886d, PipelineID=60de1ca6-4115-415b-bbf1-06b86113df94]
2020-04-08 10:49:44,576 WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2020-04-08 10:49:44,579 WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2020-04-08 10:49:44,579 WARN  db.DBDefinition (DBDefinition.java:createDBStoreBuilder(63)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
{code}",TriagePending flaky-test ozone-flaky-test,['test'],HDDS,Sub-task,Major,2020-04-08 12:33:36,6
13297051,Add check for import from shaded package,Add a checkstyle rule to reject import from shaded packages.  This would help avoid accidentally using shaded transitive dependencies.,pull-request-available,['build'],HDDS,Improvement,Major,2020-04-08 11:49:18,1
13296898,Add wait time between client retries to OM,"Currently, client keeps retrying to connect to leader OM in a tight loop and fails after configured number of retires/ failovers.
If the Leader OM is not ready, the client can timeout quickly. So, we should instead try each OM once, and then wait before retrying again. ",pull-request-available,[],HDDS,Sub-task,Major,2020-04-07 19:19:59,43
13296879,OM HA replay optimization,"This Jira is to improve the OM HA replay scenario.
Attached the design document which discusses about the proposal and issue in detail.
",Triaged,"['OM HA', 'Ozone Manager']",HDDS,Improvement,Major,2020-04-07 18:03:53,13
13296835,Remove unnecessary transitive hadoop-common dependencies on server side.,Similar to HDDS-3312 we can exclude dependencies coming from hadoop-ozone. (Eg. curator / zookeeper.),pull-request-available,[],HDDS,Improvement,Major,2020-04-07 14:05:26,6
13296807,Support for native ozone filesystem client using libhdfs,This jira is to bring in support for native ozone filesystem client using libhdfs.,pull-request-available,['Ozone Client'],HDDS,New Feature,Major,2020-04-07 11:58:20,27
13296684,Remove unnecessary dependency Curator,"It seems when we have separated the main pom.xml from Hadoop pom, we copied most of the dependencies blindly to ozone pom, and we still cary some parts of this.

Due to an internal dependency checking, it turned out we still have curator defined as a dependency, even though we don't have Zookeeper already, and we do not use it.

I am posting a PR to remove curator from the dependencies, locally mvn clean install ran fine with it.",pull-request-available,['build'],HDDS,Improvement,Major,2020-04-06 23:33:20,63
13296535,Ozone Retry Policy Improvements,Currently any ozone client request can spend a huge amount of time in retries and ozone client can retry its requests very aggressively. The waiting time can thus be very high before a client request fails. Further aggressive retries by ratis client used by ozone can bog down a ratis pipeline leader. The Jira aims to make changes to the current retry behavior in Ozone client. ,Triaged pull-request-available,['Ozone Client'],HDDS,Improvement,Blocker,2020-04-06 14:57:09,35
13296407,Fix two link addresses in README.md,"Currently the ""Please open a [Jira|https://issues.apache.org/jira] issue"" just open issues.apache.org.

For a newbie to apache, the [https://issues.apache.org/jira/projects/HDDS/issues] will be friendly

And fix the [Contribution guideline|https://github.com/apache/hadoop-ozone/blob/master/CONTRIBUTION.md] from ./CONTRIBUTION.md to ./CONTRIBUTING.md",pull-request-available,[],HDDS,Improvement,Minor,2020-04-06 04:00:19,74
13296380,scmcli container info command shows the wrong container state.,The container state shown in _ozone scmcli container info_ command's output is not reliable.,pull-request-available,[],HDDS,Bug,Major,2020-04-05 19:53:50,19
13296378,PipelineActionHandler should handle unknown pipeline.,"PipelineActionHandler is not able to handle PipelineActions for pipelines which is not present in SCM (deleted pipelines)

After SCM sends out the close pipeline command to datanodes, it removes the pipeline information from the pipeline db. If a datanode sends PipelineAction for the deleted pipeline, PipelineActionHandler cannot handle it.
In such a situation SCM should fire ClosePipelineCommand. ",pull-request-available,['SCM'],HDDS,Bug,Major,2020-04-05 19:45:41,19
13296118,Remove Jenkins file,[~elek] recommended we can delete this file as it is no longer in use.,pull-request-available,['build'],HDDS,Bug,Major,2020-04-03 20:55:35,81
13296113,Merge Master branch into decom branch,Its been a long time since Master was merged into the decom branch. Merge the branches and resolve the conflicts.,pull-request-available,[],HDDS,Sub-task,Major,2020-04-03 20:41:22,11
13296109,Ozone admin shell commands do not print or log exceptions on failures,Ozone shell commands such as `ozone admin container create` does not print any information on failure which makes it very difficult to debug the underlying issue.,TriagePending,['Ozone CLI'],HDDS,Bug,Major,2020-04-03 20:05:41,37
13296055,Checkstyle fails for new modules/versions,{{checkstyle.sh}} fails for any new intermediate module (used by other modules) or new version (eg. bump from 0.5.0-SNAPSHOT to 0.6.0-SNAPSHOT).  It does not perform a complete build and tries to fetch these artifacts from Maven repo.,pull-request-available,['build'],HDDS,Bug,Major,2020-04-03 15:44:18,1
13296046,Move Ozone Shell from ozone-manager to tools,"Ozone Shell is currently part of the {{ozone-manager}} module.  I think it would be more at home in the {{tools}} module.

Also rename the package name {{ozShell}} to {{shell}}, as package names should be all lowercase.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-04-03 15:03:56,1
13296028,Rename CONTRIBUTION.md to CONTRIBUTING.md,"If we follow the naming convention suggested by github:

https://help.github.com/en/github/building-a-strong-community/setting-guidelines-for-repository-contributors

The contribution guideline will be displayed at multiple locations (eg. when a new contributor create a new PR).",pull-request-available,[],HDDS,Improvement,Trivial,2020-04-03 13:25:31,6
13295956,Metrics for Recon OzoneManager DB sync.,"Some useful metrics for tracking how Recon's OM DB requests are going on.

{code}
  @Metric(about = ""Number of OM snapshot requests made by Recon."")
  @Metric(about = ""Number of OM snapshot requests that failed."")
  @Metric(about = ""OM snapshot request latency"")
  @Metric(about = ""Number of OM delta requests made by Recon."")
  @Metric(about = ""Number of OM delta requests that failed."")
  @Metric(about = ""OM delta request latency"")
  @Metric(about = ""Total number of updates got through OM delta request"")
  @Metric(about = ""Average number of updates got per OM delta request"")
{code}",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-04-03 06:56:50,30
13295920,Recon unit tests cleanup.,"The Guice bindings required for some of the unit tests are hard to follow and need some cleanup.


*Work done*

* Created class to setup a recon test injector, with any combination of sub modules that are specified.
* Created class that provides a Recon SQL DB with all the tables created, and APIs to access the DAOs easily.
* Cleaned up injector usage in Recon tests.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-04-03 00:51:27,30
13295868,OM Client failover to next OM on NotLeaderException,"On receiving NotLeaderException from OM server, we should failover to the next OM in the list instead of the suggested Leader. This will ensure that all the OMs are contacted in a round robin way. Failing always to the suggested leader is not robust.",pull-request-available,[],HDDS,Sub-task,Major,2020-04-02 18:18:13,43
13295830,Recon UI: All the pages should auto reload,Overview page should auto refresh to fetch updated data and alerts if any.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-04-02 16:10:10,37
13295792,Simplify s3bucket -> ozone volume/bucket mapping,"The current s3bucket -> ozone volume/bucket mapping is very confusing. Let's improve it.

(See the design docs for more details).",pull-request-available,[],HDDS,Improvement,Major,2020-04-02 13:07:19,6
13295789,TestDeleteWithSlowFollower is still flaky,"{code}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 666.209 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 640.745 s  <<< ERROR!
java.io.IOException: INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:229)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:402)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:347)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:458)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:509)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
        at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)
{code}

I learned this from [~shashikant]

bq. we kill a datanode after some IO, SCM is out of safe mode by then . SCM takes time to destroy a pipeline and form a new one
bq. With only minimal set of dn in cluster, if we want to write again, we need to wait for a new pipeline to open up before writing again

Will turn off this test until the fix.
",TriagePending ozone-flaky-test pull-request-available,['test'],HDDS,Improvement,Major,2020-04-02 12:36:32,16
13295773,Ozone cluster expansion: Block deletion mismatch,"SCM logs keep printing this when we expand Ozone cluster with more datanodes.

 

2020-04-02 19:45:42,745 [EventQueue-PendingDeleteStatusForPendingDeleteHandler] INFO org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService: Block deletion txnID mismatch in datanode 1eacbd89-a835-438e-aa4b-5bc78adb7c8c for containerID 314. Datanode delete txnID: 0, SCM txnID: 1208
2020-04-02 19:45:42,745 [EventQueue-PendingDeleteStatusForPendingDeleteHandler] INFO org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService: Block deletion txnID mismatch in datanode 1eacbd89-a835-438e-aa4b-5bc78adb7c8c for containerID 351. Datanode delete txnID: 0, SCM txnID: 662
2020-04-02 19:45:42,745 [EventQueue-PendingDeleteStatusForPendingDeleteHandler] INFO org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService: Block deletion txnID mismatch in datanode 1eacbd89-a835-438e-aa4b-5bc78adb7c8c for containerID 352. Datanode delete txnID: 0, SCM txnID: 1085",TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2020-04-02 11:47:44,35
13295767,List design docs as part of the documentation page,"In HDDS-1659 a new generic way was introduced  to make the design docs more public (collect them as part of the documentation whether they are written as part of the docs or as separated pdf/google docs).

This patch:
 
 * List the design doc as part of the documentation
 * Adds link to some of the existing / earlier design docs

I believe that this can be a good extension to the documentation page as some of the low level internals are discussed only in design docs.",pull-request-available,['documentation'],HDDS,Improvement,Major,2020-04-02 11:17:07,6
13295758,BUCKET_NOT_FOUND occurs when I create the bucket using the aws s3api,"1、When the acl is enabled(not enable security), It report an error to execute the following command to create the bucket. This is successful when the acl is not enabled. 
{code:java}
aws s3api --endpoint-url http://localhost:9878 create-bucket --bucket=bucket1
{code}
The error log is in the attachment.

2、Besides, acl initialization is still a problem in ozone. Before the old cluster enabled acls, the acl of the old key（created through s3g）will set to the s3g startup user, which should be set to AccesskeyId. Because the AccesskeyId is used as the user name to access these keys when the acl is enabled. 

The reason for the above two problems is the same, I will use a PR to fix it

 ",pull-request-available,['S3'],HDDS,Bug,Major,2020-04-02 10:49:39,4
13295698,Handle Resource Unavailable exception in OM HA,"Right now, when the future fails with an exception, we send that exception to the client, and retry with a new server. but when using ratis server when resource unavailable exception future fails with exceptionally. So, in this case we need to wrap the exception and retry to the same server with some retry policy like MultiLinearRandomRetry or some retry policy.

{code:java}
try {
 raftClientReply = server.submitClientRequestAsync(raftClientRequest)
          .get();
    } catch (Exception ex) {
      throw new ServiceException(ex.getMessage(), ex);
    }
{code}

",TriagePending,"['OM HA', 'Ozone Manager']",HDDS,Bug,Major,2020-04-02 06:22:29,13
13295660,OM Client fails with StringIndexOutOfBoundsException,"OM Client fails with StringIndexOutOfBoundsException: 

 
{code:java}
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: Caused by: java.io.IOException: Couldn't create RpcClient protocol
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:199)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:175)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:86)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:168)
2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:158)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3391)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:158)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3451)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3419)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:513)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:558)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	... 26 more
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at java.base/java.lang.String.substring(String.java:1841)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.computeDelegationTokenService(OMFailoverProxyProvider.java:207)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:84)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:208)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:192)
2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: 	... 39 more

 {code}
 

This can happen for two reasons:
 # If user has configured OM addresses incorrectly which are not resolvable to any host.
 # In Docker world when the client starts if all OM containers are down, then OM addresses will be unresolvable. ",pull-request-available,[],HDDS,Bug,Major,2020-04-02 00:01:08,13
13295610,StandAlone Pipelines are created in an infinite loop,"_BackgroundPipelineCreator_ keeps creating pipelines of configured Replication type and all available Replication factors until some exception occurs while creating the pipeline such as no more available nodes.

When Replication Type is set to STAND_ALONE, we do not check if a DN has already been used to create a pipeline of same factor or not and keep reusing the same DNs to create new pipelines. This causes the pipeline creation to happen in an infinite loop.",pull-request-available,[],HDDS,Bug,Critical,2020-04-01 19:23:39,43
13295584,Prometheus endpoint should have an option to be configured with Token based authentication. ,"Prometheus does not support targets which have Kerberos SPNEGO based authentication. Hence, on a secure cluster, if we have prometheus endpoint enabled, it makes sense to skip the authentication filter for it.",pull-request-available,[],HDDS,Bug,Major,2020-04-01 17:03:51,30
13295550,Add read only based load generator to MiniOzoneChaosCluster,This jira proposes to add a read-only workload to MiniOzoneChaosCluster. The set of files will be initially written and then read multiple times while nodes are shutdown.,pull-request-available,[],HDDS,Bug,Major,2020-04-01 14:52:38,18
13295558,Handle HA for BasicOzoneClientAdapterImpl$Renewer#renew/cancel(),"Currently, only the getDelegationtoken has been updated with the proper HA client proxy setup. renew and cancel will fail in OM HA setup.

",pull-request-available,[],HDDS,Bug,Major,2020-04-01 15:23:07,28
13295554,Support /close command in the Github comments,"As it's discussed during the last community meeting we can close pending pull requests after 30 days. /close command can make it easier.

We can also add pending label to the pending pull requests.",pull-request-available,[],HDDS,Improvement,Major,2020-04-01 15:00:56,6
13295517,Checkstyle check fails silently in case of mvn related errors,"From the last master build:

https://github.com/apache/hadoop-ozone/runs/550746880?check_suite_focus=true

Checkstyle is failed due to a maven error:

{code}
[ERROR] Failed to execute goal on project hadoop-hdds-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-config:jar:0.6.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
[ERROR] Failed to execute goal on project hadoop-hdds-client: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-client:jar:0.6.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT, org.apache.hadoop:hadoop-hdds-config:jar:0.6.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
{code}

But it remained green.

We should fail the check if the maven run couldn't succeed.",pull-request-available,[],HDDS,Bug,Blocker,2020-04-01 11:40:03,6
13295515,Use EventQueue for delayed/immediate safe mode rule notification,"SCM is built from loosely coupled components which communicate with async event with each other.

Using the same abstraction (EventQueue) has the benefit that we can use the same visibility / testing tools such as the 'ozone insight' definition (which makes visible all the messages) or the test handler (which can wait until all the event queue messages are processed) 

During the review of HDDS-3221 it was suggested (by me) to use the EventQueue instead of the new SafeModeNotification interface. 

There was only one counter argument against it:

bq. I personally find the event queue logic hard to follow due to its async nature (you cannot just follow method calls in the IDE). Its not bad, but more difficult when you don't yet understand it, while registering some instances to be notified is easy to follow in an IDE. This is of course a subjective opinion :)

I respect this opinion, but I think it's better to use one abstraction and a consistent architecture inside one component (together with all the existing limitations). The EventQueue is not the only one possible solution, but an existing one. We can either design and switch to a new one or use the existing one.

In this patch I would like to show how the previous listener interface can be replaced by the EventQueue.

It (hopefully) shows that this is not complex, and in fact can help us to decouple different component from each other    ",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-04-01 11:26:06,6
13295497,FIx ContainerOperationClient#readContainer to use Grpc Client to read from datanode,"config set before running the command :

""ozone.scm.stale.node.interval"": ""2m"",
 ""ozone.scm.dead.node.interval"": ""4m"",
 ""hdds.scm.replication.thread.interval"": ""12s"",
 ""ozone.scm.container.size"": ""1GB""

 

steps taken :

1) write a key (less than a block size)

2) shutdown two container replica datanodes.

3) Tried to query container info

Container info command failed .

 

 
{noformat}
ozone debug chunkinfo <KeyUri> 
Failed to execute command cmdType: ReadContainer
{noformat}
 

scm log during that time range :
{noformat}
2020-04-01 10:09:29,665 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for hrt_qa@ROOT.HWX.SITE (auth:KERBEROS)
2020-04-01 10:09:29,706 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for hrt_qa@ROOT.HWX.SITE (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2020-04-01 10:09:55,283 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for dn/quasar-fjgcwr-2.quasar-fjgcwr.root.hwx.site@ROOT.HWX.SITE (auth:KERBEROS)
2020-04-01 10:09:55,287 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for dn/quasar-fjgcwr-2.quasar-fjgcwr.root.hwx.site@ROOT.HWX.SITE (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2020-04-01 10:09:55,474 INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Starting Replication Monitor Thread.
2020-04-01 10:09:55,486 INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Replication Monitor Thread took 10 milliseconds for processing 33 containers.
2020-04-01 10:10:07,488 INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 33 containers.
2020-04-01 10:10:17,996 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for dn/quasar-fjgcwr-7.quasar-fjgcwr.root.hwx.site@ROOT.HWX.SITE (auth:KERBEROS)
2020-04-01 10:10:18,001 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for dn/quasar-fjgcwr-7.quasar-fjgcwr.root.hwx.site@ROOT.HWX.SITE (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2020-04-01 10:10:19,491 INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 33 containers.
2020-04-01 10:10:31,494 INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 33 containers.
2020-04-01 10:10:43,495 INFO org.apache.hadoop.hdds.scm.container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 33 containers.
2020-04-01 10:10:47,987 ERROR org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler: Received pipeline action CLOSE for Pipeline[ Id: 763bd379-a703-4dc0-85c5-bf385cdc0b18, Nodes: 92f73ec3-9ed8-41c8-9103-c4c1b2b365e1{ip: 172.27.120.0, host: quasar-fjgcwr-1.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: null}b60097cf-7dff-44dc-800f-3500dda636f6{ip: 172.27.123.128, host: quasar-fjgcwr-4.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: null}ea2322d9-8ede-4f48-a72d-693e809d2b95{ip: 172.27.12.195, host: quasar-fjgcwr-7.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:b60097cf-7dff-44dc-800f-3500dda636f6, CreationTimestamp2020-04-01T10:04:47.723688Z] from datanode ea2322d9-8ede-4f48-a72d-693e809d2b95{ip: 172.27.12.195, host: quasar-fjgcwr-7.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: 12651664310640168}. Reason : ea2322d9-8ede-4f48-a72d-693e809d2b95 is in candidate state for 61616ms
2020-04-01 10:10:47,988 INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 763bd379-a703-4dc0-85c5-bf385cdc0b18, Nodes: 92f73ec3-9ed8-41c8-9103-c4c1b2b365e1{ip: 172.27.120.0, host: quasar-fjgcwr-1.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: null}b60097cf-7dff-44dc-800f-3500dda636f6{ip: 172.27.123.128, host: quasar-fjgcwr-4.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: null}ea2322d9-8ede-4f48-a72d-693e809d2b95{ip: 172.27.12.195, host: quasar-fjgcwr-7.quasar-fjgcwr.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:b60097cf-7dff-44dc-800f-3500dda636f6, CreationTimestamp2020-04-01T10:04:47.723688Z]{noformat}
 ",pull-request-available,"['SCM', 'SCM Client']",HDDS,Bug,Major,2020-04-01 10:30:53,26
13295478,Remove sever-side dependencies from hdds/ozone-common,"hadoop-ozone/common and hadoop-hdds/common projects are common between *client and server*. Therefore we should remove any server side utilities / dependencies from them as they would be added to the client classpath which makes harder the Ozone client / ozonefs adoption.

 * hadoop-hdds should depend on a minimal set of hadoop dependencies (can be managed a separated technical project / pom.xml)
* code shared between server side projects (and not with the client) should be moved to the framework
* OM related code should be moved to the ozone-manager instead of ozone/common",pull-request-available,[],HDDS,Bug,Critical,2020-04-01 09:29:04,6
13295453,OM logs not available for OM HA acceptance test,"Logs collected for OM HA acceptance test have no information about what's happening during the test.  Only the output of {{om --init}} is present:

{code:title=last few lines of log}
...
om1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-0d8a21ae-1a87-400b-bba3-d61cd4c01ce4
om1_1       | 2020-04-01 03:27:44 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG:
om1_1       | /************************************************************
om1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 2f4e39e19fea/172.21.0.6
om1_1       | ************************************************************/
om1_1       | Enabled profiling in kernel
{code}

Thus we have no information about why the test is failing intermittently:

https://github.com/apache/hadoop-ozone/runs/549544110",pull-request-available,['test'],HDDS,Bug,Major,2020-04-01 08:13:02,1
13295354,Add TimedOutTestsListener to surefire and add timeout to integration tests,"Add TimedOutTestsListener as a listener to maven-surefire-plugin like Hadoop does:
https://github.com/apache/hadoop/blob/1189af4746919774035f5d64ccb4d2ce21905aaa/hadoop-hdfs-project/hadoop-hdfs/pom.xml#L233-L238 (Credit: [~elek])",pull-request-available,['test'],HDDS,Test,Major,2020-03-31 20:33:48,12
13295319,Update Ratis snapshot,"Update Ratis snapshot version to {{7c5b30d}}, which includes RATIS-816, required for HDDS-3023.",pull-request-available,['build'],HDDS,Task,Major,2020-03-31 17:52:25,1
13295221,Improve write efficiency by avoiding reverse DNS lookup,[getHostName|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/OMClientRequest.java#L130] cost about 15ms.  It's also reported in [JDK-6450279|https://bugs.openjdk.java.net/browse/JDK-6450279],pull-request-available,[],HDDS,Improvement,Major,2020-03-31 10:21:41,65
13295177,Add datanode Chinese documentation,Add datanode Chinese documentation,pull-request-available,['website'],HDDS,Improvement,Major,2020-03-31 07:19:15,87
13295015,Add ability in MiniOzoneCluster to create multiple storage directories,MiniOzoneCluster currently only creates one data storage directory. This jira proposes to add an option to have multiple storage directories.,MiniOzoneChaosCluster pull-request-available,['test'],HDDS,Bug,Major,2020-03-30 15:06:13,18
13294858,TestFailureHandlingByClient.testDatanodeExclusionWithMajorityCommit is intermittent,See the attachments.,TriagePending flaky-test ozone-flaky-test,['test'],HDDS,Bug,Critical,2020-03-29 18:35:37,16
13294823,TestOzoneClientKeyGenerator is flaky,Sometimes it's hanging and stopped after a timeout.,TriagePending flaky-test ozone-flaky-test pull-request-available,['test'],HDDS,Sub-task,Critical,2020-03-29 13:08:26,27
13294584,Ozone admin should always have read/write ACL permission on ozone objects,"Ozone admin should always have read/write acl permission to ozone objects. This way, if owner incorrectly set the acls and lose access, admin can always help to get acces back. 

",Triaged,['Security'],HDDS,Bug,Major,2020-03-28 01:14:13,28
13294562,Ozone admins getting Permission Denied error while creating volume ,"Even when a user is added to ozone.administrators,  Permission Denied error is thrown while creating a new volume.",pull-request-available,['Security'],HDDS,Bug,Major,2020-03-27 22:06:48,37
13294549,Flaky test TestContainerStateMachineFailureOnRead#testReadStateMachineFailureClosesPipeline,"Shows up in a PR: https://github.com/apache/hadoop-ozone/runs/540133363

{code:title=log}
[INFO] Running org.apache.hadoop.ozone.client.rpc.TestContainerStateMachineFailureOnRead
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.766 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestContainerStateMachineFailureOnRead
[ERROR] testReadStateMachineFailureClosesPipeline(org.apache.hadoop.ozone.client.rpc.TestContainerStateMachineFailureOnRead)  Time elapsed: 49.623 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.ozone.client.rpc.TestContainerStateMachineFailureOnRead.testReadStateMachineFailureClosesPipeline(TestContainerStateMachineFailureOnRead.java:204)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

{code}

{code:title=Location of NPE at TestContainerStateMachineFailureOnRead.java:204}
    // delete the container dir from leader
    FileUtil.fullyDelete(new File(
        leaderDn.get().getDatanodeStateMachine()
            .getContainer().getContainerSet()
            .getContainer(omKeyLocationInfo.getContainerID()).getContainerData() <-- this line
            .getContainerPath()));
{code}",TriagePending flaky-test ozone-flaky-test,['test'],HDDS,Sub-task,Major,2020-03-27 20:01:22,27
13294533,read operation failing when two container replicas are corrupted,"steps taken :

1) Mounted noise injection FUSE on all datanodes.

2) Write a key ( multi blocks)

3) Select one of the container ids ,  inject error on 2 container replicas for that container id.

4) Run GET key operation.

GET key operation fails intermittenly.

Error seen :

-------------

 
{noformat}
20/03/27 18:30:40 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
E 20/03/27 18:30:40 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
E 20/03/27 18:30:40 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
E 20/03/27 18:31:12 ERROR scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
E traceID: ""f80a51eaec481a1c:cbb8e92869015a53:f80a51eaec481a1c:0""
E containerID: 67
E datanodeUuid: ""96101390-2446-40e6-a54e-36e170497e57""
E readChunk {
E blockID {
E containerID: 67
E localID: 103896435892617248
E blockCommitSequenceId: 1010
E }
E chunkData {
E chunkName: ""103896435892617248_chunk_28""
E offset: 113246208
E len: 4194304
E checksumData {
E type: CRC32
E bytesPerChecksum: 1048576
E checksums: ""\034\376\313\031""
E checksums: "";U\225\037""
E checksums: ""\327m\332.""
E checksums: ""|\307\004E""
E }
E }
E }
E on the pipeline Pipeline[ Id: bce6316c-9690-452b-80e3-0f3590533444, Nodes: 96101390-2446-40e6-a54e-36e170497e57{ip: 172.27.111.129, host: quasar-olrywk-3.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}3e85204d-2399-43b5-952a-55b837eb4c1d{ip: 172.27.100.0, host: quasar-olrywk-1.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}5af0340a-6fee-4ce8-9f68-37fa35566a5a{ip: 172.27.73.0, host: quasar-olrywk-9.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:96101390-2446-40e6-a54e-36e170497e57, CreationTimestamp2020-03-27T03:36:51.880Z].
E Unexpected OzoneException: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 84603913ns. [remote_addr=/172.27.73.0:9859]]{noformat}
 

 

 ",fault_injection,['Ozone Datanode'],HDDS,Bug,Major,2020-03-27 18:37:06,16
13294514,Support Hadoop 3.3,Hadoop 3.3.1 is coming out soon. We should start testing Ozone on Hadoop 3.3,pull-request-available,[],HDDS,Task,Major,2020-03-27 17:05:07,1
13294510,Write operation when both OM followers are shutdown,"steps taken :
--------------
1. In OM HA environment, shutdown both OM followers.
2. Start PUT key operation.

PUT key operation is hung.

Cluster details : https://quasar-vwryte-1.quasar-vwryte.root.hwx.site:7183/cmf/home

Snippet of OM log on LEADER:


{code:java}
2020-03-24 04:16:46,249 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,249 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,250 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:46,250 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:46,750 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,750 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:46,750 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:46,750 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,250 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,251 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,251 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,251 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,751 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,751 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:47,752 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:47,752 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,252 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,252 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,252 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,252 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,752 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,752 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:48,753 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:48,753 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,254 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,254 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,254 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,254 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,754 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,754 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-03-24 04:16:49,754 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359
2020-03-24 04:16:49,754 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359
{code}

Reported by [~nilotpalnandi]
",pull-request-available,[],HDDS,Bug,Major,2020-03-27 16:44:16,13
13294467,Remove deprecated RandomKeyGenerator,Our first Freon test (RandomKeyGenerator) is depracated as we have all the functionalities with a simplified architecture (BaseFreonGenerator). We can remove it (especially as it's flaky...),TriagePending pull-request-available,[],HDDS,Improvement,Major,2020-03-27 13:41:10,61
13294452,Add a freon generator to create nested directories,"This Jira proposes to add a functionality to freon to create nested directories. Also, multiple child directories can be created inside the leaf directory and also multiple top level directories can be created.
This functionality will help to determine the scalability of ozone by creating nested directories(may be in millions) and also it will allow to control nested directories properties, just by passing appropriate parameters. This all can be done just by executing a single command.",pull-request-available,['Tools'],HDDS,Improvement,Major,2020-03-27 12:36:19,27
13294440,Update default RPC handler SCM/OM count to 100 ,"Presently, default PC handler count of {{ozone.scm.handler.count.key=10}} and {{ozone.om.handler.count.key=20}} are too small values and its good to increase the default values to a realistic value.
{code:java}
ozone.om.handler.count.key=100

ozone.scm.handler.count.key=100
{code}
 

 ",pull-request-available,"['Ozone Manager', 'SCM']",HDDS,Improvement,Minor,2020-03-27 11:42:06,40
13294344,BasicOzoneFileSystem  support batchDelete,"    Currently delete file is to get all the keys in the directory, and then delete one by one. And the same with rename. This makes for poor performance.

    By tested the deletion path with 100,000 files, which took 3718.70 sec. And rename it took 7327.936.

    HDDS-2939 can able to optimize this part, but at present the hdds-2939 is slow and still a long way to go. So we optimized the batch operation based on the current interface. We were able to get better performance with this PR before the hdds-2939 came in.",Triaged pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2020-03-27 03:35:07,4
13294235,MiniOzoneChaosCluster exits because of deadline exceeding,"2020-03-26 21:26:48,869 [pool-326-thread-2] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 1: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.
grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now

{code}
2020-03-26 21:26:48,866 [pool-326-thread-2] ERROR loadgenerators.LoadExecutors (LoadExecutors.java:load(64)) - FileSystem LOADGEN: null Exiting due to exception
java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:359)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:281)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:259)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:119)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:199)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:133)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:254)
        at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:197)
        at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:63)
        at java.io.DataInputStream.read(DataInputStream.java:100)
        at org.apache.hadoop.ozone.utils.LoadBucket$ReadOp.doPostOp(LoadBucket.java:205)
        at org.apache.hadoop.ozone.utils.LoadBucket$Op.execute(LoadBucket.java:121)
        at org.apache.hadoop.ozone.utils.LoadBucket$ReadOp.execute(LoadBucket.java:180)
        at org.apache.hadoop.ozone.utils.LoadBucket.readKey(LoadBucket.java:82)
        at org.apache.hadoop.ozone.loadgenerators.FilesystemLoadGenerator.generateLoad(FilesystemLoadGenerator.java:54)
        at org.apache.hadoop.ozone.loadgenerators.LoadExecutors.load(LoadExecutors.java:62)
        at org.apache.hadoop.ozone.loadgenerators.LoadExecutors.lambda$startLoad$0(LoadExecutors.java:78)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:336)
        ... 20 more
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now
        at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:533)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:442)
        at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
       at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:700)
        at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
        at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:399)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:510)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:66)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:630)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:518)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:692)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:681)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
{code}",MiniOzoneChaosCluster,['Ozone Datanode'],HDDS,Bug,Blocker,2020-03-26 16:06:10,16
13294147,ozonesecure-mr test fails due to lack of disk space,{{ozonesecure-mr}} acceptance test is failing with {{No space available in any of the local directories.}},pull-request-available,['test'],HDDS,Bug,Major,2020-03-26 09:14:04,1
13294059,Improve write efficiency by decreasing the number of force sync to disk,"The whole write cost about 140ms, and 110ms was cost by force sync to disk. The force sync operations are as follows.
*RocksDB*

1.  When create key, the response was put into [omDoubleBufferHelper|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyCreateRequest.java#L264], and sync to disk by [writeOptions.setSync(true)|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStoreBuilder.java#L171]
2.  When commit key, the response was put into [omDoubleBufferHelper|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyCommitRequest.java#L222], and sync to disk by [writeOptions.setSync(true)|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStoreBuilder.java#L171]

*Ratis*

1. leader and follower [sync |https://github.com/apache/incubator-ratis/blob/master/ratis-server/src/main/java/org/apache/ratis/server/raftlog/segmented/BufferedWriteChannel.java#L86] raftlog to disk

",TriagePending performance,[],HDDS,Improvement,Major,2020-03-26 01:34:53,65
13294048,ozone.http.filter.initializers can't be set properly for SPNEGO auth,"After HDDS-2950, we change to use ozone's own initializer defined by ozone.http.filter.initializers instead the one configured with hadoop.http.filter.initializers.

The FilterInitializer interface was also forked from hadoop common  that prevents us from using org.apache.hadoop.security.AuthenticationFilterInitializer with the following error. 

This ticket is opened to fix it. ",pull-request-available,[],HDDS,Bug,Blocker,2020-03-26 00:22:44,28
13294045,Add timeouts to all robot tests,"We have seen in some CI runs that the acceptance test suit is getting cancelled as it runs for more than 6 hours. Because of this, the test results and logs are also not saved. 

This Jira aims to add a 5 minute timeout to all robot tests. In case some tests require more time, we can update the timeout. This would help to isolate the test which could be causing the whole acceptance test suit to time out.",pull-request-available,['test'],HDDS,Bug,Major,2020-03-25 23:39:20,43
13294040,Ozone BaseHTTPServer should honor ozone.security.enabled config,Ozone BaseHTTPServer tries to start HTTP server with Spnego Principal and Keytab even if ozone.security.enabled flag is set to false. It should honor ozone.security.enabled flag and start the server accordingly.,pull-request-available,['Security'],HDDS,Bug,Major,2020-03-25 23:09:33,37
13293855,Rebase OFS branch,"Merge commits on master branch to OFS dev branch: {{git merge master}}

Also need to manually apply a couple of changes in master branch to OFS classes:
- HDDS-3049
- HDDS-2914 (HDDS-2188)
- HDDS-3065",pull-request-available,[],HDDS,Sub-task,Major,2020-03-25 15:44:50,12
13293714,OM HA: getconf must return all OMs,"Discovered by [~xyao] when testing 0.5.0-beta rc2:

 

ozone getconf -ozonemanagers does not return all the om instances
bash-4.2$ ozone getconf -ozonemanagers
0.0.0.0",pull-request-available,['Tools'],HDDS,Bug,Major,2020-03-25 04:36:32,86
13293713,Smoke Test: hdfs commands failing on hadoop 27 docker-compose,"Discovered by [~bharat] when testing 0.5.0-beta RC2.

 

 

issue when running hdfs commands on hadoop 27
docker-compose. I see the same test failing when running the smoke test.


$ docker exec -it c7fe17804044 bash

bash-4.4$ hdfs dfs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/kk

2020-03-22 04:40:14 WARN  NativeCodeLoader:60 - Unable to load
native-hadoop library for your platform... using builtin-java classes where
applicable

2020-03-22 04:40:15 INFO  MetricsConfig:118 - Loaded properties from
hadoop-metrics2.properties

2020-03-22 04:40:16 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot
period at 10 second(s).

2020-03-22 04:40:16 INFO  MetricsSystemImpl:191 - XceiverClientMetrics
metrics system started

-put: Fatal internal error

java.lang.NullPointerException: client is null

at java.util.Objects.requireNonNull(Objects.java:228)

at
org.apache.hadoop.hdds.scm.XceiverClientRatis.getClient(XceiverClientRatis.java:201)

at
org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:227)

at
org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:305)

at
org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:315)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:599)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:452)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:463)

at
org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:486)

at
org.apache.hadoop.ozone.[client.io|http://client.io/].BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:144)

at
org.apache.hadoop.ozone.client.io.KeyOutputStream.handleStreamAction(KeyOutputStream.java:481)

at
org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:455)

at
org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:508)

at
org.apache.hadoop.fs.ozone.OzoneFSOutputStream.close(OzoneFSOutputStream.java:56)

at
org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)

at
org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)

at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:62)

at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:120)

at
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:466)

at
org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:391)

at
org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:328)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:263)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:248)

at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:317)

at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:289)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:243)

at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:271)

at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)

at
org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:220)

at
org.apache.hadoop.fs.shell.CopyCommands$Put.processArguments(CopyCommands.java:267)

at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:201)

at org.apache.hadoop.fs.shell.Command.run(Command.java:165)

at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)

at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)

at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)

at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)


The same command when using ozone fs is working fine.

 docker exec -it fe5d39cf6eed bash

bash-4.2$ ozone fs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/kk

2020-03-22 04:41:10,999 [main] INFO impl.MetricsConfig: Loaded properties
from hadoop-metrics2.properties

2020-03-22 04:41:11,123 [main] INFO impl.MetricsSystemImpl: Scheduled
Metric snapshot period at 10 second(s).

2020-03-22 04:41:11,127 [main] INFO impl.MetricsSystemImpl:
XceiverClientMetrics metrics system started

bash-4.2$ ozone fs -ls o3fs://bucket1.vol1/

Found 1 items

-rw-rw-rw-   3 hadoop hadoop      17540 2020-03-22 04:41
o3fs://bucket1.vol1/kk",TriagePending,['test'],HDDS,Bug,Blocker,2020-03-25 04:34:34,6
13293652,Allow safemode listeners to be notified when some precheck rules pass,"As part of the SCM safemode process, there are some rules which must pass before safemode can be exited.

One of these rules is the number of registered datanodes and another is that at least one pipeline must be created and open.

Currently, pipeline creation is attempted each time a node registers. As soon as the 3rd node registers, pipelines will be created.

There are two issue with this:

1. With network topology, if the first 3 nodes are all from the same rack, a non-rackaware pipeline will get created.

2. With multi-raft, it would be better if more nodes are registered to allow the multiple pipelines per node to be spread across all the available nodes.

The proposal here, is to introduce a new sub-state into the safemode process, call ""preCheckComplete"". When adding rules to the Safemode Manager, some rules can be tagged as ""preCheck"" (eg number of datanodes registered). When all all the pre-check rules have passed a notification will be sent to all safemode listeners:

{code}
  safeModeIsOn -> true
  preCheckComplete -> true
{code}

That will allow the listener to take action on this first stage completing. In the case of PipelineManager, it will then allow pipelines to be created.

After the remaining rules have been passed, safemode will exit as normal, by sending a second event:

{code}
  safeModeIsOn -> false
  preCheckComplete -> true
{code}",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-03-24 21:52:17,11
13293648,CommitWatcher#watchForCommit does not timeout,"Seems the property *ozone.client.watch.request.timeout* was removed by HDDS-2920.  Note this is a client side property to wait for the future return. Without it, the client may wait for the future return forever in certain cases.  

",TriagePending,['Ozone Client'],HDDS,Bug,Major,2020-03-24 21:45:24,28
13293622,Replace ContainerCache in BlockUtils by LoadingCache,"As discussed in [here|https://github.com/apache/hadoop-ozone/pull/705] current version of ContainerCache is just used by BlockUtils and has several architectural issues. for example:
 * It uses a ReentrantLock which could be replaced by synchronized methods
 * It should maintain a referenceCount for each DBHandler
 * It extends LRUMap while it would be better to hide it by the composition and not expose LRUMap related methods.

As [~pifta] suggests, we could replace all ContainerCache functionality by using Guava LoadingCache.

This new LoadingCache could be configured to evict by size, by this configuration the functionality would be slightly different as it may evict DBHandlers while they are in use (referenceCount>0) but we can configure it to use reference base eviction based on CacheBuilder.weakValues() 

I want to open this discussion here instead of Github so I created this ticket.",pull-request-available,[],HDDS,Improvement,Minor,2020-03-24 19:37:37,80
13293561,Intermittent timeout in integration tests,"Even after the changes done in HDDS-3086, some integration tests (especially in it-freon) are intermittently timing out.",TriagePending,[],HDDS,Bug,Critical,2020-03-24 14:02:22,16
13293402,Initialize Recon metrics for prometheus at /prom endpoint,"The endpoint /prom is available for Recon from Base HTTP server, but the metrics are not initialized.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2020-03-23 22:35:13,37
13293358,Bump version to 0.6.0-SNAPSHOT,"As 0.5.0 release is almost here, our daily builds should use a newer version.",pull-request-available,[],HDDS,Improvement,Major,2020-03-23 17:52:06,6
13293318,Create a separate log file for Warnings and Errors in MiniOzoneChaosCluster,In MiniOzoneChaosCluster multiple log files get overlapped and that causes debugging difficult. This jira proposes to have a different log file for errors and warning,pull-request-available,['test'],HDDS,Bug,Major,2020-03-23 14:33:47,18
13293250,TestContainerCache fails if runs after TestBlockDeletingService ,"it fails on the line:

Assert.assertTrue(cache.isFull());

because the ContainerCache.INSTANCE was created before with a different size in TestBlockDeletingService.",pull-request-available,[],HDDS,Test,Minor,2020-03-23 08:58:07,80
13293248,TestHddsDispatcher and TestHandler fails if run after other tests,"TestHddsDispatcher and TestHandler fails if run after other tests with a error like this:

 
{code:java}
org.apache.hadoop.metrics2.MetricsException: Metrics source StorageContainerMetrics already exists! at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152) at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229) at org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics.create(ContainerMetrics.java:94) at org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher.createDispatcher(TestHddsDispatcher.java:271) at org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher.testCreateContainerWithWriteChunk(TestHddsDispatcher.java:156) ...
{code}
I think it's needed to call DefaultMetricsSystem.instance().shutdown() beforehand{color:#808080}
{color}

 ",pull-request-available,[],HDDS,Test,Minor,2020-03-23 08:49:22,80
13293215,"Include OM hostname info in ""getserviceroles"" subcommand of OM CLI","Currently,  ""getserviceroles"" subcommand of OM CLI displays only  node ID along with its serviceRole. 

ozone admin om getserviceroles -id=ozone1
om2 : FOLLOWER
om3 : FOLLOWER
om1 : LEADER

Need to include  Hostname info",Triaged pull-request-available,"['Ozone CLI', 'Ozone Manager']",HDDS,Improvement,Major,2020-03-23 02:39:58,20
13293138,Improve write efficiency by opening RocksDB only once,"when datanode create each container, a new RocksDB instance will be [created|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java#L76], but then the created RocksDB was [closed|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java#L83], until PutBlock the RocsDB will be [opend|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ContainerCache.java#L123] again, so the RocksDB was open twice.  [RocksDB.open|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/RocksDBStore.java#L68] cost about 150ms-200ms,  it's  a waste to open it twice.
  I will try to open the RocksDB only once.
",pull-request-available,[],HDDS,Improvement,Major,2020-03-22 02:26:29,65
13292948,Invalid container reported to SCM should be deleted,"For the invalid or out-updated container reported by Datanode, ContainerReportHandler in SCM only prints error log and doesn't 
 take any action.

{noformat}
2020-03-15 05:19:41,072 ERROR org.apache.hadoop.hdds.scm.container.ContainerReportHandler: Received container report for an unknown container 37 from datanode 0d98dfab-9d34-46c3-93fd-6b64b65ff543{ip: xx.xx.xx.xx, host: lyq-xx.xx.xx.xx, networkLocation: /dc2/rack1, certSerialId: null}.
org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: Container with id #37 not found.
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:542)
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.getContainerInfo(ContainerStateMap.java:188)
        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.getContainer(ContainerStateManager.java:484)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainer(SCMContainerManager.java:204)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:85)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:126)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:97)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:46)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2020-03-15 05:19:41,073 ERROR org.apache.hadoop.hdds.scm.container.ContainerReportHandler: Received container report for an unknown container 38 from datanode 0d98dfab-9d34-46c3-93fd-6b64b65ff543{ip: xx.xx.xx.xx, host: lyq-xx.xx.xx.xx, networkLocation: /dc2/rack1, certSerialId: null}.
org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: Container with id #38 not found.
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:542)
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.getContainerInfo(ContainerStateMap.java:188)
        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.getContainer(ContainerStateManager.java:484)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainer(SCMContainerManager.java:204)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:85)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:126)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:97)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:46)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
{noformat}

Actually SCM should inform Datanode to delete its outdated container. Otherwise, Datanode will always report this invalid container and this dirty container data will be always kept in Datanode. Sometimes, we bring back a node that be repaired and it maybe stores stale data and we should have a way to auto-cleanup them.

We could have a setting to control this auto-deletion behavior if this is a little risk approach.
 ",pull-request-available,[],HDDS,Bug,Major,2020-03-20 14:19:34,68
13292917,Improve write efficiency by creating container in parallel.,"Now follower cannot create container until leader finish creating container. But follower and leader can create container in parallel rather than in sequential.
*Why leader and follower create container in sequential now:*
1. From the code,  the [future thread|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L672] do getCachedStateMachineData  in readStateMachineData and the [future thread|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L459] do createContainer in writeStateMachineData  are the same [thread|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L505]. Because writeStateMachineData  called before readStateMachineData. So leader must wait createContainer finish then getCachedStateMachineData and append logs to the follower, so leader and follower are not independent in createContainer, follower must wait leader finish createContainer.  
2. From the jaeger UI, you can also see follower create container after leader finishing it currently.
 !screenshot-2.png! 

*How to improve it:*
I think this order can be improved by distinguishing the thread used by getCachedStateMachineData  and createContainer , and  [data = readStateMachineData(requestProto, term, logIndex)|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L619]  use same thread with createContainer . If [stateMachineDataCache.get(logIndex)|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L617] does not return null,  leader can get stateMachineData from cache and need not wait createContainer finish, thus leader and follower can be independent. But if it return null, leader must finish createContainer and then apennd logs to the follower, so I think [data = readStateMachineData(requestProto, term, logIndex)|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L619] should use the same thread with createContainer rather than the whole [getCachedStateMachineData|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java#L614]. ",pull-request-available,[],HDDS,Improvement,Major,2020-03-20 11:51:08,65
13292893,Provide message-level metrics from the generic protocol dispatch ,"Ozone RPC protocols (due to the limitation of protobuf) are very simple: There is only one message per service and the message is routed to the appropriate method based on the type. 

It makes very easy to add tracing / metrics as we have one generic dispatcher (OzoneProtocolMessageDispatcher.java) where we implemented opentracing support and basic metric collection.

In this patch I propose to improve this metric to add the message type to the metrics as a tag to make it easier to follow what's going on... (eg. see the distribution of the incoming message types)

We can also measure the total amount of time spent to serve one specific type of requests which can be used to calculate the moving average for latency.",pull-request-available,[],HDDS,Improvement,Major,2020-03-20 10:55:19,6
13292806,Recon should provide the list of datanodes that a missing container was present in.,"Currently, Recon does not have a way to determine the datanodes that had replica of a missing container. Add this information to the missing containers endpoint response. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-20 00:26:22,37
13292687,Fix Dropwizard metrics mapping for latest Ratis metrics,"Ratis use dropwizard metrics where the key parameters of the metrics (like group name or instance id) are part of the name of the metrics instead of using a tag.

For example
{code:java}
ratis.log_appender.851cb00a-af97-455a-b079-d94a77d2a936@group-C14654DE8C2C.follower_65f881ea-8794-403d-be77-a030ed79c341_match_index {code}
Instead of
{code:java}
ratis.log_appender_match_index{group=""group-C14654DE8C2C"",...} {code}
It makes hard to combine the same metrics (match_index) from different sources.

HDDS-2950 implemented a regexp based workaround, but the regexp doesn't match for the latest Ratis metrics.",pull-request-available,[],HDDS,Improvement,Major,2020-03-19 12:07:28,6
13292626,Change to default of max retry count for Ozone client ,"Currently, default value of ""ozone.client.max.retries"" is 100. This number coupled with ratis client max retry count set to 180 is too high for any operation to fail/succeed. The idea here is to limit the no of maxRetry count for ozone client retry to 5 or so.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-03-19 06:06:04,26
13292617,Fix retry interval default in Ozone client,"{code:java}
    <name>dfs.ratis.client.request.retry.interval</name>
    <value>1000ms</value>
{code}

Change the default value to 15s. As with 1s sleep, we will see retry happening at a very fast interval. 
In the billion object test, we see after changing the value to 15s, queue limit has never reached the limit.
",TriagePending billiontest pull-request-available,[],HDDS,Bug,Major,2020-03-19 04:43:46,13
13292454,2-way commit did not happen when WRITE failure injected in one of the datanodes of a piepeline,"This is an extension of bug HDDS-3214.

steps taken :

1) Mounted noise injection FUSE on all datanodes

2) Selected 1 datanode from each open pipeline (factor=3)

3) Injected WRITE FAILURE noise with error code - ENOENT on ""hdds.datanode.dir"" path of list of datanodes selected in step 2)

4) start PUT key operation of size  32 MB.

 

Observation :

----------------

PUT key operation failed. 

As there is a WRITE failure in one of the datanodes in the pipeline, 3 way commit should fail.

But it should proceed with 2-way commit and the operation should have been successful.

 

 ",fault_injection,[],HDDS,Bug,Major,2020-03-18 12:19:46,16
13292444,Ensure eviction of stateMachineData from cache only when both followers catch up,"Currently, the data in the StateMachineCache is evicted as soon as the applyTransaction call is issued for a transaction in Ratis. In our testing with keys in few kbs of size, it was figured that the data is evicted from the cache before append requests can be processed in a slightly slow follower thereby making leader read the chunk data from underlying fs/disk very frequently. This leads to slowing down the leader as well as well as overall throughput of the pipeline. 

The idea here is to ensure the data is evicted from the cache only when both followers have caught up with the match index. If a follower is really slow, it will eventually be marked slow after nodeFailureTimeout and pipeline will be destroyed.",Triaged,['Ozone Datanode'],HDDS,Bug,Major,2020-03-18 11:28:00,16
13292292,Tracing Ozone Manager DB write batch operations ,"Currently the OM DB write is asynchronously handled in OzoneManagerDoubleBuffer. But the OM response does not have traceID properly populated. As a result, we can't get insight for the DB part of the OM request handling. 

This ticket is opened to add traceID properly so that the addBatch and commitBatch cost of the request can be shown up in properly in Opentracing/Jaeger. ",pull-request-available,[],HDDS,Improvement,Major,2020-03-17 23:18:49,28
13292123,Enforce volume and bucket name rule at create time,"After volume is created through API application, it cannot be listed through CLI. 
ozone sh volume list -u=admin
{noformat}
{
  ""metadata"" : { },
  ""name"" : ""instagram"",
  ""admin"" : ""admin"",
  ""owner"" : ""admin"",
  ""creationTime"" : ""2020-03-04T07:18:38.183Z"",
  ""acls"" : [ ],
  ""quota"" : 1152921504606846976
}
{
  ""metadata"" : { },
  ""name"" : ""3"",
  ""admin"" : ""admin"",
  ""owner"" : ""admin"",
  ""creationTime"" : ""2020-03-17T07:31:56.987Z"",
  ""acls"" : [ ],
  ""quota"" : 1152921504606846976
}
{
  ""metadata"" : { },
  ""name"" : ""333"",
  ""admin"" : ""admin"",
  ""owner"" : ""admin"",
  ""creationTime"" : ""2020-03-17T07:34:18.361Z"",
  ""acls"" : [ ],
  ""quota"" : 1152921504606846976
}
{noformat}
ozone sh bucket list 3       
Bucket or Volume length is illegal, valid length is 3-63 characters

ozone sh bucket list 333     
Bucket or Volume name cannot be an IPv4 address or all numeric

We should enforece this rules at the create time. 
",pull-request-available,[],HDDS,Bug,Major,2020-03-17 07:40:02,86
13292100,Improve s3g read 1GB object efficiency by 100 times ,"*What's the problem ?*
Read 1000M object, it cost about 470 seconds, i.e. 2.2M/s, which is too slow. 

*What's the reason ?*
When read 1000M file, there are 50 GET requests, each GET request read 20M. When do GET, the stack is: [IOUtils::copyLarge|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java#L262] -> [IOUtils::skipFully|https://github.com/apache/commons-io/blob/master/src/main/java/org/apache/commons/io/IOUtils.java#L1190] -> [IOUtils::skip|https://github.com/apache/commons-io/blob/master/src/main/java/org/apache/commons/io/IOUtils.java#L2064] -> [InputStream::read|https://github.com/apache/commons-io/blob/master/src/main/java/org/apache/commons/io/IOUtils.java#L1957].

It means, the 50th GET request which should read 980M-1000M, but to skip 0-980M, it also [InputStream::read|https://github.com/apache/commons-io/blob/master/src/main/java/org/apache/commons/io/IOUtils.java#L1957] 0-980M. So the 1st GET request read 0-20M, the 2nd GET request read 0-40M, the 3rd GET request read 0-60M, ..., the 50th GET request read 0-1000M. So the GET  request from 1st-50th become slower and slower.

You can also refer it [here|https://issues.apache.org/jira/browse/IO-203] why IOUtils implement skip by read rather than real skip, e.g. seek.",pull-request-available,[],HDDS,Improvement,Critical,2020-03-17 03:30:51,65
13292084,Add integration test for Recon FSCK.,Recon tracks the containers that are missing in the cluster. We have to add an integration tests that mimics this scenario to make sure there are no regressions along Recon's receipt of this information and subsequent processing. ,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-17 00:09:05,30
13292082,Refactor SafeModeHandler to use a Notification Interface,"The SafeModeHandler currently accepts several objects which it notifies when the safe mode status changes.

Each of these object are notified using a different method (there is no ""notification interface"") and some of the logic which really belongs in those objects (ie what to do when safemode goes on or off) is in the safemode classes rather than in the receiving class.

As we may need to extend safemode somewhat to delay pipeline creation until sufficient nodes have registered, I think it is worthwhile to refactor this area to do the following:

1. Introduce a new Interface ""SafeModeTransition"" which must be implemented by any object which wants to listen for safemode starting or ending.

{code}
public interface SafeModeTransition {
  void handleSafeModeTransition(SCMSafeModeManager.SafeModeStatus status);
}
{code}

2. Pass the SafeModeStatus object over this new interface. That way, we can extend SafeModeStatus to include more states in the future than just safemode = true / false.

3. Change the constructor of SafeModeHandler to allow any number of objects to be registered to make it more flexible going forward.

4. Ensure the logic of what action to take on safemode transition lives within the notified objects rather than in the Safemode clases.",pull-request-available,[],HDDS,Improvement,Major,2020-03-16 23:11:01,11
13292072,Filesystem client should not retry on AccessControlException,"When running an ofs/o3fs operation without kerberos credentials, the client seems to go in a tight retry loop, before eventually giving up.

In this case no retry should be necessary.

Also the retry policy may need another look, there should be some wait between the retries.",pull-request-available,[],HDDS,Bug,Critical,2020-03-16 21:33:18,86
13292028,Datanode startup is slow due to iterating container DB 2-3 times,"During Datanode startup, for each container we iterate 2 times entire DB
1. For Setting block length
2. For finding delete Key count.

And for open containers, we do step 1 again.

*Code Snippet:*
*ContainerReader.java:*

*For setting Bytes Used:*
{code:java}
      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()
          .getRangeKVs(null, Integer.MAX_VALUE,
              MetadataKeyFilters.getNormalKeyFilter());

      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {
        BlockData blockData;
        try {
          blockData = BlockUtils.getBlockData(e.getValue());
          return blockData.getSize();
        } catch (IOException ex) {
          return 0L;
        }
      }).sum();
      kvContainerData.setBytesUsed(bytesUsed);
{code}

*For setting pending deleted Key count*

{code:java}
          MetadataKeyFilters.KeyPrefixFilter filter =
              new MetadataKeyFilters.KeyPrefixFilter()
                  .addFilter(OzoneConsts.DELETING_KEY_PREFIX);
          int numPendingDeletionBlocks =
              containerDB.getStore().getSequentialRangeKVs(null,
                  Integer.MAX_VALUE, filter)
                  .size();
          kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);
{code}

*For open Containers*

{code:java}
          if (kvContainer.getContainerState()
              == ContainerProtos.ContainerDataProto.State.OPEN) {
            // commitSpace for Open Containers relies on usedBytes
            initializeUsedBytes(kvContainer);
          }
{code}


*Jstack of DN during startup*
{code:java}
""Thread-8"" #34 prio=5 os_prio=0 tid=0x00007f5df5070000 nid=0x8ee runnable [0x00007f4d840f3000]
   java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksIterator.next0(Native Method)
        at org.rocksdb.AbstractRocksIterator.next(AbstractRocksIterator.java:70)
        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:195)
        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:155)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:158)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:191)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:168)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:146)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:101)
        at java.lang.Thread.run(Thread.java:748)
{code}
",billiontest pull-request-available,['Ozone Datanode'],HDDS,Improvement,Blocker,2020-03-16 17:52:36,13
13291916,Unhealthy datanodes repeatedly participate in pipeline creation,"steps taken :

1) Mounted noise injection FUSE on all datanodes

2) Selected 1 datanode from each open pipeline (factor=3)

3) Injected WRITE FAILURE noise with error code - ENOENT on ""hdds.datanode.dir"" path of list of datanodes selected in step 2)

4) start PUT key operation of size  32 MB.

 

Observation :

----------------
 # Commit failed, pipelines were moved to exclusion list.
 # Client retries , new pipeline is created with same set of datanodes. Container creation fails as WRITE  FAILURE injection present.
 # Pipeline is closed and the process is repeated for ""ozone.client.max.retries"" retries.

Everytime, same set of datanodes are selected for pipeline creation which include 1 unhealthy datanode. 

Expectation - pipeline should have been created by selecting 3 healthy  datanodes available.

 

cc - [~ljain]

 ",TriagePending fault_injection,['SCM'],HDDS,Bug,Blocker,2020-03-16 08:18:41,44
13291909,Allow Enabling Purge SCM Ratis log,Based on Ratis snapshot.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-16 07:47:40,54
13291900, DeleteBlock via Ratis in SCM HA,DeleteBlock needs to add/update/delete DELETED_BLOCKS_TABLE in SCMMetadataDB. So SCM does these operations via ratis.,pull-request-available,[],HDDS,Sub-task,Major,2020-03-16 07:39:00,65
13291884,New PipelineManager interface to persist to RatisServer,This applies to DestroyPipeline as well createPipeline,Triaged pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-16 06:31:58,77
13291813,Switch current pipeline interface to the new Replication based interface to write to Ratis,"Due to consistency concern, SCM needs to applyTransaction to RaftLog before it writes to local database and in memory maps. Need refactor the current codes to put this part to Ratis.

Ratis will write to DB on behalf of SCM.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-15 13:58:11,36
13291809,Add failover proxy to SCM block protocol,Need to supports 2N + 1 SCMs. Add configs and logic to support multiple SCMs.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-15 13:48:40,66
13291808,SCM StateMachine,SCM needs a StateMachine to manage states. StateMachine supports applyTransaction and call RatisServer API.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-15 13:47:04,77
13291807,Introduce generic SCMRatisRequest and SCMRatisResponse,This jira will introduce generic SCMRatisRequest and SCMRatisResponse which will be used by all the Ratis operations inside SCM. We will also have a generic StateMachine which will dispatch the request to registered handlers.,pull-request-available,['SCM HA'],HDDS,Sub-task,Major,2020-03-15 13:45:31,19
13291806,Standalone SCM RatisServer,"In order to have Ratis support, SCM need RatisServer. This task is to start from Standalone RatisServer in SCM.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-15 13:43:46,77
13291797,Fix MiniOzoneChaosTest to set the correct defaults.,Fix MiniOzoneChaosCluster by adding correct defaults,pull-request-available,['test'],HDDS,Bug,Major,2020-03-15 10:57:36,18
13291780,Deprecate old Recon HTTP Server Keytab config key,The current config key for Recon HTTP Server Keytab file is `ozone.recon.keytab.file`. It needs to renamed to `ozone.recon.http.kerberos.keytab.file` for consistency.,pull-request-available,['Ozone Recon'],HDDS,Task,Minor,2020-03-15 07:18:07,37
13291779,Add unit tests to Recon Frontend,Add enzyme and jest libraries as dev dependencies and improve code coverage with unit tests in Recon UI.,Triaged,['Ozone Recon'],HDDS,Task,Major,2020-03-15 07:12:32,37
13291773,Intermittent failure in TestReconWithOzoneManager due to BindException,"TestReconWithOzoneManager may fail with BindException:

{code:title=https://github.com/apache/hadoop-ozone/pull/677/checks?check_run_id=507376007}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.707 s <<< FAILURE! - in org.apache.hadoop.ozone.recon.TestReconWithOzoneManager
org.apache.hadoop.ozone.recon.TestReconWithOzoneManager  Time elapsed: 19.706 s  <<< ERROR!
picocli.CommandLine$ExecutionException: Error while calling command (org.apache.hadoop.ozone.recon.ReconServer@23f74a49): java.net.BindException: Port in use: 0.0.0.0:36263
	...
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:534)
	at org.apache.hadoop.ozone.recon.TestReconWithOzoneManager.init(TestReconWithOzoneManager.java:109)
	...
Caused by: java.net.BindException: Port in use: 0.0.0.0:36263
	at org.apache.hadoop.hdds.server.http.HttpServer2.constructBindException(HttpServer2.java:1200)
	at org.apache.hadoop.hdds.server.http.HttpServer2.bindForSinglePort(HttpServer2.java:1222)
	at org.apache.hadoop.hdds.server.http.HttpServer2.openListeners(HttpServer2.java:1281)
	at org.apache.hadoop.hdds.server.http.HttpServer2.start(HttpServer2.java:1136)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:252)
	at org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:128)
	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:106)
	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50)
	at picocli.CommandLine.execute(CommandLine.java:1173)
	... 27 more
{code}

{code:title=test output}
2020-03-14 06:17:08,677 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(284)) - HTTP server of ozoneManager listening at http://0.0.0.0:36263
...
2020-03-14 06:17:11,589 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(170)) - Starting Web-server for recon at: http://0.0.0.0:36263
...
2020-03-14 06:17:12,756 [main] INFO  recon.ReconServer (ReconServer.java:start(125)) - Starting Recon server
2020-03-14 06:17:12,757 [main] INFO  http.HttpServer2 (HttpServer2.java:start(1139)) - HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:36263
...
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-03-15 06:14:08,1
13291718,Datanode fails to start due to confused inconsistent volume state,"I meet an error in my testing ozone cluster when I restart datanode. From the log, it throws inconsistent volume state but without other detailed helpful info:
{noformat}
2020-03-14 02:31:46,204 [main] INFO  (LogAdapter.java:51)     - registered UNIX signal handlers for [TERM, HUP, INT]
2020-03-14 02:31:46,736 [main] INFO  (HddsDatanodeService.java:204)     - HddsDatanodeService host:lyq-xx.xx.xx.xx ip:xx.xx.xx.xx
2020-03-14 02:31:46,784 [main] INFO  (HddsVolume.java:177)     - Creating Volume: /tmp/hadoop-hdfs/dfs/data/hdds of storage type : DISK and capacity : 20063645696
2020-03-14 02:31:46,786 [main] ERROR (MutableVolumeSet.java:202)     - Failed to parse the storage location: file:///tmp/hadoop-hdfs/dfs/data
java.io.IOException: Volume is in an INCONSISTENT state. Skipped loading volume: /tmp/hadoop-hdfs/dfs/data/hdds
        at org.apache.hadoop.ozone.container.common.volume.HddsVolume.initialize(HddsVolume.java:226)
        at org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:180)
        at org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:71)
        at org.apache.hadoop.ozone.container.common.volume.HddsVolume$Builder.build(HddsVolume.java:158)
        at org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet.createVolume(MutableVolumeSet.java:336)
        at org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet.initializeVolumeSet(MutableVolumeSet.java:183)
        at org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet.<init>(MutableVolumeSet.java:139)
        at org.apache.hadoop.ozone.container.common.volume.MutableVolumeSet.<init>(MutableVolumeSet.java:111)
        at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.<init>(OzoneContainer.java:97)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.<init>(DatanodeStateMachine.java:128)
        at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:235)
        at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:179)
        at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:154)
        at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:78)
        at picocli.CommandLine.execute(CommandLine.java:1173)
        at picocli.CommandLine.access$800(CommandLine.java:141)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
        at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
        at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
        at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
        at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
        at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
        at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:137)
2020-03-14 02:31:46,795 [shutdown-hook-0] INFO  (LogAdapter.java:51)     - SHUTDOWN_MSG:
{noformat}

Then I look into the code and the root cause is that the version file was lost in that node.
We need to log key message as well to help user quickly know the root cause of this.",pull-request-available,[],HDDS,Improvement,Major,2020-03-14 12:06:55,68
13291670,Pipeline placement based on Topology does not have fall back protection,"When rack awareness and topology is enabled, pipeline placement can fail when there is only one node on the rack.

 

Should add fall back logic to search for nodes from other racks.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-03-14 01:51:27,77
13291651,Add unit tests for OMGetDelegationToken Request and Response,The OM Client Request OMGetDelegationTokenRequest should have unit tests similar to other client requests/ responses to test its functionality.,OMHATest pull-request-available,[],HDDS,Sub-task,Major,2020-03-13 21:14:29,31
13291618,Remove unused dependency version strings,"After the repo was split from hadoop, there are a few unused dependencies/version strings left in pom.xml. They can be removed.

Example: 

{code}
    <hbase.one.version>1.2.6</hbase.one.version>
    <hbase.two.version>2.0.0-beta-1</hbase.two.version>
{code}
There may be more.",newbie,[],HDDS,Task,Minor,2020-03-13 17:21:24,1
13291553,Healthy datanodes are marked as stale,"healthy datanodes are marked as stale due to which pipelines are not getting created

 

scm log snippet:
{noformat}
020-03-08 18:06:03,613 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode c03617f8-ff70-4cc6-bdf4-33441ca71471\{ip: 172.27.106.64, host: quasar-elfnqw-2.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=dc0e7e66-c40e-4bac-86d2-3b311db482c7, PipelineID=ac390e8e-3a19-45f4-9b28-d96f1deabfca]
2020-03-08 18:06:03,614 INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dc0e7e66-c40e-4bac-86d2-3b311db482c7, Nodes: c03617f8-ff70-4cc6-bdf4-33441ca71471\{ip: 172.27.106.64, host: quasar-elfnqw-2.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-03-08T18:01:02.964455Z]
2020-03-08 18:06:03,614 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline Pipeline[ Id: dc0e7e66-c40e-4bac-86d2-3b311db482c7, Nodes: c03617f8-ff70-4cc6-bdf4-33441ca71471\{ip: 172.27.106.64, host: quasar-elfnqw-2.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:null, CreationTimestamp2020-03-08T18:01:02.964455Z] moved to CLOSED state
2020-03-08 18:06:03,620 INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ac390e8e-3a19-45f4-9b28-d96f1deabfca, Nodes: 2ba0ecb0-0739-4da9-9541-5fef23479f28\{ip: 172.27.138.192, host: quasar-elfnqw-7.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}bbc2192c-382e-45c9-979b-912108b7e915\{ip: 172.27.86.128, host: quasar-elfnqw-3.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}c03617f8-ff70-4cc6-bdf4-33441ca71471\{ip: 172.27.106.64, host: quasar-elfnqw-2.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:bbc2192c-382e-45c9-979b-912108b7e915, CreationTimestamp2020-03-08T18:01:05.580596Z]
2020-03-08 18:06:03,620 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ac390e8e-3a19-45f4-9b28-d96f1deabfca, Nodes: 2ba0ecb0-0739-4da9-9541-5fef23479f28\{ip: 172.27.138.192, host: quasar-elfnqw-7.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}bbc2192c-382e-45c9-979b-912108b7e915\{ip: 172.27.86.128, host: quasar-elfnqw-3.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}c03617f8-ff70-4cc6-bdf4-33441ca71471\{ip: 172.27.106.64, host: quasar-elfnqw-2.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:bbc2192c-382e-45c9-979b-912108b7e915, CreationTimestamp2020-03-08T18:01:05.580596Z] moved to CLOSED state
2020-03-08 18:06:06,613 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode 2ba0ecb0-0739-4da9-9541-5fef23479f28\{ip: 172.27.138.192, host: quasar-elfnqw-7.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=0321db64-fa26-4c5a-a45e-59f6ab1d31c4, PipelineID=ac390e8e-3a19-45f4-9b28-d96f1deabfca]
2020-03-08 18:06:06,613 INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 0321db64-fa26-4c5a-a45e-59f6ab1d31c4, Nodes: 2ba0ecb0-0739-4da9-9541-5fef23479f28\{ip: 172.27.138.192, host: quasar-elfnqw-7.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-03-08T18:01:05.548579Z]
2020-03-08 18:06:06,613 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 0321db64-fa26-4c5a-a45e-59f6ab1d31c4, Nodes: 2ba0ecb0-0739-4da9-9541-5fef23479f28\{ip: 172.27.138.192, host: quasar-elfnqw-7.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:null, CreationTimestamp2020-03-08T18:01:05.548579Z] moved to CLOSED state
2020-03-08 18:06:06,614 INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ac390e8e-3a19-45f4-9b28-d96f1deabfca, Nodes: 2ba0ecb0-0739-4da9-9541-5fef23479f28\{ip: 172.27.138.192, host: quasar-elfnqw-7.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}bbc2192c-382e-45c9-979b-912108b7e915\{ip: 172.27.86.128, host: quasar-elfnqw-3.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}c03617f8-ff70-4cc6-bdf4-33441ca71471\{ip: 172.27.106.64, host: quasar-elfnqw-2.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:bbc2192c-382e-45c9-979b-912108b7e915, CreationTimestamp2020-03-08T18:01:05.580596Z]
2020-03-08 18:06:09,613 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode 9cf2c807-18d9-41bf-8abb-465cba14e26e\{ip: 172.27.82.64, host: quasar-elfnqw-9.quasar-elfnqw.root.hwx.site, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=62a7a71b-8f0a-43ac-8a04-b72fe0c3549d]
{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-03-13 13:29:05,35
13291549,Rebalance integration tests,"With more integration tests enabled recently, it-client-and-hdds takes much more time (~50 mins) than other splits (20-25 mins), increasing overall delay in getting test results.",pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-03-13 13:08:21,1
13291510,Use DBStore instead of  MetadataStore in SCM,"The MetadataStore interface provides a generic view to any key / value store with a LevelDB and RocksDB implementation.

Since the early version of MetadataStore we also go the DBStore interface which is more andvanced (it supports DB profiles and ColumnFamilies).

To simplify the introduction of new features (like versioning or rocksdb tuning) we should use the new interface everywhere instead of the old interface.

We should update SCM and Datanode to use the DBStore instead of MetadataStore. ",backward-incompatible pull-request-available,[],HDDS,Improvement,Critical,2020-03-13 08:58:08,6
13291413,Fix issues in File count by size task.,"* Handle DELETE key operation correctly.
* Handle PUT key operation for an existing key.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-12 19:34:59,30
13291218,Improve read efficiency by merging a lot of RPC call getContainerWithPipeline into one, !screenshot-3.png! ,pull-request-available,[],HDDS,Improvement,Major,2020-03-12 03:06:27,65
13291166,Integrate Recon missing containers UI with endpoint.,Integrate missing containers endpoint (/api/v1/containers/missing) with Recon UI.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-11 20:18:30,37
13291165,Add Recon endpoint to serve missing containers and its metadata.,Add a Recon API endpoint to serve missing containers information from Recon DB.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-11 20:16:50,37
13291126,write Key is hung when write delay is injected in datanode dir,"steps taken :

-------------

1. Mounted noise injection FUSE on all datanodes.

2. Select one datanode from each open pipeline

3. Inject delay of 120 seconds on chunk file path of selected datanodes

4. Start PUT key operation.

PUT Key operation is stuck and does not return any success/error .",TriagePending fault_injection,['Ozone Datanode'],HDDS,Bug,Major,2020-03-11 16:31:39,35
13291074,we need to block illegal characters when creating keys,"Currently, ozone does not impose effective restrictions on illegal characters when creating keys.

!image-2020-03-11-20-35-16-762.png|width=563,height=80!",Triaged pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-03-11 12:41:49,71
13291071,Disable index and filter block cache for RocksDB,"During preformance tests It was noticed that the OM performance is dropped after 10-20 million of keys. (see the screenshot).

By default cache_index_and_filter_blocks is enabled for all of our RocksDB instances (see DBProfile) which is not the best option. (For example see this thread: https://github.com/facebook/rocksdb/issues/3961#)

With turning on this cache the indexes and bloom filters are cached **inside the block cache** which makes slower the cache when we have significant data.

Without turning it on (based on my understanding) all the indexes will remain open without any cache. With our current settings we have only a few number of sst files (even with million of keys) therefore it seems to be safe to turn this option off.

With turning this option of I was able to write >100M keys with high throughput. ",pull-request-available,[],HDDS,Improvement,Minor,2020-03-11 12:31:23,6
13291070,Bump RocksDB version to the latest one,"6.0.1 -- our current version from RocksDB -- released one year ago. Since than many new versions are released with important bug fixes.

I propose to update to the latest one...",pull-request-available,[],HDDS,Improvement,Minor,2020-03-11 12:30:23,6
13291002,update allocateContainer to remove additional createPipeline step.,"AllocateContainer now tries allocatePipelines. But with multi-raft, it should not worry about whether there are available pipelines.",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-03-11 07:10:01,77
13290970,Improved ozone client flush implementation to make it faster.,"Background:

    When we execute mapreduce in the ozone, we find that the task will be stuck for a long time after the completion of Map and Reduce. The log is as follows:
{code:java}
//Refer to the attachment: stdout
20/03/05 14:43:30 INFO mapreduce.Job: map 100% reduce 33% 
20/03/05 14:43:33 INFO mapreduce.Job: map 100% reduce 100% 
20/03/05 15:29:52 INFO mapreduce.Job: Job job_1583385253878_0002 completed successfully{code}
    By looking at AM's log(Refer to the amlog for details), we found that the time of over 40 minutes is AM writing a task log into ozone.

    At present, after MR execution, the Task information is recorded into the log on HDFS or ozone by AM.  Moreover, the task information is flush to HDFS or ozone one by one ([details|https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java#L1640]). The problem occurs when the number of task maps is large. 

     Currently, each flush operation in ozone generates a new chunk file in real time on the disk. This approach is not very efficient at the moment. For this we can refer to the implementation of HDFS flush. Instead of writing to disk each time flush writes the contents of the buffer to the datanode's OS buffer. In the first place, we need to ensure that this content can be read by other datanodes.

 ",pull-request-available,[],HDDS,Improvement,Blocker,2020-03-11 03:25:48,4
13290917,Intermittent failure in Test2WayCommitInRatis,"Test2WayCommitInRatis may fail due to {{TimeoutIOException: Request #8 timeout 3s}} from Ratis while closing the container.  [~shashikant], can you please take a look? 
 Logs with RaftClient set to debug level attached.",pull-request-available,[],HDDS,Bug,Major,2020-03-10 21:33:24,16
13290901,Create REST API to serve Recon Dashboard and integrate with UI in Recon.,"Add a REST API to serve information required for recon dashboard

!Screen Shot 2020-03-10 at 12.10.41 PM.png!",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-10 19:11:18,37
13290799,Reduce number of chunkwriter threads in integration tests,"Integration tests run multiple datanodes in the same JVM.  Each datanode comes with 60 chunk writer threads by default (may be decreased in HDDS-3053).  This makes thread dumps (eg. produced by {{GenericTestUtils.waitFor}} on timeout) really hard to navigate, as there may be 300+ such threads.

Since integration tests are generally run with a single disk which is even shared among the datanodes, a few threads per datanode should be enough.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-03-10 12:55:21,1
13290769,Intermittent timeout in TestCloseContainerHandlingByClient#testMultiBlockWrites3#testDiscardPreallocatedBlocks,"{code:title=https://github.com/apache/hadoop-ozone/runs/495906854}
Tests run: 8, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 210.963 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestCloseContainerHandlingByClient
testMultiBlockWrites3(org.apache.hadoop.ozone.client.rpc.TestCloseContainerHandlingByClient)  Time elapsed: 108.777 s  <<< ERROR!
java.util.concurrent.TimeoutException:
...
  at org.apache.hadoop.ozone.container.TestHelper.waitForContainerClose(TestHelper.java:251)
  at org.apache.hadoop.ozone.container.TestHelper.waitForContainerClose(TestHelper.java:151)
  at org.apache.hadoop.ozone.client.rpc.TestCloseContainerHandlingByClient.waitForContainerClose(TestCloseContainerHandlingByClient.java:342)
  at org.apache.hadoop.ozone.client.rpc.TestCloseContainerHandlingByClient.testMultiBlockWrites3(TestCloseContainerHandlingByClient.java:310)
{code}",pull-request-available,['test'],HDDS,Sub-task,Major,2020-03-10 10:15:25,27
13290642,Implement getIfExist in Table and use it in CreateKey/File,"With replay, now we use directly get() API.

Previously the code

OMKeyRequest.java

 
{code:java}
else if (omMetadataManager.getKeyTable().isExist(dbKeyName)) {
 // TODO: Need to be fixed, as when key already exists, we are
 // appending new blocks to existing key.
 keyInfo = omMetadataManager.getKeyTable().get(dbKeyName);{code}
 

Now for every create key/File we use get API, this is changed for replay
{code:java}
OmKeyInfo dbKeyInfo =
 omMetadataManager.getKeyTable().get(dbKeyName);
if (dbKeyInfo != null) {{code}

The proposal is to change get with getIfExist, and make use of keyMayExist.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Blocker,2020-03-09 20:34:44,13
13290565,Make o3fs support set and get acl,"Currently, ozone supports acl operations, but FileSystem through the hadoop API does not. For example:

!image-2020-03-13-15-14-44-840.png|width=876,height=41!

This jira will be used to refine this interface.",Triaged pull-request-available,['Ozone Filesystem'],HDDS,Sub-task,Major,2020-03-09 15:29:10,4
13290555,Logs cluttered by AlreadyExistsException from Ratis,"Ozone startup logs are cluttered by printing stack trace of AlreadyExistsException related to group addition.  Example:

{code}
2020-03-09 13:53:01,563 [grpc-default-executor-0] WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$groupAddAsync$11(390)) - 7a07f161-9144-44b2-8baa-73f0e9299675: Failed groupAdd* GroupManagementRequest:client-27FB1A91809E->7a07f161-9144-44b2-8baa-73f0e9299675@group-E151028E3AC0, cid=2, seq=0, RW, null, Add:group-E151028E3AC0:[18f4e257-bf09-482e-b1bb-a2408a093ff7:172.17.0.2:43845, 7a07f161-9144-44b2-8baa-73f0e9299675:172.17.0.2:41551, 8a66c80e-ab55-4975-92a9-8aaf06ab418a:172.17.0.2:36921]
java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 7a07f161-9144-44b2-8baa-73f0e9299675: Failed to add group-E151028E3AC0:[18f4e257-bf09-482e-b1bb-a2408a093ff7:172.17.0.2:43845, 7a07f161-9144-44b2-8baa-73f0e9299675:172.17.0.2:41551, 8a66c80e-ab55-4975-92a9-8aaf06ab418a:172.17.0.2:36921] since the group already exists in the map.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:631)
	at java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2006)
	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.protocol.AlreadyExistsException: 7a07f161-9144-44b2-8baa-73f0e9299675: Failed to add group-E151028E3AC0:[18f4e257-bf09-482e-b1bb-a2408a093ff7:172.17.0.2:43845, 7a07f161-9144-44b2-8baa-73f0e9299675:172.17.0.2:41551, 8a66c80e-ab55-4975-92a9-8aaf06ab418a:172.17.0.2:36921] since the group already exists in the map.
	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
	... 13 more
{code}

Since these are ""normal"", I think stack trace should be suppressed.

CC [~nanda]",pull-request-available,['Ozone Datanode'],HDDS,Wish,Major,2020-03-09 14:48:08,86
13290532,Rename silently ignored tests,"Surefire plugin is configured to run {{Test*}} classes, but there are two test classes named {{*Test}}:

{code}
$ find */*/src/test/java -name '*Test.java' | xargs grep -l '@Test'
hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/HddsServerUtilTest.java
hadoop-ozone/insight/src/test/java/org/apache/hadoop/ozone/insight/LogSubcommandTest.java
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-03-09 13:34:54,1
13290503,Create isolated environment for OM to test it without SCM,"OmKeyGenerator class from Freon can generate keys (open key + commit key). But this test tests both OM and SCM performance. It seems to be useful to have a method to test only the OM performance with faking the response from SCM.  

Can be done easily with the same approach what we have in HDDS-3023: A simple utility class can be implemented and with byteman we can replace the client calls with the fake method.",pull-request-available,[],HDDS,Improvement,Major,2020-03-09 10:59:23,6
13290498,Unit check fails to execute insight and mini-chaos-tests modules,"This was observed in unit check run for 0.5.0 RC.

{code:title=https://github.com/apache/hadoop-ozone/runs/490978126?check_suite_focus=true}
2020-03-06T19:13:08.6122969Z [ERROR] Failed to execute goal on project hadoop-ozone-insight: Could not resolve dependencies for project org.apache.hadoop:hadoop-ozone-insight:jar:0.5.0-beta: Could not find artifact org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
2020-03-06T19:13:08.6180318Z [ERROR] Failed to execute goal on project mini-chaos-tests: Could not resolve dependencies for project org.apache.hadoop:mini-chaos-tests:jar:0.5.0-beta: Failure to find org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in https://repository.apache.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced -> [Help 1]
{code}

Unit check skips {{integration-test}}, but these 2 modules depend on it.",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2020-03-09 10:10:22,1
13290495,Remove hard-coded SNAPSHOT version from GitHub workflows,Ozone's GitHub Actions workflows only work with SNAPSHOT versions due to hard-coded {{ozone-*-SNAPSHOT}} in target path.,pull-request-available,['build'],HDDS,Improvement,Major,2020-03-09 09:59:24,1
13290376,Pipeline placement should select lowest load datanode as anchor,"PipelinePlacementPolicy should select datanodes with lowest load as anchor. Current logic is to random choose one, which could cause imbalance. ",pull-request-available,['SCM'],HDDS,Improvement,Major,2020-03-08 14:07:56,77
13290375,Fix pipeline datanode limit on cluster with rack awareness,"Deployed on cluster with 8 datanodes and rack awareness enabled. (2/3/3). Pipeline limit on datanode is 5.

 

turned out there are pipeline which can have over 5.

 

Perhaps should not count out CLOSE pipeline when filter the limit.",TriagePending,['SCM'],HDDS,Bug,Major,2020-03-08 14:05:23,85
13290187,OM RpcClient fail with java.lang.IllegalArgumentException,"In OM HA cluster, when one of the om service is down, during creation of RpcClient it will fail with below error.

 

 
{code:java}
java.lang.IllegalArgumentException: java.net.UnknownHostException: om1
 at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:447)
 at org.apache.hadoop.ozone.om.ha.OMProxyInfo.<init>(OMProxyInfo.java:40)
 at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:115)
 at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:207)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:198)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:124)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:249)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:274)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:82)
 at picocli.CommandLine.execute(CommandLine.java:1173)
 at picocli.CommandLine.access$800(CommandLine.java:141)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
 at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
 at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
 at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
 at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
 at org.apache.hadoop.ozone.freon.Freon.execute(Freon.java:72)
 at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
 at org.apache.hadoop.ozone.freon.Freon.main(Freon.java:98)
Caused by: java.net.UnknownHostException: om1
 ... 22 more
 
{code}
 ",OMHATest pull-request-available,['OM HA'],HDDS,Sub-task,Blocker,2020-03-06 20:45:31,13
13290095,Enable test added in HDDS-3084 when blocking issues are resolved,"Once the blocking issues HDDS-3107 and HDDS-3116 are resolved the test added by HDDS-3084 show be enabled by renaming ""hadoop-ozone/dist/src/main/compose/ozone-topology/hdds-3084.sh"" to ""test.sh"".",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-03-06 14:35:29,11
13290055,Debug Tool that gives chunk location information given a key,"Debug Tool that helps to find information like chunk Path ,the data nodes and containers on which the key is stored. This tool can be handy to check whether data has been corrupted while storing  in chunks.

Command for getting chunk-info is:

*ozone debug chunkinfo <URI of KEY>.*

 ",newbie pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2020-03-06 12:30:30,26
13289684,Skip KeyTable check in OMKeyCommit when ratis is disabled in OM.,"With replay logic, we have additional keyTable check to detect whether it is replay or not.

In non-HA case, we don't need this check. So this Jira is to skip that check in case of non-HA when ratis is not enabled.

 

*Ran simple test to know the perf impact:*
 
2295 Keys/sec with Additional Key Table check
2824 Keys/sec with removing that check
 

 

 ",pull-request-available,[],HDDS,Bug,Major,2020-03-05 01:27:32,13
13289442,DBUpdateResponse message could be much larger than ipc.maximum.data.length,"HDDS-1391 introduce a new OM RPC to allow Recon server to get delta of OM metadata update. However, the delta itself could be large. This causes ERROR on OM like below. 

Should we consider sending update in chunks over hadoop RPC instead of all in one piece (1.5 GB in this case)?

4:34:56.403 PM	WARN	OzoneManagerProtocolServerSideTranslatorPB	
##Response for request DBUpdates is too big size 1584040343
4:34:57.022 PM	WARN	Server	
Error serializing call response for call Call#12 Retry#15 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.17.112.109:58674
com.google.protobuf.CodedOutputStream$OutOfSpaceException: CodedOutputStream was writing to a flat byte array and ran out of space.
	at com.google.protobuf.CodedOutputStream.refreshBuffer(CodedOutputStream.java:828)
	at com.google.protobuf.CodedOutputStream.writeRawBytes(CodedOutputStream.java:959)
	at com.google.protobuf.CodedOutputStream.writeRawBytes(CodedOutputStream.java:905)
	at com.google.protobuf.CodedOutputStream.writeBytesNoTag(CodedOutputStream.java:386)
	at com.google.protobuf.CodedOutputStream.writeBytes(CodedOutputStream.java:229)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$DBUpdatesResponse.writeTo(OzoneManagerProtocolProtos.java)
	at com.google.protobuf.CodedOutputStream.writeMessageNoTag(CodedOutputStream.java:380)
	at com.google.protobuf.CodedOutputStream.writeMessage(CodedOutputStream.java:222)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OMResponse.writeTo(OzoneManagerProtocolProtos.java:15959)
	at org.apache.hadoop.ipc.Server.setupResponseForProtobuf(Server.java:3216)
	at org.apache.hadoop.ipc.Server.setupResponse(Server.java:3165)
	at org.apache.hadoop.ipc.Server.setupResponse(Server.java:3141)
	at org.apache.hadoop.ipc.Server.access$200(Server.java:139)
	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1061)
	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:858)
	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:844)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1001)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)",TriagePending,['Ozone Recon'],HDDS,Bug,Major,2020-03-04 06:29:26,30
13289369,Time interval calculate error ,"Just as the image shows, the time interval in log ""Unable to communicate to SCM server at scm-0.scm:9861 for past "" is 0, 3000 seconds, but actually it is 0, 300 seconds.
 !screenshot-1.png! ",pull-request-available,[],HDDS,Bug,Major,2020-03-04 01:46:51,65
13289350,Create REST API to serve Pipeline information and integrate with UI in Recon.,We need a REST API to serve Pipeline information in Recon and integrate with existing Recon UI.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-03-03 22:59:40,37
13289083,Fix TestSCMPipelineBytesWrittenMetrics,"In the test, we have Thread.sleep and then check the metric value. It will be better to use GenerictestUtils.waitFor and check the value of the metric. In few of the runs we have seen this test failed.
{code:java}
Thread.sleep(100 * 1000L);
metrics =
 getMetrics(SCMPipelineMetrics.class.getSimpleName());
for (Pipeline pipeline : cluster.getStorageContainerManager()
 .getPipelineManager().getPipelines()) {
 Assert.assertEquals(bytesWritten, getLongCounter(
 SCMPipelineMetrics.getBytesWrittenMetricName(pipeline), metrics));
}{code}",pull-request-available,[],HDDS,Bug,Major,2020-03-03 01:33:18,13
13289072,Freon work with OM HA,Make Freon commands work with OM HA,pull-request-available,[],HDDS,New Feature,Major,2020-03-02 23:58:05,13
13289065,"When ratis is enabled in OM, double Buffer metrics not getting updated ","DoubleBuffer metrics are not getting updated when ratis is enabled in OM.

There is no issue when ratis is not enabled, double buffer metrics are updating fine.
{code:java}
{""name"": ""Hadoop:service=OzoneManager,name=OzoneManagerDoubleBufferMetrics"",""modelerType"": ""OzoneManagerDoubleBufferMetrics"",""tag.Hostname"": ""hw13865.hitronhub.home"",""TotalNumOfFlushOperations"": 0,""TotalNumOfFlushedTransactions"": 0,""MaxNumberOfTransactionsFlushedInOneIteration"": 0,""FlushTimeNumOps"": 0,""FlushTimeAvgTime"": 0,""AvgFlushTransactionsInOneIteration"": 0},{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-03-02 23:11:47,30
13289052,Possible deadlock in LockManager,"{{LockManager}} has a possible deadlock.

# Number of locks is limited by using a {{GenericObjectPool}}.  If N locks are already acquired, new requestors need to wait.  This wait in {{getLockForLocking}} happens in a callback executed from {{ConcurrentHashMap#compute}} while holding a lock on a map entry.
# While releasing a lock, {{decrementActiveLockCount}} implicitly requires a lock on an entry in {{ConcurrentHashMap}}.",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-03-02 21:52:59,13
13288884,NPE seen in datanode log as ApplyTransaction failed,"Errors seen in datanode log. 

 
{noformat}
1:04:32.860 PM ERROR ContainerStateMachine
gid group-00234F8B3578 : ApplyTransaction failed. cmd PutBlock logIndex 56 msg : ContainerID 16 does not exist Container Result: CONTAINER_NOT_FOUND
1:04:32.860 PM ERROR XceiverServerRatis
pipeline Action CLOSE on pipeline PipelineID=b9601efc-f8bf-4b72-8077-00234f8b3578.Reason : Ratis Transaction failure in datanode 2ba0ecb0-0739-4da9-9541-5fef23479f28 with role FOLLOWER .Triggering pipeline close action.
1:04:32.860 PM ERROR ContainerStateMachine
gid group-00234F8B3578 : ApplyTransaction failed. cmd WriteChunk logIndex 59 exception {}
java.lang.NullPointerException
 at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:226)
 at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:396)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:406)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction$6(ContainerStateMachine.java:745)
 at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
 at java.base/java.lang.Thread.run(Thread.java:834){noformat}
 
{noformat}
1:04:32.861 PM ERROR ContainerStateMachine
gid group-00234F8B3578 : ApplyTransaction failed. cmd PutBlock logIndex 60 msg : ContainerID 16 does not exist Container Result: CONTAINER_NOT_FOUND
1:04:32.861 PM ERROR XceiverServerRatis
pipeline Action CLOSE on pipeline PipelineID=b9601efc-f8bf-4b72-8077-00234f8b3578.Reason : Ratis Transaction failure in datanode 2ba0ecb0-0739-4da9-9541-5fef23479f28 with role FOLLOWER .Triggering pipeline close action.
1:04:32.904 PM ERROR StateContext
Critical error occurred in StateMachine, setting shutDownMachine
1:04:34.862 PM ERROR DatanodeStateMachine
DatanodeStateMachine Shutdown due to an critical error{noformat}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-03-02 13:56:35,86
13288858,Add new Freon test for putBlock,The goal of this task is to introduce a new Freon test that issues putBlock commands.,pull-request-available,['test'],HDDS,Improvement,Major,2020-03-02 12:07:30,1
13288601,Add unit test for container replication behavior under different container placement policy,"Currently, the unit test for ReplicationManager only tested for container state change and container placement policy only focus on the policy algorithm.
And we lack of one integration unit test for testing container replication behavior under different container placement policy. Including some corner cases, like not enough candidate node, fallback cases in rack awareness policy.",pull-request-available,[],HDDS,Test,Major,2020-03-02 03:00:48,68
13288383,Fix race condition in Recon's container and pipeline handling.,"Fix the following issues in Recon
* Both the Incremental container report handler and the regular container report handler add new containers from SCM whenever they see a new container. This test and add step must be synchronized between the 2 handlers to avoid any inconsistent metadata state.
* NodeStateMap in allow does not addition of a single container to the Map of Node -> Set of Containers since it instantiates with a Collections.emptySet(), and then relies on a map.put() to update the value. Changing this to a ""new HashSet"" allows addition of a container one by one which is possible in Recon.
* Improve logging in Recon Container Manager when it receives a container report from a node before receiving the pipeline report for a newly created pipeline.",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-02-28 19:42:12,30
13288382,Refactor 'Recon' in MiniOzoneCluster to use ephemeral port.,"Currently, Recon uses an ephemeral port only in the integration test for Recon. In all other integration tests, we end up using the default (9888) that causes failures in other integration tests that start up a Mini ozone cluster. In addition, we want to start up Recon in MiniOzoneCluster by explicitly requesting it rather than by default.",pull-request-available,[],HDDS,Bug,Major,2020-02-28 19:37:12,30
13288338,Remove unused ForkJoinPool in RatisPipelineProvider,"The RatisPipelineProvider has a ForkJoinPool that is never used anywhere except when it is shutdown.

I suspect it was used previously and then some refactoring has made it redundant and it got left behind.

This jira is to remove that unused code.",pull-request-available,['SCM'],HDDS,Bug,Major,2020-02-28 16:38:34,11
13288269,Intermittent timeout in TestOzoneManagerDoubleBufferWithOMResponse#testDoubleBuffer,"{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/474452740}
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 505.227 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
[ERROR] testDoubleBuffer(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 500.142 s  <<< ERROR!
java.lang.Exception: test timed out after 500000 milliseconds
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:394)
  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:130)
{code}

Also in: https://github.com/apache/hadoop-ozone/pull/590/checks?check_run_id=467388979

CC [~bharat]",pull-request-available,['test'],HDDS,Bug,Critical,2020-02-28 12:11:20,1
13288266,Integration test crashes due to critical error in datanode,"{code:title=test log}
2020-02-28 07:36:17,759 [Datanode State Machine Thread - 0] ERROR statemachine.StateContext (StateContext.java:execute(420)) - Critical error occurred in StateMachine, setting shutDownMachine
...
2020-02-28 07:36:21,216 [Datanode State Machine Thread - 0] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 1: ExitException
{code}

{code:title=build output}
[ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
{code}

https://github.com/adoroszlai/hadoop-ozone/runs/474218807
https://github.com/adoroszlai/hadoop-ozone/suites/487650271/artifacts/2327174",pull-request-available,[],HDDS,Bug,Major,2020-02-28 12:01:11,86
13288227,ozone getconf command should use the GenericCli parent class,"org.apache.hadoop.ozone.freon.OzoneGetCOnf implements a tool to print out current configuration values

With all the other CLI tools we already started to use picocli and the GenericCli parent class.

To provide better user experience we should migrate the tool to use GenericCli (+move it to the tools project + remove freon from the package name)",incompatible newbie pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2020-02-28 08:53:53,54
13288225,Depend on lightweight ConfigurationSource interface instead of Hadoop Configuration,"To make it possible to create different client jars compiled with different version of Hadoop we need clear and Hadoop independent hdds-common (and hdds-client) projects.

(For more details about the motivation, check this design doc: https://lists.apache.org/thread.html/rd0ea00f958368e888db1947eb71e514fb977df0b7baaad928ac50e94%40%3Cozone-dev.hadoop.apache.org%3E)

Our current blocker is the usage of `org.apache.hadoop.conf.Configuration`. Configuration class is a heavyweight object from hadoop-common which introduce a lot of unnecessary dependencies. It also violates multiple [OOP principles|https://en.wikipedia.org/wiki/SOLID], for example the *Dependency inversion principle*.

To make our components more independent I propose to depend on a lightweight ConfigurationSource interface which includes all the required getXXX methods. OzoneConfiguration can implement that interface (and with older Hadoop we can create direct adapters).

",pull-request-available,[],HDDS,Sub-task,Major,2020-02-28 08:49:02,6
13288209,TestDeleteWithSlowFollower is failing intermittently,"{code}
[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 641.212 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 617.198 s  <<< ERROR!
java.io.IOException: INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:228)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:401)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:346)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:457)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:508)
	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
	at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:224)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-02-28 07:11:51,16
13288122,SCM does not exit Safe mode,"In a few scenarios, like Disks are gone, the datanode is not up or any other case, we may try to close pipelines.

If we close pipelines after SCM restart, SCM will not come out of safe mode. This is because of the current implementation where we get the count of the pipeline from DB when creating a SafeMode rule object. During this, if any pipeline is closed/removed from DB, the Rule does not know about it, and it PipelineSafeMode rule is never met, this causes a situation where we never come out of safe mode.

 

 

 ",Triaged,['SCM'],HDDS,Bug,Critical,2020-02-27 18:39:20,13
13288114,Save each output of smoketest executed multiple times,"Acceptance tests may invoke the same smoketest multiple times to verify behaviour in different states.  Currently output is saved to a file named based on _environment_, _test_ and _container_, so each execution's output overwrites the previous one.  We should check if the file already exists and add a suffix if necessary to avoid overwriting previous logs.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-02-27 17:56:27,1
13288109,Allow forced overwrite of local file,"{{ozone sh key get}} refuses to overwrite existing local file.  I would like to add a {{--force}} flag (default: false) to allow overriding this behavior, to make it easier to repeatedly get a key without forcing me to delete it locally first.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Minor,2020-02-27 17:40:53,1
13288074,Duplicate large key test,"{{TestDataValidate}} has 2 large key tests:

* {{ratisTestLargeKey}}
* {{standaloneTestLargeKey}}

But both of these test RATIS/3 replication since HDDS-675.  I think {{standaloneTestLargeKey}} can be removed.",pull-request-available,['test'],HDDS,Bug,Minor,2020-02-27 14:42:41,1
13287978,Fix logging in OMFileRequest and OzoneManager,"HDDS-2940 introduced a INFO level log in 
hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileCreateRequest.java
This needs to be a TRACE because it occurs in the regular file create path.

Also, trace logs introduced in OzoneManager and OMFileRequest.java need to be parameterized.

",Triaged,['Ozone Manager'],HDDS,Sub-task,Trivial,2020-02-27 09:33:42,88
13287960,TestSCMNodeManager intermittent crash,"TestSCMNodeManager crashed in one of the runs, although it passes usually:

{code:title=https://github.com/apache/hadoop-ozone/pull/601/checks?check_run_id=471611827}
[ERROR] Crashed tests:
[ERROR] org.apache.hadoop.hdds.scm.node.TestSCMNodeManager
{code}

{code:title=hs_err_pid9082.log}
siginfo: si_signo: 11 (SIGSEGV), si_code: 2 (SEGV_ACCERR), si_addr: 0x00007f378cf6f340

Stack: [0x00007f37626fb000,0x00007f37627fc000],  sp=0x00007f37627f9e48,  free space=1019k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  0x00007f378cf6f340
C  [librocksdbjni3775377216204452319.so+0x2a05dd]  rocksdb::DB::Delete(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&)+0x4d
C  [librocksdbjni3775377216204452319.so+0x2a0641]  rocksdb::DBImpl::Delete(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&)+0x11
C  [librocksdbjni3775377216204452319.so+0x1a931a]  rocksdb::DB::Delete(rocksdb::WriteOptions const&, rocksdb::Slice const&)+0xba
C  [librocksdbjni3775377216204452319.so+0x19f3e0]  rocksdb_delete_helper(JNIEnv_*, rocksdb::DB*, rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int, int)+0x130
C  [librocksdbjni3775377216204452319.so+0x19f4a1]  Java_org_rocksdb_RocksDB_delete__J_3BII+0x41
j  org.rocksdb.RocksDB.delete(J[BII)V+0
j  org.rocksdb.RocksDB.delete([B)V+13
j  org.apache.hadoop.hdds.utils.RocksDBStore.delete([B)V+9
j  org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(Lorg/apache/hadoop/hdds/scm/pipeline/PipelineID;)V+35
j  org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(Lorg/apache/hadoop/hdds/scm/pipeline/Pipeline;)V+27
...
j  org.apache.hadoop.hdds.scm.node.DeadNodeHandler.destroyPipelines(Lorg/apache/hadoop/hdds/protocol/DatanodeDetails;)V+28
j  org.apache.hadoop.hdds.scm.node.DeadNodeHandler.onMessage(Lorg/apache/hadoop/hdds/protocol/DatanodeDetails;Lorg/apache/hadoop/hdds/server/events/EventPublisher;)V+6
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-27 08:04:23,1
13287915,Failure running integration test it-freon ,"Observed a time-out during pr-check/it-freon for HDDS-2940. Failure appears unrelated to the changes in the patch. 

[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 67.193 s - in org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations
2862
[INFO] Running org.apache.hadoop.ozone.freon.TestFreonWithDatanodeRestart
2863
[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 30.559 s - in org.apache.hadoop.ozone.freon.TestFreonWithDatanodeRestart
2864
[INFO] 
2865
[INFO] Results:
2866
[INFO] 
2867
[WARNING] Tests run: 16, Failures: 0, Errors: 0, Skipped: 3
2868
[INFO] 
2869
[INFO] ------------------------------------------------------------------------
2870
[INFO] BUILD FAILURE
2871
[INFO] ------------------------------------------------------------------------
2872
[INFO] Total time:  28:58 min
2873
[INFO] Finished at: 2020-02-26T17:55:42Z
2874
[INFO] ------------------------------------------------------------------------
2875
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork -> [Help 1]
2876
[ERROR] 
2877
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2878
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
2879
[ERROR] 
2880
[ERROR] For more information about the errors and possible solutions, please read the following articles:
2881
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
",pull-request-available,['freon'],HDDS,Bug,Major,2020-02-27 04:10:53,16
13287869,OM Delta updates request in Recon should work with secure Ozone Manager.,"* Introduced kerberos principal and keytab file configs for Recon
* Login Recon user with KDC while Recon starts up",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-26 23:06:59,37
13287732,Extend network topology acceptance test to read data when datanodes are stopped,"It would be good to create a smoke test which:

1. Writes some data on a network aware cluster
2. Stops 1 rack and ensures the data is still readable
3. Restart the rack and stop the other rack and again check the data is readable

That way we can have some confidence the data is being written to both racks OK.

One issue with a test like this on a small cluster, is that there is a high chance the data will end up on 2 racks naturally, even if no network topology is configured. If that was the case, we would expect intermittent test failures. 

However, if network topology is working fine, then we would not expect any failures.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-02-26 11:38:46,11
13287730,"Refactor recon missing containers task to detect under, over and mis-replicated containers","With network topology enabled, we need a tool to check that all containers on the cluster meet the replication policy and possibly correct them if they do not (HDDS-3081).

Do we need a tool like ""hdfs fsck"" for this, or is this something we should build into Recon?",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-02-26 11:34:15,11
13287729,Replication manager should detect and correct containers which don't meet the replication policy,"In the current implementation, replication manager does not consider the container placement when checking if a container is healthy. Only the number of replicas are checked.

Now we have network topology available, we should consider whether replication manager should detect and correct mis-replicated containers.

In HDFS, the namenode will not automatically correct mis-replicated containers automatically, except at startup when all blocks are checked.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-02-26 11:30:44,11
13287717,Include output of timed out test in bundle,"Sometimes a unit/integration test does not complete, nor does it crash.  We should collect the output of such tests in the result bundle for analysis.

Example:

{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/469172863}
2020-02-26T08:15:58.2297584Z [INFO] Running org.apache.hadoop.ozone.freon.TestRandomKeyGenerator
2020-02-26T08:30:59.6189916Z [INFO] Running org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations
...
2020-02-26T08:32:47.6155975Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork
{code}

In this case TestRandomKeyGenerator had this problem.  It might be a bit tricky to find such tests, since these are not explicitly listed at the end, unlike failed or crashed tests.",pull-request-available,"['build', 'test']",HDDS,Improvement,Major,2020-02-26 10:45:15,1
13287662,container replica is not replicated when datanode dir is deleted from one of the datanodes,"steps taken :
---------------
1. written a key.
2. Deleted datanode directory of one of the datanodes containing container replica.
3. Wait for more than an hour.

Replication manager did not replicate the missing container. Also, container info command still shows the old container replica info.",fault_injection,['SCM'],HDDS,Bug,Major,2020-02-26 05:15:56,35
13287656,Improve query result of container info in scmcli when container doesn't exist,"When *ozone scmcli container info* queried the *<containerID>* that doesn't exist, the query result would only show *#<containerID>*.

I propose that we should inform user the container doesn't exist.",pull-request-available,[],HDDS,Improvement,Major,2020-02-26 03:53:04,86
13287655,Make the configuration of container scrub consistent,"The prefix of configuration of container scrub in [ozone-site.xml|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/common/src/main/resources/ozone-default.xml] is *hdds.container.scrub*, but in [ContainerScrubberConfiguration|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerScrubberConfiguration.java] is *hdds.containerscrub*.

This situation would lead to not work under configuration.
 For example, when we set *hdds.container.scrub.enabled* true the cluster didn't work on container scrub, and if we set *hdds.containerscrub.enable* true, it did work.",pull-request-available,[],HDDS,Sub-task,Minor,2020-02-26 03:35:46,71
13287635,Implement ofs://: Fix listStatus continuation,Supplement to HDDS-2928,pull-request-available,[],HDDS,Sub-task,Major,2020-02-25 23:33:12,12
13287592,SCM scrub pipeline should be started after coming out of safe mode,"We should start scrubbing pipelines after SCM is out of safe mode.

Reasons to do this:
 # Right now, we do scrub pipeline as part of triggerPipelineCreation, now when we scrub pipelines in allocated state for more than ""ozone.scm.pipeline.allocated.timeout"", we might close some pipelines and with this, we might not be able to come out of safeMode. As in SafeModeRules, we get pipeline count from pipelineDB during initialization.

Example scenario:
 # Stop 3 Datanodes. 
 # Restart SCM.
 # Start Datanode after 6 mts. We shall never come out of safe mode, as pipeline in allocated state will meet scrubber time out condition.

To not to be in these kinds of scenarios, better thing to be done here is scrub pipelines after SCM out of the safe mode

 ",pull-request-available,[],HDDS,Bug,Blocker,2020-02-25 20:17:55,13
13287563,Datanodes unable to connect to recon in Secure Environment,"Datanodes throw this exception while connecting to recon.
{code:java}
datanode_1  | java.io.IOException: DestHost:destPort recon:9891 , LocalHost:localPort 6a99ad69685d/192.168.48.4:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  | java.io.IOException: DestHost:destPort recon:9891 , LocalHost:localPort 6a99ad69685d/192.168.48.4:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)datanode_1  |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)datanode_1  |  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)datanode_1  |  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)datanode_1  |  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)datanode_1  |  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)datanode_1  |  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)datanode_1  |  at org.apache.hadoop.ipc.Client.call(Client.java:1457)datanode_1  |  at org.apache.hadoop.ipc.Client.call(Client.java:1367)datanode_1  |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)datanode_1  |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)datanode_1  |  at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)datanode_1  |  at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)datanode_1  |  at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)datanode_1  |  at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)datanode_1  |  at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)datanode_1  |  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)datanode_1  |  at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)datanode_1  |  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)datanode_1  |  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)datanode_1  |  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)datanode_1  |  at java.base/java.lang.Thread.run(Thread.java:834)datanode_1  | Caused by: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:866)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)datanode_1  |  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)datanode_1  |  at org.apache.hadoop.ipc.Client.call(Client.java:1403)datanode_1  |  ... 14 moredatanode_1  | Caused by: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1  |  at java.security.jgss/sun.security.krb5.PrincipalName.validateNameStrings(PrincipalName.java:174)datanode_1  |  at java.security.jgss/sun.security.krb5.PrincipalName.<init>(PrincipalName.java:397)datanode_1  |  at java.security.jgss/sun.security.krb5.PrincipalName.<init>(PrincipalName.java:471)datanode_1  |  at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:172)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.getServerPrincipal(SaslRpcClient.java:305)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:234)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:160)datanode_1  |  at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)datanode_1  |  at java.base/java.security.AccessController.doPrivileged(Native Method)datanode_1  |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)datanode_1  |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)datanode_1  |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)datanode_1  |  ... 17 more
{code}
Recon throws an exception while connecting to SCM:
{code:java}
recon_1     | 2020-02-25 17:48:14,506 [main] ERROR scm.ReconStorageContainerManagerFacade: Exception encountered while getting pipelines from SCM.recon_1     | 2020-02-25 17:48:14,506 [main] ERROR scm.ReconStorageContainerManagerFacade: Exception encountered while getting pipelines from SCM.recon_1     | java.io.IOException: DestHost:destPort scm:9860 , LocalHost:localPort recon/192.168.48.8:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1     |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)recon_1     |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)recon_1     |  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)recon_1     |  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)recon_1     |  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)recon_1     |  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)recon_1     |  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)recon_1     |  at org.apache.hadoop.ipc.Client.call(Client.java:1457)recon_1     |  at org.apache.hadoop.ipc.Client.call(Client.java:1367)recon_1     |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)recon_1     |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)recon_1     |  at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)recon_1     |  at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)recon_1     |  at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.listPipelines(StorageContainerLocationProtocolClientSideTranslatorPB.java:322)recon_1     |  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)recon_1     |  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)recon_1     |  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)recon_1     |  at java.base/java.lang.reflect.Method.invoke(Method.java:566)recon_1     |  at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)recon_1     |  at com.sun.proxy.$Proxy42.listPipelines(Unknown Source)recon_1     |  at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipelines(StorageContainerServiceProviderImpl.java:49)recon_1     |  at org.apache.hadoop.ozone.recon.scm.ReconStorageContainerManagerFacade.initializePipelinesFromScm(ReconStorageContainerManagerFacade.java:223)recon_1     |  at org.apache.hadoop.ozone.recon.scm.ReconStorageContainerManagerFacade.start(ReconStorageContainerManagerFacade.java:183)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:118)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:95)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:39)recon_1     |  at picocli.CommandLine.execute(CommandLine.java:1173)recon_1     |  at picocli.CommandLine.access$800(CommandLine.java:141)recon_1     |  at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)recon_1     |  at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)recon_1     |  at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)recon_1     |  at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)recon_1     |  at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)recon_1     |  at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)recon_1     |  at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)recon_1     |  at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:52)recon_1     | Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1     |  at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:760)recon_1     |  at java.base/java.security.AccessController.doPrivileged(Native Method)recon_1     |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)recon_1     |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:723)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:817)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)recon_1     |  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)recon_1     |  at org.apache.hadoop.ipc.Client.call(Client.java:1403)recon_1     |  ... 28 morerecon_1     | Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1     |  at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)recon_1     |  at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)recon_1     |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)recon_1     |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)recon_1     |  at java.base/java.security.AccessController.doPrivileged(Native Method)recon_1     |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)recon_1     |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)recon_1     |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)recon_1     |  ... 31 more
{code}
 ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-25 17:52:46,30
13287496,NPE when stop recon server while recon server was not really started before,"I met a NPE error when I did test for Ozone. Seems the root cause is that recon server was not really started however we still try to stop it.

{noformat}
2020-02-25 20:22:44,296 [Thread-0] ERROR ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:build(525)) - Exception while shutting down the Recon.
java.lang.NullPointerException
	at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.stop(ReconTaskControllerImpl.java:237)
	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.stop(OzoneManagerServiceProviderImpl.java:229)
	at org.apache.hadoop.ozone.recon.ReconServer.stop(ReconServer.java:132)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.stopRecon(MiniOzoneClusterImpl.java:470)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.access$200(MiniOzoneClusterImpl.java:87)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:523)
	at org.apache.hadoop.fs.ozone.TestOzoneFileSystem.testFileSystem(TestOzoneFileSystem.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
{noformat}

",pull-request-available,['Ozone Recon'],HDDS,Bug,Minor,2020-02-25 13:29:49,68
13287468,UpdateID check should be skipped for non-HA OzoneManager,"Delete key is failing . Here is the stack trace of the failure:

 

 
{noformat}
INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 26 which is not greater than the current value of 433 for OMKeyInfo{volume='vol-test-restartcomponentozonereaddata-1582093704', bucket='buck-test-restartcomponentozonereaddata-1582093704', key='ReadOzoneFile_1582093709', dataSize='10485760', creationTime='1582093712218', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862. Trying to failover immediately.
 
..
..
..
..
 
20/02/19 03:37:17 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 22 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862 after 15 failover attempts. Trying to failover immediately. E 2020-02-19 03:37:17,895 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(287)) - Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E 20/02/19 03:37:17 ERROR ha.OMFailoverProxyProvider: Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E Trying to set updateID to 23 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'}]
{noformat}
 

 ",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2020-02-25 11:04:09,13
13287409,OM crash during startup does not print any error message to log,"During code read, found a similar thing, we don't log for OM start also. As OM startup also using similar code for the startup.

 ",OMHATest,['Ozone Manager'],HDDS,Improvement,Major,2020-02-25 04:07:54,13
13287376,Fix Bug in Scrub Pipeline causing destory pipelines after SCM restart,"Currently, the scrubber is run as part of create pipeline. 

When SCM is started, scrubber is coming up and cleaning up all the containers in SCM. Because when loading pipelines, the pipelineCreationTimeStamp is set from when the pipeline is created.

 

Because of this, below condition is satisfied and destroying all the pipelines when SCM is restarted. This can be easily reproduced start SCM, wait for 10 minutes and restart SCM.

 
{code:java}
List<Pipeline> needToSrubPipelines = stateManager.getPipelines(type, factor,
 Pipeline.PipelineState.ALLOCATED).stream()
 .filter(p -> currentTime.toEpochMilli() - p.getCreationTimestamp()
 .toEpochMilli() >= pipelineScrubTimeoutInMills)
 .collect(Collectors.toList());
for (Pipeline p : needToSrubPipelines) {
 LOG.info(""srubbing pipeline: id: "" + p.getId().toString() +
 "" since it stays at ALLOCATED stage for "" +
 Duration.between(currentTime, p.getCreationTimestamp()).toMinutes() +
 "" mins."");
 finalizeAndDestroyPipeline(p, false);
}{code}
 

*Log showing scrubbing of pipeline*

 
{code:java}
2020-02-20 12:42:18,946 [RatisPipelineUtilsThread] INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: srubbing pipeline: id: PipelineID=35dff62d-9bfa-449b-b6e8-6f00cc8c1b6e since it stays at ALLOCATED stage for -1003 mins.{code}
 

 

 ",OMHATest pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-02-24 23:50:46,13
13287285,SCM startup failed  during loading containers from DB," This is happening because pipeline scrubber came and removed pipeline, and it closed pipeline and removed from DB and triggered close containers to set them to CLOSING. When SCM is restarted before close container command is handled and change the state to CLOSING, the below issue can happen.

 

This can happen in other scenarios like when safeModeHandler calls finalizeAndDestroyPipeline and do SCM restart. 

 

The root cause for this is Pipeline removed from DB and the container is in open state in this scenario, and when trying to get pipeline we will crash SCM due to the {{PipelineNotFoundException error.}}

{{}}
{code:java}
 2020-02-21 13:57:34,888 [main] ERROR org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SCM start failed with exception org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=35dff62d-9bfa-449b-b6e8-6f00cc8c1b6e not found at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133) at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.addContainerToPipeline(PipelineStateMap.java:110) at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addContainerToPipeline(PipelineStateManager.java:59) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.addContainerToPipeline(SCMPipelineManager.java:309) at org.apache.hadoop.hdds.scm.container.SCMContainerManager.loadExistingContainers(SCMContainerManager.java:121) at org.apache.hadoop.hdds.scm.container.SCMContainerManager.<init>(SCMContainerManager.java:107) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:412) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:283) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:215) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:612) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:142) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:117) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:66) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:42) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:55) 2020-02-21 13:57:34,892 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down StorageContainerManager at om-ha-1.vpc.cloudera.com/10.65.51.49 ************************************************************/{code}
{{}}",OMHATest pull-request-available,['SCM'],HDDS,Bug,Blocker,2020-02-24 18:33:00,13
13287250,Ozone Filesystem should return real default replication,Ozone {{FileSystem}} implementation should return the actual configured replication factor for {{getDefaultReplication()}}.,pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-02-24 15:58:57,1
13287229,Get Key is hung when READ delay is injected in chunk file path,"Steps taken :

------------------
 # Mounted noise injection FUSE on all datanodes.
 # Write a key.
 # In one of the container replicas of the key, Inject READ delay of 5 seconds on chunk file directory path.
 # Run get Key operation.

Get Key operation is stuck and does not return any success/error .",fault_injection pull-request-available,[],HDDS,Bug,Major,2020-02-24 14:55:07,16
13287221,Add test to verify replication factor of ozone fs,Currently no test verifies that {{ozone fs}} creates keys for files with the expected replication factor.,pull-request-available,['test'],HDDS,Improvement,Minor,2020-02-24 14:14:13,1
13287204,Fix TestOzoneRpcClientAbstract.testListVolume,TestOzoneRpcClientAbstract.testListVolume is disabled due to intermittent issues. It should be fixed / rewritten or deleted.,pull-request-available,['test'],HDDS,Sub-task,Major,2020-02-24 12:27:59,35
13287203,Fix TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel,TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel is disabled due to intermittent issues. It should be fixed / rewritten or deleted.,TriagePending,['test'],HDDS,Sub-task,Major,2020-02-24 12:26:26,6
13287161,/retest github comment does not work,"Earlier we introduced an additional github workflow to process `/command` style commands from github comments. But in this case `/retest` didn't create the new commit to trigger a new build:

Pull request:

 https://github.com/apache/hadoop-ozone/pull/393

Run:

https://github.com/apache/hadoop-ozone/runs/463225330?check_suite_focus=true",pull-request-available,['CI'],HDDS,Bug,Major,2020-02-24 09:15:39,1
13287069,OzoneManager#listFileStatus should be auditted as READ operation instead of WRITE operation,"Currently, listFileStatus use AUDIT.logWriteSuccess and AUDIT.logWriteFailure to log AUDIT info. It should use AUDIT.logReadSuccess and AUDIT.logReadFailure instead.",pull-request-available,[],HDDS,Bug,Minor,2020-02-23 15:25:46,89
13287066,OzoneFileSystem should override unsupported set type FileSystem API,"Currently, OzoneFileSystem only implements some common useful FileSystem APIs and most of other API are not supported and inherited from parent class FileSystem by default. However, FileSystem do nothing in some set type method, like setReplication, setOwner.
{code:java}
 public void setVerifyChecksum(boolean verifyChecksum) {
    //doesn't do anything
  }

  public void setWriteChecksum(boolean writeChecksum) {
    //doesn't do anything
  }

  public boolean setReplication(Path src, short replication)
    throws IOException {
    return true;
  }

  public void setPermission(Path p, FsPermission permission
      ) throws IOException {
  }

  public void setOwner(Path p, String username, String groupname
      ) throws IOException {
  }

  public void setTimes(Path p, long mtime, long atime
      ) throws IOException {
  }
{code}
This set type functions depend on the sub-filesystem implementation. We need to to throw unsupported exception if sub-filesystem cannot support this. Otherwise, it will make users confused to use hadoop fs -setrep command or call setReplication api. Users will not see any exception but the command/API can execute fine. This is happened when I tested for the OzoneFileSystem via hadoop fs command way.",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2020-02-23 14:35:03,68
13286934,Improve Ozone Shell ACL operations' help text readability,"Currently:
{code:bash|title=ozone sh volume addacl -h}
$ ozone sh volume addacl -h
Usage: ozone sh volume addacl [-hV] -a=<acl> [-s=<storeType>] <uri>
Add a new Acl.
      <uri>                 URI of the volume/bucket.
                            Ozone URI could start with o3:// or without prefix. URI
                              may contain the host and port of the OM server. Both
                              are optional. If they are not specified it will be
                              identified from the config files.
  -a, --acl=<acl>           Add acl.r = READ,w = WRITE,c = CREATE,d = DELETE,l =
                              LIST,a = ALL,n = NONE,x = READ_AC,y = WRITE_ACEx user:
                              user1:rw or group:hadoop:rw
  -h, --help                Show this help message and exit.
  -s, --store=<storeType>   store type. i.e OZONE or S3
  -V, --version             Print version information and exit.
{code}
{code:bash|title=ozone sh bucket addacl -h}
$ ozone sh bucket addacl -h
Usage: ozone sh bucket addacl [-hV] -a=<acl> [-s=<storeType>] <uri>
Add a new Acl.
      <uri>                 URI of the volume/bucket.
                            Ozone URI could start with o3:// or without prefix. URI
                              may contain the host and port of the OM server. Both
                              are optional. If they are not specified it will be
                              identified from the config files.
  -a, --acl=<acl>           new acl.r = READ,w = WRITE,c = CREATE,d = DELETE,l =
                              LIST,a = ALL,n = NONE,x = READ_AC,y = WRITE_ACEx user:
                              user1:rw or group:hadoop:rw
  -h, --help                Show this help message and exit.
  -s, --store=<storeType>   store type. i.e OZONE or S3
  -V, --version             Print version information and exit.
{code}
Same for {{ozone sh (volume|bucket|key) (addacl|removeacl|setacl|-getacl-)}}

It would look much nicer to have a line separator or space between {{acl.}} and {{.r = READ,...}}.

And improves the prompt on error and overall readability, and correct typo: {{READ_AC -> READ_ACL}}, {{WRITE_AC -> WRITE_ACL}}.",pull-request-available,[],HDDS,Improvement,Major,2020-02-22 03:12:34,12
13286925,"Allow users to list volumes they have access to, and optionally allow all users to list all volumes","Current implementation of {{listVolumes}} only returns the volumes the user creates.
And there's no existing OM public API to return a list of users or return all volumes. Which means we must add new APIs to OM to either return user list or all volumes in order for this feature to work.

-We can open another jira on master branch to add those APIs, get back to this jira and add this function to OFS.-
-After a discussion with [~arpaga], Sanjay suggested we should allow *all* users to list *all* volumes for now. (Users still won't be able to access volumes which they don't have permission to.)-
Note: As HDDS-2610 is committed, it enabled clients to perform {{listAllVolumes}}, which allows admin users to list all volumes. This jira just need to make sure non-admin users can list all volumes as well.

In fact this jira tries to achieve two objectives:
1. Allow users to list volumes they have access to
2. Optionally allow all users to list all volumes",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2020-02-22 01:20:06,12
13286899,SCM crash during startup does not print any error message to log,"SCM start up failed due to a pipelineNotFoundException, there is no error message logged in to SCM log.

In the log file, we can see just below log message no reason for the crash is logged.

 

 
{code:java}
2020-02-20 15:37:56,079 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at xx.xx.xx/10.65.51.49
{code}
In the out file, we can see below, but not complete exception message.
{code:java}
PipelineID=xxxxx not found{code}
 

The actual reason for failure is not clearly logged if an exception has occurred during SCM startup.

 ",OMHATest pull-request-available,['SCM'],HDDS,Improvement,Major,2020-02-21 22:07:01,13
13286813,OzoneFileStatus#getModificationTime should return actual directory modification time when its OmKeyInfo is available,"As of current implementation, [{{getModificationTime()}}|https://github.com/apache/hadoop-ozone/blob/c9f26ccf9f93a052c5c0c042c57b6f87709597ae/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java#L90-L107] always returns ""fake"" modification time (current time) for directory due to the reason that a directory in Ozone might be faked from a file key.

But, there are cases where real directory key exists in OzoneBucket. For example when user calls {{fs.mkdirs(directory)}}. In this case, a reasonable thing to do would be getting the modification time from the OmInfoKey, rather than faking it.

CC [~xyao]


My POC for the fix:
{code:java|title=Diff}
diff --git a/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java b/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java
index 8717946512..708e62d692 100644
--- a/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java
+++ b/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java
@@ -93,7 +93,7 @@ public FileStatus makeQualified(URI defaultUri, Path parent,
    */
   @Override
   public long getModificationTime(){
-    if (isDirectory()) {
+    if (isDirectory() && super.getModificationTime() == 0) {
       return System.currentTimeMillis();
     } else {
       return super.getModificationTime();
diff --git a/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
index 1be5fb3f3c..cb8f647a41 100644
--- a/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
+++ b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
@@ -2004,8 +2004,14 @@ public OmKeyInfo lookupFile(OmKeyArgs args, String clientAddress)
               } else {
                 // if entry is a directory
                 if (!deletedKeySet.contains(entryInDb)) {
-                  cacheKeyMap.put(entryInDb,
-                      new OzoneFileStatus(immediateChild));
+                  if (!entryKeyName.equals(immediateChild)) {
+                    cacheKeyMap.put(entryInDb,
+                        new OzoneFileStatus(immediateChild));
+                  } else {
+                    // If entryKeyName matches dir name, we have the info
+                    cacheKeyMap.put(entryInDb,
+                        new OzoneFileStatus(value, 0, true));
+                  }
                   countEntries++;
                 }
                 // skip the other descendants of this child directory.
{code}",pull-request-available,[],HDDS,Improvement,Major,2020-02-21 15:04:07,12
13286779,Decrease the number of the chunk writer threads,"As of now we create 60 threads (
dfs.container.ratis.num.write.chunk.threads) to write chunk data to the disk. As the write is limited by the IO I can't see any benefit to have so many threads. High number of thread means a high context switch overhead, therefore it seems to be more reasonable to use only a limited number of threads.

For example 10 threads should be enough even with 5 external disk.

If you know any reason to keep the number 60, please let me know...
",pull-request-available,[],HDDS,Improvement,Major,2020-02-21 12:48:27,6
13286749,Use meaningful name for ChunkWriter threads,ChunkWriter threads acreated with a naming schema 'pool-[x]-thread-[y]'. We can use better naming (especially as we have 60 threads...),pull-request-available,[],HDDS,Improvement,Major,2020-02-21 11:58:37,6
13286682,Improve Ozone o3fs API function,"We tested and compared Ozone o3fs API functions with HDFS, found there are improvements areas.  Basically, 4 categories, 
1. existing suspicious bugs
2. FS API supported but actually no effect
3. FS API not supported，while the feature is already supported on object level
4. Feature totally not supported so far，such as append，truncate and others. 
See the attached file.

Use this Umbrella JIRA to track all the fixes, improvements and discusses on whether or how to support the feature. 
 ",Triaged,['Ozone Filesystem'],HDDS,Improvement,Major,2020-02-21 08:44:21,4
13286625,ObjectStore#listVolumesByUser and CreateVolumeHandler#call should get principal name by default,"[{{ObjectStore#listVolumesByUser}}|https://github.com/apache/hadoop-ozone/blob/2fa37ef99b8fb4575169ba8326eeb677b3d2ed74/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java#L249-L256] is using {{getShortUserName()}} by default (when user is empty or null):
{code:java|title=ObjectStore#listVolumesByUser}
  public Iterator<? extends OzoneVolume> listVolumesByUser(String user,
      String volumePrefix, String prevVolume)
      throws IOException {
    if(Strings.isNullOrEmpty(user)) {
      user = UserGroupInformation.getCurrentUser().getShortUserName();  // <--
    }
    return new VolumeIterator(user, volumePrefix, prevVolume);
  }
{code}

It should use {{getUserName()}} instead.

For a quick reference for the difference between {{getUserName()}} and {{getShortUserName()}}:
{code:java|title=UserGroupInformation#getUserName}
  /**
   * Get the user's full principal name.
   * @return the user's full principal name.
   */
  @InterfaceAudience.Public
  @InterfaceStability.Evolving
  public String getUserName() {
    return user.getName();
  }
{code}

{code:java|title=UserGroupInformation#getShortUserName}
  /**
   * Get the user's login name.
   * @return the user's name up to the first '/' or '@'.
   */
  public String getShortUserName() {
    return user.getShortName();
  }
{code}

This won't cause issue if Kerberos is not in use. However, once Kerberos is enabled, {{getUserName()}} and {{getShortUserName()}} result differs and can cause some issues.

When Kerberos is enabled, {{getUserName()}} returns full principal name e.g. {{om/om@EXAMPLE.COM}}, but {{getShortUserName()}} will return login name e.g. {{hadoop}}.

If {{hadoop.security.auth_to_local}} is set, {{getShortUserName()}} result can become very different from full principal name.
For example, when {{hadoop.security.auth_to_local = RULE:[2:$1@$0](.*)s/.*/root/}},
{{getShortUserName()}} returns {{root}}, while {{getUserName()}} still gives {{om/om@EXAMPLE.COM}}.)

This can lead to user experience issue (when Kerberos is enabled) where the user creates a volume with ozone shell ([uses {{getUserName()}}|https://github.com/apache/hadoop-ozone/blob/ecb5bf4df1d80723835a1500d595102f3f861708/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/CreateVolumeHandler.java#L63-L65] internally) then try to list it with {{ObjectStore#listVolumesByUser(null, ...)}} ([uses {{getShortUserName()}} by default|https://github.com/apache/hadoop-ozone/blob/2fa37ef99b8fb4575169ba8326eeb677b3d2ed74/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java#L238-L256] when user param is empty or null), the user won't see any volumes because of the mismatch.

We should also double check *all* usages that uses {{getShortUserName()}}.

*Update:*
Xiaoyu and I checked that the usage of {{getShortUserName()}} on the server side shouldn't become a problem. Because server should've maintained it's own auth_to_local rules (admin should make sure they separate each user into different short names. just don't map multiple principal names into the same then it won't be a problem).
The usage in {{BasicOzoneFileSystem}} itself also seems valid because that {{getShortUserName()}} is only used for client side purpose (to set {{workingDir}}, etc.).
But the usage in {{ObjectStore#listVolumesByUser}} is confirmed problematic at the moment, which needs to be fixed. Same for [{{CreateVolumeHandler#call}}|https://github.com/apache/hadoop-ozone/blob/ecb5bf4df1d80723835a1500d595102f3f861708/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/CreateVolumeHandler.java#L81-L83]:
{code:java|title=CreateVolumeHandler#call}
      } else {
        rootName = UserGroupInformation.getCurrentUser().getShortUserName();
      }
{code}
It should pass full principal name to server.

CC [~xyao] [~aengineer] [~arp] [~bharat]",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-02-21 03:35:49,12
13286563,Fix Retry handling in ozone RPC Client,"Right now for all other exceptions other than serviceException we use FailOverOnNetworkException.

This Exception policy is created with 15 max fail overs and 15 retries. 

 
{code:java}
retryPolicyOnNetworkException.shouldRetry(
 exception, retries, failovers, isIdempotentOrAtMostOnce);{code}
*2 issues with this:*
 # When shouldRetry returns action FAILOVER_AND_RETRY, it will stuck with same OM, and does not perform failover to next OM.  As OMFailoverProxyProvider#performFailover() is a dummy call does not perform any failover.
 # When ozone.client.failover.max.attempts is set to 15, now with 2 policies with each set to 15, we will retry 15*2 times in worst scenario. 

 

 ",OMHA OMHATest pull-request-available,[],HDDS,Bug,Major,2020-02-20 20:34:39,13
13286422,Support running full Ratis pipeline from IDE (IntelliJ) ,"HDDS-1522 introduced a method to run full cluster in IntelliJ. The runner configurations can be copied with a shell script and a basic ozone-site.xml and log configuration to make it easy to run ozone from IDE.

Unfortunately this setup supports only one Datanode and it's harder to debug full Ozone pipeline (3 datanodes) from IDE.

This patch provides 3 different configuration for 3 datanodes with different ports to make it possible to run them on the same host from the IDE.",pull-request-available,[],HDDS,Improvement,Major,2020-02-20 09:14:37,6
13286349,Update Ratis version to 0.5.0 released.,Update Ozone to use latest released version of Ratis 0.5.0.,pull-request-available,[],HDDS,Task,Major,2020-02-19 23:25:51,28
13286332,TestRatisPipelineLeader fails since we no longer wait for leader in the HealthyPipelineSafeModeExitRule,"{code:title=https://github.com/apache/hadoop-ozone/runs/456217344}
[ERROR] Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 29.823 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.TestRatisPipelineLeader
[ERROR] testLeaderIdAfterLeaderChange(org.apache.hadoop.hdds.scm.TestRatisPipelineLeader)  Time elapsed: 5.367 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestRatisPipelineLeader.verifyLeaderInfo(TestRatisPipelineLeader.java:125)
  at org.apache.hadoop.hdds.scm.TestRatisPipelineLeader.testLeaderIdAfterLeaderChange(TestRatisPipelineLeader.java:106)

[ERROR] testLeaderIdUsedOnFirstCall(org.apache.hadoop.hdds.scm.TestRatisPipelineLeader)  Time elapsed: 0.008 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.hdds.scm.TestRatisPipelineLeader.verifyLeaderInfo(TestRatisPipelineLeader.java:125)
  at org.apache.hadoop.hdds.scm.TestRatisPipelineLeader.testLeaderIdUsedOnFirstCall(TestRatisPipelineLeader.java:76)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-19 21:46:01,86
13286328,Hide JooQ welcome message on start,"Ozone recon start prints out this self-ad message:

{code}
2020-02-19 13:23:18,671 [main] INFO  jooq.Constants (JooqLogger.java:info(338)) - 
                                      
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@
@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@
@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@
@@@@@@@@@@        @@        @@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@        @@        @@@@@@@@@@
@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@
@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@
@@@@@@@@@@        @@  @  @  @@@@@@@@@@
@@@@@@@@@@        @@        @@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.11.9

{code}",pull-request-available,['Ozone Recon'],HDDS,Wish,Minor,2020-02-19 21:24:15,86
13286283,Add ability to enable Ratis metrics in OzoneManager,"Whenever OM uses Ratis, we may need the ability to collect its metrics through OM JMX. This should be a straightforward change, similar to org.apache.hadoop.ozone.HddsDatanodeService#start(). 

{code}
  public void start() {

    //All the Ratis metrics (registered from now) will be published via JMX and
    //via the prometheus exporter (used by the /prom servlet
    MetricRegistries.global()
        .addReporterRegistration(MetricsReporting.jmxReporter());
    MetricRegistries.global().addReporterRegistration(
        registry -> CollectorRegistry.defaultRegistry.register(
            new RatisDropwizardExports(
                registry.getDropWizardMetricRegistry())));

{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-02-19 17:16:40,86
13286231,Broken return code check in unit/integration,"HDDS-2915 fixed unit/integration check result in case of Maven error.  However, return code check was broken by output redirection via pipeline added in HDDS-2833 and HDDS-2960:

bq. The return status of a pipeline is the exit status of the last command, unless the pipefail option is enabled.",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2020-02-19 12:40:00,1
13286167,Cannot write 32MB chunks,"Writing 32MB chunks fails with various errors.

{code:title=steps to reproduce}
ozone freon dcg -t 1 -n 1 -s 33554432
{code}

1. With Ratis 0.5.0-90cd474-SNAPSHOT (used by current Ozone master):
{code}
org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: gRPC message exceeds maximum size 33554432: 33554686
  at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:524)
  at org.apache.ratis.thirdparty.io.grpc.internal.MessageDeframer.processHeader(MessageDeframer.java:387)
{code}

Which is strange, because [Datanode attempts to set max. message size|https://github.com/apache/hadoop-ozone/blob/4ba1932dab4692a9cc1bcfb8903ef650e32ec7ba/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/XceiverServerRatis.java#L204-L206] to 32MB + 16KB.

2. With Ratis built locally from current Ratis master (46f255cb):

{code}
Caused by: org.apache.ratis.protocol.StateMachineException: org.apache.ratis.server.raftlog.RaftLogIOException from Server ccb25fbf-9bd1-4094-a632-00f4168213bb@group-B1FA90A78F31: Log entry size 33554666 exceeds the max buffer limit of 33554432
	at org.apache.ratis.server.raftlog.RaftLog.appendImpl(RaftLog.java:178)
	at org.apache.ratis.server.raftlog.RaftLog.lambda$append$2(RaftLog.java:157)
	at org.apache.ratis.server.raftlog.RaftLogSequentialOps$Runner.runSequentially(RaftLogSequentialOps.java:68)
	at org.apache.ratis.server.raftlog.RaftLog.append(RaftLog.java:157)
	at org.apache.ratis.server.impl.ServerState.appendLog(ServerState.java:282)
	at org.apache.ratis.server.impl.RaftServerImpl.appendTransaction(RaftServerImpl.java:518)
	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:604)
{code}

With {{ozone.scm.chunk.size=32MB}} setting, {{ozone freon ockg -n 1 -t 1 -s 33554432}} also fails, but without apparent errors in the datanode log.",TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2020-02-19 07:25:38,13
13286129,OM HA- Client requests get LeaderNotReadyException after OM restart,"Scenario:

1.Set up OM HA cluster.

2. Perform some write operations.

3. Restart OM's.

4. Now try any write operation.

Below error will be thrown for 15 times, and finally, client request will fail.
{code:java}
 
2020-02-15 10:11:23,244 [qtp2025269734-19] INFO org.apache.hadoop.io.retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMLeaderNotReadyException): om1@group-D0D586AF6951 is in LEADER state but not ready yet.
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.processReply(OzoneManagerRatisServer.java:177)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.submitRequest(OzoneManagerRatisServer.java:136)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:162)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:118)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
        at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
        at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
, while invoking $Proxy81.submitRequest over nodeId=om1,nodeAddress=om-ha-1.vpc.cloudera.com:9862 after 1 failover attempts. Trying to failover immediately.
{code}
 ",OMHA OMHATest pull-request-available,[],HDDS,Bug,Major,2020-02-19 00:48:37,13
13286128,Key Rename should preserve the ObjectID,"On Key Renames, objectID should be preserved from the original Key. 
Currently it is being set to the new transactionLogIndex of the rename request.",OMHA pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2020-02-19 00:34:11,13
13286020,Use own version from InterfaceAudience/Stability version,"Current Ozone code uses the Hadoop version from @InterfaceAudience and @InterfaceStability annotations.

While Hadoop uses the annotations during the javadoc generation, in Ozone they are used only as markers as Ozone doesn't generate javadoc during the releases.

The two annotations are in the Hadoop common project. I propose to copy them and use the copied annotations instead of the original one. It would help us to reduce the dependencies on Hadoop (the hadoop-common which contains the original annotations has 87 transitive dependencies!!)",pull-request-available,[],HDDS,Improvement,Major,2020-02-18 15:40:18,6
13115723,Ozone: Ensure usage of parameterized slf4j log syntax for ozone,"various places use LOG.info(""text "" + something). they should all move to LOG.info(""text {}"", something)",newbie pull-request-available,[],HDDS,Improvement,Trivial,2017-11-02 19:39:12,89
13285913,OzoneManager#listStatus should be auditted as READ operation instead of WRITE operation,"Currently, listStatus use  AUDIT.logWriteSuccess and  AUDIT.logWriteFailure to log AUDIT info. It should use  AUDIT.logReadSuccess and  AUDIT.logReadFailure instead. ",pull-request-available,[],HDDS,Bug,Major,2020-02-18 07:21:38,89
13285828,README is missing from the source release tar,When we do a dist build with -Psrc the README.md of the root project is not packaged to the tar file which makes it impossible to do a build from the source package as the README.md is required by the dist script.,pull-request-available,[],HDDS,Bug,Blocker,2020-02-17 17:14:41,6
13285777,Create Freon test to test isolated Ratis LEADER,This is the pair/follow-up of HDDS-2974. It provides a new freon test which can set up a new Ratis ring. If the Datanode was instrumented earlier to mock leaders a real Ratis leader can be tested / measured without any overhead on the leader side...,pull-request-available,[],HDDS,Improvement,Major,2020-02-17 13:00:57,6
13285460,Datanode unable to close Pipeline after disk out of space,"Datanode gets into a loop and keeps throwing errors while trying to close pipeline


{code:java}
2020-02-14 00:25:10,208 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07: changes role from  FOLLOWER to CANDIDATE at term 6240 for changeToCandidate
2020-02-14 00:25:10,208 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=02e7e10e-2d50-4ace-a18b-701265ec9f07.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 is in candidate state for 31898494ms
2020-02-14 00:25:10,208 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start LeaderElection
2020-02-14 00:25:10,223 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032: begin an election at term 6241 for 0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,259 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032: Election REJECTED; received 0 response(s) [] and 2 exception(s); 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07:t6241, leader=null, voted=285cac09-7622-45e6-be02-b3c68ebf8b10, raftlog=285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-SegmentedRaftLog:OPENED:c4,f4,i14, conf=0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-701265EC9F07 not found.
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07: changes role from CANDIDATE to FOLLOWER at term 6241 for DISCOVERED_A_NEW_TERM
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown LeaderElection
2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start FollowerState
2020-02-14 00:25:10,680 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-DD847EC75388->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12669,entriesCount=0,lastEntry=null
2020-02-14 00:25:10,752 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=7ad5ce51-d3fa-4e71-99f2-dd847ec75388.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31623987ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31618878ms
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.FollowerState: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-FollowerState: change to CANDIDATE, lastRpcTime:5021ms, electionTimeout:5017ms
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown FollowerState
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9: changes role from  FOLLOWER to CANDIDATE at term 6220 for changeToCandidate
2020-02-14 00:25:10,894 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=179ac1d0-e5d5-4898-bef7-0068fd2ea2c9.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 is in candidate state for 31805092ms
2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start LeaderElection
2020-02-14 00:25:10,917 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033: begin an election at term 6221 for 0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033: Election REJECTED; received 0 response(s) [] and 2 exception(s); 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9:t6221, leader=null, voted=285cac09-7622-45e6-be02-b3c68ebf8b10, raftlog=285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-SegmentedRaftLog:OPENED:c0,f0,i8, conf=0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-0068FD2EA2C9 not found.
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9: changes role from CANDIDATE to FOLLOWER at term 6221 for DISCOVERED_A_NEW_TERM
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown LeaderElection
2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start FollowerState
2020-02-14 00:25:11,134 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-DD847EC75388->cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12669,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,218 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=7ad5ce51-d3fa-4e71-99f2-dd847ec75388.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31624453ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31619344ms
2020-02-14 00:25:11,347 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-2338B042C07B->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12579,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,361 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-2338B042C07B->cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12577,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,399 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=6a851c59-0345-4ad8-ac31-2338b042c07b.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31396085ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31391530ms
2020-02-14 00:25:11,406 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=6a851c59-0345-4ad8-ac31-2338b042c07b.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31396092ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31391537ms
2020-02-14 00:25:11,423 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-BA1E8724EE74->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12817,entriesCount=0,lastEntry=null
2020-02-14 00:25:11,490 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=1ed1be53-b526-41af-bdf9-ba1e8724ee74.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31946345ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31945978ms
2020-02-14 00:25:11,909 INFO org.apache.ratis.server.impl.FollowerState: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-D506E1A1894E-FollowerState: change to CANDIDATE, lastRpcTime:5094ms, electionTimeout:5093ms
2020-02-14 00:25:11,909 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown FollowerState
{code}",TriagePending,['Ozone Datanode'],HDDS,Bug,Critical,2020-02-15 00:42:24,16
13285409, Ozone S3 CLI path command not working on HA cluster,"ozone s3 path <<bucketname>>

ozone s3 path are not working on OM HA cluster

 

Because these commands do not take URI as a parameter. And for shell in HA, passing URI is mandatory. 

 

Below is the output when running on OM HA cluster:

 
{code:java}
$ozone s3 path
Service ID or host name must not be omitted when ozone.om.service.ids is defined.
{code}
 

HDDS-2279 fixed only getsecret, and this is missed because parameter added om-service-id is applicable only getsecret command.

 

This Jira suggests to do, move parameter to common class named S3Handler, and make all S3 commands inherit this, so in future any new commands, this parameter will be applicable. 

 

 

Thank You [~chinseone] for reporting this issue.",OMHA OMHATest pull-request-available,"['OM HA', 'Ozone CLI', 'Ozone Manager', 'S3']",HDDS,Sub-task,Major,2020-02-14 20:17:29,13
13285251,Fix TestOzoneClientRetriesOnException.java,The test failure is bcoz the exception msg in the KeyOutputStream is overriden with a hardcoded string in case all failures where the test expects the underlying exception msg to be propagated.,pull-request-available,['Ozone Client'],HDDS,Sub-task,Major,2020-02-14 04:31:23,16
13285250,Fix TestContainerStateMachine.java,The Jira aims to enable {color:#172b4d}TestContainerStateMachine tests.{color},pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-02-14 04:30:46,16
13285249,Fix TestContainerStateMachineFailures.java,"The unit tests are written withe single node ratis into consideration. The expectation is the datanode fails, client should see an exception after next io as there is no new dn for new pipeline to form which is not happening as the cluster is created with multiple dns.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2020-02-14 04:30:12,16
13285213,Refreshing Recon UI results in 404,"Refreshing any URL of Recon UI (ex: [http://localhost:9888/Datanodes|http://vsubramanian-cdh-4.vpc.cloudera.com:9888/Datanodes]) throws 404 error.


{code:java}
HTTP ERROR 404 Not Found
URI:/Datanodes
STATUS:404
MESSAGE:Not Found
SERVLET:org.eclipse.jetty.servlet.DefaultServlet-4d9d1b69
{code}

This 404 should only happen with URLs that match ""/api"" pattern and not with the ""/"". ",TriagePending,['Ozone Recon'],HDDS,Bug,Critical,2020-02-13 23:12:33,37
13285074,parse and dump ozonemanager ratis segment file to printable text,"With RATIS-755, a log dump utility for ratis logs has been added. however to parse SM data, a toString supplier is needed to dump the log to printable form. This can be in the form of JSON , XML.

cc:  [~hanishakoneru][~bharat]",pull-request-available,[],HDDS,New Feature,Major,2020-02-13 12:56:20,13
13285013,Fix CI test failure for TestSCMNodeManager,"TestSCMNodeManager would cause CI test to crash since it's creating too many datanodes every time.

 

From Attila, ""unit artifact now has output of this crashed test, it shows 11133 datanode registrations
[https://github.com/apache/hadoop-ozone/suites/449815171/artifacts/1739020|https://slack-redir.net/link?url=https%3A%2F%2Fgithub.com%2Fapache%2Fhadoop-ozone%2Fsuites%2F449815171%2Fartifacts%2F1739020]""

 

!https://files.slack.com/files-pri/T4S1WH2J3-FTJNURLN7/image.png|width=1052,height=460!

 ",pull-request-available,['test'],HDDS,Sub-task,Major,2020-02-13 08:03:09,77
13284749,OM HA stability issues,"To conclude a little, _+{color:#ff0000}major issues{color}+_ that I find:
 # When I do a long running s3g writing to cluster with OM HA and I stop the Om leader to force a re-election, the writing will stop and can never recover.

--updates 2020-02-20:

https://issues.apache.org/jira/browse/HDDS-3031 {color:#ff0000}fixes{color} this issue.

 

2. If I force a OM re-election and do a scm restart after that, the cluster cannot see any leader datanode and no datanodes are able to send pipeline reports, which makes the cluster unavailable as well. I consider this a multi-failover case when the leader OM and SCM are on the same node and there is a short outage happen to the node.

 

--updates 2020-02-20:

 When you do a jar swap for a new version of Ozone and enable OM HA while keeping the same ozone-site.xml as last time, if you've written some data into the last Ozone cluster (and therefore there are existing versions and metadata for om and scm), SCM cannot be up after the jar swap.

{color:#ff0000}Error logs{color}: PipelineID=aae4f728-82ef-4bbb-a0a5-7b3f2af030cc not found in scm out logs when scm process cannot be started.

 

--updates 2020-02-24:

After I add some logs to SCM starter:
Assuming SCM is only bounced after the leader OM is stopped
1. If SCM is bounced {color:#de350b}after{color} former leader OM is restarted, meaning all OMs are up, SCM will be bootstrapped correctly but there will be missing pipeline report from the node who doesn't have OM process on it (it's always him tho). This would cause all pipelines stay at ALLOCATED state and cluster will be in safemode. At this point, if I {color:#de350b}restart the blacksheep datanode{color}, it will come back and send the pipeline report to SCM and all pipelines will be at OPEN state.
2. If SCM is bounced {color:#de350b}before{color} the former leader OM is restarted, meaning not all OMs in ratis ring are up, SCM {color:#de350b}cannot{color} be bootstrapped correctly and it shows Pipeline not found.

 

Original posting:

Use S3 gateway to keep writing data into a specific s3 gateway endpoint. After the writer starts to work, I kill the OM process on the OM leader host. After that, the s3 gateway can never allow writing data and keeps reporting InternalError for all new coming keys.

Process Process-488:
{noformat}
 S3UploadFailedError: Failed to upload ./20191204/file1056.dat to ozone-test-reproduce-123/./20191204/file1056.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-489:
 S3UploadFailedError: Failed to upload ./20191204/file9631.dat to ozone-test-reproduce-123/./20191204/file9631.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-490:
 S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-491:
 S3UploadFailedError: Failed to upload ./20191204/file4220.dat to ozone-test-reproduce-123/./20191204/file4220.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-492:
 S3UploadFailedError: Failed to upload ./20191204/file5523.dat to ozone-test-reproduce-123/./20191204/file5523.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-493:
 S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
{noformat}

That's a partial list and note that all keys are different. I also tried re-enable the OM process on previous leader OM, but it doesn't help since the leader has changed. Also attach partial OM logs:
{noformat}
 2020-02-12 14:57:11,128 [IPC Server handler 72 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 72 on 9862, call Call#4859 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 2020-02-12 14:57:11,918 [IPC Server handler 159 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 159 on 9862, call Call#4864 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 2020-02-12 14:57:15,395 [IPC Server handler 23 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 23 on 9862, call Call#4869 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{noformat}
 

 

Also attach the ozone-site.xml config to enable OM HA:
{noformat}
<property>
 <name>ozone.om.service.ids</name>
 <value>OMHA</value>
 </property>
 <property>
 <name>ozone.om.nodes.OMHA</name>
 <value>om1,om2,om3</value>
 </property>
 <property>
 <name>ozone.om.node.id</name>
 <value>om1</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om1</name>
 <value>9.134.50.210:9862</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om2</name>
 <value>9.134.51.215:9862</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om3</name>
 <value>9.134.51.25:9862</value>
 </property>
 <property>
 <name>ozone.om.ratis.enable</name>
 <value>true</value>
 </property>
 <property>
 <name>ozone.enabled</name>
 <value>true</value>
 <tag>OZONE, REQUIRED</tag>
 <description>
 Status of the Ozone Object Storage service is enabled.
 Set to true to enable Ozone.
 Set to false to disable Ozone.
 Unless this value is set to true, Ozone services will not be started in
 the cluster.

Please note: By default ozone is disabled on a hadoop cluster.
 </description>
 </property>
{noformat}",OMHATest,['Ozone Manager'],HDDS,Bug,Blocker,2020-02-12 08:47:01,13
13284667,NFS support for Ozone,Provide NFS support for Ozone,pull-request-available,['Ozone Filesystem'],HDDS,New Feature,Major,2020-02-11 21:03:21,44
13284596,Move server-related shared utilities from common to framework,"hdds-common project is shared between all the client and server projects. hdds-server-framework project is shared between all the server side services.

To reduce unnecessary dependencies (to Hadoop, for example) we can move all the server-side related classes (eg. rocksdb layer, certificate tools) to the framework from the common.

We don't need the rocksdb utilities and certificate tools on the client side.",pull-request-available,[],HDDS,Improvement,Major,2020-02-11 14:22:18,6
13284530,Improve test coverage of audit logging,"Review audit logging tests, and add assertions about the different audit log contents we expect to have in the audit log.
A good place to start with is TestOMKeyRequest where we create an audit logger mock, via that one most likely the assertions can be done for all the requests.

This is a follow up on HDDS-2946.",Triaged newbie++,['test'],HDDS,Improvement,Major,2020-02-11 09:16:16,63
13284523,Support github comment based commands,"Before we started to use github actions we had the opportunity to use some ""commands"" in github commants. For example when a `/label xxx` comment has been added to a PR, a bot added the label (by default just the committers can use labels, but with this approach it was possible for everyone).

 

Since the move to use github actions I got multiple question about re-triggering the test. Even it it's possible to do with pushing an empty commit (only by the owner or committer) I think it would be better to restore the support of comment commands.

 

This patch follows a very simple approach. The available commands are store in a separated subdirectory as shell scripts and they are called by a lightweight wrapper.",pull-request-available,[],HDDS,Improvement,Major,2020-02-11 08:33:47,6
13284447,Create REST API to serve Node information and integrate with UI in Recon.,We need a REST API in Recon to serve up information for the Datanodes page (HDDS-2827). The REST API can also include other useful methods present in NodeManager that gives the user information about the Nodes in the cluster.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-10 23:44:42,37
13284430,Add integration test for Recon's Passive SCM state.,"* Verify Recon gets pipeline, node and container report from Datanode.
* Verify SCM metadata state == Recon metadata state (Create pipeline , Close pipeline, create container)

",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-02-10 23:01:01,30
13284415,"KeyManagerImpl#getFileStatus shouldn't always return null for permission, owner and group","The {{getFileStatus}} API always returns null for permisson, owner and group at the moment.

From the [code|https://github.com/apache/hadoop-ozone/blob/5950224c735c994d0acfaada87e3eef6c306299e/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java#L1689-L1692]:
{code}
      if (fileKeyInfo != null) {
        // this is a file
        return new OzoneFileStatus(fileKeyInfo, scmBlockSize, false);
      }
{code}

into the [constructor|https://github.com/apache/hadoop-ozone/blob/2e9265864af3b1d520dc7cdca3698d306f28cd14/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java#L41-L45]:
{code}
  public OzoneFileStatus(OmKeyInfo key, long blockSize, boolean isDirectory) {
    super(key.getDataSize(), isDirectory, key.getFactor().getNumber(),
        blockSize, key.getModificationTime(), getPath(key.getKeyName()));
    keyInfo = key;
  }
{code}

into [super|https://github.com/apache/hadoop/blob/branch-3.2.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileStatus.java#L109-L115] (hadoop-common 3.2.0 jar):
{code}
  //We should deprecate this soon?
  public FileStatus(long length, boolean isdir, int block_replication,
                    long blocksize, long modification_time, Path path) {

    this(length, isdir, block_replication, blocksize, modification_time,
         0, null, null, null, path);
  }
{code}

The constructor [params|https://github.com/apache/hadoop/blob/branch-3.2.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileStatus.java#L117-L127]:
{code}
  /**
   * Constructor for file systems on which symbolic links are not supported
   */
  public FileStatus(long length, boolean isdir,
                    int block_replication,
                    long blocksize, long modification_time, long access_time,
                    FsPermission permission, String owner, String group, 
                    Path path) {
    this(length, isdir, block_replication, blocksize, modification_time,
         access_time, permission, owner, group, null, path);
  }
{code}

You can see that the constructor used for Ozone's getFileStatus is always filling null for permission/owner/group.

We might want to fix this.

CC [~xyao] [~aengineer]",Triaged,"['Ozone Manager', 'Security']",HDDS,Bug,Major,2020-02-10 21:44:08,12
13284345,Handle existing volume/bucket in contract tests,"Contract tests use a random volume/bucket for each test case.  Volume/bucket creation fails if the same random number happens to be generated for another test case in the same class.

{code:title=https://github.com/apache/hadoop-ozone/runs/432759893}
2020-02-07T21:45:36.7489239Z [ERROR] Tests run: 18, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 43.647 s <<< FAILURE! - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus
2020-02-07T21:45:36.7492173Z [ERROR] testListStatusNoDir(org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus)  Time elapsed: 0.053 s  <<< ERROR!
2020-02-07T21:45:36.7493036Z VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
...
2020-02-07T21:45:36.7776813Z 	at org.apache.hadoop.ozone.TestDataUtil.createVolumeAndBucket(TestDataUtil.java:60)
2020-02-07T21:45:36.7779898Z 	at org.apache.hadoop.ozone.TestDataUtil.createVolumeAndBucket(TestDataUtil.java:93)
2020-02-07T21:45:36.7780272Z 	at org.apache.hadoop.fs.ozone.contract.OzoneContract.getTestFileSystem(OzoneContract.java:83)
2020-02-07T21:45:36.7784467Z 	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:181)
2020-02-07T21:45:36.7784916Z 	at org.apache.hadoop.fs.contract.AbstractContractGetFileStatusTest.setup(AbstractContractGetFileStatusTest.java:56)
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-02-10 15:11:34,1
13283951,Set the default value of grpc flow control window for ratis client and ratis server,"The grpc flow control window by default is set to 1Mb by default. During performance tests, it was observed that the flow control window has to be greater than the chunk size for optimum performance.",pull-request-available teragentest,"['Ozone Client', 'Ozone Datanode']",HDDS,Improvement,Major,2020-02-07 14:27:35,13
13283858,Intermittent timeout in TestBlockManager,"{code:title=https://github.com/apache/hadoop-ozone/runs/430663688}
2020-02-06T21:44:53.5319531Z [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.344 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.block.TestBlockManager
2020-02-06T21:44:53.5319796Z [ERROR] testMultipleBlockAllocation(org.apache.hadoop.hdds.scm.block.TestBlockManager)  Time elapsed: 1.167 s  <<< ERROR!
2020-02-06T21:44:53.5319942Z java.util.concurrent.TimeoutException: 
2020-02-06T21:44:53.5320496Z Timed out waiting for condition. Thread diagnostics:
2020-02-06T21:44:53.5320839Z Timestamp: 2020-02-06 09:44:52,261
2020-02-06T21:44:53.5320901Z 
2020-02-06T21:44:53.5321178Z ""Thread-26""  prio=5 tid=46 runnable
2020-02-06T21:44:53.5321292Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5321391Z         at java.lang.Thread.dumpThreads(Native Method)
2020-02-06T21:44:53.5326891Z         at java.lang.Thread.getAllStackTraces(Thread.java:1610)
2020-02-06T21:44:53.5327144Z         at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87)
2020-02-06T21:44:53.5327309Z         at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73)
2020-02-06T21:44:53.5327465Z         at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)
2020-02-06T21:44:53.5327618Z         at org.apache.hadoop.hdds.scm.block.TestBlockManager.testMultipleBlockAllocation(TestBlockManager.java:280)
2020-02-06T21:44:53.5388042Z         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-02-06T21:44:53.5388702Z         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-02-06T21:44:53.5388905Z         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-02-06T21:44:53.5389045Z         at java.lang.reflect.Method.invoke(Method.java:498)
2020-02-06T21:44:53.5389195Z         at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
2020-02-06T21:44:53.5389331Z         at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-02-06T21:44:53.5389662Z         at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
2020-02-06T21:44:53.5389776Z         at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-02-06T21:44:53.5389916Z         at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-02-06T21:44:53.5390040Z ""Signal Dispatcher"" daemon prio=9 tid=4 runnable
2020-02-06T21:44:53.5390156Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5390783Z ""EventQueue-CloseContainerForCloseContainerEventHandler""  prio=5 tid=32 in Object.wait()
2020-02-06T21:44:53.5390916Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5391019Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5391149Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5391299Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5391448Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5391587Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5391721Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5391844Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5391971Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5392100Z ""IPC Server idle connection scanner for port 43801"" daemon prio=5 tid=24 in Object.wait()
2020-02-06T21:44:53.5392227Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5392347Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5392463Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5392567Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5392694Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5393004Z ""Thread-28"" daemon prio=5 tid=48 timed_waiting
2020-02-06T21:44:53.5393121Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5393232Z         at java.lang.Thread.sleep(Native Method)
2020-02-06T21:44:53.5393352Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler.lambda$onMessage$0(SafeModeHandler.java:113)
2020-02-06T21:44:53.5393504Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler$$Lambda$38/725596393.run(Unknown Source)
2020-02-06T21:44:53.5393634Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5393927Z ""pool-9-thread-1""  prio=5 tid=45 timed_waiting
2020-02-06T21:44:53.5394061Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5394260Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5406780Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-02-06T21:44:53.5427435Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-02-06T21:44:53.5428120Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-02-06T21:44:53.5428601Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-02-06T21:44:53.5428758Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5428918Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5429052Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5429184Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5429297Z ""main""  prio=5 tid=1 timed_waiting
2020-02-06T21:44:53.5429405Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5429501Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5429775Z         at java.lang.Thread.join(Thread.java:1260)
2020-02-06T21:44:53.5429900Z         at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:26)
2020-02-06T21:44:53.5430034Z         at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:17)
2020-02-06T21:44:53.5430162Z         at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-02-06T21:44:53.5468996Z         at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-02-06T21:44:53.5469178Z         at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
2020-02-06T21:44:53.5469334Z         at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-02-06T21:44:53.5469466Z         at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-02-06T21:44:53.5469587Z         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
2020-02-06T21:44:53.5469708Z         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
2020-02-06T21:44:53.5469841Z         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
2020-02-06T21:44:53.5469967Z         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
2020-02-06T21:44:53.5470091Z         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
2020-02-06T21:44:53.5470215Z         at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
2020-02-06T21:44:53.5470339Z         at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
2020-02-06T21:44:53.5470449Z         at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
2020-02-06T21:44:53.5470577Z         at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
2020-02-06T21:44:53.5470706Z         at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-02-06T21:44:53.5470851Z         at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-02-06T21:44:53.5470989Z         at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-02-06T21:44:53.5471123Z         at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-02-06T21:44:53.5471252Z         at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-02-06T21:44:53.5471391Z         at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-02-06T21:44:53.5471525Z         at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-02-06T21:44:53.5471791Z         at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-02-06T21:44:53.5471932Z ""IPC Server idle connection scanner for port 37265"" daemon prio=5 tid=39 in Object.wait()
2020-02-06T21:44:53.5472061Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5472164Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5472277Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5472395Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5509093Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5509324Z ""Socket Reader #1 for port 37265""  prio=5 tid=38 runnable
2020-02-06T21:44:53.5509481Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5509580Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-02-06T21:44:53.5509772Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-02-06T21:44:53.5509940Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-02-06T21:44:53.5510121Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-02-06T21:44:53.5510287Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-02-06T21:44:53.5510624Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-02-06T21:44:53.5510740Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)
2020-02-06T21:44:53.5510912Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)
2020-02-06T21:44:53.5511550Z ""EventQueue-DatanodeCommandForDatanodeCommandHandler""  prio=5 tid=31 in Object.wait()
2020-02-06T21:44:53.5511733Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5511887Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5511993Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5512181Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5512357Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5512530Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5512725Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5512895Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5513065Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5513394Z ""ForkJoinPool.commonPool-worker-1"" daemon prio=5 tid=30 timed_waiting
2020-02-06T21:44:53.5513552Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5513695Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5513849Z         at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
2020-02-06T21:44:53.5753126Z         at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
2020-02-06T21:44:53.5806455Z         at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
2020-02-06T21:44:53.5807208Z ""EventQueue-DatanodeCommandForDatanodeCommandHandler""  prio=5 tid=49 in Object.wait()
2020-02-06T21:44:53.5807347Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5807455Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5807562Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5807703Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5807837Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5807968Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5808098Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5808388Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5808510Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5808851Z ""EventQueue-SafemodestatusForSafeModeHandler""  prio=5 tid=47 in Object.wait()
2020-02-06T21:44:53.5808977Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5809088Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5809206Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5809346Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5809471Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5809599Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5809731Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5809862Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5809982Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5810366Z ""Thread-14"" daemon prio=5 tid=29 timed_waiting
2020-02-06T21:44:53.5810463Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5810573Z         at java.lang.Thread.sleep(Native Method)
2020-02-06T21:44:53.5810706Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler.lambda$onMessage$0(SafeModeHandler.java:113)
2020-02-06T21:44:53.5810853Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler$$Lambda$38/725596393.run(Unknown Source)
2020-02-06T21:44:53.5810983Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5811095Z ""Finalizer"" daemon prio=8 tid=3 in Object.wait()
2020-02-06T21:44:53.5811193Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5811312Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5811435Z         at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-02-06T21:44:53.5811563Z         at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-02-06T21:44:53.5811691Z         at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-02-06T21:44:53.5811875Z ""IPC Server idle connection scanner for port 39545"" daemon prio=5 tid=16 in Object.wait()
2020-02-06T21:44:53.5811991Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5812106Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5812217Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5812333Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5812449Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5812553Z ""Reference Handler"" daemon prio=10 tid=2 in Object.wait()
2020-02-06T21:44:53.5812666Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5812784Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5812896Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5813013Z         at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-02-06T21:44:53.5813143Z         at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-02-06T21:44:53.5813253Z ""Socket Reader #1 for port 45489""  prio=5 tid=34 runnable
2020-02-06T21:44:53.5813364Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5813478Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-02-06T21:44:53.5813599Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-02-06T21:44:53.5813725Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-02-06T21:44:53.5813836Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-02-06T21:44:53.5813963Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-02-06T21:44:53.5814141Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-02-06T21:44:53.5814273Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)
2020-02-06T21:44:53.5814402Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)
2020-02-06T21:44:53.5814533Z ""Socket Reader #1 for port 39771""  prio=5 tid=42 runnable
2020-02-06T21:44:53.5814630Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5814744Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-02-06T21:44:53.5814866Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-02-06T21:44:53.5814990Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-02-06T21:44:53.5815117Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-02-06T21:44:53.5815222Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-02-06T21:44:53.5815342Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-02-06T21:44:53.5815472Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)
2020-02-06T21:44:53.5815601Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)
2020-02-06T21:44:53.5816007Z ""EventQueue-SafemodestatusForSafeModeHandler""  prio=5 tid=28 in Object.wait()
2020-02-06T21:44:53.5816130Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5816230Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5816350Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-02-06T21:44:53.5816493Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-02-06T21:44:53.5816633Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-02-06T21:44:53.5816769Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5816906Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5817026Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5817155Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5817278Z ""IPC Server idle connection scanner for port 39771"" daemon prio=5 tid=43 in Object.wait()
2020-02-06T21:44:53.5817404Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5817520Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5817628Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5817728Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5817844Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5817967Z ""IPC Server idle connection scanner for port 45489"" daemon prio=5 tid=35 in Object.wait()
2020-02-06T21:44:53.5818089Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5818205Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5818301Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5818413Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5818533Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5818647Z ""process reaper"" daemon prio=10 tid=11 timed_waiting
2020-02-06T21:44:53.5818757Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5818864Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5818970Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-02-06T21:44:53.5819108Z         at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-02-06T21:44:53.5819245Z         at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-02-06T21:44:53.5819376Z         at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-02-06T21:44:53.5819581Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-02-06T21:44:53.5819716Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5819837Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5819951Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5820283Z ""surefire-forkedjvm-command-thread"" daemon prio=5 tid=9 runnable
2020-02-06T21:44:53.5820398Z java.lang.Thread.State: RUNNABLE
2020-02-06T21:44:53.5820509Z         at java.io.FileInputStream.readBytes(Native Method)
2020-02-06T21:44:53.5820617Z         at java.io.FileInputStream.read(FileInputStream.java:255)
2020-02-06T21:44:53.5820740Z         at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-02-06T21:44:53.5820867Z         at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-02-06T21:44:53.5820996Z         at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-02-06T21:44:53.5821131Z         at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-02-06T21:44:53.5821272Z         at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:390)
2020-02-06T21:44:53.5821432Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5821557Z ""IPC Server idle connection scanner for port 40327"" daemon prio=5 tid=20 in Object.wait()
2020-02-06T21:44:53.5821681Z java.lang.Thread.State: WAITING (on object monitor)
2020-02-06T21:44:53.5821796Z         at java.lang.Object.wait(Native Method)
2020-02-06T21:44:53.5821906Z         at java.lang.Object.wait(Object.java:502)
2020-02-06T21:44:53.5822020Z         at java.util.TimerThread.mainLoop(Timer.java:526)
2020-02-06T21:44:53.5822121Z         at java.util.TimerThread.run(Timer.java:505)
2020-02-06T21:44:53.5822442Z ""surefire-forkedjvm-ping-30s"" daemon prio=5 tid=10 timed_waiting
2020-02-06T21:44:53.5822562Z java.lang.Thread.State: TIMED_WAITING
2020-02-06T21:44:53.5822668Z         at sun.misc.Unsafe.park(Native Method)
2020-02-06T21:44:53.5822785Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-02-06T21:44:53.5822918Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-02-06T21:44:53.5823066Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-02-06T21:44:53.5823218Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-02-06T21:44:53.5823353Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-02-06T21:44:53.5823483Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-02-06T21:44:53.5823618Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-02-06T21:44:53.5823739Z         at java.lang.Thread.run(Thread.java:748)
2020-02-06T21:44:53.5823800Z 
2020-02-06T21:44:53.5823847Z 
2020-02-06T21:44:53.5823955Z 	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)
2020-02-06T21:44:53.5824106Z 	at org.apache.hadoop.hdds.scm.block.TestBlockManager.testMultipleBlockAllocation(TestBlockManager.java:280)
2020-02-06T21:44:53.5824242Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-02-06T21:44:53.5824367Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-02-06T21:44:53.5824639Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-02-06T21:44:53.5824876Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-02-06T21:44:53.5825000Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
2020-02-06T21:44:53.5825204Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-02-06T21:44:53.5825339Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
2020-02-06T21:44:53.5825465Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-02-06T21:44:53.5825599Z 	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-07 07:51:31,1
13283797,Use getPropertiesByPrefix instead of regex in matching ratis client and server properties,"This Jira is to use getPropertiesByPrefix that match with ""datanode.ratis"" and set them during ratisClient and ratisServer creation.

 

Right now in the current code we use getValByRegex to match with regex, we can avoid that.

*2 Changes will go in this Jira.*

1.  Use getPropertiesByPrefix and set RatisClient and Server Conf.

2. Use the same prefix for RatisClient and Server Conf. (Right now we use prefix only for matching Ratis Server Conf)

 

cc [~aengineer]",pull-request-available teragentest,[],HDDS,Bug,Major,2020-02-06 22:43:02,13
13283787,Add metrics to OM DoubleBuffer,"Add a metric that will help in understanding the Average flush time which will help in understanding how the flush time increases over time.

Add another metric to show the average number of flush transactions in an iteration. This will show how many transactions are flushed in a single iteration over time.",pull-request-available,[],HDDS,Bug,Major,2020-02-06 21:51:33,13
13283743,MiniOzoneChaosCluster exits because of datanode shutdown,"MiniOzoneChaosCluster exits because of datanode shutdown

{code}
2020-02-06 18:50:50,760 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(219)) - DatanodeStateMachine Shutdown due to an critical error
2020-02-06 18:50:50,772 [RatisApplyTransactionExecutor 0] INFO  interfaces.Container (KeyValueContainer.java:flushAndSyncDB(400)) - Container 30 is synced with bcsId 7490.
2020-02-06 18:50:50,774 [RatisApplyTransactionExecutor 0] INFO  interfaces.Container (KeyValueContainer.java:close(338)) - Container 30 is closed with bcsId 7490.
2020-02-06 18:50:50,774 [Datanode State Machine Thread - 0] ERROR report.ReportManager (ReportManager.java:shutdown(82)) - Failed to shutdown Report Manager
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
        at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1475)
        at org.apache.hadoop.ozone.container.common.report.ReportManager.shutdown(ReportManager.java:80)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.stopDaemon(DatanodeStateMachine.java:411)
        at org.apache.hadoop.ozone.HddsDatanodeService.stop(HddsDatanodeService.java:474)
        at org.apache.hadoop.ozone.HddsDatanodeService.terminateDatanode(HddsDatanodeService.java:454)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:220)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:365)
        at java.lang.Thread.run(Thread.java:748)
2020-02-06 18:50:50,774 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:close(272)) - Error attempting to shutdown.
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2067)
        at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1475)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.close(DatanodeStateMachine.java:264)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.stopDaemon(DatanodeStateMachine.java:412)
        at org.apache.hadoop.ozone.HddsDatanodeService.stop(HddsDatanodeService.java:474)
        at org.apache.hadoop.ozone.HddsDatanodeService.terminateDatanode(HddsDatanodeService.java:454)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:220)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:365)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster,['Ozone Datanode'],HDDS,Sub-task,Major,2020-02-06 16:38:42,86
13283590,Allocate Block failing with NPE,"When running teragen in one of the run observed this error.
{code:java}
05 14:43:16,635 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: java.lang.NullPointerException at java.util.Objects.requireNonNull(Objects.java:203) at java.util.Optional.<init>(Optional.java:96) at java.util.Optional.of(Optional.java:108) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineMetrics.incNumBlocksAllocated(SCMPipelineMetrics.java:118) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.incNumBlocksAllocatedMetric(SCMPipelineManager.java:520) at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.newBlock(BlockManagerImpl.java:265) at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:233) at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:188) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:159) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:117) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:98) at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:792) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createFile(OzoneManagerProtocolClientSideTranslatorPB.java:1596) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71) at com.sun.proxy.$Proxy18.createFile(Unknown Source) at org.apache.hadoop.ozone.client.rpc.RpcClient.createFile(RpcClient.java:1071) at org.apache.hadoop.ozone.client.OzoneBucket.createFile(OzoneBucket.java:538) at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.createFile(BasicOzoneClientAdapterImpl.java:208) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.createOutputStream(BasicOzoneFileSystem.java:256) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.create(BasicOzoneFileSystem.java:237) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1133) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1113) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1002) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:990) at org.apache.hadoop.examples.terasort.TeraOutputFormat.getRecordWriter(TeraOutputFormat.java:141) at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:659) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:779) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168){code}
 ",pull-request-available teragentest,[],HDDS,Bug,Major,2020-02-05 22:48:27,1
13283476,Acceptance test failures due to lack of disk space,"Recently acceptance tests have been failing with {{DISK_OUT_OF_SPACE}} error and/or timeouts.

{code}
Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE
...
DiskOutOfSpaceException: Out of space: The volume with the most available space (=5069360506 B) is less than the container size (=5368709120 B).
{code}",pull-request-available,['test'],HDDS,Bug,Critical,2020-02-05 13:28:32,1
13283377,Recon server start failed with CNF exception in a cluster with Auto TLS enabled.,"{code}
Caused by: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ssl/SslContextFactory$Server
        at org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1590)
        at org.apache.hadoop.hdds.server.BaseHttpServer.<init>(BaseHttpServer.java:83)
        at org.apache.hadoop.ozone.recon.ReconHttpServer.<init>(ReconHttpServer.java:35)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at com.google.inject.internal.DefaultConstructionProxyFactory$2.newInstance(DefaultConstructionProxyFactory.java:86)
        at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:105)
        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:267)
        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)
        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1103)
        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
        at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:145)
        at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)
        at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016)
        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)
        at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012)
        ... 13 more
Caused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.ssl.SslContextFactory$Server
        at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:583)
        at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
        ... 32 more
{code}
",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-02-05 06:02:23,30
13283342,Add unit tests for Proto [de]serialization,"Every proto must have tests for checking serialization and deserialization. Some of the protos are missing these tests. For example - OzoneManagerProto#PrefixInfo.
There might be more protos which are missing these tests.",TriagePending newbie pull-request-available,['test'],HDDS,Test,Major,2020-02-05 00:13:40,90
13283341,Delete replayed entry from OpenKeyTable during commit,"During KeyCreate (and S3InitiateMultipartUpload), we do not check the OpenKeyTable if the key already exists. If it does exist and the transaction is replayed, we just override the key in OpenKeyTable. This is done to avoid extra DB reads.

During KeyCommit (or S3MultipartUploadCommit), if the key was already committed, then we do not replay the transaction. This would result in the OpenKeyTable entry to remain in the DB till it is garbage collected. 

To avoid storing stale entries in OpenKeyTable, during commit replays, we should check the openKeyTable and delete the entry if it exists.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Blocker,2020-02-05 00:07:17,43
13283316,Implement ofs://: Fix getFileStatus for mkdir volume,"[~xyao] discovered that when running {{ozone fs -mkdir -p ofs://om/vol1/}} (only volume name is given, no bucket name or key path), the command would fail in {{getFileStatus}} (before reaching {{createDirectory()}}) in Hadoop common code:

{code:bash}
bash-4.2$ ozone fs -mkdir -p ofs://om/vol1/
-mkdir: Bucket or Volume length is illegal, valid length is 3-63 characters

# Same w/o -p
bash-4.2$ ozone fs -mkdir ofs://om/vol1/
-mkdir: Bucket or Volume length is illegal, valid length is 3-63 characters
{code}

And we discovered with debugger attached that the root cause is that {{getFileStatus()}} is not behaving as expected.

Solution: Patch existing OFS code, throw proper exception in {{getBucket()}} code to make Hadoop common happy.",pull-request-available,[],HDDS,Sub-task,Major,2020-02-04 20:34:29,12
13283310,Intermittent failure in TestResourceLimitCache,"{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/425954260}
2020-02-04T18:13:01.6043382Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.393 s <<< FAILURE! - in org.apache.hadoop.hdds.utils.TestResourceLimitCache
2020-02-04T18:13:01.6044180Z [ERROR] testResourceLimitCache(org.apache.hadoop.hdds.utils.TestResourceLimitCache)  Time elapsed: 0.285 s  <<< FAILURE!
2020-02-04T18:13:01.6045063Z java.lang.AssertionError: expected null, but was:<a>
...
2020-02-04T18:13:01.6072118Z 	at org.apache.hadoop.hdds.utils.TestResourceLimitCache.testResourceLimitCache(TestResourceLimitCache.java:85)
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2020-02-04 18:51:16,71
13283218,ozonesecure acceptance test fails due to unexpected error message,"{code}
Create volume with non-admin user                                     | FAIL |
...
PERMISSION_DENIED Only admin users are authorized to create Ozone volumes. User: testuser2/scm@EXAMPLE.COM' does not contain 'Client cannot authenticate via'
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-02-04 10:36:52,1
13283059,Recon throws error while trying to get snapshot in secure environment,"Recon throws the following exception while trying to get snapshot from OM in a secure env:


{code:java}
10:19:24.743 PMINFO OzoneManagerServiceProviderImpl Obtaining full snapshot from Ozone Manager
10:19:24.754 PMERROR OzoneManagerServiceProviderImpl Unable to obtain Ozone Manager DB Snapshot. 
javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:154)
	at sun.security.ssl.SSLSocketImpl.recvAlert(SSLSocketImpl.java:2020)
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1127)
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379)
	at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:394)
	at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:353)
	at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:141)
	at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)
	at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)
	at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
	at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)
	at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)
	at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
	at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at org.apache.hadoop.ozone.recon.ReconUtils.makeHttpCall(ReconUtils.java:232)
	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getOzoneManagerDBSnapshot(OzoneManagerServiceProviderImpl.java:239)
	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.updateReconOmDBWithNewSnapshot(OzoneManagerServiceProviderImpl.java:267)
	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:358)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
10:19:24.755 PMERROR OzoneManagerServiceProviderImpl Null snapshot location got from OM.
{code}",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Critical,2020-02-03 22:31:32,44
13282995,Print Freon summary to log in non-interactive mode,"Freon prints progress to log in non-interactive mode since HDDS-2861, but summary statistics are still printed to standard output.",pull-request-available,['freon'],HDDS,Improvement,Minor,2020-02-03 15:01:41,1
13282816,Fix root deletion logic in delete API,"Logic to prevent root from being getting deleted seems to be not working.

{code:java}
      if (key.equals(""/"")) {
        LOG.warn(""Cannot delete root directory."");
        return false;
      }
{code}
If it is root, the key will be empty not ""/"", As pathToKey() will remove the ""/"" and make it """" and there after addTrailingSlashIfNeeded() will not add a slash as it has a check !isEmpty(key) before adding slash.


Follow up from discussion at [HDDS-2685|https://github.com/apache/hadoop-ozone/pull/321]
",pull-request-available,[],HDDS,Bug,Major,2020-02-02 07:00:34,59
13282746,Any container replication error can terminate SCM service,"I found there any container replication error thrown in ReplicationManager can terminates SCM service. It's a very expensive behavior to terminate the SCM service just because of one container replication error.

It's not worth to shutdown the SCM. We can be friendly to deal with this, catch the exception and print the warn message with thrown exception.

The shutdown info:
{noformat}
2020-01-30 08:16:04,705 ERROR org.apache.hadoop.hdds.scm.container.ReplicationManager: Exception in Replication Monitor Thread.
java.lang.IllegalArgumentException: Affinity node /dc1/rack1/b9343ca0-a4bc-4436-9671-bc1de6c8bd89 is not a member of topology
        at org.apache.hadoop.hdds.scm.net.NetworkTopologyImpl.checkAffinityNode(NetworkTopologyImpl.java:789)
        at org.apache.hadoop.hdds.scm.net.NetworkTopologyImpl.chooseRandom(NetworkTopologyImpl.java:399)
        at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware.chooseNode(SCMContainerPlacementRackAware.java:249)
        at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware.chooseDatanodes(SCMContainerPlacementRackAware.java:173)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.handleUnderReplicatedContainer(ReplicationManager.java:515)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.processContainer(ReplicationManager.java:311)
        at java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649)
        at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.run(ReplicationManager.java:223)
        at java.lang.Thread.run(Thread.java:745)
2020-01-30 08:16:04,730 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.lang.IllegalArgumentException: Affinity node /dc1/rack1/b9343ca0-a4bc-4436-9671-bc1de6c8bd89 is not a member of topology
2020-01-30 08:16:04,734 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:
{noformat}",pull-request-available,['SCM'],HDDS,Bug,Major,2020-02-01 08:15:06,68
13282675,Add Transaction Log Index to OM Audit Logs,"When ratis is enabled for OM, it is possible that transactions will be replayed. Though most of the replays would be caught and ignored, there are some scenarios in which a transaction could be replayed.

For example, Key1 is created and deleted. If Key1 create transaction is replayed, there is no way to determine if this is replayed transaction as Key1 would not be existing in the DB anymore. Replaying this transaction would be ok as the delete Key1 transaction would also be replayed.

Though in such scenarios replaying transactions is ok, the audit log would also have a duplicate entry corresponding to the replayed transaction. To distinguish duplicate transactions, we propose adding the transactionLogIndex to the audit log for OM operations.",TriagePending,"['OM HA', 'Ozone Manager']",HDDS,Improvement,Major,2020-01-31 18:18:16,81
13282505,Use RequestDependentRetry Policy along with ExceptionDependentRetry Policy in OzoneClient,"This Jira is to use RequestDependentRetry Policy with ExceptionDependentRetry Policy so that for different exceptions and for different kinds of requests we can use different RetryPolicies.

Dependent on RATIS-799

 ",TriagePending,['Ozone Client'],HDDS,Bug,Major,2020-01-31 01:02:38,18
13282460,Handle replay of OM Prefix ACL requests,"To ensure that Prefix acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMPrefixAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 19:31:24,43
13282347,Collect output of crashed tests,"Save output of crashed unit/integration tests to the diagnostic bundle, similar to how it's done for failed tests.",pull-request-available,['build'],HDDS,Improvement,Major,2020-01-30 10:05:09,1
13282296,Handle replay of OM Key ACL requests,"To ensure that Key acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 01:11:33,43
13282295,Handle replay of OM Volume ACL requests,"To ensure that volume acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMVolumeAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 01:09:03,43
13282291,listBuckets result should include the exact match of bucketPrefix,"{{OzoneVolume.listBuckets(String bucketPrefix)}} behaves differently than {{ObjectStore.listVolumes(String volumePrefix)}} in terms of given prefix. In short, {{listBuckets}} ignores the {{bucketPrefix}} in the result if it is an *exact* match, while {{listVolumes}} doesn't.

e.g. If we have a bucket named {{bucket-12345}}, currently {{OzoneVolume.listBuckets(""bucket-12345"")}} will NOT return {{bucket-12345}} in its result.

Please see my attached test case for this. - I will move the test from {{TestOzoneFileSystem}} to a proper place ({{TestOzoneRpcClientAbstract}}, probably) in the PR.",pull-request-available,[],HDDS,Bug,Major,2020-01-30 00:44:25,12
13282287,Handle Replay of AllocateBlock request,"To ensure that allocate block operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMAllocateBlockRequest is made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-30 00:08:00,43
13282220,Unnecessary log messages in DBStoreBuilder,"DBStoreBuilder logs some table-related at INFO level.  This is fine for DBs that are created once per run, eg. OM or SCM, but Recon builds a new DB for each OM snapshot:

{code}
recon_1     | 2020-01-29 15:20:32,466 [pool-7-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/recon/om.snapshot.db_1580311232241
recon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: userTable
recon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: volumeTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: bucketTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTable
recon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: keyTable
recon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: deletedTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: openKeyTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTable
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3Table
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Table
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTable
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: dTokenTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTable
recon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: prefixTable
recon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTable
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: default
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default options. DBProfile.DISK
recon_1     | 2020-01-29 15:20:32,514 [pool-7-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB snapshot at /data/metadata/recon/om.snapshot.db_1580311232241.
{code}",pull-request-available,[],HDDS,Improvement,Minor,2020-01-29 15:41:08,1
13282027,Handle replay of S3 requests,"To ensure that S3 operations is idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

In this Jira, the following requests are made idempotent:
* S3InitiateMultipartUploadRequest
* S3MultipartUploadCommitPartRequest
* S3MultipartUploadCompleteRequest
* S3MultipartUploadAbortRequest",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-28 18:28:04,43
13282026,Ensure ozone manager service user is part of ozone.administrators,"Current we only add scm service principal to scmadmins at runtime. The ozone manager service principal is not honored as om admins. As a result, if user does not specify any user in 

ozone.administrators, they will not be able to create a volume. ",pull-request-available,[],HDDS,Sub-task,Major,2020-01-28 18:24:13,28
13282025,Improve usability issues of ozone.administrators,This is an umbrella Jira to improve usability issues around ozone.administrators.,Triaged,['Security'],HDDS,Improvement,Major,2020-01-28 18:19:54,28
13281922,Upgrade jetty to the latest 9.4 release,"The jetty which is used by web interfaces of Ozone is from the September of 2018.

Since then many bug and security fixes added to the Jetty project. I would suggest to use the latest jetty (from January of 2020).

As HttpServer2 (hadoop 3.2 class) has a strong dependency on the older version of Jetty (it uses SessionManager which is removed), it seems to be easier to clone HttpServer2 and move it to the Ozone project.

It provides us the flexibility:
 # To upgrade jetty independent from the Hadoop releases
 # To remove unused features
 # To support ozone style configuration
 # Add additional insights/metrics to our own HttpServer
 # Simplify the current server initialization (current BaseHttpServer class of hdds/ozone is a wrapper around the Hadoop class, but we can combine the two functionalities)",pull-request-available,[],HDDS,Improvement,Blocker,2020-01-28 11:40:27,6
13281841,mkdir : store directory entries in a separate table,"As of HDDS-2940, all the directories from the path prefix get created as entries in the key table. as per the namespace proposal attached to HDDS-2939, directory entries need to be stored in a separate ""directory"" table. Files will continue to be stored in the key table, which can be thought of as the ""file"" table.

The advantage of a separate directory table is to make directory lookup more efficient - the entire table would fit into memory for a typical file based dataset. ",backward-incompatible pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-01-28 03:48:47,40
13281819,Rename audit log should contain both srcKey and dstKey not just key,"Currently a rename key operation logs just the key to be renamed, in the audit log there should be the source and destination present as well for a rename operation if we want to have traceability over a file properly.",newbie pull-request-available,[],HDDS,Improvement,Major,2020-01-27 23:26:25,63
13281816,Implement ofs://: Add robot tests for mkdir,We need to add extra robot tests case (in addition to the existing ones adapted from o3fs) for ofs.,pull-request-available,[],HDDS,Sub-task,Major,2020-01-27 23:03:56,28
13281800,Handle replay of KeyCommitRequest and DirectoryCreateRequest,"To ensure that key commit operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyCommitRequest is made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-27 20:48:00,43
13281759,Parameterize unit tests for chunk manager implementation,Some existing unit tests rely on implementation details of existing ChunkManager (eg. TestKeyValueContainerCheck).  These should run common test cases on both implementations.  Implementation-specific test cases should be extracted to separate test classes.,pull-request-available,['test'],HDDS,Task,Major,2020-01-27 16:44:48,1
13281758,Putkey : create key table entries for intermediate directories in the key path,"Using path delimiter ('/'), parse the key as a FS file path. then create entries in OM key table for every directory element occurring in the path.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-01-27 16:41:52,40
13281756,file create : create key table entries for intermediate directories in the path,"similar to and a follow-up pf HDDS-2940
this change covers the file create request handler in the OM.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-01-27 16:36:20,88
13281753,mkdir : create key table entries for intermediate directories in the path,"During mkdir - the directory create request handler in the OM, create separate key table entries for each of the directories in the path.

This will ensure that every directory that exists in the FS namespace is explicitly listed as an entry in the key table -  instead of being implicit as part of the key of the leaf level directory or file.

This subsequently makes file create operation efficient - there is no requirement to run an expensive iterator on the key table to determine if the file name already exists as a directory.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2020-01-27 16:33:51,88
13281750,Ozone FS namespace,"Create the structures and metadata layout required to support efficient FS namespace operations in Ozone - operations involving folders/directories required to support the Hadoop compatible Filesystem interface.

The details are described in the attached document. The work is divided up into sub-tasks as per the task list in the document.",Triaged,['Ozone Manager'],HDDS,New Feature,Major,2020-01-27 16:27:06,40
13281320,Use regex to match with ratis  grpc properties when creating ratis server,"This Jira is to use regex which are matching withratis grpc properties and set them when creating ratis server. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis server, we can set them and use them without any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove in a clean up Jira to remove ozone config for ratis server or leave as it is for existing ones.

 

HDDS-2903 already taken care of properties matching with raft.server.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-01-24 01:41:16,13
13281237,Document bucket encryption option in shell/BucketCommands.md,"--bucketkey option is missing from shell/BucketCommands.md

 ",pull-request-available,[],HDDS,Improvement,Major,2020-01-23 16:44:57,28
13281228,Hive queries fail at readFully,"When running Hive queries on a 1TB dataset for TPC-DS tests, we started to see an exception coming out from FSInputStream.readFully.
This does not happen with a smaller 100GB dataset, so possibly multi block long files are the cause of the trouble, and the issue was not seen with a build from early december, so we most likely to blame a recent change since then. The build I am running with is from the hash 929f2f85d0379aab5aabeded8a4d3a5056777706 of master branch but with HDDS-2188 reverted from the code.

The exception I see:
{code}
Error while running task ( failure ) : attempt_1579615091731_0060_9_05_000029_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
        at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
        at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.io.IOException: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
        at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)
        at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)
        at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
        at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:532)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:178)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
        ... 16 more
Caused by: java.io.IOException: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:422)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
        ... 27 more
Caused by: java.io.EOFException: End of file reached before reading fully.
        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:126)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
        at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readStripeFooter(RecordReaderUtils.java:269)
        at org.apache.orc.impl.RecordReaderImpl.readStripeFooter(RecordReaderImpl.java:308)
        at org.apache.orc.impl.RecordReaderImpl.beginReadStripe(RecordReaderImpl.java:1089)
        at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1051)
        at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1219)
        at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1254)
        at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:284)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:67)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:83)
        at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:145)
        at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:135)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2046)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419)
        ... 28 more
{code}",pull-request-available,[],HDDS,Bug,Critical,2020-01-23 15:50:22,16
13281104,appropriate log message when creating keys fail due to failed server,"In a 3 node cluster, if one of the nodes is down, creating a key fails without proper error message.

Here's the sequence 

[root@ppogde-1 ~]# ozone fs -ls o3fs://bucket-73-13891.vol-1-96040/

Found 4 items

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-0-11625

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-1-10554

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-2-21504

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-3-54266

 

[root@ppogde-1 ~]# ozone fs -mkdir o3fs://bucket-73-13891.vol-1-96040/dir1

[root@ppogde-1 ~]# ozone fs -ls o3fs://bucket-73-13891.vol-1-96040/

Found 5 items

drwxrwxrwx   - root root          0 2020-01-22 10:21 o3fs://bucket-73-13891.vol-1-96040/dir1

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-0-11625

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-1-10554

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-2-21504

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-3-54266

[root@ppogde-1 ~]# 

[root@ppogde-1 ~]# ozone fs -put ./xyz o3fs://bucket-73-13891.vol-1-96040/dir1 

put: Allocated 0 blocks. Requested 1 blocks

[root@ppogde-1 ~]# ozone fs -ls o3fs://bucket-73-13891.vol-1-96040/

Found 5 items

drwxrwxrwx   - root root          0 2020-01-22 10:23 o3fs://bucket-73-13891.vol-1-96040/dir1

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-0-11625

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-1-10554

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-2-21504

-rw-rw-rw-   1 root root      10240 2019-12-06 14:34 o3fs://bucket-73-13891.vol-1-96040/key-3-54266

[root@ppogde-1 ~]# 

in the above sequence put has failed not giving much clue as to one of the nodes could have been down causing this failure.",TriagePending,['Ozone Filesystem'],HDDS,Bug,Major,2020-01-23 03:07:52,44
13281073,Implement ofs://: temp directory mount,"Because of ofs:// filesystem hierarchy starts with volume then bucket, an application typically won't be able to write directly under a first-level folder, e.g. ofs://service-id1/tmp/. /tmp/ is a special case that we need to handle since that is the default location most legacy Hadoop applications write to for swap/temporary files. In order to address this, we would introduce /tmp/ as a client-side ""mount"" to another Ozone bucket.

Note that the preliminary implementation would only allow for /tmp/ to be a mount but not any user-defined path.

This depends on HDDS-2840 and HDDS-2928.",pull-request-available,[],HDDS,Sub-task,Major,2020-01-22 22:04:05,12
13281066,Implement ofs://: listStatus,"ofs:// should be able to handle list (recursive or not) under ""root"" and under each volume ""directory"", as if it is a flat filesystem (if the user has permissions to see the volumes).

This is dependent on HDDS-2840. Will post PR after HDDS-2840 is reviewed and committed.",pull-request-available,[],HDDS,Sub-task,Major,2020-01-22 21:40:17,12
13280974,Cache EndPoint tasks instead of creating them all the time in RunningDatanodeState,"Currently, we create EndPoint tasks all the time. This is an inefficient way, we could cache these task as TODO comment suggested.

{code}
  //TODO : Cache some of these tasks instead of creating them
  //all the time.
  private Callable<EndpointStateMachine.EndPointStates>
      getEndPointTask(EndpointStateMachine endpoint) {
    switch (endpoint.getState()) {
    case GETVERSION:
      return new VersionEndpointTask(endpoint, conf, context.getParent()
          .getContainer());
    case REGISTER:
      return  RegisterEndpointTask.newBuilder()
          .setConfig(conf)
          .setEndpointStateMachine(endpoint)
          .setContext(context)
          .setDatanodeDetails(context.getParent().getDatanodeDetails())
          .setOzoneContainer(context.getParent().getContainer())
          .build();
    case HEARTBEAT:
      return HeartbeatEndpointTask.newBuilder()
          .setConfig(conf)
          .setEndpointStateMachine(endpoint)
          .setDatanodeDetails(context.getParent().getDatanodeDetails())
          .setContext(context)
          .build();
    case SHUTDOWN:
      break;
    default:
      throw new IllegalArgumentException(""Illegal Argument."");
     }
    return null;
   }
{code}",pull-request-available,[],HDDS,Improvement,Minor,2020-01-22 14:27:28,68
13280947,Remove hdfs-client dependency from hdds-common,"As of now we have a hdds-common > hdfs-client dependency but in reality we don't use an important thing only a few string utils.--

I would propose to remove this dependency to have a safer / smaller and more independent ozone package.",pull-request-available,[],HDDS,Improvement,Major,2020-01-22 11:08:59,6
13280913,Fix Pipeline#nodeIdsHash collision issue,"Currently SCM relies on the XOR of the three node ID hash to detect overlapped pipelines. Per offline discussion with [~aengineer], this could potentially give false positive result when detecting overlapped pipelines. This ticket is opened to fix the issue.  ",pull-request-available,[],HDDS,Sub-task,Major,2020-01-22 07:40:28,28
13280912,Add fall-back protection for rack awareness in pipeline creation,PipelinePlacementPolicy should have fallback logic to pick node when rack awareness cannot select a valid node.,pull-request-available,[],HDDS,Sub-task,Major,2020-01-22 07:34:01,77
13280905,Balance ratis leader distribution in datanodes,Ozone should be able to recommend leader host to Ratis via pipeline creation. The leader host can be recommended based on rack awareness and load balance.,pull-request-available,[],HDDS,Improvement,Major,2020-01-22 06:43:15,65
13280873,Remove ozone ratis server specific config keys,"Once HDDS-2903 went in, now we can use direct ratis server configurations in XceiverClientRatis. This Jira is to clean up the old configuration and add any new required configuration.",pull-request-available teragentest,[],HDDS,Bug,Major,2020-01-22 01:07:58,13
13280868,Remove ozone ratis client specific config keys,"HDDS-2896 went in, now we can use direct ratis client configurations in OzoneClient. This Jira is to clean up the old configuration and add any new required configuration.",pull-request-available teragentest,[],HDDS,Bug,Major,2020-01-22 00:30:46,13
13280790,Increase timeout of safe-mode exit in acceptance tests,"Acceptance tests seem to be randomly failing recently with SCM not exiting safe mode within the 90 seconds timeout.  This task proposes to increase the timeout to check if it helps.

Also, when acceptance test fails with this condition, containers are not stopped and logs not copied.  This should be fixed.

{code:title=example}
2020-01-21T10:48:10.2663685Z WARNING! Safemode is still on. Please check the docker-compose files
2020-01-21T10:48:10.2683558Z ERROR: Test execution of /home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31 is FAILED!!!!
2020-01-21T10:48:10.2712438Z cp: cannot stat '/home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31/result/robot-*.xml': No such file or directory
2020-01-21T10:48:10.2716708Z cp: cannot stat '/home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31/result/docker-*.log': No such file or directory
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-01-21 15:21:06,1
13280718,Add a different retry policy for watch requests,"Currently, every request submitted to client is retried with a default retry policy in ozone.

For watch requests , ideally the timeout is higher and there should be no retries. The idea is to address that.",Triaged pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2020-01-21 10:22:26,16
13280680,OM HA cli getserviceroles not working,"started docker based cluster with ""ozone.om.ratis.enable"" = true

OM started with ratis backend.

 
{noformat}
om_1        | 2020-01-20 09:02:48,116 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]om_1        | 2020-01-20 09:02:48,116 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]om_1        | 2020-01-20 09:02:49,213 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled clusterom_1        | 2020-01-20 09:02:49,279 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.18.0.2:9862om_1        | 2020-01-20 09:02:49,280 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefaultom_1        | 2020-01-20 09:02:49,294 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1        | 2020-01-20 09:02:49,315 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1        | 2020-01-20 09:02:50,268 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1        | 2020-01-20 09:02:50,309 [main] INFO util.log: Logging initialized @3941msom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: using custom profile for table: userTableom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTableom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: using custom profile for table: volumeTableom_1        | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: bucketTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: keyTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: deletedTableom_1        | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: openKeyTableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: s3Tableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Tableom_1        | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: dTokenTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: prefixTableom_1        | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTableom_1        | 2020-01-20 09:02:50,433 [main] INFO db.DBStoreBuilder: using custom profile for table: defaultom_1        | 2020-01-20 09:02:50,433 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:defaultom_1        | 2020-01-20 09:02:50,435 [main] INFO db.DBStoreBuilder: Using default options. DBProfile.DISKom_1        | 2020-01-20 09:02:50,620 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirsom_1        | 2020-01-20 09:02:50,647 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872om_1        | 2020-01-20 09:02:50,682 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)om_1        | 2020-01-20 09:02:50,762 [main] INFO grpc.GrpcFactory: PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.om_1        | It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.om_1        | 2020-01-20 09:02:50,767 [main] INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.port = 9872 (custom)om_1        | 2020-01-20 09:02:50,768 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)om_1        | 2020-01-20 09:02:50,770 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)om_1        | 2020-01-20 09:02:50,771 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)om_1        | 2020-01-20 09:02:50,771 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)om_1        | 2020-01-20 09:02:51,110 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)om_1        | 2020-01-20 09:02:51,117 [main] INFO impl.RaftServerProxy: a7718018-f8c6-4b70-90b7-aadd8f920710: addNew group-C5BA1605619E:[a7718018-f8c6-4b70-90b7-aadd8f920710:om:9872] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@2b0b7e5a[Not completed]om_1        | 2020-01-20 09:02:51,123 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
 
{noformat}
 

Ran ""getserviceroles"" command from CLI. ""getserviceroles"" API not working

 
{noformat}
/opt/hadoop/bin/ozone admin om getserviceroles -id=omServiceIdDefault/opt/hadoop/bin/ozone admin om getserviceroles -id=omServiceIdDefaultCouldn't create RpcClient protocol exception:java.lang.IllegalArgumentException: Could not find any configured addresses for OM. Please configure the system with ozone.om.address at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:138) at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:208) at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:155) at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:190) at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:122) at org.apache.hadoop.ozone.admin.om.OMAdmin.createClient(OMAdmin.java:59) at org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand.call(GetServiceRolesSubcommand.java:49) at org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand.call(GetServiceRolesSubcommand.java:32) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.ozone.admin.OzoneAdmin.main(OzoneAdmin.java:66)Couldn't create RpcClient protocol{noformat}",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Bug,Major,2020-01-21 08:52:42,13
13280540,Unit check passes despite Maven error,"Currently unit tests are broken due to some Surefire exception (HDDS-2898), but unit check is still passing.",pull-request-available,['build'],HDDS,Bug,Critical,2020-01-20 12:00:01,1
13280522,Certain Hive queries started to fail on generating splits,"After updating a cluster where I am running TPCDS queries, some queries started to fail.
The update happened from an early dec state to the jan 10 state of master.

Most likely the addition of HDDS-2188 is related to the problem, but it is still under investigation.

The exception I see in the queries:
{code}
[ERROR] [Dispatcher thread {Central}] |impl.VertexImpl|: Vertex Input: inventory initializer failed, vertex=vertex_XXXX [Map 1]
org.apache.tez.dag.app.dag.impl.AMUserCodeException: java.lang.RuntimeException: ORC split generation failed with exception: java.io.IOException: File o3fs://hive.warehouse.fqdn:9862/warehouse/tablespace/managed/hive/100/inventory/delta_0000001_0000001_0000/bucket_00000 should have had overlap on block starting at 0
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallback.onFailure(RootInputInitializerManager.java:328)
	at com.google.common.util.concurrent.Futures$6.run(Futures.java:1764)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456)
	at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:817)
	at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:753)
	at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:634)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:110)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: ORC split generation failed with exception: java.io.IOException: File o3fs://hive.warehouse.fqdn:9862/warehouse/tablespace/managed/hive/100/inventory/delta_0000001_0000001_0000/bucket_00000 should have had overlap on block starting at 0
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1915)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:2002)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:532)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:789)
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	... 5 more
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: File o3fs://hive.warehouse.fqdn:9862/warehouse/tablespace/managed/hive/100/inventory/delta_0000001_0000001_0000/bucket_00000 should have had overlap on block starting at 0
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1909)
	... 17 more
Caused by: java.io.IOException: File o3fs://hive.warehouse.fqdn:9862/warehouse/tablespace/managed/hive/100/inventory/delta_0000001_0000001_0000/bucket_00000 should have had overlap on block starting at 0
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.createSplit(OrcInputFormat.java:1506)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.generateOrUpdateSplit(OrcInputFormat.java:1678)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.generateSplitsFromStripes(OrcInputFormat.java:1661)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1609)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2900(OrcInputFormat.java:1383)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1568)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1565)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1565)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1383)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
{code}

I haven't found any more specific stack traces so far.",pull-request-available,[],HDDS,Bug,Critical,2020-01-20 10:31:14,63
13280494,Rename multi-raft ozone default configs for better understanding.,"Comments from: [https://docs.google.com/document/d/1NxCiHhn0u9BqgjuUXB8zxGtny69Qek4yTFe1QqUHiqM/edit]

 

Rename ozone.scm.datanode.max.pipeline.engagement to point at RATIS and fix some typos.",pull-request-available,[],HDDS,Sub-task,Major,2020-01-20 08:01:51,77
13280402,lastUsed and stateEnterTime value in container info is not human friendly,"{code}
ozone scmcli container list -s=7
{code}

{code}
{
  ""state"" : ""CLOSED"",
  ""replicationFactor"" : ""THREE"",
  ""replicationType"" : ""RATIS"",
  ""usedBytes"" : 4794248299,
  ""numberOfKeys"" : 7649,
  ""lastUsed"" : 5388521335,
  ""stateEnterTime"" : 808947405,
  ""owner"" : ""a46123a8-be63-4736-9478-ce4d8ac845cc"",
  ""containerID"" : 8,
  ""deleteTransactionId"" : 0,
  ""sequenceId"" : 0,
  ""open"" : false
}
{code}",newbie pull-request-available,[],HDDS,Improvement,Major,2020-01-19 13:37:49,4
13280375,OzoneManager startup failure with throwing unhelpful exception message,"Testing for the OM HA feature, I update the HA specific configurations and then start up the OM service. But I find OM is not startup succeed and I check the log, I get this error info and no any other helpful message.

{noformat}
...
2020-01-17 08:57:55,210 [main] INFO       - registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-17 08:57:55,846 [main] INFO       - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
2020-01-17 08:57:55,872 [main] INFO       - Found matching OM address with OMServiceId: om-service-test, OMNodeId: omNode-1, RPC Address: lyq-m1-xx.xx.xx.xx:9862 and Ratis port: 9872
2020-01-17 08:57:55,872 [main] INFO       - Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.omNode-1: lyq-m1-xx.xx.xx.xx:9874
2020-01-17 08:57:55,872 [main] INFO       - Setting configuration key ozone.om.https-address with value of key ozone.om.https-address.omNode-1: lyq-m1-xx.xx.xx.xx:9875
2020-01-17 08:57:55,872 [main] INFO       - Setting configuration key ozone.om.address with value of key ozone.om.address.omNode-1: lyq-m1-xx.xx.xx.xx:9862
OM not initialized.
2020-01-17 08:57:55,887 [shutdown-hook-0] INFO       - SHUTDOWN_MSG:
{noformat}

""OM not initialized"" doesn't give me enough info, then I have to check the related logic code. Finally, I find I have made a mistake that I forgot to do the om --init command first before startup the OM.

We can additionally add with suggestion here that will help users quickly know the error and how to resolved that.",pull-request-available,[],HDDS,Improvement,Minor,2020-01-19 08:45:07,68
13280331,Make checkPointManager and checkpoint folder of RDBStore configurable,Based on the review of HDDS-2889 in [https://github.com/apache/hadoop-ozone/pull/441] the location of the checkpoints might be more flexible if it's can be configured.,newbie,[],HDDS,Improvement,Major,2020-01-19 02:03:16,69
13280305,Add Unit Test cases for CRLCodec.,"At present, any certificate which gets revoked in SCM, is not transparently notified to DNs and local certificate copy on DNs are not being removed. 

This is the component for generating CRL for the revoked certificates. ",pull-request-available,[],HDDS,Sub-task,Major,2020-01-18 17:10:03,91
13280289,Remove unusued BlockLocation protocol related classes,"We have two classes which use org.apache.hadoop.hdfs.protocol.DatanodeInfo (HDFS!!) inside common. They are not used any more.

Even if they keep memories from the good old times, I would suggest to remove them to reduce the Hadoop dependency...",pull-request-available,[],HDDS,Improvement,Major,2020-01-18 12:38:55,6
13280277,Remove default dependencies from hadoop-hdds/pom.xml,"There are two ways to add certain set of dependencies to all the maven projects.
 # You can add it to the parent project which will be inherited to all the children projects
 # You can add it only to the required project and will be used via transitive dependencies

I think the 2nd approach is safest as we might need to create a new child project *without* Hadoop dependencies which is not possible with the 1st approach.",pull-request-available,[],HDDS,Improvement,Major,2020-01-18 08:40:03,6
13280213, Use regex to match with ratis properties when creating ratis server,"This Jira is to use regex which are matching with ratis server and ratis grpc properties and set them when creating ratis server. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis server, we can set them and use them without any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove in a clean up Jira to remove ozone config for ratis server or leave as it is for existing ones.",pull-request-available teragentest,['Ozone Datanode'],HDDS,Bug,Major,2020-01-17 19:48:11,13
13280206,execute_robot_test on unknown/unavailable container should fail acceptance test,"Before the fix for HDDS-2897, {{recon}} container in {{ozonesecure}} environment exited with error soon after start.  While working on the fix [~avijayan] noticed that the following test case (which tries to run some test on {{recon}} container) does not catch this error:

{code}
execute_robot_test recon <any test>
{code}

Ideally this should fail, since {{recon}} container is down.  But all commands that reference the container are run with [{{set +e}} in effect|https://github.com/apache/hadoop-ozone/blob/1caf1e30865aa2b380a7fca6d87f5ae8034fee4e/hadoop-ozone/dist/src/main/compose/testlib.sh#L99-L107], so the container's unavailibility is basically ignored, and acceptance test passes.",pull-request-available,['test'],HDDS,Bug,Major,2020-01-17 19:15:54,1
13280192,List Trash - Fix Cluster Max Keys Check,"The check for list trash max keys requested should be '<=' to cluster max keys.  If we set the cluster limit at 1,000 keys, I would expect to get back 1,000 at the max.

 

[https://github.com/apache/hadoop-ozone/blob/a47974307d8e8a1f42748346674845ba5c48bfb8/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java#L871]

 ",pull-request-available,[],HDDS,Bug,Minor,2020-01-17 17:24:55,92
13280095,Avoid logging NPE when space usage check is not configured,"If {{hdds.datanode.du.factory.classname}} is not configured, a harmless but annoying NPE appears in datanode log during startup.",pull-request-available,[],HDDS,Bug,Minor,2020-01-17 09:32:18,1
13280040,Build with test fails due to ClassFormatError due to hadoop-hdds-client test,"output of mvn clean package, not sure if it's normal
{code:java}
[INFO] Reactor Summary for Apache Hadoop Ozone Main 0.5.0-SNAPSHOT:
[INFO]
[INFO] Apache Hadoop Ozone Main ........................... SUCCESS [  0.789 s]
[INFO] Apache Hadoop HDDS ................................. SUCCESS [  2.994 s]
[INFO] Apache Hadoop HDDS Config .......................... SUCCESS [  4.201 s]
[INFO] Apache Hadoop HDDS Common .......................... SUCCESS [02:16 min]
[INFO] Apache Hadoop HDDS Client .......................... FAILURE [  1.052 s]
[INFO] Apache Hadoop HDDS Server Framework ................ SKIPPED
[INFO] Apache Hadoop HDDS Container Service ............... SKIPPED
[INFO] Apache Hadoop HDDS/Ozone Documentation ............. SKIPPED
[INFO] Apache Hadoop HDDS SCM Server ...................... SKIPPED
[INFO] Apache Hadoop HDDS Tools ........................... SKIPPED
[INFO] Apache Hadoop Ozone ................................ SKIPPED
[INFO] Apache Hadoop Ozone Common ......................... SKIPPED
[INFO] Apache Hadoop Ozone Client ......................... SKIPPED
[INFO] Apache Hadoop Ozone Manager Server ................. SKIPPED
[INFO] Apache Hadoop Ozone FileSystem ..................... SKIPPED
[INFO] Apache Hadoop Ozone Tools .......................... SKIPPED
[INFO] Apache Hadoop Ozone S3 Gateway ..................... SKIPPED
[INFO] Apache Hadoop Ozone CSI service .................... SKIPPED
[INFO] Apache Hadoop Ozone Recon CodeGen .................. SKIPPED
[INFO] Apache Hadoop Ozone Recon .......................... SKIPPED
[INFO] Apache Hadoop Ozone Integration Tests .............. SKIPPED
[INFO] Apache Hadoop Ozone Datanode ....................... SKIPPED
[INFO] Apache Hadoop Ozone In-Place Upgrade ............... SKIPPED
[INFO] Apache Hadoop Ozone Insight Tool ................... SKIPPED
[INFO] Apache Hadoop Ozone FileSystem Single Jar Library .. SKIPPED
[INFO] Apache Hadoop Ozone FileSystem Legacy Jar Library .. SKIPPED
[INFO] Apache Hadoop Ozone Distribution ................... SKIPPED
[INFO] Apache Hadoop Ozone Fault Injection Tests .......... SKIPPED
[INFO] Apache Hadoop Ozone Network Tests .................. SKIPPED
[INFO] Apache Hadoop Ozone Mini Ozone Chaos Tests ......... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:26 min
[INFO] Finished at: 2020-01-17T11:31:57+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-hdds-client: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test failed: java.lang.ClassFormatError: Illegal field name ""org.apache.hadoop.hdds.scm.storage.TestBlockInputStream$this"" in class org/apache/hadoop/hdds/scm/storage/TestBlockInputStream$DummyBlockInputStreamWithRetry -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-hdds-client
{code}",pull-request-available,[],HDDS,Bug,Critical,2020-01-17 04:05:11,6
13279983,Ozone recon Start failed due to Kerberos principal not being found.,"Ozone recon Start failed due to Kerberos principal not being found.
{code}
Caused by: javax.servlet.ServletException: javax.servlet.ServletException: Principal not defined in configuration
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180)
	at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)
	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:881)
	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349)
	at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406)
	at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368)
	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778)
	at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262)
	at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113)
	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
	at org.eclipse.jetty.server.Server.start(Server.java:427)
	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105)
	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
	at org.eclipse.jetty.server.Server.doStart(Server.java:394)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1199)
	... 13 more
Caused by: javax.servlet.ServletException: Principal not defined in configuration
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:137)
	... 35 more {code}",pull-request-available,[],HDDS,Bug,Major,2020-01-16 21:08:12,30
13279858,Use regex to match with ratis properties when creating ratis client,"This Jira is to use regex which are matching with ratis client and ratis grpc properties and set them when creating ratis client. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis client, we can set them and use them with out any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove a clean up Jira to remove ozone config for ratis client or leave as it is for existing ones.",pull-request-available teragentest,['Ozone Client'],HDDS,Bug,Major,2020-01-16 15:39:45,13
13279779,Generate only the required keytabs for docker based secure tests,"We have acceptance tests with the help of docker/docker-compose where we generate the keytab files on the fly with the help of a lightweight (unsecure) REST endpoint.

But this generation can be very slow especially when the DNS is slow. When I start a VPN the secure cluster can't be started in the 90 sec period. (>100 keytabs are generated and one keytab generation is ~5 sec).

The solutions is to generate only the *required* keystabs in each of the containers.

Instead of request all the possible keytabs in the *generic* docker-config:
{code:java}
KERBEROS_KEYTABS=dn om scm HTTP testuser testuser2 s3g {code}
We can defined the required keytabs per service (in docker-compose.yaml)
{code:java}
environment:
  KERBEROS_KEYTABS=scm HTTP{code}
With this approach ~20 keytab file will be generated instead of >100 and the secure tests will be significant faster.",pull-request-available,[],HDDS,Improvement,Major,2020-01-16 08:58:12,6
13279695,Handle replay of KeyDelete and KeyRename Requests,"To ensure that key deletion and rename operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyDeleteRequest and OMKeyRenameRequest are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-15 23:05:35,43
13279694,Handle replay of KeyPurge Request,"If KeyPurgeRequest is replayed, we do not want to purge the keys which were created after the original purge request was received. This could happen if a key was deleted, purged and then created and deleted again. The the purge request was replayed, it would purge the key deleted after the original purge request was completed.
Hence, to maintain idempotence, we should only purge those keys from DeletedKeys table that have updateID < transactionLogIndex of the request.
",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-15 23:02:26,43
13279558,Fix typo in OzoneManagerRatisUtils#createClientRequest,"The description of *OzoneManagerRatisUtils#createClientRequest* is 
 *Create OMClientRequest which enacpsulates the OMRequest.* for now.

There is a typo *enacpsulates* that should be *encapsulates*.",newbie pull-request-available,[],HDDS,Improvement,Trivial,2020-01-15 10:31:21,89
13279555,Apache NiFi PutFile processor is failing with secure Ozone S3G," 

(1) Create a simple PutS3Object processor in NiFi

(2) The request from NiFi to S3g will fail with HTTP 500

(3) The exception in the s3g log:

 
{code:java}
 s3g_1       | Caused by: java.io.IOException: Couldn't create RpcClient protocol
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:197)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:173)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:74)
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:114)
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)
s3g_1       | 	... 92 more
s3g_1       | Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid S3 identifier:OzoneToken owner=testuser/scm@EXAMPLE.COM, renewer=, realUser=, issueDate=0, maxDate=0, sequenceNumber=0, masterKeyId=0, strToSign=AWS4-HMAC-SHA256
s3g_1       | 20200115T101329Z
s3g_1       | 20200115/us-east-1/s3/aws4_request
s3g_1       | (hash), signature=(sign), awsAccessKeyId=testuser/scm@EXAMPLE.COM{code}
 ",pull-request-available,[],HDDS,Bug,Critical,2020-01-15 10:24:15,6
13279492,Make DBStore and RDBStore more commons,"As a user of  RDBStore and TypedTable, i only use these class to maintain my rocksdb, i use hadoop-hdds-common as a maven dependency. 

But during my progress running, i find a folder named `om.db.checkpoints`,  it is terrible, i do not know om, i only want to use a common typedTable which help me maintains rocksdb.

So we should rename it, remove the concept of OM. ",pull-request-available,['SCM'],HDDS,Bug,Major,2020-01-15 03:47:26,69
13279441,Refactor Datanode StateContext to send reports and actions to all configured SCMs,"Currently the StateContext in Datanode has the set of actions and reports to be pushed to SCM(s) in the next Heartbeat. Since the state context is shared across all the configured SCM (& Recon) endpoints, an action or report is only pushed to one SCM server based on FCFS. This needs to be changed to a model every SCM server gets the reports and actions. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-14 21:07:55,30
13279379,Add config to tune replication level of watch requests in Ozone client,"Currently, while sending watch requests in ozone client, it sends watch requests with ratis replication level set to ALL_COMMITTED and in case it fails, it sends the request  with MAJORITY_COMMITTED semantics. The idea is to configure the replication level for watch requests so as to measure performance.",Triaged,['Ozone Client'],HDDS,Bug,Major,2020-01-14 15:13:30,16
13279318,parse and dump datanode segment file to pritable text,"Add a tool to parse and dump datanode ratis log files and dump them in a string format.

This tool will help in debugging of ratis issues.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2020-01-14 10:54:38,18
13279227,Change the default client settings accordingly with change in default chunk size,"This Jira proposes to change the following values

ozone.client.stream.buffer.flush.size - 64MB -> 16MB

ozone.client.stream.buffer.max.size - 128 MB -> 32MB

ozone.scm.chunk.size - 16MB -> 4MB

 

In this way, the client-side holding buffer size will be reduced, and data is transferred at smaller size intervals.

 

This Jira is to discuss about these config settings changes and change these values accordingly.",pull-request-available,[],HDDS,Bug,Major,2020-01-14 00:14:24,13
13279225,Change the default setting of request.timeout.duration for ratis client and server,"This Jira proposes to change the default value of dfs.ratis.client.request.timeout.duration and dfs.ratis.server.request.timeout.duration.

 

In Teragen testing, we have seen many requests failing with timeout, so this Jira is to change the value of the above config from default 3s -> 10s",Triaged performance,[],HDDS,Bug,Critical,2020-01-14 00:10:53,13
13279033,Legacy (isolated) ozonefs couldn't work in secure environments,"The unsecure ozonefs is tested together with different version of Hadoop (2.7, 3.1, 3.2). The ""legacy"" (isolated) ozonefs package makes it possible to use it together with older Hadoop versions.

But the ozonesecure-mr (testing ozonefs with mapreduce in secure Ozone environment) is executed only with Hadoop 3.2 (which uses the ""current"" (shared) ozonefs instead of the ""legacy"" (isolated))

It turned on that the ""legacy"" ozonefs couldn't work in secure environment which means Ozone can be used only together with Hadoop 3.2 if the Hadoop/Ozone environment is secure

There are multiple problems. The first visible one is the following (run cd compose/ozonesecure-mr && ./test.sh after s/current/legacy/ in docker-config/docker-compose.yaml):
{code:java}
2020-01-13 10:03:39 ERROR OzoneClientFactory:193 - Couldn't create RpcClient protocol exception: 
java.io.IOException: DestHost:destPort om:9862 , LocalHost:localPort rm/172.21.0.10:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at sun.reflect.GeneratedConstructorAccessor2.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy13.submitRequest(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy13.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:394)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1283)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71) {code}",Triaged,[],HDDS,Bug,Critical,2020-01-13 09:58:56,6
13278974,Update return description of OzoneManager#openKey(OmKeyArgs args) ,"The return type of [*OzoneManager#openKey(OmKeyArgs args)*|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L1977#L2010] is *OpenKeySession*, 
 and we should update the description of return from
*@return OmKeyInfo - the info about the allocated key.* 
to 
*@return OpenKeySession - a handler to key that client uses to talk to container.*",Triaged pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2020-01-13 02:24:25,55
13278908,Refactor MiniOzoneLoadGenerator to add more load generators to chaos testing,This jira proposes to add new load generators to chaos testing to test new modes of failures.,pull-request-available,['test'],HDDS,Bug,Major,2020-01-12 13:30:41,18
13278897,Fix description of return type,"In this [method|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java#L616#L623], the return type is *List<OzoneKey>*.

The description of return type is *List<OzoneVolume>* for now, we should update it to *List<OzoneKey>*",newbie pull-request-available,[],HDDS,Improvement,Minor,2020-01-12 07:37:27,71
13278790,Consolidate ObjectID and UpdateID from Info objects into one class,"We use ObjectID and BucketID in OMVolumeArgs, OMBucketInfo, OMKeyInfo and OMMultipartKeyInfo. We can consolidate by having these Info objects extend a ""WithObjectID"" class which can host the common fields - objectID and updateID.
",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-10 20:40:48,43
13278642,Add a config in ozone to tune max outstanding requests in raft client,"Add a config to tune the config value of ""raft.client.async.outstanding-requests.max"" config in raft client. ",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2020-01-10 08:13:19,13
13278637,Fix description of return in getNextListOfBuckets,"In this [line|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java#L319], the description of return should be *List<OzoneBucket>*",newbie pull-request-available,[],HDDS,Improvement,Minor,2020-01-10 07:42:10,71
13278633,ozone.recon.scm.db.dirs missing from ozone-default.xml,"{{ozone.recon.scm.db.dirs}} is reported by {{TestOzoneConfigurationFields}} to be missing from {{ozone-default.xml}}. If it is to be documented, then please add the property to {{ozone-default.xml}}. If it's a developer-only setting, please add as exception in {{TestOzoneConfigurationFields#addPropertiesNotInXml}}.

(Sorry for reporting this post-commit. {{TestOzoneConfigurationFields}} will be run by CI once we have integration tests enabled again.)",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2020-01-10 07:34:50,30
13278590,Handle replay of KeyCreate requests,"To ensure that key creation operation is idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.

OMKeyCreateRequest and OMFileCreateRequest are made idempotent in this Jira.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2020-01-10 01:52:34,43
13278546,Add ObjectID and UpdateID to OMKeyInfo,"Similar to OMVolumeInfo and OMBucketInfo, we need objectID and updateID in OMKeyInfo as well to ensure that transactions are idempotent.
This Jira adds objectID and updateID to OMKeyInfo proto and sets these values when creating/ updating keys. ",pull-request-available,[],HDDS,Improvement,Major,2020-01-09 22:05:36,43
13278540,Generic Extensible Token support for Ozone,This is the umbrella Jira to add generic token support across ozone components. I will attach a design spec for review and comments. ,Triaged,['Security'],HDDS,New Feature,Major,2020-01-09 21:43:12,28
13278530,Intermittent failure in TestOzoneManagerRocksDBLogging,"{{TestOzoneManagerRocksDBLogging}} fails intermittently.  I think the problem is that RocksDB-specific string may appear in the log in the {{Finalizer}} thread after after OM is shutdown and restarted in {{Thread-1}}:

{code:title=output}
2020-01-09 16:17:29,372 [Thread-1] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:390] Shutdown: canceling all background work
2020-01-09 16:17:29,373 [Thread-1] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:563] Shutdown complete
2020-01-09 16:17:29,373 [Thread-1] INFO  om.OzoneManager (OzoneManager.java:restart(1127)) - OzoneManager RPC server is listening at localhost/127.0.0.1:33899
...
2020-01-09 16:17:29,561 [Thread-1] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(216)) - HTTP server of ozoneManager listening at http://0.0.0.0:34495
...
2020-01-09 16:17:37,988 [Finalizer] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:390] Shutdown: canceling all background work
2020-01-09 16:17:37,989 [Finalizer] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:563] Shutdown complete
{code}

Swapping the order of tests (ie. running with logging disabled first) would guarantee that stray messages from the logger do not affect it.",pull-request-available,['test'],HDDS,Bug,Minor,2020-01-09 20:24:35,1
13278506,TestOMDbCheckpointServlet fails due to real Recon,"TestOMDbCheckpointServlet verifies that no checkspoints have been taken before it invokes the servlet.  However, the same servlet is used by the actual Recon server in the mini cluster.

{code:title=test failure}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.313 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet
testDoGet(org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet)  Time elapsed: 30.192 s  <<< FAILURE!
java.lang.AssertionError
...
	at org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet.testDoGet(TestOMDbCheckpointServlet.java:155)
{code}

{code:title=output}
2020-01-09 16:13:56,913 [pool-61-thread-1] INFO  impl.OzoneManagerServiceProviderImpl (OzoneManagerServiceProviderImpl.java:syncDataFromOM(325)) - Syncing data from Ozone Manager.
2020-01-09 16:13:56,914 [pool-61-thread-1] INFO  impl.OzoneManagerServiceProviderImpl (OzoneManagerServiceProviderImpl.java:syncDataFromOM(356)) - Obtaining full snapshot from Ozone Manager
{code}",pull-request-available,['test'],HDDS,Bug,Major,2020-01-09 18:04:16,1
13278499,BindException in TestSCMRestart,"Recon exposes SCM-like RPC endpoints on (possibly) different port than SCM. However, when the RPC server [updates the config with the actual address|https://github.com/apache/hadoop-ozone/blob/046a06f02783da516179ee8d8d1bed862d22f78d/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeProtocolServer.java#L166-L168] after startup, it does so using the SCM-specific config key. Now that Recon is part of {{MiniOzoneCluster}}, this causes BindException in {{TestSCMRestart}} (and possibly other integration tests):
{code:java|title=output}
2020-01-09 16:07:45,370 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(775)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:36225
...
2020-01-09 16:07:51,594 [main] INFO  scm.ReconStorageContainerManager (ReconStorageContainerManager.java:start(91)) - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:38907
{code}
{code:java|title=test failure}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.653 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart
org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart  Time elapsed: 33.642 s  <<< ERROR!
java.net.BindException: Problem binding to [0.0.0.0:38907]
...
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.startRpcServer(StorageContainerManager.java:579)
	at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.<init>(SCMDatanodeProtocolServer.java:158)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:327)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:212)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:594)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.restartStorageContainerManager(MiniOzoneClusterImpl.java:295)
	at org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.init(TestSCMRestart.java:78)
{code}",pull-request-available,"['Ozone Recon', 'SCM']",HDDS,Bug,Major,2020-01-09 17:46:17,1
13278470,Support Freon progressbar in non-interactive environment,"Freon tests use the ProgressBar to show the current progress of the Freon tests.

 

Progress bar use ""\r"" to print out a nice, interactive progressbar to the same line.

 

Unfortunately this doesn't work very well when the standard output is redirected to a file (or in a containerized environment). I would suggest to use a simplified method using log4j in non-interactive environments.

 

The easiest way to do is to use ""System.console()"" java method. If it returns with null we are in a non interactive environment. We can also increase the logging period from 1s to 10s in case of non-interactive environment (==long term testing).",pull-request-available,[],HDDS,Improvement,Major,2020-01-09 14:41:17,6
13278466,Cluster disk space metrics should reflect decommission and maintenance states,"Now we have decommission states, we need to adjust the cluster capacity, space used and available metrics which are exposed via JMX.

For a node decommissioning, the space used on the node effectively needs to be transfer to other nodes via container replication before decommission can complete, but this is difficult to track from a space usage perspective. When a node completes decommission, we can assume it provides no capacity to the cluster and uses none. Therefore, for decommissioning + decommissioned nodes, the simplest calculation is to exclude the node completely in a similar way to a dead node.

For maintenance nodes, things are even less clear. For a maintenance node, it is read only so it cannot provide capacity to the cluster, but it is expected to return to service, so excluding it completely probably does not make sense. However, perhaps the simplest solution is to do the following:

1. For any node not IN_SERVICE, do not include its usage or space in the cluster capacity totals.
2. Introduce some new metrics to account for the maintenance and perhaps decommission capacity, so it is not lost eg:

{code}
# Existing metrics
""DiskCapacity"" : 62725623808,
""DiskUsed"" : 4096,
""DiskRemaining"" : 50459619328,

# Suggested additional new ones, with the above only considering IN_SERVICE nodes:

""MaintenanceDiskCapacity"": 0
""MaintenanceDiskUsed"": 0
""MaintenanceDiskRemaining"": 0
""DecommissionedDiskCapacity"": 0
""DecommissionedDiskUsed"": 0
""DecommissionedDiskRemaining"": 0
...
{code}

That way, the cluster totals are only what is currently ""online"", but we have the other metrics to track what has been removed etc. The key advantage of this, is that it is easy to understand.

There could also be an argument that the new decommissionedDisk metrics are not needed as that capacity is technically lost from the cluster forever.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2020-01-09 14:31:23,11
13278456,Hugo error should be propagated to build,"Currently build is successful even if {{hugo}} encounters error.  Example:

{code}
Error: Error building site: ""hadoop-hdds/docs/content/start/_index.zh.md:39:119"": unterminated quoted string in shortcode parameter-argument: 'start
'
...
[INFO] BUILD SUCCESS
{code}

The error itself is [being fixed|https://github.com/apache/hadoop-ozone/pull/417/files#diff-3553ca3378ab646dea469e6a1c58c100] in HDDS-2726.  However, it would be nice to make such a Hugo error result in failed build, so that such issues can be caught by CI.",pull-request-available,['build'],HDDS,Bug,Major,2020-01-09 13:56:24,1
13278330,EncryptionInfo should be generated by leader OM,"Right now each OM generates file encryption Info. In OM HA, the leader should generate encryptionInfo, and followers need to use the same information and store it in OM DB.",Triaged,[],HDDS,Bug,Critical,2020-01-09 01:19:07,13
13278250,Ozone Recon fails to start while ozone install,"While trying to install ozone in cdpd cluster, ozone Recon failed to start.

{code}
exec /opt/cloudera/parcels/CDH-7.1.0-1.cdh7.1.0.p0.1728961/lib/hadoop-ozone/../../bin/ozone recon
log4j:ERROR Could not instantiate class [org.cloudera.log4j.redactor.RedactorAppender].
java.lang.ClassNotFoundException: org.cloudera.log4j.redactor.RedactorAppender
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:124)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:785)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
	at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
	at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
	at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
	at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
	at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)
	at org.apache.hadoop.ozone.recon.ReconServer.<clinit>(ReconServer.java:38)
log4j:ERROR Could not instantiate appender named ""redactorForRootLogger"".
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2

{code}",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-08 21:30:06,30
13278249,Recon should use Java based configuration API.,"* Configs in org.apache.hadoop.hdds.recon.ReconConfigKeys need to be moved to Java based configurations.
* ReconConfigKeys needs to be added to org.apache.hadoop.ozone.TestOzoneConfigurationFields.
* Add properties in org.apache.hadoop.ozone.recon.ReconServerConfigKeys that are not already in ozone-default.xml",newbie,['Ozone Recon'],HDDS,Bug,Major,2020-01-08 21:28:26,44
13278247,Recon getContainers API should return a maximum of 1000 containers by default.,"Following the example of OM and SCM, Recon should by default limit the number of containers returned in the getContainers API to 1000. Currently, the default is ALL.

Suggested by [~aengineer]",pull-request-available,[],HDDS,Sub-task,Major,2020-01-08 21:17:52,37
13278054,NPE in OzoneContainer Start,"{code:java}
Unable to communicate to SCM server at ozone-test-bh-3.vpc.cloudera.com:9861 for past 2400 seconds.
 java.io.IOException: java.lang.NullPointerException
 at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
 at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
 at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:70)
 at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:284)
 at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:296)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:433)
 at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:232)
 at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:110)
 at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: java.lang.NullPointerException
 at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.triggerHeartbeat(DatanodeStateMachine.java:384)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.sendPipelineReport(XceiverServerRatis.java:756)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.notifyGroupAdd(XceiverServerRatis.java:737)
 at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.initialize(ContainerStateMachine.java:213)
 at org.apache.ratis.server.impl.ServerState.initStatemachine(ServerState.java:160)
 at org.apache.ratis.server.impl.ServerState.<init>(ServerState.java:112)
 at org.apache.ratis.server.impl.RaftServerImpl.<init>(RaftServerImpl.java:113)
 at org.apache.ratis.server.impl.RaftServerProxy.lambda$newRaftServerImpl$2(RaftServerProxy.java:208)
 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
 ... 3 more{code}",pull-request-available,[],HDDS,Bug,Major,2020-01-08 02:08:19,13
13278017,Some OM Ratis config properties missing from ozone-default.xml,"The following config properties missing from {{ozone-default.xml}} were pointed out by {{TestOzoneConfigurationFields}}:
 * {{ozone.om.ratis.client.request.timeout.duration}}
 * {{ozone.om.ratis.segment.preallocated.size}}",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2020-01-07 21:19:26,13
13277987,Handle Create container use case in Recon.,"CREATE container needs to be handled differently in Recon since these actions are initiated in the SCM, and Recon does not know about that. It should not throw ContainerNotFoundException when it suddenly sees a new container.

The idea is to let Recon ask SCM about the new container whenever it sees a new one.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-07 18:47:40,30
13277889,OM and SCM Web-server report HTTP 404 error when accessing '/' after cluster runs for several weeks,"<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 404 Not Found</title>
</head>
<body><h2>HTTP ERROR 404</h2>
<p>Problem accessing /. Reason:
<pre>    Not Found</pre></p>
</body>
</html>

Update:
Access ""/conf"" and ""/prom"" return the correct responses.  It seems only the access ""/"" reports 404. ",pull-request-available,[],HDDS,Bug,Major,2020-01-07 09:17:00,5
13277803,Recon changes to make snapshots work with OM HA,"Recon talks to OM in 2 ways - Through HTTP to get DB snapshot, and through RPC to get delta updates.

Since Recon already uses the OzoneManagerClientProtocol to query the OzoneManager RPC, the RPC client automatically routes the request to the leader on an OM HA cluster. Recon only needs the updates from the OM RocksDB store, and does not need the in flight updates in the OM DoubleBuffer. Due to the guarantee from Ratis that the leader’s RocksDB will always be up to date, Recon does not need to worry about going back in time when a current OM leader goes down. We have to pass in the om service ID to the Ozone Manager client in Recon, and the failover works internally. Currently we pass in 'null'.

To make the HTTP call to work against OM HA, Recon has to find out the current OM leader and download the snapshot from that OM instance. We can use the way this has been implemented in org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand. We can get the roles of OM instances and then determine the leader from that. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-06 23:29:47,86
13277802,Add Recon tasks for tracking missing containers (FSCK) and syncing deleted pipelines from SCM.,"* Recon should track the list of containers that have no replicas in its own SQL DB. This information will be used to serve the Missing containers endpoint that returns the list of containers missing along with keys that were part of it.
* If SCM CLI is used to close a pipeline in SCM, Recon does not get any ACK from the Datanode. This patch adds a pipeline sync task in Recon that asks SCM for a list of pipelines and cleans up invalid pipelines.",pull-request-available,[],HDDS,Sub-task,Major,2020-01-06 23:27:41,30
13277800,Handle Datanode Pipeline & Container Reports reports in Recon,"Recon should be able to process Container and Pipeline reports from registered Datanodes. It still wont be able to understand new containers and pipelines though. Along with HDDS-2846, HDDS-2852 and HDDS-2869, Recon will be a fully functional passive SCM.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-06 23:24:23,30
13277789,Fix listing keys for setting --start with last key,"When we list keys and set *--start* with last key for now, the result shows all keys.
But the proper result is empty.

So this Jira aims to fix it.
Example is attached.
 ",pull-request-available,[],HDDS,Bug,Major,2020-01-06 21:25:13,31
13277761,Implement ofs://: mkdir,A sub-task in HDDS-2665 to lay the foundation and make mkdir work in the new filesystem.,pull-request-available,[],HDDS,Sub-task,Major,2020-01-06 18:54:13,12
13277742,update ozone to latest ratis,There are some critical fixes that went in latest ratis . Need the changes available in ozone.,pull-request-available,[],HDDS,Bug,Major,2020-01-06 17:13:49,16
13277728,Directly read into ByteBuffer if it has array,{{OzoneFSInputStream#read(ByteBuffer)}} can avoid buffer copy if the target buffer has an array by directly reading into it.,pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2020-01-06 16:10:35,1
13277720,Enable integrations tests for github actions,"When we switched to use github actions the integration tests are disabled due to the flakyness.

We should disable all the flaky tests and enable the remaining integration tests...",pull-request-available,['test'],HDDS,Improvement,Major,2020-01-06 14:48:33,1
13277707,Fix listing buckets for setting --start with last bucket,"When we list bucket and set *--start* with last bucket for now, the result shows all buckets.
But the proper result is empty.

So this Jira aims to fix it.
 ",pull-request-available,[],HDDS,Bug,Major,2020-01-06 13:56:40,31
13277246,Remove unnecessary sleep in acceptance test,"{{start_docker_env}} has a 10-second sleep after environment is up.  I think it was originally added to give SCM a chance to get out of safe mode after all datanodes are up.  Now that acceptance test has a proper {{wait_for_safemode_exit}} condition, I think the sleep is no longer necessary.",pull-request-available,['test'],HDDS,Improvement,Minor,2020-01-02 17:45:03,1
13277159,Deduplicate KDC docker image definition,Docker image definition and associated files are duplicated between {{ozonesecure}} and {{ozonesecure-mr}} environments.  They should be defined in one place and referenced by both.,pull-request-available,['docker'],HDDS,Improvement,Minor,2020-01-02 08:33:59,1
13277134,Add initial UI of Pipelines in Recon,The Pipelines page in Recon should give the Recon admin users a detailed view of active Ratis Data Pipelines in Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this pipelines page is attached.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-02 05:16:01,37
13277132,Add initial UI of Datanodes in Recon,The Datanodes page in Recon should give the Recon admin users a detailed view of datanodes present in the Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this Datanodes page is attached.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-02 05:13:01,37
13277131,Add initial UI of Dashboard for Recon,The dashboard for Recon should give the Recon admin users an overview of Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this dashboard is attached.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2020-01-02 05:11:36,37
13276872,SCM HA Support ,"OM HA is close to feature complete now. It's time to support SCM HA, to make sure there is no SPoF in the system.

 

Design doc: https://docs.google.com/document/d/1vr_z6mQgtS1dtI0nANoJlzvF1oLV-AtnNJnxAgg69rM/edit?usp=sharing",pull-request-available,['SCM HA'],HDDS,New Feature,Major,2019-12-30 12:19:32,66
13276769,OM Ratis dir creation may fail,"OM Ratis dir creation is attempted from 2 threads:

# Ratis server initializes storage dir in async CompletableFuture
# OM init directly creates it

The latter may encounter the following exception:

{code}
java.lang.IllegalArgumentException: Unable to create path: /github/workspace/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bdc6aaad-da50-4196-ab1e-80aad183d7b2/omNode-2/ratis
 	at org.apache.hadoop.ozone.OmUtils.createOMDir(OmUtils.java:493)
 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:426)
 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:862)
 	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.createOMService(MiniOzoneHAClusterImpl.java:267)
 	at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.build(MiniOzoneHAClusterImpl.java:199)
{code}

at:

{code:title=https://github.com/apache/hadoop-ozone/blob/529438a28882b085d1095feccf6c7d6782a6a833/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java#L448-L454}
  public static File createOMDir(String dirPath) {
    File dirFile = new File(dirPath);
    if (!dirFile.exists() && !dirFile.mkdirs()) {
      throw new IllegalArgumentException(""Unable to create path: "" + dirFile);
    }
    return dirFile;
  }
{code}

It may happen that {{exists()}} returns {{false}} because the dir is does not exist yet, but {{mkdirs()}} also returns {{false}} because the dir is created in the meantime by the Ratis thread.",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-12-29 14:27:13,1
13276731,Timeout threshold  is too small to exit Safemode when security enabled,"When I run ozone-0.5.0-SNAPSHOT/compose/ozonesecure/test.sh on computer with 8 core 1999.971 Mhz, 16G memory, it always fail and report ""Safemode is still on"". Then I enlarge the timeout threshold in testlib.sh from 90 seconds to 360 seconds, and record the time to exist safemode 10 times,  and the average time to exit safemode is about 160 seconds.

!image-2019-12-29-11-35-51-332.png!

!image-2019-12-29-11-16-41-739.png!",pull-request-available,['SCM'],HDDS,Bug,Major,2019-12-29 02:53:59,93
13276675,Fix listing buckets for setting --prefix equal to bucket name,"When we list bucket and set *--prefix* with some bucket name.
We should include the bucket name that is the same as <prefix> but now we exclude it.

This Jira aims to fix it.",pull-request-available,[],HDDS,Bug,Major,2019-12-28 06:37:30,31
13276674,Fix shell description for --start parameter of listing keys,"In master-branch, the listing key show the description 
 *--start=<startKey> The first key to start the listing*

We should update the description to ""*The key to start the listing from. This will be excluded from the result.*""",newbie pull-request-available,[],HDDS,Improvement,Minor,2019-12-28 06:18:25,55
13276673,Fix shell description for --start parameter of listing buckets,"In master-branch, the listing bucket show the description 
 *--start=<startBucket> The first bucket to start the listing*

We should update the description to ""*The listing will start from bucket after the startBucket.*""",newbie pull-request-available,[],HDDS,Improvement,Minor,2019-12-28 06:09:49,31
13276537,Fail to connect  s3g in docker container with network proxy,"When run compose/ozones3-haproxy/test.sh on computer which connect network by proxy, it fails to curl [http://scm:9876|http://scm:9876/static/bootstrap-3.4.1/js/bootstrap.min.js] as the image show. Because curl try to resolve [http://scm:9876|http://scm:9876/static/bootstrap-3.4.1/js/bootstrap.min.js] by the network proxy, which caused fail.  Actually, [http://scm:9876|http://scm:9876/] is the local realm name used by the docker container which should not resolved by proxy.

!image-2019-12-27-13-32-20-850.png!",pull-request-available,['test'],HDDS,Bug,Major,2019-12-27 03:36:48,93
13276442,Fix low version wget cannot resolve the proxy of https," 

When run compose/ozonesecure/test.sh and compose/ozonesecure-mr/test.sh on computer which connect outer network by proxy, it fails to {{wget https://github.com}} as the image show. Because the version of wget in{{ openjdk:8u191-jdk-alpine3.9}} is too low, it cannot resolve the proxy of https.

{{openjdk:8u191-jdk-alpine3.9}} was used at [https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/dist/src/main/compose/ozonesecure/docker-image/docker-krb5/Dockerfile-krb5#L18].

!image-2019-12-26-19-27-51-010.png!",pull-request-available,['test'],HDDS,Bug,Major,2019-12-26 09:41:41,93
13276359," Remove the LongCodec in the metadata package, and redundancy addCodec for Long class",For another LongCodec is provided by default in CodecRegistry and so this duplicated class can be safely removed.,pull-request-available,"['Ozone Recon', 'SCM']",HDDS,Improvement,Minor,2019-12-25 12:55:10,69
13276345,sudo reset the environment variables about proxy,"When run the  compose/ozone-mr/hadoop27/test.sh in my machine which connect  network by proxy, it fails to execute sudo apk add --update py-pip in docker container. Because sudo will reset all environment variables including the proxy, so it fails to download  [http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz.] By sudo -E, it will execute the command with the environment of the user, thus the proxy can be valid. ",pull-request-available,['test'],HDDS,Bug,Major,2019-12-25 08:51:39,93
13276332,beyond/Containers.md translation,[Doc in official page.|https://hadoop.apache.org/ozone/docs/0.4.1-alpha/beyond/containers.html],pull-request-available,[],HDDS,Sub-task,Major,2019-12-25 07:18:10,94
13276023,Fix the wrong source link in docs,"In ozone docs site's navbar link the source to hadoop, we should fix it to ozone.",newbie pull-request-available,['documentation'],HDDS,Bug,Major,2019-12-23 10:56:23,95
13275975,Use try-with-resources or close resource in finally block,"In several files, resources are not closed in ""finally"" block.

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5rBvceSc1R0gwbF9Y1&resolved=false&severities=BLOCKER&types=BUG] 

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5rBvc0Sc1R0gwbF9ZC&resolved=false&severities=BLOCKER&types=BUG]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-c9KcVY8lQ4Zr3s&resolved=false&severities=BLOCKER&types=BUG]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-c9KcVY8lQ4Zr3r&resolved=false&severities=BLOCKER&types=BUG]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-cqKcVY8lQ4Zr3N&resolved=false&severities=BLOCKER&types=BUG]",pull-request-available sonar,[],HDDS,Bug,Major,2019-12-23 03:49:37,37
13275763,ITestOzoneContractSeek zero byte file failures,"{{ITestOzoneContractSeek}} fails at {{testReadFullyZeroByteFile}} and {{testSeekZeroByteFile}} with:

{code}
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:657)
	at java.util.ArrayList.get(ArrayList.java:433)
	at org.apache.hadoop.ozone.client.io.KeyInputStream.seek(KeyInputStream.java:241)
	at org.apache.hadoop.fs.ozone.OzoneFSInputStream.seek(OzoneFSInputStream.java:65)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65)
{code}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-12-20 17:03:13,1
13275704,Move ozonefs and tools minicluster tests to integration-test,"{{ozonefs}} and {{tools}} modules in {{hadoop-ozone}} have a mix of unit and integration tests.  This issue proposes to

# switch dependency order: let {{integration-test}} depend on these modules instead of the other way around
# move integration tests (those that use {{Mini*Cluster}}) from these modules to {{integration-test}}
# let {{unit}} check run remaining tests in these modules

This improves code coverage in CI.",pull-request-available,['test'],HDDS,Improvement,Major,2019-12-20 10:48:26,1
13275694,Read to ByteBuffer uses wrong offset,"{{OzoneFSInputStream#read(ByteBuffer)}} uses the target buffer's position for offsetting into the temporary array:

{code:title=https://github.com/apache/hadoop-ozone/blob/b834fa48afef4ee4c73577c7af564e1e97cb9d5b/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSInputStream.java#L90-L97}
  public int read(ByteBuffer buf) throws IOException {

    int bufInitPos = buf.position();
    int readLen = Math.min(buf.remaining(), inputStream.available());

    byte[] readData = new byte[readLen];
    int bytesRead = inputStream.read(readData, bufInitPos, readLen);
    buf.put(readData);
{code}

Given a buffer with capacity=10 and position=8, this results in the following:

 * {{readLen}} = 2 => {{readData.length}} = 2
 * {{bufInitPos}} = 8

So {{inputStream}} reads 2 bytes and writes it into {{readData}} starting at offset 8, which results in an {{IndexOutOfBoundsException}}.

offset should always be 0, since the temporary array is sized exactly for the length to read, and it has no extra data at the start.",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-12-20 09:47:37,1
13275594,Add ObjectID and updateID to BucketInfo to avoid replaying transactions,"This Jira has 2 objectives:
1. Add objectID and updateID to BucketInfo proto persisted to DB.
2. To ensure that bucket operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-12-19 20:55:10,43
13275589,Fix javadoc of OMVolume response classes,"Fix javadoc of OMVolumeCreateResponse and OMVolumeDeleteResponse
{code:java}
/**
 * Response for CreateBucket request.
 */
public class OMVolumeCreateResponse extends OMClientResponse{code}
This should be ""Response for CreateVolume request"".

 
{code:java}
/**
 * Response for CreateVolume request.
 */
public class OMVolumeDeleteResponse extends OMClientResponse{code}
 

This should be ""Response for DeleteVolume request"".",newbie pull-request-available,[],HDDS,Bug,Minor,2019-12-19 20:40:25,71
13275571,Fix list volume for --start parameter,The command *ozone sh vol list --start=<startVolume>* can't work with setting the startVolume.,pull-request-available,[],HDDS,Bug,Major,2019-12-19 18:30:05,31
13275448,Add bytes read statistics to Ozone FileSystem implementation,"In Hive, or in MR jobs the FileSystem counters are reported based on the statistics inside the FileSystem implementation, at the moment we do not have any read bytes statistics reported, while we have the number of bytes written and the read and write operation count.

This JIRA is to add the number of bytes read statistics and record it in the FileSystem implementation.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-12-19 09:09:21,63
13275274,NPE when stop recon while start recon failed,"java.lang.NullPointerException
        at org.apache.hadoop.ozone.recon.ReconServer.stop(ReconServer.java:101)
        at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:86)
        at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:36)
        at picocli.CommandLine.execute(CommandLine.java:1173)
        at picocli.CommandLine.access$800(CommandLine.java:141)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
        at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
        at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
        at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
        at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
        at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
        at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:44)
",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-12-18 14:24:57,69
13275187,Hadoop 3.1 acceptance test fails with apk command not found,"{{ozone-mr/hadoop31}} test is failing with:

{code}
sudo: apk: command not found
{code}

New hadoop:3.1.2 image is based on CentOS, not Alpine.",pull-request-available,['test'],HDDS,Bug,Critical,2019-12-18 08:13:53,1
13275167,Configure Goldmark renderer,"Starting with Hugo 0.60, the new Goldmark renderer is configured to skip HTML fragments.  This breaks the doc layout in a few places, eg.:

 * _Easy start_, _Recommended_ etc. headers
 * tables
 * warning {{div}}",pull-request-available,['documentation'],HDDS,Improvement,Major,2019-12-18 07:08:02,1
13275153,Better management for pipeline creation limitation,Cannot let pipeline be created without any limitation after multi-raft is enabled.,pull-request-available,[],HDDS,Sub-task,Major,2019-12-18 04:49:22,77
13275103,Handle Datanode registration and SCM Node management in Recon.,"* Datanode should use Recon address to register and heartbeat to it.
* Recon will act as a ""passive"" SCM to the Datanode. Here ""passive"" means that the Datanode will not get any commands from this Recon SCM.
* Integrating SCM Node Manager into Recon.",pull-request-available,[],HDDS,Sub-task,Major,2019-12-17 21:59:42,30
13275092,Fix Sonar warnings in SCMBlockProtocolServer and BlockOutputStreamEntryPool,https://sonarcloud.io/project/issues?fileUuids=AW5md-HeKcVY8lQ4ZrXe&id=hadoop-ozone&resolved=false,pull-request-available,['SCM'],HDDS,Bug,Major,2019-12-17 21:11:57,86
13274937,"Too much ""Failed to close the container"" message because pipeline is removed from PipelineStateMap before close container command returned","2019-12-17 10:21:14,751 ERROR org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler: Failed to close the container #240.
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:351)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:112)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:286)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:331)
        at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:66)
        at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:45)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2019-12-17 10:21:14,751 INFO org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler: Close container Event triggered for container : #241
2019-12-17 10:21:14,752 ERROR org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler: Failed to close the container #241.
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:351)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:112)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:286)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:331)
        at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:66)
        at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:45)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Major,2019-12-17 08:47:12,5
13274921,Handle pipeline creation failure in different way when it exceeds pipeline limit,"In current codes, pipeline creation failure will print out entire error trace when it exceeds pipeline number limit., which is very like false alarms. We should limit printing this type of trace and don't consider it as an error.",pull-request-available,[],HDDS,Sub-task,Minor,2019-12-17 07:46:59,77
13274817,Test OM Volume and Bucket operations are idempotent,"On OM restarts, it is possible that already applied ratis logs will be replayed. Hence, we need to ensure that all OM write operations are idempotent.
This Jira aims to add unit tests for testing that OM volume and bucket related operations are idempotent. 
",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-12-16 18:36:37,43
13274622,Unnecessary calls to isNoneEmpty and isAllEmpty,{{isNoneEmpty}} and {{isAllEmpty}} check variable number of strings.  For single string they can be replaced with {{isNotEmpty}} and {{isEmpty}}.,pull-request-available,['S3'],HDDS,Bug,Trivial,2019-12-15 09:36:07,1
13274620,Wrong number of placeholders in log message,Fix log messages where number of placeholders does not match the number of parameters provided.,pull-request-available,[],HDDS,Bug,Major,2019-12-15 09:16:56,1
13274619,OzoneFSInputStream to support StreamCapabilities,"Thanks [~bharat] for the comments in https://github.com/apache/hadoop-ozone/pull/345#pullrequestreview-332178572.
 If it is something we want to support, we can address this in a new Jira.

This Jira is for adding StreamCapabilities to let client probe for abilities corresponding the change of HDDS-2703.",pull-request-available,[],HDDS,Improvement,Major,2019-12-15 08:52:50,31
13274568,No need to try install awscli before each test,S3 acceptance test attempts to install {{awscli}} prior to each test case.  It is enough to do so before each suite.,pull-request-available,['test'],HDDS,Improvement,Minor,2019-12-14 13:44:52,1
13274552,Insight point should provide service type to get correct config,"Currently, ozone insight cmd can help us get config that being used in ozone service. However, sometimes it maybe not redirect the request for the right service.

For example, we want to query the om insight point, om.key-manager, but ConfigurationSubCommand hard coded use SCM type to get service conf. The method getHost will return the address of SCM service but actually we need to get the OM service which is the right service we want to query from.

 
{code:java}
  private void showConfig(Class clazz) {
    OzoneConfiguration conf = new OzoneConfiguration();
    conf.addResource(getHost(conf, new Component(Type.SCM)) + ""/conf"");
    ConfigGroup configGroup =
        (ConfigGroup) clazz.getAnnotation(ConfigGroup.class);
    if (configGroup == null) {
      return;
    }

    String prefix = configGroup.prefix();

    for (Method method : clazz.getMethods()) {
      if (method.isAnnotationPresent(Config.class)) {
        Config config = method.getAnnotation(Config.class);
        String key = prefix + ""."" + config.key();
        System.out.println("">>> "" + key);
        System.out.println(""       default: "" + config.defaultValue());
        System.out.println(""       current: "" + conf.get(key));
        System.out.println();
        System.out.println(config.description());
        System.out.println();
        System.out.println();

      }
    }

  }
{code}
 ",pull-request-available,['Tools'],HDDS,Bug,Minor,2019-12-14 09:34:00,31
13274546,Let GitHub Actions run acceptance check in parallel,"Currently GitHub Actions workflows are configured to run all checks in parallel, except acceptance test.  The rationale is that acceptance test takes the most time, and there is no reason to run it if a cheaper check catches some problem.

I propose to let GitHub Actions run acceptance test in parallel to address the following concerns:

# Although acceptance test is the slowest (~60 minutes), unit test also takes quite some time (~20-25 minutes).  Serializing these two checks increases the time to get feedback on PRs and commits by ~33-40%.
# For PRs and post-commit builds in forks, running all checks regardless of the result of independent checks allows authors to reduce the number of rounds they need to address any problems.
# For post-commit builds in Apache master, we expect all checks to pass.  However, checks sometime fail eg. due to transient network errors.  Skipping acceptance test due to such a problem in another check provides no benefit.",pull-request-available,['build'],HDDS,Improvement,Minor,2019-12-14 08:02:40,1
13274407,Enable multilingual Hugo features in ozone docs,"We need to reconfigure hugo to support multilingual site:

https://gohugo.io/content-management/multilingual/",pull-request-available,['documentation'],HDDS,Sub-task,Major,2019-12-13 12:42:38,6
13274404,Remove methods of internal representation from DatanodeAdminMontor interface ,"# DatanodeAdminMonitorInterface has some setter and getters but they are not part of the contract they are used by the implementation. An other implementation of the interface may require different setters. Therefore the interface should contain only the required fields.
 # DatanodeAdminMonitorInterface can be renamed to DatanodeAdminMonitor to follow the naming convention in the code (*Impl)
 # PipelineManager is unused field can be removed from DatanodeAdminMonitorImpl",pull-request-available,[],HDDS,Sub-task,Major,2019-12-13 12:21:29,6
13274237,Wrap InputStream with try-with-resource in TestOzoneFSInputStream,"Thanks [~weichiu] for the proposal.

To ensure resource is closed properly, we could wrap the fs.open() in a try.
The detail is on https://github.com/apache/hadoop-ozone/pull/345/files#r357315867
",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2019-12-12 20:47:54,31
13274224,Let ChunkManager read/write ChunkBuffer instead of ByteBuffer,Change {{ChunkManager}} read/write methods to accept/return {{ChunkBuffer}} instead of {{ByteBuffer}}.  This allows seamlessly passing multiple buffers without further interface change.,pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-12-12 18:53:36,1
13274223,Allow wrapping list of ByteBuffers with ChunkBuffer,"{{ChunkBuffer}} is a useful abstraction over {{ByteBuffer}} to hide whether it's backed by a single {{ByteBuffer}} or multiple ones ({{IncrementalChunkBuffer}}).  However, {{IncrementalChunkBuffer}} allocates its own {{ByteBuffer}} instances and only works with uniform buffer sizes.  The goal of this task is to allow wrapping an existing  {{List<ByteBuffer>}} in {{ChunkBuffer}}.",pull-request-available,[],HDDS,Improvement,Major,2019-12-12 18:51:16,1
13274221,Ozone Failure injection Service,"This will be used to track development for failure injection service. This can be used to inject various failures/delays in an ozone cluster and validate ozone in presence of these failures or extreme conditions.

Attached document provides a brief overview for this failure injection service and how it could be leveraged to validate ozone in stressful environments.",Triaged pull-request-available,"['Ozone Datanode', 'Ozone Filesystem', 'Ozone Manager', 'Ozone Recon', 'SCM']",HDDS,New Feature,Major,2019-12-12 18:47:25,44
13274131,Rename S3Token to S3AuthInfo,"Rename S3Token to something like S3AuthInfo

Item #24 in the list of ideas for new contributors to work on

https://cwiki.apache.org/confluence/display/HADOOP/Ozone+project+ideas+for+new+contributors",pull-request-available,['S3'],HDDS,Improvement,Minor,2019-12-12 12:20:07,96
13274084,Handle chunk increments in datanode,Let datanode handle incremental additions to chunks (data with non-zero offset).,pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-12-12 09:44:53,1
13274082,ReadStateMachine failure should close down the ratis pipeline ,"Currently, in case of readStateMachineFailures, the containerStateMachine is marked unhealthy which stalls all the pipeline operations. But the pipeline will get destroyed in the next snapshot execution. The idea here is to just destroy the pipeline as soon as readStateMachine call fails.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-12-12 09:42:34,86
13273948,OM does not report JVM metrics,"JVM metrics are available from Datanode and SCM, but not from OM.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-12-11 19:30:56,1
13273939,Add Filesystem functionality in MiniOzoneChaosCluster,Add filesystem read/write/delete to MiniOzoneChaosCluster.,pull-request-available,"['chaos', 'test']",HDDS,Bug,Major,2019-12-11 18:31:11,18
13273899,Maven property skipShade should not skip ozonefs compilation,"Currently, if {{-DskipShade}} is specified when running {{mvn}}, it will skip {{ozonefs}} module ({{hadoop-ozone-filesystem}} / Apache Hadoop Ozone FileSystem) compilation:
{code:xml|title=hadoop-ozone/pom.xml}
    <profile>
      <id>build-with-ozonefs</id>
      <activation>
        <property>
          <name>!skipShade</name>
        </property>
      </activation>
      <modules>
        <module>ozonefs</module>
        <module>ozonefs-lib-current</module>
        <module>ozonefs-lib-legacy</module>
      </modules>
    </profile>
{code}

As result of this, when I make code change under {{./hadoop-ozone/ozonefs/}} then run {{mvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true -DskipShade}}, the change won't be reflected in the dist. Property {{skipShade}} should not be expected to do this.

We should compile {{ozonefs}} regardless of {{-DskipShade}}.",pull-request-available,[],HDDS,Bug,Major,2019-12-11 14:50:15,12
13273881,Translate docs to Chinese,"According to [https://cwiki.apache.org/confluence/display/HADOOP/Ozone+project+ideas+for+new+contributors], I understand that Chinese docs are needed. I am interested in this, could somebody give me some advice to get started ?",Triaged,"['documentation', 'upgrade']",HDDS,Improvement,Major,2019-12-11 13:36:32,94
13273714,Add class CRLCodec - used for certificate revocation list. ,"At present, any certificate which gets revoked in SCM, is not transparently notified to DNs and local certificate copy on DNs are not being removed. 

This is the component for generating CRL for the revoked certificates. ",pull-request-available,[],HDDS,Sub-task,Major,2019-12-10 22:09:04,91
13273672,Use Ozone specific LICENCE and NOTICE in the root of the git repo,"We have three places where we need the LICENSE/NOTICE files:

 
 #  In the binary tar file (binary specific license)
 #  In the src tar file (source specific license)
 # In the root of the git repository

1 and 2 are fine now (we have dedicated file under hadoop-ozone/dist) but as we have shared the repository with main hadoop the root LICENSE/NOTICE (3) still contains the hadoop specific content.

We need to use the files from (2) for (3).",pull-request-available,[],HDDS,Bug,Blocker,2019-12-10 17:27:06,6
13273669,Source tar file is not created during the relase build,"Thanks to [~dineshchitlangia] who reported this problem.

With a release build:
{code:java}
mvn clean install -Dmaven.javadoc.skip=true -DskipTests -Psign,dist,src -Dtar -Dgpg.keyname=$CODESIGNINGKEY {code}
The source package (*the* release) is not created.

In fact it's created, but the problem with the order of clean and install:
 * clean is executed in the root project
 * install is executed in the root project (creates hadoop-ozone/dist/target/..src.tar.gx
 * .....
 * clean is executed in the hadoop-ozone/dist project *(here the src package is deleted)*
 * install is executed in the hadoop-ozone/dist project

One possible fix is to move the creation of the src package to the hadoop-ozone/dist project (but do it from the project root)",pull-request-available,[],HDDS,Bug,Blocker,2019-12-10 17:13:22,6
13273642,OzoneFSInputStream to support ByteBufferReadable,"This was found by [~cxorm] via HDDS-2443.

ByteBufferReadable could help certain application performance, such as Impala. (See HDFS-14111)

Additionally, if we support ByteBufferPositionedReadable, it would benefit HBase. (see HDFS-3246)

Finally, we should add StreamCapabilities to let client probe for these abilities. (See HDFS-11644)",pull-request-available,[],HDDS,Improvement,Major,2019-12-10 15:33:30,31
13273596,Client failed to recover from ratis AlreadyClosedException exception,"Run teragen, and it failed to enter the mapreduce stage and print the following warnning message on console endlessly. 


{noformat}

19/12/10 19:23:54 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-FBD45C9313A5->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0
19/12/10 19:23:54 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}
19/12/10 19:26:16 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-7C5A7B5CFC31->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0
19/12/10 19:26:16 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}
19/12/10 19:28:38 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-B3D8C0052C4E->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0
19/12/10 19:28:38 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}

{noformat}




",TriagePending,['Ozone Client'],HDDS,Bug,Blocker,2019-12-10 11:47:48,16
13273577,Avoid read from temporary chunk file in datanode,Currently we try reading chunk data from the temp file if chunk file does not exist. The fix was added in HDDS-2372 due to race condition between readStateMachineData and writeStateMachineData in ContainerStateMachine. After HDDS-2542 is fixed the read from the temp file can be avoided by making sure that chunk data remains in cache until the chunk file is generated.,TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2019-12-10 10:20:38,35
13273424,SCM log is flooded with block deletion txId mismatch messages,"When you run Hive queries on the cluster, but I think this is true for other MapReduce stuff as well, then the interim and temporary data is created and deleted quite often.
This leads to the flood of similar messages in the SCM log:
{code}
2019-12-07 05:00:41,112 INFO org.apache.hadoop.hdds.scm.block.SCMBlockDeletingService: Block deletion txnID mismatch in datanode e590d08a-4a4e-428a-82e8-80f7221f639e for containerID 307. Datanode delete txnID: 25145, SCM txnID: 25148
{code}

Either we need to decrease the log level of this messages, or we need to get rid of the cause of the message. In a single log file I see over 21k lines containing this message from ~37k lines of log.",TriagePending,['SCM'],HDDS,Improvement,Critical,2019-12-09 16:08:45,63
13273420,SCM is not able to start under certain conditions,"Given
- a cluster where RATIS-677 happened, and DataNodes are already failing to start properly due to the issue

When
- I restart the cluster and start to see the exceptions as described in RATIS-677
- I stop the 3 DN that has the failing pipeline
- remove the ratis metadata for the pipeline
- close the pipeline with scmcli
- restart the 3 DN

Then
- SCM is unable to come out of safe mode, the log shows the following possible reason:
{code}
2019-12-09 01:13:38,437 INFO org.apache.hadoop.hdds.scm.safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 4
{code}

If after this I restart the SCM, it fails without logging any exception, and the standard error contains the following message es the last one:
{code}
PipelineID=<id_of_pipeline_that_has_been_closed> not found
{code}

Also scmcli did not list the closed pipeline after I closed it and checked the active pipelines.",Triaged,['SCM'],HDDS,Bug,Critical,2019-12-09 15:59:15,63
13273334,HddsVolume#readVersionFile fails when reading older versions,"{{HddsVolume#layoutVersion}} is a version number, supposed to be used for handling upgrades from older versions.  Currently only one version is defined.  But should a new version be introduced, HddsVolume would fail to read older version file.  This is caused by a check in {{HddsVolumeUtil}} that only considers the latest version as valid:

{code:title=https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java#L137-L153}
  /**
   * Returns layOutVersion if it is valid. Throws an exception otherwise.
   */
  @VisibleForTesting
  public static int getLayOutVersion(Properties props, File versionFile) throws
      InconsistentStorageStateException {
    String lvStr = getProperty(props, OzoneConsts.LAYOUTVERSION, versionFile);

    int lv = Integer.parseInt(lvStr);
    if(DataNodeLayoutVersion.getLatestVersion().getVersion() != lv) {
      throw new InconsistentStorageStateException(""Invalid layOutVersion. "" +
          ""Version file has layOutVersion as "" + lv + "" and latest Datanode "" +
          ""layOutVersion is "" +
          DataNodeLayoutVersion.getLatestVersion().getVersion());
    }
    return lv;
  }
{code}

I think this should check whether the version number identifies a known {{DataNodeLayoutVersion}}.",Triaged upgrade,['Ozone Datanode'],HDDS,Improvement,Critical,2019-12-09 11:42:47,30
13273328,HddsVolume mixes ChunkLayOutVersion and DataNodeLayoutVersion,"{{HddsVolume}} [initializes {{layoutVersion}} using latest {{ChunkLayOutVersion}}|https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java#L268].  But when writing the same info to file, it [verifies {{layoutVersion}} matches the latest {{DataNodeLayoutVersion}}|https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java#L292-L293].  To satisfy the condition {{ChunkLayOutVersion}} and {{DataNodeLayoutVersion}} have to be in sync, which means only one of them is necessary.  I think the intention was to use {{DataNodeLayoutVersion}} in both cases, as {{ChunkLayOutVersion}} is for key-value container internal structure.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-12-09 11:11:58,1
13272963,Seek to file end throws EOF Exception,"Seek to File Length position throws EOF.
Seeks by default should throw EOF past FileLength, not at file length position.

Even in DFSInputStream it throws EOF above FileLength :

{code:java}
  public synchronized void seek(long targetPos) throws IOException {
    if (targetPos > getFileLength()) {
      throw new EOFException(""Cannot seek after EOF"");
    }
{code}

Due to this behaviour EOF is shown as part of tail command too.

{noformat}
ayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ./ozone fs -tail -f /file
2019-12-08 01:21:34,088 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2019-12-08 01:21:34,137 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-12-08 01:21:34,137 [main] INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
Test
tail: EOF encountered at pos: 5 for key: file

{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-12-07 20:08:23,59
13272954,Ozone Shell code cleanup,Sonar reports lots of duplication around {{AclHandler}} classes in Ozone Shell.,pull-request-available,[],HDDS,Improvement,Major,2019-12-07 16:54:33,1
13272947,Improve the command usage of audit parser tool,"I did the test for ozone-0.4, but found audit parser tool is not so friendly to use for user. I input -h option to get help usage, then only this few messages:
{noformat}
Usage: ozone auditparser [-hV] [--verbose] [-D=<String=String>]... <database>
                         [COMMAND]
Shell parser for Ozone Audit Logs
      <database>   Existing or new .db file
      --verbose    More verbose output. Show the stack trace of the errors.
  -D, --set=<String=String>

  -h, --help       Show this help message and exit.
  -V, --version    Print version information and exit.
Commands:
  load, l      Load ozone audit log files
  template, t  Execute template query
  query, q     Execute custom query
{noformat}
Although it shows me 3 options we can use, but I still not know the complete command to execute, like how to load audit to db, which available template I can use?


 Then I have to get detailed usage from tool doc then come back to execute command, it's not effective to use. It will be better to add some necessary usage (table structure, available templates, ...)info in command usage.",pull-request-available,['Tools'],HDDS,Improvement,Minor,2019-12-07 15:59:12,31
13272939,OMException NOT_A_FILE missing space in the exception message,"{code:java}
ayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ./ozone fs -fs o3fs://bucket.hive -put -d ozone /dir/ozone/file
put: NOT_A_FILE: Can not create file: dir/ozone/fileas there is already file in the given path
{code}

""as"" got attached to the name of the file.

OMFileCreateRequest L211 and L246 : Need to add space before ""as""",newbie pull-request-available,[],HDDS,Bug,Trivial,2019-12-07 13:23:19,30
13272909,Refactor container response builders to hdds-common,"{{ContainerUtils}} and {{BlockUtils}} have some helper functions to build responses to container commands.  These would be useful for client-side unit tests, but {{client}} does not depend on {{container-service}} since the interfaces and messages it needs are defined in {{common}}.  This issue proposes to move these helpers to {{common}} to avoid duplicating the functionality for tests.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-12-07 10:05:01,1
13272880,Fix sonar issues in package org.apache.hadoop.ozone.recon.api.,Fix all sonar issues in package org.apache.hadoop.ozone.recon.api.,pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-12-07 00:34:30,30
13272753,Fix Rename API in BasicOzoneFileSystem,"In the Rename API :
1.  This doesn't work if one of the path contains URI and other doesn't.

{code:java}
    if (src.equals(dst)) {
      return true;
    }
{code}

2. This check is suppose to be done only for directories, but is done for Files too, it can be moved after getting the FileStatus and checking the type. 

{code:java}
    // Cannot rename a directory to its own subdirectory
    Path dstParent = dst.getParent();
    while (dstParent != null && !src.equals(dstParent)) {
      dstParent = dstParent.getParent();
    }
    Preconditions.checkArgument(dstParent == null,
        ""Cannot rename a directory to its own subdirectory"");
{code}

3.  This too doesn't work (similar to 1.)

{code:java}
    if (srcStatus.isDirectory()) {
      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {
        LOG.trace(""Cannot rename a directory to a subdirectory of self"");
        return false;
      }
{code}

4. Rename is even success if the URI provided is of different FileSystem.
In general HDFS/Other FS shall throw IllegalArgumentException if the path doesn't belong to the same FS.",pull-request-available,[],HDDS,Bug,Major,2019-12-06 13:35:29,59
13272728,Refactor common test utilities to hadoop-hdds/common,"Expose test code from {{hadoop-hdds/common}} to other modules.  Move some ""common"" test utilities.  Example: random {{DatanodeDetails}} creation.",pull-request-available,['test'],HDDS,Improvement,Major,2019-12-06 11:45:13,1
13272659,Ratis MaxBuffer should be the same size as the segment size,"As noticed with RATIS-767, the write buffer is allocated with 32MB size whereas the raft log segment is 1MB. The rest of the buffer will not be used and the memory is wasted allocation.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-12-06 05:53:02,18
13272654,OM File create request does not check for existing directory with the same name,"Assume the following sequence of operations/requests:
Req 1. create file -> /d1/d2/d3/d4/k1 (d3 implicitly is a sub-directory inside /d1/d2)
Req 2. create file -> /d1/d2/d3 (d3 as a file inside /d1/d2)

When processing request 2, OMFileCreateRequest needs to check if 'd1/d2/d3' is the name of an existing file or an existing directory. In which case the request has to fail.

Currently for request 2, OM will check explicitly if there is a key '/d1/d2/d3' in the key table.
Also for non-recursive create requests, OM will check if parent directory /d1/d2 already exists. For this, the OM iterates the key table to check if 'd1/d2' occurs as a prefix of any key in the key table - checkKeysUnderPath()

What is missing in current behaviour?
For OM File create, the table iterator must also determine if '/d1/d2/d3' exists as a prefix for any key in the key table - not just '/d1/d2'.

This fix is required for the correctness of OzoneFS namespace. There is a potential performance impact - which is outside the scope of this jira and will be addressed separately.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-12-06 05:20:23,40
13272649,Add leak detection memory flags to MiniOzoneChaosCluster,"This jira proposes to add some of netty and native memory tracking flags.

-Dio.netty.leakDetection.level=advanced and -XX:NativeMemoryTracking=detail to help debug some of the native allocations
",pull-request-available,"['chaos', 'test']",HDDS,Bug,Major,2019-12-06 04:23:52,18
13272609,Fix updating lastAppliedIndex in OzoneManagerStateMachine,"This Jira is to fix a bug that has been caused by HDDS-2637.

Now we have applyTransactionMap where we put all entries of ratisTransactionIndex before complete.

 

And when updating lastAppliedIndex, now there is a chance that we updateLastAppliedIndex before DoubleBuffer flush has committed transactions to DB.

 

 

Let's take 1-10 are apply transaction entries which are not flushed, 11th is indexUpdate a metadata/conf entry transaction, now when notifyIndexUpdate is called with 11, we updateLastAppliedIndex to 11. (Which we should not do) This Jira is to fix this issue.",pull-request-available,[],HDDS,Bug,Major,2019-12-05 22:32:02,13
13272540,Ratis ring creation might be failed with async pipeline creation ,"The problem introduced with async pipeline creation:

 
 # Let's say the SCM got registration from three datanodes.
 # A Ratis/THREE pipeline will be created on SCM
 # With the next HB Datanode1(DN1) will receive the CreatePipeline command
 # Datanode1 will start the Ratis server which tries to get votes from DN2 and DN3
 # If DN2 has not yet received the CreatePipeline command (which has high chance with 30sec HB) it will refuse to vote to DN1
 # DN1 will request a  pipeline close from the SCM as there are no votes from DN2 and DN3
 # Pipeline is closed on SCM side, but in the mean time DN2 (finally) receives the pipeline creation command and tries to get votes, but DN1 has a newer group/pipeline id.
 # And so on

If we are lucky enough after a while all DN will receive the container creation at more or less the same time, but if not, SCM couldn't create an Open Ratis

 

Possible solutions:
 * At the very beginning datanode can trust in the peers and learn the group id (but it doesn't cover the case when one pipeline has been closed on DN1 *and* a new pipeline is created but DN2 still has the old pipeline).
 * We can use bidirectional GRPC streaming for datanode scm communication (which is a good idea anyway to make the communication faster). It makes the communication faster but the problem is still there if there is a network blip between scm and DN1

This log shows the initial problem (but in this case we were lucky enough to get the CreatePipeline at the same time):
{code}
2019-12-05 12:06:08,457 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-7E586EC20819: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:06:13,604 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-7E586EC20819: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-12-05 12:06:13,622 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-7E586EC20819: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-12-05 12:06:44,652 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-6CDAAB81725E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:09:45,682 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-8076DB6A465A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:09:50,764 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-8076DB6A465A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-12-05 12:09:50,936 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-8076DB6A465A: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
2019-12-05 12:09:55,963 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-8076DB6A465A: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2019-12-05 12:09:56,011 INFO impl.RaftServerImpl: 7d6522ce-f918-4b92-a65f-4cf668c838ee@group-8076DB6A465A: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
 elek  om  ~  projects  …  ozone-0.5.0-SNAPSHOT  compose  ozoneperf  master  2⬆  3⚑  %   docker logs ozoneperf_datanode_2 2>&1 | grep ""changes role""
2019-12-05 12:06:28,401 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-77C0C0D747E8: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:06:28,457 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-6CDAAB81725E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:06:33,460 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-77C0C0D747E8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-12-05 12:06:33,468 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-77C0C0D747E8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-12-05 12:06:33,570 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-6CDAAB81725E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-12-05 12:06:33,805 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-6CDAAB81725E: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
2019-12-05 12:06:38,835 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-6CDAAB81725E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2019-12-05 12:06:38,887 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-6CDAAB81725E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2019-12-05 12:09:53,856 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-8076DB6A465A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:09:56,002 INFO impl.RaftServerImpl: ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63@group-8076DB6A465A: changes role from  FOLLOWER to FOLLOWER at term 2 for recognizeCandidate:7d6522ce-f918-4b92-a65f-4cf668c838ee
 elek  om  ~  projects  …  ozone-0.5.0-SNAPSHOT  compose  ozoneperf  master  2⬆  3⚑  %   docker logs ozoneperf_datanode_3 2>&1 | grep ""changes role""
2019-12-05 12:06:28,220 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-22BE899E4998: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:06:28,273 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-6CDAAB81725E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-12-05 12:06:33,265 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-22BE899E4998: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-12-05 12:06:33,278 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-6CDAAB81725E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-12-05 12:06:33,288 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-22BE899E4998: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-12-05 12:06:33,804 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-6CDAAB81725E: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
2019-12-05 12:06:38,877 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-6CDAAB81725E: changes role from  FOLLOWER to FOLLOWER at term 2 for recognizeCandidate:ff9ad02f-a8b1-4641-8ddb-fec62ddd3a63
2019-12-05 12:09:53,878 INFO impl.RaftServerImpl: 1483994f-8a56-4838-b941-3c12e79b2f80@group-8076DB6A465A: changes role from      null to FOLLOWER at term 0 for startAsFollower

{code}",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Improvement,Blocker,2019-12-05 16:04:00,19
13272507,Add thread name to log pattern,"Ozone's default log4j patterns should include thread name, as it helps a bit in understanding events.",pull-request-available,[],HDDS,Improvement,Minor,2019-12-05 13:43:13,1
13272505,Acceptance test may fail despite success status,"Found this in a local acceptance test run:

{code}
Start freon testing                                                   | FAIL |
'2019-12-05 12:25:24,744 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2019-12-05 12:25:24,934 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-12-05 12:25:24,935 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2019-12-05 12:25:26,690 [main] INFO freon.RandomKeyGenerator: Number of Threads: 1
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Volumes: 5.
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Buckets per Volume: 5.
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Keys per Bucket: 5.
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Key size: 10240 bytes
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Buffer size: 4096 bytes
2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: validateWrites : false
    [ Message content over the limit has been removed. ]
....util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

***************************************************
Status: Success
Git Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf
Number of Volumes created: 5
Number of Buckets created: 25
Number of Keys added: 125
Ratis replication factor: THREE
Ratis replication type: RATIS
Average Time spent in volume creation: 00:00:00,210
Average Time spent in bucket creation: 00:00:00,213
Average Time spent in key creation: 00:00:37,506
Average Time spent in key write: 00:01:42,157
Total bytes written: 1280000
Total Execution time: 00:02:31,516
***************************************************' contains 'ERROR'
{code}

Need to check if {{Status: Success}} is true (ie. if keys were indeed successfully created), and if so, {{ERROR}} in output should make not the test failed.",pull-request-available,"['freon', 'test']",HDDS,Bug,Minor,2019-12-05 13:39:16,1
13272490,Sonar : fix issues in PipelineStateManager,"Sonar ""Code smell"" category issues:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-uCKcVY8lQ4ZsFf&open=AW5md-uCKcVY8lQ4ZsFf
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-uCKcVY8lQ4ZsFd&open=AW5md-uCKcVY8lQ4ZsFd
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-uCKcVY8lQ4ZsFg&open=AW5md-uCKcVY8lQ4ZsFg
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-uCKcVY8lQ4ZsFe&open=AW5md-uCKcVY8lQ4ZsFe
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-uCKcVY8lQ4ZsFh&open=AW5md-uCKcVY8lQ4ZsFh
",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-12-05 12:53:04,88
13272486,Have NodeManager.getNodeStatus throw NodeNotFoundException,"Currently, the SCM node manager method getNodeStatus catches any NodeNotFoundException and returns null.

{code}
  /**
   * Returns the node status of a specific node.
   *
   * @param datanodeDetails Datanode Details
   * @return NodeStatus for the node
   */
  @Override
  public NodeStatus getNodeStatus(DatanodeDetails datanodeDetails) {
    try {
      return nodeStateManager.getNodeStatus(datanodeDetails);
    } catch (NodeNotFoundException e) {
      // TODO: should we throw NodeNotFoundException?
      return null;
    }
  }
{code}

This should probably throw the exception to ensure downstream code does not need to perform a null check each time it is called.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-12-05 12:42:46,11
13272480,Sonar : fix issues in scm Pipeline,"all ""Code Smell"" category issues in Pipeline.java:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-3UKcVY8lQ4ZsOn&open=AW5md-3UKcVY8lQ4ZsOn

unnecessary invocations of toString():
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-3UKcVY8lQ4ZsOo&open=AW5md-3UKcVY8lQ4ZsOo
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-3UKcVY8lQ4ZsOp&open=AW5md-3UKcVY8lQ4ZsOp
",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-12-05 12:27:57,88
13272461,Sonar : fix issues reported in BlockManagerImpl,"Sonar ""code smell"" type issues:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-8_KcVY8lQ4ZsTd&open=AW5md-8_KcVY8lQ4ZsTd

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-8_KcVY8lQ4ZsTe&open=AW5md-8_KcVY8lQ4ZsTe

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-8_KcVY8lQ4ZsTc&open=AW5md-8_KcVY8lQ4ZsTc

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-8_KcVY8lQ4ZsTf&open=AW5md-8_KcVY8lQ4ZsTf
",pull-request-available,['SCM'],HDDS,Improvement,Critical,2019-12-05 11:10:03,88
13272398,Prometheus reports invalid metric type,"Promethues version 2.14.0

configuration set in promethues.yml, in which 10.120.110.183 is the master with OM and SCM. All others are datanodes.

{code}
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'ozone'
    metrics_path: /prom

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets:
      - ""10.120.110.183:8080""
      - ""10.120.110.183:8081""
      - ""10.120.139.122:9882""
      - ""10.120.139.111:9882""
      - ""10.120.113.172:9882""
      - ""10.121.124.44:9882""
{code}

The UP datanoe 122 is not part of the THREE factor pipeline.  The later is formed by the other 3 datanodes which are all DOWN. ",pull-request-available,[],HDDS,Bug,Major,2019-12-05 06:41:32,1
13272337,Suppress loader constraint violation message in TestOzoneFileSystemWithMocks,"{{TestOzoneFileSystemWithMocks}} throws LinkageError error when run (but test succeeds):

{code}
ERROR StatusLogger Could not reconfigure JMX
 java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/MockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
	at org.powermock.core.classloader.MockClassLoader.loadUnmockedClass(MockClassLoader.java:250)
	at org.powermock.core.classloader.MockClassLoader.loadModifiedClass(MockClassLoader.java:194)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:71)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:335)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:259)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:164)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:558)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:619)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:636)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:231)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
	at org.apache.commons.logging.LogAdapter$Log4jLog.<clinit>(LogAdapter.java:135)
	at org.apache.commons.logging.LogAdapter$Log4jAdapter.createLog(LogAdapter.java:102)
	at org.apache.commons.logging.LogAdapter.createLog(LogAdapter.java:79)
	at org.apache.commons.logging.LogFactoryService.getInstance(LogFactoryService.java:46)
	at org.apache.commons.logging.LogFactoryService.getInstance(LogFactoryService.java:41)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:136)
	at org.apache.hadoop.fs.ozone.TestOzoneFileSystemWithMocks.testFSUriWithHostPortOverrides(TestOzoneFileSystemWithMocks.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:316)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:88)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:96)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:300)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:131)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.access$100(PowerMockJUnit47RunnerDelegateImpl.java:59)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner$TestExecutorStatement.evaluate(PowerMockJUnit47RunnerDelegateImpl.java:147)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.evaluateStatement(PowerMockJUnit47RunnerDelegateImpl.java:107)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:288)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:86)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:49)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:208)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:147)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:121)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:33)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:45)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:123)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:121)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:53)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)

Process finished with exit code 0
{code}

Goal: Suppress this error message.",pull-request-available,[],HDDS,Improvement,Major,2019-12-04 22:56:59,12
13272307,Implement new Ozone Filesystem scheme ofs://,"Implement a new scheme for Ozone Filesystem where all volumes (and buckets) can be accessed from a single root.

Also known as Rooted Ozone Filesystem.",Triaged pull-request-available,['Ozone Filesystem'],HDDS,New Feature,Blocker,2019-12-04 19:54:45,12
13272272,Update gRPC and datanode protobuf version in Ozone,"This jira is in continuation of RATIS-752. With Ozone updated to latest ratis snapshot, the protobuf and grpc compiler version can be updated as well.",pull-request-available,[],HDDS,Bug,Major,2019-12-04 15:38:41,18
13272226,Use field based Config annotation instead of method based,"HDDS-2413 proposes an additional usage of the @Config annotation: to set configuration based on an existing configuration class.

But as of now we annotate the setters instead of the fields. To avoid annotation duplication (we need to read the values from the getters or the fields) I propose to switch to use field based annotations instead of setter based annotation.

I think it's more readable and additional validation (even the class level validations) can be done in a @PostConstruct method.",pull-request-available,[],HDDS,Improvement,Major,2019-12-04 13:22:19,6
13272190,Create insight point for datanode container protocol,The goal of this task is to create a new insight point for the datanode container protocol ({{HddsDispatcher}}) to be able to debug {{client<->datanode}} communication.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-12-04 11:25:41,6
13272020,KeyValueHandler#handleCreateContainer should log the exception on container creation failure,"KeyValueHandler#handleCreateContainer does not log the stack trace of the StorageContainerException cause. This leads to difficulty in debugging the reason for failure.

 
{code:java}
2019-11-04 00:40:56,719 INFO org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler: Operation: CreateContainer : Trace ID: 1f7530ed73cf2e47:74588d4de9f1d702:1f7530ed73cf2e47:0 : Message: Container creation failed. : Result: CONTAINER_INTERNAL_ERROR
2019-11-04 00:40:56,720 INFO org.apache.hadoop.ozone.container.common.impl.HddsDispatcher: Operation: WriteChunk : Trace ID: 1f7530ed73cf2e47:74588d4de9f1d702:1f7530ed73cf2e47:0 : Message: ContainerID 11 creation failed : Result: CONTAINER_INTERNAL_ERROR
{code}
 ",newbie pull-request-available,[],HDDS,Bug,Major,2019-12-03 17:50:02,31
13271888,Insight log level reset does not work,"{{ozone insight log}} command changes log level to debug or trace.  After streaming is stopped, it attempts to reset to info.  This does not seem to work, probably because the process is abruptly stopped (Ctrl-C).",pull-request-available,['Tools'],HDDS,Bug,Major,2019-12-03 08:11:20,1
13271755,Key get command creates the output file even in case of KEY_NOT_FOUND,"If a key isn't found, the key get command still creates an output file of 0 size.

{noformat}
ayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ll
total 36
drwxr-xr-x  3 ayush ayush  4096 Dec  2 22:41 ./
drwxr-xr-x 13 ayush ayush  4096 Nov 30 19:39 ../
-rwxr-xr-x  1 ayush ayush 12786 Nov 30 19:35 ozone*
ayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ./ozone sh key get ""hive/bucket/key 21"" ""file 21""
KEY_NOT_FOUND Key not found
ayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ll
total 36
drwxr-xr-x  3 ayush ayush  4096 Dec  2 22:42  ./
drwxr-xr-x 13 ayush ayush  4096 Nov 30 19:39  ../
-rw-r--r--  1 ayush ayush     0 Dec  2 22:42 'file 21'
-rwxr-xr-x  1 ayush ayush 12786 Nov 30 19:35  ozone*
{noformat}
",pull-request-available,[],HDDS,Bug,Major,2019-12-02 17:46:37,59
13271626,Prefer execute() over submit() if the returned Future is ignored,"{{Future submit(Runnable)}} and {{void execute(Runnable)}} in {{ExecutorService}} have the same result.  If the returned {{Future}} is ignored, {{execute}} can be used instead of {{submit}} to avoid creating some objects.",pull-request-available,[],HDDS,Improvement,Minor,2019-12-02 07:37:05,1
13271520,Use pre-compiled Pattern in NetUtils#normalize,"{{NetUtils#normalize}} uses {{String#replaceAll}}, which creates a {{Pattern}} for each call.  It could be replaced with a pre-compiled {{Pattern}}.",pull-request-available,[],HDDS,Improvement,Minor,2019-11-30 19:13:11,1
13271512,Conditionally enable profiling at the kernel level,"Extend {{entrypoint.sh}} to set the kernel parameters required for profiling if the {{ASYNC_PROFILER_HOME}} environment variable (used by {{ProfileServlet}}) is set.

Ref:

{code:title=https://cwiki.apache.org/confluence/display/HADOOP/Java+Profiling+of+Ozone}
echo 1 > /proc/sys/kernel/perf_event_paranoid
echo 0 > /proc/sys/kernel/kptr_restrict
{code}",pull-request-available,['docker'],HDDS,Improvement,Minor,2019-11-30 17:04:27,1
13271503,Improve executor memory usage in new Freon tests,"New Freon tests (descendants of {{BaseFreonGenerator}}) suffer from a similar memory issue due to concurrency handling as HDDS-1785.

Steps to reproduce:

{code}
export HADOOP_OPTS='-Xmx256M -XX:+HeapDumpOnOutOfMemoryError'
ozone freon omkg -F ONE -n 33554432 -t 10 -p omkg
{code}

Freon attempts to submit 32M tasks to the executor, requiring at least 1.5GB memory.",pull-request-available,['freon'],HDDS,Improvement,Major,2019-11-30 14:33:39,1
13271435,BlockOutputStreamEntryPool.java:allocateNewBlock spams the log with ExcludeList information,"BlockOutputStreamEntryPool.java:allocateNewBlock spams the log with ExcludeList information with the following log lines

{code}
2019-11-29 20:22:51,590 [pool-244-thread-9] INFO  io.BlockOutputStreamEntryPool (BlockOutputStreamEntryPool.java:allocateNewBlock(257)) - Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = []}
{code}",MiniOzoneChaosCluster newbie pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-11-29 15:08:07,86
13271355,Make startId parameter non-mandatory while listing containers through shell command,"ozone scmcli container list -s=2

Here the startId  ""--start | -s "" is a mandatory parameter .

Need to make it as non-mandatory parameter as the default value for startId is already defined.",pull-request-available,[],HDDS,Bug,Minor,2019-11-29 09:56:39,20
13271345,Fix createPipeline and make createPipeline CLI message based.,Caused by: java.lang.IllegalArgumentException: Can not set org.apache.hadoop.hdds.scm.cli.SCMCLI field org.apache.hadoop.hdds.scm.cli.pipeline.CreatePipelineSubcommand.parent to org.apache.hadoop.hdds.scm.cli.pipeline.PipelineCommandsCaused by: java.lang.IllegalArgumentException: Can not set org.apache.hadoop.hdds.scm.cli.SCMCLI field org.apache.hadoop.hdds.scm.cli.pipeline.CreatePipelineSubcommand.parent to org.apache.hadoop.hdds.scm.cli.pipeline.PipelineCommands at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:167) at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:171) at sun.reflect.UnsafeObjectFieldAccessorImpl.set(UnsafeObjectFieldAccessorImpl.java:81) at java.lang.reflect.Field.set(Field.java:764) at picocli.CommandLine$Model$CommandReflection.initParentCommand(CommandLine.java:6476),pull-request-available,[],HDDS,Sub-task,Minor,2019-11-29 09:00:21,77
13271333,TestOzoneManagerDoubleBufferWithOMResponse,"The test is flaky:

 

Example run: [https://github.com/apache/hadoop-ozone/runs/325281277]

 

Failure:
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
-------------------------------------------------------------------------------
Tests run: 3, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 5.31 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
testDoubleBufferWithMixOfTransactionsParallel(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 0.282 s  <<< FAILURE!
java.lang.AssertionError: expected:<32> but was:<29>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.junit.Assert.assertEquals(Assert.java:542)
        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBufferWithMixOfTransactionsParallel(TestOzoneManagerDoubleBufferWithOMResponse.java:247)
 {code}",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Blocker,2019-11-29 08:04:04,13
13271258,Start acceptance tests only if at least one THREE pipeline is available,"After HDDS-2034 (or even before?) pipeline creation (or the status transition from ALLOCATE to OPEN) requires at least one pipeline report from all of the datanodes. Which means that the cluster might not be usable even if it's out from the safe mode AND there are at least three datanodes.

It makes all the acceptance tests unstable.

For example in [this|https://github.com/apache/hadoop-ozone/pull/263/checks?check_run_id=324489319] run.
{code:java}
scm_1         | 2019-11-28 11:22:54,401 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode 548f146f-2166-440a-b9f1-83086591ae26
scm_1         | 2019-11-28 11:22:54,402 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c
scm_1         | 2019-11-28 11:22:54,404 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode 47dbb8e4-bbde-4164-a798-e47e8c696fb5
scm_1         | 2019-11-28 11:22:54,405 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 8dc4aeb6-5ae2-46a0-948d-287c97dd81fb, Nodes: 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}47dbb8e4-bbde-4164-a798-e47e8c696fb5{ip: 172.24.0.2, host: ozoneperf_datanode_2.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED]
scm_1         | 2019-11-28 11:22:56,975 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:22:58,018 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:23:01,871 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:23:02,817 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}
scm_1         | 2019-11-28 11:23:02,847 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null} {code}
As you can see the pipeline is created but the the cluster is not usable as it's not yet reporter back by datanode_2:
{code:java}
scm_1         | 2019-11-28 11:23:13,879 WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Retrying get pipelines c
all once.
scm_1         | org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 0 nodes.{code}
 The quick fix is to configure all the compose clusters to wait until one pipeline is available. This can be done by adjusting the number of the required datanodes:
{code:java}
// We only care about THREE replica pipeline
int minHealthyPipelines = minDatanodes /
    HddsProtos.ReplicationFactor.THREE_VALUE; {code}
 ",pull-request-available,[],HDDS,Bug,Blocker,2019-11-28 16:01:00,6
13271253,Refactor MiniOzoneChaosCluster to a different package to add filesystem tests,Refactor MiniOzoneChaosCluster  to fault-injection-tests. Also add a dependency to hadoop-ozone-filesystem to add filesystem tests later.,MiniOzoneChaosCluster pull-request-available,['test'],HDDS,Bug,Major,2019-11-28 15:47:06,18
13271178,Add leaderID information in pipeline list subcommand,"Need to add leaderID information in listPipeline subcommand.

i.e,

ozone scmcli pipeline list

 ",pull-request-available,['SCM Client'],HDDS,Bug,Major,2019-11-28 10:29:29,20
13271173,TestTableCacheImpl is flaky,"Run(master): [https://github.com/apache/hadoop-ozone/runs/324342299]

 
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl
-------------------------------------------------------------------------------
Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.955 s <<< FAILURE! - in org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl
testPartialTableCacheWithOverrideAndDelete[0](org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl)  Time elapsed: 0.039 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<6>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl.testPartialTableCacheWithOverrideAndDelete(TestTableCacheImpl.java:308)

 {code}
*How to reproduce it locally?*

Replace the last tableCache.evict call of testPartialTableCacheWithOverrideAndDelete to System.out.println(tableCache.size()).

You will see that the cache size is 2 even before the cleanup therefore the next GeneriTestUtils.waitFor is useless (it doesn't guarantee that the cleanup is finished).

*Fix:*

I propose to call the cleanup sync with using the Impl class instead of the interface. It simplifies the test but still validates the behavior.",pull-request-available,[],HDDS,Bug,Major,2019-11-28 10:13:36,6
13271103,Handle LeaderNot ready exception in OzoneManager StateMachine and upgrade ratis to latest version.,This Jira is to handle LeaderNotReadyException in OM and also update to latest ratis version.,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-27 22:05:55,13
13271030,Refresh pipeline information in OzoneManager lookupFile call,"{{lookupFile}} call in OzoneManager doesn't refresh the pipeline information. As a result of this, wrong pipeline information is returned to the client and the client fails eventually.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-27 15:32:23,19
13271012,Typo in hdds.scm.safemode.healthy.pipelie.pct,"Typo in hdds.scm.safemode.healthy.pipelie.pct as in [https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsConfigKeys.java#L96].

 

Should've been hdds.scm.safemode.healthy.pipeline.pct. So as in ozone-default.",pull-request-available,[],HDDS,Improvement,Minor,2019-11-27 14:18:12,77
13270983,Let findbugs.sh skip frontend plugin for Recon,Findbugs/Spotbugs only checks Java code.  We can skip frontend plugin execution for Recon to save ~2 minutes.  Makes a difference mostly when running it locally.,pull-request-available,"['build', 'test']",HDDS,Improvement,Minor,2019-11-27 12:59:08,1
13270961,Fix TestContainerPersistence#testDeleteChunk,"{{TestContainerPersistence#testDeleteChunk}} is failing due to unexpected exception message.  This is caused by mix of two commits:

 * https://github.com/apache/hadoop-ozone/commit/fe7fccf2b changed actual message
 * https://github.com/apache/hadoop-ozone/commit/4a9174500 moved the test case from integration to unit

Each of these was tested without the other.",pull-request-available,['test'],HDDS,Bug,Minor,2019-11-27 11:23:09,1
13270902,NullPointerException in S3g,See attached log file,pull-request-available,['S3'],HDDS,Bug,Critical,2019-11-27 07:43:34,13
13270898,Ozone CLI: CreationTime/modifyTime of volume/bucket/key info are not formatted,"There is a regression from HDDS-738, where the time stamp formatting was lost during the json output. (Detail can be found from the removed methods from OzoneClientUtils#asVolumeInfo, asBucketInfo, asKeyInfo). As a result, only raw time is output in the json output for volume/bucket/key info. 
{code:java}
bash-4.2$ ozone sh vol create /vol1
obash-4.2$ ozone sh vol info /vol1
{
 ""metadata"" : { },
 ""name"" : ""vol1"",
 ""admin"" : ""hadoop"",
 ""owner"" : ""hadoop"",
 ""creationTime"" : 1574836577298,
 ""acls"" : [ {
 ""type"" : ""USER"",
 ""name"" : ""hadoop"",
 ""aclScope"" : ""ACCESS"",
 ""aclList"" : [ ""ALL"" ]
 }, {
 ""type"" : ""GROUP"",
 ""name"" : ""users"",
 ""aclScope"" : ""ACCESS"",
 ""aclList"" : [ ""ALL"" ]
 } ],
 ""quota"" : 1152921504606846976
}{code}",pull-request-available,[],HDDS,Bug,Major,2019-11-27 06:54:52,28
13270874,Make AuditMessage parameters strongly typed,Improve type safety in {{AuditMessage$Builder}} for methods {{forOperation}} and {{withResult}} by using existing {{interface AuditAction}} and {{enum AuditEventStatus}} respectively instead of Strings.,pull-request-available,[],HDDS,Improvement,Minor,2019-11-27 03:45:55,1
13270320,Skip sonar check in forks,"_unit_ step of Github Actions-based CI checks is failing for commits pushed to forks due to lack of {{SONARCLOUD_TOKEN}}.

Background: HDDS-2587 added Sonar check in post-commit workflow, publishing results to SonarCloud.  It does not work in forks, as it requires SonarCloud token.  This causes _unit_ step to fail completely.  Example: https://github.com/bharatviswa504/hadoop-ozone/runs/316829850",pull-request-available,['build'],HDDS,Bug,Major,2019-11-24 17:26:07,1
13270317,Avoid hostname lookup for invalid local IP addresses,"{{OzoneSecurityUtil#getValidInetsForCurrentHost}} performs hostname lookup for all local network interfaces, even for invalid addresses.  This significantly slows down some secure tests ({{TestHddsSecureDatanodeInit}}, {{TestSecureOzoneCluster}}) when run on a machine with special IPv6 network interfaces due to timeout reaching IPv6 DNS servers.

This issue proposes to disable the lookup for invalid addresses.",pull-request-available,[],HDDS,Improvement,Minor,2019-11-24 15:51:13,1
13270289,Expose SCMDatanodeProtocolServer RPC endpoint through Recon.,SCM should expose the 'SCMDatanodeProtocol' RPC protocol server as an endpoint. This is the first step in sending DN reports to Recon.,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-11-24 05:51:18,30
13270185,Add config parameter for setting limit on total bytes of pending requests in Ratis,RATIS-714 introduced a config setting for limiting the max number of bytes of pending requests. This Jira aims to add a config in Ozone to set the same in DN Ratis.,pull-request-available,[],HDDS,Bug,Major,2019-11-22 22:00:16,43
13270180,Enable OM HA acceptance tests,OM HA robot tests were disable in HDDS-2533 as they were failing intermittently. HDDS-2454 fixes some issues in the HA tests. Creating this Jira so as to run re-enable the HA acceptance tests.,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-22 20:32:41,43
13270163,Fix listMultipartupload API,"This Jira is to fix listMultiparts API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listParts of a MPU key, it should use both in-memory cache and rocksdb mpu table to list parts of a mpu key.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-11-22 19:20:37,13
13270019,Fix Sonar issues in ReconTaskControllerImpl,https://sonarcloud.io/project/issues?directories=hadoop-ozone%2Frecon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Frecon%2Ftasks&id=hadoop-ozone&open=AW5md-jJKcVY8lQ4Zr9D&resolved=false,pull-request-available sonar,['Ozone Recon'],HDDS,Bug,Major,2019-11-22 05:47:40,30
13270017,Sonar : reduce Cognitive Complexity of readVolume in ContainerReader,"Sonar reports CC value of readVolume in ContainerReader is 31:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9fKcVY8lQ4ZsUU&open=AW5md-9fKcVY8lQ4ZsUU",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 05:09:40,88
13270016,Sonar : reduce Cognitive Complexity of runIteration in ContainerDataScanner,"CC value of runIteration is reported as 19:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9kKcVY8lQ4ZsUY&open=AW5md-9kKcVY8lQ4ZsUY

minor, duplicate string literal issue in same file:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9kKcVY8lQ4ZsUW&open=AW5md-9kKcVY8lQ4ZsUW
",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 05:07:23,88
13270014,Sonar : reduce Cognitive Complexity of isDeletionAllowed in BlockDeletingService,"CC of isDeletionAllowed is reported as 21 :

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-8zKcVY8lQ4ZsTQ&open=AW5md-8zKcVY8lQ4ZsTQ
",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 05:03:21,88
13270012,Sonar : reduce Cognitive Complexity of scanData in KeyValueContainerCheck,"CC of scanData method is reported as 31 :
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-82KcVY8lQ4ZsTS&open=AW5md-82KcVY8lQ4ZsTS",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 04:59:49,88
13270008,Sonar : reduce Cognitive Complexity of applyTransaction in ContainerStateMachine,"Sonar reports CC of applyTransaction is 22:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-65KcVY8lQ4ZsRU&open=AW5md-65KcVY8lQ4ZsRU

minor(Sonar ""critical"") duplicate string literal issues:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-65KcVY8lQ4ZsRM&open=AW5md-65KcVY8lQ4ZsRM
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-65KcVY8lQ4ZsRN&open=AW5md-65KcVY8lQ4ZsRN
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-65KcVY8lQ4ZsRO&open=AW5md-65KcVY8lQ4ZsRO
",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 04:50:25,88
13270007,Sonar : reduce Cognitive Complexity of deleteBlocksCommandHandler,"Sonar reports CC of deleteKeyValueContainerBlocks is 25:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-7XKcVY8lQ4ZsRr&open=AW5md-7XKcVY8lQ4ZsRr",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 04:47:56,88
13270006,Sonar : Reduce Cognitive Complexity of dispatchRequest in HddsDispatcher,"Sonar reports CC of dispatchRequest is 49, reduce to 15.

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6ZKcVY8lQ4ZsQs&open=AW5md-6ZKcVY8lQ4ZsQs

minor duplicate string literal issue in same source file.
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6ZKcVY8lQ4ZsQo&open=AW5md-6ZKcVY8lQ4ZsQo",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 04:43:49,88
13270005,Sonar : resolve issues reported in ContainerSet,"Sonar issues:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQf&open=AW5md-6RKcVY8lQ4ZsQf

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQg&open=AW5md-6RKcVY8lQ4ZsQg

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQh&open=AW5md-6RKcVY8lQ4ZsQh

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQi&open=AW5md-6RKcVY8lQ4ZsQi

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQj&open=AW5md-6RKcVY8lQ4ZsQj

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQk&open=AW5md-6RKcVY8lQ4ZsQk

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6RKcVY8lQ4ZsQl&open=AW5md-6RKcVY8lQ4ZsQl

",sonar,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-22 04:36:59,88
13269884,Move plain unit tests out of integration tests,"Some test classes in {{integration-test}} are actually unit tests: they do not start a mini cluster, nor even multiple components.  These can be moved to the subprojects they belong to (eg. {{container-service}}.  The benefit is that it will be easier to spot if they are broken, since integration tests are executed less frequently.",pull-request-available,['test'],HDDS,Improvement,Major,2019-11-21 16:01:41,1
13269847,Provide command to wait until SCM is out from the safe-mode,"The safe mode can be checked with ""ozone scmcli safemode status"". But for acceptance tests there is no easy way to check if the cluster is ready to execute the tests (See HDDS-2606 for example).

One easy solution is to create a polling version from ""safemode status"".

""safemode wait --timeout ..."" can be blocked until the scm is out from the safe mode.

Wit proper safe mode rules (min datanodes + min pipline numbers) it can help us to check if the acceptance tests are ready to test.

Same command can be used in k8s as well to test if the cluster is ready to start the freon commands...",pull-request-available,['Tools'],HDDS,Improvement,Major,2019-11-21 13:31:37,6
13269840,DeadNodeHandler should not remove replica for a dead maintenance node,"Normally, when a node goes dead, the DeadNodeHandler removes all the containers and replica associated with the node from the ContainerManager.

If a node is IN_MAINTENANCE and goes dead, then we do not want to remove its replica. They should remain present in the system to prevent the container being marked as under-replicated.

We also need to consider the case where the node is dead, and then maintenance expires automatically. In that case, the replica associated with the node must be removed and the affected containers will become under-replicated.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-11-21 13:11:55,11
13269798,Acceptance tests are flaky due to async pipeline creation,"Some of the acceptance tests are failing because the first Ratis.THREE pipeline is not created on time:

For example in a HA proxy acceptance test the first block allocation is tarted before moving the Ratis.THREE pipeline to OPEN state.
{code:java}
scm_1       | 2019-11-20 14:45:38,359 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 4d27f405-d257-4b1b-a7b3-51bbd57356db, Nodes: 2c010ef7-8c0e-45e3-b230-326bf759773b{ip: 172.25.0.7, host: ozones3-haproxy_datanode_2.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] moved to OPEN state
scm_1       | 2019-11-20 14:45:46,944 WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Retrying get pipelines call once.
scm_1       | org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 0 nodes.
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:153)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:58)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:155)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:198)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:187)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:159)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:117)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:98)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2019-11-20 14:45:46,944 INFO block.BlockManagerImpl: Could not find available pipeline of type:RATIS and factor:THREE even after retrying
scm_1       | 2019-11-20 14:45:46,945 ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2019-11-20 14:45:49,274 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: e208588c-9a16-4519-89dc-7bad5bae4331, Nodes: 1da10d32-12be-4328-bc0a-f3d8de21b056{ip: 172.25.0.8, host: ozones3-haproxy_datanode_3.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}d9edb776-ee02-48a1-8c73-40f33bc0d128{ip: 172.25.0.5, host: ozones3-haproxy_datanode_1.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}2c010ef7-8c0e-45e3-b230-326bf759773b{ip: 172.25.0.7, host: ozones3-haproxy_datanode_2.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] moved to OPEN state {code}",TriagePending,['SCM'],HDDS,Bug,Major,2019-11-21 10:44:29,6
13269794,Use LongSupplier to avoid boxing,"Freon's {{ProgressBar}} uses {{Supplier<Long>}}, which could be replaced with {{LongSupplier}} to avoid boxing.",pull-request-available,['freon'],HDDS,Improvement,Trivial,2019-11-21 10:25:58,1
13269787,Avoid unnecessary boxing in XceiverClientReply,{{XceiverClientReply#logIndex}} is unnecessarily boxed/unboxed.,pull-request-available,[],HDDS,Improvement,Minor,2019-11-21 10:01:55,1
13269785,Add a property to enable/disable ONE replica pipeline auto creation in SCMPipelineManager,ONE RATIS replica not favored in production cluster. Add a property to disable automatically create ONE RATIS pipeline in SCMPipelineManager,pull-request-available,[],HDDS,Improvement,Major,2019-11-21 10:00:42,77
13269766,Fix Broken Link In Website,"In the FAQ Page :
https://hadoop.apache.org/ozone/faq/

The last line points to how to contribute. That seems broken :

As of now it is :
https://wiki.apache.org/hadoop/HowToContribute

it should be :
https://cwiki.apache.org/confluence/display/hadoop/How+To+Contribute",pull-request-available,[],HDDS,Bug,Minor,2019-11-21 08:20:52,97
13269732,Move chaos test to org.apache.hadoop.ozone.chaos package,This is a simple refactoring change where all the chaos test are moved to  org.apache.hadoop.ozone.chaos package,pull-request-available,['test'],HDDS,Bug,Major,2019-11-21 04:39:56,30
13269707,"Remove unused private field ""LOG""",https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_APKcVY8lQ4ZsWS&open=AW5md_APKcVY8lQ4ZsWS,pull-request-available,[],HDDS,Improvement,Major,2019-11-21 00:49:45,91
13269706,Remove toString() as log calls it implicitly,"No need to call ""toString()"" method as formatting and string conversion is done by the Formatter.

 

Related to [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AVKcVY8lQ4ZsWb&open=AW5md_AVKcVY8lQ4ZsWb]",pull-request-available,[],HDDS,Improvement,Major,2019-11-21 00:45:42,91
13269699,S3 RangeReads failing with NumberFormatException," 
{code:java}
2019-11-20 15:32:04,684 WARN org.eclipse.jetty.servlet.ServletHandler:
javax.servlet.ServletException: java.lang.NumberFormatException: For input string: ""3977248768""
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)
        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1780)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1609)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.eclipse.jetty.server.Server.handle(Server.java:539)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
{code}
 ",pull-request-available,[],HDDS,Bug,Major,2019-11-21 00:05:51,13
13269684,DatanodeAdminMonitor should track under replicated containers and complete the admin workflow accordingly,"HDDS-2459 allowed the replicationManager to take care of containers which are under-replicated due to decommission and maintenance.

Its also exposed a new API to return a ContainerReplicaCount object:

{code}
getContainerReplicaCount(Container container)
{code}

This object will allow the DatanodeAdminMonitor to check if each container is ""sufficiently replicated"" before decommission or maintenance can complete and hence can be used to track the progress of each node as it progresses though the admin workflow.

We should track the containers on each node in administration and ensure that each is closed and sufficiently replicated before allowing decommission to complete.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-11-20 22:25:33,11
13269683,Add Datanode command to allow the datanode to persist its admin state ,"When the operational state of a datanode changes, an async command should be triggered to persist the new state on the datanodes. For maintenance mode, the datanode should also store the maintenance end time. The datanode will then report the new state (and optional maintenance end time) back via its heartbeat.

The purpose of the DN persisting this information and heartbeating it back to SCM is to allow the operation state to be recovered after a SCM reboot, as SCM does not persist any of this information. It also allows ""Recon"" to learn the datanode states.

If SCM is restarted, then it will forget all knowledge of the datanodes. When they register, their operational state will be reported and SCM can set it correctly.

Outside of registration (ie during normal heartbeats), the SCM state is the source of truth for the operational state and if the DN heartbeat reports a state that is not the same as SCM, SCM should issue another command to the datanode to set its state to the SCM value. There is a chance the state miss match is due to an unprocessed command triggered by the SCM state change, but the worst case is an extra command sent to the datanode. This is a very lightweight command, so that is not an issue.

One open question is whether to persist intermediate states on the DN. Ie for decommissioning, the DN will first persist ""Decommissioning"" and then transition to ""Decommissioned"" when SCM is satisfied all containers are replicated. It would be possible to persist both these states in turn on the datanode quite easily in turn. Or, we set the end state (Decommissioned) on the datanode and allow SCM to get the node to that state. For the latter, if SCM is restarted, then the DN will report ""Decommissioned"" on registration, but SCM will set its internal state to Decommissioning and then ensure all containers are replicated before transitioning the node to Decommissioned. This seems like a safer approach, but there are advantages of tracking the intermediate states on the DNs too.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2019-11-20 22:18:43,11
13269665,No tailMap needed for startIndex 0 in ContainerSet#listContainer,"{{ContainerSet#listContainer}} has this code:

{code:title=https://github.com/apache/hadoop-ozone/blob/3c334f6a7b344e0e5f52fec95071c369286cfdcb/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerSet.java#L198}
map = containerMap.tailMap(containerMap.firstKey(), true);
{code}

This is equivalent to:

{code}
map = containerMap;
{code}

since {{tailMap}} is a sub-map with all keys larger than or equal to ({{inclusive=true}}) {{firstKey}}, which is the lowest key in the map.  So it is a sub-map with all keys, ie. the whole map.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-20 20:05:50,1
13269660,Integration tests for Recon with Ozone Manager.,"Currently, Recon has only unit tests. We need to add the following integration tests to make sure there are no regressions or contract breakage with Ozone Manager. 

The first step would be to add Recon as a new component to Mini Ozone cluster.

* *Test 1* - *Verify Recon can get full snapshot and subsequent delta updates from Ozone Manager on startup.*
  > Start up a Mini Ozone cluster (with Recon) with a few keys in OM.
  > Verify Recon gets full DB snapshot from OM.
  > Add 100 keys to OM
  > Verify Recon picks up the new keys using the delta updates mechanism.
  > Verify OM DB seq number == Recon's OM DB snapshot's seq number

* *Test 2* - *Verify Recon restart does not cause issues with the OM DB syncing.*
   > Startup Mini Ozone cluster (with Recon).
   > Add 100 keys to OM
   > Verify Recon picks up the new keys.
   > Stop Recon Server
   > Add 5 keys to OM.
   > Start Recon Server
   > Verify that Recon Server does not request full snapshot from OM (since only a small 
       number of keys have been added, and hence Recon should be able to get the 
       updates alone)
   > Verify OM DB seq number == Recon's OM DB snapshot's seq number

*Note* : This exercise might expose a few bugs in Recon-OM integration which is perfectly normal and is the exact reason why we want these tests to be written. Please file JIRAs for any major issues encountered and link them here. Minor issues can hopefully be fixed as part of this effort. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-11-20 19:37:05,98
13269613,Consolidate compose environments,"There are a few slightly different sample docker compose environments: ozone, ozoneperf, ozones3, ozone-recon. This issue proposes to merge these 4 by minor additions to ozoneperf:

 # add {{recon}} service from {{ozone-recon}}
 # run GDPR and S3 tests
 # expose datanode web port (eg. for profiling)

Plus: also run ozone-shell test (from basic suite), which is currently run only in ozonesecure

I also propose to rename {{ozoneperf}} to {{ozone}} for simplicity.

Consolidating these 4 environments would slightly reduce both code duplication and the time needed for acceptance tests.

CC [~elek]",pull-request-available,['docker'],HDDS,Improvement,Major,2019-11-20 15:24:52,1
13269563,Enable sonarcloud measurement as part of CI builds,"As sonarcloud has been started to use, it would be great to upload measurement data from github actions steps...",pull-request-available,[],HDDS,Improvement,Major,2019-11-20 10:28:38,6
13269512,Use Java Configs for OM HA,"This Jira is created based on the comments from [~aengineer] during HDDS-2536 review.

Can we please use the Java Configs instead of this old-style config to add a config?

 

This Jira it to make all HA OM config to the new style (Java config based approach)",newbie,['OM HA'],HDDS,Bug,Major,2019-11-20 06:07:27,99
13269503,Ensure resources are closed in Get/PutKeyHandler,"Use try-with-resources or close this ""FileOutputStream"" in a ""finally"" clause.

GetKeyHandler: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6HHKTfdBVcJdcVFsvC&open=AW6HHKTfdBVcJdcVFsvC]

 

Use try-with-resources or close this ""OzoneOutputStream"" in a ""finally"" clause.

PutKeyHandler: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6HHKRodBVcJdcVFsvB&open=AW6HHKRodBVcJdcVFsvB]

 ",newbie pull-request-available sonar,[],HDDS,Bug,Major,2019-11-20 04:48:26,1
13269501,Ozone client should refresh pipeline info if reads from all Datanodes fail.,"Currently, if the client reads from all Datanodes in the pipleine fail, the read fails altogether. There may be a case when the container is moved to a new pipeline by the time client reads. In this case, the client should request for a refresh pipeline from OM, and read it again if the new pipeline returned from OM is different. 

This behavior is consistent with that of HDFS.
cc [~msingh] / [~shashikant] / [~hanishakoneru]",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-11-20 04:41:57,30
13269499,Handle InterruptedException in RandomKeyGenerator," RandomKeyGenerator: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-cqKcVY8lQ4Zr3f&open=AW5md-cqKcVY8lQ4Zr3f]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:32:12,81
13269498,Handle InterruptedException in OzoneManagerProtocolServerSideTranslatorPB,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-Z7KcVY8lQ4Zr1l&open=AW5md-Z7KcVY8lQ4Zr1l,newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:31:20,30
13269497,Handle InterruptedException in ratis related files,"OzoneManagerDoubleBuffer: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-VxKcVY8lQ4Zrtu&open=AW5md-VxKcVY8lQ4Zrtu]

OzoneManagerRatisClient: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-VsKcVY8lQ4Zrtf&open=AW5md-VsKcVY8lQ4Zrtf]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:29:39,100
13269496,Handle InterruptedException in LogSubcommand,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-mpKcVY8lQ4ZsAH&open=AW5md-mpKcVY8lQ4ZsAH,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:27:48,100
13269495,Handle InterruptedException in OzoneDelegationTokenSecretManager,"Fix 2 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-gpKcVY8lQ4Zr64&open=AW5md-gpKcVY8lQ4Zr64]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-gpKcVY8lQ4Zr67&open=AW5md-gpKcVY8lQ4Zr67]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:26:31,81
13269494,Handle InterruptedException in KeyOutputStream,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-m5KcVY8lQ4ZsAc&open=AW5md-m5KcVY8lQ4ZsAc,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:25:22,30
13269493,Handle InterruptedException in SCMSecurityProtocolServer,"Fix 2 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-tDKcVY8lQ4ZsEg&open=AW5md-tDKcVY8lQ4ZsEg]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-tDKcVY8lQ4ZsEi&open=AW5md-tDKcVY8lQ4ZsEi]

 

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:24:26,81
13269492,Handle InterruptedException in SCMPipelineManager,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6BMuREm2E_7tGaNiTh&open=AW6BMuREm2E_7tGaNiTh]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:23:32,30
13269490,Handle InterruptedException in ProfileServlet,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-x8KcVY8lQ4ZsIJ&open=AW5md-x8KcVY8lQ4ZsIJ,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:22:32,30
13269489,Handle InterruptedException in LogStreamServlet,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-yJKcVY8lQ4ZsIf&open=AW5md-yJKcVY8lQ4ZsIf,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:21:49,100
13269488,Handle InterruptedException in OzoneContainer,"https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9sKcVY8lQ4ZsUh&open=AW5md-9sKcVY8lQ4ZsUh

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:21:04,100
13269487,Handle InterruptedException in ContainerMetadataScanner,"Fix 2 instances:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9vKcVY8lQ4ZsUj&open=AW5md-9vKcVY8lQ4ZsUj

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9vKcVY8lQ4ZsUk&open=AW5md-9vKcVY8lQ4ZsUk]

 

 

 

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:19:56,100
13269486,Handle InterruptedException in ContainerDataScanner,"Fix 2 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9kKcVY8lQ4ZsUZ&open=AW5md-9kKcVY8lQ4ZsUZ]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9kKcVY8lQ4ZsUb&open=AW5md-9kKcVY8lQ4ZsUb]

 

 

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:18:41,81
13269485,Handle InterruptedException in VolumeSet,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-7yKcVY8lQ4ZsR9&open=AW5md-7yKcVY8lQ4ZsR9,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:17:04,31
13269484,Handle InterruptedException in ContainerStateMachine,"https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-65KcVY8lQ4ZsRV&open=AW5md-65KcVY8lQ4ZsRV

 

 ",Triaged newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:15:38,81
13269483,Handle InterruptedException in RunningDatanodeState,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6pKcVY8lQ4ZsRC&open=AW5md-6pKcVY8lQ4ZsRC]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:14:58,81
13269481,Handle InterruptedException in DatanodeStateMachine,"Fix 2 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-7fKcVY8lQ4ZsRv&open=AW5md-7fKcVY8lQ4ZsRv]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-7fKcVY8lQ4ZsRx&open=AW5md-7fKcVY8lQ4ZsRx]

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:13:24,81
13269480,Handle InterruptedException in LeaseManager,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zSKcVY8lQ4ZsJj&open=AW5md-zSKcVY8lQ4ZsJj,newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:12:23,94
13269479,Handle InterruptedException in Scheduler,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-0nKcVY8lQ4ZsLH&open=AW5md-0nKcVY8lQ4ZsLH]

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:11:28,37
13269478,Handle InterruptedException in BackgroundService,"Fix 2 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-02KcVY8lQ4ZsLU&open=AW5md-02KcVY8lQ4ZsLU]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-02KcVY8lQ4ZsLV&open=AW5md-02KcVY8lQ4ZsLV]

 

 

 

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:10:00,37
13269477,Handle InterruptedException in XceiverClientSpi,"Fix 2 instances:

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2aKcVY8lQ4ZsNW&open=AW5md-2aKcVY8lQ4ZsNW]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2aKcVY8lQ4ZsNX&open=AW5md-2aKcVY8lQ4ZsNX]

 

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:08:44,37
13269476,Handle InterruptedException in CommitWatcher,"[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVw&open=AW5md-_8KcVY8lQ4ZsVw]

 ",newbie sonar,[],HDDS,Sub-task,Major,2019-11-20 04:07:45,37
13269475,Handle InterruptedException in BlockOutputStream,"Fix these 5 instances

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVe&open=AW5md-_2KcVY8lQ4ZsVe]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVf&open=AW5md-_2KcVY8lQ4ZsVf]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVh&open=AW5md-_2KcVY8lQ4ZsVh|https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV9&open=AW5md_AGKcVY8lQ4ZsV9]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVi&open=AW5md-_2KcVY8lQ4ZsVi]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVl&open=AW5md-_2KcVY8lQ4ZsVl]

 ",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:04:27,81
13269474,Handle InterruptedException in XceiverClientGrpc,"Fix these 3 instances

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV5&open=AW5md_AGKcVY8lQ4ZsV5]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV6&open=AW5md_AGKcVY8lQ4ZsV6]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV9&open=AW5md_AGKcVY8lQ4ZsV9]",newbie pull-request-available sonar,[],HDDS,Sub-task,Major,2019-11-20 04:02:27,81
13269473,Sonar: Null pointers should not be dereferenced,[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6BMuP1m2E_7tGaNiTf&open=AW6BMuP1m2E_7tGaNiTf],TriagePending newbie,[],HDDS,Improvement,Major,2019-11-20 03:55:08,98
13269470,Sonar: Save and reuse Random object in GenesisUtil,[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-cLKcVY8lQ4Zr2o&open=AW5md-cLKcVY8lQ4Zr2o],pull-request-available,[],HDDS,Improvement,Major,2019-11-20 03:32:40,37
13269469,Sonar: Non-primitive fields should not be volatile,"Fix following 8 instances:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVd&open=AW5md-_2KcVY8lQ4ZsVd]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2nKcVY8lQ4ZsNi&open=AW5md-2nKcVY8lQ4ZsNi]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2yKcVY8lQ4ZsN4&open=AW5md-2yKcVY8lQ4ZsN4]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-1FKcVY8lQ4ZsLn&open=AW5md-1FKcVY8lQ4ZsLn]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md--bKcVY8lQ4ZsU6&open=AW5md--bKcVY8lQ4ZsU6]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9GKcVY8lQ4ZsT0&open=AW5md-9GKcVY8lQ4ZsT0]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-VxKcVY8lQ4Zrtj&open=AW5md-VxKcVY8lQ4Zrtj]

 

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-VxKcVY8lQ4Zrtk&open=AW5md-VxKcVY8lQ4Zrtk]

 ",TriagePending codehealth,[],HDDS,Improvement,Major,2019-11-20 03:26:35,98
13269391,Sonar: OzoneClient should be closed in GetAclKeyHandler,This is a followup to HDDS-2490 where we need similar changes in GetAclKeyHandler.,pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-11-19 20:53:38,44
13269389,Invoke method(s) only conditionally,Related to : https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AVKcVY8lQ4ZsWU&open=AW5md_AVKcVY8lQ4ZsWU,pull-request-available,[],HDDS,Improvement,Major,2019-11-19 20:51:37,91
13269380,"The return type of this method should be an interface such as ""ConcurrentMap"" rather than the implementation ""ConcurrentHashMap""",Related to : https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AKKcVY8lQ4ZsWH&open=AW5md_AKKcVY8lQ4ZsWH,pull-request-available,[],HDDS,Improvement,Minor,2019-11-19 19:48:29,91
13269379,Sonar: remove volatile keyword from BlockOutputStream blockID field (#79),"Sonar report :
[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-_2KcVY8lQ4ZsVd&resolved=false&types=BUG|https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4jKcVY8lQ4ZsPQ&open=AW5md-4jKcVY8lQ4ZsPQ]",pull-request-available pull-requests-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-19 19:47:10,96
13269376,Reorder the modifiers to comply with the Java Language Specification,Related to : https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AbKcVY8lQ4ZsWo&open=AW5md_AbKcVY8lQ4ZsWo,pull-request-available,[],HDDS,Improvement,Minor,2019-11-19 19:31:54,91
13269374,Remove empty statement,Related to : https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsWF&open=AW5md_AGKcVY8lQ4ZsWF,pull-request-available,[],HDDS,Improvement,Minor,2019-11-19 19:24:21,91
13269373,"No need to call ""toString()"" method as formatting and string conversion is done by the Formatter.",Related to: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsWC&open=AW5md_AGKcVY8lQ4ZsWC,pull-request-available,[],HDDS,Improvement,Minor,2019-11-19 19:16:22,91
13269371,Format specifiers should be used instead of string concatenation,"Related to : [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV-&open=AW5md_AGKcVY8lQ4ZsV-]

 ",pull-request-available,[],HDDS,Improvement,Minor,2019-11-19 19:12:20,91
13269311,Race condition between read and write stateMachineData,"The write payload (the chunk itself) is sent to the Ratis as an external, binary byte array. It's not part of the LogEntry and saved from an async thread with calling ContainerStateMachine.writeStateMachineData

 

As it's an async thread it's possible that the stateMachineData is not yet written when the data should be sent to the followers in the next heartbeat.

By design a cache is used to avoid this issue but there are multiple problems with the cache.

First, the current cache size is chunkExecutor.getCorePoolSize() which is not enough. By default it means 60 executor threads and a cache with size 60. But in case of one very slow and 59 very fast writer the cache entries can be invalidated before the write.

In my tests (freon datanode-chunk-writer-generator) I have seen missed cache hits even with cache size 5000.

Second: as the readStateMachineData and writeStateMachien data are called from two different thread there is a race condition independent from the the cache size. It's possible that the write thread has not yet added the data to the cache but the read thread needs it.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-11-19 12:53:18,35
13269299,CI builds should use merged code state instead of the forked branch,"As of now the github actions based CI runs uses the branch of the PR which is the forked repo most of the time.

It would be better to force a rebase/merge (without push) before the builds to test the possible state after the merge not before.

For example if a PR branch uses elek/hadoop-ozone:HDDS-1234 and request a merge to apache/hadoop-ozone:master then the build should download the HDDS-1234 from elek/hadoop-ozone AND *rebase/merge* to the apache/hadoop-ozone *before* the build.

This merge is temporary just for the build/checks (no push at all).",pull-request-available,['build'],HDDS,Improvement,Major,2019-11-19 11:40:39,6
13269237,Fix accepetance test failure introduced by wait_for_safemode_exit,"https://github.com/apache/hadoop-ozone/blob/1b72718dcab7f83ebdac67b6242c729f03a8f103/hadoop-ozone/dist/src/main/compose/testlib.sh#L97

-         status=`docker-compose -f ""${compose_file}"" exec -T scm bash -c ""kinit -k HTTP/scm@EXAMPLE.COM -t /etc/security/keytabs/HTTP.keytab && $command'""`
+         status=`docker-compose -f ""${compose_file}"" exec -T scm bash -c ""kinit -k HTTP/scm@EXAMPLE.COM -t /etc/security/keytabs/HTTP.keytab && $command""`",pull-request-available,[],HDDS,Bug,Major,2019-11-19 06:23:32,5
13269236,Sonar: Fix warnings in SCMContainerManager class,https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-sEKcVY8lQ4ZsDQ&resolved=false&types=BUG,pull-request-available,['SCM'],HDDS,Bug,Major,2019-11-19 06:00:00,86
13269233,Sonar: Fix issues found in DatabaseHelper in ozone audit parser package,https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-dWKcVY8lQ4Zr39&resolved=false&severities=BLOCKER&types=BUG,pull-request-available sonar,['Tools'],HDDS,Bug,Major,2019-11-19 05:20:00,86
13269175,Add ozone.om.internal.service.id to OM HA configuration,"This Jira is to add ozone.om.internal.serviceid to let OM knows it belong to a particular service.

 

As now we have ozone.om.service.ids -≥ where we can define all service id's in a cluster.(This can happen if the same config is shared across the cluster)",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-11-18 22:03:48,13
13269074,TestOzoneManagerDoubleBufferWithOMResponse is flaky,"Flakiness can be reproduced locally. Usually it passes, but when I started to run it 100 times parallel with high cpu load it failed with the 3rd attempt (timed out)
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
-------------------------------------------------------------------------------
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 503.297 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse
testDoubleBuffer(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 500.122 s  <<< ERROR!
java.lang.Exception: test timed out after 500000 milliseconds
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:382)
        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:385)
        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:129)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
 {code}
Independent from the flakiness I think a test where the timeout is 8 minutes and starts 1000 threads to insert 500 buckets (500_000 buckets all together) it's more like an integration test and would be better to move the slowest part to the integration-test project.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-18 13:28:30,13
13269025,scmcli container delete not working,"{code:java}
java.lang.IllegalArgumentException: Unknown command type: DeleteContainer
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:219)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:112)
        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:30454)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

{code}",Triaged,['SCM'],HDDS,Bug,Major,2019-11-18 09:26:51,69
13269015,Disable failing acceptance and unit tests,"As of now we have 40 pull requests in open change but the CI gate are broken by earlier commit.

 

I propose the following solution to make it possible to safely merge pull requests:
 # Disable the failing tests (we identified them at [https://github.com/apache/hadoop-ozone/pull/203)]
 # Commit the immediate fixes (like HDDS-2521)
 # Start a discussion on ozone-dev to use a more strict revert policy (revert immediately)",pull-request-available,[],HDDS,Improvement,Blocker,2019-11-18 09:03:04,6
13268986,Sonar : fix issues in OzoneQuota,"Sonar issues : 
remove runtime exception from declaration.
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4NKcVY8lQ4ZsO_&open=AW5md-4NKcVY8lQ4ZsO_

use primitive boolean expression.
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4NKcVY8lQ4ZsO-&open=AW5md-4NKcVY8lQ4ZsO-
",pull-request-available sonar,['Ozone Client'],HDDS,Improvement,Minor,2019-11-18 04:10:39,31
13268984,Sonar : remove duplicate string literals in BlockOutputStream,"Sonar issue in executePutBlock, duplicate string literal ""blockID"" :

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_1KcVY8lQ4ZsVa&open=AW5md-_1KcVY8lQ4ZsVa

format specifiers in Log:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVg&open=AW5md-_2KcVY8lQ4ZsVg

define string constant instead of duplicate string literals.
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVb&open=AW5md-_2KcVY8lQ4ZsVb
",pull-request-available sonar,['Ozone Client'],HDDS,Improvement,Minor,2019-11-18 03:59:39,98
13268983,Sonar : refactor verifyResourceName in HddsClientUtils to fix Sonar errors,"Sonar report : 
Reduce cognitive complexity from 33 to 15
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_APKcVY8lQ4ZsWR&open=AW5md_APKcVY8lQ4ZsWR


https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_APKcVY8lQ4ZsWQ&open=AW5md_APKcVY8lQ4ZsWQ

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_APKcVY8lQ4ZsWJ&open=AW5md_APKcVY8lQ4ZsWJ



",pull-request-available sonar,['Ozone Client'],HDDS,Improvement,Major,2019-11-18 03:48:22,88
13268982,Sonar : return interface instead of implementation class in XceiverClientRatis getCommintInfoMap,"Sonar report :
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AKKcVY8lQ4ZsWH&open=AW5md_AKKcVY8lQ4ZsWH
",sonar,['SCM'],HDDS,Improvement,Minor,2019-11-18 03:41:30,88
13268981,Sonar : code smell category issues in CommitWatcher,"Sonar issues for CommitWatcher.java:

use interface instead of implementation:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVr&open=AW5md-_8KcVY8lQ4ZsVr

redundant return:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVv&open=AW5md-_8KcVY8lQ4ZsVv

format specifiers instead of concatenation in Log:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVu&open=AW5md-_8KcVY8lQ4ZsVu
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVt&open=AW5md-_8KcVY8lQ4ZsVt

redundant temporary variable:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVs&open=AW5md-_8KcVY8lQ4ZsVs
",pull-request-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-18 03:34:56,88
13268980,Sonar : remove redundant temporary assignment in HddsVersionProvider,"Sonar report :
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4AKcVY8lQ4ZsO6&open=AW5md-4AKcVY8lQ4ZsO6
",pull-request-available sonar,[],HDDS,Improvement,Minor,2019-11-18 03:31:07,98
13268978,Sonar : use format specifiers in Log inside HddsConfServlet ,"Sonar report :
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4jKcVY8lQ4ZsPQ&open=AW5md-4jKcVY8lQ4ZsPQ
",pull-request-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-18 03:23:00,88
13268975,Sonar : replace lambda with method reference in SCM BufferPool,"As per Sonar, method references are more compact than lambda - this applies to java 8, not older versions.

Sonar report:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_5KcVY8lQ4ZsVn&open=AW5md-_5KcVY8lQ4ZsVn
",pull-request-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-18 03:09:16,88
13268972,Sonar : clumsy error handling in BlockOutputStream validateResponse,"Link to Sonar report : 
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_2KcVY8lQ4ZsVk&open=AW5md-_2KcVY8lQ4ZsVk
",pull-request-available sonar,['Ozone Client'],HDDS,Improvement,Minor,2019-11-18 02:59:17,88
13268960,BufferPool.releaseBuffer may release a buffer different than the head of the list,"{code}
//BufferPool
  public void releaseBuffer(ByteBuffer byteBuffer) {
    // always remove from head of the list and append at last
    ByteBuffer buffer = bufferList.remove(0);
    // Ensure the buffer to be removed is always at the head of the list.
    Preconditions.checkArgument(buffer.equals(byteBuffer));
    buffer.clear();
    bufferList.add(buffer);
    Preconditions.checkArgument(currentBufferIndex >= 0);
    currentBufferIndex--;
  }
{code}
In the code above, it expects buffer and byteBuffer are the same object, i.e.  buffer == byteBuffer.  However the precondition is checking buffer.equals(byteBuffer). Unfortunately, both buffer and byteBuffer have remaining() == 0 so that equals(..) returns true and the precondition does not catch the bug.
",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-11-18 00:07:50,1
13268922,Fix TestSecureOzoneCluster,"TestSecureOzoneCluster is failing with {{failure to login}}.

{code:title=https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2291-5997d/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestSecureOzoneCluster.txt}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.TestSecureOzoneCluster
-------------------------------------------------------------------------------
Tests run: 10, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 23.937 s <<< FAILURE! - in org.apache.hadoop.ozone.TestSecureOzoneCluster
testSCMSecurityProtocol(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 2.474 s  <<< ERROR!
org.apache.hadoop.security.KerberosAuthException: 
failure to login: for principal: scm/pr-hdds-2291-5997d-4279494527@EXAMPLE.COM from keytab /workdir/hadoop-ozone/integration-test/target/test-dir/TestSecureOzoneCluster/scm.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1847)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1215)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1008)
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:315)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.loginAsSCMUser(StorageContainerManager.java:508)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:254)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:212)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:600)
	at org.apache.hadoop.hdds.scm.HddsTestUtils.getScm(HddsTestUtils.java:91)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSCMSecurityProtocol(TestSecureOzoneCluster.java:299)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-11-17 14:45:03,1
13268908,Multipart upload failing with NPE,S3 multipart upload is [failing|https://elek.github.io/ozone-ci-03/pr/pr-hdds-2501-b5dhd/acceptance/summary.html#s1-s11-s5] with [NPE|https://github.com/elek/ozone-ci-03/blob/ddbaf4dd92ee5f855fea3e84c59b702fb2dda663/pr/pr-hdds-2501-b5dhd/acceptance/docker-ozones3-ozones3-s3-scm.log#L740-L747].,pull-request-available,['S3'],HDDS,Bug,Critical,2019-11-17 09:07:04,1
13268775,Sonar: Avoid temporary variable scmSecurityClient,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_APKcVY8lQ4ZsWL&open=AW5md_APKcVY8lQ4ZsWL,pull-request-available sonar,[],HDDS,Bug,Major,2019-11-16 07:00:47,81
13268774,Sonar: Double Brace Initialization should not be used,https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_APKcVY8lQ4ZsWN&open=AW5md_APKcVY8lQ4ZsWN,pull-request-available sonar,[],HDDS,Bug,Major,2019-11-16 06:35:38,81
13268718,Ensure RATIS leader info is properly updated with pipeline report. ,HDDS-2034 added async pipeline creation and report handling to SCM. The leader information is not properly populated as manifested in the test failures from TestSCMPipelineManager#testPipelineReport. This ticket is opened to fix it. cc: [~sammichen],pull-request-available,[],HDDS,Bug,Major,2019-11-15 20:03:12,28
13268661,Immediately return this expression instead of assigning it to the temporary variable,"Related to : [https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md_AGKcVY8lQ4ZsV1&resolved=false]

Immediately return this expression instead of assigning it to the temporary variable",pull-request-available sonar,[],HDDS,Improvement,Major,2019-11-15 19:38:24,91
13268659,Code cleanup in EventQueue,"

https://sonarcloud.io/project/issues?fileUuids=AW5md-HgKcVY8lQ4ZrfB&id=hadoop-ozone&resolved=false",pull-request-available sonar,[],HDDS,Improvement,Major,2019-11-15 19:34:37,1
13268658,"No need to call ""toString()"" method as formatting and string conversion is done by the Formatter","[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md_AGKcVY8lQ4ZsV4&resolved=false]

Class:  XceiverClientGrpc
{code:java}
 if (LOG.isDebugEnabled()) {      LOG.debug(""Nodes in pipeline : {}"", pipeline.getNodes().toString());
{code}",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-15 19:25:55,91
13268657,"Remove this unused method parameter ""encodedToken""","Remove this unused method parameter ""encodedToken""

method: connectToDatanode

Class: XceiverClientGrpc

 ",pull-request-available sonar,[],HDDS,Improvement,Major,2019-11-15 19:24:16,91
13268656,"Remove this unused ""COMPONENT"" private field.","Remove this unused ""COMPONENT"" private field in class 

XceiverClientGrpc

[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md_AGKcVY8lQ4ZsWG&resolved=false]",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-15 19:21:16,91
13268652,Sonar TraceAllMethod NPE Could be Thrown,"Sonar cleanup: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2WKcVY8lQ4ZsNQ&open=AW5md-2WKcVY8lQ4ZsNQ]

 

 ",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-15 19:11:06,92
13268628,Fix Sonar issues in OzoneManagerServiceProviderImpl,Link to the list of issues : https://sonarcloud.io/project/issues?fileUuids=AW5md-HdKcVY8lQ4ZrUn&id=hadoop-ozone&resolved=false,pull-request-available sonar,['Ozone Recon'],HDDS,Bug,Minor,2019-11-15 17:10:21,30
13268625,Use isEmpty() to check whether the collection is empty or not in Ozone Manager module,"Ozone Manager has a number of instances where the following sonar rule is flagged.

bq. Using Collection.size() to test for emptiness works, but using Collection.isEmpty() makes the code more readable and can be more performant. The time complexity of any isEmpty() method implementation should be O(1) whereas some implementations of size() can be O(n).

An example of a flagged instance - https://sonarcloud.io/issues?myIssues=true&open=AW5md-W4KcVY8lQ4Zrv_&resolved=false
",pull-request-available sonar,['Ozone Manager'],HDDS,Bug,Major,2019-11-15 17:01:31,30
13268599,Code cleanup in replication package,Fix couple of [issues reported|https://sonarcloud.io/project/issues?directories=hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Freplication%2Chadoop-hdds%2Fcontainer-service%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Freplication&id=hadoop-ozone&resolved=false] in {{org.apache.hadoop.ozone.container.replication}} package.,pull-request-available sonar,[],HDDS,Improvement,Major,2019-11-15 15:30:04,1
13268575,Remove the hard-coded exclusion of TestMiniChaosOzoneCluster,"We excluded the execution of TestMiniChaosOzoneCluster from the hadoop-ozone/dev-support/checks/integration.sh because it was not stable enough.

Unfortunately this exclusion makes it impossible to use custom exclusion lists (-Dsurefire.excludesFile=....) as excludesFile can't be used if -Dtest=!... is already used.

I propose to remove this exclusion to make it possible to use different exclusion for different runs (pr check, daily, etc.)",pull-request-available,[],HDDS,Improvement,Major,2019-11-15 13:23:30,6
13268567,Remove keyAllocationInfo and replication info from the auditLog,"During the review of HDDS-2470 I found that the full keyLocationInfo is added to the audit log for s3 operations:

 
{code:java}

2019-11-15 12:34:18,538 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_KEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[]} | ret=SUCCESS |  2019-11-15 12:34:20,576 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_KEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[]} | ret=SUCCESS |  2019-11-15 12:34:20,626 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_BLOCK {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=THREE, keyLocationInfo=[], clientID=103141950132977668} | ret=SUCCESS |  2019-11-15 12:34:51,705 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {  containerBlockID {    containerID: 1    localID: 103141950135009280  }  blockCommitSequenceId: 2}offset: 0length: 3813createVersion: 0pipeline {  members {    uuid: ""eefe54e8-5723-458e-9204-207c6b97c9b3""    ipAddress: ""192.168.16.3""    hostName: ""ozones3_datanode_1.ozones3_default""    ports {      name: ""RATIS""      value: 9858    }    ports {      name: ""STANDALONE""      value: 9859    }    networkName: ""eefe54e8-5723-458e-9204-207c6b97c9b3""    networkLocation: ""/default-rack""  }  members {    uuid: ""ebf127d7-90a9-4f06-8fe5-a0c9c9adb743""    ipAddress: ""192.168.16.7""    hostName: ""ozones3_datanode_2.ozones3_default""    ports {      name: ""RATIS""      value: 9858    }    ports {      name: ""STANDALONE""      value: 9859    }    networkName: ""ebf127d7-90a9-4f06-8fe5-a0c9c9adb743""    networkLocation: ""/default-rack""  }  members {    uuid: ""9979c326-4982-4a4c-b34e-e70c1d825f5f""    ipAddress: ""192.168.16.6""    hostName: ""ozones3_datanode_3.ozones3_default""    ports {      name: ""RATIS""      value: 9858    }    ports {      name: ""STANDALONE""      value: 9859    }    networkName: ""9979c326-4982-4a4c-b34e-e70c1d825f5f""    networkLocation: ""/default-rack""  }  state: PIPELINE_OPEN  type: RATIS  factor: THREE  id {    id: ""69ba305b-fe89-4f5c-97cd-b894d5ee8f2b""  }  leaderID: """"}], partNumber=1, partName=/s3b607288814a5da737a92fb067500396e/bucket1/key1103141950132977668} | ret=SUCCESS |  2019-11-15 12:42:10,883 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=COMPLETE_MULTIPART_UPLOAD {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=0, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[], multipartList=[partNumber: 1partName: ""/s3b607288814a5da737a92fb067500396e/bucket1/key1103141950132977668""]} | ret=SUCCESS |  
 {code}
Including the full keyLocation info in the audit log may cause some problems:
 * It makes the the audit log slower
 * It makes harder to parse the audit log

I think it's better to separate the debug log (which can be provided easily with ozone insight tool) from the audit log. Therefore I suggest to remove the keyLocationInfo, replicationType, replicationFactor from the aduit log.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-15 12:50:16,1
13268565,Fix logic related to SCM address calculation in HddsUtils,"{{HddsUtils}} has 3 methods to calculate SCM address for various client types.  All have an unreachable {{if}} branch, because:

# {{iterator().next()}} throws exception for empty list
# {{getSCMAddresses}} never returns empty list anyway, it throws exception

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPX&open=AW5md-4qKcVY8lQ4ZsPX
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPY&open=AW5md-4qKcVY8lQ4ZsPY
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPW&open=AW5md-4qKcVY8lQ4ZsPW

Ideally code duplication among these methods should be reduced, too.

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPU&open=AW5md-4qKcVY8lQ4ZsPU
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPT&open=AW5md-4qKcVY8lQ4ZsPT
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPV&open=AW5md-4qKcVY8lQ4ZsPV

Complete list of issues in the same file:

https://sonarcloud.io/project/issues?fileUuids=AW5md-HhKcVY8lQ4Zrjn&id=hadoop-ozone&resolved=false",pull-request-available sonar,['SCM'],HDDS,Bug,Major,2019-11-15 12:45:48,1
13268559,Handle InterruptedException properly,"{quote}Either re-interrupt or rethrow the {{InterruptedException}}
{quote}
in several files (42 issues)

[https://sonarcloud.io/project/issues?id=hadoop-ozone&resolved=false&rules=squid%3AS2142&statuses=OPEN&types=BUG]

 ",Triaged newbie sonar,[],HDDS,Bug,Major,2019-11-15 12:28:41,81
13268531,Close FlushOptions in RDBStore,"{{FlushOptions}} should be closed after use.

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ4&open=AW5md-zwKcVY8lQ4ZsJ4
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ5&open=AW5md-zwKcVY8lQ4ZsJ5

Sonar also reported 15 further issues in the same file:

https://sonarcloud.io/project/issues?fileUuids=AW5md-HgKcVY8lQ4Zrga&id=hadoop-ozone&resolved=false",pull-request-available sonar,[],HDDS,Bug,Major,2019-11-15 09:41:27,1
13268528,Close ScmClient in RatisInsight,"{{ScmClient}} in {{RatisInsight}} should be closed after use.

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-mYKcVY8lQ4Zr_s&open=AW5md-mYKcVY8lQ4Zr_s

Also two other minor issues reported in the same file:

https://sonarcloud.io/project/issues?fileUuids=AW5md-HeKcVY8lQ4ZrXL&id=hadoop-ozone&resolved=false",pull-request-available sonar,[],HDDS,Bug,Major,2019-11-15 09:23:18,86
13268521,Sonar: Fix issues found in the ObjectEndpoint class,"Ensure {{ObjectOutputStream}} is closed in {{ObjectEndpoint}}:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-j-KcVY8lQ4Zr96&open=AW5md-j-KcVY8lQ4Zr96

And fix other issues in the same file:

https://sonarcloud.io/project/issues?fileUuids=AW5md-HdKcVY8lQ4ZrVc&id=hadoop-ozone&resolved=false",pull-request-available sonar,['S3'],HDDS,Bug,Major,2019-11-15 08:53:01,86
13268514,Avoid fall-through in CloseContainerCommandHandler,"Two instances of fall-through:

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-7UKcVY8lQ4ZsRk&open=AW5md-7UKcVY8lQ4ZsRk
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-7UKcVY8lQ4ZsRj&open=AW5md-7UKcVY8lQ4ZsRj

Both seem OK, but unnecessary (the next branch is {{break}}-only).  Could be made more explicit by moving/adding {{break}}.",pull-request-available sonar,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-15 08:30:25,101
13268456,Sonar: Fix issues found in StorageContainerManager class,https://sonarcloud.io/project/issues?fileUuids=AW5md-HfKcVY8lQ4ZrcG&id=hadoop-ozone&open=AW5md-tIKcVY8lQ4ZsEr&resolved=false,pull-request-available sonar,['SCM'],HDDS,Bug,Major,2019-11-15 03:12:52,86
13268436,Delegate Ozone volume create/list ACL check to authorizer plugin,"Today Ozone volume create/list ACL check are not sent to authorization plugins. This cause problem when authorization plugin is enabled. Admin still need to modify ozone-site.xml to change ozone.administrators to configure admin to create volume

 

This ticket is opened to have a consistent ACL check for all Ozone resources requests including admin request like volume create. This way, the admin defined by the authorization plugin can be honored during volume provision without restart ozone services. ",Triaged,"['Ozone Manager', 'Security']",HDDS,Improvement,Major,2019-11-14 22:27:27,37
13268435,"Sonar - ""notify"" may not wake up the appropriate thread","Addresses same issue within ReplicationManager:

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-sVKcVY8lQ4ZsDi&open=AW5md-sVKcVY8lQ4ZsDi]

[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-sVKcVY8lQ4ZsDh&open=AW5md-sVKcVY8lQ4ZsDh]",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-14 22:26:27,92
13268431,Sonar - BigDecimal(double) should not be used,"Sonar Issue:  [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-0AKcVY8lQ4ZsKR&open=AW5md-0AKcVY8lQ4ZsKR]

 

 ",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-14 21:42:48,92
13268429,Sonar: Locking on a parameter in NetUtils.removeOutscope,https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-2hKcVY8lQ4ZsNd&resolved=false&types=BUG,pull-request-available sonar,['SCM'],HDDS,Bug,Major,2019-11-14 21:19:09,86
13268412,Fix test clean up issue in TestSCMPipelineManager,"This was opened based on [~sammichen]'s investigation on HDDS-2034.

 
{quote}Failure is caused by newly introduced function TestSCMPipelineManager#testPipelineOpenOnlyWhenLeaderReported which doesn't close pipelineManager at the end. It's better to fix it in a new JIRA.
{quote}",pull-request-available,[],HDDS,Bug,Major,2019-11-14 19:38:42,77
13268405,Ensure OzoneClient is closed in Ozone Shell handlers,"OzoneClient should be closed in all command handlers ({{Handler}} subclasses).

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-Y0KcVY8lQ4Zrz6&open=AW5md-Y0KcVY8lQ4Zrz6
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-ZGKcVY8lQ4Zr0b&open=AW5md-ZGKcVY8lQ4Zr0b
etc.",pull-request-available sonar,['Ozone CLI'],HDDS,Bug,Minor,2019-11-14 18:59:51,44
13268402,Sonar: Anonymous class based initialization in HddsClientUtils,https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md_APKcVY8lQ4ZsWN&resolved=false&types=BUG,pull-request-available sonar,['SCM Client'],HDDS,Bug,Major,2019-11-14 18:46:43,86
13268398,Not enough arguments for log messages in GrpcXceiverService,"GrpcXceiverService has log messages with too few arguments for placeholders.  Only one of them is flagged by Sonar, but all seem to have the same problem.

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-69KcVY8lQ4ZsRZ&open=AW5md-69KcVY8lQ4ZsRZ",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-14 18:41:46,30
13268397,Ensure streams are closed,"* ContainerDataYaml: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6IKcVY8lQ4ZsQU&open=AW5md-6IKcVY8lQ4ZsQU
* OmUtils: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-hdKcVY8lQ4Zr76&open=AW5md-hdKcVY8lQ4Zr76",pull-request-available sonar,[],HDDS,Bug,Major,2019-11-14 18:38:26,1
13268394,Sonar: Avoid empty test methods,"{{TestRDBTableStore#toIOException}} is empty.

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-5kKcVY8lQ4ZsQH&open=AW5md-5kKcVY8lQ4ZsQH

Also {{TestTypedRDBTableStore#toIOException}}:

https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-5qKcVY8lQ4ZsQJ&open=AW5md-5qKcVY8lQ4ZsQJ",pull-request-available sonar,['test'],HDDS,Bug,Minor,2019-11-14 18:21:51,81
13268393,Disable XML external entity processing,"Disable XML external entity processing in

* NodeSchemaLoader: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2nKcVY8lQ4ZsNm&open=AW5md-2nKcVY8lQ4ZsNm
* ConfigFileAppender:
** https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_uKcVY8lQ4ZsVY&open=AW5md-_uKcVY8lQ4ZsVY
** https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_uKcVY8lQ4ZsVZ&open=AW5md-_uKcVY8lQ4ZsVZ
* MultiDeleteRequestUnmarshaller: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-kDKcVY8lQ4Zr-N&open=AW5md-kDKcVY8lQ4Zr-N",pull-request-available sonar,['Security'],HDDS,Bug,Major,2019-11-14 18:14:11,1
13268388,Ozone Manager - New Metrics for Trash Key Lists and Fails,"We should add new metrics to track trash key lists and fails to OMMetrics

Naming recommendations: NumTrashKeyLists, NumTrashKeyListFails",pull-request-available,[],HDDS,Sub-task,Major,2019-11-14 17:50:43,92
13268358,Avoid fall-through in HddsUtils#getBlockID,"{{switch}} in {{HddsUtils#getBlockID}} has potential fall-through.  It should be handled explicitly (eg. throw exception or {{return null}}).

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPe&open=AW5md-4qKcVY8lQ4ZsPe
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPf&open=AW5md-4qKcVY8lQ4ZsPf
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPg&open=AW5md-4qKcVY8lQ4ZsPg
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPh&open=AW5md-4qKcVY8lQ4ZsPh
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPi&open=AW5md-4qKcVY8lQ4ZsPi
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPj&open=AW5md-4qKcVY8lQ4ZsPj",pull-request-available sonar,[],HDDS,Bug,Minor,2019-11-14 16:06:06,101
13268331,Enable github actions for pull requests,"HDDS-2400 introduced a github actions workflow for each ""push"" event. It turned out that pushing to a forked repository doesn't trigger this event even if it's part of a PR.

 

We need to enable the execution for pull_request events:

References:

 [https://github.community/t5/GitHub-Actions/Run-a-GitHub-action-on-pull-request-for-PR-opened-from-a-forked/m-p/31147#M690]

[https://help.github.com/en/actions/automating-your-workflow-with-github-actions/events-that-trigger-workflows#pull-request-events-for-forked-repositories]
{noformat}
Note: By default, a workflow only runs when a pull_request's activity type is opened, synchronize, or reopened. To trigger workflows for more activity types, use the types keyword.{noformat}
 

 

 

 ",pull-request-available,[],HDDS,Improvement,Major,2019-11-14 14:30:16,6
13268251,Close streams in TarContainerPacker,"Ensure various streams are closed in {{TarContainerPacker}}:

* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUH&open=AW5md-9bKcVY8lQ4ZsUH
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUL&open=AW5md-9bKcVY8lQ4ZsUL
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUK&open=AW5md-9bKcVY8lQ4ZsUK
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUJ&open=AW5md-9bKcVY8lQ4ZsUJ
* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUI&open=AW5md-9bKcVY8lQ4ZsUI",pull-request-available sonar,['Ozone Datanode'],HDDS,Bug,Minor,2019-11-14 09:33:06,1
13268216,Sonar : remove log spam for exceptions inside XceiverClientGrpc.reconnect,"Sonar issue:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsWE&open=AW5md_AGKcVY8lQ4ZsWE
",pull-request-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-14 07:26:19,88
13268214,Sonar : replace instanceof with catch block in XceiverClientGrpc.sendCommandWithRetry,"Sonar issue:
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV_&open=AW5md_AGKcVY8lQ4ZsV_
",pull-request-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-14 07:20:21,88
13268209,Sonar : remove temporary variable in XceiverClientGrpc.sendCommand,"Sonar issues :
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV1&open=AW5md_AGKcVY8lQ4ZsV1
https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md_AGKcVY8lQ4ZsV2&open=AW5md_AGKcVY8lQ4ZsV2
",pull-request-available sonar,['SCM'],HDDS,Improvement,Minor,2019-11-14 07:04:49,88
13268160,TableCache cleanup issue for OM non-HA,"In OM in non-HA case, the ratisTransactionLogIndex is generated by OmProtocolServersideTranslatorPB.java. And in OM non-HA validateAndUpdateCache is called from multipleHandler threads. So think of a case where one thread which has an index - 10 has added to doubleBuffer. (0-9 still have not added). DoubleBuffer flush thread flushes and call cleanup. (So, now cleanup will go and cleanup all cache entries with less than 10 epoch) This should not have cleanup those which might have put in to cache later and which are in process of flush to DB. This will cause inconsitency for few OM requests.

 

 

Example:

4 threads Committing 4 parts.

1st thread - part 1 - ratis Index - 3

2nd thread - part 2 - ratis index - 2

3rd thread - part3 - ratis index - 1

 

First thread got lock, and put in to doubleBuffer and cache with OmMultipartInfo (with part1). And cleanup is called to cleanup all entries in cache with less than 3. In the mean time 2nd thread and 1st thread put 2,3 parts in to OmMultipartInfo in to Cache and doubleBuffer. But first thread might cleanup those entries, as it is called with index 3 for cleanup.

 

Now when the 4th part upload came -> when it is commit Multipart upload when it gets multipartinfo it get Only part1 in OmMultipartInfo, as the OmMultipartInfo (with 1,2,3 is still in process of committing to DB). So now after 4th part upload is complete in DB and Cache we will have 1,4 parts only. We will miss part2,3 information.

 

So for non-HA case cleanup will be called with list of epochs that need to be cleanedup.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-11-14 00:21:47,13
13268149,Unregister ContainerMetadataScrubberMetrics on thread exit,{{ContainerMetadataScanner}} thread should call {{ContainerMetadataScrubberMetrics#unregister}} before exiting.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-11-13 23:00:27,1
13268144,Remove OzoneClient exception Precondition check,"If RaftCleintReply encounters an exception other than NotLeaderException, NotReplicatedException, StateMachineException or LeaderNotReady, then it sets success to false but there is no exception set. This causes a Precondition check failure in XceiverClientRatis which expects that there should be an exception if success=false.",TriagePending,[],HDDS,Bug,Major,2019-11-13 22:11:59,43
13268142,Fix code reliability issues found by Sonar in Ozone Recon module.,"sonarcloud.io has flagged a number of code reliability issues in Ozone recon (https://sonarcloud.io/code?id=hadoop-ozone&selected=hadoop-ozone%3Ahadoop-ozone%2Frecon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Frecon).

Following issues will be triaged / fixed.
* Double Brace Initialization should not be used
* Resources should be closed
* InterruptedException should not be ignored
",pull-request-available sonar,['Ozone Recon'],HDDS,Bug,Major,2019-11-13 21:57:18,30
13268141,Use try-with-resources while creating FlushOptions in RDBStore.,Link to the sonar issue flag - https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ4&open=AW5md-zwKcVY8lQ4ZsJ4. ,pull-request-available sonar,['Ozone Manager'],HDDS,Bug,Major,2019-11-13 21:50:41,30
13268130,Improve exception message for CompleteMultipartUpload,"When InvalidPart error occurs, the exception message does not have any information about partName and partNumber, it will be good to have this information.",pull-request-available,['S3'],HDDS,Bug,Major,2019-11-13 20:51:58,13
13268112,"Add partName, partNumber for CommitMultipartUpload","Right now when complete Multipart Upload is not printing partName and  partNumber into the audit log. This will help in analyzing audit logs for MPU.

 

 

2019-11-13 15:14:10,191 | INFO  | OMAudit | user=root | ip=xx.xx.xx.xx | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=ozone-test, key=plc_1570850798896_2991, dataSize=5242880, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {

  containerBlockID

{     containerID: 2     localID: 103129366531867089   }

  blockCommitSequenceId: 4978

}

offset: 0

length: 5242880

createVersion: 0

pipeline {

  leaderID: """"

  members {

    uuid: ""5d03aed5-cfb3-4689-b168-0c9a94316551""

    ipAddress: ""xx.xx.xx.xx""

    hostName: ""xx.xx.xx.xx""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""5d03aed5-cfb3-4689-b168-0c9a94316551""

    networkLocation: ""/default-rack""

  }

  members {

    uuid: ""a71462ae-7865-4ed5-b84e-60616df60a0d""

    ipAddress: ""9.134.51.25""

    hostName: ""9.134.51.25""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""a71462ae-7865-4ed5-b84e-60616df60a0d""

    networkLocation: ""/default-rack""

  }

  members {

    uuid: ""79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03""

    ipAddress: ""9.134.51.215""

    hostName: ""9.134.51.215""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03""

    networkLocation: ""/default-rack""

  }

  state: PIPELINE_OPEN

  type: RATIS

  factor: THREE

  id

{     id: ""ec6b06c5-193f-4c30-879b-5a12284dc4f8""   }

}

]} | ret=SUCCESS |",pull-request-available,['S3'],HDDS,Bug,Major,2019-11-13 19:02:12,13
13268103,Avoid changing client-side key metadata,Ozone RPC client should not change input map from client while creating keys.,pull-request-available,['Ozone Client'],HDDS,Bug,Minor,2019-11-13 18:23:24,1
13268083,scmcli close pipeline command not working,"Close pipeline command is failing with the following exception
{noformat}
java.lang.IllegalArgumentException: Unknown command type: ClosePipeline
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:219)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:112)
	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:29883)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{noformat}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-11-13 16:41:50,19
13267996,Allow running Freon validators with limited memory,"Freon validators read each item to be validated completely into a {{byte[]}} buffer.  This allows timing only the read (and buffer allocation), but not the subsequent digest calculation.  However, it also means that memory required for running the validators is proportional to key size.

I propose to add a command-line flag to allow calculating the digest while reading the input stream.  This changes timing results a bit, since values will include the time required for digest calculation.  On the other hand, Freon will be able to validate huge keys with limited memory.",pull-request-available,['freon'],HDDS,Improvement,Major,2019-11-13 11:26:08,1
13267846,Avoid unnecessary allocations for FileChannel.open call,"{{ChunkUtils}} calls {{FileChannel#open(Path, OpenOption...)}}.  Vararg array elements are then added to a new {{HashSet}} to call {{FileChannel#open(Path, Set<? extends OpenOption>, FileAttribute<?>...)}}.  We can call the latter directly instead.",performance pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-12 22:09:51,1
13267834,Reduce unnecessary getServiceInfo calls,"OzoneManagerProtocolClientSideTranslatorPB.java Line 766-772 has multiple impl.getServiceInfo() which can be reduced by adding a local variable. 
{code:java}
 
resp.addAllServiceInfo(impl.getServiceInfo().getServiceInfoList().stream()
 .map(ServiceInfo::getProtobuf)
 .collect(Collectors.toList()));
if (impl.getServiceInfo().getCaCertificate() != null) {
 resp.setCaCertificate(impl.getServiceInfo().getCaCertificate()); {code}",pull-request-available,[],HDDS,Bug,Major,2019-11-12 20:57:55,28
13267759,Add jq dependency in Contribution guideline,"Docker based tests are using JQ to parse JMX pages of different processes, but the documentation does not mention it as a dependency.

Add it to CONTRIBUTION.MD in the ""Additional requirements to execute different type of tests"" section.",pull-request-available,[],HDDS,Improvement,Major,2019-11-12 16:02:38,63
13267744,Logging by ChunkUtils is misleading,"During a k8s based test I found a lot of log message like:
{code:java}
2019-11-12 14:27:13 WARN  ChunkManagerImpl:209 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='A9UrLxiEUN_testdata_chunk_4465025, offset=0, len=1024} {code}
I was very surprised as at ChunkManagerImpl:209 there was no similar lines.

It turned out that it's logged by ChunkUtils but it's used the logger of ChunkManagerImpl.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-11-12 15:10:17,6
13267675,Default checksum type is wrong in description,"Default client checksum type is CRC32, but the config item's description says it's SHA256 (leftover from HDDS-1149).  The description should be updated to match the actual default value.

{code:title=https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/common/src/main/resources/ozone-default.xml#L1489-L1497}
  <property>
    <name>ozone.client.checksum.type</name>
    <value>CRC32</value>
    <tag>OZONE, CLIENT, MANAGEMENT</tag>
    <description>The checksum type [NONE/ CRC32/ CRC32C/ SHA256/ MD5] determines
      which algorithm would be used to compute checksum for chunk data.
      Default checksum type is SHA256.
    </description>
  </property>
{code}",newbie pull-request-available,['Ozone Client'],HDDS,Bug,Trivial,2019-11-12 11:30:36,97
13267512,Refactor ReplicationManager to consider maintenance states,"In its current form the replication manager does not consider decommission or maintenance states when checking if replicas are sufficiently replicated. With the introduction of maintenance states, it needs to consider decommission and maintenance states when deciding if blocks are over or under replicated.

It also needs to provide an API to allow the decommission manager to check if blocks are over or under replicated, so the decommission manager can decide if a node has completed decommission and maintenance or not.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-11-11 16:32:17,11
13267415,Avoid list copy in ChecksumData,"{{ChecksumData}} is initially created with empty list of checksums, then it is updated with computed checksums, copying the list.  The computed list can be set directly.",performance pull-request-available,['Ozone Datanode'],HDDS,Improvement,Minor,2019-11-11 07:35:52,1
13267351,Add explicit base image version for images derived from ozone-runner,"{{ozone-om-ha}} and {{ozonescripts}} build images based on {{apache/ozone-runner}}.

Problem: They do not specify base image versions, so it defaults to {{latest}}.  If a new {{ozone-runner}} image is published on Docker Hub, developers needs to manually pull the {{latest}} image for it to take effect on these derived images.

Solution: Use explicit base image version (defined by {{OZONE_RUNNER_VERSION}} variable in {{.env}} file.",pull-request-available,['docker'],HDDS,Improvement,Minor,2019-11-10 13:38:16,1
13267254,Implement MiniOzoneHAClusterImpl#getOMLeader,Implement MiniOzoneHAClusterImpl#getOMLeader and use it.,pull-request-available,[],HDDS,Improvement,Major,2019-11-09 12:40:21,12
13267214,Improve OM HA robot tests,"In one CI run, testOMHA.robot failed because robot framework SSH commands failed. This Jira aims to verify that the command execution succeeds.",pull-request-available,[],HDDS,Improvement,Major,2019-11-09 00:01:38,43
13267178,Wrong condition for re-scheduling in ReportPublisher,"It seems the condition for scheduling next run of {{ReportPublisher}} is wrong:

{code:title=https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/ReportPublisher.java#L74-L76}
    if (!executor.isShutdown() ||
        !(context.getState() == DatanodeStates.SHUTDOWN)) {
      executor.schedule(this,
{code}

Given the condition above, the task may be scheduled again if the executor is shutdown, but the state machine is not set to shutdown (or vice versa).  I think the condition should have an {{&&}}, not {{||}}.  (Currently it is unlikely to happen, since [context state is set to shutdown before the report executor|https://github.com/apache/hadoop-ozone/blob/f928a0bdb4ea2e5195da39256c6dda9f1c855649/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java#L392-L393].)

[~nanda], can you please confirm if this is a typo or intentional?",newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Trivial,2019-11-08 19:37:19,97
13267171,Use lazy string evaluation in preconditions,"Avoid eagerly evaluating error messages of preconditions (similarly to HDDS-2318, but there may be other occurrences of the same issue).",performance pull-request-available,[],HDDS,Improvement,Major,2019-11-08 18:49:12,1
13267127,Datanode ReplicateContainer thread pool should be configurable,"The replicateContainer command uses a ReplicationSupervisor object to implement a threadpool used to process replication commands.

In DatanodeStateMachine this thread pool is initialized with a hard coded number of threads (10). This should be made configurable with a default value of 10.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-08 15:23:08,11
13267126,Delete block command should use a thread pool,"The datanode receives commands over the heartbeat and queues all commands on a single queue in StateContext.commandQueue. Inside DatanodeStateMachine a single thread is used to process this queue (started by initCommandHander thread) and it passes each command to a ‘handler’. Each command type has its own handler.

The delete block command immediately executes the command on the thread used to process the command queue. Therefore if the delete is slow for some reason (it must access disk, so this is possible) it could cause other commands to backup.

This should be changed to use a threadpool to queue the deleteBlock command, in a similar way to ReplicateContainerCommand.
",TriagePending,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-08 15:16:30,11
13267125,Delete container command should used a thread pool,"The datanode receives commands over the heartbeat and queues all commands on a single queue in StateContext.commandQueue. Inside DatanodeStateMachine a single thread is used to process this queue (started by initCommandHander thread) and it passes each command to a ‘handler’. Each command type has its own handler.

The delete container command immediately executes the command on the thread used to process the command queue. Therefore if the delete is slow for some reason (it must access disk, so this is possible) it could cause other commands to backup.

This should be changed to use a threadpool to queue the deleteContainer command, in a similar way to ReplicateContainerCommand.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-11-08 15:12:19,11
13267080,ContainerReplica should contain DatanodeInfo rather than DatanodeDetails,"The ContainerReplica object is used by the SCM to track containers reported by the datanodes. The current fields stored in ContainerReplica are:

{code}
final private ContainerID containerID;
final private ContainerReplicaProto.State state;
final private DatanodeDetails datanodeDetails;
final private UUID placeOfBirth;
{code}

Now we have introduced decommission and maintenance mode, the replication manager (and potentially other parts of the code) need to know the status of the replica in terms of IN_SERVICE, DECOMMISSIONING, DECOMMISSIONED etc to make replication decisions.

The DatanodeDetails object does not carry this information, however the DatanodeInfo object extends DatanodeDetails and does carry the required information.

As DatanodeInfo extends DatanodeDetails, any place which needs a DatanodeDetails can accept a DatanodeInfo instead.

In this Jira I propose we change the DatanodeDetails stored in ContainerReplica to DatanodeInfo.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-11-08 10:52:32,11
13267078,Replace ToStringBuilder in BlockData,"{{BlockData#toString}} uses {{ToStringBuilder}} for ease of implementation.  This has a few problems:

# {{ToStringBuilder}} uses {{StringBuffer}}, which is synchronized
# the default buffer is 512 bytes, more than needed here
# {{BlockID}} and {{ContainerBlockID}} both use another {{StringBuilder}} or {{StringBuffer}} for their {{toString}} implementation, leading to several allocations and copies

The flame graph shows that {{BlockData#toString}} may be responsible for 1.5% of total allocations while putting keys.",performance pull-request-available,[],HDDS,Improvement,Minor,2019-11-08 10:49:49,1
13267063,Remove server side dependencies from ozonefs jar files,"During the review of HDDS-2427 we found that some of the server side dependencies (container-service, framework) are added to the ozonefs library jars. Server side dependencies should be excluded from the client side to make the client safer and the build faster.",pull-request-available,['Ozone Filesystem'],HDDS,Task,Major,2019-11-08 10:00:30,6
13266965,Add ServiceName support for getting Signed Cert.,We need to add support for adding Service name into the Certificate Signing Request.,pull-request-available,"['OM HA', 'Ozone Manager', 'SCM']",HDDS,Sub-task,Major,2019-11-07 23:31:09,91
13266942,Exclude webapps from hadoop-ozone-filesystem-lib-current uber jar,"This has caused issue for DN UI loading.

hadoop-ozone-filesystem-lib-current-xx.jar is in the classpath which accidentally loaded Ozone datanode web application instead of Hadoop datanode application. This leads to the reported error. ",pull-request-available,[],HDDS,Bug,Major,2019-11-07 22:40:50,13
13266940, Support recover-trash to an existing bucket.,"Support recovering trash to an existing bucket.

*Note*
1. We should also add a config key that prevents this mode, so admins can force the recovery to a new bucket always.
 >> Yeah, we should add a config to enable admins always recover trash to new buckets.
 But the config checking would be implemented in Ozone CLI part

2. A new table *(TrashTable)* is introduced to implement this jira",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-11-07 22:39:34,31
13266937,Add the recover-trash command server side handling.,"GAdd the standard server side code for command handling.

The components should to be updated including
 *OzoneManager*
 -> *KeyManager* / *KeyManagerImpl*
 -> *OMMetadataManager* / *OmMetadataManagerImpl*

 The PR on this issue also added *OMTrashRecoverRequest* for handling request with OMHA.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-11-07 22:37:03,31
13266936,Add the recover-trash command client side code,"Add protobuf, RpcClient and ClientSideTranslator code for the recover-trash command.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-11-07 22:35:43,31
13266931,Add the list trash command server side handling.,Add the standard code for any command handling in the server side.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-11-07 22:30:49,92
13266930,Add the list trash command to the client side,Add the list-trash command to the protobuf files and to the client side translator.,pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-11-07 22:28:40,92
13266856,Completely disable tracer if hdds.tracing.enabled=false,"There is a config setting to enable/disable OpenTracing-based distributed tracing in Ozone ({{hdds.tracing.enabled}}).  However, setting it to false does not prevent tracer initialization, which causes unnecessary object allocations.",performance pull-request-available,[],HDDS,Improvement,Minor,2019-11-07 15:39:56,1
13266800,Simplify robot tests with removing output greps,"The robot tests under hadoop-ozone/dist/src/main/smoketest contain a lot of grep fragments to filter out the output of the CLI commands:
{code:java}
ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.name' {code}
It was introduced because we had some unrelated logs on the console.

Fortunately the ozone log4j.properties has been adjusted to hide the unrelated lines.

It would be great to remove all of these ""grep -v ...."" fragments to make sure that all the CLI work well with annoying logging.

For example the previous line should work as
{code:java}
ozone sh key list o3://om/fstest/bucket1 | jq -r '.name'  {code}",newbie pull-request-available,[],HDDS,Task,Major,2019-11-07 10:35:15,94
13266796,Set configuration variables from annotated java objects,"HDDS-1469 introduced a new method to handle configuration. Configuration can be injected directly to java objects which makes all the java constants unnecessary.

 

Almost.

 

To read the configuration it's enough to have an annotated java object. For example:

 
{code:java}
@ConfigGroup(prefix = ""hdds.scm"")
public class ScmConfig {
  private String principal;
  private String keytab;  @Config(key = ""kerberos.principal"",
        type = ConfigType.STRING,
        defaultValue = """",
        tags = { ConfigTag.SECURITY, ConfigTag.OZONE },
        description = ""This Kerberos principal is used by the SCM service.""
  )
  public void setKerberosPrincipal(String kerberosPrincipal) {
    this.principal = kerberosPrincipal; {code}
And the configuration can be set in ozone-site.xml

Unfortunately during the unit testing we need to inject the configuration variables programmatically which requires a String constant:
{code:java}
configuration.set(ScmConfig.ConfigStrings.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,      
            ""scm/"" + host + ""@"" + realm); {code}
I propose to implement a simple setter in the OzoneConfiguration which may help to set configuration based on an annotated configuration object instance:
{code:java}
OzoneConfiguration conf = new OzoneConfiguration();

SCMHTTPServerConfig httpConfig = SCMHTTPServerConfig(principal1,...);

conf.setFromObject(httpConfig){code}
This is the opposite direction of the existing OzoneConfiguration.getObject() and can be implemented with a similar approach.",pull-request-available,[],HDDS,Task,Major,2019-11-07 10:26:56,1
13266786,Define description/topics/merge strategy for the github repository with .asf.yaml,".asf.yaml helps to set different parameters on github repositories without admin privileges:

[https://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories]

This basic .asf.yaml defines description/url/topics and the allowed merge buttons.",pull-request-available,[],HDDS,Task,Major,2019-11-07 09:58:11,6
13266771,Create DataChunkValidator Freon test,"HDDS-2327 introduced a new load test which generates a lot of WriteChunk request.

As with other freon test (for example with. HadoopFsGenerator/HadoopFsValidator) we need an other load test for validation/read path.

It should be almost the same DatanodeChunkGenerator but it should read the first chunk and compare all the others (very similar to the HadoopFsValidator or OzoneClientKeyValidator)",newbie pull-request-available,['freon'],HDDS,Task,Major,2019-11-07 09:11:54,67
13266764,Ozoneperf docker cluster should use privileged containers,"The profiler [servlet|https://github.com/elek/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] (which helps to run java profiler in the background and publishes the result on the web interface) requires privileged docker containers.

 

This flag is missing from the ozoneperf docker-compose cluster (which is designed to run performance tests).

 

 ",pull-request-available,[],HDDS,Task,Major,2019-11-07 08:38:00,6
13266590,Separate OM Storage initialization from OzoneManager,"The Ozone Manager Storage initialization is partly using the scm security client protocol.

As a first step of cleaning up high level protobuf usage from the code, it is beneficial to separate this initialization code, so that it is clearly visible what parts of the SCM it is using, and later it helps to figure out the better design for the SCM security client related calls.

This JIRA is aiming for the following separation tasks:
- Clear out the OM Storage initialization related code from OzoneManager class
- Clear out and move the common code that is used by OzoneManager class initialization and OzoneManager Storage initialization logic to avoid any duplication.
- Clear unused stuff from OzoneManager after the changes.
- To be the first step in a longer refactoring activity that clears out the SCM client and security client calls related code in OzoneManager and from things like KeyManager to have these abstracted from protobuf",pull-request-available,[],HDDS,Sub-task,Major,2019-11-06 13:37:40,63
13266490,Reduce log level of per-node failure in XceiverClientGrpc,"When reading from a pipeline, client should not care if some datanode could not service the request, as long as the pipeline as a whole is OK.  The [log message|https://github.com/apache/hadoop-ozone/blob/2529cee1a7dd27c51cb9aed0dc57af283ff24e26/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L303-L304] indicating node failure was [increased to error level|https://github.com/apache/hadoop-ozone/commit/a79dc4609a975d46a3e051ad6904fb1eb40705ee#diff-b9b6f3ccb12829d90886e041d11395b1R288] in HDDS-1780.  This task proposes to change it back to debug.",pull-request-available,['Ozone Client'],HDDS,Task,Minor,2019-11-06 05:39:38,1
13266204,int2ByteString unnecessary byte array allocation,"{{int2ByteString}} implementations (currently duplicated in [RatisHelper|https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/ratis/RatisHelper.java#L280-L289] and [Checksum|https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/Checksum.java#L64-L73], but the first one is being removed in HDDS-2375) result in unnecessary byte array allocations:

# {{ByteString.Output}} creates 128-byte buffer by default, which is too large for writing a single int
# {{DataOutputStream}} allocates an [extra 8-byte array|https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/io/DataOutputStream.java#l204], used only for writing longs
# {{ByteString.Output}} also creates 10-element array for {{flushedBuffers}}",performance pull-request-available,[],HDDS,Bug,Minor,2019-11-04 22:20:44,1
13266199,Add support for Registered id as service identifier for CSR.,"The SCM HA needs the ability to represent a group as a single entity. So that Tokens for each of the OM which is part of an HA group can be honored by the data nodes. 

This patch adds the notion of a service group ID to the Certificate Infrastructure. In the next JIRAs, we will use this capability when issuing certificates to OM -- especially when they are in HA mode.",pull-request-available,"['OM HA', 'Ozone Manager', 'SCM']",HDDS,Sub-task,Major,2019-11-04 22:01:10,91
13266191,Remove leftover reference to OUTPUT_FILE from shellcheck.sh,"{{shellcheck.sh}} gives the following error (but works fine otherwise):

{noformat}
$ hadoop-ozone/dev-support/checks/shellcheck.sh
hadoop-ozone/dev-support/checks/shellcheck.sh: line 23: : No such file or directory
...
{noformat}

This happens because {{OUTPUT_FILE}} variable is undefined:

{code:title=https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-ozone/dev-support/checks/shellcheck.sh#L23}
echo """" > ""$OUTPUT_FILE""
{code}

The command can be removed.",newbie pull-request-available,[],HDDS,Bug,Trivial,2019-11-04 21:26:30,97
13266155,Adapt hadolint check to improved CI framework,"HDDS-1515 added a script to run [hadolint|https://github.com/hadolint/hadolint] as a CI check.  However, the CI check framework contract has been improved a bit in HDDS-2030 since HDDS-1515 was filed.  More details can be found in the [README|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/dev-support/checks/README.md].

The goal of this task is to update the script to create the extra output files:

 * save list of failures to {{summary.txt}} (while also outputting everything to stdout)
 * output number of failures to {{failures}}

Refer to [checkstyle.sh|https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/dev-support/checks/checkstyle.sh] for example.",newbie pull-request-available,['build'],HDDS,Improvement,Minor,2019-11-04 17:50:27,7
13266096,Enable github actions based builds for Ozone,"Current PR checks are executed in a private branch based on the scripts in [https://github.com/elek/argo-ozone]

but the results are stored in a public repositories:

[https://github.com/elek/ozone-ci-q4|https://github.com/elek/ozone-ci-q3]

[https://github.com/elek/ozone-ci-03]

 

As we discussed during the community calls, it would be great to use github actions (or any other cloud based build) to make all the build definitions more accessible for the community.

[~vivekratnavel] checked CircleCI which has better reporting capabilities. But INFRA has concerns about the permission model of circle-ci:
{quote}it is highly unlikley we will allow a bot to be able to commit code (whether or not that is the intention, allowing circle-ci will make this possible, and is a complete no)
{quote}
See:

https://issues.apache.org/jira/browse/INFRA-18131

[https://lists.apache.org/thread.html/af52e2a3e865c01596d46374e8b294f2740587dbd59d85e132429b6c@%3Cbuilds.apache.org%3E]

 

Fortunately we have a clear contract. Or build scripts are stored under _hadoop-ozone/dev-support/checks_ (return code show the result, details are printed out to the console output). It's very easy to experiment with different build systems.

 

Github action seems to be an obvious choice: it's integrated well with GitHub and it has more generous resource limitations.

 

With this Jira I propose to enable github actions based PR checks for a few tests (author, rat, unit, acceptance, checkstyle, findbugs) as an experiment.

 ",pull-request-available,['build'],HDDS,Improvement,Major,2019-11-04 11:25:13,6
13266089,Update mailing list information in CONTRIBUTION and README files,"We have new mailing lists:

 [ozone-dev@hadoop.apache.org|mailto:ozone-dev@hadoop.apache.org]

[ozone-issues@hadoop.apache.org|mailto:ozone-issues@hadoop.apache.org]

[ozone-commits@hadoop.apache.org|mailto:ozone-commits@hadoop.apache.org]

 

We need to update CONTRIBUTION.md and README.md to use ozone-dev instead of hdfs-dev (optionally we can mention the issues/commits lists, but only in CONTRIBUTION.md)",newbie pull-request-available,[],HDDS,Improvement,Major,2019-11-04 10:54:53,71
13265830,Remove usage of LogUtils class from ratis-common,"MiniOzoneChaoasCluster.java for setting log level it uses LogUtils from ratis-common. But this is removed from LogUtils as part of Ratis-508.

We can avoid depending on ratis for this, and use GenericTestUtils from hadoop-common test.

LogUtils.setLogLevel(GrpcClientProtocolClient.LOG, Level.WARN);",pull-request-available,[],HDDS,Bug,Major,2019-11-01 20:20:43,13
13265692,Fix calling cleanup for few missing tables in OM,"After DoubleBuffer flushes, we call cleanup cache to cleanup tables cache.

For few tables cleanup of cache is missed:
 # PrefixTable
 # S3SecretTable
 # DelegationTable",pull-request-available,[],HDDS,Bug,Major,2019-11-01 04:29:51,13
13265654,Handle Ozone S3 completeMPU to match with aws s3 behavior.,"# When uploaded 2 parts, and when complete upload 1 part no error
 # During complete multipart upload name/part number not matching with uploaded part and part number then InvalidPart error
 # When parts are not specified in sorted order InvalidPartOrder
 # During complete multipart upload when no uploaded parts, and we specify some parts then also InvalidPart
 # Uploaded parts 1,2,3 and during complete we can do upload 1,3 (No error)
 # When part 3 uploaded, complete with part 3 can be done",pull-request-available,"['Ozone Manager', 'S3']",HDDS,Bug,Major,2019-10-31 22:40:46,13
13265648,Ozone S3 Gateway allows bucket name with underscore to be created but throws an error during put key operation,"Steps to reproduce:
aws s3api --endpoint http://localhost:9878 create-bucket --bucket ozone_test

aws s3api --endpoint http://localhost:9878 put-object --bucket ozone_test --key ozone-site.xml --body /etc/hadoop/conf/ozone-site.xml

S3 gateway throws a warning:
{code:java}
javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : _
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : _
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1780)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1628)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	... 13 more
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-31 22:11:00,37
13265645,HDDS-1847 broke some unit tests,"Siyao Meng commented on HDDS-1847:
----------------------------------

Looks like this commit breaks {{TestKeyManagerImpl}} in {{setUp()}} and {{cleanup()}}. Run {{TestKeyManagerImpl#testListStatus()}} to steadily repro. I believe there could be other tests that are broken by this.

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerHttpServer.getSpnegoPrincipal(StorageContainerManagerHttpServer.java:74)
        at org.apache.hadoop.hdds.server.BaseHttpServer.<init>(BaseHttpServer.java:81)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerHttpServer.<init>(StorageContainerManagerHttpServer.java:36)
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:330)
        at org.apache.hadoop.hdds.scm.TestUtils.getScm(TestUtils.java:544)
        at org.apache.hadoop.ozone.om.TestKeyManagerImpl.setUp(TestKeyManagerImpl.java:150)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
        at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
        at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
        at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
        at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}

{code}
java.lang.NullPointerException
        at org.apache.hadoop.ozone.om.TestKeyManagerImpl.cleanup(TestKeyManagerImpl.java:176)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
        at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
        at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
        at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
        at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}",pull-request-available,[],HDDS,Bug,Major,2019-10-31 22:08:46,99
13265531,add toStateMachineLogEntryString provider in Ozone's ContainerStateMachine,This jira proposes to add a new toStateMachineLogEntryString provider in Ozone's ContainerStateMachine to print extra log debug statements in Ratis.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-10-31 13:11:35,18
13265395,Implement incremental ChunkBuffer,"HDDS-2375 introduces a ChunkBuffer for flexible buffering. In this JIRA, we implement ChunkBuffer with an incremental buffering so that the memory spaces are allocated incrementally.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-10-30 21:45:46,9
13265357,"As admin, volume list command should list all volumes not just admin user owned volumes","The command `ozone sh volume ls` lists only the volumes that are owned by the user.

 

Expected behavior: The command should list all the volumes in the system if the user is an ozone administrator. ",TriagePending,['Ozone CLI'],HDDS,Task,Major,2019-10-30 18:05:56,31
13265263,Large chunks during write can have memory pressure on DN with multiple clients,"During large file writes, it ends up writing {{16 MB}} chunks.  

https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java#L691

In large clusters, 100s of clients may connect to DN. In such cases, depending on the incoming write workload mem load on DN can increase significantly. 

",Triaged performance,['Ozone Datanode'],HDDS,Improvement,Major,2019-10-30 10:35:40,16
13265237,Consider reducing number of file::exists() calls during write operation,"When writing 100-200 MB files with multiple threads, observed lots of {{[file::exists(])}} checks.

For every 16 MB chunk, it ends up checking whether {{chunksLoc}} directory exists or not. (ref: [https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L239])

Also, this check ({{ChunkUtils.getChunkFile}}) happens from 2 places.

1.org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk

2.org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction

Note that these are folders and not actual chunk filenames. It would be helpful to reduce this check, if we track create/delete of these folders.",Triaged performance,['Ozone Datanode'],HDDS,Bug,Major,2019-10-30 09:15:53,1
13265138,"In ExcludeList, add if not exist only","Created based on comment from [~chinseone] in HDDS-2356

https://issues.apache.org/jira/browse/HDDS-2356?focusedCommentId=16960796&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16960796

 

 ",pull-request-available,[],HDDS,Bug,Major,2019-10-29 22:45:46,13
13265109,Use the Table.isExist API instead of get() call while checking for presence of key.,"Currently, when OM creates a file/directory, it checks the absence of all prefix paths of the key in its RocksDB. Since we don't care about the deserialization of the actual value, we should use the isExist API added in org.apache.hadoop.hdds.utils.db.Table which internally uses the more performant keyMayExist API of RocksDB.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-29 18:58:25,30
13265106,OM terminates with RocksDB error while continuously writing keys.,"Exception trace after writing around 800,000 keys.


{code}
2019-10-29 11:15:15,131 ERROR org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer: Terminating with exit status 1: During flush to DB encountered err
or in OMDoubleBuffer flush thread OMDoubleBufferFlushThread
java.io.IOException: Unable to write the batch.
        at org.apache.hadoop.hdds.utils.db.RDBBatchOperation.commit(RDBBatchOperation.java:48)
        at org.apache.hadoop.hdds.utils.db.RDBStore.commitBatchOperation(RDBStore.java:240)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:146)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.rocksdb.RocksDBException: unknown WriteBatch tag
        at org.rocksdb.RocksDB.write0(Native Method)
        at org.rocksdb.RocksDB.write(RocksDB.java:1421)
        at org.apache.hadoop.hdds.utils.db.RDBBatchOperation.commit(RDBBatchOperation.java:46)
        ... 3 more
{code}

Assigning to [~bharat] since he has already started work on this. ",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2019-10-29 18:41:07,13
13265099,"Change ""OZONE"" as string used in the code where OzoneConsts.OZONE is suitable","Based on a review I have done a quick check, and there are quite a few places where we have hardcoded ""ozone"" as String literal or a capital version of it into the code.

Let's check then one by one, and where it is possible replace it with OzoneConsts.OZONE, or if the lower case version is not acceptable at all places, then create an other constant with the uppercase version and use that.

This is the search, and the results:
{code:bash}
find . -name *.java | while read FILE; do NUM=`grep -c -i ""\""OZONE\"""" $FILE`; if [ $NUM -gt 0 ]; then echo $FILE; fi; done | sort | uniq
./hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/RocksDBStore.java
./hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java
./hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java
./hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestContainerDataYaml.java
./hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestBlockManagerImpl.java
./hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainer.java
./hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ServerUtils.java
./hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/metrics/SCMContainerManagerMetrics.java
./hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMContainerPlacementMetrics.java
./hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeMetrics.java
./hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/SCMPipelineMetrics.java
./hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMContainerMetrics.java
./hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java
./hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/block/TestBlockManager.java
./hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestCloseContainerEventHandler.java
./hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestSCMContainerManager.java
./hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java
./hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/CreateSubcommand.java
./hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/util/OzoneVersionInfo.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/container/TestContainerStateManagerIntegration.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/container/metrics/TestSCMContainerManagerMetrics.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestContainerOperations.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestContainerStateMachineIdempotency.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestStorageContainerManager.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/Test2WayCommitInRatis.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClientAbstract.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestWatchForCommit.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/ozShell/TestS3Shell.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestAllocateContainer.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestContainerSmallFile.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestGetCommittedBlockLengthAndPutKey.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestXceiverClientManager.java
./hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestXceiverClientMetrics.java
./hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestS3BucketManager.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ratis/TestOzoneManagerDoubleBufferWithOMResponse.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMAllocateBlockRequest.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyCommitRequest.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyCreateRequest.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/bucket/TestS3BucketDeleteRequest.java
./hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/bucket/TestS3BucketDeleteResponse.java
./hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/CommonHeadersContainerResponseFilter.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestAbortMultipartUpload.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketDelete.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketHead.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestInitiateMultipartUpload.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestListParts.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestMultipartUploadComplete.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestMultipartUploadWithCopy.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectPut.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestPartUpload.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestRootList.java
./hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/header/TestAuthorizationHeaderV4.java
./hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkContainerStateMap.java
{code}",pull-request-available,['test'],HDDS,Improvement,Major,2019-10-29 17:49:57,63
13265038,Speed up TestOzoneManagerHA#testOMRetryProxy and #testTwoOMNodesDown,"Marton's comment:
https://github.com/apache/hadoop-ozone/pull/30#pullrequestreview-302465440

Out of curiosity, I ran entire TestOzoneManagerHA locally. The entire test class finished in 10m 30s. I discovered {{testOMRetryProxy}} and {{testTwoOMNodesDown}} are taking the most time (2m and 2m 30s respectively) to finish. Most time are wasted on retry and wait. We could reasonably reduce the amount of time on the wait.

As I tested, with the patch, {{testOMRetryProxy}} and {{testTwoOMNodesDown}} finish in 20 sec each, saving almost 4 min runtime on those two tests alone. The whole TestOzoneManagerHA test finishes in 5m 44s with the patch.",pull-request-available,[],HDDS,Improvement,Major,2019-10-29 13:34:29,12
13264936,Refactor BlockOutputStream to allow flexible buffering,"In HDDS-2331, we found that Ozone client allocates a ByteBuffer with chunk size (e.g. 16MB ) to store data, unregarded the actual data size.  The ByteBuffer will create a  byte[] with chunk size.  When the ByteBuffer is wrapped to a ByteString the byte[] remains in the ByteString.

As a result, when the actual data size is small (e.g. 1MB), a lot of memory spaces (15MB) are wasted.

In this JIRA, we refactor BlockOutputStream so that the buffering becomes more flexible.  In a later JIRA (HDDS-2386), we implement chunk buffer using a list of smaller buffers which are allocated only if needed.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-10-29 02:21:44,9
13264923,Make Ozone Readme.txt point to the Ozone websites instead of Hadoop.,See the title.,pull-request-available,[],HDDS,Improvement,Major,2019-10-28 22:31:48,102
13264872,Move isUseRatis getFactor and getType from XCeiverClientManager,"The given methods in XCeiverClientManager are working based on the configuration supplied in the constructor of XCeiverClientManager class.

The only real code usage of this is in ContainerOperationsClient.

Refactor the ContainerOperationsClient constructor to work based on the configuration, and then move these values there directly and set the values of them at the constructor. Clean up all test references to the methods, and remove the methods from the XCeiverClientManager.",pull-request-available,[],HDDS,Sub-task,Major,2019-10-28 17:54:31,63
13264797,Datanode pipeline is failing with NoSuchFileException,"Found it on a k8s based test cluster using a simple 3 node cluster and HDDS-2327 freon test. After a while the StateMachine become unhealthy after this error:
{code:java}
datanode-0 datanode java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: java.nio.file.NoSuchFileException: /data/storage/hdds/2a77fab9-9dc5-4f73-9501-b5347ac6145c/current/containerDir0/1/chunks/gGYYgiTTeg_testdata_chunk_13931.tmp.2.20830 {code}
Can be reproduced.",pull-request-available,[],HDDS,Bug,Critical,2019-10-28 10:53:27,6
13264785,Print out the ozone version during the startup instead of hadoop version,"Ozone components printing out the current version during the startup:

 
{code:java}
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = om/10.8.0.145
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.2.0
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-{code}
But as it's visible the build / compiled information is about hadoop not about hadoop-ozone.

(And personally I prefer to use a github compatible url instead of the SVN style -r. Something like:
{code:java}
STARTUP_MSG: build =  https://github.com/apache/hadoop-ozone/commit/8541c5694efebb58f53cf4665d3e4e6e4a12845c ; compiled by '....' on ...{code}
 ",Triaged pull-request-available,[],HDDS,Improvement,Major,2019-10-28 10:04:06,1
13264729,Support Ozone HddsDatanodeService run as plugin with HDFS Datanode,"In RunningWithHDFS.md 
{code:java}
export HADOOP_CLASSPATH=/opt/ozone/share/hadoop/ozoneplugin/hadoop-ozone-datanode-plugin.jar{code}
ozone-hdfs/docker-compose.yaml

 
{code:java}
      environment:
         HADOOP_CLASSPATH: /opt/ozone/share/hadoop/ozoneplugin/*.jar
{code}
when i run hddsdatanodeservice as pulgin in hdfs datanode, it comes out with the error below , there is no constructor without parameter.

 

 
{code:java}
2019-10-21 21:38:56,391 ERROR datanode.DataNode (DataNode.java:startPlugins(972)) - Unable to load DataNode plugins. Specified list of plugins: org.apache.hadoop.ozone.HddsDatanodeService
java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.ozone.HddsDatanodeService.<init>()
{code}
what i doubt is that, ozone-0.5 not support running as a plugin in hdfs datanode now ? if so, 

why donnot  we remove doc RunningWithHDFS.md ? ",pull-request-available,['documentation'],HDDS,Task,Major,2019-10-28 02:49:10,103
13264643,Fix typo in param description.,"In many addAcl(), the annotation param acl should be
{code}
ozone acl to be added.
{code}

but now is 
{code}
ozone acl top be added.
{code}

The files as follows:
{code}
hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/protocol/ClientProtocol.java
614:   * @param acl ozone acl top be added.

hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/RpcClient.java
1029:   * @param acl ozone acl top be added.

hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java
453:   * @param acl ozone acl top be added.

hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/IOzoneAcl.java
36:   * @param acl ozone acl top be added.

hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/PrefixManagerImpl.java
96:   * @param acl ozone acl top be added.

hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/VolumeManagerImpl.java
481:   * @param acl ozone acl top be added.

hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManagerImpl.java
379:   * @param acl ozone acl top be added.

hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
1475:   * @param acl ozone acl top be added.

hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java
2868:   * @param acl ozone acl top be added.

hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocol/OzoneManagerProtocol.java
486:   * @param acl ozone acl top be added.

hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocolPB/OzoneManagerProtocolClientSideTranslatorPB.java
1405:   * @param acl ozone acl top be added.
{code}",newbie pull-request-available,[],HDDS,Task,Trivial,2019-10-27 08:26:21,71
13264617,TestOzoneManagerDoubleBufferWithDummyResponse failing intermittently,"{noformat}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.479 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse
testDoubleBufferWithDummyResponse(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse)  Time elapsed: 1.404 s  <<< FAILURE!
java.lang.AssertionError
...
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.testDoubleBufferWithDummyResponse(TestOzoneManagerDoubleBufferWithDummyResponse.java:116)
{noformat}

* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2345-jsf2s/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.txt 
* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2272-bfh6s/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.txt
 ",pull-request-available,['test'],HDDS,Bug,Minor,2019-10-26 17:33:52,1
13264538,Remove ozone.enabled flag,"Now when ozone is started the start-ozone.sh/stop-ozone.sh script check whether this property is enabled or not to start ozone services. Now, this property and this check can be removed.

 

This was needed when ozone is part of Hadoop, and we don't want to start ozone services by default. Now there is no such requirement.",newbie pull-request-available,[],HDDS,Task,Major,2019-10-25 20:00:30,86
13264412,TestRatisPipelineProvider#testCreatePipelinesDnExclude is flaky,"TestRatisPipelineProvider#testCreatePipelinesDnExclude is flaky, failing in CI intermittently:

* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2360-9pxww/integration/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt
* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2352-cxhw9/integration/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt",pull-request-available,['test'],HDDS,Bug,Minor,2019-10-25 05:20:21,1
13264409,Add a OM metrics to find the false positive rate for the keyMayExist,Add a OM metrics to find the false positive rate for the keyMayExist.,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-25 04:54:27,30
13264404,Failed to create Ratis container,"Error logs;
2019-10-29 10:24:59,553 [pool-7-thread-1] ERROR      - org.rocksdb.RocksDBException Failed init RocksDB, db path : /data2/hdds/efe9f8f3-86be-417c-93cd-24bbeceee86f/current/containerDir2/1126/metadata/1126-dn-container.db, exception :/data2/hdds/efe9f8f3-86be-417c-93cd-24bbeceee86f/current/containerDir2/1126/metadata/1126-dn-container.db: does not exist (create_if_missing is false)

CACHED_OPTS is a RockDB options cache in MetadataStoreBuilder.  The cache keeps the old rocksdb options which is not refreshed with new option values at new call. 

Logs as following didn't reveal the true failure of write failure.  Will improve following logs too. 

2019-10-24 17:43:53,460 [pool-7-thread-1] INFO       - Operation: CreateContainer : Trace ID:  : Message: Container creation failed. : Result: CONTAINER_INTERNAL_ERROR
2019-10-24 17:43:53,478 [pool-7-thread-1] INFO       - Operation: WriteChunk : Trace ID:  : Message: ContainerID 402 creation failed : Result: CONTAINER_INTERNAL_ERROR",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-10-25 03:45:27,5
13264347,Ozone Manager init & start command prints out unnecessary line in the beginning.,"{code}
[root@avijayan-om-1 ozone-0.5.0-SNAPSHOT]# bin/ozone --daemon start om
Ozone Manager classpath extended by
{code}

We could probably print this line only when extra elements are added to OM classpath or skip printing this line altogether.",pull-request-available,[],HDDS,Bug,Major,2019-10-24 20:44:42,31
13264316,Update Ratis snapshot to d6d58d0,"Update Ratis dependency version to snapshot [d6d58d0|https://github.com/apache/incubator-ratis/commit/d6d58d0], to fix memory issues (RATIS-726, RATIS-728).",pull-request-available,"['Ozone Client', 'Ozone Datanode']",HDDS,Task,Major,2019-10-24 19:40:50,1
13264247,Seeking randomly in a key with more than 2 blocks of data leads to inconsistent reads,"During Hive testing we found the following exception:

{code}
TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1569246922012_0214_1_03_000000_3:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: error iterating
    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
    at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
    at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
    at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
    at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
    at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
    at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: error iterating
    at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:80)
    at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426)
    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267)
    ... 16 more
Caused by: java.io.IOException: java.io.IOException: error iterating
    at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
    at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:366)
    at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
    at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
    at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
    at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
    at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
    ... 18 more
Caused by: java.io.IOException: error iterating
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:835)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:74)
    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:361)
    ... 24 more
Caused by: java.io.IOException: Error reading file: o3fs://hive.warehouse.vc0136.halxg.cloudera.com:9862/data/inventory/delta_0000001_0000001_0000/bucket_00000
    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1283)
    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.nextBatch(RecordReaderImpl.java:156)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$1.next(VectorizedOrcAcidRowBatchReader.java:150)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$1.next(VectorizedOrcAcidRowBatchReader.java:146)
    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:831)
    ... 26 more
Caused by: java.io.IOException: Inconsistent read for blockID=conID: 2 locID: 102851451236759576 bcsId: 14608 length=26398272 numBytesRead=6084153
    at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:176)
    at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
    at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:75)
    at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
    at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
    at org.apache.orc.impl.RecordReaderUtils.readDiskRanges(RecordReaderUtils.java:557)
    at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readFileData(RecordReaderUtils.java:276)
    at org.apache.orc.impl.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:1189)
    at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1057)
    at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1208)
    at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1243)
    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1279)
    ... 30 more
{code}

Evaluating the code path, the following is the issue:
given a file with more data than 2 blocks
when there are random seeks in the file to the end then to the beginning
then the read fails with the final cause of the exception above.

[~shashikant] has a solution already for this issue, which we have successfully tested internally with Hive, I am assigning this JIRA to him to post the PR.",pull-request-available,[],HDDS,Bug,Critical,2019-10-24 12:15:01,16
13264168,SCM log is full of AllocateBlock logs,"Make them Debug logs.

2019-10-24 03:17:43,087 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

scm_1       | 2019-10-24 03:17:43,088 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

scm_1       | 2019-10-24 03:17:43,089 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

scm_1       | 2019-10-24 03:17:43,093 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \{datanodes = [], containerIds = [], pipelineIds = []}

 ",pull-request-available,['SCM'],HDDS,Task,Minor,2019-10-24 03:28:20,13
13264100,Client gets internal error instead of volume not found in secure cluster,"New Freon generators create volume and bucket if necessary.  This does not work in secure cluster for volume, but works for bucket:

{code}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozonesecure
$ docker-compose exec scm bash
$ kinit -kt /etc/security/keytabs/testuser.keytab testuser/scm@EXAMPLE.COM
$ ozone freon ockg -n 1
...
Check access operation failed for volume:vol1
...
Successful executions: 0
$ ozone sh volume create vol1
$ ozone freon ockg -n 1
...
2019-10-23 18:30:27,279 [main] INFO       - Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false
...
Successful executions: 1
{code}

The problem is that {{VOLUME_NOT_FOUND}} result is lost during ACL check, and client gets {{INTERNAL_ERROR}} instead.",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-10-23 18:38:25,1
13264021,Fix write performance issue in Non-HA OM ,"HDDS-2333 enables sync option in OM non-HA mode. However, this flushes very frequently causing disk to saturate its IOPS soon. It creates way too small write workloads and disk hits limit.

To put it in perspective, in simple write benchmark of creating keys with 10 clients, it generates {{0.33MB/s}} write workload with {{116 IOPS}}. This causes disk to saturate at 98%.

 

!Screenshot 2019-10-23 at 2.27.05 PM.png|width=621,height=370!

 

Reverting back HDDS-2333 fixes this issue. I see >{{10x}} degradation with HDDS-2333.

In case non-HA is supported in OM, it would be good to call it out. Currently, code explicitly enables sync option. [https://github.com/apache/hadoop-ozone/commit/c6c9794fc590371ad9c3b8fdcd7a36ed42909b40#diff-3ed3ab4891d7b4fa31ca96740b78ae5bR261]

 

I used  {{commit 1baa5a158d13f469c12bef86ef288d60ef0eee85}} in master branch.

 ",TriagePending performance,['Ozone Manager'],HDDS,Bug,Major,2019-10-23 13:28:26,19
13263962,QueryNode does not respect null values for opState or state,"In HDDS-2197, the queryNode API call was changed to allow operational state (in_service, decommissioning etc) to be passed along with the node health state. This changed allowed for a null state to indicate a wildcard, so passing:

opState = null
healthState = HEALTHY

Allows one to find all the healthy nodes, irrespective of their opState.

However, for an enum protobuf field, if no value is specified, the first enum in the set is returned as the default. This means that when a null is passed for opState, only the IN_SERVICE nodes are returned. Similar for health state - passing a null will return only HEALTHY nodes.

This PR will fix this issue so the null value acts as a wildcard as intended.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-10-23 08:23:01,11
13263803,remove log4j properties for org.apache.hadoop.ozone,Properties int log4j.properties cause logger in package org.apache.hadoop.ozone cannot write log to .log file ;such as OM startup_msg .,pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-10-22 13:25:00,103
13263779,XCeiverClientGrpc's parallel use leads to NPE,"This issue came up when testing Hive with ORC tables on Ozone storage backend, I so far I could not reproduce it locally within a JUnit test but the issue.

I am attaching a diff file that shows what logging I have added in XCevierClientGrpc and in KeyInputStream to get the results that made me arrive to the following understanding of the scenario:
- Hive starts a couple of threads to work on the table data during query execution
- There is one RPCClient that is being used by these threads
- The threads are opening different stream to read from the same key in ozone
- The InputStreams internally are using the same XCeiverClientGrpc
- XCeiverClientGrpc throws the following NPE intermittently:

{code}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandAsync(XceiverClientGrpc.java:398)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:295)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:259)
        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:242)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:169)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:224)
        at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:173)
        at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)
        at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:75)
        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
        at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:555)
        at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:370)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:61)
        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:105)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:1708)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1596)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2900(OrcInputFormat.java:1383)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1568)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1565)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1565)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1383)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
{code}

I have two proposals to fix this issue, one is the easy answer to put synchronization to the XCeiverClientGrpc code, the other one is a bit more complicated, let me explain below.

Naively I would assume that when I get a client SPI instance from XCeiverClientManager, that instance is ready to use. In fact it is not, and when the user of the SPI instance sends the first request that is the point when the client gets essentially ready. Now if we put synchronization to this code, that is the easy solution, but my pragmatic half screams for a better solution, that ensures that the Manager essentially manages the clients that is giving to it's users, and the clients themselves are not getting ready by accident.
I am working on a proposal that moves things around a bit, and looking for possible other solutions that does not feel hacky as I feel with the easy solution.

I am attaching the followings:
- a diff that shows the added extended logging in XCeiverClientGrpc and KeyInputStream.
- a job log snippet from a Hive query that shows the relevant output from the extensive logging added by the diff in a cluster.
- later a proposal for the fix I need to work on a bit more",pull-request-available,['Ozone Client'],HDDS,Improvement,Critical,2019-10-22 11:25:48,63
13263742,Replication manager config has wrong description,"Replication manager's configuration for its own interval:

{code:title=https://github.com/apache/hadoop-ozone/blob/eb1d77e3206fab1a4ac0573507c9deb2b56b9ea1/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java#L808-L822}
    @Config(key = ""thread.interval"",
        type = ConfigType.TIME,
        defaultValue = ""300s"",
        tags = {SCM, OZONE},
        description = ""When a heartbeat from the data node arrives on SCM, ""
            + ""It is queued for processing with the time stamp of when the ""
            + ""heartbeat arrived. There is a heartbeat processing thread ""
            + ""inside ""
            + ""SCM that runs at a specified interval. This value controls how ""
            + ""frequently this thread is run.\n\n""
            + ""There are some assumptions build into SCM such as this ""
            + ""value should allow the heartbeat processing thread to run at ""
            + ""least three times more frequently than heartbeats and at least ""
            + ""five times more than stale node detection time. ""
            + ""If you specify a wrong value, SCM will gracefully refuse to ""
            + ""run. ""
            + ""For more info look at the node manager tests in SCM.\n""
            + ""\n""
            + ""In short, you don't need to change this.""
    )
{code}

duplicates SCM heartbeat interval doc:

{code:title=https://github.com/apache/hadoop-ozone/blob/eb1d77e3206fab1a4ac0573507c9deb2b56b9ea1/hadoop-hdds/common/src/main/resources/ozone-default.xml#L973-L991}
  <property>
    <name>ozone.scm.heartbeat.thread.interval</name>
    <value>3s</value>
    <tag>OZONE, MANAGEMENT</tag>
    <description>
      When a heartbeat from the data node arrives on SCM, It is queued for
      processing with the time stamp of when the heartbeat arrived. There is a
      heartbeat processing thread inside SCM that runs at a specified interval.
      This value controls how frequently this thread is run.

      There are some assumptions build into SCM such as this value should allow
      the heartbeat processing thread to run at least three times more
      frequently than heartbeats and at least five times more than stale node
      detection time. If you specify a wrong value, SCM will gracefully refuse
      to run. For more info look at the node manager tests in SCM.

      In short, you don't need to change this.
    </description>
  </property>
{code}",pull-request-available,['SCM'],HDDS,Bug,Minor,2019-10-22 08:24:58,1
13263701,Add a UT for newly added clone() in OmBucketInfo,Add a UT for newly added clone() method in OMBucketInfo as part of HDDS-2333.,newbie pull-request-available,[],HDDS,Task,Major,2019-10-22 05:48:11,31
13263699,Add immutable entries in to the DoubleBuffer for Volume requests.,"OMVolumeCreateRequest.java L159:
{code:java}
omClientResponse =
 new OMVolumeCreateResponse(omVolumeArgs,volumeList, omResponse.build());{code}
 

We add this to double-buffer, and double-buffer flushThread which is running in the background when picks up, converts to protoBuf and to ByteArray and write to rocksDB tables. So, during this conversion(This conversion will be done without any lock acquire), if any other request changes internal structure(like acls list) of OmVolumeArgs we might get ConcurrentModificationException.

 ",pull-request-available,[],HDDS,Task,Major,2019-10-22 05:07:07,13
13263654,Add immutable entries in to the DoubleBuffer for Bucket requests.,"OMBucketCreateRequest.java L181:

omClientResponse =
 new OMBucketCreateResponse(omBucketInfo,
 omResponse.build());

 

We add this to double-buffer, and double-buffer flushThread which is running in the background when picks up, converts to protoBuf and to ByteArray and write to rocksDB tables. So, during this conversion(This conversion will be done without any lock acquire), if any other request changes internal structure(like acls list) of OMBucketInfo we might get ConcurrentModificationException.

 ",pull-request-available,[],HDDS,Task,Major,2019-10-21 21:13:13,13
13263641,ContainerStateMachine$chunkExecutor threads hold onto native memory,"In a heap dump many threads in ContainerStateMachine$chunkExecutor holds onto native memory in the ThreadLocal map. Every such thread holds onto chunk worth of DirectByteBuffer. Since these threads are involved in write and read chunk operations, the JVM allocates chunk (16MB) worth of DirectByteBuffer in the ThreadLocalMap for every thread involved in IO. Also the native memory would not be GC'ed as long as the thread is alive.

It would be better to reduce the default number of chunk executor threads and have them in proportion to number of disks on the datanode. We should also use DirectByeBuffers for the IO on datanode. Currently we allocate HeapByteBuffer which needs to be backed by DirectByteBuffer. If we can use a DirectByteBuffer we can avoid a buffer copy.",TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2019-10-21 19:47:07,35
13263637,Validate tar entry path during extraction,Containers extracted from tar.gz should be validated to confine entries to the archive's root directory.,pull-request-available,[],HDDS,Improvement,Major,2019-10-21 19:27:30,1
13263609,Update RATIS snapshot version,Update RATIS version to incorporate fix that went into RATIS-707 among others.,pull-request-available,['build'],HDDS,Improvement,Major,2019-10-21 17:19:26,86
13263543,Add OzoneManager to MiniOzoneChaosCluster,This jira proposes to add OzoneManager to MiniOzoneChaosCluster with OzoneHA implementation done. This will help in discovering bugs in Ozone Manager HA,pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-21 13:26:08,43
13263401,Fix checkstyle errors,"Checkstyle errors intoduced in HDDS-2281:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2281-wfpgn/checkstyle/summary.txt}
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java
 465: Line is longer than 80 characters (found 81).
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ContainerTestHelper.java
 244: Line is longer than 80 characters (found 84).
hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestContainerStateMachineFailures.java
 30: Unused import - org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException.
 506: &apos;;&apos; is preceded with whitespace.
 517: &apos;;&apos; is preceded with whitespace.
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-10-20 11:40:25,1
13263399,Fix TestKeyValueContainer#testRocksDBCreateUsesCachedOptions,"TestKeyValueContainer#testRocksDBCreateUsesCachedOptions, introduced in HDDS-2283, is failing:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2283-cnrrq/unit/hadoop-hdds/container-service/org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.txt}
testRocksDBCreateUsesCachedOptions(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.135 s  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<11>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testRocksDBCreateUsesCachedOptions(TestKeyValueContainer.java:406)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-10-20 11:25:15,1
13263389,Params not included in AuditMessage,"HDDS-2323 introduced the following Findbugs violation:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/trunk/trunk-nightly-20191020-r5wzl/findbugs/summary.txt}
M P UrF: Unread field: org.apache.hadoop.ozone.audit.AuditMessage$Builder.params  At AuditMessage.java:[line 106]
{noformat}

Which reveals that {{params}} is now not logged in audit messages:

{noformat}
2019-10-20 08:41:35,248 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=CREATE_VOLUME | ret=SUCCESS |
2019-10-20 08:41:35,312 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=CREATE_BUCKET | ret=SUCCESS |
2019-10-20 08:41:35,407 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=ALLOCATE_KEY | ret=SUCCESS |
2019-10-20 08:41:37,355 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=COMMIT_KEY | ret=SUCCESS |
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-10-20 08:42:29,1
13263388,Dummy chunk manager fails with length mismatch error,"HDDS-1094 added a config option ({{hdds.container.chunk.persistdata=false}}) to drop chunks instead of writing them to disk.  Currently this option triggers the following error with any key size:

{noformat}
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: data array does not match the length specified. DataLen: 16777216 Byte Array: 16777478
	at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDummyImpl.writeChunk(ChunkManagerDummyImpl.java:87)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleWriteChunk(KeyValueHandler.java:695)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:176)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:277)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:150)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:413)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:423)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$1(ContainerStateMachine.java:458)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-10-20 08:06:36,1
13263286,Enable sync option for OM non-HA ,"In OM non-HA when double buffer flushes, it should commit with sync turned on. As in non-HA when power failure/system crashes, the operations which are acknowledged by OM might be lost in this kind of scenario. (As in rocks DB with Sync false, the flush is asynchronous and it will not persist to storage system)

 

In HA, this is not a problem because the guarantee is provided by ratis and ratis logs.",pull-request-available,[],HDDS,Task,Major,2019-10-18 23:34:40,13
13263070,Random key generator can get stuck,"Freon's random key generator can get stuck waiting for completion (without any hint to what's happening) if object creation encounters any non-IOException.

Steps to reproduce:

# Start Ozone cluster with 1 datanode
# Start Freon (5K keys of size 1MB)

Result: after a few hundred keys progress stops.

{noformat}
$ docker-compose exec scm ozone freon rk --numOfThreads 1 --numOfVolumes 1 --numOfBuckets 1 --replicationType RATIS --factor ONE --keySize $(echo '2^20' | bc -lq) --numOfKeys $(echo '5 * 2^10' | bc -lq) --bufferSize $(echo '2^16' | bc -lq)
2019-10-18 10:44:45,224 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2019-10-18 10:44:45,381 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-18 10:44:45,381 INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2019-10-18 10:44:47,140 [main] INFO       - Number of Threads: 1
2019-10-18 10:44:47,145 [main] INFO       - Number of Volumes: 1.
2019-10-18 10:44:47,146 [main] INFO       - Number of Buckets per Volume: 1.
2019-10-18 10:44:47,146 [main] INFO       - Number of Keys per Bucket: 5120.
2019-10-18 10:44:47,147 [main] INFO       - Key size: 1048576 bytes
2019-10-18 10:44:47,147 [main] INFO       - Buffer size: 65536 bytes
2019-10-18 10:44:47,147 [main] INFO       - validateWrites : false
2019-10-18 10:44:47,151 [main] INFO       - Starting progress bar Thread.
...
 7.07% |????????                                                                                             |  362/5120 
{noformat}",pull-request-available,['freon'],HDDS,Bug,Major,2019-10-18 10:53:03,1
13263066,Destroy pipelines on any decommission or maintenance nodes,"When a node is marked for decommission or maintenance, the first step in taking the node out of service is to destroy any pipelines the node is involved in and confirm they have been destroyed before getting the container list for the node.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-10-18 10:42:20,11
13263052,Support large-scale listing ,"Large-scale listing of directory contents takes a lot longer time and also has the potential to run into OOM. I have > 1 million entries in the same level and it took lot longer time with {{RemoteIterator}} (didn't complete as it was stuck in RDB::seek).

S3A batches it with 5K listing per fetch IIRC.  It would be good to have this feature in ozone as well.",TriagePending performance,['Ozone Manager'],HDDS,Bug,Critical,2019-10-18 09:16:22,43
13263050,Provide new Freon test to test Ratis pipeline with pure XceiverClientRatis,"[~xyao] suggested during an offline talk to implement one additional Freon test to test the ratis part only.

It can use XceiverClientManager which creates a pure XceiverClientRatis. The client can be used to generate chunks as the datanode accepts any container id / block id.

With this approach we can stress-test one selected ratis pipeline without having full end2end overhead of the key creation (OM, SCM, etc.)",pull-request-available,['freon'],HDDS,New Feature,Major,2019-10-18 09:01:56,6
13263048,Http server of Freon is not started for new Freon tests,"HDDS-2022 introduced new Freon tests but the Freon http server is not started for the new tests.

Freon includes a http server which can be turned on with the '–server' flag. It helps to monitor and profile the freon as the http server contains by default the prometheus and profiler servlets.

The server should be started if's requested.",pull-request-available,['freon'],HDDS,New Feature,Major,2019-10-18 08:58:59,6
13263047,BenchMarkDatanodeDispatcher genesis test is failing with NPE,"## What changes were proposed in this pull request?

Genesis is a microbenchmark tool for Ozone based on JMH ([https://openjdk.java.net/projects/code-tools/jmh/).]

 

Due to the recent Datanode changes the BenchMarkDatanodeDispatcher is failing with NPE:

 
{code:java}
java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.common.interfaces.Handler.<init>(Handler.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.<init>(KeyValueHandler.java:114)
	at org.apache.hadoop.ozone.container.common.interfaces.Handler.getHandlerForContainerType(Handler.java:78)
	at org.apache.hadoop.ozone.genesis.BenchMarkDatanodeDispatcher.initialize(BenchMarkDatanodeDispatcher.java:115)
	at org.apache.hadoop.ozone.genesis.generated.BenchMarkDatanodeDispatcher_createContainer_jmhTest._jmh_tryInit_f_benchmarkdatanodedispatcher0_G(BenchMarkDatanodeDispatcher_createContainer_jmhTest.java:438)
	at org.apache.hadoop.ozone.genesis.generated.BenchMarkDatanodeDispatcher_createContainer_jmhTest.createContainer_Throughput(BenchMarkDatanodeDispatcher_createContainer_jmhTest.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
 {code}
And this is the just the biggest problem there are a few other problems. I propose the following fixes:

*fix 1*: NPE is thrown because the 'context' object is required by KeyValueHandler/Handler classes.

In fact the context is not required, we need two functionalities/info from the context: the ability to send icr (IncrementalContainerReport) and the ID of the datanode.

Law of Demeter principle suggests to have only the minimum required information from other classes.

For example instead of having context but using only context.getParent().getDatanodeDetails().getUuidString() we can have only the UUID string which makes more easy to test (unit and benchmark) the Handler/KeyValueHandler.

This is the biggest (but still small change) in this patch: I started to use the datanodeId and an icrSender instead of having the full context.

*fix 2,3:* There were a few other problems. The scmId was missing if the writeChunk was called from Benchmark and and the Checksum was also missing.

*fix 4:* I also had a few other problems: very huge containers are used (default 5G) and as the benchmark starts with creating 100 containers it requires 500G space by default. I adjusted the container size to make it possible to run on local machine.

 

## How this patch can be tested?
{code:java}
./ozone genesis -benchmark=BenchMarkDatanodeDispatcher.writeChunk{code}
 ",pull-request-available,[],HDDS,New Feature,Major,2019-10-18 08:56:53,6
13263035,Enhance locking mechanism in OzoneManager,"OM has reentrant RW lock. With 100% read or 100% write benchmarks, it works out reasonably fine. There is already a ticket to optimize the write codepath (as it incurs reading from DB for key checks).

However, when small amount of write workload (e.g 3-5 threads) is added to the running read benchmark, throughput suffers significantly. This is due to the fact that the reader threads would get blocked often.  I have observed around 10x slower throughput (i.e 100% read benchmark was running at 12,000 TPS and with couple of writer threads added to it, it goes down to 1200-1800 TPS).

1. Instead of single write lock, one option could be good to scale out the write lock depending on the number of cores available in the system and acquire relevant lock by hashing the key.

2. Another option is to explore if we can make use of StampedLocks of JDK 8.x, which scales well when multiple readers and writers are there. But it is not a reentrant lock. So need to explore whether it can be an option or not.

 

 

 ",Triaged performance,['Ozone Manager'],HDDS,Bug,Critical,2019-10-18 08:01:54,40
13262991,Mem allocation: Optimise AuditMessage::build(),"String format allocates/processes more than {color:#000000}OzoneAclUtil.fromProtobuf in write benchmark.{color}

{color:#000000}Would be good to use + instead of format.{color}",performance,['Ozone Manager'],HDDS,Bug,Major,2019-10-18 03:00:46,86
13262983,DoubleBuffer flush termination and OM shutdown's after that.,"om1_1       | 2019-10-18 00:34:45,317 [OMDoubleBufferFlushThread] ERROR      - Terminating with exit status 2: OMDoubleBuffer flush threadOMDoubleBufferFlushThreadencountered Throwable error

om1_1       | java.util.ConcurrentModificationException

om1_1       | at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1660)

om1_1       | at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)

om1_1       | at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)

om1_1       | at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)

om1_1       | at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)

om1_1       | at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)

om1_1       | at org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup.getProtobuf(OmKeyLocationInfoGroup.java:65)

om1_1       | at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)

om1_1       | at java.base/java.util.Collections$2.tryAdvance(Collections.java:4745)

om1_1       | at java.base/java.util.Collections$2.forEachRemaining(Collections.java:4753)

om1_1       | at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)

om1_1       | at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)

om1_1       | at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)

om1_1       | at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)

om1_1       | at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)

om1_1       | at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getProtobuf(OmKeyInfo.java:362)

om1_1       | at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.toPersistedFormat(OmKeyInfoCodec.java:37)

om1_1       | at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.toPersistedFormat(OmKeyInfoCodec.java:31)

om1_1       | at org.apache.hadoop.hdds.utils.db.CodecRegistry.asRawData(CodecRegistry.java:68)

om1_1       | at org.apache.hadoop.hdds.utils.db.TypedTable.putWithBatch(TypedTable.java:125)

om1_1       | at org.apache.hadoop.ozone.om.response.key.OMKeyCreateResponse.addToDBBatch(OMKeyCreateResponse.java:58)

om1_1       | at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.lambda$flushTransactions$0(OzoneManagerDoubleBuffer.java:139)

om1_1       | at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)

om1_1       | at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:137)

om1_1       | at java.base/java.lang.Thread.run(Thread.java:834)",pull-request-available,[],HDDS,Task,Major,2019-10-18 01:06:00,13
13262932,Ozone Block Token verify should not apply to all datanode cmd,"DN container protocol has cmd send from SCM or other DN, which do not bear OM block token like OM client. We should restrict the OM Block token check only for those issued from OM client. ",pull-request-available,[],HDDS,Bug,Major,2019-10-17 19:15:58,28
13262927,Negative value seen for OM NumKeys Metric in JMX.,"While running teragen/terasort on a cluster and verifying number of keys created on Ozone Manager, I noticed that the value of NumKeys counter metric to be a negative value !Screen Shot 2019-10-17 at 11.31.08 AM.png! .

",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-17 18:39:27,30
13262855,Avoid proto::tostring in preconditions to save CPU cycles,"[https://github.com/apache/hadoop-ozone/blob/61f4aa30f502b34fd778d9b37b1168721abafb2f/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerProtocolServerSideTranslatorPB.java#L117]

 

This ends up converting proto toString in precondition checks and burns CPU cycles. {{request.toString()}} can be added in debug log on need basis.

 

 ",performance pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-17 12:44:56,13
13262617,Support to skip recon and/or ozonefs during the build,"(I almost use this Jira summary: ""Fast-lane to ozone build"" It was very hard to resist...)

 

 The two slowest part of Ozone build as of now:


 # The (multiple) shading of ozonefs
 # And the frontend build/obfuscation of ozone recon

[~aengineer] suggested to introduce options to skip them as they are not required for the build all the time.

This patch introduces '-DskipRecon' and '-DskipShade' options to provide a faster way to create a *partial* build.",pull-request-available,[],HDDS,New Feature,Major,2019-10-16 12:11:05,6
13262515,Duplicate release of lock in OMKeyCommitRequest,"{noformat}
om_1        | 2019-10-16 05:33:57,413 [IPC Server handler 19 on 9862] ERROR      - Trying to release the lock on /bypdd/mybucket4, which was never acquired.
om_1        | 2019-10-16 05:33:57,414 WARN ipc.Server: IPC Server handler 19 on 9862, call Call#4 Retry#8 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.29.0.4:37018
om_1        | java.lang.IllegalMonitorStateException: Releasing lock on resource /bypdd/mybucket4 without acquiring lock
om_1        | 	at org.apache.hadoop.ozone.lock.LockManager.getLockForReleasing(LockManager.java:220)
om_1        | 	at org.apache.hadoop.ozone.lock.LockManager.release(LockManager.java:168)
om_1        | 	at org.apache.hadoop.ozone.lock.LockManager.writeUnlock(LockManager.java:148)
om_1        | 	at org.apache.hadoop.ozone.om.lock.OzoneManagerLock.unlock(OzoneManagerLock.java:364)
om_1        | 	at org.apache.hadoop.ozone.om.lock.OzoneManagerLock.releaseWriteLock(OzoneManagerLock.java:329)
om_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCommitRequest.validateAndUpdateCache(OMKeyCommitRequest.java:177)
{noformat}",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2019-10-16 05:39:52,1
13262505,Fix typo in ozone command,"{noformat:title=ozone}
Usage: ozone [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
...
insight       tool to get runtime opeartion information
...
{noformat}

Should be ""operation"".",pull-request-available,['Ozone CLI'],HDDS,Bug,Trivial,2019-10-16 04:44:39,1
13262470,Fix logic of RetryPolicy in OzoneClientSideTranslatorPB,"OzoneManagerProtocolClientSideTranslatorPB.java

L251: if (cause instanceof NotLeaderException) {
 NotLeaderException notLeaderException = (NotLeaderException) cause;
 omFailoverProxyProvider.performFailoverIfRequired(
 notLeaderException.getSuggestedLeaderNodeId());
 return getRetryAction(RetryAction.RETRY, retries, failovers);
 }

 

The suggested leader returned from Server is not used during failOver, as the cause is a type of RemoteException. So with current code, it does not use suggested leader for failOver at all and by default with each OM, it tries max retries.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Blocker,2019-10-15 21:57:59,43
13262454,Add support to add ozone ranger plugin to Ozone Manager classpath,"Currently, there is no way to add Ozone Ranger plugin to Ozone Manager classpath. 

We should be able to set an environment variable that will be respected by ozone and added to Ozone Manager classpath.

 

 ",pull-request-available,['Ozone Manager'],HDDS,Task,Major,2019-10-15 20:41:36,37
13262333,Optimise OzoneManagerDoubleBuffer::flushTransactions to flush in batches,"When running a write heavy benchmark, {{{color:#000000}org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.flushTransactions{color}}} was invoked for pretty much every write.

This forces {{cleanupCache}} to be invoked which ends up choking in single thread executor. Attaching the profiler information which gives more details.

Ideally, {{flushTransactions}} should batch up the work to reduce load on rocksDB.

 

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java#L130]

 

[https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java#L322]

 

 ",TriagePending performance,['Ozone Manager'],HDDS,Bug,Major,2019-10-15 10:55:10,13
13262307,Switch to centos with the apache/ozone-build docker image,"I realized multiple JVM crashes in the daily builds:

 
{code:java}

ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter9018689154779946208.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7569723928289175829tmp surefire_947955725320624341206tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139
      
      
        [ERROR] Crashed tests:
      
      
        [ERROR] org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
      
      
        [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter5429192218879128313.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7227403571189445391tmp surefire_1011197392458143645283tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139
      
      
        [ERROR] Crashed tests:
      
      
        [ERROR] org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
      
      
        [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter1355604543311368443.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire3938612864214747736tmp surefire_933162535733309260236tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139
      
      
        [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
      
      
        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter9018689154779946208.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7569723928289175829tmp surefire_947955725320624341206tmp
      
      
        [ERROR] Error occurred in starting fork, check output in log
      
      
        [ERROR] Process Exit Code: 139 {code}
 

Based on the crash log (uploaded) it's related to the rocksdb JNI interface.

In the current ozone-build docker image (which provides the environment for build) we use alpine where musl libc is used instead of the main glibc. I think it would be more safe to use the same glibc what is used in production.

I tested with centos based docker image and it seems to be more stable. Didn't see any more JVM crashes.",pull-request-available,[],HDDS,Improvement,Major,2019-10-15 07:20:16,6
13262295,"ContextFactory.java contains Windows '^M"" at end of each line",Covert the file to Unix format. ,newbie,[],HDDS,Bug,Major,2019-10-15 06:15:46,31
13262187,Update Ozone to latest ratis snapshot(0.5.0-3f446aa-SNAPSHOT),"This jira will update ozone to latest ratis snapshot. for commit corresponding to 

{code}
commit 3f446aaf27704b0bf929bd39887637a6a71b4418 (HEAD -> master, origin/master, origin/HEAD)
Author: Tsz Wo Nicholas Sze <szetszwo@apache.org>
Date:   Fri Oct 11 16:35:38 2019 +0800

    RATIS-705. GrpcClientProtocolClient#close Interrupts itself.  Contributed by Lokesh Jain
{code}",pull-request-available,[],HDDS,Bug,Major,2019-10-14 17:11:58,18
13262155,Manage common pom versions in one common place,"Some of the versions (eg. ozone.version, hdds.version, ratis.version) are required for both ozone and hdds subprojects. As we have a common pom.xml it can be safer to manage them in one common place at the root pom.xml instead of managing them multiple times.",pull-request-available,['build'],HDDS,Improvement,Major,2019-10-14 12:56:11,6
13262151,Write path: Reduce read contention in rocksDB,"Benchmark: 
 ------------
 Simple benchmark which creates 100 and 1000s of keys (empty directory) in OM. This is done in a tight loop and multiple threads from client side to add enough load on CPU. Note that intention is to understand the bottlenecks in OM (intentionally avoiding interactions with SCM & DN).

Observation:
 -------------
 During write path, Ozone checks {{OMFileRequest.verifyFilesInPath}}. This internally calls {{omMetadataManager.getKeyTable().get(dbKeyName)}} for every write operation. This turns out to be expensive and chokes the write path.

[https://github.com/apache/hadoop/blob/trunk/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMDirectoryCreateRequest.java#L155]

[https://github.com/apache/hadoop/blob/trunk/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileRequest.java#L63]

In most of the cases, directory creation would be fresh entry. In such cases, it would be good to try with {{RocksDB::keyMayExist.}}

 

 ",performance,[],HDDS,Bug,Major,2019-10-14 12:48:11,88
13262135,Publish normalized Ratis metrics via the prometheus endpoint,"Latest Ratis contains very good metrics about the status of the ratis ring.

After RATIS-702 it will be possible to adjust the repoter of the Dropwizard based ratis metrics and export them directly to the /prom http endpoint (used by ozone insight and ratis).

Unfortunately Dropwizard is very simple, there is no tag support. All of the instance specific strings are part of the metric name. For example:
{code:java}
""ratis_grpc.log_appender.72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67@group""
 + ""-72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67""
 + "".grpc_log_appender_follower_75fa730a-59f0-4547""
 + ""-bd68-216162c263eb_latency"", {code}
In this patch I will use a simple method: during the export of the dropwizard metrics based on the well known format of the ratis metrics, they are converted to proper prometheus metrics where the instance information is included as tags:
{code:java}
ratis_grpc.log_appender.grpc_log_appender_follower_latency{instance=""72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67""}
 {code}
With this approach we can:

 1. monitor easily all the Ratis pipelines with one simple query

 2. Use the metrics for ozone insight which will show health state of the Ratis pipeline",pull-request-available,[],HDDS,Bug,Major,2019-10-14 10:41:46,6
13262120,BlockManager should allocate a block in excluded pipelines if none other left,"In SCM, BlockManager#allocateBlock does not allocate a block in the excluded pipelines or datanodes if requested by the client. But there can be cases where excluded pipelines and datanodes are the only ones left. In such a case SCM should allocate a block in such pipelines and return to the client. The client can choose to use or discard the block.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-10-14 09:45:56,35
13262119,Fix maven warning about duplicated metrics-core jar,"Maven build of Ozone is starting with a warning:
{code:java}
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-tools:jar:0.5.0-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: io.dropwizard.metrics:metrics-core:jar -> version 3.2.4 vs (?) @ line 94, column 17
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
 {code}
It's better to avoid it.",pull-request-available,['build'],HDDS,Bug,Major,2019-10-14 09:36:45,6
13262109,Enable Opentracing for new Freon tests,"HDDS-2022 introduced new freon tests, but the initial root span of opentracing is not created before the test execution. We need to enable opentracing to get better view about the executions of the new freon test.",pull-request-available,['freon'],HDDS,Improvement,Major,2019-10-14 09:08:55,6
13262105,ozoneperf compose cluster shouln't start freon by default,"During the original creation of the compose/ozoneperf we added an example freon execution to make it clean how the data can be generated. This freon process starts all the time when ozoneperf cluster is started (usually I notice it when my CPU starts to use 100% of the available resources).

Since the creation of this cluster definition we implemented multiple type of freon tests and it's hard predict which tests should be executed. I propose to remove the default execution of the random key generation but keep the opportunity to run any of the tests.",pull-request-available,['docker'],HDDS,Improvement,Major,2019-10-14 08:58:23,6
13262104,Display log of freon on the standard output,"HDDS-2042 disabled the console logging for all of the ozone command line tools including freon.

But freon is different, it has a different error handling model. For freon we need all the log on the console.

 1. To follow all the different errors

 2. To get information about the used (random) prefix which can be reused during the validation phase.

 

I propose to restore the original behavior for Ozone.",pull-request-available,[],HDDS,Improvement,Major,2019-10-14 08:42:08,6
13262103,Create a new HISTORY.md in the new repository,"During the apache/hadoop.git --> apache/hadoop-ozone.git move we rewrote the (git) history to simplify the work. Unfortunately some of the early work of HDDS-7280 is not part of the hadoop-ozone repository just the hadoop repository. As it suggested by Anu Engineer, we can explain this in a separated file and show how the origin of Ozone can be found.

 

cc [~aengineer]",pull-request-available,[],HDDS,Improvement,Major,2019-10-14 08:35:52,6
13262098,Create a new CONTRIBUTION.md for the new repository,Github supports CONTIRUTION.md which is displayed during the creation of a new Github PR. We can copy the content of the wiki page about how to contribut / how to build.,pull-request-available,[],HDDS,Improvement,Major,2019-10-14 07:59:16,6
13262097,Create Ozone specific README.md to the new hadoop-ozone repository,The current README is main Hadoop specific. We can create an ozone specific.,pull-request-available,[],HDDS,Improvement,Major,2019-10-14 07:58:32,6
13262023,Acceptance tests for OM HA,Add robot tests to test OM HA functionality.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-13 19:57:18,43
13262016,Rename pom.ozone.xml to pom.xml,"Since we have a separate git repository for Ozone now, we should rename {{pom.ozone.xml}} to {{pom.xml}}",pull-request-available,[],HDDS,Bug,Major,2019-10-13 18:58:41,19
13261986,Put testing information and a problem description to the github PR template,"This is suggested by [~aengineer] during an offline discussion to add more information to the github PR template based on the template of ambari (by Vivek):

https://github.com/apache/ambari/commit/579cec8cf5bcfe1a1a0feacf055ed6569f674e6a",pull-request-available,[],HDDS,Improvement,Major,2019-10-13 08:31:00,6
13261708,GetBlock and ReadChunk commands should be sent to the same datanode,"I can be observed that the GetBlock and ReadChunk command is sent to 2 different datanodes. It should be sent to the same datanode to re-use the connection.

{code}
19/10/10 00:43:42 INFO scm.XceiverClientGrpc: Send command GetBlock to datanode 172.26.32.224
19/10/10 00:43:42 INFO scm.XceiverClientGrpc: Send command ReadChunk to datanode 172.26.32.231
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-10-11 06:28:53,43
13261706,XceiverClientMetrics should be initialised as part of XceiverClientManager constructor,"XceiverClientMetrics is currently initialized in the read write path, the metric should be initialized while creating XceiverClientManager",TriagePending,['Ozone Client'],HDDS,Bug,Major,2019-10-11 06:25:19,18
13261705,Container creation on datanodes take time because of Rocksdb option creation.,"Container Creation on datanodes take around 300ms due to rocksdb creation. Rocksdb creation is taking a considerable time and this needs to be optimized.

Creating a rocksdb per disk should be enough and each container can be table inside the rocksdb.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-10-11 06:22:22,86
13261649,scmcli pipeline list command throws NullPointerException,"ozone scmcli pipeline list
{noformat}
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
	at org.apache.hadoop.hdds.scm.XceiverClientManager.<init>(XceiverClientManager.java:98)
	at org.apache.hadoop.hdds.scm.XceiverClientManager.<init>(XceiverClientManager.java:83)
	at org.apache.hadoop.hdds.scm.cli.SCMCLI.createScmClient(SCMCLI.java:139)
	at org.apache.hadoop.hdds.scm.cli.pipeline.ListPipelinesSubcommand.call(ListPipelinesSubcommand.java:55)
	at org.apache.hadoop.hdds.scm.cli.pipeline.ListPipelinesSubcommand.call(ListPipelinesSubcommand.java:30)
	at picocli.CommandLine.execute(CommandLine.java:1173)
	at picocli.CommandLine.access$800(CommandLine.java:141)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
	at org.apache.hadoop.hdds.scm.cli.SCMCLI.main(SCMCLI.java:101){noformat}",pull-request-available,[],HDDS,Bug,Major,2019-10-10 21:17:38,28
13261629,ContainerStateMachine#handleWriteChunk should ignore close container exception ,"Currently, ContainerStateMachine#applyTrannsaction ignores close container exception.Similarly,ContainerStateMachine#handleWriteChunk call also should ignore close container exception.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-10-10 18:40:46,16
13261549,HddsUtils#CheckForException should not return null in case the ratis exception cause is not set,"HddsUtils#CheckForException checks for the cause to be set properly to one of the defined/expected exceptions. In case, ratis throws up any runtime exception, HddsUtils#CheckForException can return null and lead to NullPointerException while write.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-10-10 11:37:44,16
13261472,Ozone S3 CLI commands not working on HA cluster,"ozone s3 getSecret

ozone s3 path are not working on OM HA cluster

 

Because these commands do not take URI as a parameter. And for shell in HA, passing URI is mandatory. 

 

Below is the output when running on OM HA cluster:

 
{code:java}
$ozone s3 getsecret
Service ID or host name must not be omitted when ozone.om.service.ids is defined.
{code}
 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-10 01:05:45,13
13261469,Run S3 test suite on OM HA cluster,"This will add a new compose setup with 3 OM's and start SCM, S3G, Datanode.

Run the existing test suite against this new docker-compose cluster.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-10 00:47:15,13
13261270,"In BatchOperation.SingleOperation, do not clone byte[]",byte[] is cloned once in the constructor and then it is cloned again in the getter methods.,pull-request-available,[],HDDS,Improvement,Major,2019-10-09 08:55:15,9
13261237,Avoid buffer copying in GrpcReplicationService,"In GrpcOutputStream, it writes data to a ByteArrayOutputStream and copies them to a ByteString.",performance pull-request-available,[],HDDS,Improvement,Major,2019-10-09 06:13:14,1
13261236,Avoid buffer copying in GrpcReplicationClient,"In StreamDownloader.onNext, CopyContainerResponseProto is copied to a byte[] and then it is written out to the stream.",performance pull-request-available,[],HDDS,Improvement,Major,2019-10-09 06:10:42,1
13261235,Avoid buffer copying in KeyValueHandler,"- In handleGetSmallFile, it first reads chunk data to a   byte[] and the copy them to a ByteString.
- In handlePutBlock/handleGetBlock, in order to get the length, it (1) builds a ContainerProtos.BlockData and then copies it to a byte[].",pull-request-available,[],HDDS,Improvement,Major,2019-10-09 06:05:20,9
13261234,Avoid buffer copying in ContainerStateMachine.loadSnapshot/persistContainerSet,"ContainerStateMachine:
- In loadSnapshot(..), it first reads the snapshotFile to a  byte[] and then parses it to ContainerProtos.Container2BCSIDMapProto.  The buffer copying can be avoided.
{code}
    try (FileInputStream fin = new FileInputStream(snapshotFile)) {
      byte[] container2BCSIDData = IOUtils.toByteArray(fin);
      ContainerProtos.Container2BCSIDMapProto proto =
          ContainerProtos.Container2BCSIDMapProto
              .parseFrom(container2BCSIDData);
      ...
    }
{code}

- persistContainerSet(..) has similar problem.",performance pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-10-09 05:49:45,1
13261233,Provide config for fair/non-fair for OM RW Lock,"Provide config in OzoneManager Lock for fair/non-fair for OM RW Lock.

Created based on review comments during HDDS-2244.",pull-request-available,[],HDDS,Improvement,Major,2019-10-09 05:12:34,13
13261192,Incorrect container checksum upon downgrade,"Container file checksum is calculated based on all YAML fields in a given Ozone version.  If the same container file is used in older Ozone, which has fewer fields, the expected checksum will be different.

Example: origin pipeline ID and origin node ID were added for HDDS-837 in Ozone 0.4.0.  Starting Ozone 0.3.0 with the same data results in checksum error.

{noformat}
datanode_1  | ... ERROR ContainerReader:166 - Failed to parse ContainerFile for ContainerID: 1
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container checksum error for ContainerID: 1.
datanode_1  | Stored Checksum: 7a6ec508d6e3796c5fe5fd52574b3d3437b0a0eaa4e053f7a96a5e39f4abb374
datanode_1  | Expected Checksum: fee023a02d3ced2f7b0b42c116cce5f03da6b57b29965ca878dc46d1213230b6
datanode_1  | 	at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.verifyChecksum(ContainerUtils.java:259)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerData(ContainerReader.java:180)
datanode_1  | 	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:142)
{noformat}",upgrade,['Ozone Datanode'],HDDS,Bug,Major,2019-10-08 20:45:48,30
13261161,Container metadata scanner interval mismatch,"Container metadata scanner can be configured to run at specific time intervals, eg. hourly ({{hdds.containerscrub.metadata.scan.interval}}).  However, the actual run interval does not match the configuration.  After a datanode restart, it runs in quick succession, later it runs at apparently random intervals.

{noformat:title=sample log}
datanode_1  | 2019-10-08 14:05:30 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 1, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 14:09:33 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 1, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
...
datanode_1  | 2019-10-08 14:09:33 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 28, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 14:21:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 29, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 14:21:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 30, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 15:30:38 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 31, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
datanode_1  | 2019-10-08 16:45:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 32, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0
{noformat}

The problem is that time elapsed is measured in nanoseconds, while the configuration is in milliseconds.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-10-08 17:51:31,1
13261031,Avoid evaluation of LOG.trace and LOG.debug statement in the read/write path (Ozone),"LOG.trace and LOG.debug with logging information will be evaluated even when debug/trace logging is disabled. This jira proposes to wrap all the trace/debug logging with
LOG.isDebugEnabled and LOG.isTraceEnabled to prevent the logging.",pull-request-available,"['Ozone CLI', 'Ozone Manager']",HDDS,Improvement,Major,2019-10-08 05:11:00,86
13260867,integration.sh may report false negative,"Sometimes integration test run gets killed, and {{integration.sh}} incorrectly reports ""success"".  Example:

{noformat:title=https://github.com/elek/ozone-ci-q4/tree/ae930d6f7f10c7d2aeaf1f2f21b18ada954ea444/pr/pr-hdds-2259-hlwmv/integration/result}
success
{noformat}

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/ae930d6f7f10c7d2aeaf1f2f21b18ada954ea444/pr/pr-hdds-2259-hlwmv/integration/output.log#L2457}
/workdir/hadoop-ozone/dev-support/checks/integration.sh: line 22:   369 Killed                  mvn -B -fn test -f pom.ozone.xml -pl :hadoop-ozone-integration-test,:hadoop-ozone-filesystem,:hadoop-ozone-tools -Dtest=\!TestMiniChaosOzoneCluster ""$@""
{noformat}",pull-request-available,"['build', 'test']",HDDS,Bug,Major,2019-10-07 11:34:25,1
13260863,Improve output of TestOzoneContainer,"TestOzoneContainer#testContainerCreateDiskFull fails intermittently (HDDS-2263), but test output does not reveal too much about the reason.  The goal of this task is to improve the assertion/output to make it easier to fix the failure.",pull-request-available,['test'],HDDS,Improvement,Minor,2019-10-07 10:59:34,1
13260852,SLEEP_SECONDS: command not found,"{noformat}
datanode_1  | /opt/hadoop/bin/docker/entrypoint.sh: line 66: SLEEP_SECONDS: command not found
datanode_1  | Sleeping for  seconds
{noformat}

Eg. https://raw.githubusercontent.com/elek/ozone-ci-q4/master/pr/pr-hdds-2238-79fll/acceptance/docker-ozonesecure-ozonesecure-s3-s3g.log",pull-request-available,['docker'],HDDS,Bug,Trivial,2019-10-07 10:06:18,1
13260829,Change readChunk methods to return ByteBuffer,"During refactoring to HDDS-2233 I realized the following:
KeyValueHandler.handleReadChunk and handleGetSmallFile methods are using ChunkManager.readChunk, which returns a byte[], but then both of them (the only usage points) converts the returning byte[] to a ByteBuffer, and then to a ByteString.
ChunkManagerImpl on the other hand in readChunk utilizes ChunkUtils.readChunk, which in order to conform the return value converts a ByteBuffer back to a byte[].

I open this JIRA to change the internal logic to fully rely on ByteBuffers instead of converting from ByteBuffer to byte[] then to ByteBuffer again.",pull-request-available,[],HDDS,Improvement,Major,2019-10-07 08:02:43,63
13260808,Avoid evaluation of LOG.trace and LOG.debug statement in the read/write path (HDDS),"LOG.trace and LOG.debug with logging information will be evaluated even when debug/trace logging is disabled. This jira proposes to wrap all the trace/debug logging with 
LOG.isDebugEnabled and LOG.isTraceEnabled to prevent the logging.",pull-request-available,"['Ozone Client', 'Ozone Datanode']",HDDS,Bug,Major,2019-10-07 03:40:42,86
13260747,Container Data Scrubber computes wrong checksum,"Chunk checksum verification fails for (almost) any file.  This is caused by computing checksum for the entire buffer, regardless of the actual size of the chunk.

{code:title=https://github.com/apache/hadoop/blob/55c5436f39120da0d7dabf43d7e5e6404307123b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerCheck.java#L259-L273}
            byte[] buffer = new byte[cData.getBytesPerChecksum()];
...
                v = fs.read(buffer);
...
                bytesRead += v;
...
                ByteString actual = cal.computeChecksum(buffer)
                    .getChecksums().get(0);
{code}

This results in marking all closed containers as unhealthy.",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Critical,2019-10-06 06:44:08,1
13260645,Fix checkstyle issues in ChecksumByteBuffer,"hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBuffer.java
 84: Inner assignments should be avoided.
 85: Inner assignments should be avoided.
 101: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 102: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 103: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 104: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 105: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 106: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 107: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.
 108: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.",newbie pull-request-available,[],HDDS,Bug,Major,2019-10-04 21:11:06,37
13260613,Improve Acl Handler Messages,"In Add/Remove/Set Acl Key/Bucket/Volume Handlers, we print a message about whether the operation was successful or not. If we are trying to add an ACL which is already existing, we convey the message that the operation failed. It would be better if the message conveyed more clearly why the operation failed i.e. the ACL already exists. ",newbie pull-request-available,['Ozone Manager'],HDDS,Improvement,Minor,2019-10-04 17:35:34,31
13260611,Fix flaky unit testTestContainerStateMachine#testRatisSnapshotRetention,"Test always fails with assertion error:

{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.ozone.client.rpc.TestContainerStateMachine.testRatisSnapshotRetention(TestContainerStateMachine.java:188)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-10-04 17:33:04,30
13260581,Enable gdpr robot test in daily build,"As reported by [~elek] in https://github.com/apache/hadoop/pull/1542#pullrequestreview-297424033

""One thing what I found, I think it's not yet enabled in the daily builds.

I think in the hadoop-ozone/dist/src/main/compose/ozone/test.sh we need a new line:

execute_robot_test gdpr.robot""",pull-request-available,['test'],HDDS,Sub-task,Major,2019-10-04 14:21:18,81
13260578,Add an option to customize unit.sh and integration.sh parameters,"hadoop-ozone/dev-support/checks/unit.sh (and same with integration) provides an easy entrypoint to execute all the unit/integration test. But in same cases it would be great to use the script but further specify the scope of the test.

With this simple patch it will be possible to adjust the surefire parameters.",pull-request-available,[],HDDS,Task,Major,2019-10-04 14:00:31,6
13260565,Generated configs missing from ozone-filesystem-lib jars,"Hadoop 3.1 and 3.2 acceptance tests started failing with HDDS-1720, which added a new, annotated configuration class.

The [change itself|https://github.com/apache/hadoop/pull/1538/files] looks fine.  The problem is that the packaging process for {{ozone-filesystem-lib}} jars keeps only 1 or 2 {{ozone-default-generated.xml}} files.  With the new config in place, client configs are missing, so Ratis client gets evicted immediately due to {{scm.container.client.idle.threshold}} = 0.  This results in NPE:

{code:title=https://elek.github.io/ozone-ci-q4/pr/pr-hdds-1720-trunk-rd9ht/acceptance/summary.html#s1-s5-t1-k2-k2}
Running command 'hdfs dfs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/ozone-14607
...
-put: Fatal internal error
java.lang.NullPointerException: client is null
	at java.util.Objects.requireNonNull(Objects.java:228)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.getClient(XceiverClientRatis.java:208)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:234)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:332)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:310)
...
{code}",pull-request-available,"['build', 'Ozone Filesystem']",HDDS,Bug,Critical,2019-10-04 12:22:46,1
13260502,Delete FileEncryptionInfo from KeyInfo when a Key is deleted,"As part of HDDS-2174 we are deleting GDPR Encryption Key on delete file operation.
However, if KMS is enabled, we are skipping GDPR Encryption Key approach when writing file in a GDPR enforced Bucket.

{code:java}
final FileEncryptionInfo feInfo = keyOutputStream.getFileEncryptionInfo();
    if (feInfo != null) {
      KeyProvider.KeyVersion decrypted = getDEK(feInfo);
      final CryptoOutputStream cryptoOut =
          new CryptoOutputStream(keyOutputStream,
              OzoneKMSUtil.getCryptoCodec(conf, feInfo),
              decrypted.getMaterial(), feInfo.getIV());
      return new OzoneOutputStream(cryptoOut);
    } else {
      try{
        GDPRSymmetricKey gk;
        Map<String, String> openKeyMetadata =
            openKey.getKeyInfo().getMetadata();
        if(Boolean.valueOf(openKeyMetadata.get(OzoneConsts.GDPR_FLAG))){
          gk = new GDPRSymmetricKey(
              openKeyMetadata.get(OzoneConsts.GDPR_SECRET),
              openKeyMetadata.get(OzoneConsts.GDPR_ALGORITHM)
          );
          gk.getCipher().init(Cipher.ENCRYPT_MODE, gk.getSecretKey());
          return new OzoneOutputStream(
              new CipherOutputStream(keyOutputStream, gk.getCipher()));
        }
      }catch (Exception ex){
        throw new IOException(ex);
      }
{code}

In such scenario, when KMS is enabled & GDPR enforced on a bucket, if user deletes a file, we should delete the {{FileEncryptionInfo}} from KeyInfo, before moving it to deletedTable, else we cannot guarantee Right to Erasure.",pull-request-available,[],HDDS,Sub-task,Major,2019-10-04 05:38:27,81
13260497,Use dynamic ports for SCM in TestSecureOzoneCluster,"{{TestSecureOzoneCluster}} is using default SCM ports, we should use dynamic ports.",newbie,['test'],HDDS,Bug,Major,2019-10-04 05:15:37,104
13260495,Use new ReadWrite lock in OzoneManager,Use new ReadWriteLock added in HDDS-2223.,pull-request-available,[],HDDS,Improvement,Major,2019-10-04 04:58:52,13
13260436,Avoid unnecessary rpc needed to discover the pipeline leader ,HDDS-1868 adds the leaderID to pipeline protobuf. Client should use this to stream requests directly to the leader vs current implementation of randomly choosing one of the datanodes in the pipeline and relying on the follower to tell the client which node is the leader.,pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-10-03 20:42:08,86
13260435,Optimize the refresh pipeline logic used by KeyManagerImpl to obtain the pipelines for a key,"Currently, while looking up a key, the Ozone Manager gets the pipeline information from SCM through an RPC for every block in the key. For large files > 1GB, we may end up making a lot of RPC calls for this. This can be optimized in a couple of ways

* We can implement a batch getContainerWithPipeline API in SCM using which we can get the pipeline info locations for all the blocks for a file. To keep the number of containers passed in to SCM in a single call, we can have a fixed container batch size on the OM side. _Here, Number of calls = 1 (or k depending on batch size)_
* Instead, a simpler change would be to have a map (method local) of ContainerID -> Pipeline that we get from SCM so that we don't need to make repeated calls to SCM for the same containerID for a key. _Here, Number of calls = Number of unique containerIDs_",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-10-03 20:38:38,30
13260360,Command line tool for OM Admin,"A command line tool (*ozone omha*) to get information related to OM HA. 
This Jira proposes to add the _getServiceState_ option for OM HA which lists all the OMs in the service and their corresponding Ratis server roles (LEADER/ FOLLOWER). 
We can later add more options to this tool.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-03 18:53:13,43
13260356,Fix TestOzoneFsHAUrls,[https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2162-pj84x/integration/hadoop-ozone/ozonefs/org.apache.hadoop.fs.ozone.TestOzoneFsHAURLs.txt],pull-request-available,[],HDDS,Bug,Major,2019-10-03 18:35:12,1
13260334,Container Data Scrubber spams log in empty cluster,"In an empty cluster (without closed containers) logs are filled with messages from completed data scrubber iterations (~3600 per second for me), if Container Scanner is enabled ({{hdds.containerscrub.enabled=true}}), eg.:

{noformat}
datanode_1  | 2019-10-03 15:43:57 INFO  ContainerDataScanner:114 - Completed an iteration of container data scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 6763, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0
{noformat} 

Also CPU usage is quite high.

I think:

# there should be a small sleep between iterations
# it should log only if any containers were scanned",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-10-03 15:51:25,1
13260331,KeyDeletingService throws NPE if it's started too early,"1. OzoneManager starts KeyManager

2. KeyManager starts KeyDeletingService

3. KeyDeletingService uses OzoneManager.isLeader()

4. OzoneManager.isLeader() uses omRatisServer

5. omRatisServer can be null (bumm)

 

Now the initialization order in OzoneManager:

 

new KeymanagerServer() *Includes start()!!!!*

omRatisServer initialization

start() (includes KeyManager.start())

 

The solution seems to be easy: start the key manager only from the OzoneManager.start() and not from the OzoneManager.instantiateServices()",pull-request-available,['Ozone Manager'],HDDS,Task,Major,2019-10-03 15:17:43,6
13260159,Ozone Datanode web page doesn't exist,"On trying to access the dn UI, the following error is seen.

http://dn_ip:9882/

{code}
HTTP ERROR 403
Problem accessing /. Reason:

    Forbidden
{code}",Triaged,['Ozone Datanode'],HDDS,Bug,Major,2019-10-02 18:12:26,97
13260158,rat.sh fails due to ozone-recon-web/build files,"[ERROR] After correcting the problems, you can resume the build with the command
[ERROR] mvn <goals> -rf :hadoop-ozone-recon
[INFO] Build failures were ignored.
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/index.html
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/2.8943d5a3.chunk.css
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/2.8943d5a3.chunk.css.map
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/main.96eebd44.chunk.css
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/runtime~main.a8a9905a.js.map
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/runtime~main.a8a9905a.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/2.ea549bfe.chunk.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/main.5bb53989.chunk.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/2.ea549bfe.chunk.js.map
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/precache-manifest.1d05d7a103ee9d6b280ef7adfcab3c01.js
hadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/service-worker.js",pull-request-available,[],HDDS,Bug,Major,2019-10-02 17:58:07,1
13260148,Remove ByteStringHelper and refactor the code to the place where it used,"See HDDS-2203 where there is a race condition reported by me.

Later in the discussion we agreed that it is better to refactor the code and remove the class completely for now, and that would also resolve the race condition.",pull-request-available,[],HDDS,Bug,Critical,2019-10-02 17:01:17,63
13260111,ozone-build docker image is failing due to a missing entrypoint script,"We have a dedicated apache/ozone-build image which contains all of the required build and test tools to build ozone.

Unfortunately  it's not working as one shell script was not added to the original patch.

This patch (to the hadoop-docker-ozone repo!!) remove the requirement of the entrypoint.sh (no more docker in docker)

 

And installs additional tools (blockade, kubectl, mailsend)

 ",pull-request-available,"['build', 'docker']",HDDS,Task,Major,2019-10-02 13:05:43,6
13260085,test-single.sh cannot copy results,"Previously {{result}} directory was created by simply {{source}}-ing {{testlib.sh}}, but HDDS-2185 changed it to avoid lost results.  {{test-single.sh}} needs to be adjusted accordingly.

{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone
$ docker-compose up -d --scale datanode=3
$ ../test-single.sh scm basic/basic.robot
...
invalid output path: directory ""hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone/result"" does not exist
{noformat}",pull-request-available,['docker'],HDDS,Bug,Minor,2019-10-02 10:35:04,1
13260052,Invalid entries in ozonesecure-mr config,"Some of the entries in {{ozonesecure-mr/docker-config}} are in invalid format, thus they end up missing from the generated config files.

{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozonesecure-mr
$ ./test.sh # configs are generated during container startup
$ cd ../..

$ grep -c 'ozone.administrators' compose/ozonesecure-mr/docker-config
1
$ grep -c 'ozone.administrators' etc/hadoop/ozone-site.xml
0

$ grep -c 'yarn.timeline-service' compose/ozonesecure-mr/docker-config
5
$ grep -c 'yarn.timeline-service' etc/hadoop/yarn-site.xml
2

$ grep -c 'container-executor' compose/ozonesecure-mr/docker-config
3
$ grep -c 'container-executor' etc/hadoop/yarn-site.xml
0
{noformat}",pull-request-available,['docker'],HDDS,Bug,Minor,2019-10-02 07:01:48,1
13260009,Fix NPE in OzoneDelegationTokenManager#addPersistedDelegationToken,"The certClient was not initialized in proper order as a result, when OM restart with delegation token issued, the ozone delegation token secret manager NPE. ",pull-request-available,[],HDDS,Improvement,Blocker,2019-10-01 22:35:48,28
13260005,GDPR key generation could benefit from secureRandom,"The SecureRandom can be used for the symetric key for GDPR. While GDPR is not a security feature, this is a good to have optional feature.

I want to thank Jonathan Leitschuh, for originally noticing this issue and reporting it.
",pull-request-available,[],HDDS,Sub-task,Major,2019-10-01 21:41:36,102
13260004,S3 Secrets should use a strong RNG,"The S3 token generation under ozone should use a strong RNG. 

I want to thank Jonathan Leitschuh, for originally noticing this issue and reporting it.
",pull-request-available,['S3'],HDDS,Improvement,Major,2019-10-01 21:40:09,102
13259992,SCM fails to start in most unsecure environments due to leftover secure config,"Intermittent failure of {{ozone-recon}} and some other acceptance tests where SCM container is not available is caused by leftover secure config in {{core-site.xml}}.

Initially the config file is [empty|https://raw.githubusercontent.com/apache/hadoop/trunk/hadoop-hdds/common/src/main/conf/core-site.xml].  Various test environments populate it with different settings.  The problem happens when a test does not specify any config for {{core-site.xml}}, in which case the previous test's config file is retained.

{code}
scm_1       | 2019-10-01 19:42:05 WARN  WebAppContext:531 - Failed startup of context o.e.j.w.WebAppContext@1cc680e{/,file:///tmp/jetty-0.0.0.0-9876-scm-_-any-1272594486261557815.dir/webapp/,UNAVAILABLE}{/scm}
scm_1       | javax.servlet.ServletException: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/HTTP.keytab
scm_1       | 	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)
...
scm_1       | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:791)
...
scm_1       | Unable to initialize WebAppContext
scm_1       | 2019-10-01 19:42:05 INFO  StorageContainerManagerStarter:51 - SHUTDOWN_MSG:
scm_1       | /************************************************************
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at 8724df7131bb/192.168.128.6
scm_1       | ************************************************************/
{code}

The problem is intermittent due to ordering of test cases being different in different runs.  If a secure test is run earlier, more tests are affected.  If secure tests are run last, the issue does not happen.",pull-request-available,['docker'],HDDS,Bug,Major,2019-10-01 20:24:08,1
13259968,Fix loadup cache for cache cleanup policy NEVER,"During initial startup/restart of OM, if table has cache cleanup policy set to NEVER, we fill the table cache and also epochEntries. We do not need to add entries to epochEntries, as the epochEntries is used for eviction from the cache, once double buffer flushes to disk.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-10-01 17:34:35,13
13259944,Support ReadWrite lock in LockManager,"Currently {{LockManager}} is using exclusive lock, instead we should support {{ReadWrite}} lock.",pull-request-available,[],HDDS,Improvement,Major,2019-10-01 15:37:11,19
13259930,Add a method to update ByteBuffer in PureJavaCrc32/PureJavaCrc32C,PureJavaCrc32 and PureJavaCrc32C implement java.util.zip.Checksum which provides only methods to update byte and byte[].  We propose to add a method to update ByteBuffer.,pull-request-available,[],HDDS,Improvement,Major,2019-10-01 14:27:41,9
13259925,Monitor datanodes in ozoneperf compose cluster,"ozoneperf compose cluster contains a prometheus but as of now it collects the data only from scm and om.

We don't know the exact number of datanodes (can be scaled up and down) therefor it's harder to configure the datanode host names. I would suggest to configure the first 10 datanodes (which covers most of the use cases)

How to test?
{code:java}
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozoneperf
docker-compose up -d
firefox http://localhost:9090/targets
  {code}
 ",pull-request-available,['docker'],HDDS,Task,Major,2019-10-01 14:15:39,6
13259917,HddsVolume needs a toString method,"This is logged to the console of datanodes:
{code:java}
2019-10-01 11:37:59 INFO  HddsVolumeChecker:202 - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 11:52:59 INFO  ThrottledAsyncChecker:139 - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 11:52:59 INFO  HddsVolumeChecker:202 - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:07:59 INFO  ThrottledAsyncChecker:139 - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:07:59 INFO  HddsVolumeChecker:202 - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:22:59 INFO  ThrottledAsyncChecker:139 - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:22:59 INFO  HddsVolumeChecker:202 - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:37:59 INFO  ThrottledAsyncChecker:139 - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:37:59 INFO  HddsVolumeChecker:202 - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:52:59 INFO  ThrottledAsyncChecker:139 - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a
2019-10-01 12:52:59 INFO  HddsVolumeChecker:202 - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5460cf3a {code}
Without a proper HddsVolume.toString it's hard to say which volume is checked...

 ",newbie pull-request-available,[],HDDS,Task,Major,2019-10-01 13:53:40,31
13259916,Move all the ozone dist scripts/configs to one location,"The hadoop distribution tar file contains jar files scripts and default configuration files.

The scripts and configuration files are stored in multiple separated projects without any reason:
{code:java}
ls hadoop-hdds/common/src/main/bin/
hadoop-config.cmd  hadoop-config.sh  hadoop-daemons.sh  hadoop-functions.sh  workers.sh

ls hadoop-ozone/common/src/main/bin 
ozone  ozone-config.sh  start-ozone.sh  stop-ozone.sh

ls hadoop-ozone/common/src/main/shellprofile.d 
hadoop-ozone.sh

ls hadoop-ozone/dist/src/main/conf 
dn-audit-log4j2.properties  log4j.properties  om-audit-log4j2.properties  ozone-shell-log4j.properties  ozone-site.xml  scm-audit-log4j2.properties
 {code}
All of these scripts can be moved to the hadoop-ozone/dist/src/shell

hadoop-ozone/dist/dev-support/bin/dist-layout-stitching also should be updated to copy all of them to the right place in the tar.

 ",newbe pull-request-available,['build'],HDDS,Task,Major,2019-10-01 13:42:16,31
13259915,Use OZONE_CLASSPATH instead of HADOOP_CLASSPATH,"HADOOP_CLASSPATH is the standard way to add additional jar files to the classpath of the mapreduce/spark/.. .jobs. If something is added to the HADOOP_CLASSPATH, than it should be on the classpath of the classic hadoop daemons.

But for the Ozone components we don't need any new jar files (cloud connectors, libraries). I think it's more safe to separated HADOOP_CLASSPATH from OZONE_CLASSPATH. If something is really need on the classpath for Ozone daemons the dedicated environment variable should be used.

 

Most probably it can be fixed in

hadoop-hdds/common/src/main/bin/hadoop-functions.sh

And the hadoop-ozone/dev/src/main/compose files also should be checked (some of them contain HADOOP_CLASSPATH",TriagePending newbie,['docker'],HDDS,Task,Major,2019-10-01 13:38:13,97
13259912,Remove log4j and audit configuration from the docker-config files,"Log4j configuration lines are added to the docker-config under hadoop-ozone/dist/src/main/compose/...

Mainly to make it easier to reconfigure the log level of any components.

As we already have a ""ozone insight"" tool which can help us to modify the log level at runtime we don't need these lines any more.
{code:java}

LOG4J.PROPERTIES_log4j.rootLogger=INFO, stdout
LOG4J.PROPERTIES_log4j.appender.stdout=org.apache.log4j.ConsoleAppender
LOG4J.PROPERTIES_log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
LOG4J.PROPERTIES_log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n
LOG4J.PROPERTIES_log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
LOG4J.PROPERTIES_log4j.logger.org.apache.ratis.conf.ConfUtils=WARN
LOG4J.PROPERTIES_log4j.logger.org.apache.hadoop.security.ShellBasedUnixGroupsMapping=ERROR
LOG4J.PROPERTIES_log4j.logger.org.apache.ratis.grpc.client.GrpcClientProtocolClient=WARN
LOG4J.PROPERTIES_log4j.logger.http.requests.s3gateway=INFO,s3gatewayrequestlog
LOG4J.PROPERTIES_log4j.appender.s3gatewayrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
LOG4J.PROPERTIES_log4j.appender.s3gatewayrequestlog.Filename=/tmp/jetty-s3gateway-yyyy_mm_dd.log
LOG4J.PROPERTIES_log4j.appender.s3gatewayrequestlog.RetainDays=3 {code}
We can remove them together with the audit log entries as we already have a default log4j.propertes / audit log4j2 config.

After the remove the clusters should be tested: Ozone CLI should not print and confusing log messages (such as NativeLib is missing or anything else). AFAIK they are already turned off in the etc/hadoop/etc log4j.properties.

 

 ",newbie pull-request-available,['docker'],HDDS,Task,Major,2019-10-01 13:28:22,99
13259911,Rename HADOOP_RUNNER_VERSION to OZONE_RUNNER_VERSION in compose .env files,"In HDDS-1698 we replaced our apache/hadoop-runner base image to apache/ozone-runner base image. 

The version of the image is set by the .env files under the hadoop-ozone/dist/src/main/compose directories
{code:java}
cd hadoop-ozone/dist/src/main/compose
grep -r HADOOP_RUNNER .
./ozoneperf/docker-compose.yaml:      image: apache/ozone-runner:${HADOOP_RUNNER_VERSION}
./ozoneperf/docker-compose.yaml:      image: apache/ozone-runner:${HADOOP_RUNNER_VERSION}
./ozoneperf/docker-compose.yaml:      image: apache/ozone-runner:${HADOOP_RUNNER_VERSION}
 {code}
But the name of the variable is HADOOP_RUNNER_VERSION instead of OZONE_RUNNER_VERSION.

Would be great to rename it the OZONE_RUNNER_VERSION.",newbie pull-request-available,['docker'],HDDS,Task,Major,2019-10-01 13:23:46,31
13259852,TestSCMContainerPlacementRackAware has an intermittent failure,"For example from the nightly build:
{code:java}
  <testcase name=""testNoFallback[8]"" classname=""org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware"" time=""0.014"">
      
      
            <failure type=""java.lang.AssertionError"">java.lang.AssertionError
   
      
        	at org.junit.Assert.fail(Assert.java:86)
      
      
        	at org.junit.Assert.assertTrue(Assert.java:41)
      
      
        	at org.junit.Assert.assertTrue(Assert.java:52)
      
      
        	at org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware.testNoFallback(TestSCMContainerPlacementRackAware.java:276)
      
      
        	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      
      
        	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      
      
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      
      
        	at java.lang.reflect.Method.invoke(Method.java:498)
      
      
        	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 {code}
The problem is in the testNoFallback:

Let's say we have 11 nodes (from parameter) and we would like to choose 5 nodes (hard coded in the test).

As the first two replicas are chosen from the same rack an all the other from different racks it's not possible, so we except a failure.

But we have an assertion that the success count is at least 3. But this is true only if the first two replicas are placed to the rack1 (5 nodes) or rack2 (5nodes). If the replica is placed to the rack3 (one node) it will fail immediately:

 

Lucky case when we have success count > 3
{code:java}
 rack1 -- node1 
 rack1 -- node2 -- FIRST replica
 rack1 -- node3 -- SECOND replica
 rack1 -- node4
 rack1 -- node5 
 rack2 -- node6
 rack2 -- node7 -- THIRD replica
 rack2 -- node8
 rack2 -- node9 
 rack2 -- node10
 rack3 -- node11 -- FOURTH replica{code}
 The specific case when we have success count == 1, as we can't choose the second replica on rack3 (This is when the test is failing)
{code:java}
 rack1 -- node1 
 rack1 -- node2
 rack1 -- node3
 rack1 -- node4
 rack1 -- node5 
 rack2 -- node6
 rack2 -- node7
 rack2 -- node8
 rack2 -- node9 
 rack2 -- node10
 rack3 -- node11 -- FIRST replica{code}
 

 

 

 

 

 ",pull-request-available,[],HDDS,Improvement,Major,2019-10-01 09:28:39,6
13259774,Reduce key provider loading log level in OzoneFileSystem#getAdditionalTokenIssuers,"OzoneFileSystem#getAdditionalTokenIssuers log an error when secure client tries to collect ozone delegation token to run MR/Spark jobs but ozone file system does not have a kms provider configured. In this case, we simply return null provider here in the code below. This is a benign error and we should reduce the log level to debug level.

{code:java}

KeyProvider keyProvider;
 try {

  keyProvider = getKeyProvider(); }

catch (IOException ioe) {

  LOG.error(""Error retrieving KeyProvider."", ioe);

  return null;

}
{code}",pull-request-available,[],HDDS,Improvement,Minor,2019-09-30 22:53:34,98
13259765,Genconf tool should generate config files for secure cluster setup,"Ozone Genconf tool currently generates a minimal ozone-site.xml file.

[~raje2411] was trying out a secure ozone setup over existing HDP-2.x cluster and found the config set up was not as straight forward.

This jira proposes to extend the Genconf tool so we can generate required template config files for a secure setup.",newbie pull-request-available,['Tools'],HDDS,Improvement,Major,2019-09-30 21:38:20,7
13259738,Collect docker logs if env fails to start,"Occasionally some acceptance test docker environment fails to start up properly.  We need docker logs for analysis, but they are not being collected.

https://github.com/elek/ozone-ci-q4/blob/master/trunk/trunk-nightly-extra-20190930-74rp4/acceptance/output.log#L3765-L3768",pull-request-available,['test'],HDDS,Improvement,Major,2019-09-30 18:28:26,1
13259734,ContainerStateMachine should not be marked unhealthy if applyTransaction fails with closed container exception,"Currently, if applyTransaction fails, the stateMachine is marked unhealthy and next snapshot creation will fail. As a result of which the the raftServer will close down leading to pipeline failure. ClosedContainer exception should be ignored while marking the stateMachine unhealthy.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-30 17:52:34,16
13259624,Update Ratis to latest snapshot,This Jira aims to update ozone with latest ratis snapshot which has a crtical fix for retry behaviour on getting not leader exception in client.,pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-09-30 09:37:59,16
13259596,checkstyle.sh reports wrong failure count,"{{checkstyle.sh}} outputs files with checkstyle violations and the violations themselves on separate lines.  It then reports line count as number of failures.

{code:title=target/checkstyle/summary.txt}
hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java
 49: Unused import - org.apache.hadoop.ozone.om.OMMetadataManager.
{code}

{code:title=target/checkstyle/failures}
2
{code}",pull-request-available,['test'],HDDS,Bug,Trivial,2019-09-30 07:32:21,1
13259568,Avoid buffer coping in checksum verification,"In Checksum.verifyChecksum(ByteString, ..), it first converts the ByteString to a byte array.  It lead to an unnecessary buffer coping.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-09-30 02:46:13,9
13259407,Remove unused import in OmUtils,"Fix hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java

Remove L49: Unused import - org.apache.hadoop.ozone.om.OMMetadataManager;

 ",newbie pull-request-available,[],HDDS,Bug,Minor,2019-09-28 05:34:16,81
13259393,Rename VolumeList to UserVolumeInfo,"Under Ozone Manager, The volume points to a structure called volumeInfo, Bucket points to BucketInfo, Key points to KeyInfo. However, User points to VolumeList. duh?

This JIRA proposes to refactor the VolumeList as UserVolumeInfo. Why not, UserInfo, because that structure is already taken by the security work of Ozone Manager.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-09-28 00:33:16,102
13259334,Recon does not handle the NULL snapshot from OM DB cleanly.,"{code}
2019-09-27 11:35:19,835 [pool-9-thread-1] ERROR      - Null snapshot location got from OM.
2019-09-27 11:35:19,839 [pool-9-thread-1] INFO       - Calling reprocess on Recon tasks.
2019-09-27 11:35:19,840 [pool-7-thread-1] INFO       - Starting a 'reprocess' run of ContainerKeyMapperTask.
2019-09-27 11:35:20,069 [pool-7-thread-1] INFO       - Creating new Recon Container DB at /tmp/recon/db/recon-container.db_1569609319840
2019-09-27 11:35:20,069 [pool-7-thread-1] INFO       - Cleaning up old Recon Container DB at /tmp/recon/db/recon-container.db_1569609258721.
2019-09-27 11:35:20,144 [pool-9-thread-1] ERROR      - Unexpected error :
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.reInitializeTasks(ReconTaskControllerImpl.java:181)
        at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:333)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.ozone.recon.tasks.ContainerKeyMapperTask.reprocess(ContainerKeyMapperTask.java:81)
        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$reInitializeTasks$3(ReconTaskControllerImpl.java:176)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
{code}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-09-27 18:57:59,30
13259319,In SCMNodeManager dnsToUuidMap cannot track multiple DNs on the same host,"Often in test clusters and tests, we start multiple datanodes on the same host.

In SCMNodeManager.register() there is a map of hostname -> datanode UUID called dnsToUuidMap.

If several DNs register from the same host, the entry in the map will be overwritten and the last DN to register will 'win'.

This means that the method getNodeByAddress() does not return the correct DatanodeDetails object when many hosts are registered from the same address.

This method is only used in SCMBlockProtocolServer.sortDatanodes() to allow it to see if one of the nodes matches the client, but it need to be used by the Decommission code.

Perhaps we could change the getNodeByAddress() method to returns a list of DNs? In normal production clusters, there should only be one returned, but in test clusters, there may be many. Any code looking for a specific DN entry would need to iterate the list and match on the port number too, as host:port would be the unique definition of a datanode.",pull-request-available,[],HDDS,Bug,Major,2019-09-27 17:26:09,11
13259286,SCM should not consider containers in CLOSING state to come out of safemode,"There are cases where SCM can be stuck in safemode for ever if it considers containers in CLOSING state for coming out of safemode

* If there are 5 containers in OPEN state inside SCM 
* Out of 5, 3 containers are created in datanodes by the client.
* 2 containers are yet to be created in datanodes
* Due to some pipeline issue, pipeline close action is sent.
* All 5 container's state are changed from OPEN to CLOSING in SCM.
* Eventually , 3 container's state moves from CLOSING to CLOSED in SCM as the datanodes closes those containers.
* 2 of the containers are still in CLOSING state.
* SCM is restarted.
* SCM will never gets container reports for the containers which were in CLOSING state as those containers were never created in datanodes.
* SCM will remain in safemode.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-09-27 14:26:27,19
13259239,Extend SCMCLI Topology command to print node Operational States,"The scmcli topology command only consider the node health (healthy, stale or dead). With decommission and maintenance stages, we need to also consider the operational states and display them with this command.",pull-request-available,"['SCM', 'SCM Client']",HDDS,Sub-task,Major,2019-09-27 10:50:24,11
13259238,Add CLI Commands and Protobuf messages to trigger decom states,"To all nodes to be decommissioned, recommissioned and put into maintenance, we need a few commands.

These will be added to the existing ""scm cli"". 3 commands are proposed:

Decommission:

ozone scmcli dnadmin decommission hosta hostb hostc:port ...

Put nodes into maintenance:

osone scmcli dnadmin maintenance hosta hostb hostc:port ... <-endHours>

Take nodes out of maintenance or halt decommission:

ozone scmcli dnadmin recommission hosta hostb hostc:port

These 3 commands will call 3 new protobuf messages and they will be part of the ""StorageContainerLocationProtocol"":
 * DecommissionNodesRequestProto
 * RecommissionNodesRequestProto
 * StartMaintenanceNodesRequestProto

In additional a new class NodeDecommissionManager will be introduced that will receive these commands and carry out the decommission steps.

In this patch NodeDecommissionManager is only a skeleton implementation to receive the commands as this patch is mainly focused on getting the CLI commands and protobuf messages in place.",pull-request-available,"['SCM', 'SCM Client']",HDDS,Sub-task,Major,2019-09-27 10:46:49,11
13259188,Apply spotbugs check to test code,"The goal of this task is to [enable Spotbugs to run on test code|https://spotbugs.github.io/spotbugs-maven-plugin/spotbugs-mojo.html#includeTests], and fix all issues it reports (both to improve code and to avoid breaking CI).",newbie pull-request-available,['test'],HDDS,Improvement,Major,2019-09-27 06:15:33,7
13259166,"Replication of Container fails with ""Only closed containers could be exported""","Replication of Container fails with ""Only closed containers could be exported""

cc: [~nanda]

{code}
2019-09-26 15:00:17,640 [grpc-default-executor-13] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(57)) - Streaming container data (37) to other
datanode
Sep 26, 2019 3:00:17 PM org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor run
SEVERE: Exception while executing runnable org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed@70e641f2
java.lang.IllegalStateException: Only closed containers could be exported: ContainerId=37
2019-09-26 15:00:17,644 [grpc-default-executor-17] ERROR replication.GrpcReplicationClient (GrpcReplicationClient.java:onError(142)) - Container download was unsuccessfull
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:527)
org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNKNOWN
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.exportContainer(KeyValueHandler.java:875)
        at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.exportContainer(ContainerController.java:134)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
        at org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
.java:64)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClient at org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolSCallListener.java:40)
erviceGrpc.java:217)
        at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
        at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls. at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
java:171)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClient at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
CallListener.java:40)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.ja at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
va:397)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
        at java.lang.Thread.run(Thread.java:748)

        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
:
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-27 01:17:59,13
13259144,Adding container related metrics in SCM,"This jira aims to add more container related metrics to SCM.
 Following metrics will be added as part of this jira:
 * Number of successful create container calls
 * Number of failed create container calls
 * Number of successful delete container calls
 * Number of failed delete container calls
 * Number of list container ops.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-09-26 22:21:11,13
13259120,Optimize Ozone CLI commands to send one ACL request to authorizers instead of sending multiple requests,"Currently, when trying to read a key, three requests are sent to the authorizer:
volume read, bucket read, key read.

 

It should instead be just one request to the authorizer.",TriagePending,['Ozone CLI'],HDDS,Bug,Major,2019-09-26 19:08:08,37
13259091,Datanode should send PipelineAction on RaftServer failure,"{code:java}
2019-09-26 08:03:07,152 ERROR org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker: 664c4e90-08f3-46c9-a073-c93ef2a55da3@group-93F633896F08-SegmentedRaftLogWorker hit exception
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:694)
        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
        at org.apache.ratis.server.raftlog.segmented.BufferedWriteChannel.<init>(BufferedWriteChannel.java:41)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogOutputStream.<init>(SegmentedRaftLogOutputStream.java:72)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$StartLogSegment.execute(SegmentedRaftLogWorker.java:566)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:289)
        at java.lang.Thread.run(Thread.java:748)
2019-09-26 08:03:07,155 INFO org.apache.ratis.server.impl.RaftServerImpl: 664c4e90-08f3-46c9-a073-c93ef2a55da3@group-93F633896F08: shutdown
{code}
On RaftServer shutdown datanode should send a PipelineAction denoting that the pipeline has been closed exceptionally in the datanode.",TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2019-09-26 15:47:16,35
13259087,Implement LocatedFileStatus & getFileBlockLocations to provide node/localization information to Yarn/Mapreduce,"For applications like Hive/MapReduce to take advantage of the data locality in Ozone, Ozone should return the location of the Ozone blocks. This is needed for better read performance for Hadoop Applications.
{code}
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
{code}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-09-26 15:22:57,30
13259080,"ozone-mr test fails with No FileSystem for scheme ""o3fs""","HDDS-2101 changed how Ozone filesystem provider is configured.  {{ozone-mr}} tests [started failing|https://github.com/elek/ozone-ci/blob/2f2c99652af6b26a95f08eece9e545f0d72ccf45/pr/pr-hdds-2101-rtz55/acceptance/output.log#L255-L263], but it [wasn't noticed|https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-2101-rtz55/acceptance/result] due to HDDS-2185.

{code}
Running command 'ozone fs -mkdir /user'
${output} = mkdir: No FileSystem for scheme ""o3fs""
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-09-26 14:55:46,1
13259068,Fix tests using MiniOzoneCluster for its memory related exceptions,"After multi-raft usage, MiniOzoneCluster seems to be fishy and reports a bunch of 'out of memory' exceptions in ratis. Attached sample stacks.

 

2019-09-26 15:12:22,824 [2e1e11ca-833a-4fbc-b948-3d93fc8e7288@group-218F3868CEA9-SegmentedRaftLogWorker] ERROR segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(323)) - 2e1e11ca-833a-4fbc-b948-3d93fc8e7288@group-218F3868CEA9-SegmentedRaftLogWorker hit exception2019-09-26 15:12:22,824 [2e1e11ca-833a-4fbc-b948-3d93fc8e7288@group-218F3868CEA9-SegmentedRaftLogWorker] ERROR segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(323)) - 2e1e11ca-833a-4fbc-b948-3d93fc8e7288@group-218F3868CEA9-SegmentedRaftLogWorker hit exceptionjava.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:694) at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) at org.apache.ratis.server.raftlog.segmented.BufferedWriteChannel.<init>(BufferedWriteChannel.java:41) at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogOutputStream.<init>(SegmentedRaftLogOutputStream.java:72) at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$StartLogSegment.execute(SegmentedRaftLogWorker.java:566) at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:289) at java.lang.Thread.run(Thread.java:748)

 

which leads to:

2019-09-26 15:12:23,029 [RATISCREATEPIPELINE1] ERROR pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$null$2(181)) - Failed invoke Ratis rpc org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider$$Lambda$297/1222454951@55d1e990 for c1f4d375-683b-42fe-983b-428a63aa88032019-09-26 15:12:23,029 [RATISCREATEPIPELINE1] ERROR pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$null$2(181)) - Failed invoke Ratis rpc org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider$$Lambda$297/1222454951@55d1e990 for c1f4d375-683b-42fe-983b-428a63aa8803org.apache.ratis.protocol.TimeoutIOException: deadline exceeded after 2999881264ns at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:82) at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75) at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:178) at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:147) at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:94) at org.apache.ratis.client.impl.RaftClientImpl.sendRequest(RaftClientImpl.java:278) at org.apache.ratis.client.impl.RaftClientImpl.groupAdd(RaftClientImpl.java:205) at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.lambda$initializePipeline$1(RatisPipelineProvider.java:142) at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.lambda$null$2(RatisPipelineProvider.java:177) at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291) at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401) at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734) at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583) at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.lambda$callRatisRpc$3(RatisPipelineProvider.java:171) at java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2999881264ns at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233) at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214) at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139) at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:274) at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$3(GrpcClientProtocolClient.java:149) at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:176) ... 25 more",flaky-test ozone-flaky-test,['test'],HDDS,Sub-task,Major,2019-09-26 14:22:05,77
13259059,createmrenv failure not reflected in acceptance test result,"Part of the MR tests fail, but it's not reflected in the test report, which shows all green.

{noformat:title=https://github.com/elek/ozone-ci/blob/679228c146628cd4d1a416e1ffc9c513d19fb43d/pr/pr-hdds-2179-9bnxk/acceptance/output.log#L718-L730}
==============================================================================
hadoop31-createmrenv :: Create directories required for MR test               
==============================================================================
Create test volume, bucket and key                                    | PASS |
------------------------------------------------------------------------------
Create user dir for hadoop                                            | FAIL |
1 != 0
------------------------------------------------------------------------------
hadoop31-createmrenv :: Create directories required for MR test       | FAIL |
2 critical tests, 1 passed, 1 failed
2 tests total, 1 passed, 1 failed
==============================================================================
Output:  /tmp/smoketest/hadoop31/result/robot-hadoop31-hadoop31-createmrenv-scm.xml
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-09-26 13:45:03,1
13259017,Rename ozone scmcli to ozone admin,"Originally ozone scmcli designed to be used only by the developers. A very cryptic name is chosen intentionally to frighten away the beginner users.

As we realized recently we started to use ""ozone scmcli"" as a generic admin tool. More and more tools has been added which are useful not only for the developers but for the administrators.

Therefore I suggest to rename ""ozone scmcli"" to something more meaningful.

For example to ""ozone admin""

 ",pull-request-available,[],HDDS,Improvement,Major,2019-09-26 10:50:29,6
13259015,Container and pipline subcommands of scmcli should be grouped,"Once upon an time when we had only a few subcommands under `ozone scmcli` to manage containers.

 

Now we have many admin commands some of them are grouped to a subcommand (eg. safemode, replicationmanager) some of are not.

 

I propose to group the container and pipeline related commands:

 

Instead of ""ozone scmcli info"" use ""ozone scmcli container info""

Instead of ""ozone scmcli list"" use ""ozone scmcli container list""

Instead of ""ozone scmcli listPipelines"" use ""ozone scmcli pipeline list""

 

And so on...",pull-request-available,[],HDDS,Improvement,Major,2019-09-26 10:40:19,31
13258924,Ozone Manager should send correct ACL type in ACL requests to Authorizer,"Currently, Ozone manager sends ""WRITE"" as ACLType for key create, key delete and bucket create operation. Fix the acl type in all requests to the authorizer.",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-09-25 23:54:24,37
13258902,Add Object ID and update ID on VolumeList Object,This JIRA proposes to add Object ID and Update IDs to the Volume List Object.,pull-request-available,[],HDDS,Improvement,Major,2019-09-25 21:42:57,102
13258884,ConfigFileGenerator fails with Java 10 or newer,"{code:title=mvn -f pom.ozone.xml -DskipTests -am -pl :hadoop-hdds-config clean package}
...
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hadoop-hdds-config ---
[INFO] Compiling 3 source files to hadoop-hdds/config/target/test-classes
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-hdds-config: Compilation failure
[ERROR] Can't generate the config file from annotation: hadoop-hdds/config/target/test-classes/ozone-default-generated.xml
{code}

The root cause is that new Java (I guess it's 9+, but tried only on 10+) throws a different {{IOException}} subclass: {{NoSuchFileException}} instead of {{FileNotFoundException}}.

{code}
java.nio.file.NoSuchFileException: hadoop-hdds/config/target/test-classes/ozone-default-generated.xml
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:374)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:425)
	at java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420)
	at java.base/java.nio.file.Files.newInputStream(Files.java:159)
	at jdk.compiler/com.sun.tools.javac.file.PathFileObject.openInputStream(PathFileObject.java:461)
	at java.compiler@13/javax.tools.ForwardingFileObject.openInputStream(ForwardingFileObject.java:74)
	at org.apache.hadoop.hdds.conf.ConfigFileGenerator.process(ConfigFileGenerator.java:62)
{code}
",pull-request-available,['build'],HDDS,Bug,Minor,2019-09-25 19:40:32,1
13258678,Delete GDPR Encryption Key from metadata when a Key is deleted,"As advised by [~arp]  & [~aengineer], when a deleteKey command is executed, delete the gdpr encryption key details from key metadata before moving it to deletedTable",pull-request-available,[],HDDS,Sub-task,Major,2019-09-25 02:44:40,81
13258559,Dangling links in test report due to incompatible realpath,"Test summaries point to wrong locations, eg.:

{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/trunk/trunk-nightly-20190924-mj2km/integration/summary.md}
 * [org.apache.hadoop.ozone.scm.node.TestQueryNode](/tmp/log/trunk/trunk-nightly-20190924-mj2km/integration/workdir/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestQueryNode.txt) ([output](/tmp/log/trunk/trunk-nightly-20190924-mj2km/integration/workdir/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestQueryNode-output.txt/))
{code}

shouldn't include {{/workdir}}, nor {{/tmp/log/}}.

The root cause is that Busybox {{realpath}} does not accept options, rather returns absolute path:

{code:title=elek/ozone-build:20190825-1}
$ cd /etc
$ realpath --relative-to=$(pwd) motd
realpath: --relative-to=/etc: No such file or directory
/etc/motd
{code}

It worked previously because the docker image [was|https://github.com/elek/argo-ozone/commit/bad4b6747fa06c227dfcbff1f098f8d9c8179b79] based on a more complete Linux.

{code:title=elek/ozone-build:test}
$ cd /etc
$ realpath --relative-to=$(pwd) motd
motd
{code}

CC [~elek]",pull-request-available,['build'],HDDS,Bug,Major,2019-09-24 14:01:58,1
13258415,Add Object IDs and Update ID to Volume Object,This patch proposes to add object ID and update ID when a volume is created. ,pull-request-available,[],HDDS,Improvement,Major,2019-09-23 23:17:41,102
13258269,Avoid buffer copies while submitting client requests in Ratis,"Currently, while sending write requests to Ratis from ozone, a protobuf object containing data encoded  and then resultant protobuf is again converted to a byteString which internally does a copy of the buffer embedded inside the protobuf again so that it can be submitted over to Ratis client. Again, while sending the appendRequest as well while building up the appendRequestProto, it might be again copying the data. The idea here is to provide client so pass the raw data(stateMachine data) separately to ratis client without copying overhead. 

 
{code:java}
private CompletableFuture<RaftClientReply> sendRequestAsync(
    ContainerCommandRequestProto request) {
  try (Scope scope = GlobalTracer.get()
      .buildSpan(""XceiverClientRatis."" + request.getCmdType().name())
      .startActive(true)) {
    ContainerCommandRequestProto finalPayload =
        ContainerCommandRequestProto.newBuilder(request)
            .setTraceID(TracingUtil.exportCurrentSpan())
            .build();
    boolean isReadOnlyRequest = HddsUtils.isReadOnly(finalPayload);

//  finalPayload already has the byteString data embedded. 
    ByteString byteString = finalPayload.toByteString(); -----> It involves a copy again.
    if (LOG.isDebugEnabled()) {
      LOG.debug(""sendCommandAsync {} {}"", isReadOnlyRequest,
          sanitizeForDebug(finalPayload));
    }
    return isReadOnlyRequest ?
        getClient().sendReadOnlyAsync(() -> byteString) :
        getClient().sendAsync(() -> byteString);
  }
}
{code}",pull-request-available,[],HDDS,Improvement,Major,2019-09-23 11:05:53,9
13258401,TestOzoneManagerDoubleBufferWithOMResponse sometimes fails with out of memory error,"testDoubleBuffer() in TestOzoneManagerDoubleBufferWithOMResponse fails with outofmemory exceptions at times in dev machines.

 ",pull-request-available,['Ozone Manager'],HDDS,Task,Major,2019-09-23 21:37:32,37
13258339,Hadoop31-mr acceptance test is failing due to the shading,"From the daily build:

{code}
 	Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/ozone/shaded/org/apache/http/client/utils/URIBuilder
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:138)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.CommandWithDestination.getRemoteDestination(CommandWithDestination.java:195)
	at org.apache.hadoop.fs.shell.CopyCommands$Put.processOptions(CopyCommands.java:259)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:175)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.ozone.shaded.org.apache.http.client.utils.URIBuilder
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 15 more
{code}

It can be reproduced locally with executing the tests:

{code}
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31
./test.sh
{code}",pull-request-available,[],HDDS,Bug,Major,2019-09-23 15:11:16,6
13258303,Some RPC metrics are missing from SCM prometheus endpoint,"In Hadoop metrics it's possible to register multiple metrics with the same name but with different tags. For example each RpcServere has an own metrics instance in SCM.

{code}
    ""name"" : ""Hadoop:service=StorageContainerManager,name=RpcActivityForPort9860"",
    ""name"" : ""Hadoop:service=StorageContainerManager,name=RpcActivityForPort9863"",
{code}

They are converted by PrometheusSink to a prometheus metric line with proper name and tags. For example:

{code}
rpc_rpc_queue_time60s_num_ops{port=""9860"",servername=""StorageContainerLocationProtocolService"",context=""rpc"",hostname=""72736061cbc5""} 0
{code}

The PrometheusSink uses a Map to cache all the recent values but unfortunately the key contains only the name (rpc_rpc_queue_time60s_num_ops in our example) but not the tags (port=...)

For this reason if there are multiple metrics with the same name, only the first one will be displayed.

As a result in SCM only the metrics of the first RPC server can be exported to the prometheus endpoint. 
",pull-request-available,[],HDDS,Bug,Major,2019-09-23 13:54:11,6
13258288,Freon fails if bucket does not exists,"{code:title=ozone freon ockg}
Bucket not found
...
Failures: 0
Successful executions: 0
{code}",pull-request-available,['Tools'],HDDS,Bug,Major,2019-09-23 13:18:00,1
13258155,om.db.checkpoints is getting filling up fast,"{{om.db.checkpoints}} is filling up fast, we should also clean this up.",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2019-09-22 15:39:42,30
13258060,"Add ""Replication factor"" to the output of list keys ","The output of ""ozone sh key list /vol1/bucket1"" does not include replication factor and it will be good to have it in the output.",pull-request-available,['Ozone CLI'],HDDS,Task,Major,2019-09-20 23:26:55,37
13258055,Make OM Generic related configuration support HA style config,"To have a single configuration to use across OM cluster, few of the configs like 

-OZONE_OM_KERBEROS_KEYTAB_FILE_KEY,-

-OZONE_OM_KERBEROS_PRINCIPAL_KEY,-

-OZONE_OM_HTTP_KERBEROS_KEYTAB_FILE,-

-OZONE_OM_HTTP_KERBEROS_PRINCIPAL_KEY need to support configs which append with service id and node id.-

 

Addressed OM_DB_DIRS, OZONE_OM_ADDRESS_KEY also in this patch.

 

This Jira is to fix the above configs.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-09-20 23:08:03,13
13258042,Create RepeatedKeyInfo structure to be saved in deletedTable,"Currently, OM Metadata deletedTable stores <Key Name, OMKeyInfo>

When a user deletes a Key, <key name, omKeyInfo> is moved to deletedTable.

If a user creates and deletes key with exact same name in quick succession repeatedly, then old <key name, omKeyInfo> can get overwritten and we may be left with dangling blocks.

To address this, currently we append delete timestamp to keyname and preserve the multiple delete attempts for same key name.

However, for GDPR compliance we need a way to check if a key is deleted from deletedTable and thus given the above explanation, we may not get accurate information and it must also confuse the users.

 

This Jira aims to:
 # Create new structure RepeatedKeyInfo which allows us to group multiple KeyInfo which can be saved to deletedTable corresponding to a keyname as <KeyName, RepeatedKeyInfo>
 # Due to this, before we move a key to deletedTable, we need to check if key with same name exists. If yes, then fetch the existing instance and add the latest key to the list, store it back to deletedTable, else create a new instance and save to table",pull-request-available,[],HDDS,Sub-task,Major,2019-09-20 21:46:39,81
13258040,Add acceptance test for ozonesecure-mr compose,This will give us coverage of running basic MR jobs on security enabled OZONE cluster against YARN. ,pull-request-available,[],HDDS,Bug,Major,2019-09-20 21:24:41,28
13258034,Fix Race condition in ProfileServlet#pid,There is a race condition in ProfileServlet. The Servlet member field pid should not be used for local assignment. It could lead to race condition.,pull-request-available,[],HDDS,Bug,Major,2019-09-20 20:03:28,43
13258023,Fix Json Injection in JsonUtils,JsonUtils#toJsonStringWithDefaultPrettyPrinter() does not validate the Json String  before serializing it which could result in Json Injection.,pull-request-available,[],HDDS,Bug,Major,2019-09-20 19:02:02,43
13258018,checkstyle: print filenames relative to project root,"Currently {{checkstyle.sh}} prints files with violations using full path, eg:

{noformat:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190920-4x9x8/checkstyle/summary.txt}
...
/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadList.java
 23: Unused import - org.apache.hadoop.hdds.client.ReplicationType.
 24: Unused import - org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor.
/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadListParts.java
 23: Unused import - org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType.
/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartKeyInfo.java
 19: Unused import - org.apache.hadoop.hdds.client.ReplicationFactor.
 20: Unused import - org.apache.hadoop.hdds.client.ReplicationType.
 26: Unused import - java.time.Instant.
...
{noformat}

{{/workdir}} is specific to the CI environment.  Similarly, local checkout directory is specific to each developer.

Printing only path relative to project root ({{/workdir}} here) would make handling these paths easier (eg. reporting errors in JIRA or opening files locally for editing).",pull-request-available,['build'],HDDS,Improvement,Minor,2019-09-20 18:43:15,1
13257783,Fix alignment issues in HDDS doc pages,The cards in HDDS doc pages don't align properly and needs to be fixed.,pull-request-available,['documentation'],HDDS,Bug,Major,2019-09-19 21:21:15,37
13257634,Fix Checkstyle issues,"Unfortunately checkstyle checks didn't work well from HDDS-2106 to HDDS-2119. 

This patch fixes all the issues which are accidentally merged in the mean time. ",pull-request-available,[],HDDS,Improvement,Major,2019-09-19 10:06:35,6
13257571,Ozone client fails with OOM while writing a large (~300MB) key.,"{code}
dd if=/dev/zero of=testfile bs=1024 count=307200
ozone sh key put /vol1/bucket1/key testfile
{code}

{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) at org.apache.hadoop.hdds.scm.storage.BufferPool.allocateBufferIfNeeded(BufferPool.java:66) at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:234) at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129) at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:211) at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193) at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49) at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96) at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:117) at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:55) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141)
{code}",TriagePending,['Ozone Client'],HDDS,Bug,Major,2019-09-19 05:12:02,16
13257570,Ozone client prints the entire request payload in DEBUG level.,"In XceiverClientRatis.java:221, we have the following snippet where we have a DEBUG line that prints out the entire Container Request proto. 

{code}
      ContainerCommandRequestProto finalPayload =
          ContainerCommandRequestProto.newBuilder(request)
              .setTraceID(TracingUtil.exportCurrentSpan())
              .build();
      boolean isReadOnlyRequest = HddsUtils.isReadOnly(finalPayload);
      ByteString byteString = finalPayload.toByteString();
      LOG.debug(""sendCommandAsync {} {}"", isReadOnlyRequest, finalPayload);
      return isReadOnlyRequest ?
          getClient().sendReadOnlyAsync(() -> byteString) :
          getClient().sendAsync(() -> byteString);
{code}

This causes OOM while writing large (~300MB) keys. 

{code}
SLF4J: Failed toString() invocation on an object of type [org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos$ContainerCommandRequestProto]
Reported exception:
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3332)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649)
	at java.lang.StringBuilder.append(StringBuilder.java:202)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormatEscaper.escapeBytes(TextFormatEscaper.java:75)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormatEscaper.escapeBytes(TextFormatEscaper.java:94)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.escapeBytes(TextFormat.java:1836)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:436)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:376)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:338)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.print(TextFormat.java:325)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:449)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:376)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:338)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.print(TextFormat.java:325)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.access$000(TextFormat.java:307)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.print(TextFormat.java:68)
	at org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.printToString(TextFormat.java:148)
	at org.apache.ratis.thirdparty.com.google.protobuf.AbstractMessage.toString(AbstractMessage.java:117)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)
	at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)
	at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:252)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:221)
	at org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:302)
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:310)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:601)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:459)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:240)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129)
SLF4J: Failed toString() invocation on an object of type [org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos$ContainerCommandRequestProto]
Reported exception:
java.lang.OutOfMemoryError: Java heap space
{code}",pull-request-available,[],HDDS,Bug,Major,2019-09-19 05:07:10,1
13257519,Update dependency versions to avoid security vulnerabilities,"The following dependency versions have known security vulnerabilities. We should update them to recent/ later versions.
 * Apache Thrift 0.11.0
 * Apache Zookeeper 3.4.13
 * Jetty Servlet 9.3.24",pull-request-available,[],HDDS,Bug,Major,2019-09-18 21:37:00,43
13257511,Replace findbugs with spotbugs,"Findbugs has been marked deprecated and all future work is now happening under SpotBugs project.

This Jira is to investigate and possibly transition to Spotbugs in Ozone

 

Ref1 - [https://mailman.cs.umd.edu/pipermail/findbugs-discuss/2017-September/004383.html]

Ref2 - [https://spotbugs.github.io/]

 

A turn off for developers is that IntelliJ does not yet have a plugin for Spotbugs - [https://youtrack.jetbrains.com/issue/IDEA-201846]",pull-request-available,[],HDDS,Improvement,Major,2019-09-18 21:17:50,1
13257453,Remove redundant code in CreateBucketHandler.java,"{code:java}
if (isVerbose()) {
      System.out.printf(""Volume Name : %s%n"", volumeName);
      System.out.printf(""Bucket Name : %s%n"", bucketName);
      if (bekName != null) {
        bb.setBucketEncryptionKey(bekName);
        System.out.printf(""Bucket Encryption enabled with Key Name: %s%n"",
            bekName);
      }
    }
{code}

This jira aims to remove the redundant line {{bb.setBucketEncryptionKey(bekName);}} as the same operation is performed in the preceding code block. This code block is to print additional details if verbose option was specified.",pull-request-available,['Ozone CLI'],HDDS,Bug,Minor,2019-09-18 18:24:41,81
13257434,Include dumpstream in test report,"Include {{*.dumpstream}} in the unit test report, which may help finding out the cause of {{Corrupted STDOUT}} warning of forked JVM.

{noformat:title=https://github.com/elek/ozone-ci/blob/5429d0982c3b13d311ec353dba198f2f5253757c/pr/pr-hdds-2141-4zm8s/unit/output.log#L333-L334}
[INFO] Running org.apache.hadoop.utils.TestMetadataStore
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /workdir/hadoop-hdds/common/target/surefire-reports/2019-09-18T12-58-05_531-jvmRun1.dumpstream
{noformat}",pull-request-available,['Tools'],HDDS,Improvement,Minor,2019-09-18 16:45:22,1
13257427,Optimize block write path performance by reducing no of watchForCommit calls,"Currently, the watchForCommit calls from client to Ratis server for All replicated semantics happens when the max buffer limit is reached which can potentially be called 4 times as per the default configs for a single full block write. The idea here is inspect and add optimizations to reduce the no of watchForCommit calls.",TriagePending,['Ozone Client'],HDDS,Bug,Major,2019-09-18 16:24:45,16
13257236,MR job failing on secure Ozone cluster,"Failing with below error:
Caused by: Client cannot authenticate via:[TOKEN, KERBEROS]
org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)
at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)
at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
at org.apache.hadoop.ipc.Client.call(Client.java:1403)
at org.apache.hadoop.ipc.Client.call(Client.java:1367)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
at com.sun.proxy.$Proxy79.submitRequest(Unknown Source)
at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy79.submitRequest(Unknown Source)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:332)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1163)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
at com.sun.proxy.$Proxy80.getServiceList(Unknown Source)
at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:248)
at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:167)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)
at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)
at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:102)
at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:155)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:268)
at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)",pull-request-available,[],HDDS,Bug,Blocker,2019-09-17 23:24:22,13
13257232,Rename classes under package org.apache.hadoop.utils,"Rename classes under package org.apache.hadoop.utils -> org.apache.hadoop.hdds.utils in hadoop-hdds-common

 

Now, with current way, we might collide with hadoop classes.",pull-request-available,[],HDDS,Bug,Major,2019-09-17 22:40:49,13
13257087,OM metrics mismatch (abort multipart request),AbortMultipartUpload failure count can be higher than request count.,pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-17 11:28:54,1
13257063,Missing total number of operations,Total number of operations is missing from some metrics graphs.,pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-17 08:59:03,1
13257031,Add robot test for GDPR feature,Add robot test for GDPR feature so it can be run during smoke tests.,pull-request-available,['test'],HDDS,Sub-task,Major,2019-09-17 06:09:39,81
13257000,Update BeanUtils and Jackson Databind dependency versions,"The following Ozone dependencies have known security vulnerabilities. We should update them to newer/ latest versions.
 * Apache Common BeanUtils version 1.9.3
 * Fasterxml Jackson version 2.9.5",pull-request-available,[],HDDS,Bug,Major,2019-09-17 01:16:53,43
13256936,OM bucket operations do not add up,"Total OM bucket operations may be higher than sum of counts for individual operation type, because S3 bucket operations are displayed in separate charts.",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-16 18:00:49,1
13256924,HddsClientUtils and OzoneUtils have duplicate verifyResourceName(),HddsClientUtils and OzoneUtils can share the method to verify resource name that verifies if the bucket/volume name is a valid DNS name.,pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-09-16 17:25:05,105
13256866,OM block allocation metric not paired with its failures,Block allocation count and block allocation failure count are shown in separate graphs.,pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-16 12:25:54,1
13256838,OM Metric mismatch (MultipartUpload failures),"{{incNumCommitMultipartUploadPartFails()}} increments {{numInitiateMultipartUploadFails}} instead of the counter for commit failures.

https://github.com/apache/hadoop/blob/85b1c728e4ed22f03db255f5ef34a2a79eb20d52/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java#L310-L312",pull-request-available,['Ozone Manager'],HDDS,Bug,Minor,2019-09-16 10:18:10,1
13256832,OM Metrics graphs include empty request type,"Ozone Manager Metrics seems to include an odd empty request type ""s"".",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-09-16 09:42:51,1
13256684,TestKeyValueContainer is failing,"{{TestKeyValueContainer}} is failing with the following exception 
{noformat}
[ERROR] testContainerImportExport(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.173 s  <<< ERROR!
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:201)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:500)
	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testContainerImportExport(TestKeyValueContainer.java:235)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-09-14 09:43:48,1
13256657,Optimize replication type and creation time calculation in S3 MPU list call,"Based on the review from [~bharatviswa]:

{code}
 hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
                  metadataManager.getOpenKeyTable();

              OmKeyInfo omKeyInfo =
                  openKeyTable.get(upload.getDbKey());
{code}

{quote}Here we are reading openKeyTable only for getting creation time. If we can have this information in omMultipartKeyInfo, we could avoid DB calls for openKeyTable.

To do this, We can set creationTime in OmMultipartKeyInfo during initiateMultipartUpload . In this way, we can get all the required information from the MultipartKeyInfo table.

And also StorageClass is missing from the returned OmMultipartUpload, as listMultipartUploads shows StorageClass information. For this, if we can return replicationType and depending on this value, we can set StorageClass in the listMultipartUploads Response.
{quote}",pull-request-available,[],HDDS,Improvement,Major,2019-09-14 02:51:11,86
13256632,Using dist profile fails with pom.ozone.xml as parent pom,The build fails with the {{dist}} profile. Details in a comment below.,pull-request-available,[],HDDS,Bug,Major,2019-09-13 22:20:12,6
13256625,Make ozone sh command work with OM HA service ids,Now that HDDS-2007 is committed. I can use some common helper function to make this work.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-09-13 22:05:23,12
13256599,Detailed Tools doc not reachable,"There are two doc pages for tools:
 * docs/beyond/tools.html
 * docs/tools.html

The latter is more detailed (has subpages for several tools), but it is not reachable (even indirectly) from the start page.  Not sure if this is intentional.

On a related note, it has two ""Testing tools"" sub-pages. One of them is empty and should be removed.",pull-request-available,['documentation'],HDDS,Bug,Major,2019-09-13 19:14:16,6
13256523,Ozone 0.4.1 branch build issue,"{{ozone=0.4.1}} branch build is failing with below error
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-ozone-integration-test: Compilation failure
[ERROR] /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestContainerReplicationEndToEnd.java:[202,9] cannot find symbol
[ERROR]   symbol:   method getBlockCommitSequenceId()
[ERROR]   location: class org.apache.hadoop.ozone.container.common.impl.ContainerData
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-09-13 13:01:56,19
13256518,maven-javadoc-plugin.version is missing in pom.ozone.xml,{{maven-javadoc-plugin.version}} is missing from {{pom.ozone.xml}} which is causing build failure.,pull-request-available,[],HDDS,Bug,Major,2019-09-13 12:33:43,19
13256516,Random next links ,"_Next>>_ links at the bottom of some documentation pages seem to be out of order.

 * _Simple Single Ozone_ (""easy start"") should link to one of the intermediate level pages, but has no _Next_ link
 * _Building From Sources_ (ninja) should be the last (no _Next_ link), but points to _Minikube_ (intermediate)
 * _Pseudo-cluster_ (intermediate) should point to the ninja level, but leads to _Simple Single Ozone_ (easy start)",pull-request-available,['documentation'],HDDS,Bug,Blocker,2019-09-13 12:17:30,1
13256495,Broken logo image on category sub-pages,"Ozone logo at the top left is broken on sub-pages, eg. _Recipes / Monitoring with Prometheus_ and _Programming Interfaces / Java API_.

 !broken_image.png! ",pull-request-available,['documentation'],HDDS,Bug,Major,2019-09-13 09:47:58,1
13256411,Create a shaded ozone filesystem (client) jar,"We need a shaded Ozonefs jar that does not include Hadoop ecosystem components (Hadoop, HDFS, Ratis, Zookeeper).

A common expected use case for Ozone is Hadoop clients (3.2.0 and later) wanting to access Ozone via the Ozone Filesystem interface. For these clients, we want to add Ozone file system jar to the classpath, however we want to use Hadoop ecosystem dependencies that are `provided` and already expected to be in the client classpath.

Note that this is different from the legacy jar which bundles a shaded Hadoop 3.2.0.",pull-request-available,['build'],HDDS,Improvement,Blocker,2019-09-12 19:24:59,6
13256402,Remove hadoop classes from ozonefs-current jar,"We have two kind of ozone file system jars: current and legacy. current is designed to work only with exactly the same hadoop version which is used for compilation (3.2 as of now).

But as of now the hadoop classes are included in the current jar which is not necessary as the jar is expected to be used in an environment where  the hadoop classes (exactly the same hadoop classes) are already there. They can be excluded.",pull-request-available,[],HDDS,Improvement,Major,2019-09-12 18:06:18,6
13256386,Use checkstyle.xml and suppressions.xml in hdds/ozone projects for checkstyle validation,"After HDDS-2106 hdds/ozone no more relies on hadoop parent pom, so we have to use separate checkstyle.xml and suppressions.xml in hdds/ozone projects for checkstyle validation.",pull-request-available,[],HDDS,Bug,Major,2019-09-12 15:44:45,19
13256305,ContainerStateMachine#writeStateMachineData times out,"The issue seems to be happening because the below precondition check fails in case two writeChunk gets executed in parallel and the runtime exception thrown is handled correctly in ContainerStateMachine.

 

HddsDispatcher.java:239
{code:java}
Preconditions
    .checkArgument(!container2BCSIDMap.containsKey(containerID));
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-12 09:02:26,16
13256293,Add smoke/acceptance test for createPipeline CLI and datanode list CLI,Currently this is no smoke/acceptance for createPipeline or listPipeline CLI.,pull-request-available,[],HDDS,Sub-task,Major,2019-09-12 08:19:56,77
13256218,Rename does not preserve non-explicitly created interim directories,"I am attaching a patch that adds a test that demonstrates the problem.

The scenario is coming from the way how Hive implements acid transactions with the ORC table format, but the test is redacted to the simplest possible code that reproduces the issue.

The scenario:
 * Given a 3 level directory structure, where the top level directory was explicitly created, and the interim directory is implicitly created (for example either by creating a file with create(""/top/interim/file"") or by creating a directory with mkdirs(""top/interim/dir""))
 * When the leaf is moved out from the implicitly created directory making this directory an empty directory
 * Then a FileNotFoundException is thrown when getFileStatus or listStatus is called on the interim directory.

The expected behaviour:

after the directory is becoming empty, the directory should still be part of the file system, moreover an empty FileStatus array should be returned when listStatus is called on it, and also a valid FileStatus object should be returned when getFileStatus is called on it.

 

 

As this issue is present with Hive, and as this is how a FileSystem is expected to work this seems to be an at least critical issue as I see, please feel free to change the priority if needed.

Also please note that, if the interim directory is explicitly created with mkdirs(""top/interim"") before creating the leaf, then the issue does not appear.",pull-request-available,[],HDDS,Bug,Critical,2019-09-11 22:59:27,35
13256157,Update JMX metrics for node count in SCMNodeMetrics for Decommission and Maintenance,"Currently the class SCMNodeMetrics exposes JMX metrics for the number of HEALTHY, STALE and DEAD nodes.

It also exposes the disk capacity of the cluster and the amount of space used and available.

We need to decide how we want to display things in JMX when nodes are in and entering maintenance, decommissioning and decommissioned.

We now have 15 states rather than the previous 3, as we can have nodes in:
 * IN_SERVICE
 * ENTERING_MAINTENANCE
 * IN_MAINTENANCE
 * DECOMMISSIONING
 * DECOMMISSIONED

And in each of these states, nodes can be:
 * HEALTHY
 * STALE
 * DEAD

The simplest case would be to expose these 15 states directly in JMX, as it gives the complete picture, but I wonder if we need any summary JMX metrics too?

 

We also need to consider how to count disk capacity and usage. For example:
 # Do we count capacity and usage on a DECOMMISSIONING node? This is not a clear cut answer, as a decommissioning node does not provide any capacity for writers in the cluster, but it does use capacity.
 # For a DECOMMISSIONED node, we probably should not count capacity or usage
 # For an ENTERING_MAINTENANCE node, do we count capacity and usage? I suspect we should include the capacity and usage in the totals, however a node in this state will not be available for writes.
 # For an IN_MAINTENANCE node that is healthy?
 # For an IN_MAINTENANCE node that is dead?

I would welcome any thoughts on this before changing the code.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-11 17:26:44,11
13256106,XSS fragments can be injected to the S3g landing page  ,"VULNERABILITY DETAILS
There is a way to bypass anti-XSS filter for DOM XSS exploiting a ""window.location.href"".

Considering a typical URL:

scheme://domain:port/path?query_string#fragment_id

Browsers encode correctly both ""path"" and ""query_string"", but not the ""fragment_id"". 

So if used ""fragment_id"" the vector is also not logged on Web Server.

VERSION
Chrome Version: 10.0.648.134 (Official Build 77917) beta

REPRODUCTION CASE
This is an index.html page:


{code:java}
aws s3api --endpoint <script>document.write(window.location.href.replace(""static/"", """"))</script> create-bucket --bucket=wordcount</pre>
{code}


The attack vector is:
index.html?#<script>alert('XSS');</script>

* PoC:
For your convenience, a minimalist PoC is located on:
http://security.onofri.org/xss_location.html?#<script>alert('XSS');</script>

* References
- DOM Based Cross-Site Scripting or XSS of the Third Kind - http://www.webappsec.org/projects/articles/071105.shtml


reference:- 

https://bugs.chromium.org/p/chromium/issues/detail?id=76796",pull-request-available,['S3'],HDDS,Bug,Major,2019-09-11 12:35:54,6
13256103,Arbitrary file can be downloaded with the help of ProfilerServlet,"The LOC 324 in the file [ProfileServlet.java|https://github.com/apache/hadoop/blob/217bdbd940a96986df3b96899b43caae2b5a9ed2/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] is prone to an arbitrary file download:-
{code:java}
protected void doGetDownload(String fileName, final HttpServletRequest req,      final HttpServletResponse resp) throws IOException {

File requestedFile = ProfileServlet.OUTPUT_DIR.resolve(fileName).toAbsolutePath().toFile();{code}
As the String fileName is directly considered as the requested file.

 

Which is called at LOC 180 with HTTP request directly passed:-
{code:java}
if (req.getParameter(""file"") != null) {      doGetDownload(req.getParameter(""file""), req, resp);      
return;    
}
{code}
 ",pull-request-available,['Native'],HDDS,Bug,Major,2019-09-11 12:18:57,6
13256092,Refactor scm.container.client config,Extract typesafe config related to HDDS client with prefix {{scm.container.client}}.,pull-request-available,['SCM Client'],HDDS,Sub-task,Major,2019-09-11 10:21:00,1
13256040,TestContainerSmallFile#testReadWriteWithBCSId failure,"{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190910-vk757/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestContainerSmallFile.txt}
Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 384.415 s <<< FAILURE! - in org.apache.hadoop.ozone.scm.TestContainerSmallFile
testReadWriteWithBCSId(org.apache.hadoop.ozone.scm.TestContainerSmallFile)  Time elapsed: 364.439 s  <<< ERROR!
java.io.IOException: 
Failed to command cmdType: PutSmallFile
...
Caused by: org.apache.ratis.protocol.AlreadyClosedException: client-8C96F0B39BBE->72902ab5-0e57-412f-a398-68ab8a9029d1 is closed.
{code}

Hi [~shashikant], this failure is consistently reproducible starting with the [commit|https://github.com/apache/hadoop/commit/469165e6f29] for HDDS-1843.  Can you please check?",TriagePending,['test'],HDDS,Bug,Major,2019-09-11 07:22:43,16
13255980,Datanodes should retry forever to connect to SCM in an unsecure environment,"In an unsecure environment, the datanodes try upto 10 times after waiting for 1000 milliseconds each time before throwing this error:
{code:java}
Unable to communicate to SCM server at scm:9861 for past 0 seconds.
java.net.ConnectException: Call From scm/10.65.36.118 to scm:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy33.getVersion(Unknown Source)
	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:112)
	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 13 more
{code}
The datanodes should try forever to connect with SCM and not throw any errors.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-10 21:38:48,37
13255751,Avoid usage of hadoop projects as parent of hdds/ozone,"Ozone uses hadoop as a dependency. The dependency defined on multiple level:

 1. the hadoop artifacts are defined in the <dependency> sections
 2. both hadoop-ozone and hadoop-hdds projects uses ""hadoop-project"" as the parent

As we already have a slightly different assembly process it could be more resilient to use a dedicated parent project instead of the hadoop one. With this approach it will be easier to upgrade the versions as we don't need to be careful about the pom contents only about the used dependencies.",pull-request-available,[],HDDS,Improvement,Blocker,2019-09-09 22:32:47,6
13255735,Merge OzoneClientFactory#getRpcClient functions,"Ref: https://github.com/apache/hadoop/pull/1360#discussion_r321585214

There will be 5 overloaded OzoneClientFactory#getRpcClient functions (when HDDS-2007 is committed). They contains some redundant logic and unnecessarily increases code paths.

Goal: Merge those functions into fewer.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-09-09 21:52:06,12
13255722,TestContainerReplication fails due to unhealthy container,"{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190907-l8mkd/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.container.TestContainerReplication.txt}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.771 s <<< FAILURE! - in org.apache.hadoop.ozone.container.TestContainerReplication
testContainerReplication(org.apache.hadoop.ozone.container.TestContainerReplication)  Time elapsed: 12.702 s  <<< FAILURE!
java.lang.AssertionError: Container is not replicated to the destination datanode
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.apache.hadoop.ozone.container.TestContainerReplication.testContainerReplication(TestContainerReplication.java:153)
{code}

caused by:

{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190907-l8mkd/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.container.TestContainerReplication-output.txt}
java.lang.IllegalStateException: Only closed containers could be exported: ContainerId=1
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:525)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.exportContainer(KeyValueHandler.java:875)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.exportContainer(ContainerController.java:134)
	at org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource.java:64)
	at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)
{code}

Container is in unhealthy state because pipeline is not found for it in {{CloseContainerCommandHandler}}.",pull-request-available,['test'],HDDS,Bug,Major,2019-09-09 20:30:15,1
13255695,HddsVolumeChecker should use java optional in place of Guava optional,"HddsVolumeChecker should use java optional in place of Guava optional, as the Guava dependency is marked unstable.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-09-09 17:04:49,18
13255403,Ozone filesystem provider doesn't exist,"We don't have a filesystem provider in META-INF. 
i.e. following file doesn't exist.
{{hadoop-ozone/ozonefs/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}}

See for example
{{hadoop-tools/hadoop-aws/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Critical,2019-09-07 04:22:01,37
13255347,Ozone shell command prints out ERROR when the log4j file is not present.,"*Exception Trace*
{code}
log4j:ERROR Could not read configuration file from URL [file:/etc/ozone/conf/ozone-shell-log4j.properties].
java.io.FileNotFoundException: /etc/ozone/conf/ozone-shell-log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
	at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
	at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
	at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
	at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
	at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)
	at org.apache.hadoop.ozone.web.ozShell.Shell.<clinit>(Shell.java:35)
log4j:ERROR Ignoring configuration file [file:/etc/ozone/conf/ozone-shell-log4j.properties].
log4j:WARN No appenders could be found for logger (io.jaegertracing.thrift.internal.senders.ThriftSenderFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{
  ""metadata"" : { },
  ""name"" : ""vol-test-putfile-1567740142"",
  ""admin"" : ""root"",
  ""owner"" : ""root"",
  ""creationTime"" : 1567740146501,
  ""acls"" : [ {
    ""type"" : ""USER"",
    ""name"" : ""root"",
    ""aclScope"" : ""ACCESS"",
    ""aclList"" : [ ""ALL"" ]
  }, {
    ""type"" : ""GROUP"",
    ""name"" : ""root"",
    ""aclScope"" : ""ACCESS"",
    ""aclList"" : [ ""ALL"" ]
  } ],
  ""quota"" : 1152921504606846976
}
{code}


*Fix*
When a log4j file is not present, the default should be console.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-09-06 18:51:06,30
13255316,Ozone ACL document missing AddAcl API,"Current Ozone Native ACL APIs document looks like below, the AddAcl is missing.

 
h3. Ozone Native ACL APIs

The ACLs can be manipulated by a set of APIs supported by Ozone. The APIs supported are:
 # *SetAcl* – This API will take user principal, the name, type of the ozone object and a list of ACLs.
 # *GetAcl* – This API will take the name and type of the ozone object and will return a list of ACLs.
 # *RemoveAcl* - This API will take the name, type of the ozone object and the ACL that has to be removed.",pull-request-available,[],HDDS,Bug,Major,2019-09-06 16:26:19,28
13254999,Failing acceptance test - smoketests.ozonesecure-s3.MultipartUpload,"{{""smoketests.ozonesecure-s3.MultipartUpload.Test Multipart Upload with the simplified aws s3 cp API""}} acceptance test is failing.",TriagePending,['test'],HDDS,Bug,Major,2019-09-05 12:58:09,6
13254956,Add CLI createPipeline,Add a SCMCLI to create pipeline for ozone.,pull-request-available,['Ozone CLI'],HDDS,Sub-task,Major,2019-09-05 09:30:40,77
13254906,Remove the hard coded config key in ChunkManager,"We have a hard-coded config key in the {{ChunkManagerFactory.java.}}

 
{code}
boolean scrubber = config.getBoolean(
 ""hdds.containerscrub.enabled"",
 false);
{code}",pull-request-available,[],HDDS,Bug,Major,2019-09-05 03:43:50,37
13254871,ReconServer throws SQLException but path present for ozone.recon.db.dir in ozone-site,"java.sql.SQLException: path to '/${ozone.recon.db.dir}/ozone_recon_sqlite.db': '/${ozone.recon.db.dir}' does not exist

But property present in ozone-site.xml:

<property>
        <name>ozone.recon.db.dir</name>
        <value>/tmp/metadata</value>
</property> 

",TriagePending,['Ozone Recon'],HDDS,Bug,Major,2019-09-04 22:52:18,98
13254529,Fix TestRatisPipelineProvider#testCreatePipelinesDnExclude,"
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider
-------------------------------------------------------------------------------
Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.374 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider
testCreatePipelinesDnExclude(org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider)  Time elapsed: 0.044 s  <<< ERROR!
org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 2 nodes.
	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:151)
	at org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.testCreatePipelinesDnExclude(TestRatisPipelineProvider.java:182)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


{code}
",pull-request-available,['test'],HDDS,Bug,Major,2019-09-04 02:14:17,30
13254474,Fix TestSecureOzoneManager,[https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-1909-plfbr/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestSecureOzoneManager.txt],pull-request-available,[],HDDS,Bug,Major,2019-09-03 20:26:39,28
13254472,Get/Renew DelegationToken NPE after HDDS-1909,[https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-1909-plfbr/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestSecureOzoneCluster.txt],pull-request-available,[],HDDS,Bug,Major,2019-09-03 20:25:22,28
13254366,Add maven-gpg-plugin.version to pom.ozone.xml,{{pom.ozone.xml}} is missing maven-gpg-plugin.version.,pull-request-available,[],HDDS,Bug,Major,2019-09-03 10:36:12,19
13254328,Read fails because the block cannot be located in the container,"Read fails as the client is not able to read the block from the container.

{code}
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Unable to find the block with bcsID 2515 .Container 7 bcsId is 0.
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:536)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.lambd2a0$getValid1a9to-08-30 12:51:20,081 | INFO  | SCMAudit | user=msingh | ip=192.168.0.r103 |List$0(ContainerP
rotocolCalls.java:569)
{code}


The client eventually exits here
{code}
2019-08-30 12:51:20,081 [pool-224-thread-6] ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:readData(176)) - LOADGEN: Read key:pool-224-thread-6_330651 failed with ex
ception
ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(121)) - LOADGEN: Exiting due to exception
{code}",MiniOzoneChaosCluster pull-request-available,"['Ozone Client', 'Ozone Datanode']",HDDS,Bug,Blocker,2019-09-03 06:11:02,16
13254203,Tracing in OzoneManager call is propagated with wrong parent,"As you can see in the attached screenshot the OzoneManager.createBucket (server side) tracing information is the children of the freon.createBucket instead of the freon OzoneManagerProtocolPB.submitRequest.

To avoid confusion the hierarchy should be fixed (Most probably we generate the child span AFTER we already serialized the parent one to the message) ",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:49:45,1
13254198,Make SCMSecurityProtocol message based,"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

SCMSecurityProtocol.proto is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-02 09:21:41,6
13254197,Make StorageContainerLocationProtocolService message based,"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

StorageContainerLocationProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-02 09:20:48,6
13254196,Support filters in ozone insight point,"With Ozone insight we can print out all the logs / metrics of one specific component s (eg. scm.node-manager or scm.node-manager).

It would be great to support additional filtering capabilities where the output is filtered based on specific keys.

For example to print out all of the logs related to one datanode or related to one type of RPC request.

Filter should be a key value map (eg. --filter datanode=sjdhfhf,rpc=createChunk) which can be defined in the ozone insight CLI.

As we have no option to add additional tags to the logs (it may be supported by log4j2 but not with slf4k), the first implementation can be implemented by pattern matching.

For example in SCMNodeManager.processNodeReport contains trace/debug logs which includes the "" [datanode={}]"" part. This formatting convention can be used to print out the only the related information. ",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:19:43,6
13254195,Create insight point to debug one specific pipeline,"Wit the first implementation of ozone insight tool we had a demo insight-point to debug Ratis pipelines. It was not stable enough to include in the first patch, this patch is about fixing it.

The goal is to implement a new insight point (eg. datanode.pipeline) which can show information about one pipeline.

It can be done with retrieving the hosts of the pipeline and generate the loggers metrics (InsightPoint.getRelatedLoggers and InsightPoint.getMetrics) based on the pipeline information (same loggers should be displayed from all the three datanodes.

The pipeline id can be defined as a filter parameter which (in this case) should be required.",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:15:13,6
13254194,Default values of properties hdds.datanode.storage.utilization.{critical | warning}.threshold are not reasonable,"Currently, hdds.datanode.storage.utilization.warning.threshold is 0.95 and hdds.datanode.storage.utilization.critical.threshold is 0.75.
The values should be exchanged. ",pull-request-available,[],HDDS,Bug,Major,2019-09-02 09:13:48,5
13254192,Make StorageContainerDatanodeProtocolService message based,"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

StorageContainerDatanodeProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-09-02 09:10:33,6
13254191,Create generic service facade with tracing/metrics/logging support,"We started to use a message based GRPC approach. Wen have only one method and the requests are routed based on a ""type"" field in the proto message. 

For example in OM protocol:

{code}
/**
 The OM service that takes care of Ozone namespace.
*/
service OzoneManagerService {
    // A client-to-OM RPC to send client requests to OM Ratis server
    rpc submitRequest(OMRequest)
          returns(OMResponse);
}
{code}

And 

{code}

message OMRequest {
  required Type cmdType = 1; // Type of the command

...
{code}

This approach makes it possible to use the same code to process incoming messages in the server side.

ScmBlockLocationProtocolServerSideTranslatorPB.send method contains the logic of:

 * Logging the request/response message (can be displayed with ozone insight)
 * Updated metrics
 * Handle open tracing context propagation.


These functions are generic. For example OzoneManagerProtocolServerSideTranslatorPB use the same (=similar) code.

The goal in this jira is to provide a generic utility and move the common code for tracing/request logging/response logging/metrics calculation to a common utility which can be used from all the ServerSide translators.",pull-request-available,[],HDDS,Sub-task,Major,2019-09-02 09:07:42,6
13254190,Improve the observability inside Ozone,"To improve the observability is a key requirement to achieve better correctness and performance with Ozone.

This jira collects some of the tasks which can provide better visibility to the ozone internals.

We have two main tools:

 * Distributed tracing (opentracing) can help to detected performance battlenecks
 * Ozone insight tool (a simple cli frontend for Hadoop metrics and log4j logging) can help to get better understanding about the current state/behavior of specific components.

Both of them can be improved to make it more powerful.",TriagePending,['Tools'],HDDS,Improvement,Major,2019-09-02 08:59:56,6
13254023,Implement OMNodeDetails#toString,Wrote this snippet while debugging OM HA. Might be useful for others when they are debugging as well.,pull-request-available,[],HDDS,Improvement,Minor,2019-08-30 22:55:13,12
13254015,Add tests for incorrect OM HA config when node ID or RPC address is not configured,"-OM will NPE and crash when `ozone.om.service.ids=id1,id2` is configured but `ozone.om.nodes.id1` doesn't exist; or `ozone.om.address.id1.omX` doesn't exist.-

-Root cause:-
-`OzoneManager#loadOMHAConfigs()` didn't check the case where `found == 0`. This happens when local OM doesn't match any `ozone.om.address.idX.omX` in the config.-

Due to the refactoring done in HDDS-2162. This fix has been included in that commit. I will repurpose the jira to add some tests for the HA config.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-30 21:50:58,12
13253920,Integration tests create untracked file audit.log,"An untracked {{audit.log}} file is created during integration test run.  Eg:

{code}
$ mvn -Phdds -pl :hadoop-ozone-integration-test test -Dtest=Test2WayCommitInRatis
...
$ git status
...
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)

	hadoop-ozone/integration-test/audit.log
{code}",pull-request-available,['test'],HDDS,Bug,Trivial,2019-08-30 12:23:20,1
13253878,Add hdds.container.chunk.persistdata as exception to TestOzoneConfigurationFields,"HDDS-1094 introduced a new config key ([hdds.container.chunk.persistdata|https://github.com/apache/hadoop/blob/96f7dc1992246a16031f613e55dc39ea0d64acd1/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsConfigKeys.java#L241-L245]), which needs to be added to {{ozone-default.xml}}, too.

https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190830-rr75b/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestOzoneConfigurationFields.txt",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-08-30 09:26:41,1
13253864,Create Ozone specific LICENSE file for the Ozone source and binary packages,"With HDDS-2058 the Ozone (source) release package doesn't contains the hadoop sources any more. We need to create an adjusted LICENSE file for the Ozone source package (We already created a specific LICENSE file for the binary package which is not changed).

In the new LICENSE file we should include entries only for the sources which are part of the Ozone release.",pull-request-available,[],HDDS,Improvement,Blocker,2019-08-30 08:25:11,6
13253689,Remove hadoop  dependencies in ozone build,"Ozone build currently depend on hadoop code, this makes it difficult to create a ozone only source tar for release. Ozone should not depend on hadoop code during build, it should only depend on hadoop via maven artifacts.",pull-request-available,[],HDDS,Bug,Major,2019-08-29 10:42:17,19
13253589,Fix TestOzoneManagerRatisServer failure,"{{TestOzoneManagerRatisServer}} is failing on trunk with the following error
{noformat}
[ERROR] verifyRaftGroupIdGenerationWithCustomOmServiceId(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerRatisServer)  Time elapsed: 0.418 s  <<< ERROR!
org.apache.hadoop.metrics2.MetricsException: Metrics source OzoneManagerDoubleBufferMetrics already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
	at org.apache.hadoop.ozone.om.ratis.metrics.OzoneManagerDoubleBufferMetrics.create(OzoneManagerDoubleBufferMetrics.java:50)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.<init>(OzoneManagerDoubleBuffer.java:110)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.<init>(OzoneManagerDoubleBuffer.java:88)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.<init>(OzoneManagerStateMachine.java:87)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.getStateMachine(OzoneManagerRatisServer.java:314)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.<init>(OzoneManagerRatisServer.java:244)
	at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.newOMRatisServer(OzoneManagerRatisServer.java:302)
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerRatisServer.verifyRaftGroupIdGenerationWithCustomOmServiceId(TestOzoneManagerRatisServer.java:209)
...
{noformat}

(Thanks [~nandakumar131] for the stack trace.)",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-28 23:26:56,28
13253586,Separate the metadata directories to store security certificates and keys for different services,"Currently, certificates and keys are stored in ozone.metadata.dirs and this needs to be moved to specific metadata dir for each service.",Triage,['Security'],HDDS,Bug,Major,2019-08-28 23:19:13,28
13253564,Rat check failure in decommissioning.md,"{code}
hadoop-hdds/docs/target/rat.txt: !????? /var/jenkins_home/workspace/ozone/hadoop-hdds/docs/content/design/decommissioning.md
{code}",pull-request-available,['documentation'],HDDS,Bug,Minor,2019-08-28 19:41:56,1
13253554,Error while compiling ozone-recon-web,"The following error is seen while compiling {{ozone-recon-web}}
{noformat}
[INFO] Running 'yarn install' in /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web
[INFO] yarn install v1.9.2
[INFO] [1/4] Resolving packages...
[INFO] [2/4] Fetching packages...
[ERROR] (node:31190) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
[INFO] [3/4] Linking dependencies...
[ERROR] warning "" > less-loader@5.0.0"" has unmet peer dependency ""webpack@^2.0.0 || ^3.0.0 || ^4.0.0"".
[INFO] [4/4] Building fresh packages...
[ERROR] warning Error running install script for optional dependency: ""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents: Command failed.
[ERROR] Exit code: 1
[ERROR] Command: node install
[ERROR] Arguments:
[ERROR] Directory: /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents
[ERROR] Output:
[ERROR] node-pre-gyp info it worked if it ends with ok
[INFO] info This module is OPTIONAL, you can safely ignore this error
[ERROR] node-pre-gyp info using node-pre-gyp@0.12.0
[ERROR] node-pre-gyp info using node@12.1.0 | darwin | x64
[ERROR] node-pre-gyp WARN Using request for node-pre-gyp https download
[ERROR] node-pre-gyp info check checked for \""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/lib/binding/Release/node-v72-darwin-x64/fse.node\"" (not found)
[ERROR] node-pre-gyp http GET https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp http 404 https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp WARN Tried to download(404): https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp WARN Pre-built binaries not found for fsevents@1.2.8 and node@12.1.0 (node-v72 ABI, unknown) (falling back to source compile with node-gyp)
[ERROR] node-pre-gyp http 404 status code downloading tarball https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz
[ERROR] node-pre-gyp ERR! build error
[ERROR] node-pre-gyp ERR! stack Error: Failed to execute 'node-gyp clean' (Error: spawn node-gyp ENOENT)
[ERROR] node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/node_modules/node-pre-gyp/lib/util/compile.js:77:29)
[ERROR] node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:196:13)
[ERROR] node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)
[ERROR] node-pre-gyp ERR! stack     at onErrorNT (internal/child_process.js:431:16)
[ERROR] node-pre-gyp ERR! stack     at processTicksAndRejections (internal/process/task_queues.js:84:17)
[ERROR] node-pre-gyp ERR! System Darwin 18.5.0
[ERROR] node-pre-gyp ERR! command \""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/target/node/node\"" \""/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/node_modules/node-pre-gyp/bin/node-pre-gyp\"" \""install\"" \""--fallback-to-build\""
[ERROR] node-pre-gyp ERR! cwd /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents
[ERROR] node-pre-gyp ERR! node -v v12.1.0
[ERROR] node-pre-gyp ERR! node-pre-gyp -v v0.12.0
[ERROR] node-pre-gyp ERR! not ok
[ERROR] Failed to execute 'node-gyp clean' (Error: spawn node-gyp ENOENT)""
[INFO] Done in 102.54s.
{noformat}",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-08-28 18:03:28,37
13253543,Fix Ozone Rest client documentation,We have removed Ozone Rest protocol support and moved to using S3 as the standard REST protocol. The ozone documentation needs to be updated for the 0.5.0 release.,TriagePending,['documentation'],HDDS,Improvement,Blocker,2019-08-28 16:56:19,102
13253454,State check during container state transition in datanode should be lock protected,Currently container state checks during state transition are not lock protected in KeyValueHandler. These can cause invalid state transitions.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-08-28 09:25:09,35
13253353,Datanodes fail to come up after 10 retries in a secure environment,"{code:java}
10:06:36.585 PM    ERROR    HddsDatanodeService    
Error while storing SCM signed certificate.
java.net.ConnectException: Call From jmccarthy-ozone-secure-2.vpc.cloudera.com/10.65.50.127 to jmccarthy-ozone-secure-1.vpc.cloudera.com:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
    at org.apache.hadoop.ipc.Client.call(Client.java:1457)
    at org.apache.hadoop.ipc.Client.call(Client.java:1367)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
    at com.sun.proxy.$Proxy15.getDataNodeCertificate(Unknown Source)
    at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.getDataNodeCertificateChain(SCMSecurityProtocolClientSideTranslatorPB.java:156)
    at org.apache.hadoop.ozone.HddsDatanodeService.getSCMSignedCert(HddsDatanodeService.java:278)
    at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:248)
    at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:211)
    at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:168)
    at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:143)
    at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:70)
    at picocli.CommandLine.execute(CommandLine.java:1173)
    at picocli.CommandLine.access$800(CommandLine.java:141)
    at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
    at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
    at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
    at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
    at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
    at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
    at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
    at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:126)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
    at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
    at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
    at org.apache.hadoop.ipc.Client.call(Client.java:1403)
    ... 21 more
{code}

Datanodes try to get SCM signed certificate for just 10 times with interval of 1 sec. When SCM takes a little longer to come up, datanodes throw an exception and fail.",pull-request-available,"['Ozone Datanode', 'Security']",HDDS,Bug,Major,2019-08-27 22:29:21,28
13253310,Partially started compose cluster left running,"If any container in the sample cluster [fails to start|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L24], all successfully started containers are left running.  This [prevents|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L59] any further acceptance tests from normal completion.  This is only a minor inconvenience, since acceptance test as a whole fails either way.",pull-request-available,"['docker', 'test']",HDDS,Bug,Minor,2019-08-27 17:40:14,1
13253299,Remove 'ozone' from the recon module names.,"Currently the module names ""ozone-recon"" and ""ozone-recon-codegen"". In order to make them similar to other modules, they need to be changed into ""recon"" and ""recon-codegen""",pull-request-available,['Ozone Recon'],HDDS,Bug,Major,2019-08-27 16:48:58,98
13253188,Avoid log on console with Ozone shell,HDDS-1489 fixed several sample docker compose configs to avoid unnecessary messages on console when running eg. {{ozone sh key put}}.  The goal of this task is to fix the remaining ones.,pull-request-available,['docker'],HDDS,Bug,Minor,2019-08-27 06:21:07,1
13253150,Don't depend on DFSUtil to check HTTP policy,"Currently, BaseHttpServer uses DFSUtil to get Http policy. With this, when http policy is set to HTTPS on hdfs-site.xml, ozone http servers try to come up with HTTPS and fail if SSL certificates are not present in the required location.

Ozone web UIs should not depend on HDFS config to determine HTTP policy. Instead, it should have its own config to determine the policy. ",pull-request-available,['website'],HDDS,Task,Blocker,2019-08-26 23:12:19,28
13253122,Add Auditlog for ACL operations,This is to add audit log support for Acl operations in HA code path.,Triaged audit log4j2,['Ozone Manager'],HDDS,Bug,Major,2019-08-26 20:02:28,81
13253065,Fix hadoop version in pom.ozone.xml,"The hadoop version in pom.ozone.xml is pointing to SNAPSHOT version, this has to be fixed.",pull-request-available,[],HDDS,Bug,Major,2019-08-26 14:59:46,19
13252919,Implement datanode level CLI to reveal pipeline info,"1. filter pipeline by datanode
",pull-request-available,[],HDDS,Sub-task,Major,2019-08-26 06:57:38,77
13252918,Async RATIS pipeline creation and destroy through heartbeat commands,"Currently, pipeline creation and destroy are synchronous operations. SCM directly connect to each datanode of the pipeline through gRPC channel to create the pipeline to destroy the pipeline.  
This task is to remove the gRPC channel, send pipeline creation and destroy action through heartbeat command to each datanode.",pull-request-available,[],HDDS,New Feature,Major,2019-08-26 06:51:56,5
13252912,Ozone client should retry writes in case of any ratis/stateMachine exceptions,"Currently, Ozone client retry writes on a different pipeline or container in case of some specific exceptions. But in case, it sees exception such as DISK_FULL, CONTAINER_UNHEALTHY or any corruption , it just aborts the write. In general, the every such exception on the client should be a retriable  exception in ozone client and on some specific exceptions, it should take some more specific exception like excluding certain containers or pipelines while retrying or informing SCM of a corrupt replica etc.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-08-26 05:34:22,16
13252834,Generate simplifed reports by the dev-support/checks/*.sh scripts,"hadoop-ozone/dev-support/checks directory contains shell scripts to execute different type of code checks (findbugs, checkstyle, etc.)

Currently the contract is very simple. Every shell script executes one (and only one) check and the shell response code is set according to the result (non-zero code if failed).

To have better reporting in the github pr build, it would be great to improve the scripts to generate simple summary files and save the relevant files for archiving.",pull-request-available,['build'],HDDS,Improvement,Major,2019-08-24 21:56:39,6
13252798,Fix license issues on ozone-0.4.1,"There are files on ozone-0.4.1 branch which doesn't have apache license header, they have to be fixed.
{noformat}
hadoop/hadoop-ozone/dist/src/main/compose/ozones3-haproxy/haproxy-conf/haproxy.cfg
hadoop/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClientForAclAuditLog.java	
hadoop/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/bucket/TestS3BucketDeleteResponse.java
hadoop/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/multipart/TestS3MultipartUploadAbortResponse.java
hadoop/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/multipart/TestS3MultipartUploadAbortRequest.java
hadoop/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMKeyPurgeResponse.java
hadoop/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyPurgeRequest.java
{noformat}",pull-request-available,[],HDDS,Bug,Blocker,2019-08-24 10:33:30,19
13252711,Overlapping chunk region cannot be read concurrently,"Concurrent requests to datanode for the same chunk may result in the following exception in datanode:

{code}
java.nio.channels.OverlappingFileLockException
   at java.base/sun.nio.ch.FileLockTable.checkList(FileLockTable.java:229)
   at java.base/sun.nio.ch.FileLockTable.add(FileLockTable.java:123)
   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.addToFileLockTable(AsynchronousFileChannelImpl.java:178)
   at java.base/sun.nio.ch.SimpleAsynchronousFileChannelImpl.implLock(SimpleAsynchronousFileChannelImpl.java:185)
   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.lock(AsynchronousFileChannelImpl.java:118)
   at org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils.readData(ChunkUtils.java:175)
   at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerImpl.readChunk(ChunkManagerImpl.java:213)
   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleReadChunk(KeyValueHandler.java:574)
   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:195)
   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:271)
   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
{code}

It seems this is covered by retry logic, as key read is eventually successful at client side.

The problem is that:

bq. File locks are held on behalf of the entire Java virtual machine. They are not suitable for controlling access to a file by multiple threads within the same virtual machine. ([source|https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileLock.html])

code ref: [{{ChunkUtils.readData}}|https://github.com/apache/hadoop/blob/c92de8209d1c7da9e7ce607abeecb777c4a52c6a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L175]",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2019-08-23 16:26:01,1
13252640,Update the Dockerfile of the official apache/ozone image and use latest 0.4.1 release,"The hadoop-docker-ozone repository contains the definition of the apache/ozone image. 

https://github.com/apache/hadoop-docker-ozone/tree/ozone-latest

It creates a docker packaging for the voted and released artifact, therefore it can be released after the final vote.

Since the latest release we did some modification in our Dockerfiles. We need to apply the changes to the official image as well. Especially:

 1. use ozone-runner as a base image instead of hadoop-runner
 2. rename ozoneManager service to om as we did everywhere
 3. Adjust the starter location (the script is moved to the released tar file)
 ",pull-request-available,[],HDDS,Improvement,Major,2019-08-23 11:25:16,6
13252619,rat.sh: grep: warning: recursive search of stdin,"Running {{rat.sh}} locally fails with the following error message (after the two Maven runs):

{code:title=./hadoop-ozone/dev-support/checks/rat.sh}
...
grep: warning: recursive search of stdin
{code}

This happens if {{grep}} is not the GNU one.

Further, {{rat.sh}} runs into: {{cat: target/rat-aggregated.txt: No such file or directory}} in subshell due to a typo, and so always exits with success:

{code}
$ ./hadoop-ozone/dev-support/checks/rat.sh
...
cat: target/rat-aggregated.txt: No such file or directory

$ echo $?
0
{code}",pull-request-available,['build'],HDDS,Bug,Minor,2019-08-23 09:35:53,1
13252601,Fix rat check failures in trunk,Several files in hadop-ozone do not have apache license headers and cause a failure in trunk. ,pull-request-available,[],HDDS,Task,Major,2019-08-23 08:20:09,37
13252597,Add additional freon tests,"Freon is a generic load generator tool for ozone (ozone freon) which supports multiple generation pattern.

As of now only the random-key-generator is implemented which uses ozone rpc client.

It would be great to add additional tests:

 * Test key generation via s3 interface
 * Test key generation via the hadoop fs interface
 * Test key reads (validation)
 * Test OM with direct RPC calls
",pull-request-available,['Tools'],HDDS,Improvement,Major,2019-08-23 08:01:27,6
13252571,Upgrade Guava library to v26 in hdds project,Upgrade Guava library to v26 in hdds project,pull-request-available,[],HDDS,Improvement,Major,2019-08-23 03:14:52,81
13252549,Remove mTLS from Ozone GRPC,"Generic GRPC support mTLS for mutual authentication. However, Ozone has built in block token mechanism for server to authenticate the client. We only need TLS for client to authenticate the server and wire encryption. 

Remove the mTLS support also simplify the GRPC server/client configuration.",pull-request-available,[],HDDS,Improvement,Major,2019-08-22 23:35:02,28
13252477,Handle Set DtService of token in S3Gateway for OM HA,"When OM HA is enabled, when tokens are generated, the service name should be set with address of all OM's.

 

Current without HA, it is set with Om RpcAddress string. This Jira is to handle:
 # Set dtService with all OM address. Right now in OMClientProducer, UGI is created with S3 token, and serviceName of token is set with OMAddress, for HA case, this should be set with all OM RPC addresses.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Critical,2019-08-22 20:11:05,13
13252476,Handle Set DtService of token for OM HA,"When OM HA is enabled, when tokens are generated, the service name should be set with address of all OM's.

 

Current without HA, it is set with Om RpcAddress string. This Jira is to handle:
 # Set dtService with all OM address.
 # Update token selector to return tokens if there is a match with Service. Because SaslRpcClient calls token selector with server address.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-22 20:10:21,13
13252459,Add option to enforce gdpr in Bucket Create command,"e2e flow where user can enforce GDPR for a bucket during creation only.
Add/update audit logs as this will be a useful action for compliance purpose.
Add docs to show usage.",pull-request-available,[],HDDS,Sub-task,Major,2019-08-22 19:12:07,81
13252458,Encrypt/decrypt key using symmetric key while writing/reading,"*Key Write Path (Encryption)*
When a bucket metadata has gdprEnabled=true, we generate the GDPRSymmetricKey and add it to Key Metadata before we create the Key.
This ensures that key is encrypted before writing.

*Key Read Path(Decryption)*
While reading the Key, we check for gdprEnabled=true and they get the GDPRSymmetricKey based on secret/algorithm as fetched from Key Metadata.
Create a stream to decrypt the key and pass it on to client.

*Test*
Create Key in GDPR Enabled Bucket -> Read Key -> Verify content is as expected -> Update Key Metadata to remove the gdprEnabled flag -> Read Key -> Confirm the content is not as expected.
",pull-request-available,[],HDDS,Sub-task,Major,2019-08-22 19:11:33,81
13252452,Support GDPR-Right to Erasure feature on Ozone,"While several aspects of GDPR can be achieved by various ways in Ozone, the [Right to Erasure|https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/individual-rights/right-to-erasure/] support will need some work.
This is an umbrella jira to add support for this feature to Ozone.

Design doc is attached.",compliance,"['Ozone Client', 'Ozone Manager']",HDDS,New Feature,Major,2019-08-22 18:47:35,81
13252397,TestRandomKeyGenerator fails due to timeout,"{{TestRandomKeyGenerator#bigFileThan2GB}} is failing intermittently due to timeout in Ratis {{appendEntries}}.  Commit on pipeline fails, and new pipeline cannot be created with 2 nodes (there are 5 nodes total).

Most recent one: https://github.com/elek/ozone-ci/tree/master/trunk/trunk-nightly-pz9vg/integration/hadoop-ozone/tools",pull-request-available,['test'],HDDS,Sub-task,Major,2019-08-22 13:59:34,27
13252309,"PipelineID management for multi-raft, in SCM or in datanode?","With the intention to support multi-raft, I wanna bring up a question on how the pipeline unique ids be managed. Since every datanode  can be member in multiple raft pipelines, the pipeline ids need to be persisted with the datanode for recovery purpose (we can talk about recovery later). Generally there are two options:
 # Store in datanode (like datanodeDetails) and every time pipelines mapping change on single datanode, pipeline ids will be serialized to local file. This way will lead to many more local serialization of things like datanodeDetails, but the updates are only for local datanode change. Improvement can be made like linking a serializable object to datanodeDetails and datanode keeps updating the new pipeline ids to the serializable object instead the details file. On the other hand, since the pipeline ids are stored only in datanode locally, there will be no global view in SCM. (or we can store a lazy copy?)
 # Stored in SCM. SCM can maintain a large mapping between datanode ids and pipeline ids. But this way will lead to an exponentially increasing frequency in SCM updates since the pipeline mapping changes are way more complex and happen all the time. Obviously this gives SCM too much pressure, but it can also give SCM a global view on the management over datanodes and multi raft pipelines. 

 

Thoughts? [~xyao] [~Sammi] ",TriagePending,['Ozone Datanode'],HDDS,New Feature,Major,2019-08-22 08:06:17,77
13252282,scm web ui should publish the list of scm pipeline by type and factor,"scm web ui should publish the list of scm pipeline by type and factor, this helps in monitoring the cluster in real time.",Triaged,['SCM'],HDDS,Bug,Major,2019-08-22 05:54:40,37
13252274,Wrong package for RatisHelper class in hadoop-hdds/common module.,"It is currently org.apache.ratis.RatisHelper. 

It should be org.apache.hadoop.hdds.ratis.RatisHelper.",pull-request-available,[],HDDS,Bug,Minor,2019-08-22 05:15:22,30
13252209,Make ozone fs shell command work with OM HA service ids,"Build an HDFS HA-like nameservice for OM HA so that Ozone client can access Ozone HA cluster with ease.

The majority of the work is already done in HDDS-972. But the problem is that the client would crash if there are more than one service ids (ozone.om.service.ids) configured in ozone-site.xml. This need to be address on client side.",pull-request-available,"['OM HA', 'Ozone Client', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-21 20:50:36,12
13252155,Update documentation for 0.4.1 release,We have to update Ozone documentation based on the latest changes/features.,pull-request-available,['documentation'],HDDS,Improvement,Major,2019-08-21 15:07:26,19
13252154,Update Ratis version to 0.4.0,Update Ratis version to 0.4.0,pull-request-available,[],HDDS,Improvement,Major,2019-08-21 15:06:03,19
13252150,Don't depend on bootstrap/jquery versions from hadoop-trunk snapshot,The OM/SCM web pages are broken due to the upgrade in HDFS-14729 (which is a great patch on the Hadoop side). To have more stability I propose to use our own instance from jquery/bootstrap instead of copying the actual version from hadoop trunk which is a SNAPSHOT build.,pull-request-available,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2019-08-21 14:41:19,37
13252115,Basic acceptance test and SCM/OM web UI broken by Bootstrap upgrade,"{code:title=https://elek.github.io/ozone-ci/trunk/trunk-nightly-9stkx/acceptance/smokeresult/log.html#s1-s8-t1}
$ curl --negotiate -u : -s -I http://scm:9876/static/bootstrap-3.3.7/js/bootstrap.min.js 2>&1
HTTP/1.1 404 Not Found
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-21 12:02:53,1
13252086,TestSecureContainerServer#testClientServerRatisGrpc is failing,"{{TestSecureContainerServer#testClientServerRatisGrpc}} is failing on trunk with the following error.

{noformat}
[ERROR] testClientServerRatisGrpc(org.apache.hadoop.ozone.container.server.TestSecureContainerServer)  Time elapsed: 7.544 s  <<< ERROR!
java.io.IOException:
Failed to command cmdType: CreateContainer
containerID: 1566379872577
datanodeUuid: ""87ebf146-2a8f-4060-8f06-615ed61a9fe0""
createContainer {
}

	at org.apache.hadoop.hdds.scm.XceiverClientSpi.sendCommand(XceiverClientSpi.java:113)
	at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServer(TestSecureContainerServer.java:206)
	at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServerRatis(TestSecureContainerServer.java:157)
	at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.testClientServerRatisGrpc(TestSecureContainerServer.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.util.concurrent.ExecutionException: org.apache.ratis.protocol.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.hadoop.hdds.scm.XceiverClientSpi.sendCommand(XceiverClientSpi.java:110)
	... 29 more
Caused by: org.apache.ratis.protocol.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction$7(ContainerStateMachine.java:701)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1595)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-21 09:33:52,104
13252080,Support copy-source-if-(un)modified-since headers for MPU key creation with copy,HDDS-1942 introduces the option to create MPU key part with copy (https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html). But the x-amz-copy-source-if-(un)modified-since headers are not supported yet. ,newbie pull-request-available,['S3'],HDDS,Sub-task,Minor,2019-08-21 09:26:27,7
13251954,Generate renewTime on OMLeader for GetDelegationToken,"Use renewTime generated by OM leader, across quorum of OM's.

 

Right now each OM generates renew time when updating token in-memory and DB.

*OzoneDelegationTokenSecretManager.java*

public long updateToken(Token<OzoneTokenIdentifier> token,
 OzoneTokenIdentifier ozoneTokenIdentifier) {
 long renewTime =
 ozoneTokenIdentifier.getIssueDate() + getTokenRenewInterval();

 

If different OM's have different token renew interval set, for the same token we will have different renewal time across a quorum of OM's.

 

This Jira is to fix this issue.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Blocker,2019-08-21 00:00:16,43
13251953,Compilation failure due to missing class ScmBlockLocationTestingClient,"The ozone build is failing due to following compilation error,

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-ozone-ozone-manager: Compilation failure: Compilation failure:
[ERROR] /Users/hgadre/git-repo/upstream/hadoop/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java:[94,17] cannot find symbol
[ERROR]   symbol:   class ScmBlockLocationTestingClient
[ERROR]   location: class org.apache.hadoop.ozone.om.TestKeyDeletingService
[ERROR] /Users/hgadre/git-repo/upstream/hadoop/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java:[116,17] cannot find symbol
[ERROR]   symbol:   class ScmBlockLocationTestingClient
[ERROR]   location: class org.apache.hadoop.ozone.om.TestKeyDeletingService
[ERROR] /Users/hgadre/git-repo/upstream/hadoop/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java:[143,17] cannot find symbol
[ERROR]   symbol:   class ScmBlockLocationTestingClient
[ERROR]   location: class org.apache.hadoop.ozone.om.TestKeyDeletingService
[ERROR] -> [Help 1]",pull-request-available,[],HDDS,Task,Major,2019-08-20 23:35:55,106
13251703,Merge OzoneManagerRequestHandler and OzoneManagerHARequestHandlerImpl,"Once HA and Non-HA code are merged to use newly OM HA code. We can merge these classes, and remove the unused code.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:25:16,13
13251698,Remove RatisClient in OM HA,"In OM, we use ratis server api's to submit request. We can remove the RatisClient code from OM, which is no more used in submitting requests to ratis.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:16:00,13
13251695,Fix ApplyTransaction error handling in OzoneManagerStateMachine,"Right now, applyTransaction calls validateAndUpdateCache. This does not return any error. We should check the OmResponse Status code, and see if it is critical error we should completeExceptionally and stop ratis server.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:12:14,13
13251693,Fix listParts API,"This Jira is to fix listParts API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listParts of a MPU key, it should use both in-memory cache and rocksdb mpu table to list parts of a mpu key.

 

No fix is required for this, as the information is retrieved from the MPU Key table, this information is not retrieved through RocksDB Table iteration. (As when we use get() this checks from cache first, and then it checks table)

 

Used this Jira to add an integration test to verify the behavior.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:09:47,13
13251692,Fix listStatus API,"This Jira is to fix listStatus API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response. It will be picked by double buffer thread and flushed to disk later. So when user call listStatus, it should use both in-memory cache and rocksdb key table to return the correct result.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:08:30,12
13251690,Fix listkeys API,"This Jira is to fix listKeys API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listkeys, it should use both in-memory cache and rocksdb key table to list keys in a bucket.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:06:41,13
13251689,Fix listVolumes API,"This Jira is to fix lisVolumes API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listVolumes, it should use both in-memory cache and rocksdb volume table to list volumes for a user.

 

No fix is required for this, as the information is retrieved from the MPU Key table, this information is not retrieved through RocksDB Table iteration. (As when we use get() this checks from cache first, and then it checks table)

 

Used this Jira to add an integration test to verify the behavior.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:05:47,13
13251688,Fix listBucket API,"This Jira is to fix listBucket API in HA code path.

In HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listBuckets, it should use both in-memory cache and rocksdb bucket table to list buckets in a volume.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-19 21:05:14,13
13251572,Extend SCMNodeManager to support decommission and maintenance states,"Currently, within SCM a node can have the following states:

HEALTHY
STALE
DEAD
DECOMMISSIONING
DECOMMISSIONED

The last 2 are not currently used.

In order to support decommissioning and maintenance mode, we need to extend the set of states a node can have to include decommission and maintenance states.

It is also important to note that a node decommissioning or entering maintenance can also be HEALTHY, STALE or go DEAD.

Therefore in this Jira I propose we should model a node state with two different sets of values. The first, is effectively the liveliness of the node, with the following states. This is largely what is in place now:

HEALTHY
STALE
DEAD

The second is the node operational state:

IN_SERVICE
DECOMMISSIONING
DECOMMISSIONED
ENTERING_MAINTENANCE
IN_MAINTENANCE

That means the overall total number of states for a node is the cross-product of the two above lists, however it probably makes sense to keep the two states seperate internally.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-08-19 12:21:43,11
13251556,Datanode should sync db when container is moved to CLOSED or QUASI_CLOSED state,Datanode should sync db when container is moved to CLOSED or QUASI_CLOSED state. This will ensure that the metadata is persisted.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-08-19 11:00:33,35
13251538,Datanode should sync the db for open containers in the pipeline during stateMachine close,Datanode should sync the db for open containers in the pipeline during stateMachine close to ensure metadata for these containers is persisted.,TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2019-08-19 10:13:43,35
13251417,Fix checkstyle errors,There are checkstyle errors in ListPipelinesSubcommand.java that needs to be fixed.,pull-request-available,['SCM'],HDDS,Task,Major,2019-08-17 22:41:50,37
13251409,Create helper script to run blockade tests,To run blockade tests as part of jenkins job we need some kind of helper script.,pull-request-available,['test'],HDDS,Improvement,Major,2019-08-17 17:56:20,19
13251394,Fix checkstyle issues introduced by HDDS-1894,"Fix the checkstyle issues introduced by HDDS-1894
{noformat}

[INFO] There are 6 errors reported by Checkstyle 8.8 with checkstyle/checkstyle.xml ruleset.
[ERROR] src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java:[41,23] (whitespace) ParenPad: '(' is followed by whitespace.
[ERROR] src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java:[42] (sizes) LineLength: Line is longer than 80 characters (found 88).
[ERROR] src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java:[46,23] (whitespace) ParenPad: '(' is followed by whitespace.
[ERROR] src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java:[47] (sizes) LineLength: Line is longer than 80 characters (found 90).
[ERROR] src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java:[59] (sizes) LineLength: Line is longer than 80 characters (found 116).
[ERROR] src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java:[60] (sizes) LineLength: Line is longer than 80 characters (found 120).
{noformat}",newbie,['SCM Client'],HDDS,Bug,Major,2019-08-17 13:54:05,104
13251349,Ozone manager init fails when certificate is missing in a kerberized cluster,"When Ozone Manager gets into a state where certificate is missing, it does not try to recover by creating a certificate.


{code:java}
3:30:48.620 PM INFO OzoneManager Initializing secure OzoneManager. 
3:30:49.788 PM INFO OMCertificateClient Loading certificate from location:/var/lib/hadoop-ozone/om/data/certs. 
3:30:49.896 PM INFO OMCertificateClient Added certificate from file:/var/lib/hadoop-ozone/om/data/certs/8136899895890.crt. 
3:30:49.904 PM INFO OMCertificateClient Added certificate from file:/var/lib/hadoop-ozone/om/data/certs/CA-1.crt. 
3:30:49.930 PM ERROR OMCertificateClient Default certificate serial id is not set. Can't locate the default certificate for this client. 
3:30:49.930 PM INFO OMCertificateClient Certificate client init case: 6 3:30:49.932 PM INFO OMCertificateClient Found private and public key but certificate is missing. 
3:30:50.194 PM INFO OzoneManager Init response: RECOVER 
3:30:50.230 PM ERROR OzoneManager OM security initialization failed. OM certificate is missing.
{code}",Triaged,['Security'],HDDS,Bug,Major,2019-08-16 22:51:35,102
13251345,Implement default acls for bucket/volume/key for OM HA code,This Jira is to implement default ACLs for Ozone volume/bucket/key.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-16 22:16:20,13
13251119,Implement OM CancelDelegationToken request to use Cache and DoubleBuffer,"Implement OM CancelDelegationToken request to use OM Cache, double buffer.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-16 00:02:23,13
13251118,Implement OM RenewDelegationToken request to use Cache and DoubleBuffer,"Implement OM RenewDelegationToken request to use OM Cache, double buffer.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-16 00:01:59,13
13251100,Provide example ha proxy with multiple s3 servers back end.,"In this Jira, we shall provide docker-compose files where we start 3 s3 gateway servers, and ha-proxy is used to load balance these S3 Gateway Servers.

 

In this Jira, all are proxy configurations are hardcoded, we can make improvements to scale and automatically configure with environment variables as a future improvement. This is just a starter example.

 ",pull-request-available,[],HDDS,New Feature,Major,2019-08-15 21:07:44,13
13251096,Update document for HDDS-1891: Ozone fs shell command should work with default port when port number is not specified,"This should've been part of HDDS-1891.

Now that fs shell command works without specifying a default OM port number. We should update the doc on https://hadoop.apache.org/ozone/docs/0.4.0-alpha/ozonefs.html:

{code}
... Moreover, the filesystem URI can take a fully qualified form with the OM host and port as a part of the path following the volume name.
{code}

CC [~bharatviswa]",pull-request-available,['documentation'],HDDS,Task,Major,2019-08-15 20:53:53,12
13250863,Implement OM GetDelegationToken request to use Cache and DoubleBuffer,"Implement OM GetDelegationToken request to use OM Cache, double buffer.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-14 19:59:55,13
13250780,Wrong expected key ACL in acceptance test,"Acceptance test fails at ACL checks:

{code:title=https://elek.github.io/ozone-ci/trunk/trunk-nightly-wxhxr/acceptance/smokeresult/log.html#s1-s16-s2-t4-k2}
[ {
  ""type"" : ""USER"",
  ""name"" : ""testuser/scm@EXAMPLE.COM"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""root"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""GROUP"",
  ""name"" : ""superuser1"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""ALL"" ]
}, {
  ""type"" : ""USER"",
  ""name"" : ""superuser1"",
  ""aclScope"" : ""ACCESS"",
  ""aclList"" : [ ""READ"", ""WRITE"", ""READ_ACL"", ""WRITE_ACL"" ]
} ]' does not match '""type"" : ""GROUP"",
.*""name"" : ""superuser1*"",
.*""aclScope"" : ""ACCESS"",
.*""aclList"" : . ""READ"", ""WRITE"", ""READ_ACL"", ""WRITE_ACL""'
{code}

The test [sets user ACL|https://github.com/apache/hadoop/blob/0e4b757955ae8da1651b870c12458e3344c0b613/hadoop-ozone/dist/src/main/smoketest/basic/ozone-shell.robot#L123], but [checks group ACL|https://github.com/apache/hadoop/blob/0e4b757955ae8da1651b870c12458e3344c0b613/hadoop-ozone/dist/src/main/smoketest/basic/ozone-shell.robot#L125].  I think this passed previously due to a bug that was [fixed|https://github.com/apache/hadoop/pull/1234/files#diff-2d061b57a9838854d07da9e0eca64f31] by HDDS-1917.",pull-request-available,['test'],HDDS,Bug,Major,2019-08-14 11:59:56,1
13250760,Compile error due to leftover ScmBlockLocationTestIngClient file,"{code:title=https://ci.anzix.net/job/ozone/17667/consoleText}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /var/jenkins_home/workspace/ozone@2/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestIngClient.java:[65,8] class ScmBlockLocationTestingClient is public, should be declared in a file named ScmBlockLocationTestingClient.java
[ERROR] /var/jenkins_home/workspace/ozone@2/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestingClient.java:[65,8] duplicate class: org.apache.hadoop.ozone.om.ScmBlockLocationTestingClient
[INFO] 2 errors 
{code}",pull-request-available,['build'],HDDS,Bug,Blocker,2019-08-14 11:00:12,1
13250740,TestOzoneClientProducer fails with ConnectException,"{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/trunk/trunk-nightly-wxhxr/unit/hadoop-ozone/s3gateway/org.apache.hadoop.ozone.s3.TestOzoneClientProducer.txt}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.ozone.s3.TestOzoneClientProducer
-------------------------------------------------------------------------------
Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 222.239 s <<< FAILURE! - in org.apache.hadoop.ozone.s3.TestOzoneClientProducer
testGetClientFailure[0](org.apache.hadoop.ozone.s3.TestOzoneClientProducer)  Time elapsed: 111.036 s  <<< FAILURE!
java.lang.AssertionError: 
 Expected to find 'Couldn't create protocol ' but got unexpected exception: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
{code}

Log output (with local log4j config) reveals that connection is attempted to 0.0.0.0:9862:

{code:title=log output}
2019-08-14 10:49:14,225 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
{code}

The address 0.0.0.0:9862 was added as default in [HDDS-1920|https://github.com/apache/hadoop/commit/bf457797f607f3aeeb2292e63f440cb13e15a2d9].",pull-request-available,['test'],HDDS,Bug,Major,2019-08-14 09:15:00,1
13250595,TestStorageContainerManager#testScmProcessDatanodeHeartbeat is flaky,"TestStorageContainerManager#testScmProcessDatanodeHeartbeat is flaky
{noformat}
[ERROR] testScmProcessDatanodeHeartbeat(org.apache.hadoop.ozone.TestStorageContainerManager)  Time elapsed: 25.057 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.ozone.TestStorageContainerManager.testScmProcessDatanodeHeartbeat(TestStorageContainerManager.java:531)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-13 16:32:06,19
13250496,Decrement purge interval for Ratis logs in datanode,"Currently purge interval for ratis log(""dfs.container.ratis.log.purge.gap"") is set at 1000000000. The Jira aims to reduce the interval and set it to 1000000.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-08-13 10:07:02,104
13250452,Aged IO Thread exits on first read,"Aged IO Thread in {{TestMiniChaosOzoneCluster}} exits on first read due to exception:

{code}
2019-08-12 22:55:37,799 [pool-245-thread-1] INFO  ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(194)) - AGED LOADGEN: Started Aged IO Thread:2139.
...
2019-08-12 22:55:47,147 [pool-245-thread-1] ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(213)) - AGED LOADGEN: 0 Exiting due to exception
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.readData(MiniOzoneLoadGenerator.java:151)
	at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.startAgedFilesLoad(MiniOzoneLoadGenerator.java:209)
	at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$1(MiniOzoneLoadGenerator.java:235)
2019-08-12 22:55:47,149 [pool-245-thread-1] INFO  ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(219)) - Terminating IO thread:2139.
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-13 07:22:39,1
13250321,TestBlockOutputStreamWithFailures#test2DatanodesFailure failing because of assertion error,The test is failing because pipeline can be closed because of the datanode shutdown. This can also cause a ContainerNotOpenException to be raised.,pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-08-12 16:38:08,18
13250198,StackOverflowError in OzoneClientInvocationHandler,"Happens if log level for {{org.apache.hadoop.ozone.client}} is set to TRACE.

{code}
SLF4J: Failed toString() invocation on an object of type [com.sun.proxy.$Proxy85]
Reported exception:
java.lang.StackOverflowError
...
	at org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)
	at com.sun.proxy.$Proxy85.toString(Unknown Source)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)
	at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)
	at org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)
	at com.sun.proxy.$Proxy85.toString(Unknown Source)
...
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Trivial,2019-08-12 07:07:32,1
13250167,Remove pipeline persistent in SCM,"Currently, SCM will persistent pipeline in metastore with datanode information locally. After SCM restart, it will reload all the pipelines from the metastore.  If there is any datanode information change during the whole SCMc lifecycle, the persisted pipeline is not updated. ",TriagePending,['SCM'],HDDS,Improvement,Major,2019-08-12 02:57:16,5
13250047,TestMiniChaosOzoneCluster may run until OOME,"{{TestMiniChaosOzoneCluster}} runs load generator on a cluster for supposedly 1 minute, but it may run indefinitely until JVM crashes due to OutOfMemoryError.

In 0.4.1 nightly build it crashed 29/30 times (and no tests were executed in the remaining one run due to some other error).

Latest:
https://github.com/elek/ozone-ci/blob/3f553ed6ad358ba61a302967617de737d7fea01a/byscane/byscane-nightly-wggqd/integration/output.log#L5661-L5662

When it crashes, it leaves GBs of data lying around.",pull-request-available,['test'],HDDS,Bug,Critical,2019-08-10 14:48:42,1
13250039,Wrong symbolic release name on 0.4.1 branch,"Should be Biscayne instead of Crater lake according to the Roadmap:

https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Road+Map
",pull-request-available,[],HDDS,Bug,Blocker,2019-08-10 13:38:51,6
13250037,S3 MPU part-list call fails if there are no parts,"If an S3 multipart upload is created but no part is upload the part list can't be called because it throws HTTP 500:

Create an MPU:

{code}
aws s3api --endpoint http://localhost:9999 create-multipart-upload --bucket=docker --key=testkeu                                         
{
    ""Bucket"": ""docker"",
    ""Key"": ""testkeu"",
    ""UploadId"": ""85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234""
}
{code}

List the parts:

{code}
aws s3api --endpoint http://localhost:9999 list-parts  --bucket=docker --key=testkeu --upload-id=85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234
{code}

It throws an exception on the server side, because in the KeyManagerImpl.listParts the  ReplicationType is retrieved from the first part:

{code}
        HddsProtos.ReplicationType replicationType =
            partKeyInfoMap.firstEntry().getValue().getPartKeyInfo().getType();
{code}

Which is not yet available in this use case.",pull-request-available,['S3'],HDDS,Bug,Major,2019-08-10 12:32:27,6
13250036,Missing or error-prone test cleanup,Some integration tests do not clean up after themselves.  Some only clean up if the test is successful.,pull-request-available,['test'],HDDS,Bug,Major,2019-08-10 11:38:51,1
13250014,S3 MPU can't be created with octet-stream content-type ,"This problem is reported offline by [~shanekumpf@gmail.com].

When aws-sdk-go is used to access to s3 gateway of Ozone it sends the Multi Part Upload initialize message with ""application/octet-stream"" Content-Type. 

This Content-Type is missing from the aws-cli which is used to reimplement s3 endpoint.

The problem is that we use the same rest endpoint for initialize and complete Multipart Upload request. For the completion we need the CompleteMultipartUploadRequest parameter which is parsed from the body.

For initialize we have an empty body which can't be serialized to CompleteMultipartUploadRequest.

The workaround is to set a specific content type from a filter which help up to create two different REST method for initialize and completion message.

Here is an example to test (using bogus AWS credentials).

{code}
curl -H 'Host:yourhost' -H 'User-Agent:aws-sdk-go/1.15.11 (go1.11.2; linux; amd64)' -H 'Content-Length:0' -H 'Authorization:AWS4-HMAC-SHA256 Credential=qwe/20190809/ozone/s3/aws4_request, SignedHeaders=content-type;host;x-amz-acl;x-amz-content-sha256;x-amz-date;x-amz-storage-class, Signature=7726ed63990ba3f4f1f796d4ab263f5d9c3374528840f5e49d106dbef491f22c' -H 'Content-Type:application/octet-stream' -H 'X-Amz-Acl:private' -H 'X-Amz-Content-Sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' -H 'X-Amz-Date:20190809T070142Z' -H 'X-Amz-Storage-Class:STANDARD' -H 'Accept-Encoding:gzip' -X POST 'http://localhost:9999/docker/docker/registry/v2/repositories/apache/ozone-runner/_uploads/2173f019-09c3-466b-bb7d-c31ce749d826/data?uploads
{code}

Without the patch it returns with HTTP 405 (Not supported Media Type).",pull-request-available,['S3'],HDDS,Bug,Major,2019-08-10 07:04:32,6
13249995,CertificateClient should not persist keys/certs to ozone.metadata.dir,"For example, when OM and SCM are deployed on the same host with ozone.metadata.dir defined. SCM can start successfully but OM can not because the key/cert from OM will collide with SCM.",pull-request-available,[],HDDS,Improvement,Major,2019-08-10 00:11:23,37
13249942,TestKeyManagerImpl.testLookupKeyWithLocation is failing,"{code}
[ERROR]   TestKeyManagerImpl.testLookupKeyWithLocation:757 expected:<102ad7e3-4226-4966-af79-2b12a56f83cb{ip: 32.53.16.224, host: localhost-32.53.16.224, networkLocation: /default-rack, certSerialId: null}> but was:<d3e07bc3-6d24-4d80-9cdb-057a475084e7{ip: 238.199.149.19, host: localhost-238.199.149.19, networkLocation: /default-rack, certSerialId: null}>
{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-08-09 17:06:52,28
13249939,Support copy during S3 multipart upload part creation,"Uploads a part by copying data from an existing object as data source

Documented here:

https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html",pull-request-available,['S3'],HDDS,Sub-task,Blocker,2019-08-09 17:01:22,6
13249930,Unused executor in SimpleContainerDownloader,"{{SimpleContainerDownloader}} has an {{executor}} that's created and shut down, but never used.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Minor,2019-08-09 15:41:17,1
13249929,Closing open container via scmcli gives false error message,"{{scmcli close}} prints an error message about invalid state transition after it had successfully closed the container.

{code:title=CLI}
$ ozone scmcli info 2
...
Container State: OPEN
...

$ ozone scmcli close 2
...
client-09830A377AA9->f27bf787-8711-41d4-b0fd-3ef50b5c076f: receive RaftClientReply:client-09830A377AA9->f27bf787-8711-41d4-b0fd-3ef50b5c076f@group-7831D6F2EF1B, cid=0, SUCCESS, logIndex=11, commits[f27bf787-8711-41d4-b0fd-3ef50b5c076f:c12, 37ba33fe-c9ed-4ac2-a6e5-57ce658168b4:c11, feb68ba4-0a8a-4eda-9915-7dc090e5f46c:c11]
Failed to update container state #2, reason: invalid state transition from state: CLOSED upon event: CLOSE.

$ ozone scmcli info 2
...
Container State: CLOSED
...
{code}

{code:title=logs}
scm_1  | 2019-08-09 15:15:01 [IPC Server handler 1 on 9860] INFO  SCMClientProtocolServer:366 - Object type container id 1 op close new stage begin
dn3_1  | 2019-08-09 15:15:02 [RatisApplyTransactionExecutor 1] INFO  Container:356 - Container 1 is closed with bcsId 3.
dn1_1  | 2019-08-09 15:15:02 [RatisApplyTransactionExecutor 1] INFO  Container:356 - Container 1 is closed with bcsId 3.
scm_1  | 2019-08-09 15:15:02 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO  IncrementalContainerReportHandler:176 - Moving container #1 to CLOSED state, datanode feb68ba4-0a8a-4eda-9915-7dc090e5f46c{ip: 10.5.1.6, host: ozone-static_dn3_1.ozone-static_net, networkLocation: /default-rack, certSerialId: null} reported CLOSED replica.
dn2_1  | 2019-08-09 15:15:02 [RatisApplyTransactionExecutor 1] INFO  Container:356 - Container 1 is closed with bcsId 3.
scm_1  | 2019-08-09 15:15:02 [IPC Server handler 3 on 9860] INFO  SCMClientProtocolServer:366 - Object type container id 1 op close new stage complete
scm_1  | 2019-08-09 15:15:02 [IPC Server handler 3 on 9860] ERROR ContainerStateManager:335 - Failed to update container state #1, reason: invalid state transition from state: CLOSED upon event: CLOSE.
scm_1  | 2019-08-09 15:15:02 [IPC Server handler 3 on 9860] INFO  Server:2726 - IPC Server handler 3 on 9860, call Call#3 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.notifyObjectStageChange from 10.5.0.71:57746
scm_1  | org.apache.hadoop.hdds.scm.exceptions.SCMException: Failed to update container state #1, reason: invalid state transition from state: CLOSED upon event: CLOSE.
scm_1  | 	at org.apache.hadoop.hdds.scm.container.ContainerStateManager.updateContainerState(ContainerStateManager.java:336)
scm_1  | 	at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:312)
scm_1  | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.notifyObjectStageChange(SCMClientProtocolServer.java:379)
scm_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerLocationProtocolServerSideTranslatorPB.notifyObjectStageChange(StorageContainerLocationProtocolServerSideTranslatorPB.java:219)
scm_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:16398)
scm_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}",incompatibleChange pull-request-available,[],HDDS,Bug,Minor,2019-08-09 15:36:18,19
13249870,Owner/group information for a file should be returned from OzoneFileStatus,"BasicOzoneFilesystem returns the file's user/group information as the current user/group. This should default to the information read from the acl's for the file.

cc [~xyao]",Triaged,"['Ozone Filesystem', 'Security']",HDDS,Bug,Critical,2019-08-09 10:46:29,12
13249807,Change omPort parameter type from String to int in BasicOzoneFileSystem#createAdapter,"The diff will be based on HDDS-1891.

Goal:
1. Change omPort type to int because it is eventually used as int anyway
2. Refactor the parser code in BasicOzoneFileSystem#initialize

Will post a PR after HDDS-1891 is merged.",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2019-08-09 04:06:14,12
13249697,Acceptance tests fail if scm webui shows invalid json,"Acceptance test of a nightly build is failed with the following error:

{code}
Creating ozonesecure_datanode_3 ... 
[7A[2K
Creating ozonesecure_kdc_1      ... [32mdone[0m
[7B[6A[2K
Creating ozonesecure_om_1       ... [32mdone[0m
[6B[8A[2K
Creating ozonesecure_scm_1      ... [32mdone[0m
[8B[1A[2K
Creating ozonesecure_datanode_3 ... [32mdone[0m
[1B[5A[2K
Creating ozonesecure_kms_1      ... [32mdone[0m
[5B[4A[2K
Creating ozonesecure_s3g_1      ... [32mdone[0m
[4B[2A[2K
Creating ozonesecure_datanode_2 ... [32mdone[0m
[2B[3A[2K
Creating ozonesecure_datanode_1 ... [32mdone[0m
[3Bparse error: Invalid numeric literal at line 2, column 0
{code}

https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-5b87q/acceptance/output.log

The problem is in the script which checks the number of available datanodes.

If the HTTP endpoint of the SCM is already started BUT not ready yet it may return with a simple HTML error message instead of json. Which can not be parsed by jq:

In testlib.sh:

{code}
  37   │   if [[ ""${SECURITY_ENABLED}"" == 'true' ]]; then
  38   │     docker-compose -f ""${compose_file}"" exec -T scm bash -c ""kinit -k HTTP/scm@EXAMPL
       │ E.COM -t /etc/security/keytabs/HTTP.keytab && curl --negotiate -u : -s '${jmx_url}'""
  39   │   else
  40   │     docker-compose -f ""${compose_file}"" exec -T scm curl -s ""${jmx_url}""
  41   │   fi \
  42   │     | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value'
{code}

One possible fix is to adjust the error handling (set +x / set -x) per method instead of using a generic set -x at the beginning. It would provide a more predictable behavior. In our case count_datanode should not fail evert (as the caller method: wait_for_datanodes can retry anyway).",pull-request-available,[],HDDS,Bug,Major,2019-08-08 15:54:34,6
13249660,Improve the visibility with Ozone Insight tool,"Visibility is a key aspect for the operation of any Ozone cluster. We need better visibility to improve correctnes and performance. While the distributed tracing is a good tool for improving the visibility of performance we have no powerful tool which can be used to check the internal state of the Ozone cluster and debug certain correctness issues.

To improve the visibility of the internal components I propose to introduce a new command line application `ozone insight`.

The new tool will show the selected metrics / logs / configuration for any of the internal components (like replication-manager, pipeline, etc.).

For each insight points we can define the required logs and log levels, metrics and configuration and the tool can display only the component specific information during the debug.

h2. Usage

First we can check the available insight point:

{code}
bash-4.2$ ozone insight list
Available insight points:


  scm.node-manager                     SCM Datanode management related information.
  scm.replica-manager                  SCM closed container replication manager
  scm.event-queue                      Information about the internal async event delivery
  scm.protocol.block-location          SCM Block location protocol endpoint
  scm.protocol.container-location      Planned insight point which is not yet implemented.
  scm.protocol.datanode                Planned insight point which is not yet implemented.
  scm.protocol.security                Planned insight point which is not yet implemented.
  scm.http                             Planned insight point which is not yet implemented.
  om.key-manager                       OM Key Manager
  om.protocol.client                   Ozone Manager RPC endpoint
  om.http                              Planned insight point which is not yet implemented.
  datanode.pipeline[id]                More information about one ratis datanode ring.
  datanode.rocksdb                     More information about one ratis datanode ring.
  s3g.http                             Planned insight point which is not yet implemented.
{code}

Insight points can define configuration, metrics and/or logs. Configuration can be displayed based on the configuration objects:

{code}
ozone insight config scm.protocol.block-location
Configuration for `scm.protocol.block-location` (SCM Block location protocol endpoint)

>>> ozone.scm.block.client.bind.host
       default: 0.0.0.0
       current: 0.0.0.0

The hostname or IP address used by the SCM block client  endpoint to bind


>>> ozone.scm.block.client.port
       default: 9863
       current: 9863

The port number of the Ozone SCM block client service.


>>> ozone.scm.block.client.address
       default: ${ozone.scm.client.address}
       current: scm

The address of the Ozone SCM block client service. If not defined value of ozone.scm.client.address is used

{code}

Metrics can be retrieved from the prometheus entrypoint:

{code}
ozone insight metrics scm.protocol.block-location
Metrics for `scm.protocol.block-location` (SCM Block location protocol endpoint)

RPC connections

  Open connections: 0
  Dropped connections: 0
  Received bytes: 0
  Sent bytes: 0


RPC queue

  RPC average queue time: 0.0
  RPC call queue length: 0


RPC performance

  RPC processing time average: 0.0
  Number of slow calls: 0


Message type counters

  Number of AllocateScmBlock: 0
  Number of DeleteScmKeyBlocks: 0
  Number of GetScmInfo: 2
  Number of SortDatanodes: 0
{code}

Log levels can be adjusted with the existing logLevel servlet and can be collected / streamd via a simple logstream servlet:

{code}
ozone insight log scm.node-manager
[SCM] 2019-08-08 12:42:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:43:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:44:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:45:37,393 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 12:46:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
{code}

The verbose mode can display the raw messages as well:

{code}
[SCM] 2019-08-08 13:16:37,398 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]
[SCM] 2019-08-08 13:16:37,400 [TRACE|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] HB is received from [datanode=ozone_datanode_1.ozone_default]: 
storageReport {
  storageUuid: ""DS-bffe6bee-1166-4502-acf5-57fc16c5aa98""
  storageLocation: ""/data/hdds""
  capacity: 470282264576
  scmUsed: 16384
  remaining: 205695963136
  storageType: DISK
  failed: false
}

{code}

h2. Use cases

Ozone insight can be used for any kind of debuging. Some problem examples from my yesterday

 1. Due to a cache problem the volumes were created twice without any error at the second time. With this tool I can check the state of the internal cache, or check if the volume is added to the rocksdb itself.

 2. After fixing this problem we found an DNS caching issue. The OM responded with an error but it was not clear where the error was propagated from (it was created in OzoneManagerProtocolClientSideTranslatorPB.handleError). With checking the traffic between SCM and OM it can be easy to track the origin of a specific error.
 
 4. After fixing this problem we found some pipline problem (reported later at HDDS-1933). With this tool I could check the content of the reports and messages to the pipeline manager.

 


h2. Implementation

We can implement the tool without any significant code change as it uses existing features:

 * Metrics can be downloaded from the `/prom` endpoint
 * Log Level can be set with the existing `/logLevel` servlet endpoint (from hadoop-common)
 * Log lines can be streamed with a very simple new servlet
 * Configuration can be displayed based on configuration points

A new interface can be introduced for `InsightPoint`s where all the affected logs/levels, metrics and config classes can be defined for each components.

Prometheus servlet endpoint can be changed to be turned on by default.",pull-request-available,[],HDDS,Sub-task,Major,2019-08-08 13:24:26,6
13249629,TestSecureOzoneCluster may fail due to port conflict,"{{TestSecureOzoneCluster}} fails if SCM is already running on same host.

Steps to reproduce:

# Start {{ozone}} docker compose cluster
# Run {{TestSecureOzoneCluster}} test

{noformat:title=https://ci.anzix.net/job/ozone/17602/consoleText}
[ERROR] Tests run: 10, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 49.821 s <<< FAILURE! - in org.apache.hadoop.ozone.TestSecureOzoneCluster
[ERROR] testSCMSecurityProtocol(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 6.59 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
	at org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)
	at org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSCMSecurityProtocol(TestSecureOzoneCluster.java:277)
...

[ERROR] testSecureOmReInit(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 5.312 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
	at org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)
	at org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSecureOmReInit(TestSecureOzoneCluster.java:743)
...

[ERROR] testSecureOmInitSuccess(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 5.312 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
	at org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)
	at org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testSecureOmInitSuccess(TestSecureOzoneCluster.java:789)
...
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-08 10:06:47,1
13249583,Datanode should use hostname in place of ip addresses to allow DN's to work when ipaddress change,"This was noticed by [~elek] while deploying Ozone on Kubernetes based environment.

When the datanode ip address change on restart, the Datanode details cease to be correct for the datanode. and this prevents the cluster from functioning after a restart.",TriagePending,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2019-08-08 07:04:46,65
13249515,OM started on recon host in ozonesecure compose ,"OM is started temporarily on {{recon}} host in {{ozonesecure}} compose:

{noformat}
recon_1     | 2019-08-07 19:41:46 INFO  OzoneManagerStarter:51 - STARTUP_MSG:
recon_1     | /************************************************************
recon_1     | STARTUP_MSG: Starting OzoneManager
recon_1     | STARTUP_MSG:   host = recon/192.168.16.4
recon_1     | STARTUP_MSG:   args = [--init]
...
recon_1     | SHUTDOWN_MSG: Shutting down OzoneManager at recon/192.168.16.4
...
recon_1     | 2019-08-07 19:41:52 INFO  ReconServer:81 - Initializing Recon server...
{noformat}",pull-request-available,['docker'],HDDS,Bug,Minor,2019-08-07 19:48:59,1
13249510,Cannot run ozone-recon compose due to syntax error,"{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-recon
$ docker-compose up -d --scale datanode=3
ERROR: yaml.scanner.ScannerError: mapping values are not allowed here
  in ""./docker-compose.yaml"", line 20, column 33
{noformat}",pull-request-available,['docker'],HDDS,Bug,Major,2019-08-07 18:45:36,1
13249501,Consolidate add/remove Acl into OzoneAclUtil class,"This Jira is created based on @xiaoyu comment on HDDS-1884

Can we abstract these add/remove logic into common AclUtil class as we can see similar logic in both bucket manager and key manager? For example,

public static boolean addAcl(List existingAcls, OzoneAcl newAcl)
public static boolean removeAcl(List existingAcls, OzoneAcl newAcl)

 

But to do this, we need both OmKeyInfo and OMBucketInfo to use list of OzoneAcl/OzoneAclInfo.

This Jira is to do that refactor, and also address above comment to move common logic to AclUtils.",pull-request-available,[],HDDS,Bug,Major,2019-08-07 18:12:49,28
13249345,The new caching layer is used for old OM requests but not updated,"HDDS-1499 introduced a new caching layer together with a double-buffer based db writer to support OM HA.

TLDR: I think the caching layer is not updated for new volume creation. And (slightly related to this problem) I suggest to separated the TypedTable and the caching layer.

## How to reproduce the problem?

1. Start a docker compose cluster
2. Create one volume (let's say `/vol1`)
3. Restart the om (!)
4. Try to create an _other_ volume twice!

```
bash-4.2$ ozone sh volume create /vol2
2019-08-07 12:29:47 INFO  RpcClient:288 - Creating Volume: vol2, with hadoop as owner.
bash-4.2$ ozone sh volume create /vol2
2019-08-07 12:29:50 INFO  RpcClient:288 - Creating Volume: vol2, with hadoop as owner.
```

Expected behavior is an error:

{code}
bash-4.2$ ozone sh volume create /vol1
2019-08-07 09:48:39 INFO  RpcClient:288 - Creating Volume: vol1, with hadoop as owner.
bash-4.2$ ozone sh volume create /vol1
2019-08-07 09:48:42 INFO  RpcClient:288 - Creating Volume: vol1, with hadoop as owner.
VOLUME_ALREADY_EXISTS 
{code}

The problem is that the new cache is used even for the old code path (TypedTable):

{code}
 @Override
  public VALUE get(KEY key) throws IOException {
    // Here the metadata lock will guarantee that cache is not updated for same
    // key during get key.

    CacheResult<CacheValue<VALUE>> cacheResult =
        cache.lookup(new CacheKey<>(key));

    if (cacheResult.getCacheStatus() == EXISTS) {
      return cacheResult.getValue().getCacheValue();
    } else if (cacheResult.getCacheStatus() == NOT_EXIST) {
      return null;
    } else {
      return getFromTable(key);
    }
  }
{code}

For volume table after the FIRST start it always returns with `getFromTable(key)` due to the condition in the `TableCacheImpl.lookup`:

{code}

  public CacheResult<CACHEVALUE> lookup(CACHEKEY cachekey) {

    if (cache.size() == 0) {
      return new CacheResult<>(CacheResult.CacheStatus.MAY_EXIST,
          null);
    }
{code}

But after a restart the cache is pre-loaded by the TypedTable.constructor. After the restart, the real caching logic will be used (as cache.size()>0), which cause a problem as the cache is NOT updated from the old code path.

An additional problem is that the cache is turned on for all the metadata table even if the cache is not required... 

## Proposed solution

As I commented at HDDS-1499 this caching layer is not a ""traditional cache"". It's not updated during the typedTable.put() call but updated by a separated component during double-buffer flash.

I would suggest to remove the cache related methods from TypedTable (move to a separated implementation). I think this kind of caching can be independent from the TypedTable implementation. We can continue to use the simple TypedTable everywhere where we don't need to use any kind of caching.

For caching we can use a separated object. It would make it more visible that the cache should always be updated manually all the time. This separated caching utility may include a reference to the original TypedTable/Table. With this approach we can separate the different responsibilities but provide the same functionality.",pull-request-available,['Ozone Manager'],HDDS,Bug,Blocker,2019-08-07 12:43:28,13
13249316,ozonesecure acceptance test broken by HTTP auth requirement,"Acceptance test is failing at {{ozonesecure}} with the following error from {{jq}}:

{noformat:title=https://github.com/elek/ozone-ci/blob/325779d34623061e27b80ade3b749210648086d1/byscane/byscane-nightly-ds7lx/acceptance/output.log#L2779}
parse error: Invalid numeric literal at line 2, column 0
{noformat}

Example compose environments wait for datanodes to be up:

{code:title=https://github.com/apache/hadoop/blob/9cd211ac86bb1124bdee572fddb6f86655b19b73/hadoop-ozone/dist/src/main/compose/testlib.sh#L71-L72}
  docker-compose -f ""$COMPOSE_FILE"" up -d --scale datanode=""${datanode_count}""
  wait_for_datanodes ""$COMPOSE_FILE"" ""${datanode_count}""
{code}

The number of datanodes up is determined via HTTP query of JMX endpoint:

{code:title=https://github.com/apache/hadoop/blob/9cd211ac86bb1124bdee572fddb6f86655b19b73/hadoop-ozone/dist/src/main/compose/testlib.sh#L44-L46}
     #This line checks the number of HEALTHY datanodes registered in scm over the
     # jmx HTTP servlet
     datanodes=$(docker-compose -f ""${compose_file}"" exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo' | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value')
{code}

The problem is that no authentication is performed before or during the request, which is no longer allowed since HDDS-1901:

{code}
$ docker-compose exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo'
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 401 Authentication required</title>
</head>
<body><h2>HTTP ERROR 401</h2>
<p>Problem accessing /jmx. Reason:
<pre>    Authentication required</pre></p>
</body>
</html>
{code}

{code}
$ docker-compose exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo' | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value'
parse error: Invalid numeric literal at line 2, column 0
{code}",pull-request-available,"['docker', 'test']",HDDS,Bug,Critical,2019-08-07 10:16:57,1
13249260,ozone sh bucket path command does not exist,ozone sh bucket path command does not exist but it is mentioned in the static/docs/interface/s3.html. The command should either be added back or a the documentation should be improved.,pull-request-available,"['documentation', 'Ozone Manager']",HDDS,Bug,Blocker,2019-08-07 06:32:03,1
13249216,TestOzoneManagerDoubleBufferWithOMResponse is flaky,"{noformat:title=https://ci.anzix.net/job/ozone/17588/testReport/org.apache.hadoop.ozone.om.ratis/TestOzoneManagerDoubleBufferWithOMResponse/testDoubleBuffer/}
java.lang.AssertionError: expected:<11> but was:<9>
...
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:362)
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:104)
{noformat}

{noformat:title=https://ci.anzix.net/job/ozone/17587/testReport/org.apache.hadoop.ozone.om.ratis/TestOzoneManagerDoubleBufferWithOMResponse/unit___testDoubleBuffer/}
java.lang.AssertionError: expected:<11> but was:<3>
...
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:362)
	at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:104)
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-06 21:36:49,1
13249214,Place ozone.om.address config key default value in ozone-site.xml,"{code:xml}
   <property>
     <name>ozone.om.address</name>
-    <value/>
+    <value>0.0.0.0:9862</value>
     <tag>OM, REQUIRED</tag>
     <description>
       The address of the Ozone OM service. This allows clients to discover
{code}",pull-request-available,[],HDDS,Improvement,Major,2019-08-06 21:13:32,12
13249146,Fix Javadoc in TestAuditParser,"The Javadoc for TestAuditParser is mentions incorrect class name.


{code:java}
/**
 * Tests GenerateOzoneRequiredConfigurations.
 */
{code}
",newbie pull-request-available,['documentation'],HDDS,Bug,Minor,2019-08-06 14:45:47,104
13249067,hadoop-ozone-tools has integration tests run as unit,"HDDS-1735 created separate test runner scripts for unit and integration tests.

Problem: {{hadoop-ozone-tools}} tests are currently run as part of the unit tests, but most of them start a {{MiniOzoneCluster}}, which is defined in {{hadoop-ozone-integration-test}}.  Thus I think these tests are really integration tests, and should be run by {{integration.sh}} instead.  There are currently only 3 real unit tests:

{noformat}
hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/audit/parser/TestAuditParser.java
hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestProgressBar.java
hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/genconf/TestGenerateOzoneRequiredConfigurations.java
{noformat}

{{hadoop-ozone-tools}} tests take ~6 minutes.

Possible solutions in order of increasing complexity:

# Run {{hadoop-ozone-tools}} tests in {{integration.sh}} instead of {{unit.sh}} (This is similar to {{hadoop-ozone-filesystem}}, which is already run by {{integration.sh}} and has 2 real unit tests.)
# Move all integration test classes to the {{hadoop-ozone-integration-test}} module, and make it depend on {{hadoop-ozone-tools}} and {{hadoop-ozone-filesystem}} instead of the other way around.
# Rename integration test classes to {{\*IT.java}} or {{IT\*.java}}, add filters for Surefire runs.",pull-request-available,"['build', 'test']",HDDS,Improvement,Minor,2019-08-06 09:54:03,1
13249058,TestOzoneRpcClientAbstract is failing,"TestOzoneRpcClientAbstract is failing with the below error
{noformat}
[ERROR] testNativeAclsForKey(org.apache.hadoop.ozone.client.rpc.TestSecureOzoneRpcClient)  Time elapsed: 0.113 s  <<< FAILURE!
java.lang.AssertionError: READ_ACL should exist in current acls:group:jenkins:a[ACCESS]
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.validateOzoneAccessAcl(TestOzoneRpcClientAbstract.java:2466)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testNativeAclsForKey(TestOzoneRpcClientAbstract.java:2300)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-08-06 09:01:05,19
13249055,Only contract tests are run in ozonefs module,"{{hadoop-ozone-filesystem}} has 6 test classes that are not being run:

{code}
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestFilteredClassLoader.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFSInputStream.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileInterfaces.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemWithMocks.java
hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFsRenameDir.java
{code}

{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-vxsck/integration/output.log}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDelete
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.956 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDelete
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.528 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractSeek
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.245 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractSeek
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractOpen
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.996 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractOpen
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.816 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.418 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.042 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractCreate
[WARNING] Tests run: 11, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 35.144 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractCreate
[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRootDir
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.986 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRootDir
[INFO] 
[INFO] Results:
[INFO] 
[WARNING] Tests run: 92, Failures: 0, Errors: 0, Skipped: 2
{code}",pull-request-available,['test'],HDDS,Bug,Minor,2019-08-06 08:46:28,1
13249046,Remove hadoop script from ozone distribution,"/bin/hadoop script is included in the ozone distribution even if we a dedicated /bin/ozone

[~arp] reported that it can be confusing, for example ""hadoop classpath"" returns with a bad classpath (ozone classpath <projectname>) should be used instead.

To avoid such confusions I suggest to remove the hadoop script from distribution as ozone script already provides all the functionalities.

It also helps as to reduce the dependencies between hadoop 3.2-SNAPSHOT and ozone as we use the snapshot hadoop script as of now.",pull-request-available,[],HDDS,Bug,Major,2019-08-06 08:10:21,6
13249041,Ozonescript example docker-compose cluster can't be started,"the compose/ozonescripts cluster provides an example environment to test the start-ozone.sh and stop-ozone.sh scripts.

It starts containers with sshd daemon but witout starting the ozone which makes it possible to start those scripts.

Unfortunately the docker files are broken since:
 * we switched from debian to centos with the base image
 * we started to use /etc/hadoop instead of /opt/hadoop/etc/hadoop for configuring the hadoop (workers file should be copied there)
 * we started to use jdk11 to execute ozone (instead of java8)

The configuration files should be updated according to these changes. 

# How to test this patch?

(1) Do a full build and try to start the ./compose/ozonescripts cluster (check the related README file in the directory):

```
docker-compose up -d
```

(2) start the ozone processes with ./start.sh (from compose/ozonescripts)

(3) wait and check if om/scm webui are working and you have 1 healthy datanode

",pull-request-available,[],HDDS,Bug,Major,2019-08-06 08:03:22,6
13248992,Fix OzoneBucket and RpcClient APIS for acl,"Fix addAcl,removeAcl in OzoneBucket to use newly added acl API's addAcl/removeAcl as part of HDDS-1739.

Remove addBucketAcls, removeBucketAcls from RpcClient. We should use addAcl/removeAcl.

 

And also fix @xiaoyu comment on HDDS-1900 jira. BucketManagerImpl#setBucketProperty() as they now require a different permission (WRITE_ACL instead of WRITE)?",pull-request-available,[],HDDS,Bug,Blocker,2019-08-06 02:30:01,13
13248984,Support Prefix ACL operations for OM HA.,+-HDDS-1608-+ adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-05 23:18:45,13
13248887,Cannot build hadoop-hdds-config from scratch in IDEA,"Building {{hadoop-hdds-config}} from scratch (eg. right after checkout or after {{mvn clean}}) in IDEA fails with the following error:

{code}
Error:java: Bad service configuration file, or exception thrown while constructing Processor object: javax.annotation.processing.Processor: Provider org.apache.hadoop.hdds.conf.ConfigFileGenerator not found
{code}",pull-request-available,['build'],HDDS,Bug,Minor,2019-08-05 14:10:30,1
13248792,Use new HA code for Non-HA in OM,This Jira is to use new HA code of OM in Non-HA code path.,pull-request-available,[],HDDS,New Feature,Major,2019-08-04 22:29:20,13
13248766,TestMultiBlockWritesWithDnFailures is failing,"TestMultiBlockWritesWithDnFailures is failing with the following exception
{noformat}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.992 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures
[ERROR] testMultiBlockWritesWithDnFailures(org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures)  Time elapsed: 30.941 s  <<< ERROR!
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:720)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.allocateBlock(OzoneManagerProtocolClientSideTranslatorPB.java:752)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool.allocateNewBlock(BlockOutputStreamEntryPool.java:248)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool.allocateBlockIfNeeded(BlockOutputStreamEntryPool.java:296)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:201)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:376)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:325)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:231)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193)
	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures.testMultiBlockWritesWithDnFailures(TestMultiBlockWritesWithDnFailures.java:144)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-04 13:56:15,1
13248765,TestOzoneRpcClientWithRatis is failing with ACL errors,"{noformat}
[ERROR] testNativeAclsForKey(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.176 s  <<< FAILURE!
java.lang.AssertionError: Current acls:,[user:nvadivelu:a[ACCESS], group:staff:a[ACCESS], group:everyone:a[ACCESS], group:localaccounts:a[ACCESS], group:_appserverusr:a[ACCESS], group:admin:a[ACCESS], group:_appserveradm:a[ACCESS], group:_lpadmin:a[ACCESS], group:com.apple.sharepoint.group.1:a[ACCESS], group:com.apple.sharepoint.group.2:a[ACCESS], group:_appstore:a[ACCESS], group:_lpoperator:a[ACCESS], group:_developer:a[ACCESS], group:_analyticsusers:a[ACCESS], group:com.apple.access_ftp:a[ACCESS], group:com.apple.access_screensharing:a[ACCESS], group:com.apple.access_ssh:a[ACCESS], group:com.apple.sharepoint.group.3:a[ACCESS]] inheritedUserAcl:user:remoteUser:r[ACCESS]

[ERROR] testNativeAclsForBucket(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.074 s  <<< FAILURE!
java.lang.AssertionError

[ERROR] testNativeAclsForPrefix(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.061 s  <<< FAILURE!
java.lang.AssertionError: Current acls:,[user:nvadivelu:a[ACCESS], group:staff:a[ACCESS], group:everyone:a[ACCESS], group:localaccounts:a[ACCESS], group:_appserverusr:a[ACCESS], group:admin:a[ACCESS], group:_appserveradm:a[ACCESS], group:_lpadmin:a[ACCESS], group:com.apple.sharepoint.group.1:a[ACCESS], group:com.apple.sharepoint.group.2:a[ACCESS], group:_appstore:a[ACCESS], group:_lpoperator:a[ACCESS], group:_developer:a[ACCESS], group:_analyticsusers:a[ACCESS], group:com.apple.access_ftp:a[ACCESS], group:com.apple.access_screensharing:a[ACCESS], group:com.apple.access_ssh:a[ACCESS], group:com.apple.sharepoint.group.3:a[ACCESS]] inheritedUserAcl:user:remoteUser:r[ACCESS]
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-04 13:20:31,28
13248764,TestScmSafeMode#testSCMSafeModeRestrictedOp is failing,"{noformat}
[ERROR] testSCMSafeModeRestrictedOp(org.apache.hadoop.ozone.om.TestScmSafeMode)  Time elapsed: 19.316 s  <<< FAILURE!
java.lang.AssertionError: Expected a org.apache.hadoop.hdds.scm.exceptions.SCMException to be thrown, but got the result: : ContainerInfo{id=1, state=OPEN, pipelineID=PipelineID=100fb566-2cc0-44d6-9897-e688af5c447f, stateEnterTime=137318188, owner=5c69dc7b-2a6b-4650-a625-a63117c11d2d} | Pipeline[ Id: 100fb566-2cc0-44d6-9897-e688af5c447f, Nodes: b91596ea-34ed-4628-a027-a1cdf05095be{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null}, Type:STAND_ALONE, Factor:ONE, State:OPEN]
	at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:492)
	at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:377)
	at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:446)
	at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeModeRestrictedOp(TestScmSafeMode.java:331)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-08-04 13:18:31,19
13248762,PipelineActionHandler is not closing the pipeline when close action is received,"PipelineActionHandler is not closing the pipeline when close action is received.
The bug was introduced as part of HDDS-1832 change.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-08-04 11:51:08,19
13248702,Use dynamic ports for SCM in TestSCMClientProtocolServer and TestSCMSecurityProtocolServer,"We should use dynamic port for SCM in the following test-cases
* TestSCMClientProtocolServer
* TestSCMSecurityProtocolServer",newbie pull-request-available,['test'],HDDS,Improvement,Major,2019-08-03 07:28:24,30
13248699,Fix Ozone HTTP WebConsole Authentication,"This was found during integration testing where the http authentication is enabled but anonymous can still access the ozone http web console like scm:9876 or om:9874. This can be reproed with the following configurations added to the ozonesecure docker-compose.

{code}

CORE-SITE.XML_hadoop.http.authentication.simple.anonymous.allowed=false

CORE-SITE.XML_hadoop.http.authentication.signature.secret.file=/etc/security/http_secret

CORE-SITE.XML_hadoop.http.authentication.type=kerberos

CORE-SITE.XML_hadoop.http.authentication.kerberos.principal=HTTP/_HOST@EXAMPLE.COM

CORE-SITE.XML_hadoop.http.authentication.kerberos.keytab=/etc/security/keytabs/HTTP.keytab

CORE-SITE.XML_hadoop.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer

{code}

After debugging into the KerberosAuthenticationFilter, the root cause is the name of the keytab does not follow the AuthenticationFilter tradition. The fix is to change 

hdds.scm.http.kerberos.keytab.file to hdds.scm.http.kerberos.keytab and
hdds.om.http.kerberos.keytab.file to hdds.om.http.kerberos.keytab

I will also add an integration test for this under ozonesecure docker-compose. ",pull-request-available,[],HDDS,Bug,Major,2019-08-03 06:50:57,28
13248679,Remove UpdateBucket handler which supports add/remove Acl,"This Jira is to remove bucket update handler.

To add acl/remove acl we should use ozone sh bucket addacl/ozone sh bucket removeacl.

 

Otherwise, when security is enabled, old Bucket update handler, uses setBucketProperty and that checks acl acces for WRITE, whereas when add/remove Acl we should check access for WRITE_ACL.

 

If we have both ways, even if a USER does not have WRITE_ACL can still add/remove Acls on a bucket.

 

This Jira is to clean up the old code and fix this security issue.",pull-request-available,[],HDDS,Bug,Critical,2019-08-02 22:59:07,13
13248536,GrpcReplicationService#download cannot replicate the container,"Replication of container is failing because of rocksdb is unable to find the underlying files.

{code}
2019-08-02 14:07:26,670 INFO  replication.GrpcReplicationService (GrpcReplicationService.java:close(124)) - 663284 bytes written to th
e rpc stream from container 12
2019-08-02 14:07:26,670 ERROR replication.GrpcReplicationService (GrpcReplicationService.java:download(65)) - Can't stream the contain
er data
java.io.FileNotFoundException: /Users/msingh/code/apache/ozone/github/chaos_runs/hadoop-ozone/integration-test/target/test/data/MiniOzoneClusterImpl-403d87c2-5cbe-4511-8e14-dce727f10cf9/datanode-7/data/containers/hdds/9f2a75dc-3243-462a-a90e-c83f63ad0d55/current/containerDir0/12/metadata/12-dn-container.db/002084.log (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includeFile(TarContainerPacker.java:243)
        at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includePath(TarContainerPacker.java:233)
        at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.pack(TarContainerPacker.java:164)
        at org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource.java:67)
        at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)
        at org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolServiceGrpc.java:217)
        at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
        at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
        at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.io.IOException: This archives contains unclosed entries.
                at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.finish(TarArchiveOutputStream.java:214)
                at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.close(TarArchiveOutputStream.java:229)
                at org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.pack(TarContainerPacker.java:173)
                ... 11 more
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-08-02 08:41:41,19
13248533,SCMNodeManager.java#getNodeByAddress cannot find nodes by addresses,"SCMNodeManager cannot find the nodes via ip addresses in MiniOzoneChaosCluster

{code}
2019-08-02 13:57:01,501 WARN  node.SCMNodeManager (SCMNodeManager.java:getNodeByAddress(599)) - Cannot find node for address 127.0.0.1
{code}

cc: [~xyao] & [~Sammi]",MiniOzoneChaosCluster,['SCM'],HDDS,Bug,Major,2019-08-02 08:35:29,77
13248497,Suppress WARN log from NetworkTopology#getDistanceCost ,"When RackAwareness is enabled and client from outside, the distance calculation flood SCM log with the following messages. This ticket is opened to suppress the WARN log.

{code}

2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology
2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology
2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-08-02 06:12:58,28
13248490,Support Key ACL operations for OM HA.,+HDDS-1541+ adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-08-02 04:53:33,13
13248483,Support listPipelines by filters in scmcli,"Today scmcli has a subcmd that allow list all pipelines. This ticket is opened to filter the results by switches, e.g., filter by Factor: THREE and State: OPEN. This will be useful for trouble shooting in large cluster.

 

{code}

bin/ozone scmcli listPipelines

Pipeline[ Id: a8d1b0c9-e1d4-49ea-8746-3f61dfb5ee3f, Nodes: cce44fde-bc8d-4063-97b3-6f557af756e1\{ip: 10.17.112.65, host: ia0230.halxg.cloudera.com, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
Pipeline[ Id: c9c453d1-d74c-4414-b87f-1d3585d78a7c, Nodes: 0b7b0b93-8323-4b82-8cc0-a9a5c10ab827\{ip: 10.17.112.29, host: ia0138.halxg.cloudera.com, networkLocation: /default-rack, certSerialId: null}c756a0e0-5a1b-4d03-ba5b-cafbcabac877\{ip: 10.17.112.27, host: ia0134.halxg.cloudera.com, networkLocation: /default-rack, certSerialId: null}bee45bd7-1ee6-4726-b3d1-81476dc1eb49\{ip: 10.17.112.28, host: ia0136.halxg.cloudera.com, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-08-02 04:15:08,77
13248455,Fix bug in removeAcl in Bucket,"{code:java}
// When we are removing subset of rights from existing acl.
for(OzoneAcl a: bucketInfo.getAcls()) {
 if(a.getName().equals(acl.getName()) &&
 a.getType().equals(acl.getType())) {
 BitSet bits = (BitSet) acl.getAclBitSet().clone();
 bits.and(a.getAclBitSet());

 if (bits.equals(ZERO_BITSET)) {
 return false;
 }
 bits = (BitSet) acl.getAclBitSet().clone();
 bits.and(a.getAclBitSet());
 a.getAclBitSet().xor(bits);

 if(a.getAclBitSet().equals(ZERO_BITSET)) {
 bucketInfo.getAcls().remove(a);
 }
 break;
 } else {
 return false;
 }{code}
In for loop, if first one is not matching with name and type, in else we return false. We should iterate entire acl list and then return response.
}",pull-request-available,"['Ozone Manager', 'Security']",HDDS,Bug,Blocker,2019-08-02 00:50:38,13
13248446,Ozone fs shell command should work with default port when port number is not specified,"{code:bash|title=Without port number -> Error}
$ ozone fs -ls o3fs://bucket.volume.localhost/
-ls: Ozone file system url should be either one of the two forms: o3fs://bucket.volume/key  OR o3fs://bucket.volume.om-host.example.com:5678/key
...
{code}
{code:bash|title=With port number -> Success}
$ ozone fs -ls o3fs://bucket.volume.localhost:9862/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop       1485 2019-08-01 21:14 o3fs://bucket.volume.localhost:9862/README.txt
{code}

We expect the first command to attempt port 9862 by default.",pull-request-available,[],HDDS,Improvement,Major,2019-08-01 23:01:03,12
13248351,Add support for verifying multiline log entry,"This jira aims to test the failure scenario where a multi-line stack trace will be added in the audit log. Currently, for test assumes that even in failure scenario we don't have multi-line log entry.
Example:

{code:java}
private static final AuditMessage READ_FAIL_MSG =
      new AuditMessage.Builder()
          .setUser(""john"")
          .atIp(""192.168.0.1"")
          .forOperation(DummyAction.READ_VOLUME.name())
          .withParams(PARAMS)
          .withResult(FAILURE)
          .withException(null).build();
{code}

Therefore in verifyLog() we only compare for first line of the log file with the expected message.
The test would fail if in future someone were to create a scenario with multi-line log entry.

1. Update READ_FAIL_MSG so that it has multiple lines of Exception stack trace.
This is what multi-line log entry could look like:
{code:java}
ERROR | OMAudit | user=dchitlangia | ip=127.0.0.1 | op=GET_ACL {volume=volume80100, bucket=bucket83878, key=null, aclType=CREATE, resourceType=volume, storeType=ozone} | ret=FAILURE
org.apache.hadoop.ozone.om.exceptions.OMException: User dchitlangia doesn't have CREATE permission to access volume
 at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1809) ~[classes/:?]
 at org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1769) ~[classes/:?]
 at org.apache.hadoop.ozone.om.OzoneManager.createBucket(OzoneManager.java:2092) ~[classes/:?]
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.createBucket(OzoneManagerRequestHandler.java:526) ~[classes/:?]
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:185) ~[classes/:?]
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:192) ~[classes/:?]
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:110) ~[classes/:?]
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) ~[classes/:?]
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) ~[hadoop-common-3.2.0.jar:?]
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) ~[hadoop-common-3.2.0.jar:?]
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) ~[hadoop-common-3.2.0.jar:?]
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) ~[hadoop-common-3.2.0.jar:?]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_144]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_144]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.2.0.jar:?]
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) ~[hadoop-common-3.2.0.jar:?]

{code}

2. Update verifyLog method to accept variable number of arguments.
3. Update the assertion so that it compares beyond the first line when the expected is a multi-line log entry.
{code:java}
assertTrue(expected.equalsIgnoreCase(lines.get(0)));
{code}
",newbie pull-request-available,['test'],HDDS,Test,Major,2019-08-01 14:18:56,90
13248308,Add containers to node2container map in SCM as part of ICR processing,"In SCM node2container and node2pipeline maps are managed by NodeManager and pipeline2container map is managed by PipelineManager.
Currently, when a container is allocated in SCM, it is added to pipeline2container map and we are not adding it to node2container map. We update the node2container map only when the datanode sends full container report.

When a node is marked as dead, DeadNodeHandler processes the event and it gets the list of containers that are hosted by the dead datanode and updates the respective container replica state in ContainerManager. The list of containers on the datanode is read from node2container map, this map will be missing containers which are created recently (after the last container report). In such cases we not be able to remove the container replica information for those containers. In reality, these containers are under replicated, but SCM will never know.

We should add containers to node2container map in SCM as soon as a container is allocated.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-08-01 10:45:53,19
13248306,Enable all the blockade test-cases,"Some of the blockade tests were {{Ignored}} because of open issues, since most of the issues are resolved we can go ahead and enable all the ignored blockade test-cases.",pull-request-available,['test'],HDDS,Improvement,Major,2019-08-01 10:39:48,19
13248255,Use ArrayList#clear to address audit failure scenario,"TestOzoneAuditLogger makes use of ArrayList#remove to clear the log file in between test runs.

When writing tests in future for more failures scenarios, the tests will fail if the log entry has multi-line stack trace in audit logs.

This jira aims to use ArrayList#clear to make the test future proof.",pull-request-available,['test'],HDDS,Test,Minor,2019-08-01 06:03:34,81
13248165,Fix bug in checkAcls in OzoneManager,"For HA scenario, checkAcls we pass the param UGI and remoteAddress. They need to be used in the checkAcls. HDDS-909 changed this behavior and not using the passed params.",pull-request-available,[],HDDS,Bug,Major,2019-07-31 17:32:00,13
13248162,Support Bucket ACL operations for OM HA.,-HDDS-15+40+- adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-31 17:10:18,13
13248091,Design doc: decommissioning in Ozone,Design doc can be attached to the documentation. In this jira the design doc will be attached and merged to the documentation page.,design pull-request-available,[],HDDS,Sub-task,Major,2019-07-31 11:56:51,6
13248089,Decommissioning and maintenance mode in Ozone ,This is the umbrella jira for decommissioning support in Ozone. Design doc will be attached soon.,pull-request-available,['SCM'],HDDS,New Feature,Major,2019-07-31 11:55:49,11
13248084,checkstyle error in ContainerStateMachine,"{noformat:title=https://ci.anzix.net/job/ozone/17488/artifact/build/checkstyle.out}
[ERROR] src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java:[186] (sizes) LineLength: Line is longer than 80 characters (found 85).
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-07-31 11:32:05,1
13248047,hadoop31-mapreduce fails due to wrong HADOOP_VERSION,"hadoop31-mapreduce fails with:

{noformat:title=https://elek.github.io/ozone-ci/byscane/byscane-nightly-gl52x/acceptance/smokeresult/log.html#s1-s2-t2-k2-k2}
JAR does not exist or is not a normal file: /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar
{noformat}

because 3.1 test is being run with {{HADOOP_VERSION=3}}:

{noformat:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-gl52x/acceptance/output.log}
Creating network ""hadoop31_default"" with the default driver
Pulling nm (flokkr/hadoop:3)...
3: Pulling from flokkr/hadoop
Digest: sha256:62e3488e64ff8c0406752fc4f263ae2549e04fedf02534469913c496c6a89d78
Status: Downloaded newer image for flokkr/hadoop:3
{noformat}

which has Hadoop 3.2.0 instead of 3.1.2:

{noformat:title=docker run -it --entrypoint /bin/bash flokkr/hadoop:3 -c 'ls -la /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples*'}
-rw-r--r--    1 hadoop   flokkr      316570 Jan  8  2019 /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar
{noformat}

{noformat:title=docker run -it --entrypoint /bin/bash flokkr/hadoop:3.1.2 -c 'ls -la /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples*'}
-rw-r--r--    1 hadoop   flokkr      316380 Jan 29  2019 /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar
{noformat}

This only happens with {{acceptance.sh}}, not when directly using {{test-all.sh}}, because the former explicitly defines {{HADOOP_VERSION}}:

{noformat:title=https://github.com/apache/hadoop/blob/d4ab9aea6f9cbcdcaf48b821e5be04b4e952b133/hadoop-ozone/dev-support/checks/acceptance.sh#L19}
export HADOOP_VERSION=3
{noformat}

so the correct value from {{.env}} file is ignored:

{noformat:title=https://github.com/apache/hadoop/blob/d4ab9aea6f9cbcdcaf48b821e5be04b4e952b133/hadoop-ozone/dist/src/main/compose/ozone-mr/hadoop31/.env#L21}
HADOOP_VERSION=3.1.2
{noformat}",pull-request-available,['test'],HDDS,Bug,Blocker,2019-07-31 08:49:22,1
13248027,hadoop27 acceptance test cannot be run,"{noformat:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-gl52x/acceptance/output.log}
Executing test in /workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27
The HADOOP_RUNNER_VERSION variable is not set. Defaulting to a blank string.
The HADOOP_IMAGE variable is not set. Defaulting to a blank string.
Removing network hadoop27_default
Network hadoop27_default not found.
The HADOOP_RUNNER_VERSION variable is not set. Defaulting to a blank string.
The HADOOP_IMAGE variable is not set. Defaulting to a blank string.
Creating network ""hadoop27_default"" with the default driver
no such image: apache/ozone-runner:: invalid reference format
ERROR: Test execution of /workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27 is FAILED!!!!
cp: cannot stat '/workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27/result/robot-*.xml': No such file or directory
{noformat}",pull-request-available,[],HDDS,Bug,Blocker,2019-07-31 07:20:35,1
13247999,Fix failures in TestS3MultipartUploadAbortResponse,[https://ci.anzix.net//job/ozone/17503//testReport/junit/org.apache.hadoop.ozone.om.response.s3.multipart/TestS3MultipartUploadAbortResponse/testAddDBToBatchWithParts/],pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-31 03:55:35,13
13247751,Add API to get last completed times for every Recon task.,"Recon stores the last ozone manager snapshot received timestamp along with timestamps of last successful run for each task.

We can add a REST endpoint that returns the contents of this stored data. 

This is important to give users a sense of how latest the current data that they are looking at is. And, we need this per task because some tasks might fail to run or might take much longer time to run than other tasks and this needs to be reflected in the UI for better and consistent user experience.",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2019-07-29 23:30:26,98
13247744,Fix entry clean up from openKeyTable during complete MPU,"# Initiate MPU adds entry to openKeyTable and multipartInfo table.
 # When completeMPU, we add the entry to keyTable and delete from multipartInfo table.

Deleting from openKeyTable is missing in complete MPU.",pull-request-available,"['Ozone Manager', 'S3']",HDDS,Bug,Major,2019-07-29 22:47:21,13
13247649,Remove anti-affinity rules from k8s minkube example,"HDDS-1646 introduced real persistence for k8s example deployment files which means that we need anti-affinity scheduling rules: Even if we use statefulset instead of daemonset we would like to start one datanode per real nodes.

With minikube we have only one node therefore the scheduling rule should be removed to enable at least 3 datanodes on the same physical nodes.

How to test:

{code}
 mvn clean install -DskipTests -f pom.ozone.xml
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/kubernetes/examples/minikube
minikube start
kubectl apply -f .
kc get pod
{code}

You should see 3 datanode instances.
",pull-request-available,['kubernetes'],HDDS,Bug,Blocker,2019-07-29 12:44:31,6
13247620,ConcurrentModification at PrometheusMetricsSink,"Encountered on {{ozoneperf}} compose env when running low on CPU:

{code}
om_1          | java.util.ConcurrentModificationException
om_1          | 	at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1493)
om_1          | 	at java.base/java.util.HashMap$ValueIterator.next(HashMap.java:1521)
om_1          | 	at org.apache.hadoop.hdds.server.PrometheusMetricsSink.writeMetrics(PrometheusMetricsSink.java:123)
om_1          | 	at org.apache.hadoop.hdds.server.PrometheusServlet.doGet(PrometheusServlet.java:43)
{code}",pull-request-available,[],HDDS,Bug,Minor,2019-07-29 09:59:40,1
13247348,Ozone pipelines should be marked as ready only after the leader election is complete,"Ozone pipeline on create and restart, start in allocated state. They are moved into open state after all the pipeline have reported to it. However, this potentially can lead into an issue where the pipeline is still not ready to accept any incoming IO operations.

The pipelines should be marked as ready only after the leader election is complete and leader is ready to accept incoming IO.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2019-07-26 15:31:04,86
13247311,Invalid Prometheus metric name from JvmMetrics,"{noformat}
target=http://scm:9876/prom msg=""append failed"" err=""invalid metric type \""_old _generation counter\""""
{noformat}",pull-request-available,[],HDDS,Bug,Major,2019-07-26 12:26:17,1
13247180,"Freon RandomKeyGenerator even if keySize is set to 0, it returns some random data to key"," 
{code:java}
***************************************************
Status: Success
Git Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf
Number of Volumes created: 1
Number of Buckets created: 1
Number of Keys added: 1
Ratis replication factor: THREE
Ratis replication type: STAND_ALONE
Average Time spent in volume creation: 00:00:00,002
Average Time spent in bucket creation: 00:00:00,000
Average Time spent in key creation: 00:00:00,002
Average Time spent in key write: 00:00:00,101
Total bytes written: 0
Total Execution time: 00:00:05,699
 
{code}
***************************************************

[root@ozoneha-2 ozone-0.5.0-SNAPSHOT]# bin/ozone sh key list /vol-0-28271/bucket-0-95211

[

{   ""version"" : 0,   ""md5hash"" : null,   ""createdOn"" : ""Fri, 26 Jul 2019 01:02:08 GMT"",   ""modifiedOn"" : ""Fri, 26 Jul 2019 01:02:09 GMT"",   ""size"" : 36,   ""keyName"" : ""key-0-98235"",   ""type"" : null }

]

 

This is because of the below code in RandomKeyGenerator:
{code:java}
for (long nrRemaining = keySize - randomValue.length;
 nrRemaining > 0; nrRemaining -= bufferSize) {
 int curSize = (int) Math.min(bufferSize, nrRemaining);
 os.write(keyValueBuffer, 0, curSize);
}
os.write(randomValue);
os.close();{code}
 ",pull-request-available,[],HDDS,Bug,Major,2019-07-26 01:07:04,13
13247165,Fix TableCacheImpl cleanup logic,"Currently in cleanup, we iterate over epochEntries and cleaup the entries from cache and epochEntries set.

 

epochEntries is a TreeSet<> which is not a concurrent datastructure of java. We may see issue some times, when cleanup tries to remove entries and some other thread tries to add entries to cache. So, we need to use some concurrent set over there.

 

During cluster testing, seen this some times randomly:
 
{code:java}
019-07-25 15:28:41,087 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9862, call Call#8974 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.65.15.233:35222 java.lang.NullPointerException at java.util.TreeMap.fixAfterInsertion(TreeMap.java:2295) at java.util.TreeMap.put(TreeMap.java:582) at java.util.TreeSet.add(TreeSet.java:255) at org.apache.hadoop.utils.db.cache.TableCacheImpl.put(TableCacheImpl.java:75) at org.apache.hadoop.utils.db.TypedTable.addCacheEntry(TypedTable.java:218) at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareCreateKeyResponse(OMKeyRequest.java:292) at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:188) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:134) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method){code}
 
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-25 23:04:54,13
13246921,mTLS support for Ozone is not correct,"Thanks to Josh for reporting that we have missing 'Not' in the if condition check.
{code}
if (conf.isGrpcMutualTlsRequired()) {
        return new GrpcTlsConfig(
            null, null, conf.getTrustStoreFile(), false);
      } else {
        return new GrpcTlsConfig(conf.getClientPrivateKeyFile(),
            conf.getClientCertChainFile(), conf.getTrustStoreFile(), true);
      }
{code}

it should have been
{code}
if (!conf.isGrpcMutualTlsRequired()) {
        return new GrpcTlsConfig(
            null, null, conf.getTrustStoreFile(), false);
      } else {
        return new GrpcTlsConfig(conf.getClientPrivateKeyFile(),
            conf.getClientCertChainFile(), conf.getTrustStoreFile(), true);
      }
{code}



",newbie,[],HDDS,Bug,Critical,2019-07-24 22:04:01,86
13246913,Make changes required for Non-HA to use new HA code in OM.,"In this Jira following things will be implemented:
 # Make the necessary changes for non-HA code path to use Cache and DoubleBuffer.

 ## When adding to double buffer, return future. This future will be used in the non-HA path to wait for this, and when it is completed return response to the client.
 ## Add to double-buffer will happen inside validateAndUpdateCache. In this way, in non-HA, when multiple RPC handler threads are calling preExecute and validateAndUpdateCache, the order inserted in to double buffer will happen in the order requests are received.

 

In this Jira, we shall not convert non-ha code path to use this, as security and acl work is not completed to use this new model.

 

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-24 21:28:58,13
13246905,TestStorageContainerManager#testScmProcessDatanodeHeartbeat is failing,"{{TestStorageContainerManager#testScmProcessDatanodeHeartbeat}} is failing with the following exception

{noformat}
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 106.315 s <<< FAILURE! - in org.apache.hadoop.ozone.TestStorageContainerManager
[ERROR] testScmProcessDatanodeHeartbeat(org.apache.hadoop.ozone.TestStorageContainerManager)  Time elapsed: 21.97 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.ozone.TestStorageContainerManager.testScmProcessDatanodeHeartbeat(TestStorageContainerManager.java:531)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-07-24 20:04:31,19
13246729,Fix failing blockade test-cases,This Jira is to fix and make sure that all the test-cases in blockade are working.,pull-request-available,['test'],HDDS,Bug,Major,2019-07-24 05:12:29,19
13246723,Fix typo in TestOmAcls,"In test class TestOmAcls.java, correct the typo {code}OzoneAccessAuthrizerTest{code}

{code:java}
class OzoneAccessAuthrizerTest implements IAccessAuthorizer {

  @Override
  public boolean checkAccess(IOzoneObj ozoneObject, RequestContext context)
      throws OMException {
    return false;
  }
{code}

Change {code}OzoneAccessAuthrizerTest{code} to {code}OzoneAccessAuthorizerTest{code}",newbie pull-request-available,['test'],HDDS,Test,Trivial,2019-07-24 04:39:45,1
13246721,ReplicationManager should consider inflight replication and deletion while picking datanode for re-replication,When choosing the target datanode for re-replication {{ReplicationManager}} should consider the datanodes which are in inflight replication and deletion for the same container.,blockade pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-24 04:35:41,19
13246691,Implement S3 Complete MPU request to use Cache and DoubleBuffer,"Implement S3 Complete MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-23 22:51:29,13
13246688,Fix TestOzoneManagerHA and TestOzoneManagerSnapShotProvider,"All tests in TestOzoneManagerHA are failing with below exception.

 

Broken by HDDS-1649. Not sure why this test is not running in CI. 

From PR HDDS-1845 run, not seeing this test run. 

[https://ci.anzix.net/job/ozone/17452/testReport/org.apache.hadoop.ozone.om/]

 
{code:java}
java.lang.Exception: test timed out after 300000 milliseconds
at java.lang.Object.wait(Native Method)
 at java.lang.Object.wait(Object.java:502)
 at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)
 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
 at org.apache.hadoop.ipc.Client.call(Client.java:1457)
 at org.apache.hadoop.ipc.Client.call(Client.java:1367)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:326)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1155)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:234)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:156)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
 at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:126)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
 
{code}
 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-23 22:26:37,13
13246675,Datanode Kerberos principal and keytab config key looks inconsistent,"Ozone Kerberos configuration can be very confusing:

| config name | Description |
| hdds.scm.kerberos.principal | SCM service principal |
| hdds.scm.kerberos.keytab.file | SCM service keytab file |
| ozone.om.kerberos.principal | Ozone Manager service principal |
| ozone.om.kerberos.keytab.file | Ozone Manager keytab file |
| hdds.scm.http.kerberos.principal | SCM service spnego principal |
| hdds.scm.http.kerberos.keytab.file | SCM service spnego keytab file |
| ozone.om.http.kerberos.principal | Ozone Manager spnego principal |
| ozone.om.http.kerberos.keytab.file | Ozone Manager spnego keytab file |
| hdds.datanode.http.kerberos.keytab | Datanode spnego keytab file |
| hdds.datanode.http.kerberos.principal | Datanode spnego principal |
| dfs.datanode.kerberos.principal | Datanode service principal |
| dfs.datanode.keytab.file | Datanode service keytab file |

The prefix are very different for each of the datanode configuration.  It would be nice to have some consistency for datanode.",newbie pull-request-available,[],HDDS,Bug,Major,2019-07-23 21:27:43,99
13246440,Default value for checksum bytes is different in ozone-site.xml and code,"<property>
 <name>ozone.client.checksum.type</name>
 <value>CRC32</value>
 <tag>OZONE, CLIENT, MANAGEMENT</tag>
 <description>The checksum type [NONE/ CRC32/ CRC32C/ SHA256/ MD5] determines
 which algorithm would be used to compute checksum for chunk data.
 Default checksum type is SHA256.
 </description>
</property>

OzoneConfigKeys.java

public static final String OZONE_CLIENT_CHECKSUM_TYPE_DEFAULT = ""SHA256"";

 

HDDS-1149 changes in ozone-default.xml, but not changed in java code. This Jira is to fix this issue.

 

 ",pull-request-available,[],HDDS,Bug,Major,2019-07-23 00:28:30,13
13246395,Fix OMVolumeSetQuota|OwnerRequest#validateAndUpdateCache return response.,"OMVolumeSetQuotaRequest#validateAndUpdateCache Line 115, we should return 

OMVolumeSetQuotaResponse in the failure case.

 

{code}

return new OMVolumeCreateResponse(null, null,
 createErrorOMResponse(omResponse, ex));

{code}",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Minor,2019-07-22 19:22:13,13
13246353,Tune the stateMachineDataCache to a reasonable fraction of Datanode Heap,"Currently, the stateMachineData which is the actual chunk data is maintained in the stateMachineCache inside ContainerStateMachine. Right now, the cache expiry is time based as well sized as per the no of parallel write chunks possible in the datanode. In case of optimal throughput, we may need to tune to it to a fraction of heap configured for the datanode process.",Triaged,['Ozone Datanode'],HDDS,Bug,Major,2019-07-22 15:27:30,16
13246352,Undetectable corruption after restart of a datanode,"Right now, all write chunks use BufferedIO ie, sync flag is disabled by default. Also, Rocks Db metadata updates are done in Rocks DB cache first at Datanode. In case, there comes a situation where the buffered chunk data as well as the corresponding metadata update is lost as a part of datanode restart, it may lead to a situation where, it will not be possible to detect the corruption (not even with container scanner) of this nature in a reasonable time frame, until and unless there is a client IO failure or Recon server detects it over time. In order to atleast to detect the problem, Ratis snapshot on datanode should sync the rocks db file . In such a way, ContainerScanner will be able to detect this.We can also add a metric around sync to measure how much of a throughput loss it can incurr.

Thanks [~msingh] for suggesting this.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2019-07-22 15:20:28,16
13246100,Implement S3 Abort MPU request to use Cache and DoubleBuffer,"Implement S3 Abort MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-20 00:54:12,13
13246095,Fix TestSecureContainerServer,"java.io.IOException: org.apache.hadoop.metrics2.MetricsException: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!

at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
 at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:106)
 at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:169)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:138)
 at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:94)
 at org.apache.ratis.client.impl.RaftClientImpl.sendRequest(RaftClientImpl.java:279)
 at org.apache.ratis.client.impl.RaftClientImpl.groupAdd(RaftClientImpl.java:206)
 at org.apache.hadoop.ozone.RatisTestHelper.initXceiverServerRatis(RatisTestHelper.java:135)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.lambda$runTestClientServerRatis$4(TestSecureContainerServer.java:159)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServer(TestSecureContainerServer.java:184)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServerRatis(TestSecureContainerServer.java:155)
 at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.testClientServerRatisGrpc(TestSecureContainerServer.java:131)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
 at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
 at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
 at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at org.apache.ratis.util.ReflectionUtils.instantiateException(ReflectionUtils.java:222)
 at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:104)
 ... 34 more
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
 at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:274)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$3(GrpcClientProtocolClient.java:140)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:167)
 ... 32 more
 Suppressed: java.lang.IllegalStateException: Failed to cast the object to class java.io.IOException
 at org.apache.ratis.util.IOUtils.readObject(IOUtils.java:204)
 at org.apache.ratis.util.IOUtils.bytes2Object(IOUtils.java:195)
 at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:93)
 at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:169)
 ... 32 more
 Caused by: java.lang.ClassCastException: Cannot cast org.apache.hadoop.metrics2.MetricsException to java.io.IOException
 at java.lang.Class.cast(Class.java:3369)
 at org.apache.ratis.util.IOUtils.readObject(IOUtils.java:200)
 ... 36 more",pull-request-available,[],HDDS,Bug,Major,2019-07-19 23:03:14,13
13246093,Fix TestSecureOzoneContainer,"org.apache.hadoop.metrics2.MetricsException: Metrics source StorageContainerMetrics already exists!

at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
 at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
 at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
 at org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics.create(ContainerMetrics.java:92)
 at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.<init>(OzoneContainer.java:93)
 at org.apache.hadoop.ozone.container.ozoneimpl.TestSecureOzoneContainer.testCreateOzoneContainer(TestSecureOzoneContainer.java:146)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

 ",pull-request-available,[],HDDS,Bug,Major,2019-07-19 22:55:29,13
13246092,Change topology sorting related logs in Pipeline from INFO to DEBUG,"This will avoid output like 

{code}

2019-07-19 22:36:40 INFO  Pipeline:342 - Serialize nodesInOrder [610d4084-7cce-4691-b43a-f9dd5cdb8809\{ip: 192.168.144.3, host: ozonesecure-mr_datanode_1.ozonesecure-mr_default, networkLocation: /default-rack, certSerialId: null}] in pipeline PipelineID=f9ba269c-aba9-4a42-946c-4048d02cb7d1

2019-07-19 22:36:40 INFO  Pipeline:342 - Deserialize nodesInOrder [610d4084-7cce-4691-b43a-f9dd5cdb8809\{ip: 192.168.144.3, host: ozonesecure-mr_datanode_1.ozonesecure-mr_default, networkLocation: /default-rack, certSerialId: null}] in pipeline PipelineID=f9ba269c-aba9-4a42-946c-4048d02cb7d1

{code}",pull-request-available,[],HDDS,Sub-task,Minor,2019-07-19 22:54:52,107
13245956,Change the default value of ratis leader election min timeout to a lower value,"The default value of min leader election timeout currently is 5s(done with HDDS-1718) by default which is leading to leader election taking much longer time to timeout in case of network failures and leading to delayed creation of pipelines in the system. The idea is to change the default value to a lower value of ""2s"" for now.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-19 09:39:42,16
13245950,parent directories not found in secure setup due to ACL check,"ozonesecure-ozonefs acceptance test is failing, because {{ozone fs -mkdir -p}} only creates key for the specific directory, not its parents.

{noformat}
ozone fs -mkdir -p o3fs://bucket1.fstest/testdir/deep
{noformat}

Previous result:

{noformat:title=https://ci.anzix.net/job/ozone-nightly/176/artifact/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/result/log.html#s1-s16-t3-k2}
$ ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.[].keyName'
testdir/
testdir/deep/
{noformat}

Current result:

{noformat:title=https://ci.anzix.net/job/ozone-nightly/177/artifact/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/result/log.html#s1-s16-t3-k2}
$ ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.[].keyName'
testdir/deep/
{noformat}

The failure happens on first operation that tries to use {{testdir/}} directly:

{noformat}
$ ozone fs -touch o3fs://bucket1.fstest/testdir/TOUCHFILE.txt
ls: `o3fs://bucket1.fstest/testdir': No such file or directory
{noformat}",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Blocker,2019-07-19 08:58:05,1
13245919,RefCountedDB printing of stacktrace should be moved to trace logging,"RefCountedDB logs the stackTrace for both increment and decrement, this pollutes the logs.",newbie,['Ozone Datanode'],HDDS,Bug,Major,2019-07-19 04:38:55,86
13245918,Improve logging for PipelineActions handling in SCM and datanode,"XceiverServerRatis should log the reason while sending the PipelineAction to the datanode.
Also on the PipelineActionHandler should also log the detailed reason for the action.",newbie pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Major,2019-07-19 04:35:47,30
13245907,OzoneManagerDoubleBuffer#stop should wait for daemon thread to die,"Based on [~arp]'s comment on HDDS-1649, OzoneManagerDoubleBuffer#stop() calls interrupt() on daemon thread but not join(). The thread might still be running when the call returns. ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-19 02:24:34,12
13245905,On OM reload/restart OmMetrics#numKeys should be updated,"When OM is restarted or the state is reloaded, OM Metrics is re-initialized. The saved numKeys value might not be valid as the DB state could have changed. Hence, the numKeys metric must be updated with the correct value on metrics re-initialization.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-19 02:17:33,12
13245886,Load Snapshot info when OM Ratis server starts,"When Ratis server is starting it looks for the latest snapshot to load it. Even though OM does not save snapshots via Ratis, we need to load the saved snaphsot index into Ratis so that the LogAppender knows to not look for logs before the snapshot index. Otherwise, Ratis will replay the logs from beginning every time it starts up.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-18 22:25:59,43
13245698,RatisPipelineProvider#initializePipeline logging needs to be verbose on failures/errors,RatisPipelineProvider#initializePipeline does not logs the information about pipeline details and the failed nodes when initializePipeline fails. The debugging needs to be verbose to help in debugging.,Triaged newbie,['SCM'],HDDS,Bug,Major,2019-07-18 07:29:39,37
13245697,NPE in SCMCommonPolicy.chooseDatanodes,"Exception is in SCMCommonPolicy.chooseDatanodes
{code}
java.lang.NullPointerException
	at java.util.Objects.requireNonNull(Objects.java:203)
	at java.util.ArrayList.removeAll(ArrayList.java:693)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMCommonPolicy.chooseDatanodes(SCMCommonPolicy.java:112)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodes(SCMContainerPlacementRandom.java:74)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.TestContainerPlacementFactory.testDefaultPolicy(TestContainerPlacementFactory.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

cc : [~xyao] [~Sammi]",pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-18 07:27:20,1
13245696,BlockOutputStream#watchForCommit fails with UnsupportedOperationException when one DN is down,"When one of the datanode from the ratis pipeline is excluded by introducing network failure, the client write is failing with the following exception
{noformat}
2019-07-18 07:13:33 WARN  XceiverClientRatis:262 - 3 way commit failed on pipeline Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
E         java.util.concurrent.ExecutionException: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 2 and log index 9 is not yet replicated to ALL_COMMITTED
E         	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
E         	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)
E         	at org.apache.hadoop.hdds.scm.XceiverClientRatis.watchForCommit(XceiverClientRatis.java:259)
E         	at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:194)
E         	at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchOnLastIndex(CommitWatcher.java:157)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:348)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)
E         	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)
E         	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)
E         	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
E         	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
E         	at java.base/java.lang.Thread.run(Thread.java:834)
E         Caused by: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 2 and log index 9 is not yet replicated to ALL_COMMITTED
E         	at org.apache.ratis.client.impl.ClientProtoUtils.toRaftClientReply(ClientProtoUtils.java:245)
E         	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:254)
E         	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:249)
E         	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)
E         	at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
E         	at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
E         	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)
E         	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
E         	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
E         	... 3 more
E         2019-07-18 07:13:33 INFO  XceiverClientRatis:280 - Could not commit index 9 on pipeline Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] to all the nodes. Server fa65a457-155d-4bf3-8d1b-b0e11ec157ae has failed. Committed by majority.
E         2019-07-18 07:13:33 WARN  BlockOutputStream:354 - Failed to commit BlockId conID: 1 locID: 102461208643108865 bcsId: 9 on Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]. Failed nodes: [fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: null, host: null, networkLocation: /default-rack, certSerialId: null}]
E         2019-07-18 07:13:33 ERROR RandomKeyGenerator:730 - Exception while adding key: key-0-06904 in bucket: bucket-0-14179 of volume: vol-0-21379.
E         java.lang.UnsupportedOperationException
E         	at java.base/java.util.AbstractList.add(AbstractList.java:153)
E         	at java.base/java.util.AbstractList.add(AbstractList.java:111)
E         	at java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:356)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)
E         	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)
E         	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)
E         	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
E         	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
E         	at java.base/java.lang.Thread.run(Thread.java:834)

E         java.lang.UnsupportedOperationException
E         	at java.base/java.util.AbstractList.add(AbstractList.java:153)
E         	at java.base/java.util.AbstractList.add(AbstractList.java:111)
E         	at java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:356)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)
E         	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)
E         	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)
E         	at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)
E         	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)
E         	at org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)
E         	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
E         	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
E         	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
E         	at java.base/java.lang.Thread.run(Thread.java:834)
E
{noformat}",blockade,['Ozone Client'],HDDS,Bug,Major,2019-07-18 07:24:39,16
13245634,Fix numKeys metrics in OM HA,"When we commit the key, we should increment numKeys in Ozone. This metrics shows the current count of keys in ozone. This is missed in OMKeyCommitRequest logic.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-17 22:56:31,13
13245610,Implement S3 Commit MPU request to use Cache and DoubleBuffer,"Implement S3 Commit MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-17 19:21:44,13
13245547,GetKey fails with IllegalArgumentException,"During get key call the client is intermittently failing with {{java.lang.IllegalArgumentException}}
{noformat}
E       AssertionError: Ozone get Key failed with output=[java.lang.IllegalArgumentException
E       	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)
E       	at org.apache.hadoop.hdds.scm.XceiverClientManager.acquireClient(XceiverClientManager.java:150)
E       	at org.apache.hadoop.hdds.scm.XceiverClientManager.acquireClientForReadData(XceiverClientManager.java:143)
E       	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:154)
E       	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:118)
E       	at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:222)
E       	at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:171)
E       	at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
E       	at java.base/java.io.InputStream.read(InputStream.java:205)
E       	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)
E       	at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
E       	at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
E       	at picocli.CommandLine.execute(CommandLine.java:1173)
E       	at picocli.CommandLine.access$800(CommandLine.java:141)
E       	at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
E       	at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
E       	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
E       	at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
E       	at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
E       	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
E       	at org.apache.hadoop.ozone.web.ozShell.OzoneShell.execute(OzoneShell.java:60)
E       	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
E       	at org.apache.hadoop.ozone.web.ozShell.OzoneShell.main(OzoneShell.java:53)]
{noformat}

This is happening when the pipeline returned by SCM doesn't have any datanode information.",blockade pull-request-available,"['Ozone Client', 'SCM']",HDDS,Bug,Major,2019-07-17 15:23:51,19
13245446,ContainerStateMachine should limit number of pending apply transactions,ContainerStateMachine should limit number of pending apply transactions in order to avoid excessive heap usage by the pending transactions.,pull-request-available,[],HDDS,Bug,Major,2019-07-17 11:06:53,35
13245331,Add SVG Logo for Ozone,"First time ever using Inkscape, so apologies in advance if I did it completely wrong...

Anyway, I created a single color SVG by tracing the bitmap from the PNG logo. I'll attach the SVG to this issue. It's supposed to match the design of Material UI icons, hence the single color. If there's a desire for the green bolt color, let me know, and I'll try to also make a colorized version.

Also, I'm no designer, but feedback is welcome :)",Triaged,[],HDDS,New Feature,Trivial,2019-07-16 21:58:25,108
13245308,Fix false warning from ozones3 acceptance test,"All acceptance passed but the results are marked failed due to the following warnings.

[https://ci.anzix.net/job/ozone/17381/RobotTests/log.html]

{code}

[ WARN ] Collapsing consecutive whitespace during parsing is deprecated. Fix ' # Bucket already is created in Test Setup.' in file '/opt/hadoop/smoketest/s3/bucketcreate.robot' on line 31.

{code}",pull-request-available,[],HDDS,Test,Minor,2019-07-16 19:10:43,28
13245281,Du while calculating used disk space reports that chunk files are file not found,"

{code}
2019-07-16 08:16:49,787 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Could not get disk usage information for path /data/3/ozone-0715
ExitCodeException exitCode=1: du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/b113dd390e68e914d3ff405f3deec564_stream_60448f
77-6349-48fa-ae86-b2d311730569_chunk_1.tmp.1.14118085': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/37993af2849bdd0320d0f9d4a6ef4b92_stream_1f68be9f-e083-45e5-84a9-08809bc392ed
_chunk_1.tmp.1.14118091': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a38677def61389ec0be9105b1b4fddff_stream_9c3c3741-f710-4482-8423-7ac6695be96b
_chunk_1.tmp.1.14118102': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a689c89f71a75547471baf6182f3be01_stream_baf0f21d-2fb0-4cd8-84b0-eff1723019a0
_chunk_1.tmp.1.14118105': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/f58cf0fa5cb9360058ae25e8bc983e84_stream_d8d5ea61-995f-4ff5-88fb-4a9e97932f00
_chunk_1.tmp.1.14118109': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a1d13ee6bbefd1f8156b1bd8db0d1b67_stream_db214bdd-a0c0-4f4a-8bc7-a3817e047e45_chunk_1.tmp.1.14118115': No such file or directory
du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/8f8a4bd3f6c31161a70f82cb5ab8ee60_stream_d532d657-3d87-4332-baf8-effad9b3db23_chunk_1.tmp.1.14118127': No such file or directory

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)
        at org.apache.hadoop.util.Shell.run(Shell.java:901)
        at org.apache.hadoop.fs.DU$DUShell.startRefresh(DU.java:62)
        at org.apache.hadoop.fs.DU.refresh(DU.java:53)
        at org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:181)
        at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['Ozone Datanode'],HDDS,Bug,Critical,2019-07-16 16:59:02,1
13245264,Prometheus metrics are broken for datanodes due to an invalid metric,"Datanodes can't be monitored with prometheus any more:

{code}
level=warn ts=2019-07-16T16:29:55.876Z caller=scrape.go:937 component=""scrape manager"" scrape_pool=pods target=http://192.168.69.76:9882/prom msg=""append failed"" err=""invalid metric type \""apache.hadoop.ozone.container.common.transport.server.ratis._csm_metrics_delete_container_avg_time gauge\""""
{code}


",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-07-16 16:32:58,1
13245227,SCM command to Activate and Deactivate pipelines,It will be useful to have scm command to temporarily deactivate and re-activate a pipeline. This will help us a lot in debugging a pipeline.,pull-request-available,"['SCM', 'SCM Client']",HDDS,New Feature,Major,2019-07-16 14:24:18,19
13245115,TestRatisPipelineCreateAndDestory#testPipelineCreationOnNodeRestart times out,"{code:java}
Error Message
test timed out after 30000 milliseconds
Stacktrace
java.lang.Exception: test timed out after 30000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:382)
	at org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineCreateAndDestory.waitForPipelines(TestRatisPipelineCreateAndDestory.java:126)
	at org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineCreateAndDestory.testPipelineCreationOnNodeRestart(TestRatisPipelineCreateAndDestory.java:121)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-16 05:53:16,1
13245114,TestWatchForCommit#testWatchForCommitForRetryfailure fails as a result of no leader election for extended period of time ,"{code:java}
org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$407/213850519@1a8843a2 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=1000ms)
Stacktrace
java.util.concurrent.ExecutionException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$407/213850519@1a8843a2 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=1000ms)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForRetryfailure(TestWatchForCommit.java:345)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
The client here retries times with a delay of 1 sec between each retry but leader eleactiocouldnot complete.
{code:java}
2019-07-12 19:30:46,451 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46: receive RaftClientReply:client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 5931fd83-b899-480e-b15a-ecb8e7f7dd46 is not the leader (null). Request must be sent to leader., logIndex=0, commits[5931fd83-b899-480e-b15a-ecb8e7f7dd46:c-1]
2019-07-12 19:30:47,469 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->d83929f1-c4db-499d-b67f-ad7f10dd7dde: receive RaftClientReply:client-6C83DC527A4C->d83929f1-c4db-499d-b67f-ad7f10dd7dde@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server d83929f1-c4db-499d-b67f-ad7f10dd7dde is not the leader (null). Request must be sent to leader., logIndex=0, commits[d83929f1-c4db-499d-b67f-ad7f10dd7dde:c-1]
2019-07-12 19:30:48,504 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46: receive RaftClientReply:client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 5931fd83-b899-480e-b15a-ecb8e7f7dd46 is not the leader (null). Request must be sent to leader., logIndex=0, commits[5931fd83-b899-480e-b15a-ecb8e7f7dd46:c-1]
2019-07-12 19:30:49,540 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509: receive RaftClientReply:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 73bdd98d-b003-44ff-a45b-bd12dfd50509 is not the leader (null). Request must be sent to leader., 
{code}",Triaged,['Ozone Client'],HDDS,Bug,Major,2019-07-16 05:47:37,16
13245099,Implement S3 Initiate MPU request to use Cache and DoubleBuffer,"Implement S3 Initiate MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-16 05:03:20,13
13245087,shellcheck.sh does not work on Mac,"# {{shellcheck.sh}} does not work on Mac
{code}
find: -executable: unknown primary or operator
{code}
# {{$OUTPUT_FILE}} only contains problems from {{hadoop-ozone}}, not from {{hadoop-hdds}}",pull-request-available,[],HDDS,Bug,Minor,2019-07-16 03:05:06,1
13245077,Add Eviction policy for table cache,"In this Jira we will add eviction policy for table cache.

In this Jira, we will add 2 eviction policies for the cache.

NEVER, // Cache will not be cleaned up. This mean's the table maintains full cache.
AFTERFLUSH // Cache will be cleaned up, once after flushing to DB.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-16 00:19:31,13
13245021,Make Topology Aware Replication/Read non-default for ozone 0.4.1   ,"This helps stablize the ozone-0.4.1 release and fix HDDS-1705, HDDS-1751, HDDS-1713 and HDDS-1770 for 0.5. ",pull-request-available,[],HDDS,Sub-task,Major,2019-07-15 17:18:16,28
13244990,Result of author check is inverted,"h2. What changes were proposed in this pull request?
h2. 
Fix:

 1. author check fails when no violations are found
 2. author check violations are duplicated in the output

Eg. https://ci.anzix.net/job/ozone-nightly/173/consoleText says that:


{code:java}
The following tests are FAILED:

[author]: author check is failed (https://ci.anzix.net/job/ozone-nightly/173//artifact/build/author.out/*view*/){code}


but no actual `@author` tags were found:

{code}
$ curl -s 'https://ci.anzix.net/job/ozone-nightly/173//artifact/build/author.out/*view*/' | wc
       0       0       0
{code}

h2. How was this patch tested?

{code}
$ bash -o pipefail -c 'hadoop-ozone/dev-support/checks/author.sh | tee build/author.out'; echo $?
0

$ wc build/author.out
       0       0       0 build/author.out

$ echo '// @author Tolkien' >> hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java

$ bash -o pipefail -c 'hadoop-ozone/dev-support/checks/author.sh | tee build/author.out'; echo $?
./hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java:// @author Tolkien
1

$ wc build/author.out
       1       3     108 build/author.out
{code}",pull-request-available,[],HDDS,Bug,Major,2019-07-15 14:14:28,1
13244955,Add goofyfs to the ozone-runner docker image,"Goofys is a s3 fuse driver which is required for the ozone csi setup.

As of now it's installed in hadoop-ozone/dist/src/main/docker/Dockerfile from a non-standard location (because it couldn't be part of hadoop-runner earlier as it's ozone specific).

It should be installed to the ozone-runner from a canonical goffys release URL.",pull-request-available,[],HDDS,Bug,Blocker,2019-07-15 12:37:48,6
13244915,Propagate failure in writeStateMachineData to Ratis,"Currently, 

writeStateMachineData() returns a future to Ratis. This future does not track any errors or failures encountered as part of the operation - WriteChunk / handleWriteChunk(). The error is propagated back to the client in the form of an error code embedded inside writeChunkResponseProto. But the error goes undetected and unhandled in the Ratis server. The future handed back to Ratis is always completed with success.

The goal is to detect any errors in writeStateMachineData in Ratis and treat is as a failure of the Ratis log. Handling for which is already implemented in HDDS-1603. 

 ",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-07-15 09:46:47,88
13244719,Implement S3 Delete Bucket request to use Cache and DoubleBuffer,"Implement S3 Bucket write requests to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-12 22:48:03,13
13244595,Acceptance test of ozone-topology cluster is failing,"Since HDDS-1586 the smoketests of the ozone-topology compose file is broken:
{code:java}
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml
must specify at least one container source
Stopping datanode_2 ... 
Stopping datanode_3 ... 
Stopping datanode_4 ... 
Stopping scm        ... 
Stopping om         ... 
Stopping datanode_1 ... 
[6A[2K
Stopping datanode_2 ... [32mdone[0m
[6B[4A[2K
Stopping datanode_4 ... [32mdone[0m
[4B[1A[2K
Stopping datanode_1 ... [32mdone[0m
[1B[5A[2K
Stopping datanode_3 ... [32mdone[0m
[5B[3A[2K
Stopping scm        ... [32mdone[0m
[3B[2A[2K
Stopping om         ... [32mdone[0m
[2BRemoving datanode_2 ... 
Removing datanode_3 ... 
Removing datanode_4 ... 
Removing scm        ... 
Removing om         ... 
Removing datanode_1 ... 
[1A[2K
Removing datanode_1 ... [32mdone[0m
[1B[2A[2K
Removing om         ... [32mdone[0m
[2B[5A[2K
Removing datanode_3 ... [32mdone[0m
[5B[4A[2K
Removing datanode_4 ... [32mdone[0m
[4B[6A[2K
Removing datanode_2 ... [32mdone[0m
[6B[3A[2K
Removing scm        ... [32mdone[0m
[3BRemoving network ozone-topology_net
[ ERROR ] Reading XML source '/var/jenkins_home/workspace/ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-topology/result/robot-*.xml' failed: No such file or directory

Try --help for usage information.
ERROR: Test execution of /var/jenkins_home/workspace/ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-topology is FAILED!!!!{code}",pull-request-available,[],HDDS,Bug,Blocker,2019-07-12 12:00:46,1
13244569,Update network-tests/src/test/blockade/README.md file,{{hadoop-ozone/fault-injection-test/network-tests/src/test/blockade/README.md}} has to be updated after HDDS-1778.,pull-request-available,['test'],HDDS,Improvement,Major,2019-07-12 09:57:50,19
13244555,Fix checkstyle issues in TestDataScrubber,"There are 4 Checkstyle issues in TestDataScrubber that has to be fixed
{noformat}
[ERROR] src/test/java/org/apache/hadoop/ozone/dn/scrubber/TestDataScrubber.java:[157] (sizes) LineLength: Line is longer than 80 characters (found 81).
[ERROR] src/test/java/org/apache/hadoop/ozone/dn/scrubber/TestDataScrubber.java:[161] (sizes) LineLength: Line is longer than 80 characters (found 82).
[ERROR] src/test/java/org/apache/hadoop/ozone/dn/scrubber/TestDataScrubber.java:[167] (sizes) LineLength: Line is longer than 80 characters (found 85).
[ERROR] src/test/java/org/apache/hadoop/ozone/dn/scrubber/TestDataScrubber.java:[187] (sizes) LineLength: Line is longer than 80 characters (found 104).
{noformat}",pull-request-available,['test'],HDDS,Bug,Minor,2019-07-12 09:09:01,19
13244480,Fix kerberos principal error in Ozone Recon,"Recon fails to startup in a kerberized cluster with the following error:


{code:java}
Failed startup of context o.e.j.w.WebAppContext@2009f9b0{/,file:///tmp/jetty-0.0.0.0-9888-recon-_-any-2565178148822292652.dir/webapp/,UNAVAILABLE}{/recon} javax.servlet.ServletException: javax.servlet.ServletException: Principal not defined in configuration at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180) at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139) at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873) at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349) at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406) at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368) at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778) at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.server.Server.start(Server.java:427) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.server.Server.doStart(Server.java:394) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1140) at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:175) at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:102) at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:61)
{code}",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-07-12 00:26:43,37
13244446,NPE thrown while trying to find DN closest to client,"cc: [~xyao] This seems related to the client side topology changes, not sure if some other Jira is already addressing this.

{code}
2019-07-10 16:45:53,176 WARN  ipc.Server (Server.java:logException(2724)) - IPC Server handler 14 on 35066, call Call#127037 Retry#0 org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol.send from 17
2.31.116.73:52540
java.lang.NullPointerException
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.lambda$sortDatanodes$0(ScmBlockLocationProtocolServerSideTranslatorPB.java:215)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.sortDatanodes(ScmBlockLocationProtocolServerSideTranslatorPB.java:215)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:124)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
2019-07-10 16:45:53,176 WARN  om.KeyManagerImpl (KeyManagerImpl.java:lambda$sortDatanodeInPipeline$7(2129)) - Unable to sort datanodes based on distance to client, volume=xqoyzocpse, bucket=vxwajaczqh, key=pool-444-thread-7-201077822, client=127.0.0.1, datanodes=[10f15723-45d7-4a0c-8f01-8b101744a110{ip: 172.31.116.73, host: sid-minichaos.gce.cloudera.com, networkLocation: /default-rack, certSerialId: null}, 7ac2777f-0a5c-4414-9e7f-bfbc47d696ea{ip: 172.31.116.73, host: sid-minichaos.gce.cloudera.com, networkLocation: /default-rack, certSerialId: null}], exception=java.lang.NullPointerException
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.lambda$sortDatanodes$0(ScmBlockLocationProtocolServerSideTranslatorPB.java:215)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.sortDatanodes(ScmBlockLocationProtocolServerSideTranslatorPB.java:215)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:124)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}",pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-07-11 20:14:54,5
13244440,Datanodes takeSnapshot should delete previously created snapshots,"Right now, after after taking a new snapshot, the previous snapshot file is left in the raft log directory. When a new snapshot is taken, the previous snapshots should be deleted.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-11 19:17:29,30
13244369,OOM error in Freon due to the concurrency handling,"HDDS-1532 modified the concurrent framework usage of Freon (RandomKeyGenerator).

The new approach uses separated tasks (Runnable) to create the volumes/buckets/keys.

Unfortunately it doesn't work very well in some cases.
 # When Freon starts it creates an executor with fixed number of threads (10)
 # The first loop submits numOfVolumes (10) VolumeProcessor tasks to the executor
 # The 10 threads starts to execute the 10 VolumeProcessor tasks
 # Each VolumeProcessor tasks creates numOfBuckets (1000) BucketProcessor tasks. All together 10000 tasks are submitted to the executor.
 # The 10 threads starts to execute the first 10 BucketProcessor tasks, they starts to create the KeyProcessor tasks: 500 000 * 10 tasks are submitted.
 # At this point of the time no keys are generated, but the next 10 BucketProcessor tasks are started to execute..
 # To execute the first key creation we should process all the BucketProcessor tasks which means that all the Key creation tasks (10 * 1000 * 500 000) are created and added to the executor
 # Which requires a huge amount of time and memory",pull-request-available,[],HDDS,Bug,Blocker,2019-07-11 14:30:54,1
13244321,Missing HostName and IpAddress in the response of register command,"{{SCMNodeManager}} sets the HostName and IpAddress to the response of register command, but that is being ignored in {{SCMDatanodeProtocolServer}} while sending the response back to the datanode.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-11 13:18:02,19
13244213,Latency metric for applyTransaction in ContainerStateMachine,"applyTransaction is invoked from the Ratis pipeline and the ContainerStateMachine

uses a async executor to complete the task.

 

We require a latency metric to track the performance of log apply operations in the state machine. This will measure the end-to-end latency of apply which includes the queueing delay in the executor queues. Combined with the latency measurement in HddsDispatcher, this will be an indicator if the executors are overloaded.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-07-11 03:56:47,30
13244210,Add an option to MiniOzoneChaosCluster to read files multiple times.,Right now MiniOzoneChaosCluster writes a file/ reads it and deletes it immediately. This jira proposes to add an option to read the file multiple time in MiniOzoneChaosCluster.,pull-request-available,['test'],HDDS,Bug,Major,2019-07-11 03:08:30,18
13244108,TestFailureHandlingByClient tests are flaky,"The tests seem to fail bcoz , when the datanode goes down with stale node interval being set to a low value, containers may get closed early and client writes might fail with closed container exception rather than pipeline failure/Timeout exceptions as excepted in the tests. The fix made here is to tune the stale node interval.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-07-10 13:09:25,16
13244070,TestWatchForCommit tests are flaky,"The tests have become flaky bcoz once  nodes are shutdown inn Ratis pipeline, a watch request can either be received at server at a server and fail with NotReplicatedException or sometimes it fails with StatusRuntimeExceptions from grpc which both need to be accounted for in the tests. Other than that, HDDS-1384 also causes bind exception to e thrown intermittently which in turn shuts down the miniOzoneCluster. To overcome this, the test class has been refactored as well.",pull-request-available,[],HDDS,Bug,Major,2019-07-10 10:08:05,16
13244055,Fix existing blockade tests,This jira is to track and fix existing blockade test cases.,pull-request-available,['test'],HDDS,Bug,Major,2019-07-10 09:27:11,19
13243962,Fix image name in some ozone docker-compose files,"The docker compose file has invalid reference to scm images, which fails the docker-compose up with errors like below. This ticket is opened to fix them.

 
{code:java}
ERROR: no such image: apache/ozone-runner::20190617-2: invalid reference format}

or 

ERROR: no such image: apache/ozone-runner:latest:20190617-2: invalid reference format{code}",pull-request-available,[],HDDS,Bug,Major,2019-07-09 20:45:30,28
13243793,Make OM KeyDeletingService compatible with HA model,"Currently OM KeyDeletingService directly deletes all the keys in DeletedTable after deleting the corresponding blocks through SCM. For HA compatibility, the key purging should happen through the OM Ratis server. This Jira introduces PurgeKeys request in OM protocol. This request will be submitted to OMs Ratis server after SCM deletes blocks corresponding to deleted keys.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-09 01:46:19,43
13243428,Audit xxxAcl methods in OzoneManager,Audit permission failures from authorizer,pull-request-available,[],HDDS,Sub-task,Major,2019-07-05 18:29:14,81
13243418,ContainerStateMachine should have its own executors for executing applyTransaction calls,Currently ContainerStateMachine uses the executors provided by XceiverServerRatis for executing applyTransaction calls. This would result in two or more ContainerStateMachine to share the same set of executors. Delay or load in one ContainerStateMachine would adversely affect the performance of other state machines in such a case. It is better to have separate set of executors for each ContainerStateMachine.,pull-request-available,[],HDDS,Bug,Major,2019-07-05 16:43:53,35
13243416,ContainerStateMachine is unable to increment lastAppliedTermIndex,ContainerStateMachine#updateLastApplied currently updates the lastAppliedTermIndex using applyTransactionCompletionMap. There are null entries in the applyTransactionCompletionMap causing the lastAppliedIndex to not be incremented.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-07-05 16:36:57,18
13243234,Fix hidden errors in acceptance tests,"[~bharatviswa] pinged me offline with the problem that in some cases the smoketest is failing even if the reports are green:
{code:java}
All smoke tests are passed, but CI is showing as Failed.

https://ci.anzix.net/job/ozone/17284/RobotTests/log.html
https://github.com/apache/hadoop/pull/1048{code}
The root cause is a few typo after HDDS-1698, which can be fixed with the uploaded PR.

*What is the problem?*

In case of any error during the test execution the smoketest is failed. In this case because the typo in two docker-compose.yaml files two of the tests can't be started.

But there is no separated robot test report and the error is visible only in the console.

*How did it happen?*

The ACL work improved some intermittency in the acceptance tests. HDDS-1698 is committed because the acceptance tests were failed with ACL errors which hide the real error (the test was red anyway).

 ",pull-request-available,[],HDDS,Bug,Blocker,2019-07-04 15:23:25,6
13243231,Use vendor neutral s3 logo in ozone doc,"In HDDS-1639 we restructured the ozone documentation and a new overview page is added to the main page.

This page contains an official aws logo, As [~bharatviswa] reported we are not sure about the exact condition to use logos / trademarks from Amazon. It's better to remain on the safe side and use a neutral S3 label.

In this patch the aws logo is replaced with an orange cloud + s3 text.",pull-request-available,['documentation'],HDDS,Bug,Major,2019-07-04 15:12:48,6
13243065,Fix class hierarchy for KeyRequest and FileRequest classes.,"The patch looks mostly fine to me. A few minor comments. -and one type error that needs to be fixed.-

I would like to see the class hierarchy refactored in a follow up patch. {{OMFileCreateRequest}}should not extend {{OMKeyCreateRequest}}. Instead they should both extend an abstract class that encapsulates the common functionality.

Generally deriving from _concrete_ classes is a bad idea.

 

This Jira is created based on [~arp] comment during review of HDDS-1731",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-03 18:18:58,13
13242895,Add replication and key deletion tests to MiniOzoneChaosCluster,This jira adds capability for deleting keys and also to test Replication Manager code in MiniOzoneChaosCluster.,pull-request-available,['test'],HDDS,Bug,Major,2019-07-03 04:10:13,18
13242825,Use ExecutorService in OzoneManagerStateMachine,"In the current code in applyTransaction we have 

CompletableFuture<Message> future = CompletableFuture
 .supplyAsync(() -> runCommand(request, trxLogIndex)); We are using ForkJoin#commonPool.

With the current approach we have 2 issues:
 # Thread exhausts when using this common pool.
 # Not a good practice of using common pool. Found some issues in our testing by using similarly in RatisPipelineUtils.
 # OM DB's across replica can be out of sync when the apply transactions are applied in out of order.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-02 18:41:37,13
13242791,DeleteContainerCommandHandler fails with NPE,"DeleteContainerCommandHandler fails with NPE

{code}
Thread[Command processor thread,5,org.apache.hadoop.ozone.TestMiniChaosOzoneCluster]
java.lang.NullPointerException
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.getHandler(ContainerController.java:138)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:128)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handle(DeleteContainerCommandHandler.java:57)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:93)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:432)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-02 16:13:04,19
13242789,getContainerWithPipeline should log the container ID in case of failure,getContainerWithPipeline should log the container ID in scm logs for easy debugging.,MiniOzoneChaosCluster,['SCM'],HDDS,Bug,Minor,2019-07-02 16:05:38,19
13242788,getContainerWithPipeline fails with PipelineNotFoundException,"Once a pipeline is closed or finalized and it was not able to close all the containers inside the pipeline. 

Then getContainerWithPipeline will try to fetch the pipeline state from pipelineManager after the pipeline has been closed.

{code}
2019-07-02 20:48:20,370 INFO  ipc.Server (Server.java:logException(2726)) - IPC Server handler 13 on 50130, call Call#17339 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.getContainerWithPipeline from 192.168.0.2:51452
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=e1a7b16a-48d9-4194-9774-ad49ec9ad78b not found
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:132)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:66)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:184)
        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getContainerWithPipeline(SCMClientProtocolServer.java:244)
        at org.apache.hadoop.ozone.protocolPB.StorageContainerLocationProtocolServerSideTranslatorPB.getContainerWithPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:144)
        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:16390)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-02 16:04:34,88
13242770,Datanode unable to find chunk while replication data using ratis.,"Leader datanode is unable to read chunk from the datanode while replicating data from leader to follower.
Please note that deletion of keys is also happening while the data is being replicated.

{code}
2019-07-02 19:39:22,604 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1
014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#70:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782
2019-07-02 19:39:22,605 ERROR impl.ChunkManagerImpl (ChunkUtils.java:readData(161)) - Unable to find the chunk file. chunk info : ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3
-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048}
2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(990)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: Failed appendEntries as latest snapshot (9770) already h
as the append entries (first index: 1)
2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1
014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#71:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782
2019-07-02 19:39:22,605 INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: ReadChunk : Trace ID: 4216d461a4679e17:4216d461a4679e17:0:0 : Message: Unable to find the c
hunk file. chunk info ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048} : Result: UNABLE_TO_FIND_CHUNK
2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(990)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: Failed appendEntries as latest snapshot (9770) already h
as the append entries (first index: 2)
2019-07-02 19:39:22,606 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1
014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#72:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782
19:39:22.606 [pool-195-thread-19] ERROR DNAudit - user=null | ip=null | op=READ_CHUNK {blockData=conID: 3 locID: 102372189549953034 bcsId: 0} | ret=FAILURE
java.lang.Exception: Unable to find the chunk file. chunk info ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048}
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:346) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:476) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$getCachedStateMachineData$2(ContainerStateMachine.java:495) ~[hadoop-hdds-container-service-0.5.0-SN
APSHOT.jar:?]
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4767) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3965) ~[guava-11.0.2.jar:?]
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4764) ~[guava-11.0.2.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.getCachedStateMachineData(ContainerStateMachine.java:494) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.ja
r:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$readStateMachineData$4(ContainerStateMachine.java:542) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-07-02 14:14:53,16
13242765,ConcurrentModificationException while handling DeadNodeHandler event,"ConcurrentModificationException while handling DeadNodeHandler event

{code}
2019-07-02 19:29:25,190 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(88)) - Error on execution message 56591ec5-c9e4-416c-9a36-db0507739fe5{ip: 192.168.0.2, host: 192.16
8.0.2, networkLocation: /default-rack, certSerialId: null}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442)
        at java.util.HashMap$KeyIterator.next(HashMap.java:1466)
        at java.lang.Iterable.forEach(Iterable.java:74)
        at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
        at org.apache.hadoop.hdds.scm.node.DeadNodeHandler.lambda$destroyPipelines$1(DeadNodeHandler.java:99)
        at java.util.Optional.ifPresent(Optional.java:159)
        at org.apache.hadoop.hdds.scm.node.DeadNodeHandler.destroyPipelines(DeadNodeHandler.java:98)
        at org.apache.hadoop.hdds.scm.node.DeadNodeHandler.onMessage(DeadNodeHandler.java:78)
        at org.apache.hadoop.hdds.scm.node.DeadNodeHandler.onMessage(DeadNodeHandler.java:44)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-07-02 14:04:35,106
13242747,replication of underReplicated container fails with SCMContainerPlacementRackAware policy,"SCM container replication fails with

{code}
2019-07-02 18:26:41,564 WARN  container.ReplicationManager (ReplicationManager.java:handleUnderReplicatedContainer(501)) - Exception while replicating container 18.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No enough datanodes to choose.
        at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware.chooseDatanodes(SCMContainerPlacementRackAware.java:100)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.handleUnderReplicatedContainer(ReplicationManager.java:487)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.processContainer(ReplicationManager.java:293)
        at java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649)
        at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
        at org.apache.hadoop.hdds.scm.container.ReplicationManager.run(ReplicationManager.java:205)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster,['SCM'],HDDS,Bug,Major,2019-07-02 13:05:08,5
13242702,Add block allocation metric for pipelines in SCM,This Jira aims to add block allocation metrics for pipelines in SCM. This would help in determining the distribution of block allocations among various pipelines in SCM.,pull-request-available,[],HDDS,Bug,Major,2019-07-02 09:41:57,35
13242682,Ozone Client should randomize the list of nodes in pipeline for reads,"Currently the list of nodes returned by SCM are static and are returned in the same order to all the clients. Ideally these should be sorted by the network topology and then returned to client.

However even when network topology in not available, then SCM/client should randomly sort the nodes before choosing the replica's to connect.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-07-02 08:35:27,30
13242679,Error message for 3 way commit failure is not verbose,"The error message for 3 way client commit is not verbose, it should include blockID and pipeline ID along with node details for debugging.

{code}
2019-07-02 09:58:12,025 WARN  scm.XceiverClientRatis (XceiverClientRatis.java:watchForCommit(262)) - 3 way commit failed 
java.util.concurrent.ExecutionException: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 39482 and log index 11562 is not yet replicated to ALL_COMMITTED
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
        at org.apache.hadoop.hdds.scm.XceiverClientRatis.watchForCommit(XceiverClientRatis.java:259)
        at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:194)
        at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchOnFirstIndex(CommitWatcher.java:135)
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:355)
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:332)
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:259)
        at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:211)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
        at java.io.OutputStream.write(OutputStream.java:75)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:103)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:147)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 39482 and log index 11562 is not yet replicated to ALL_COMMITTED
        at org.apache.ratis.client.impl.ClientProtoUtils.toRaftClientReply(ClientProtoUtils.java:245)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:254)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:249)
        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)
        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        ... 3 more
{code}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-07-02 08:31:46,88
13242601,Merge ozone-perf and ozonetrace example clusters,"We have multiple example clusters in hadoop-ozone/dist/src/main/compose to demonstrate how different type of configuration can be set with ozone.

But some of them can be consolidated. I propose to combine ozonetrace to ozoneperf to one ozoneperf which includes all the required components for a local performance testing:
 # opentracing (jaeger component in docker-compose + environment variables)
 # monitoring (grafana + prometheus)
 # perf profile (as of now it's enabled only in the ozone cluster[1])

 

[1]
{code:java}
cat compose/ozone/docker-config | grep prof

OZONE-SITE.XML_hdds.profiler.endpoint.enabled=true
ASYNC_PROFILER_HOME=/opt/profiler
{code}
 ",pull-request-available,['docker'],HDDS,Improvement,Minor,2019-07-01 20:32:44,63
13242599,Fix prometheus configuration in ozoneperf example cluster,"HDDS-1216 renamed the ozoneManager components to om in the docker-compose file. But the prometheus configuration of the compose/ozoneperf environment is not updated.

We need to updated it to get meaningful metrics from om.",pull-request-available,['docker'],HDDS,Improvement,Trivial,2019-07-01 20:24:11,63
13242407,Add nullable annotation for OMResponse classes,"This is to address [~arp] comment.
A future improvement unrelated to your patch - replace null with Optional. Or at least add [@nullable|https://github.com/nullable] annotation on the parameter.

Add @nullable for all OMResponse fields, where fields can be null.
 
  ",newbie pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-01 03:06:43,31
13242405,Add Volume check in KeyManager and File Operations,"This is to address a TODO to check volume checks when performing Key/File operations.

 

// TODO: Not checking volume exist here, once we have full cache we can
// add volume exist check also.

 ",newbie pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-07-01 03:03:54,31
13242320,Cleanup 2phase old HA code for Key requests.,"HDDS-1638 brought in HA code for Key operations like allocateBlock,createKey etc., 

Old code changes which are added as part of HDDS-1250 and HDDS-1262 for allocateBlock and openKey.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-29 22:01:30,13
13242256,Create separated unit and integration test executor dev-support scripts,"hadoop-ozone/dev-support/checks directory contains multiple helper script to execute different type of testing (findbugs, rat, unit, build).

They easily define how tests should be executed, with the following contract:

 * The problems should be printed out to the console

 * in case of test failure a non zero exit code should be used

 

The tests are working well (in fact I have some experiments with executing these scripts on k8s and argo where all the shell scripts are executed parallel) but we need some update:

 1. Most important: the unit tests and integration tests can be separated. Integration tests are more flaky and it's better to have a way to run only the normal unit tests

 2. As HDDS-1115 introduced a pom.ozone.xml it's better to use them instead of the magical ""am pl hadoop-ozone-dist"" trick--

 3. To make it possible to run blockade test in containers we should use - T flag with docker-compose

 4. checkstyle violations are printed out to the console",pull-request-available,[],HDDS,Improvement,Major,2019-06-28 23:51:33,6
13242230,Fix Ozone documentation,"JIRA to fix various typo, image and other issues in the ozone documentation.",pull-request-available,['documentation'],HDDS,Improvement,Major,2019-06-28 19:41:48,81
13242059,Implement File CreateFile Request to use Cache and DoubleBuffer,"In this Jira, we shall implement createFile request according to the HA model, and use cache and double buffer.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-27 23:53:25,13
13242026,Implement File CreateDirectory Request to use Cache and DoubleBuffer,"In this Jira, we shall implement createDirectory request according to the HA model, and use cache and double buffer.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-27 20:52:52,13
13241957,Ozone Client should timeout if the put block futures are taking a long time,"Ozone client currently enqueues a put future to the future map, However if the pipeline is slow, the client does not timeout and wait for the future to finish. For reasonable latency in the system, the client should timeout",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-06-27 14:29:08,18
13241872,Add metrics for leader's latency in ContainerStateMachine,This jira proposes to add metrics around leaders round trip reply to ratis client. This will be done via startTransaction api ,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-06-27 07:26:46,18
13241569,Use generation of resourceName for locks in OzoneManagerLock,"In this Jira, we shall use generate Resourcename from actual resource names like volume/bucket/user/key inside OzoneManagerLock. In this way, users using these locking API's no need to worry of calling these additional API of generateResourceName in OzoneManagerLockUtil. And this also reduces code during acquiring locks in OM operations.",pull-request-available,[],HDDS,Sub-task,Major,2019-06-25 19:19:37,13
13241308,pv-test example to test csi is not working,"[~rmaruthiyodan] reported two problems regarding to the pv-test example in csi examples folder.

pv-test folder contains an example nginx deployment which can use an ozone PVC/PV to publish content of a folder via http.

Two problems are identified:
 * The label based matching filter of service doesn't point to the nginx deployment
 * The configmap mounting is missing from nginx deployment",pull-request-available,[],HDDS,Bug,Blocker,2019-06-24 17:13:47,6
13240985,Create new OzoneManagerLock class,"This Jira is to use bit manipulation, instead of hashmap in OzoneManager lock logic. And also this Jira follows the locking order based on the document attached to HDDS-1672 jira.

This Jira is created based on [~anu] comment during review of HDDS-1672.

Not a suggestion for this patch. But more of a question, should we just maintain a bitset here, and just flip that bit up and down to see if the lock is held. Or we can just maintain 32 bit integer, and we can easily find if a lock is held by Xoring with the correct mask. I feel that might be super efficient. [@nandakumar131|https://github.com/nandakumar131] . But as I said let us not do that in this patch.

 

This Jira will add new class, integration of this new class into code will be done in a new jira. 

Clean up of old code also will be done in new jira.",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-06-22 00:05:04,13
13240949,Use the bindings in ReconSchemaGenerationModule to create Recon SQL tables on startup,"Currently the table creation is done for each schema definition one by one. 

Setup sqlite DB and create Recon SQL tables.
cc [~vivekratnavel], [~swagle]",newbie pull-request-available,['Ozone Recon'],HDDS,Task,Major,2019-06-21 20:18:33,30
13240940,Client Metrics are not being pushed to the configured sink while running a hadoop command to write to Ozone.,Client Metrics are not being pushed to the configured sink while running a hadoop command to write to Ozone.,pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-06-21 19:12:05,30
13240939,Add ability to configure RocksDB logs for Ozone Manager,"While doing performance testing, it was seen that there was no way to get RocksDB logs for Ozone Manager. Along with Rocksdb metrics, this may be a useful mechanism to understand the health of Rocksdb while investigating large clusters. ",pull-request-available,[],HDDS,Task,Major,2019-06-21 19:10:49,30
13240936,Increase ratis log segment size to 1MB.,"While testing out ozone with long running clients which continuously write data, it was noted ratis logs were rolled 1-2 times every second. This adds unnecessary overhead to the pipeline thereby affecting write throughput. Increasing the size of the log segment to 1MB will decrease the overhead.",pull-request-available,['Ozone Datanode'],HDDS,Task,Major,2019-06-21 19:02:23,30
13240935,Increase Ratis Leader election timeout default,"While testing out ozone with long running clients which continuously write data, it was noted that whenever a 1 second GC pause occurs in the leader, it triggers a leader election thereby disturbing the steady state of the system for more time than the GC pause delay.  ",pull-request-available,['Ozone Datanode'],HDDS,Task,Critical,2019-06-21 19:01:31,86
13240851,MR Job fails as OMFailoverProxyProvider has dependency hadoop-3.2,"Mapreduce Jobs are failing with exception ??Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient exception??

Ozone hadoop-ozone-filesystem-lib-current.jar copied to HDP cluster's hadoop and mapreduce classpath under :

{code:java}
/usr/hdp/3.1.0.0-78/hadoop/lib/hadoop-ozone-filesystem-lib-current-0.5.0-SNAPSHOT.jar
/usr/hdp/3.1.0.0-78/hadoop-mapreduce/hadoop-ozone-filesystem-lib-current-0.5.0-SNAPSHOT.jar
{code}

Excerpt from exception :
{code:java}
2019-06-21 10:07:57,982 ERROR [main] org.apache.hadoop.ozone.client.OzoneClientFactory: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient exception:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)
	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)
	at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.lang.VerifyError: Cannot inherit from final class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
	... 33 more
2019-06-21 10:07:57,985 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state INITED
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:299)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)
	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)
	at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)
	... 11 more
Caused by: java.lang.VerifyError: Cannot inherit from final class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
	... 28 more
2019-06-21 10:07:57,987 ERROR [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:299)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
	at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)
	at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
	at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)
	at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)
	... 11 more
Caused by: java.lang.VerifyError: Cannot inherit from final class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
	... 28 more
2019-06-21 10:07:57,988 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient

End of LogType:syslog
***********************************************************************
{code}


PFA YARN application logs for detailed exception.
",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Blocker,2019-06-21 11:58:20,43
13240844,Smoketest results are generated with an internal user,"[~eyang] reported the problem in HDDS-1609 that the smoketest results are generated a user (the user inside the docker container) which can be different from the host user.

There is a minimal risk that the test results can be deleted/corrupted by an other users if the current user is different from uid=1000

I opened this issue because [~eyang] said me during an offline discussion that HDDS-1609 is a more complex issue and not only about the ownership of the test results.

I suggest to handle the two problems in different way. With this patch, the permission of the test result files can be fixed easily.

In HDDS-1609 we can discuss about general security problems and try to find generic solution for them.

Steps to reproduce _this_ problem:
 # Use a user which is different from uid=1000
 # Create a new ozone build (mvn clean install -f pom.ozone.xml -DskipTests)
 # Go to a compose directory (cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/)
 # Execute tests (./test.sh)
 # check the ownership of the results (ls -lah ./results)

Current result: the owner of the result files are the user uid=1000

Expected result: the owner of the files should be always the current user (even if the current uid is different)

 ",pull-request-available,[],HDDS,Bug,Minor,2019-06-21 11:19:28,6
13240828,Update the Intellij runner definitition of SCM to use the new class name ,"HDDS-1622 changed the CLI framework of SCM and with a new additional class (StorageContainerMangerStarter) it made it more testable.

But the intellij runner definitions are not (yet) updated to use the new class name for SCM/SCM-init (they are updated for OM in HDDS-1660).

We need to adjust the main class names in:
{code:java}
hadoop-ozone/dev-support/intellij/runConfigurations/StorageContainerManager.xml
hadoop-ozone/dev-support/intellij/runConfigurations/StorageContainerManagerInit.xml{code}
 ",pull-request-available,['Tools'],HDDS,Bug,Trivial,2019-06-21 10:00:51,11
13240801,"When restart om with Kerberos, NPException happened at addPersistedDelegationToken ","the error stack:
{code:java}
2019-06-21 15:17:41,744 [main] INFO - Loaded 11 tokens
2019-06-21 15:17:41,745 [main] INFO - Loading token state into token manager.
2019-06-21 15:17:41,748 [main] ERROR - Failed to start the OzoneManager.
java.lang.NullPointerException
at org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.addPersistedDelegationToken(OzoneDelegationTokenSecretManager.java:371)
at org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.loadTokenSecretState(OzoneDelegationTokenSecretManager.java:358)
at org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.<init>(OzoneDelegationTokenSecretManager.java:96)
at org.apache.hadoop.ozone.om.OzoneManager.createDelegationTokenSecretManager(OzoneManager.java:608)
at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:332)
at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:941)
at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)
2019-06-21 15:17:41,753 [pool-2-thread-1] INFO - SHUTDOWN_MSG:

{code}",TriagePending,['Ozone Manager'],HDDS,Bug,Major,2019-06-21 07:34:37,28
13240710,ReplicationManager fail to find proper node topology based on Datanode details from heartbeat,"DN does not have the topology info included in its heartbeat message for container report/pipeline report.

SCM is where the topology information is available. During the processing of heartbeat, we should not rely on the datanodedetails from report to choose datanodes for close container. Otherwise, all the datanode locations of existing container replicas will fallback to /default-rack.

 

The fix is to retrieve the corresponding datanode locations from scm nodemanager, which has authoritative network topology information. ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-06-20 18:35:13,5
13240688,Remove sudo access from Ozone docker image,Ozone docker image is given unlimited sudo access to hadoop user.  This poses a security risk where host level user uid 1000 can attach a debugger to the container process to obtain root access.,pull-request-available,[],HDDS,Bug,Major,2019-06-20 16:20:16,109
13240607,Publish JVM metrics via Hadoop metrics,"In ozone metrics can be published with the help of hadoop metrics (for example via PrometheusMetricsSink)

The basic jvm metrics are not published by the metrics system (just with JMX)

I am very interested about the basic JVM metrics (gc count, heap memory usage) to identify possible problems in the test environment.

Fortunately it's very easy to turn it on with the help of org.apache.hadoop.metrics2.source.JvmMetrics.",pull-request-available,"['Ozone Datanode', 'Ozone Manager', 'SCM']",HDDS,Improvement,Major,2019-06-20 08:27:24,6
13240605,TestScmSafeNode is flaky,"org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode is failed at last night with the following error:
{code:java}
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode(TestScmSafeMode.java:285) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}
Locally it can be tested but it's very easy to reproduce by adding an additional sleep DataNodeSafeModeRule:
{code:java}
+++ b/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/DataNodeSafeModeRule.java
@@ -63,7 +63,11 @@ protected boolean validate() {
 
   @Override
   protected void process(NodeRegistrationContainerReport reportsProto) {
-
+    try {
+      Thread.sleep(3000);
+    } catch (InterruptedException e) {
+      e.printStackTrace();
+    }{code}
This is a clear race condition:

DatanodeSafeModeRule and ContainerSafeModeRule are processing the same events but it can be possible (in case of an accidental sleep) that the container safe mode rule is done, but DatanodeSafeModeRule didn't process the new event (yet).

As a result the test execution will continue:
{code:java}
GenericTestUtils
    .waitFor(() -> scm.getCurrentContainerThreshold() == 1.0, 100, 20000);
{code}
(This line is waiting ONLY for the ContainerSafeModeRule).

The fix is easy, let's wait for the processing of all the async events:
{code:java}
EventQueue eventQueue =
    (EventQueue) cluster.getStorageContainerManager().getEventQueue();
eventQueue.processAll(5000L);{code}
As we are sure that the events are already sent to the EventQueue (because we have the previous waitFor), it should be enough.",pull-request-available,"['SCM', 'test']",HDDS,Bug,Critical,2019-06-20 08:17:12,6
13240509,Expose metrics for unhealthy containers,HDDS-1201 introduced a capability for datanode to report unhealthy containers to SCM. This Jira is to expose this information as a metric for user visibility.,pull-request-available,['SCM'],HDDS,Sub-task,Critical,2019-06-19 21:19:24,106
13240059,Optimize Ozone Recon build time ,"Currently, hadoop-ozone-recon node_modules folder is copied to target folder and this takes a lot of time while building hadoop-ozone project. Reduce the build time by excluding node_modules folder.",pull-request-available,[],HDDS,Task,Major,2019-06-17 20:48:38,37
13240022,Move dockerbin script to libexec,Ozone tarball structure contains a new bin script directory called dockerbin.  These utility script can be relocated to OZONE_HOME/libexec because they are internal binaries that are not intended to be executed directly by users or shell scripts.,pull-request-available,[],HDDS,Bug,Major,2019-06-17 18:22:06,31
13240009,Switch to use apache/ozone-runner in the compose/Dockerfile,"Since HDDS-1634 we have an ozone specific runner image to run ozone with docker-compose based pseudo clusters.

As the new apache/ozone-runner image is uploaded to the dockerhub we can switch our scripts and use the new image.",pull-request-available,['docker'],HDDS,Improvement,Critical,2019-06-17 17:40:13,6
13240005,RocksDB use separate Write-ahead-log location for RocksDB.,"This will help on production systems where WAL logs and db actual data files will be in a different location. During compaction, it will not affect actual writes.

 

Suggested by [~msingh]",pull-request-available,[],HDDS,Bug,Major,2019-06-17 17:08:32,13
13239968,TestNodeReportHandler is failing with NPE,"{code:java}
FAILURE in ozone-unit-076618677d39x4h9/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.txt
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.scm.node.TestNodeReportHandler
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.43 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.node.TestNodeReportHandler
testNodeReport(org.apache.hadoop.hdds.scm.node.TestNodeReportHandler)  Time elapsed: 0.288 s  <<< ERROR!
java.lang.NullPointerException
    at org.apache.hadoop.hdds.scm.node.SCMNodeManager.<init>(SCMNodeManager.java:122)
    at org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.resetEventCollector(TestNodeReportHandler.java:53)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

2019-06-16 23:52:29,345 INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(119)) - Entering startup safe mode.

{code}",pull-request-available,"['SCM', 'test']",HDDS,Bug,Critical,2019-06-17 15:17:41,6
13239813,RDBTable#iterator should disabled caching of the keys during iterator,"Iterator normally do a bulk load of the keys, this causes thrashing of the actual keys in the DB.

This option is documented here:-
https://github.com/facebook/rocksdb/wiki/Basic-Operations#cache",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-06-16 16:59:06,13
13239812,RDBTable#isExist should use Rocksdb#keyMayExist,"RDBTable#isExist can use Rocksdb#keyMayExist, this avoids the cost of reading the value for the key.

Please refer, 
https://github.com/facebook/rocksdb/blob/7a8d7358bb40b13a06c2c6adc62e80295d89ed05/java/src/main/java/org/rocksdb/RocksDB.java#L2184",pull-request-available,[],HDDS,Improvement,Major,2019-06-16 16:54:06,30
13239656,ContainerController should provide a way to retrieve containers per volume,Ozone data scrubber needs to start a dedicated thread per volume to perform checksum validation for all the containers in the given volume. But currently ContainerController does not expose an API to fetch the list of containers for a given volume. This Jira is to add the relevant API to ContainerController class so that we can retrieve list of containers for a given HDDS volume.,pull-request-available,['Ozone Datanode'],HDDS,Sub-task,Major,2019-06-14 22:45:55,106
13239655,Implement S3 Create Bucket request to use Cache and DoubleBuffer,"Implement S3 Bucket write requests to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-14 22:31:50,13
13239388,"Recon: Add support for ""start"" query param to containers and containers/{id} endpoints","* Support ""start"" query param to seek to the given key in RocksDB.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-06-13 22:33:30,37
13239387,OM should create Ratis related dirs only if ratis is enabled,"In OM, Ratis related dirs (storage, snapshot etc.) should only be created if OM ratis is enabled.",pull-request-available,[],HDDS,Bug,Major,2019-06-13 22:33:10,43
13239295,TestEventWatcher.testMetrics is flaky,"TestEventWatcher is intermittent. (Failed twice out of 44 executions).

Error is:

{code}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.764 s <<< FAILURE! - in org.apache.hadoop.hdds.server.events.TestEventWatcher
testMetrics(org.apache.hadoop.hdds.server.events.TestEventWatcher)  Time elapsed: 2.384 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<3>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdds.server.events.TestEventWatcher.testMetrics(TestEventWatcher.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

In the test we do the following:

 1. fire start-event1
 2. fire start-event2
 3. fire start-event3
 4. fire end-event1
 5. wait

Usually the event2 and event3 are timed out and event1 is completed but in case of an accidental time between 3 and 4 (in fact between 1 and 4) the event1 also can be timed out.

I improved the unit test and fixed the metrics calculation (completed message should be incremented only if it's not yet timed out).",pull-request-available,['test'],HDDS,Bug,Major,2019-06-13 14:25:37,6
13239290,TestNodeReportHandler failing because of NPE,"
{code}
[INFO] Running org.apache.hadoop.hdds.scm.node.TestNodeReportHandler
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.469 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.node.TestNodeReportHandler
[ERROR] testNodeReport(org.apache.hadoop.hdds.scm.node.TestNodeReportHandler)  Time elapsed: 0.31 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.hdds.scm.node.SCMNodeManager.<init>(SCMNodeManager.java:122)
	at org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.resetEventCollector(TestNodeReportHandler.java:53)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",TriagePending,['test'],HDDS,Sub-task,Major,2019-06-13 13:55:39,19
13239270,Create missing parent directories during the creation of HddsVolume dirs,"I started to execute all the unit tests continuously (in kubernetes with argo workflow).

Until now I got the following failures (number of failures / unit test name):

```
      1 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir
      1 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename
      3 org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware
     31 org.apache.hadoop.ozone.container.common.TestDatanodeStateMachine
     31 org.apache.hadoop.ozone.container.common.volume.TestVolumeSet
      1 org.apache.hadoop.ozone.freon.TestDataValidateWithSafeByteOperations
```

TestVolumeSet is also failed locally:

{code}
2019-06-13 14:23:18,637 ERROR volume.VolumeSet (VolumeSet.java:initializeVolumeSet(184)) - Failed to parse the storage location: /home/elek/projects/hadoop/hadoop-hdds/container-service/target/test-dir/dfs
java.io.IOException: Cannot create directory /home/elek/projects/hadoop/hadoop-hdds/container-service/target/test-dir/dfs/hdds
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.initialize(HddsVolume.java:208)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:179)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:72)
	at org.apache.hadoop.ozone.container.common.volume.HddsVolume$Builder.build(HddsVolume.java:156)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.createVolume(VolumeSet.java:311)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.initializeVolumeSet(VolumeSet.java:165)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.<init>(VolumeSet.java:130)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.<init>(VolumeSet.java:109)
	at org.apache.hadoop.ozone.container.common.volume.TestVolumeSet.testFailVolumes(TestVolumeSet.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}

The problem here is that the parent directory of the volume dir is missing. I propose to use hddsRootDir.mkdirs() instead of hddsRootDir.mkdir() which creates the missing parent directories.
",pull-request-available,[],HDDS,Bug,Major,2019-06-13 12:27:35,6
13239202,Default image name for kubernetes examples should be ozone and not hadoop,"During the build the kubernetes example files are adjusted to use a specific docker image name.

By default it should be the apache/ozone:${VERSION} to make it possible to use the examples without any build from the release artifact. With the examples of the release artifact the user can use the latest released apache/ozone:${VERSION} from docker hub.

For development build the image can be set with -Ddocker.image (or -Dozone.docker.image with HDDS-1667).

Unfortunately -- due to a small typo -- apace/hadoop image is used by default instead of apache/ozone.  ",pull-request-available,['docker'],HDDS,Bug,Blocker,2019-06-13 08:49:14,6
13239198,Auditparser robot test shold use a world writable working directory,"When I tried to reproduce a problem which is reported by [~eyang], I found that the auditparser robot test uses the /opt/hadoop directory as a working directory to generate the audit.db export.

/opt/hadoop is may or may not be writable, it's better to use /tmp instead.",pull-request-available,['test'],HDDS,Bug,Major,2019-06-13 08:16:17,6
13239152,Cleanup Volume Request 2 phase old code,This Jira is to clean up the old 2 phase HA code for Volume requests.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-13 01:05:10,13
13239010,Make ScmBlockLocationProtocol message type based,"Most of the Ozone protocols are ""message type based"" and not ""method based"".

For example in OzoneManagerProtocol.proto there is only one method:

{code}
service OzoneManagerService {
    // A client-to-OM RPC to send client requests to OM Ratis server
    rpc submitRequest(OMRequest)
          returns(OMResponse);
}
{code}

And the exact method is determined by the type of the message:

{code}

message OMResponse {
  required Type cmdType = 1; // Type of the command

  // A string that identifies this command, we generate  Trace ID in Ozone
  // frontend and this allows us to trace that command all over ozone.
  optional string traceID = 2;

  optional bool success = 3 [default=true];

  optional string message = 4;

  required Status status = 5;

  optional string leaderOMNodeId = 6;

  optional CreateVolumeResponse              createVolumeResponse          = 11;
  optional SetVolumePropertyResponse         setVolumePropertyResponse     = 12;
  optional CheckVolumeAccessResponse         checkVolumeAccessResponse     = 13;
....
}


enum Type {
  CreateVolume = 11;
  SetVolumeProperty = 12;
  CheckVolumeAccess = 13;
  InfoVolume = 14;
  DeleteVolume = 15;
  ListVolume = 16;
....
{code}

This is not the most natural way to use protobuf services but it has the additional benefit that we can propagate traceId / exception in a common way.

Earlier there was an agreement to modify all the protocols to use this ""message type based"" approach to make it possible to provide proper error handling.

In this issue  the ScmBlockLocationProtocol.proto should be modified to use only one message:

{code}
service ScmBlockLocationProtocolService {

  rpc send (SCMBlockLocationRequest) returns (SCMBlockLocationResponse);
}
{code}

It also requires to create the common request and response objects (with the common fields like type, traceId, success, message, status as they are used in the OzoneManagerProtocol.proto).

To make it work, the ScmBlockLocationProtocolClientSideTranslatorPB and the ScmBlockLocationProtocolServerSideTranslatorPB should be improved to wrap/unwrap the original message to/from the generic message. 

I propose to only the protocol change here (if possible) we can keep the message/status fields empty and fix the error propagation in HDDS-1258

",pull-request-available,[],HDDS,Improvement,Major,2019-06-12 11:27:27,11
13238936,Improve locking in OzoneManager,"In this Jira, we shall follow the new lock ordering. In this way, in volume requests we can solve the issue of acquire/release/reacquire problem. And few bugs in the current implementation of S3Bucket/Volume operations.

 

Currently after acquiring volume lock, we cannot acquire user lock. 

This is causing an issue in Volume request implementation, acquire/release/reacquire volume lock.

 

Case of Delete Volume Request: 
 # Acquire volume lock.
 # Get Volume Info from DB
 # Release Volume lock. (We are releasing the lock, because while acquiring volume lock, we cannot acquire user lock0
 # Get owner from volume Info read from DB
 # Acquire owner lock
 # Acquire volume lock
 # Do delete logic
 # release volume lock
 # release user lock

 

We can avoid this acquire/release/reacquire lock issue by making volume lock as low weight. 

 

In this way, the above deleteVolume request will change as below
 # Acquire volume lock
 # Get Volume Info from DB
 # Get owner from volume Info read from DB
 # Acquire owner lock
 # Do delete logic
 # release owner lock
 # release volume lock. 

Same issue is seen with SetOwner for Volume request also.

During HDDS-1620 [~arp] brought up this issue. 

I am proposing the above solution to solve this issue. Any other idea/suggestions are welcome.

This also resolves a bug in setOwner for Volume request.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-06-12 05:52:03,13
13238902,Add limit support to /api/containers and /api/containers/{id} endpoints,Add support for limit query param to limit the results of /api/containers and /api/containers/\{id} endpoints,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-06-11 23:48:19,37
13238771,SCM startup is failing if network-topology-default.xml is part of a jar,"network-topology-default.xml can be loaded from file or classpath. But the NodeSchemaLoader assumes that the files on the classpath can be opened as a file. It's true if the file is in etc/hadoop (which is part of the classpath) but not true if the file is packaged to a jajr file:

{code}
scm_1         | 2019-06-11 13:18:03 INFO  NodeSchemaLoader:118 - Loading file from jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar!/network-topology-default.xml
scm_1         | 2019-06-11 13:18:03 ERROR NodeSchemaManager:74 - Failed to load schema file:network-topology-default.xml, error:
scm_1         | java.lang.IllegalArgumentException: URI is not hierarchical
scm_1         | 	at java.io.File.<init>(File.java:418)
scm_1         | 	at org.apache.hadoop.hdds.scm.net.NodeSchemaLoader.loadSchemaFromFile(NodeSchemaLoader.java:119)
scm_1         | 	at org.apache.hadoop.hdds.scm.net.NodeSchemaManager.init(NodeSchemaManager.java:67)
scm_1         | 	at org.apache.hadoop.hdds.scm.net.NetworkTopologyImpl.<init>(NetworkTopologyImpl.java:63)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:382)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:275)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:208)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:586)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:139)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:115)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:67)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:42)
scm_1         | 	at picocli.CommandLine.execute(CommandLine.java:1173)
scm_1         | 	at picocli.CommandLine.access$800(CommandLine.java:141)
scm_1         | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
scm_1         | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
scm_1         | 	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
scm_1         | 	at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
scm_1         | 	at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
scm_1         | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
scm_1         | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
scm_1         | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:56)
scm_1         | Failed to load schema file:network-topology-default.xml, error:
{code}

The quick fix is to keep the current behaviour but read the file from classloader.getResourceAsStream() instead of classloader.getResource().toURI()",pull-request-available,[],HDDS,Bug,Blocker,2019-06-11 13:46:48,6
13238720,Add liveness probe to the example k8s resources files,"In kubernetes resources we can define livebess probes which can help to detect any failure. If the define port is not available the pod will be rescheduled.

We need to add the liveness probes to our k8s resource files.

Note: We shouldn't add readiness probes. Readiness probe is about the service availability. The service/dns can be available only after the service is restarted. This is not good for us as:

 * We need DNS resolution during the startup (See OzoneManager.loadOMHAConfigs)
 * We already implemented retry in case of missing DNS entries",pull-request-available,[],HDDS,Sub-task,Major,2019-06-11 09:37:57,6
13238619,Improve logic in openKey when allocating block,"We set size as below

{code}
final long size = args.getDataSize() >= 0 ?
 args.getDataSize() : scmBlockSize;
{code}
 

and create OmKeyInfo with below size set. But when allocating Block for openKey, we use as below.

allocateBlockInKey(keyInfo, args.getDataSize(), currentTime);

 

I feel here, we should use size which is set above so that we allocate at least a block when the openKey call happens.",pull-request-available,[],HDDS,Bug,Major,2019-06-10 19:57:52,13
13238362,Missing test resources of integrataion-test project in target directory after compile,"The integration-test project, its origin resources missed in target directory after compile. ",pull-request-available,[],HDDS,Sub-task,Major,2019-06-08 15:52:49,5
13238164,Use Picocli for Ozone Manager,"Replicate the changes made in HDDS-1622 for the StorageContainerManager to the Ozone Manager, so it also uses Picocli for the command line interface.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-06-07 10:33:00,11
13238135,Define the process to add proposal/design docs to the Ozone subproject,"We think that it would be more effective to collect all the design docs in one place and make it easier to review them by the community.

We propose to follow an approach where the proposals are committed to the hadoop-hdds/docs project and the review can be the same as a review of a PR",pull-request-available,[],HDDS,Task,Major,2019-06-07 07:26:48,6
13238098,RaftRetryFailureException & AlreadyClosedException should not exclude pipeline from client,"This problem can be seen at https://builds.apache.org/job/hadoop-multibranch/job/PR-846/6/testReport/org.apache.hadoop.ozone.client.rpc/TestBCSID/testBCSID/.

As seen here, after a RaftRetryFailureException, the pipeline is excluded from the pipeline and that leads to SCM create a new pipeline. Creation of a new pipeline might not be possible in a test cluster because of limited number of nodes.

{code}
2019-06-06 22:29:23,311 WARN  KeyOutputStream - Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-AD0A1CB44582->73f367e6-7f91-4409-b4d3-b831e0bfb585@group-31FAD62742D6, cid=1, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$313/1466662004@60d08041 for 180 attempts with RetryLimited(maxAttempts=180, sleepTime=1000ms) on the pipeline Pipeline[ Id: 27d23af1-7180-42f5-b3c7-31fad62742d6, Nodes: 73f367e6-7f91-4409-b4d3-b831e0bfb585{ip: 172.17.0.2, host: 5e847226af57, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]. The last committed block length is 0, uncommitted data length is 5 retry count 0
2019-06-06 22:29:23,343 WARN  BlockManagerImpl - Pipeline creation failed for type:RATIS factor:ONE. Retrying get pipelines call once.
org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 1 using 0 nodes.
	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:151)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:57)
	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:149)
	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:190)
	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
	at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)
	at 
{code}",Triaged,['Ozone Client'],HDDS,Bug,Major,2019-06-07 03:27:19,16
13238054,Fix parallelStream usage in volume and key native acl.,Fix bug in volume and key native acl.,pull-request-available,[],HDDS,Bug,Major,2019-06-06 21:39:19,110
13238016,Redundant toString() call for metaDataPath in KeyValueContainerCheck,"Redundant toString() call for variable metadataPath at: 

https://github.com/apache/hadoop/blob/trunk/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerCheck.java#L284

",pull-request-available,[],HDDS,Improvement,Trivial,2019-06-06 18:49:33,98
13237876,Ensure container state on datanode gets synced to disk whenever state change happens,"Currently, whenever there is a container state change, it updates the container but doesn't sync.

The idea is here to is to force sync the state to disk everytime there is a state change.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-06-06 05:31:49,16
13237858,"Add option to ""ozone scmcli printTopology"" to order the output acccording to topology layer","Add option to order the output acccording to topology layer.
For example, for /rack/node topolgy, we can show,
State = HEALTHY
/default-rack:
ozone_datanode_1.ozone_default/172.18.0.3
ozone_datanode_2.ozone_default/172.18.0.2
ozone_datanode_3.ozone_default/172.18.0.4
/rack1:
ozone_datanode_4.ozone_default/172.18.0.5
ozone_datanode_5.ozone_default/172.18.0.6
For /dc/rack/node topology, we can either show
State = HEALTHY
/default-dc/default-rack:
ozone_datanode_1.ozone_default/172.18.0.3
ozone_datanode_2.ozone_default/172.18.0.2
ozone_datanode_3.ozone_default/172.18.0.4
/dc1/rack1:
ozone_datanode_4.ozone_default/172.18.0.5
ozone_datanode_5.ozone_default/172.18.0.6

or

State = HEALTHY
default-dc:
default-rack:
ozone_datanode_1.ozone_default/172.18.0.3
ozone_datanode_2.ozone_default/172.18.0.2
ozone_datanode_3.ozone_default/172.18.0.4
dc1:
rack1:
ozone_datanode_4.ozone_default/172.18.0.5
ozone_datanode_5.ozone_default/172.18.0.6",pull-request-available,[],HDDS,Sub-task,Major,2019-06-06 01:58:42,28
13237828,HddsDispatcher should not shutdown volumeSet,"Currently both OzoneContainer#stop() and HddsDispatcher#stop() both invoke volumeSet.shutdown() explicitly to shutdown the same volume set.

 

In addition, OzoneContainer#stop() will invoke HddsDispatcher#stop(). Since the volume set object is created by OzoneContainer object, it should be the responsibility of OzoneContainer to shutdown. This ticket is opened to remove the volumeSet.shutdown() from HddsDispatcher#stop(). 

 

There are benchmark tools relies on HddsDispatcher#stop() to shutdown volumeSet object, that we could fix with explict volumeSet#shutdown call. ",pull-request-available,[],HDDS,Bug,Major,2019-06-05 22:01:34,28
13237822,Create a http.policy config for Ozone,"Ozone currently uses dfs.http.policy for HTTP policy. Ozone should have its own ozone.http.policy configuration and if undefined, then fallback to dfs.http.policy.",newbie pull-request-available,[],HDDS,Bug,Major,2019-06-05 21:47:17,98
13237819,Fix Ozone tests leaking volume checker thread,There are a few test leaking hdds volume checker thread. This ticket is opened to fix them. ,pull-request-available,[],HDDS,Bug,Major,2019-06-05 21:35:56,28
13237782,"On installSnapshot notification from OM leader, download checkpoint and reload OM state","Installing a DB checkpoint on the OM involves following steps:
 1. When an OM follower receives installSnapshot notification from OM leader, it should initiate a new checkpoint on the OM leader and download that checkpoint through Http. 
 2. After downloading the checkpoint, the StateMachine must be paused so that the old OM DB can be replaced with the new downloaded checkpoint. 
 3. The OM should be reloaded with the new state . All the services having a dependency on the OM DB (such as MetadataManager, KeyManager etc.) must be re-initialized/ restarted. 
 4. Once the OM is ready with the new state, the state machine must be unpaused to resume participating in the Ratis ring.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-05 18:22:52,43
13237764,Recon config tag does not show up on Ozone UI.,Recon tag does not show up on the list of tags on /conf page. ,pull-request-available,[],HDDS,Bug,Major,2019-06-05 16:59:11,30
13237720,Support real persistence in the k8s example files ,"Ozone release contains example k8s deployment files to make it easier to deploy Ozone to kubernetes. As of now we use emptyDir everywhere, we should support the configuration of host volumes (hostPath or Local Persistent volumes).

The big question here is the default:

* Make the examples easy to start and ephemeral
* Make the examples more safe, by default (but couldn't be started without additional administration).

(Note this conversation is started in the review of HDDS-1508)

Xiaoyu:  Can we support mount hostVolume for datanode daemons?

Marton: Yes, we can.

AFAIK there are two options:
 * using [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)
 * or with [Local PersistentVolumes](https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/)

The first one requires the knowledge of directory names on the host.
The second one is recommended but it requires the creation of PersistentVolumes or install a PersistentVolume provider

I am not sure what is the best approach, my current proposal is:

 * Use empty dir everywhere to make it easier to start simple ozone cluster
 * Provide simple option to turn on any of theses persistence (the kubernetes files are generated and the generation can be parametrized)
 * Document how to customize the kubernetes resources files

Summary: it's question of the defaults:
 
  1. Use a complex, but persistent solution, which may not work out of the box   
  2. Use a simple, but ephemeral solution (as default)

I started to use (2) but I am open to change.

",pull-request-available,[],HDDS,Sub-task,Critical,2019-06-05 12:33:01,6
13237686,Change the version of Pico CLI to the latest 3.x release - 3.9.6,"The version of Pico CLI used in the project is 3.5.2. The current stable release is 3.9.6 and it supports some good new features, such as being able to create sub-commands as methods rather than standalone classes.

We should increase the Pico CLI version in the project pom.xml to 3.9.6 to take advantage of these new features as the CLI code is refactored to use Pico CLI, such as in HDDS-1622.",pull-request-available,['Ozone CLI'],HDDS,Improvement,Major,2019-06-05 09:27:37,11
13237565,Send hostName also part of OMRequest,"This Jira is created based on the comment from [~eyang] on HDDS-1600 jira.

[~bharatviswa] can hostname be used as part of OM request? For running in docker container, virtual private network address may not be routable or exposed to outside world. Using IP to identify the source client location may not be enough. It would be nice to have ability support hostname based request too.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-04 19:34:47,31
13237491,Csi server fails because transitive Netty dependencies,"CSI server can't be started because an ClassNotFound exception.

It turned out that with using the new configuration api we got old netty jar files as transitive dependencies. (hdds-configuration depends on hadoop-common, hadoop-commons depends on the word)

We should exclude all the old netty version from the classpath of the CSI server.",pull-request-available,[],HDDS,Bug,Blocker,2019-06-04 13:20:14,6
13237438,Reduce the size of recon jar file,"hadoop-ozone-recon-0.5.0-SNAPSHOT.jar is 73 MB, mainly because the node_modules are included (full typescript source, eslint, babel, etc.):

{code}
unzip -l hadoop-ozone-recon-0.5.0-SNAPSHOT.jar | grep node_modules | wc
{code}

Fix me if I am wrong, but I think node_modules is not required in the distribution as the dependencies are already included in the compiled javascript files.

I propose to remove the node_modules from the jar file.
",pull-request-available,['Ozone Recon'],HDDS,Improvement,Major,2019-06-04 09:37:36,37
13237425,Restructure documentation pages for better understanding,"Documentation page should be updated according to the recent changes:

In the uploaded PR I modified the following:

 #  Pages are restructured to use a similar structure what is intruced on the wiki by [~anu]. (Getting started guides are separated for different environments)
 # The width of the menu is increased (to make it more readable)
 # The logo is moved from the main page from the menu (to get more space for the menu items)
 # 'Requirements' section is added to each 'Getting started' page
 # Test tools / docker image / kubernetes pages are imported from the wiki. ",pull-request-available,['documentation'],HDDS,Improvement,Blocker,2019-06-04 08:41:54,6
13237383,Implement Key Write Requests to use Cache and DoubleBuffer,"Implement Key write requests to use OM Cache, double buffer. 

In this Jira will add the changes to implement key operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-06-04 02:33:34,13
13237309,Fix random test failure TestSCMContainerPlacementRackAware,"This has been seen randomly in latest trunk CI, e.g., [https://ci.anzix.net/job/ozone/16980/testReport/org.apache.hadoop.hdds.scm.container.placement.algorithms/TestSCMContainerPlacementRackAware/testFallback/]

 ",pull-request-available,[],HDDS,Sub-task,Major,2019-06-03 19:26:46,5
13237232,Tracing id is not propagated via async datanode grpc call,"Recently a new exception become visible in the datanode logs, using standard freon (STANDLAONE)

{code}
datanode_2  | 2019-06-03 12:18:21 WARN  PropagationRegistry$ExceptionCatchingExtractorDecorator:60 - Error when extracting SpanContext from carrier. Handling gracefully.
datanode_2  | io.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 7576cabf-37a4-4232-9729-939a3fdb68c4WriteChunk150a8a848a951784256ca0801f7d9cf8b_stream_ed583cee-9552-4f1a-8c77-63f7d07b755f_chunk_1
datanode_2  | 	at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:49)
datanode_2  | 	at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:34)
datanode_2  | 	at io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)
datanode_2  | 	at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)
datanode_2  | 	at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)
datanode_2  | 	at io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)
datanode_2  | 	at org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:102)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
datanode_2  | 	at org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
{code}

It turned out that the tracingId propagation between XCeiverClient and Server doesn't work very well (in case of Standalone and async commands)

 1. there are many places (on the client side) where the traceId filled with  UUID.randomUUID().toString();  
 2. This random id is propagated between the Output/InputStream and different part of the clients
 3. It is unnecessary, because in the XceiverClientGrpc and XceiverClientGrpc the traceId field is overridden with the real opentracing id anyway (sendCommand/sendCommandAsync)
 4. Except in the XceiverClientGrpc.sendCommandAsync where this part is accidentally missing.

Things to fix:

 1. fix XceiverClientGrpc.sendCommandAsync (replace any existing traceId with the good one)
 2. remove the usage of the UUID based traceId (it's not used)
 3. Improve the error logging in case of an invalid traceId on the server side.",pull-request-available,[],HDDS,Bug,Major,2019-06-03 12:29:29,6
13237216,Maintain docker entrypoint and envtoconf inside ozone project,"During an offline discussion with [~eyang] and [~arp], Eric suggested to maintain the source of the docker specific start images inside the main ozone branch (trunk) instead of the branch of the docker image.

With this approach the ozone-runner image can be a very lightweight image and the entrypoint logic can be versioned together with the ozone itself.

An other use case is a container creation script. Recently we [documented|https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Docker+images] that hadoop-runner/ozone-runner/ozone images are not for production (for example because they contain development tools).

We can create a helper tool (similar what Spark provides) to create Ozone container images from any production ready base image. But this tool requires the existence of the scripts inside the distribution.

(ps: I think sooner or later the functionality of envtoconf.py can be added to the OzoneConfiguration java class and we can parse the configuration values directly from environment variables.

In this patch I copied the required scripts to the ozone source tree and the new ozone-runner image (HDDS-1634) is designed to use it from this specific location.
",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 11:10:51,6
13237212,Introduce a new ozone specific runner image,"Ozone compose files use apache/hadoop-runner to provide a fixed environment to run any Ozone distribution.

 It can be better to use separated hadoop-runner and ozone-runner:

 1. To make it easier to include Ozone specific behaviour (For example goofys install, scm/om initialization)
 2. To make it clean which feature is required by all the subprojects of Hadoop and which one is Ozone specific (base on the comment from [~eyang] in HADOOP-16092)
 3. for hadoop-runner we maintain two tags (jdk11/jdk8/latest). And it seems to be hard to maintain all of them. jdk8 is required only for hadoop and with separating hadoop-runner/ozone-runner we can use only one simple branch for ozone-runner development (and we can create incremental fixed tags very easily)",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 10:43:05,6
13237182,Update rat from 0.12 to 0.13 in hadoop-runner build script,"We have a new rat, the old one is not available. The url should be updated.",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 08:29:19,6
13237179,Make the hadoop home world readable and avoid sudo in hadoop-runner,"[~eyang] reporeted in HDDS-1609 that the hadoop-runner image can be started *without* mounting a real hadoop (usually, it's ounted) AND using a different uid:

{code}
docker run -it  -u $(id -u):$(id -g) apache/hadoop-runner bash
docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""chdir to cwd (\""/opt/hadoop\"") set in config.json failed: permission denied"": unknown.
{code}

There are two blocking problems here:

 * the /opt/hadoop directory (which is the CWD inside the container) is 700 instead of 755
 * The usage of sudo in started scripts (sudo is not possible if the real user is not added to the /etc/passwd)

Both of them are addressed by this patch.",pull-request-available,[],HDDS,Improvement,Trivial,2019-06-03 08:19:45,6
13237176,Fix auditparser smoketests,"In HDDS-1518 we modified the location of the var and config files inside the container.

There are three problems with the current auditparser smokest:

 1. The default audit log4j files are not part of the new config directory (fixed with HDDS-1630)
 2. The smoketest is executed in scm container instead of om
 3. The log directory is hard coded

The 2 and 3 will be fined in this patch. ",pull-request-available,['test'],HDDS,Improvement,Major,2019-06-03 08:04:59,6
13237174,Copy default configuration files to the writeable directory,"HDDS-1518 separates the read-only directories (/opt/ozone, /opt/hadoop) from the read-write directories (/etc/hadoop, /var/log/hadoop). 

The configuration directory and log directory should be writeable and to make it easier to run the docker-compose based pseudo clusters with *different* host uid we started to use different config dir.

But we need all the defaults in the configuration dir. In this patch I add a small fragments to the hadoop-runner image to copy the default files (if available).",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 08:00:53,6
13237161,Tar file creation can be optional for non-dist builds,"Ozone tar file creation is a very time consuming step. I propose to make it optional and create the tar file only if the dist profile is enabled (-Pdist)

The tar file is not required to test ozone as the same content is available from hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT which is enough to run docker-compose pseudo clusters, smoketests. 

If it's required, the tar file creation can be requested by the dist profile.
 
On my machine (ssd based) it can cause 5-10% time improvements as the tar size is ~500MB and it requires a lot of IO.",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 06:27:34,6
13237158,Fix the execution and return code of smoketest executor shell script,"Problem: Some of the smoketest executions were reported to green even if they contained failed tests.

Root cause: the legacy test executor (hadoop-ozone/dist/src/main/smoketest/test.sh) which just calls the new executor script (hadoop-ozone/dist/src/main/compose/test-all.sh) didn't handle the return code well (the failure of the smoketests should be signalled by the bash return code)

This patch:
 * Fixes the error code handling in smoketest/test.sh
 * Fixes the test execution in compose/test-all.sh (should work from any other directories)
 * Updates hadoop-ozone/dev-support/checks/acceptance.sh to use the newer test-all.sh executor instead of the old one.",pull-request-available,['test'],HDDS,Bug,Blocker,2019-06-03 06:06:24,6
13237155,Make the version of the used hadoop-runner configurable,"During an offline discussion with [~arp] and [~eyang] we agreed that it could be more safe to fix the tag of the used hadoop-runner images during the releases.

It also requires fix tags from hadoop-runner, but after that it's possible to use the fixed tags.

This patch makes it possible to define the required version/tag in pom.xml

 1. the default hadoop-runner.version is added to all .env files  during the build
 2. If a variable is added to the .env, it can be used from docker-compose files AND can be overridden by environment variables (it makes it possible to define custom version during a local run) ",pull-request-available,[],HDDS,Improvement,Major,2019-06-03 05:54:45,6
13236973,ConcurrentModificationException when SCM has containers of different owners,"{{SCMBlockProtocolServer#allocateBlock}} throws {{ConcurrentModificationException}} when SCM has containers of different owners.

{code}
2019-05-31 13:53:16,808 WARN org.apache.hadoop.hdds.scm.container.SCMContainerManager: Container allocation failed for pipeline=Pipeline[ Id: 3e0eec4d-67d1-4582-a9e9-e68b0a340de6, Nodes: abaea3d2-a8c1-47de-8cdb-7cc5ed8f23a6{ip: 10.17.219.50, host: v
c1340.halxg.cloudera.com, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] requiredSize=268435456 {}
java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1211)
        at java.util.TreeMap$KeyIterator.next(TreeMap.java:1265)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:473)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:394)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:203)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
{code}",pull-request-available,[],HDDS,Bug,Critical,2019-05-31 21:32:18,30
13236952,Refactor operations inside the bucket lock in OM key write.,"There are a few steps that are done inside the OM bucket lock that are lock invariant and can be done outside the lock. This patch refactors those steps. It also adds an isExist API in the metadata store so that we dont need to deserialize the byte[] to Object while doing a simple _table.get(key) != null_ check. 

On applying the patch, the OM + SCM (With dummy datanodes) write performance improves by around 3 - 6x based on number of existing keys in the OM RocksDB. 

Thanks to [~nandakumar131] who helped with this patch. ",pull-request-available,['Ozone Manager'],HDDS,Bug,Critical,2019-05-31 19:09:05,30
13236897,Use picocli for StorageContainerManager,"Recently we switched to use PicoCli with (almost) all of our daemons (eg. s3 Gateway, Freon, etc.)

PicoCli has better output, it can generate nice help, and easier to use as it's enough to put a few annotations and we don't need to add all the boilerplate code to print out help, etc.

StorageContainerManager and OzoneManager is not yet  supported. The previous issue was closed HDDS-453 but since then we improved the GenericCli parser (eg. in HDDS-1192), so I think we are ready to move.

The main idea is to create a starter java similar to org.apache.hadoop.ozone.s3.Gateway and we can start StorageContainerManager from there.


 ",pull-request-available,[],HDDS,Improvement,Major,2019-05-31 15:50:12,11
13236791,writeData in ChunkUtils should not use AsynchronousFileChannel,"Currently, chunks writes are not synced to disk by default. When flushStateMachineData gests invoked from Ratis, it should also ensure all the pending chunk writes should be flushed to disk.",pull-request-available,[],HDDS,Sub-task,Major,2019-05-31 09:15:22,88
13236726,Implement Volume Write Requests to use Cache and DoubleBuffer,"Implement Volume write requests to use OM Cache, double buffer. 

In this Jira will add the changes to implement volume operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-31 00:28:23,13
13236714,Support volume acl operations for OM HA.,[HDDS-1539] adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-30 22:59:02,28
13236710,Merge code for HA and Non-HA OM requests for bucket,"In this Jira, we shall use the new code added in HDDS-1551 for Non-HA flow.

 

This Jira modifies the bucket requests only, further requests will be handled in further Jira's.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-30 22:11:13,13
13236633,Restructure the code layout for Ozone Manager,"The Ozone Manager has a flat structure that deals with lot of specific functions. This Jira proposes to refactor ozone managers code base and move function specific packages.

 ",Triaged backlog,['Ozone Manager'],HDDS,Improvement,Major,2019-05-30 21:41:26,102
13236453,ManagedChannel references are being leaked in while removing RaftGroup,"ManagedChannel references are being leaked in while removing RaftGroup

{code}
May 30, 2019 8:12:20 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference cleanQueue
SEVERE: *~*~*~ Channel ManagedChannelImpl{logId=1805, target=192.168.0.3:49867} was not shutdown properly!!! ~*~*~*
    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.
java.lang.RuntimeException: ManagedChannel allocation site
        at org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:103)
        at org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)
        at org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)
        at org.apache.ratis.thirdparty.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:411)
        at org.apache.ratis.grpc.client.GrpcClientProtocolClient.<init>(GrpcClientProtocolClient.java:118)
        at org.apache.ratis.grpc.client.GrpcClientRpc.lambda$new$0(GrpcClientRpc.java:55)
        at org.apache.ratis.util.PeerProxyMap$PeerAndProxy.lambda$getProxy$0(PeerProxyMap.java:61)
        at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)
        at org.apache.ratis.util.PeerProxyMap$PeerAndProxy.getProxy(PeerProxyMap.java:60)
        at org.apache.ratis.util.PeerProxyMap.getProxy(PeerProxyMap.java:107)
        at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:91)
        at org.apache.ratis.client.impl.RaftClientImpl.sendRequest(RaftClientImpl.java:409)
        at org.apache.ratis.client.impl.RaftClientImpl.groupRemove(RaftClientImpl.java:281)
        at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineUtils.destroyPipeline(RatisPipelineUtils.java:97)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:100)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:80)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:44)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-05-30 10:13:24,18
13236452,ManagedChannel references are being leaked in ReplicationSupervisor.java,"ManagedChannel references are being leaked in ReplicationSupervisor.java

{code}
May 30, 2019 8:10:56 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference cleanQueue
SEVERE: *~*~*~ Channel ManagedChannelImpl{logId=1495, target=192.168.0.3:49868} was not shutdown properly!!! ~*~*~*
    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.
java.lang.RuntimeException: ManagedChannel allocation site
        at org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:103)
        at org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)
        at org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)
        at org.apache.ratis.thirdparty.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:411)
        at org.apache.hadoop.ozone.container.replication.GrpcReplicationClient.<init>(GrpcReplicationClient.java:65)
        at org.apache.hadoop.ozone.container.replication.SimpleContainerDownloader.getContainerDataFromReplicas(SimpleContainerDownloader.java:87)
        at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:118)
        at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:115)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-05-30 10:07:02,18
13236449,Container Missing in the datanode after restart,"Container missing on the datanode after a restart.

{code}
08:10:44.308 [pool-2131-thread-1] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 34 locID: 102182684750055212 bcsId: 6198} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 34 has been lost and and cannot be recreated on this DataNode
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:207) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:149) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$0(ContainerStateMachine.java:385) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
{code}",MiniOzoneChaosCluster,['Ozone Datanode'],HDDS,Bug,Major,2019-05-30 09:59:24,16
13236443,"Opening of rocksDB in datanode fails with ""No locks available""","Block read fails with 

{code}
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Unable to find the block with bcsID 11777 .Container 68 bcsId is 0.
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:573)
        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:120)
        at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.initializeBlockInputStream(KeyInputStream.java:295)
        at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.getStream(KeyInputStream.java:265)
        at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.access$000(KeyInputStream.java:229)
        at org.apache.hadoop.ozone.client.io.KeyInputStream.getStreamEntry(KeyInputStream.java:107)
        at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:140)
        at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
        at java.io.InputStream.read(InputStream.java:101)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:114)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:147)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}


Looking at the 3 datanodes, the containers are in bcs id of 11748, 11748 and 0.

{code}
2019-05-30 08:28:05,348 INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: GetBlock : Trace ID: 93a2a596076d2ee4:93a2a596076d2ee4:0:0 : Message: Unable to find the block with bcsID 11777 .Container 68 bcsId is 11748. : Result: UNKNOWN_BCSID


2019-05-30 08:28:05,363 INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: GetBlock : Trace ID: 93a2a596076d2ee4:93a2a596076d2ee4:0:0 : Message: Unable to find the block with bcsID 11777 .Container 68 bcsId is 11748. : Result: UNKNOWN_BCSID


2019-05-30 08:28:05,377 INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: GetBlock : Trace ID: 93a2a596076d2ee4:93a2a596076d2ee4:0:0 : Message: Unable to find the block with bcsID 11777 .Container 68 bcsId is 0. : Result: UNKNOWN_BCSID
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-05-30 09:43:48,18
13236389,"Add ""scmcli printTopology"" shell command to print datanode topology ","Command ""ozone scmcli printTopology"".  It will show nodes of all kinds of state(HEALTHY,STALE,DEAD,DECOMMISSIONING,DECOMMISSIONED). 
Here is an example,

{noformat}
State = HEALTHY
ozone_datanode_1.ozone_default/172.18.0.3    /default-rack
ozone_datanode_2.ozone_default/172.18.0.2    /default-rack
ozone_datanode_3.ozone_default/172.18.0.4    /default-rack
{noformat}
",pull-request-available,[],HDDS,Sub-task,Major,2019-05-30 02:43:09,5
13236296,applyTransaction failure should not be lost on restart,"If the applyTransaction fails in the containerStateMachine, then the container should not accept new writes on restart,.

This can occur if
# chunk write applyTransaction fails
# container state update to UNHEALTHY also fails
# Ratis snapshot is taken
# Node restarts
# container accepts new transactions",pull-request-available,[],HDDS,Bug,Blocker,2019-05-29 17:36:42,16
13236220,Create smoketest for non-secure mapreduce example,"We had multiple problems earlier with the classpath separation and the internal ozonefs classloader. Before fixing all the issues I propose to create a smoketest to detect if the classpath separation is broken again .

As a first step I created an smoketest/ozone-mr environment (based on the  work of [~xyao], which is secure) and a smoketest 

Possible follow-up works:

 * Adapt the test.sh for the ozonesecure-mr
 * Include test runs with older hadoop versions ",pull-request-available,['test'],HDDS,Improvement,Major,2019-05-29 11:45:45,6
13236073,Implement AuditLogging for OM HA Bucket write requests,"In this Jira, we shall implement audit logging for OM HA Bucket write requests.

As now we cannot use userName,  IpAddress from Server API's as these will be null, because the requests are executed under GRPC context. So, in our AuditLogger API's we need to pass username and remoteAddress which we can get from OMRequest after HDDS-1600 and use these during audit logging.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-28 18:46:59,13
13236055,ContainerReader#initializeUsedBytes leaks DB reference,"This was caught by the New ContainerCache with reference counting from HDDS-1449. The root cause is an unclosed KeyValueBlockIterator from ContainerReader#initializeUsedBytes.

I will post a patch shortly, which will fix some UT failures exposed by -HDDS-1449,- such as TestBCSID#testBCSID, etc.",pull-request-available,[],HDDS,Bug,Major,2019-05-28 17:33:59,28
13235890,Handle Ratis Append Failure in Container State Machine,"RATIS-573 would add notification to the State Machine on encountering failure during Log append. 

The scope of this jira is to build on RATIS-573 and define the handling for log append failure in Container State Machine.
1. Enqueue pipeline unhealthy action to SCM, add a reason code to the message.
2. Trigger heartbeat to SCM
3. Notify Ratis volume unhealthy to the Datanode, so that DN can trigger async volume checker

Changes in the SCM to leverage the additional failure reason code, is outside the scope of this jira.
",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2019-05-28 02:03:00,88
13235880,Fix TestContainerPersistence#testDeleteBlockTwice,[https://ci.anzix.net/job/ozone/16899/testReport/org.apache.hadoop.ozone.container.common.impl/TestContainerPersistence/testDeleteBlockTwice/],pull-request-available,[],HDDS,Task,Major,2019-05-27 23:00:34,13
13235874,Implement updating lastAppliedIndex after buffer flush to OM DB.,This Jira is to implement updating lastAppliedIndex in OzoneManagerStateMachine once after the buffer is flushed to OM DB. ,pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-27 21:36:21,13
13235857,Add userName and IPAddress as part of OMRequest.,"In OM HA, the actual execution of request happens under GRPC context, so UGI object which we retrieve from ProtobufRpcEngine.Server.getRemoteUser(); will not be available.

In similar manner ProtobufRpcEngine.Server.getRemoteIp().

 

So, during preExecute(which happens under RPC context) extract userName and IPAddress and add it to the OMRequest, and then send the request to ratis server.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-27 18:13:22,13
13235852,Fix TestReplicationManager and checkstyle issues.,"When working on HDDS-1551, found some test failures which are not related to HDDS-1551.

This is caused by HDDS-700. 

 

This has not caught by Jenkins run because our Jenkins run does not run UT's for all the sub-modules. In this case, it should have run UT's for hadoop-hdds-server-scm, as there are some changes in src/test files in that module, but still, it has not run for it. I think Jenkins run for ozone project is not properly setup.

[https://ci.anzix.net/job/ozone/16895/testReport/]

 ",pull-request-available,[],HDDS,Task,Major,2019-05-27 17:24:56,13
13235850,Fix Ozone checkstyle issues on trunk,"Some small checkstyle issues are accidentally committed with HDDS-700.

Trivial fixes are coming here...
",pull-request-available,[],HDDS,Bug,Blocker,2019-05-27 16:57:33,6
13235840,Remove hdds-server-scm dependency from ozone-common,"I noticed that the hadoop-ozone/common project depends on hadoop-hdds-server-scm project.

The common projects are designed to be a shared artifacts between client and server side. Adding additional dependency to the common pom means that the dependency will be available for all the clients as well.

(See the attached artifact about the current, desired structure).

We definitely don't need scm server dependency on the client side.

The code dependency is just one class (ScmUtils) and the shared code can be easily moved to the common.",pull-request-available,[],HDDS,Bug,Major,2019-05-27 15:18:53,6
13235502,Add per pipeline metrics,"Add metrics on per pipeline basis:
- bytes read/written
- container metrics (state changes)",pull-request-available,['SCM'],HDDS,Task,Major,2019-05-24 18:18:56,77
13235277,Support dynamically adding delegated class to filteredclass loader,"HDDS-922 added a filtered class loader with a list of delegated classes that will be loaded with the app launcher's classloader. With security enabled on ozone-0.4, there are some incompatible changes from Hadoop-common and hadoop-auth module from Hadoop-2.x to Hadoop-3.x. Some examples can be seen HDDS-1080, where the fix has to be made along with a rebuild/release. 

 

This ticket is opened to allow dynamically adding delegated classes or class prefix via environment variable. This way, we can easily adjust the setting in different deployment without rebuild/release.

 

 ",pull-request-available,[],HDDS,Bug,Major,2019-05-24 03:13:48,107
13235241,Allow Ozone RPC client to read with topology awareness,The idea is to leverage the node location from the block locations and perfer read from closer block replicas. ,pull-request-available,[],HDDS,Sub-task,Major,2019-05-23 21:40:31,5
13234966,Fix TestFailureHandlingByClient tests,"The test failures are caused bcoz the test relies on KeyoutputStream#getLocationList() to validate the no of preallocated blocks, but it has been changed recently to exclude the empty blocks. The fix is mostly to use KeyOutputStream#getStreamEntries() to get the no of preallocated blocks.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-05-22 18:19:25,16
13234774,Atleast one of the metadata dir config property must be tagged as REQUIRED,"This issue was discovered while working on HDDS-373 to generate a minimal ozone-site.xml with required values.

{panel:title=ozone-default.xml}
<property>
    <name>ozone.metadata.dirs</name>
    <value/>
    <tag>OZONE, OM, SCM, CONTAINER, STORAGE</tag>
    <description>
      This setting is the fallback location for SCM, OM and DataNodes
      to store their metadata. This setting may be used in test/PoC clusters
      to simplify configuration.

      For production clusters or any time you care about performance, it is
      recommended that ozone.om.db.dirs, ozone.scm.db.dirs and
      dfs.container.ratis.datanode.storage.dir be configured separately.
    </description>
  </property>
{panel}

However, none of the properties listed above are tagged as REQUIRED.

For starters, as the goal of HDDS-373 is to generate a simple minimal ozone-site.xml that can be used to start ozone, I propose that we do either of the following:
1. Tag ozone.metadata.dirs as REQUIRED 
OR
2. Tag ozone.om.db.dirs, ozone.scm.db.dirs and dfs.container.ratis.datanode.storage.dir as REQUIRED

For simplicity, I would prefer option 1 as that is the fallback config. We have already stated that for production use, we must defined the granular properties instead of relying on this fallback property ozone.metadata.dirs

As advised by [~xyao], we are going with Option 1 and adding more details to description of other metadata related properties that would use ozone.metadata.dirs as a fallback.",configuration pull-request-available,[],HDDS,Bug,Major,2019-05-22 02:34:26,81
13234747,Obtain Handler reference in ContainerScrubber,Obtain reference to Handler based on containerType in scrub() in ContainerScrubber.java,pull-request-available,[],HDDS,Sub-task,Major,2019-05-21 23:04:24,98
13234741,Create OMDoubleBuffer metrics,"This Jira is to implement OMDoubleBuffer metrics, to show metrics like.
 # flushIterations.
 # totalTransactionsflushed.

 

Any other related metrics. This Jira is created based on the comment by [~anu] during HDDS-1512 review.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-21 22:40:00,13
13234739,Add libstdc++ to ozone build docker image,"libstdc++ is required for node install in alpine builds. Otherwise we get this error:


{code:java}
[ERROR] node: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory{code}",pull-request-available,['Ozone Recon'],HDDS,Task,Major,2019-05-21 22:28:04,37
13234735,Add default pipeline placement policy implementation,"This is a simpler implementation of the PipelinePlacementPolicy that can be utilized if no network topology is defined for the cluster. We try to form pipelines from existing HEALTHY datanodes randomly, as long as they satisfy PipelinePlacementCriteria.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-05-21 22:22:22,77
13234734,Support configure more than one raft log storage to host multiple pipelines,"Support configure multiple raft log storage to host multiple THREE factor RATIS pipelines. 
Unless the storage is a fast media, datanode should try best to allocate different raft log storage for new pipeline. 
",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2019-05-21 22:21:00,5
13234731,Average out pipeline allocation on datanodes and add metrics/test,Details in design doc.,pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-05-21 22:14:18,77
13234729,Implement a Pipeline scrubber to clean up non-OPEN pipeline,"The design document talks about initial requirements for the pipeline scrubber.
 - Scan the pipelines that the nodes are a part of to select candidates for teardown.
 - Scan pipelines that do not have open containers currently in use and datanodes are in violation.
 - Schedule tear down operation if a candidate pipeline is found.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-05-21 22:11:32,77
13234727,Create an interface for pipeline placement policy to support network topologies,"Leverage the work done in HDDS-700 for pipeline creation for open containers.

Create an interface that can provide different policy implementations for pipeline creation. The default implementation should take into account no topology information is configured.",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-05-21 21:51:54,77
13234723,Add ability to SCM for creating multiple pipelines with same datanode,"- Refactor _RatisPipelineProvider.create()_ to be able to create pipelines with datanodes that are not a part of sufficient pipelines
- Define soft and hard upper bounds for pipeline membership
- Create SCMAllocationManager that can be leveraged to get a candidate set of datanodes based on placement policies
- Add the datanodes to internal datastructures
",pull-request-available,['SCM'],HDDS,Sub-task,Major,2019-05-21 21:47:12,77
13234718,Add RocksDB metrics to OM,"RocksDB statistics need to sinked to hadoop-metrics2 for Ozone Manager to understand how OM behaves under heavy load.
Example: ""rocksdb.bytes.written""

https://github.com/facebook/rocksdb/wiki/Statistics

",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-05-21 20:41:48,30
13234652,Rename k8s-dev and k8s-dev-push profiles to docker and docker-push,"Based on the feedback from [~eyang] I realized that the names of the k8s-dev and k8s-dev-push profiles are not expressive enough as the created containers can be used not only for kubernetes but can be used together with any other container orchestrator.

I propose to rename them to docker/docker-push.",Triaged,[],HDDS,Improvement,Major,2019-05-21 15:54:39,6
13234452,Ozone multi-raft support,"Apache Ratis supports multi-raft by allowing the same node to be a part of multiple raft groups. The proposal is to allow datanodes to be a part of multiple raft groups. The attached design doc explains the reasons for doing this as well a few initial design decisions. 

Some of the work in this feature also related to HDDS-700 which implements rack-aware container placement for closed containers.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,New Feature,Major,2019-05-21 04:09:14,77
13234215,Mark OPEN containers as QUASI_CLOSED as part of Ratis groupRemove,"Right now, if a pipeline is destroyed by SCM, all the container on the pipeline are marked as quasi closed when datanode received close container command. SCM while processing these containers reports, marks these containers as closed once majority of the nodes are available.

This is however not a sufficient condition in cases where the raft log directory is missing or corrupted. As the containers will not have all the applied transaction. 
To solve this problem, we should QUASI_CLOSE the containers in datanode as part of ratis groupRemove. If a container is in OPEN state in datanode without any active pipeline, it will be marked as Unhealthy while processing close container command.


cc [~jnp], [~shashikant], [~sdeka], [~nandakumar131]",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Bug,Blocker,2019-05-20 07:17:58,35
13234196,Include committedBytes to determine Out of Space in VolumeChoosingPolicy,"This is a follow-up from HDDS-1511 and HDDS-1535

Currently  when creating a new Container, the DN invokes RoundRobinVolumeChoosingPolicy:chooseVolume(). This routine checks for (volume available space > container max size). If no eligible volume is found, the policy throws a DiskOutOfSpaceException. This is the current behaviour.

However, the computation of available space does not take into consideration the space
that is going to be consumed by writes to existing containers which are still Open and accepting chunk writes.

This Jira proposes to enhance the space availability check in chooseVolume by inclusion of committed space(committedBytes in HddsVolume) in the equation.

The handling/management of the exception in Ratis will not be modified in this Jira. That will be scoped separately as part of Datanode IO Failure handling work.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-05-20 03:30:13,88
13234180,IllegalArgumentException while processing container Reports,"IllegalArgumentException while processing container Reports

{code}
2019-05-19 23:15:04,137 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(88)) - Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@1a117ebc
java.lang.IllegalArgumentException
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:178)
        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:85)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:124)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:97)
        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:46)
        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-05-19 17:46:00,16
13234155,Datanode exits because Ratis fails to shutdown ratis server ,"Datanode exits because Ratis fails to shutdown ratis server 

{code}
2019-05-19 12:07:19,276 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(965)) - 80747533-f47c-43de-85b8-e70db448c63f: inconsistency entries. Reply:99930d0a-72ab-4795-a3ac-f3c
fb61ca1bb<-80747533-f47c-43de-85b8-e70db448c63f#3132:FAIL,INCONSISTENCY,nextIndex:9057,term:33,followerCommit:9057
2019-05-19 12:07:19,276 WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(320)) - e143b976-ab35-4555-a800-7f05a2b1b738: Failed to close GRPC server
java.io.InterruptedIOException: e143b976-ab35-4555-a800-7f05a2b1b738: shutdown server with port 64605 failed
        at org.apache.ratis.util.IOUtils.toInterruptedIOException(IOUtils.java:48)
        at org.apache.ratis.grpc.server.GrpcService.closeImpl(GrpcService.java:160)
        at org.apache.ratis.server.impl.RaftServerRpcWithProxy.lambda$close$2(RaftServerRpcWithProxy.java:76)
        at org.apache.ratis.util.LifeCycle.lambda$checkStateAndClose$2(LifeCycle.java:231)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:251)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:229)
        at org.apache.ratis.server.impl.RaftServerRpcWithProxy.close(RaftServerRpcWithProxy.java:76)
        at org.apache.ratis.server.impl.RaftServerProxy.lambda$close$4(RaftServerProxy.java:318)
        at org.apache.ratis.util.LifeCycle.lambda$checkStateAndClose$2(LifeCycle.java:231)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:251)
        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:229)
        at org.apache.ratis.server.impl.RaftServerProxy.close(RaftServerProxy.java:313)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.stop(XceiverServerRatis.java:432)
        at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.stop(OzoneContainer.java:201)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.close(DatanodeStateMachine.java:270)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.stopDaemon(DatanodeStateMachine.java:394)
        at org.apache.hadoop.ozone.HddsDatanodeService.stop(HddsDatanodeService.java:449)
        at org.apache.hadoop.ozone.HddsDatanodeService.terminateDatanode(HddsDatanodeService.java:429)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:208)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:349)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.awaitTermination(ServerImpl.java:282)
        at org.apache.ratis.grpc.server.GrpcService.closeImpl(GrpcService.java:158)
        ... 19 more
{code}",MiniOzoneChaosCluster TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2019-05-19 07:34:49,30
13234149,Disable install snapshot for ContainerStateMachine,"In case a follower lags behind the leader by a large number, the leader tries to send the snapshot to the follower. For ContainerStateMachine, the information in the snapshot it not the entire state machine data. InstallSnapshot for ContainerStateMachine should be disabled.


{code}
2019-05-19 10:58:22,198 WARN  server.GrpcLogAppender (GrpcLogAppender.java:installSnapshot(423)) - GrpcLogAppender(e3e19760-1340-4acd-b50d-f8a796a97254->28d9bd2f-3fe2-4a69-8120-757a00fa2f20): failed to install snapshot [/Users/msingh/code/apache/ozone/github/git_oz_bugs_fixes/hadoop-ozone/integration-test/target/test/data/MiniOzoneClusterImpl-c2a863ef-8be9-445c-886f-57cad3a7b12e/datanode-6/data/ratis/fb88b749-3e75-4381-8973-6e0cb4904c7e/sm/snapshot.2_190]: {}
java.lang.NullPointerException
        at org.apache.ratis.server.impl.LogAppender.readFileChunk(LogAppender.java:369)
        at org.apache.ratis.server.impl.LogAppender.access$1100(LogAppender.java:54)
        at org.apache.ratis.server.impl.LogAppender$SnapshotRequestIter$1.next(LogAppender.java:318)
        at org.apache.ratis.server.impl.LogAppender$SnapshotRequestIter$1.next(LogAppender.java:303)
        at org.apache.ratis.grpc.server.GrpcLogAppender.installSnapshot(GrpcLogAppender.java:412)
        at org.apache.ratis.grpc.server.GrpcLogAppender.runAppenderImpl(GrpcLogAppender.java:101)
        at org.apache.ratis.server.impl.LogAppender$AppenderDaemon.run(LogAppender.java:80)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-05-19 05:32:10,86
13233934,Add metrics in rack aware container placement policy,"To collect following statistics, 
1. total requested datanode count (A)
2. success allocated datanode count without constrain compromise (B)
3. success allocated datanode count with some comstrain compromise (C)

B includes C, failed allocation = (A - B)",pull-request-available,[],HDDS,Sub-task,Major,2019-05-17 09:47:04,5
13233931,RpcClient works with both Hadoop-3 and Hadoop-2,Provide a RpcClient for Hadoop 2. Current RpcClient depends on class KeyProviderTokenIssuer which is not available in Hadoop 2. ,TriagePending,[],HDDS,Task,Major,2019-05-17 09:25:40,5
13233847,Implement Bucket Write Requests to use Cache and DoubleBuffer,"Implement Bucket write requests to use OM Cache, double buffer.

And also in OM previously we used to Ratis client for communication to Ratis server, instead of that use Ratis server API's.

 

In this Jira will add the changes to implement bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-16 19:33:43,13
13233777,MiniOzoneCluster is not shutting down all the threads during shutdown.,MiniOzoneCluster does not shutdown all the threads during shutdown. All the threads must be shutdown to close the cluster correctly.,pull-request-available,['test'],HDDS,Test,Major,2019-05-16 13:51:26,18
13233542,"Cli to add,remove,get and delete acls for Ozone objects","Update Ozone Cli to add,remove,get and delete acls for Ozone objects",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:45,110
13233541,"Support default Acls for volume, bucket, keys and prefix","Add dAcls for volume, bucket, keys and prefix",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:32,28
13233540,"Implement addAcl,removeAcl,setAcl,getAcl  for Prefix","Implement addAcl,removeAcl,setAcl,getAcl  for Prefix",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:18,28
13233539,Create Radix tree to support ozone prefix ACLs ,Create Radix tree to support ozone prefix ACLs.,pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:37:03,28
13233537,"Implement addAcl,removeAcl,setAcl,getAcl  for Key","Implement addAcl,removeAcl,setAcl,getAcl  for Key",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:36:47,110
13233536,"Implement addAcl,removeAcl,setAcl,getAcl  for Bucket","Implement addAcl,removeAcl,setAcl,getAcl  for Bucket",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:36:33,110
13233535,"Implement addAcl,removeAcl,setAcl,getAcl for Volume","Implement addAcl,removeAcl,setAcl,getAcl for Volume",pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:36:21,110
13233534,Update ozone protobuf message for ACLs,Update ozone protobuf message for ACLs,pull-request-available,[],HDDS,Sub-task,Major,2019-05-15 17:36:00,110
13233513,testSCMSafeModeRestrictedOp is failing consistently,"The test is failing with the following stack trace.
{code}
[ERROR] testSCMSafeModeRestrictedOp(org.apache.hadoop.ozone.om.TestScmSafeMode)  Time elapsed: 9.79 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeModeRestrictedOp(TestScmSafeMode.java:304)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-05-15 15:48:35,28
13233448,Space tracking for Open Containers : Handle Node Startup,"This is related to HDDS-1511

Space tracking for Open Containers (committed space in the volume) relies on usedBytes in the Container state. usedBytes is not persisted for every update (chunkWrite). So on a node restart the value is stale.

The proposal is to:
iterate the block DB for each open container during startup and compute the used space.
The block DB process will be accelerated by spawning executors for each container.
This process will be carried out as part of building the container set during startup.
",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-05-15 10:53:25,88
13233426,JVM exit on TestHddsDatanodeService,"JVM exits when running TestHddsDatanodeService
https://builds.apache.org/job/hadoop-multibranch/job/PR-812/3/artifact/out/patch-unit-hadoop-hdds.txt

test encounters same failure on trunk without my patch for HDDS-1511


",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-05-15 09:13:36,18
13233406,Ozone: Freon: Improve the concurrent testing framework.,"Currently, Freon's concurrency framework is just on volume-level, but in actual testing, users are likely to provide a smaller volume number（typically 1）, and a larger bucket number and key number, in which case the existing concurrency framework can not make good use of the thread pool.

We need to improve the concurrency policy, make the volume creation task, bucket creation task, and key creation task all can be equally submitted to the thread pool as a general task. ",pull-request-available,['test'],HDDS,Bug,Major,2019-05-15 08:05:31,111
13233378,Disable the sync flag by default during chunk writes in Datanode,"Currently, by default while doing the chunk writes on datanodes, the sync flag is ON by default. This needs to be turned off by default.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-05-15 05:45:38,16
13232586,"Ozone: Freon: Support big files larger than 2GB and add ""--bufferSize"" and ""--validateWrites"" options.","*Current problems:*
 1. Freon does not support big files larger than 2GB because it use an int type ""keySize"" parameter and also ""keyValue"" buffer size.
 2. Freon allocates a entire buffer for each key at once, so if the key size is large and the concurrency is high, freon will report OOM exception frequently.
 3. Freon lacks option such as ""--validateWrites"", thus users cannot manually specify that verification is required after writing.

*Some solutions:*
 1. Use a long type ""keySize"" parameter, make sure freon can support big files larger than 2GB.
 2. Use a small buffer repeatedly than allocating the entire key-size buffer at once, the default buffer size is 4K and can be configured by ""–bufferSize"" parameter.
 3. Add a ""--validateWrites"" option to Freon command line, users can provide this option to indicate that a validation is required after write.
 

 

 ",pull-request-available,['test'],HDDS,Bug,Major,2019-05-10 07:23:13,111
13233326,HDDS Datanode start fails due to datanode.id file read errors,"* Ozone datanode start fails when there is an existing Datanode.id file which is non yaml format. (Yaml format was added through HDDS-1473.).
* Further, when 'ozone.scm.datanode.id' is not configured, the datanode.id file is created in a different directory than the fallback dir (ozone.metadata.dirs). Restart fails since it looks for the datanode.id in ozone.metadata.dirs. ",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-05-14 21:08:36,86
13233220,Publish ozone 0.4.0 to the dockerhub,"As 0.4.0-alpha is released we can publish the latest stable version on dockerhub.

Required steps:

 * Update ozone-latest branch in apache/hadoop-docker-ozone repository.
 * Create a new branch ozone-0.4.0 from the ozone-latest

Dockerhub is configured to create new tags from any branch which starts with ozone-:

branchname: ozone-XXXX --> docker image: apache/ozone:XXXX

Note: this releasing is in sync with ASF policies (See LEGAL-270)

bq. The main Docker Hub at hub.docker.com is a public-facing downstream
distribution channel – similar to Maven Central, PyPI, Debian package
management, etc.

bq. It is appropriate to distribute official releases through downstream channels, but inappropriate to distribute unreleased materials through them. (That's
why having `latest` on hub.docker.com point to git `master` is problematic.)
See Apache's formal Release Policy and Release Distribution Policy documents


This is a downstream distribution of the already voted and release ozone package. Please note that the Dockerfile points to the official download page. 

The ozone version command can show that we have exactly the same, voted and released bits inside: 

{code}
docker run apache/ozone:latest ozone version
                  //////////////                 
               ////////////////////              
            ////////     ////////////////        
           //////      ////////////////          
          /////      ////////////////  /         
         /////            ////////   ///         
         ////           ////////    /////        
        /////         ////////////////           
        /////       ////////////////   //        
         ////     ///////////////   /////        
         /////  ///////////////     ////         
          /////       //////      /////          
           //////   //////       /////           
             ///////////     ////////            
               //////  ////////////              
               ///   //////////                  
              /    0.4.0-alpha(Badlands)

Source code repository https://github.com/apache/hadoop.git -r 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b
Compiled by ajay.kumar on 2019-04-30T03:25Z
Compiled with protoc 2.5.0
From source with checksum 45e58ba9203a1b4470e183bf90281b20

Using HDDS 0.4.0-alpha
Source code repository https://github.com/apache/hadoop.git -r 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b
Compiled by ajay.kumar on 2019-04-30T03:24Z
Compiled with protoc 2.5.0
From source with checksum 57412e0def0317aed91721fb7ef5
{code}

How to test:

1. Single image version

{code}
./build.sh

docker run -p 9878:9878 -p 9876:9876  apache/ozone:latest

ls > /tmp/testfile
aws s3api --endpoint http://localhost:9878/ create-bucket --bucket=bucket1
aws s3 --endpoint http://localhost:9878 cp --storage-class REDUCED_REDUNDANCY /tmp/testfile  s3://bucket1/testfile
{code}

Yes, it's an ozone cluster in one line.

2. pseudo-cluster version

{code}
./build.sh
docker tag apache/ozone:latest  apache/ozone:0.4.0

mkdir /tmp/ozonetest
cd /tmp/ozonetest
#The required config files are inlined!!!
docker run apache/ozone:latest cat docker-config > docker-config
docker run apache/ozone:latest cat docker-config.yaml > docker-config.yaml

docker-compose up -d
docker-compose scale datanode=3

ls > /tmp/testfile
aws s3api --endpoint http://localhost:9878/ create-bucket --bucket=bucket1
aws s3 --endpoint http://localhost:9878 cp /tmp/testfile  s3://bucket1/testfile
{code}",pull-request-available,[],HDDS,Task,Major,2019-05-14 11:44:22,6
13233115,Mapreduce failure when using Hadoop 2.7.5,"Integrate Ozone(0.4 branch) with Hadoop 2.7.5, ""hdfs dfs -ls /"" can pass, while teragen  failed. 

When add  -verbose:class to java options, it shows that class KeyProvider is loaded twice by different classloader while it is only loaded once when execute  ""hdfs dfs -ls /"" 

All jars under share/ozone/lib are added into hadoop classpath except ozone file system current lib jar.
",pull-request-available,['Ozone Filesystem'],HDDS,Bug,Blocker,2019-05-14 02:15:15,6
13233018,Provide intellij runConfiguration for Ozone components,"Sometimes I need to start ozone cluster from intellij to debug issues. It's possible but it requires to create many runConfiguration object inside my IDE.

I propose here to share the intellij specific runtimeConfigs to make it easy for anybody (who uses intellij) to run full ozone cluster from the IDE (1 datanode only).",pull-request-available,[],HDDS,Task,Major,2019-05-13 16:24:42,6
13232697,Use /etc/ozone for configuration inside docker-compose ,"As [~eyang] reported the docker-compose clusters write the config files with uid=1000. In case of the build is created with different user (eg id=401) the hadoop user inside the container (id=100) can't work to the ozone/etc/hadoop directory.

I propose to generate the configuration file to /etc/hadoop (And add that directory to the classpath). In that case the volume mount of the ozone distribution folder can be read only.",pull-request-available,[],HDDS,Sub-task,Major,2019-05-10 15:54:33,6
13232692,AllocateBlock call fails with ContainerNotFoundException,"In allocateContainer call,  the container is first added to pipelineStateMap and then added to container cache. If two allocate blocks execute concurrently, it might happen that one find the container to exist in the pipelineStateMap but the container is yet to be updated in the container cache, hence failing with CONTAINER_NOT_FOUND exception.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-05-10 15:47:44,16
13232593,Create ozone dev-support script to check hadolint violiations,"hadoop-ozone/dev-support/checks/ directory contains helper scripts to execute different code quality checks locally.

They are different from yetus as they can be executed in an easy way and they check _ALL_ the violation of the current code base. 

We need to create a new script to check the [hadolint|https://github.com/hadolint/hadolint] errors in the hadoop-ozone and hadoop-hdds projects.

The contracts of the check scripts:

#  Exit code should define the result (0: passed, <>0 failed)
# Violation should be printed out to the stdout

We can assume that the hadolint is part of the development environment. For jenkins we can put it to the image of the dev builds.

As the check introduce zero-tolerance for the hadolint violations the biggest issue here is to eliminate all of the existing issues.

Thanks to [~eyang] for reporting that it's still missing.",newbie pull-request-available,[],HDDS,Improvement,Major,2019-05-10 07:54:36,112
13232530,Implement DoubleBuffer in OzoneManager,"This Jira is created to implement DoubleBuffer in OzoneManager to flush transactions to OM DB.

 
h2. Flushing Transactions to RocksDB:

We propose using an implementation similar to the HDFS EditsDoubleBuffer.  We shall flush RocksDB transactions in batches, instead of current way of using rocksdb.put() after every operation. At a given time only one batch will be outstanding for flush while newer transactions are accumulated in memory to be flushed later.

 

In DoubleBuffer it will have 2 buffers one is currentBuffer, and the other is readyBuffer. We add entry to current buffer, and we check if another flush call is outstanding. If not, we flush to disk Otherwise we add entries to otherBuffer while sync is happening.

 

In this if sync is happening, we shall add new requests to other buffer and when we can sync we use *RocksDB batch commit to sync to disk, instead of rocksdb put.*

 

Note: If flush to disk is failed on any OM, we shall terminate the OzoneManager, so that OM DB’s will not diverge. Flush failure should be considered as catastrophic failure.

 

Scope of this Jira is to add DoubleBuffer implementation, integrating to current OM will be done in further jira's.

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-09 20:39:41,13
13232486,Space tracking for Open Containers in HDDS Volumes,"For every HDDS Volume, track the space usage in open containers. Introduce a counter committedBytes in HddsVolume - this counts the remaining space in Open containers until they reach max capacity. The counter is incremented (by container max capacity) for every container create. And decremented (by chunk size) for every chunk write.

Space tracking for open containers will enable adding a safety check during container create.
If there is not sufficient free space in the volume, the container create operation can be failed.

The scope of this jira is to just add the space tracking for Open Containers. Checking for space and failing container create will be introduced in a subsequent jira.
",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-05-09 15:19:56,88
13232456,Classpath files are deployed to the maven repository as pom/jar files,"1. Classpath files are plain text files which are generatede for each ozone projects. Classpath files are used to defined the classpath of a module (om, scm, etc) based on the maven classpath.

Example classpath file:

{code}
classpath=$HDDS_LIB_JARS_DIR/kerb-simplekdc-1.0.1.jar:$HDDS_LIB_JARS_DIR/hk2-utils-2.5.0.jar:$HDDS_LIB_JARS_DIR/jackson-core-2.9.5.jar:$HDDS_LIB_JARS_DIR/ratis-netty-0.4.0-fe2b15d-SNAPSHOT.jar:$HDDS_LIB_JARS_DIR/protobuf-java-2.5.0.jar:... 
{code}

Classpath files are maven artifacts and copied to share/ozone/classpath in the distribution

2. 0.4.0 was the first release when we deployed the artifacts to the apache nexus. [~ajayydv] reported the problem that the staging repository can't be closed: INFRA-18344

It turned out that the classpath files are uploaded with jar extension to the repository. We deleted all the classpath files manually and the repository became closable.

To avoid similar issues we need to fix this problem and make sure that the classpath files are not uploaded to the repository during a 'mvn deploy' or uploaded but with a good extension.

ps: I don't know the exact solution yet, but I can imagine that bumping the version of maven deploy plugin can help. Seems to be a bug in the plugin.

ps2: This is blocker as we need to fix it before the next release",pull-request-available,['build'],HDDS,Bug,Blocker,2019-05-09 12:00:31,6
13232453,TestBlockOutputStreamWithFailures#test2DatanodesFailure fails intermittently,"The test fails because, the test expects a exception after 2 datanodes failures to be of type RaftRetryFailureException. But it might happen that, the pipeline gets destroyed quickly then actual write executes over Ratis, hence it will fail with GroupMismatchhException in such case.",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-05-09 11:51:57,16
13232411,Provide example k8s deployment files for the new CSI server,Issue HDDS-1382 introduced a new internal CSI server. We should provide example deployment files to make it easy to deploy it to any kubernetes cluster.,pull-request-available,[],HDDS,Sub-task,Major,2019-05-09 09:00:10,6
13232285,Reduce garbage generated by non-netty threads in datanode ratis server,We use GRPC protocol for rpc communication in Ratis. By default thread caches are generated even for non-netty threads. This Jira aims to add a default JVM parameter for disabling thread caches for non-netty threads in datanode ratis server.,pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-05-08 17:28:14,35
13232175,Add metrics for Ozone Ratis performance,"This jira will add some metrics for Ratis pipeline performance
1) number of bytes written
2) number Read state Machine calls

3) no of Read StateMachine Fails",pull-request-available,[],HDDS,Bug,Major,2019-05-08 08:25:29,16
13232076,Allocate block failures in client should print exception trace.,"The following error is seen intermittently in the Ozone client logs while writing large keys. We need to log the entire exception trace to find out more about the failure.

{code}
19/04/22 10:13:32 ERROR io.KeyOutputStream: Try to allocate more blocks for write failed, already allocated 0 blocks for this write.
{code}",pull-request-available,[],HDDS,Improvement,Major,2019-05-07 17:58:08,30
13232068,OzoneManager Cache,"In this Jira, we shall implement a cache for Table.

As with OM HA, we are planning to implement double buffer implementation to flush transaction in a batch, instead of using rocksdb put() for every operation. When this comes in to place we need cache in OzoneManager HA to handle/server the requests for validation/returning responses.

 

This Jira will implement Cache as an integral part of the table. In this way users using this table does not need to handle like check cache/db. For this, we can update get API in the table to handle the cache.

 

This Jira will implement:
 # Cache as a part of each Table.
 # Uses this cache in get().
 # Exposes api for cleanup, add entries to cache.

Usage to add the entries in to cache will be done in further jira's.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-05-07 17:31:14,13
13231920,Support partial chunk reads and checksum verification,"BlockInputStream#readChunkFromContainer() reads the whole chunk from disk even if we need to read only a part of the chunk.
This Jira aims to improve readChunkFromContainer so that only that part of the chunk file is read which is needed by client plus the part of chunk file which is required to verify the checksum.



For example, lets say the client is reading from index 120 to 450 in the chunk. And let's say checksum is stored for every 100 bytes in the chunk i.e. the first checksum is for bytes from index 0 to 99, the next for bytes from index 100 to 199 and so on. To verify bytes from 120 to 450, we would need to read from bytes 100 to 499 so that checksum verification can be done.",pull-request-available,[],HDDS,Improvement,Major,2019-05-07 04:04:42,43
13231824,Generated chunk size name too long.,"Following exception is seen in SCM logs intermittently. 

{code}
java.lang.RuntimeException: file name 'chunks/2a54b2a153f4a9c5da5f44e2c6f97c60_stream_9c6ac565-e2d4-469c-bd5c-47922a35e798_chunk_10.tmp.2.23115' is too long ( > 100 bytes)
{code}

We may have to limit the name of the chunk to 100 bytes.",pull-request-available,[],HDDS,Bug,Critical,2019-05-06 16:56:28,16
13231680,Ozone KeyInputStream seek() should not read the chunk file,"KeyInputStream#seek() calls BlockInputStream#seek() to adjust the buffer position to the seeked position. As part of the seek operation, the whole chunk is read from the container and stored in the buffer so that the buffer position can be advanced to the seeked position. 

We should not read from disk on a seek() operation. Instead, for a read operation, when the chunk file is read and put in the buffer, at that time, we can advance the buffer position to the previously seeked position.",pull-request-available,[],HDDS,Bug,Major,2019-05-05 22:08:18,43
13231632,"Support configurable container placement policy through ""ozone.scm.container.placement.classname""","Support system property ""ozone.scm.container.placement.classname"" in ozone-site.xml. User can specify the implementation class name as the value of the property.  Here is an example, 
 <property>
    <name>ozone.scm.container.placement.classname</name>
    <value>org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware</value>ß
 </property>

If this property is not set, then default org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware will be used. 
",pull-request-available,[],HDDS,Sub-task,Major,2019-05-05 07:00:43,5
13231564,Unnecessary log messages on console with Ozone shell ,"The following log messages are printed on the console when running putkey
{code}
$ ozone sh key put /vol1/bucket1/key1 myfile
2019-05-03 23:25:15 INFO  GrpcClientProtocolClient:254 - client-9A5E39BD681D->96c8bede-ba3f-4e01-86d8-53f97957f140: receive RaftClientReply:client-9A5E39BD681D->96c8bede-ba3f-4e01-86d8-53f97957f140@group-8B0913807C4D, cid=0, SUCCESS, logIndex=1, commits[96c8bede-ba3f-4e01-86d8-53f97957f140:c2]
2019-05-03 23:25:16 INFO  GrpcClientProtocolClient:254 - client-9A5E39BD681D->96c8bede-ba3f-4e01-86d8-53f97957f140: receive RaftClientReply:client-9A5E39BD681D->96c8bede-ba3f-4e01-86d8-53f97957f140@group-8B0913807C4D, cid=1, SUCCESS, logIndex=3, commits[96c8bede-ba3f-4e01-86d8-53f97957f140:c4]
{code}

These are unnecessary noise and should be suppressed by default.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-05-03 23:27:39,86
13231397,Scm cli command to start/stop replication manager,It would be nice to have scmcli command to start/stop the ReplicationManager thread running in SCM,pull-request-available,['SCM'],HDDS,Improvement,Blocker,2019-05-03 06:15:11,19
13231347,Bootstrap React framework for Recon UI,"Bootstrap React with Typescript, Ant, LESS and other necessary libraries for Recon UI. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-05-02 20:09:02,37
13231137,Fix getMultipartKey javadoc,"{code:java}
/**
<<<<<<< HEAD
 * Returns the DB key name of a multipart upload key in OM metadata store.
 *
 * @param volume - volume name
 * @param bucket - bucket name
 * @param key - key name
 * @param uploadId - the upload id for this key
 * @return bytes of DB key.
 */
 String getMultipartKey(String volume, String bucket, String key, String
 uploadId);{code}
 

Remove *<<<<<<< HEAD* unwanted change.",newbie pull-request-available,[],HDDS,Improvement,Minor,2019-05-01 20:21:40,81
13231110,Use strongly typed codec implementations for the S3Table,"HDDS-864 added the implementation for Strongly typed codec implementation for the tables of OmMetadataManager.

 

Tables which are added as part of S3 Implementation are not using this. This Jira is address to this.",pull-request-available,[],HDDS,Improvement,Major,2019-05-01 17:53:11,13
13231051,Cleanup BasicOzoneFileSystem#mkdir,Currently BasicOzoneFileSystem#mkdir does not have the optimizations made in HDDS-1300. The changes for this function were missed in HDDS-1460.,pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2019-05-01 06:55:22,35
13231023,Ip address should not be a part of the DatanodeID since it can change,"The DatanodeID identified by the DatanodeDetails object is persisted to disk and read back on restart. The following fields are currently being serialized and we should omit ip address from this set.

{quote}
UUID uuid;
String ipAddress;
String hostName;
List<Port> ports;
String certSerialId;
{quote}

cc: [~arpaga] this is follow-up from HDDS-1473",Triaged newbie,['Ozone Datanode'],HDDS,Improvement,Major,2019-04-30 23:54:59,44
13230785,Update S3.md documentation,"HDDS-791 implemented range get operation.

 

S3.md documentation has below line: 

GET Object | implemented | Range headers are not supported

 

This should be updated to remove the part `Range headers are not supported`.",newbie pull-request-available,[],HDDS,Bug,Major,2019-04-29 22:59:24,37
13230702,Provide k8s resources files for prometheus and performance tests,"Similar to HDDS-1412 we can further improve the available k8s resources with providing example resources to:

1) install prometheus
2) execute freon test and check the results.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-29 13:45:43,6
13230406,Fix logIfNeeded logic in EndPointStateMachine,"{code:java}
public void E(Exception ex) {
 LOG.trace(""Incrementing the Missed count. Ex : {}"", ex);
this.incMissed();
 if (this.getMissedCount() % getLogWarnInterval(conf) ==
 0) {
 LOG.error(
 ""Unable to communicate to SCM server at {} for past {} seconds."",
 this.getAddress().getHostString() + "":"" + this.getAddress().getPort(),
 TimeUnit.MILLISECONDS.toSeconds(
 this.getMissedCount() * getScmHeartbeatInterval(this.conf)), ex);
 }

}{code}
This method will be called when any exception occur in stateMachine to log an exception. But to not log aggresively we have this ozone.scm.heartbeat.log.warn.interval.count property to control  logging. 

 

There is a small issue here, we don't log the exception first time when it occurred. So, we need to log for the first time and then increment the missingCount.

 

Fix is to move the this.incMissed() to end of the method so that we log it for the first time exception occurred and after that every log.warn.interval.count exceptions happened.

 ",newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-26 23:00:47,86
13230404,Fix OzoneContainer start method,"In OzoneContainer start() we have 
{code:java}
startContainerScrub();
writeChannel.start();
readChannel.start();
hddsDispatcher.init();
hddsDispatcher.setScmId(scmId);{code}
 

Suppose here if readChannel.start() failed due to some reason, from VersionEndPointTask, we try to start OzoneContainer again. This can cause an issue for writeChannel.start() if it is already started. 

 

Fix the logic such a way that if service is started, don't attempt to start the service again. Similar changes needed to be done for stop().",newbie pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-26 22:56:11,30
13230403,"""ozone.scm.datanode.id"" config should take path for a dir and not a file","Currently, the ozone config ""ozone.scm.datanode.id"" takes file path as its value. It should instead take dir path as its value and assume a standard filename ""datanode.id""",newbie pull-request-available,['Ozone Datanode'],HDDS,Task,Minor,2019-04-26 22:53:17,37
13230402,DataNode ID file should be human readable,"The DataNode ID file should be human readable to make debugging easier. We should use YAML as we have used it elsewhere for meta files.

Currently it is a binary file whose contents are protobuf encoded. This is a tiny file read once on startup, so performance is not a concern.",newbie pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-04-26 22:46:24,86
13230390,Add retry to kinit command in smoketests,Add retry to kinit command in smoketests,pull-request-available,[],HDDS,Sub-task,Major,2019-04-26 20:33:21,110
13230388,Update ratis dependency to 0.3.0,Update ratis dependency to 0.3.0,pull-request-available,[],HDDS,Sub-task,Major,2019-04-26 20:22:53,110
13230170,Implement a CLI tool to dump the contents of rocksdb metadata,"The DataNode plugin for Ozone stores the protobuf message as the value in the rocksdb metadata store. Since the protobuf message contents are not human readable, it is difficult to introspect (e.g. for debugging). This Jira is to add a command-line tool to dump the contents of rocksdb database in human readable format (e.g. json or yaml).",Triaged,['Tools'],HDDS,Improvement,Minor,2019-04-25 20:08:49,26
13230095,Generate default configuration fragments based on annotations,"See the design doc in the parent jira for more details.

In this jira I introduce a new annotation processor which can generate ozone-default.xml fragments based on the annotations which are introduced by HDDS-1468.

The ozone-default-generated.xml fragments can be used directly by the OzoneConfiguration as I added a small code to the constructor to check ALL the available ozone-default-generated.xml files and add them to the available resources.

With this approach we don't need to edit ozone-default.xml as all the configuration can be defined in java code.

As a side effect each service will see only the available configuration keys and values based on the classpath. (If the ozone-default-generated.xml file of OzoneManager is not on the classpath of the SCM, SCM doesn't see the available configs.) 

",pull-request-available,[],HDDS,Sub-task,Major,2019-04-25 13:26:53,6
13230062,Inject configuration values to Java objects,"According to the design doc in the parent issue we would like to support java configuration objects which are simple POJO but the fields/setters are annotated. As a first step we can introduce the OzoneConfiguration.getConfigObject() api which can create the config object and inject configuration.

Later we can improve it with annotation processor which can generate the ozone-default.xml.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-25 09:24:22,6
13230001,Document the container replica state machine,Let's document the container states and the state transitions.,Triaged,['documentation'],HDDS,Improvement,Critical,2019-04-25 01:09:10,113
13229990,Client should have different retry policies for different exceptions,"Client should have different retry policies for different type of failures.

For example, If a key write fails because of ContainerNotOpen exception, the client should wait for a specified interval before retrying. But if the key write fails because of lets say ratis leader election or request timeout, we want the client to retry immediately.",pull-request-available,[],HDDS,Improvement,Major,2019-04-24 22:46:57,86
13229902,Fix content and format of Ozone documentation,"During the review of HDDS-1457 I realized that the current documentation contains many outdated information regarding the usage of docker, build commands or s3 usage.

The security information is also rendered in an incorrect way.

The png files for the prometheus page are missing (were included in the patch of HDDS-846 but missing from the commit).",pull-request-available,['documentation'],HDDS,Bug,Blocker,2019-04-24 15:17:44,6
13229890,Optimize listStatus api in OzoneFileSystem,"Currently in listStatus we make multiple getFileStatus calls. This can be optimized by converting to a single rpc call for listStatus.

Also currently listStatus has to traverse a directory recursively in order to list its immediate children. This happens because in OzoneManager all the metadata is stored in rocksdb sorted on keynames. The Jira also aims to fix this by using seek api provided by rocksdb.",pull-request-available,"['Ozone Filesystem', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-24 14:17:54,35
13229848,Add the optmizations of HDDS-1300 to BasicOzoneFileSystem,Some of the optimizations made in HDDS-1300 were reverted in HDDS-1333. This Jira aims to bring back those optimizations.,pull-request-available,[],HDDS,Bug,Major,2019-04-24 11:42:18,35
13229709,Create a maven profile to run fault injection tests,"Some fault injection tests have been written using blockade.  It would be nice to have ability to start docker compose and exercise the blockade test cases against Ozone docker containers, and generate reports.  This is optional integration tests to catch race conditions and fault tolerance defects. 

We can introduce a profile with id: it (short for integration tests).  This will launch docker compose via maven-exec-plugin and run blockade to simulate container failures and timeout.

Usage command:
{code}
mvn clean verify -Pit
{code}",pull-request-available,[],HDDS,Test,Major,2019-04-23 20:58:43,109
13229680,"Stop the datanode, when any datanode statemachine state is set to shutdown","Recently we have seen an issue, in InitDatanodeState, there is error during create Path for volume. We set the state to shutdown and this has caused DatanodeStateMachine to stop, but datanode is still running. In this case we should stop Datanode, otherwise, user will know about this when running ozone commands or when user observed metrics like healthy nodes.

 

cc [~vivekratnavel]",pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-23 17:51:01,13
13229660,Inconsistent naming convention with Ozone Kerberos configuration,"In SetupSecureOzone.md, the naming convention for keytab files are different from code.

{code}
hdds.scm.http.kerberos.keytab
ozone.om.http.kerberos.keytab
{code}

In ozone-default.xml, it is looking for:

{code}
hdds.scm.http.kerberos.keytab
ozone.om.http.kerberos.keytab.file
{code}

For the non http version of keytab, they are branded as:

{code}
hdds.scm.kerberos.keytab.file
ozone.om.kerberos.keytab.file
{code}

It is best to shorten the name to remove .file suffix from the code to be consistent with Hadoop naming convention.  The second nitpick is hdds and ozone prefix.  Is there a good reason to have distinct prefix for both that work closely together?  How about hadoop.ozone prefix?  From usability point of view, the current prefix are very confusing.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-23 16:21:56,28
13229546,GC other system pause events can trigger pipeline destroy for all the nodes in the cluster,In a MiniOzoneChaosCluster run it was observed that events like GC pauses or any other pauses in SCM can mark all the datanodes as stale in SCM. This will trigger multiple pipeline destroy and will render the system unusable. ,MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-23 09:00:16,88
13229505,Fix unit test TestConfigurationFields broken on trunk,"Unit test failure::

{code}
[INFO] Running org.apache.hadoop.ozone.TestOzoneConfigurationFields
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.772 s <<< FAILURE! - in org.apache.hadoop.ozone.TestOzoneConfigurationFields
[ERROR] testCompareConfigurationClassAgainstXml(org.apache.hadoop.ozone.TestOzoneConfigurationFields)  Time elapsed: 0.052 s  <<< FAILURE!
java.lang.AssertionError: class org.apache.hadoop.ozone.OzoneConfigKeys class org.apache.hadoop.hdds.scm.ScmConfigKeys class org.apache.hadoop.ozone.om.OMConfigKeys class org.apache.hadoop.hdds.HddsConfigKeys class org.apache.hadoop.ozone.recon.ReconServerConfigKeys class org.apache.hadoop.ozone.s3.S3GatewayConfigKeys has 1 variables missing in ozone-default.xml Entries:   ozone.scm.network.topology.schema.file.type expected:<0> but was:<1>
	at org.junit.Assert.fail(Assert.java:88)
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-23 04:27:33,86
13229410,SCMBlockManager findPipeline and createPipeline are not lock protected,"SCM BlockManager may try to allocate pipelines in the cases when it is not needed. This happens because BlockManagerImpl#allocateBlock is not lock protected, so multiple pipelines can be allocated from it. One of the pipeline allocation can fail even when one of the existing pipeline already exists.


{code}
2019-04-22 22:34:14,336 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 6f4bb2d7-d660-4f9f-bc06-72b10f9a738e, Nodes: 76e1a493-fd55-4d67-9f5
5-c04fd6bd3a33{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}2b9850b2-aed3-4a40-91b5-2447dc5246bf{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}12248721-ea6a-453f-8dad-fc7fbe692f
d2{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,386 INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - e17b7852-4691-40c7-8791-ad0b0da5201f: shutdown LeaderElection
2019-04-22 22:34:14,388 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 552e28f3-98d9-41f3-86e0-c1b9494838a5, Nodes: e17b7852-4691-40c7-879
1-ad0b0da5201f{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}fd365bac-e26e-4b11-afd8-9d08cd1b0521{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}9583a007-7f02-4074-9e26-19bc18e29e
c5{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,388 INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e17b7852-4691-40c7-8791-ad0b0da5201f: start FollowerState
2019-04-22 22:34:14,388 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 5383151b-d625-4362-a7dd-c0d353acaf76, Nodes: 80f16ad6-3879-4a64-a3c
7-7719813cc139{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}082ce481-7fb0-4f88-ac21-82609290a6a2{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}dd5f5a70-0217-4577-b7a2-c42aa139d1
8a{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,389 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: be4854e5-7933-4caa-b32e-f482cf500247, Nodes: 6e2356f1-479d-498b-876
a-1c90623c498b{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}8ac46d94-9975-4eea-9448-2618c69d7bf3{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}a3ed36a1-44ca-47b2-b9b3-5aeef04595
18{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,390 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 21e368e2-f82a-4c61-9cc3-06e8de22ea6b, Nodes: 82632040-5754-4122-b187-331879586842{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}923c8537-b869-4085-adcb-0a9accdcd089{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}c6d790bf-e3a6-4064-acb5-f74796cd38a9{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,390 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: cccbc2ed-e0e2-4578-a8a2-94f4b645be52, Nodes: 91ae6848-a778-43be-a4a1-5855f7adc0d8{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}8f330a03-40e2-4bd1-9b43-5e05b13d89f0{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}4f3070dc-650b-48d7-87b5-d2076104e7b4{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-04-22 22:34:14,392 ERROR block.BlockManagerImpl (BlockManagerImpl.java:allocateBlock(192)) - Pipeline creation failed for type:RATIS factor:THREE
org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 2 nodes 20 healthy nodes 20 all nodes.
        at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:122)
        at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:57)
        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:148)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:190)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:7533)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
2019-04-22 22:34:14,395 ERROR block.BlockManagerImpl (BlockManagerImpl.java:allocateBlock(213)) - Unable to allocate a block for the size: 16384, type: RATIS, factor: THREE
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-22 17:24:48,30
13229398,Fix nightly run failures after HDDS-976,[https://ci.anzix.net/job/ozone-nightly/72/testReport/],pull-request-available,[],HDDS,Bug,Minor,2019-04-22 16:11:23,28
13229395,JVM Exit in datanode while committing a key,"Saw the following trace in MiniOzoneChaosCluster run.

{code}
C  [librocksdbjni17271331491728127.jnilib+0x9755c]  Java_org_rocksdb_RocksDB_write0+0x1c
J 13917  org.rocksdb.RocksDB.write0(JJJ)V (0 bytes) @ 0x00000001102ff62e [0x00000001102ff580+0xae]
J 17167 C2 org.apache.hadoop.utils.RocksDBStore.writeBatch(Lorg/apache/hadoop/utils/BatchOperation;)V (260 bytes) @ 0x0000000111bbd01c [0x0000000111bbcde0+0x23c]
J 20434 C1 org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(Lorg/apache/hadoop/ozone/container/common/interfaces/Container;Lorg/apache/hadoop/ozone/container/common/helpers/BlockData;)J (261 bytes) @ 0x0000000111c267ac [0x0000000111c25640+0x116c]
J 19262 C2 org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(Lorg/apache/hadoop/hdds/protocol/datanode/proto/ContainerProtos$ContainerCommandRequestProto;Lorg/apache/hadoop/ozone/container/common/transport/server/ratis/DispatcherContext;)Lorg/apache/hadoop/hdds/protocol/datanode/proto/ContainerProtos$ContainerCommandResponseProto; (866 bytes) @ 0x00000001125c5aa0 [0x00000001125c1560+0x4540]
J 15095 C2 org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(Lorg/apache/hadoop/hdds/protocol/datanode/proto/ContainerProtos$ContainerCommandRequestProto;Lorg/apache/hadoop/ozone/container/common/transport/server/ratis/DispatcherContext;)Lorg/apache/hadoop/hdds/protocol/datanode/proto/ContainerProtos$ContainerCommandResponseProto; (142 bytes) @ 0x0000000110ffc940 [0x0000000110ffc0c0+0x880]
J 19301 C2 org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(Lorg/apache/hadoop/hdds/protocol/datanode/proto/ContainerProtos$ContainerCommandRequestProto;Lorg/apache/hadoop/ozone/container/common/transport/server/ratis/DispatcherContext;)Lorg/apache/hadoop/hdds/protocol/datanode/proto/ContainerProtos$ContainerCommandResponseProto; (146 bytes) @ 0x0000000111396144 [0x0000000111395e60+0x2e4]
J 15997 C2 org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine$$Lambda$776.get()Ljava/lang/Object; (16 bytes) @ 0x0000000110138e54 [0x0000000110138d80+0xd4]
J 15970 C2 java.util.concurrent.CompletableFuture$AsyncSupply.run()V (61 bytes) @ 0x000000010fc80094 [0x000000010fc80000+0x94]
J 17368 C2 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x0000000110b0a7a0 [0x0000000110b0a5a0+0x200]
J 7389 C1 java.util.concurrent.ThreadPoolExecutor$Worker.run()V (9 bytes) @ 0x000000011012a004 [0x0000000110129f00+0x104]
J 6837 C1 java.lang.Thread.run()V (17 bytes) @ 0x000000011002b144 [0x000000011002b000+0x144]
v  ~StubRoutines::call_stub
V  [libjvm.dylib+0x2ef1f6]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x6ae
V  [libjvm.dylib+0x2ef99a]  JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x164
V  [libjvm.dylib+0x2efb46]  JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x4a
V  [libjvm.dylib+0x34a46d]  thread_entry(JavaThread*, Thread*)+0x7c
V  [libjvm.dylib+0x56eb0f]  JavaThread::thread_main_inner()+0x9b
V  [libjvm.dylib+0x57020a]  JavaThread::run()+0x1c2
V  [libjvm.dylib+0x48d4a6]  java_start(Thread*)+0xf6
C  [libsystem_pthread.dylib+0x3305]  _pthread_body+0x7e
C  [libsystem_pthread.dylib+0x626f]  _pthread_start+0x46
C  [libsystem_pthread.dylib+0x2415]  thread_start+0xd
C  0x0000000000000000
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-22 16:07:35,18
13229384,RatisPipelineProvider should only consider open pipeline while excluding dn for pipeline allocation,"While allocation pipelines, Ratis pipeline provider considers all the pipelines irrespective of the state of the pipeline. This can lead to case where all the datanodes are up but the pipelines are in closing state in SCM.",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-22 14:20:33,30
13228497,Add handling of NotReplicatedException in OzoneClient,"In MiniOzoneChaosCluster some of the calls fail with NotReplicatedException. This Exception needs to be handled in OzoneClient

{code}
2019-04-17 10:13:47,254 INFO  client.GrpcClientProtocolService (GrpcClientProtocolService.java:lambda$processClientRequest$0(264)) - Failed RaftClientRequest:client-43B95E0E3BE0->1ebec547-8cf8-4466-bf43-ea9f19fb546b@group-1B28E0BF6CBC, cid=800, seq=0, Watch-ALL_COMMITTED(234), Message:<EMPTY>, reply=RaftClientReply:client-43B95E0E3BE0->1ebec547-8cf8-4466-bf43-ea9f19fb546b@group-1B28E0BF6CBC, cid=800, FAILED org.apache.ratis.protocol.NotReplicatedException: Request with call Id 800 and log index 234 is not yet replicated to ALL_COMMITTED, logIndex=234, commits[1ebec547-8cf8-4466-bf43-ea9f19fb546b:c267, 7b200ef5-7711-437d-a9bc-ad0e18fdf6bb:c267, ffbfb65f-a622-466d-b6e8-47038cc15e0b:c226]
{code}",MiniOzoneChaosCluster,[],HDDS,Bug,Major,2019-04-17 05:00:39,16
13228488,add spark container to ozonesecure-mr compose files,add spark container to ozonesecure-mr compose files,pull-request-available,[],HDDS,New Feature,Major,2019-04-17 04:27:44,110
13228233,Remove usage of getRetryFailureException,"Per [~szetszwo]'s comment on RATIS-518, we can remove the usage of getRetryFailureException.",pull-request-available,[],HDDS,Improvement,Major,2019-04-16 02:31:44,86
13227855,TestCommitWatcher#testReleaseBuffersOnException fails with IllegalStateException,"the test is failing with the following exception

{code}
java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
	at org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:191)
	at org.apache.hadoop.ozone.client.rpc.TestCommitWatcher.testReleaseBuffersOnException(TestCommitWatcher.java:277)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}


https://ci.anzix.net/job/ozone-nightly/63/testReport/org.apache.hadoop.ozone.client.rpc/TestCommitWatcher/testReleaseBuffersOnException/
",ozone-flaky-test,['Ozone Client'],HDDS,Bug,Major,2019-04-13 08:51:20,16
13227662,TestDatanodeStateMachine is flaky,"TestDatanodeStateMachine is flaky.
 It has failed in the following build
 [https://builds.apache.org/job/PreCommit-HDDS-Build/2650/artifact/out/patch-unit-hadoop-hdds.txt]
 [https://builds.apache.org/job/hadoop-multibranch/job/PR-661/6/artifact/out/patch-unit-hadoop-hdds_container-service.txt]
 [https://builds.apache.org/job/PreCommit-HDDS-Build/2635/artifact/out/patch-unit-hadoop-hdds.txt]

Stack trace:
{noformat}
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)
	at org.apache.hadoop.ozone.container.common.TestDatanodeStateMachine.testStartStopDatanodeStateMachine(TestDatanodeStateMachine.java:166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   TestDatanodeStateMachine.testStartStopDatanodeStateMachine:166 ? Timeout Timed...
{noformat}",ozone-flaky-test pull-request-available,['test'],HDDS,Bug,Major,2019-04-12 09:11:00,1
13227657,Rename GetScmInfoRespsonseProto to GetScmInfoResponseProto due to typos,"We got a typo in hdds.proto file
- {{GetScmInfoRespsonseProto}}",newbie,[],HDDS,Bug,Major,2019-04-12 08:47:10,95
13227621,Ozone client list command truncates response without any indication,"Trying to list all keys in a bucket gives truncated results unless the limit override is provided as such:
{code}
bin/ozone --config /etc/ozone/conf sh key list -l 5000 /vol-test-1/bucket1/
{code}

The proposal is to provide a warning about possible truncation of results and the override option.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-04-12 06:27:31,86
13227580,NPE if secure ozone if KMS uri is not defined.,OzoneKMSUtil.getKeyProvider throws NPE if KMS uri is not defined. ,pull-request-available,[],HDDS,Sub-task,Major,2019-04-11 23:49:56,110
13227574,Remove benign warning in handleCreateContainer,"The following log message in KeyValueHandler#handleCreateContainer can be removed or moved to _debug_ level.

{code}
        // The create container request for an already existing container can
        // arrive in case the ContainerStateMachine reapplies the transaction
        // on datanode restart. Just log a warning msg here.
        LOG.warn(""Container already exists."" +
            ""container Id "" + containerID);
{code}",newbie,[],HDDS,Improvement,Major,2019-04-11 22:49:20,86
13227564,Minor logging improvements for MiniOzoneChaosCluster,"- Add log messages when starting/stopping services
- Change log file name so output files are sorted by date/time. Also {{:}} is not a valid file character on some platforms.",pull-request-available,[],HDDS,Improvement,Major,2019-04-11 21:41:41,113
13227467,Ozone compose files are not compatible with the latest docker-compose,"I upgraded my docker-compose to the latest available one (1.24.0)

But after the upgrade I can't start the docker-compose based cluster any more:

{code}
./test.sh 
-------------------------------------------------
Executing test(s): [basic]

  Cluster type:      ozone
  Compose file:      /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/smoketest/../compose/ozone/docker-compose.yaml
  Output dir:        /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/smoketest/result
  Command to rerun:  ./test.sh --keep --env ozone basic
-------------------------------------------------
ERROR: In file /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozone/docker-config: environment variable name 'LOG4J2.PROPERTIES_appender.rolling.file 
{code}

It turned out that the line of LOG4J2.PROPERTIES_appender.rolling.file contains an unnecessary space which is not accepted by the latest docker-compose any more.",pull-request-available,[],HDDS,Bug,Blocker,2019-04-11 14:05:16,6
13227465,Support multi-container robot test execution,"The ./smoketest folder in the distribution package contains robotframework based test scripts to test the main behaviour of Ozone.

The tests have two layers:

1. robot test definitions to execute commands and assert the results (on a given host machine)
2. ./smoketest/test.sh which starts/stops the docker-compose based environments AND execute the selected robot tests inside the right hosts

The second one (test.sh) has some serious limitations:

1. all the tests are executed inside the same container (om):

https://github.com/apache/hadoop/blob/5f951ea2e39ae4dfe554942baeec05849cd7d3c2/hadoop-ozone/dist/src/main/smoketest/test.sh#L89

Some of the tests (ozonesecure-mr, ozonefs) may require the flexibility to execute different robot tests in different containers.

2. The definition of the global test set is complex and hard to understood. 

The current code is:
{code}
   TESTS=(""basic"")
   execute_tests ozone ""${TESTS[@]}""
   TESTS=(""auditparser"")
   execute_tests ozone ""${TESTS[@]}""
   TESTS=(""ozonefs"")
   execute_tests ozonefs ""${TESTS[@]}""
   TESTS=(""basic"")
   execute_tests ozone-hdfs ""${TESTS[@]}""
   TESTS=(""s3"")
   execute_tests ozones3 ""${TESTS[@]}""
   TESTS=(""security"")
   execute_tests ozonesecure .
{code} 

For example for ozonesecure the TESTS is not used. And the usage of bash lists require additional complexity in the execute_tests function.

I propose here a very lightweight refactor. Instead of including both the test definitions AND the helper methods in test.sh I would separate them.

Let's put a test.sh to each of the compose directories. The separated test.sh can include common methods from a main shell script. For example:

{code}

source ""$COMPOSE_DIR/../testlib.sh""

start_docker_env

execute_robot_test scm basic/basic.robot

execute_robot_test scm s3

stop_docker_env

generate_report

{code}

This is a more clean and more flexible definition. It's easy to execute just this test (as it's saved to the compose/ozones3 directory. And it's more flexible.

Other example, where multiple containers are used to execute tests:

{code}

source ""$COMPOSE_DIR/../testlib.sh""

start_docker_env

execute_robot_test scm ozonefs/ozonefs.robot



export OZONE_HOME=/opt/ozone

execute_robot_test hadoop32 ozonefs/hadoopo3fs.robot

execute_robot_test hadoop31 ozonefs/hadoopo3fs.robot

stop_docker_env

generate_report
{code}

With this separation the definition of the helper methods (eg. execute_robot_test or stop_docker_env) would also be simplified.",pull-request-available,['test'],HDDS,Bug,Major,2019-04-11 13:50:14,6
13227365,Exception during DataNode shutdown,"The following exception during DN shutdown should be avoided, as it adds noise to the logs and is not a real issue.
{code}
2019-04-10 17:48:27,307 WARN  volume.VolumeSet (VolumeSet.java:getNodeReport(476)) - Failed to get scmUsed and remaining for container storage location /Users/agarwal/src/hadoop/hadoop-ozone/integration-test/target/test/data/MiniOzoneClusterImpl-f4d89966-146a-4690-8841-36af1993522f/datanode-17/data/containers
java.io.IOException: Volume Usage thread is not running. This error is usually seen during DataNode shutdown.
  at org.apache.hadoop.ozone.container.common.volume.VolumeInfo.getScmUsed(VolumeInfo.java:119)
  at org.apache.hadoop.ozone.container.common.volume.VolumeSet.getNodeReport(VolumeSet.java:472)
  at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.getNodeReport(OzoneContainer.java:238)
  at org.apache.hadoop.ozone.container.common.states.endpoint.RegisterEndpointTask.call(RegisterEndpointTask.java:115)
  at org.apache.hadoop.ozone.container.common.states.endpoint.RegisterEndpointTask.call(RegisterEndpointTask.java:47)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,[],HDDS,Bug,Major,2019-04-11 01:05:00,113
13227353,Avoid unnecessary object allocations in TracingUtil,Avoid unnecessary object allocations in TracingUtil#exportCurrentSpan and #exportSpan.,pull-request-available,['tracing'],HDDS,Bug,Major,2019-04-10 23:34:47,113
13227350,Tracing exception in DataNode HddsDispatcher,"The following exception is seen in some unit tests:
{code}
2019-04-10 13:00:27,537 WARN  internal.PropagationRegistry$ExceptionCatchingExtractorDecorator (PropagationRegistry.java:extract(60)) - Error when extracting SpanContext from carrier. Handling gracefully.
io.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 90041ce6-81f3-4733-8e2b-6aceaa697b77
    at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:49)
    at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:34)
    at io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)
    at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)
    at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)
    at io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)
    at org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:98)
    at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
    at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347)
    at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354)
    at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction$5(ContainerStateMachine.java:613)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
{code}",pull-request-available,"['Ozone Datanode', 'tracing']",HDDS,Bug,Major,2019-04-10 23:12:41,113
13227308,Fix shellcheck errors in start-chaos.sh,"Fix the following shellcheck errors in start-chaos.sh:
{code}
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:18:6: note: Use $(..) instead of legacy `..`. [SC2006]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:27:19: note: Double quote to prevent globbing and word splitting. [SC2086]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:28:20: note: Double quote to prevent globbing and word splitting. [SC2086]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:31:33: note: Double quote to prevent globbing and word splitting. [SC2086]
hadoop-ozone/integration-test/src/test/bin/start-chaos.sh:35:23: note: Double quote to prevent globbing and word splitting. [SC2086]
{code}",newbie pull-request-available,['test'],HDDS,Bug,Major,2019-04-10 19:58:46,1
13227296,Move bang line to the start of the start-chaos.sh script,The start-chaos.sh script has a bang line but it is not the first line in the script.,pull-request-available,[],HDDS,Bug,Major,2019-04-10 19:04:37,113
13227281,"After successfully importing a container, datanode should delete the container tar.gz file from working directory","Whenever we want to replicate or copy a container from one datanode to another, we compress the container data and create a tar.gz file. This tar file is then copied from source datanode to destination datanode. In destination, we use a temporary working directory where this tar file is copied. Once the copying is complete we import the container. After importing the container we no longer need the tar file in the working directory of destination datanode, this has to be deleted.",pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-04-10 18:21:12,19
13227270,MiniOzoneCluster should set custom value for hdds.datanode.replication.work.dir,"Datanode uses a temporary working directory for copying/replicating containers, the default location of this directory is read from system property {{java.io.tmpdir}}. Since all the datanodes are running in same machine/jvm in MiniOzoneCluster we might corrupt the data while the datanodes are moving containers as all the datanodes will be using the same working directory.

While configuring datanode for MiniOzoneCluster, we should set custom value for {{hdds.datanode.replication.work.dir}} in each datanode instance.
",newbie,['test'],HDDS,Test,Major,2019-04-10 17:49:27,114
13227261,TestOzoneClientRetriesOnException.testGroupMismatchExceptionHandling is flaky,[https://ci.anzix.net/job/ozone/16618/testReport/org.apache.hadoop.ozone.client.rpc/TestOzoneClientRetriesOnException/testGroupMismatchExceptionHandling/],TriagePending ozone-flaky-test,['test'],HDDS,Bug,Major,2019-04-10 17:23:23,16
13227230,TestOzoneManagerHA.testMultipartUploadWithOneOmNodeDown is flaky,"TestOzoneManagerHA.testMultipartUploadWithOneOmNodeDown is flaky, we get the below exception when it fails.

{code}
org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client client-04649B8D5AF3->RAFT is closed.
 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommand(OzoneManagerRatisClient.java:133)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:83)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
Caused by: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client client-04649B8D5AF3->RAFT is closed.
 at org.apache.ratis.util.SlidingWindow$Client.alreadyClosed(SlidingWindow.java:350)
 at org.apache.ratis.util.SlidingWindow$Client.submitNewRequest(SlidingWindow.java:224)
 at org.apache.ratis.client.impl.RaftClientImpl.sendAsync(RaftClientImpl.java:207)
 at org.apache.ratis.client.impl.RaftClientImpl.sendAsync(RaftClientImpl.java:174)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendRequestAsync(OzoneManagerRatisClient.java:208)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommandAsync(OzoneManagerRatisClient.java:168)
 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommand(OzoneManagerRatisClient.java:132)
 ... 11 more
Caused by: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-04649B8D5AF3->omNode-1@group-523986131536, cid=71, seq=1*, RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$396/1529424209@7ae5da75 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms)
 at org.apache.ratis.client.impl.RaftClientImpl.newRaftRetryFailureException(RaftClientImpl.java:383)
 at org.apache.ratis.client.impl.RaftClientImpl.handleAsyncRetryFailure(RaftClientImpl.java:388)
 at org.apache.ratis.client.impl.RaftClientImpl.lambda$sendRequestAsync$14(RaftClientImpl.java:370)
 at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
 at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.completeReplyExceptionally(GrpcClientProtocolClient.java:329)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.access$000(GrpcClientProtocolClient.java:245)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:257)
 at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:248)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
 at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)
 at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
 at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
{code}",TriagePending ozone-flaky-test,['test'],HDDS,Bug,Major,2019-04-10 16:27:19,43
13227228,TestCloseContainerCommandHandler is flaky,"TestCloseContainerCommandHandler.testCloseContainerViaStandalone is flaky, we get the below exception when it fails.

{code}
org.apache.ratis.protocol.NotLeaderException: Server a200dff7-f26d-4be3-addd-e8e0ca569ae0 is not the leader (null). Request must be sent to leader.
	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:448)
	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:419)
	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:514)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$7(RaftServerProxy.java:333)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$5(RaftServerProxy.java:328)
	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:109)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$6(RaftServerProxy.java:328)
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)
	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:327)
	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:333)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.submitRequest(XceiverServerRatis.java:485)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.TestCloseContainerCommandHandler.createContainer(TestCloseContainerCommandHandler.java:310)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.TestCloseContainerCommandHandler.testCloseContainerViaStandalone(TestCloseContainerCommandHandler.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

Full log is uploaded.",ozone-flaky-test pull-request-available,[],HDDS,Bug,Major,2019-04-10 16:22:23,1
13227222,Provide example k8s deployment files as part of the release package,"In HDDS-872 we added Dockerfile and skaffold definition to run dev builds on kubernetes. But it would be great to include example k8s resource definitions helping the deployment of ozone to any kubernetes cluster.

In this patch I will

1. Add k8s resources files to the release tar file to deploy basic ozone cluster
2. Add Dockerfile to the release tar file to create custom ozone image any time
3. Add additional maven profiles to build and push development docker images.
4. We don't need skaffold any more as the maven based approach is more flexible (we can support multiple k8s definitions)

To easily support multiple type of configuration (simple ozone, minikube, csi) we need a basic set of k8s resources files and additional transformations to generate the ready-to-use files for each specific use-cases.

The easiest way to do this is adopting the existing structure from https://github.com/flokkr/k8s and use https://github.com/elek/flekszible tool. But the tool itself is not required at runtime as we generate all the required k8s resources files during the development and add the results to the version control. ",pull-request-available,[],HDDS,Sub-task,Major,2019-04-10 16:04:40,6
13227175,Add unit test to check if SCM correctly sends close commands for containers in closing state after a restart,"When the container is in CLOSING state, SCM keeps sending close command to the datanode until the container is either moved to QUASI_CLOSED or CLOSED state. The frequency in which the close command is sent by SCM depends on the property {{hdds.scm.replication.thread.interval}}. 

We have to add a test case to verify whether SCM is sending close commands for containers in the closing state even after a restart.",newbie pull-request-available,['test'],HDDS,Test,Major,2019-04-10 12:50:53,86
13227168,TestSCMNodeMetrics is flaky,"TestSCMNodeMetrics is flaky
https://ci.anzix.net/job/ozone/16617/testReport/org.apache.hadoop.ozone.scm.node/TestSCMNodeMetrics/testNodeReportProcessing/
{noformat}

java.lang.AssertionError: Bad value for metric NumNodeReportProcessed expected:<2> but was:<1>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.apache.hadoop.test.MetricsAsserts.assertCounter(MetricsAsserts.java:227)
	at org.apache.hadoop.ozone.scm.node.TestSCMNodeMetrics.testNodeReportProcessing(TestSCMNodeMetrics.java:107)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",ozone-flaky-test,['test'],HDDS,Test,Major,2019-04-10 12:22:39,86
13227028,Avoid usage of commonPool in RatisPipelineUtils,"We use parallelStream in during createPipline, this internally uses commonPool. Use Our own ForkJoinPool with parallelisim set with number of processors.",pull-request-available,[],HDDS,Improvement,Major,2019-04-09 19:38:00,13
13226886,ITestOzoneContractCreate is failing,ITestOzoneContractCreate and ITestOzoneContractMkdir are failing with FileAlreadyExistsException. The issue is around the file imported in BasicOzoneClientAdapterImpl. The class needs to import org.apache.hadoop.fs.FileAlreadyExistsException but currently imports java.nio.file.FileAlreadyExistsException.,pull-request-available,[],HDDS,Bug,Major,2019-04-09 08:48:05,35
13226851,Fix typos in HDDS,"[https://github.com/apache/hadoop/blob/trunk/hadoop-hdds/common/src/main/proto/DatanodeContainerProtocol.proto#L465]

[https://github.com/apache/hadoop/blob/trunk/hadoop-hdds/common/src/main/proto/StorageContainerLocationProtocol.proto#L37]

 ",newbie,['documentation'],HDDS,Bug,Trivial,2019-04-09 05:28:27,87
13226761,KeyOutputStream writes fails after max retries while writing to a closed container,Currently a Ozone Client retries a write operation 5 times. It is possible that the container being written to is already closed by the time it is written to. The key write will fail after retrying multiple times with this error. This needs to be fixed as this is an internal error.,MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-08 18:22:26,43
13226746,Remove unused ScmBlockLocationProtocol from ObjectStoreHandler,"When I analyzed the usages of the available RPC protocols in Ozone I found that the ScmBlockLocationProtocol is not used in ObjectStore at all.

I would propose to remove it...",pull-request-available,[],HDDS,Improvement,Major,2019-04-08 16:27:50,6
13226558,Static ContainerCache in Datanodes can result in overwrite of container db,"The datanodes use a static Container cache, and this container cache is key'd using the container ID.
In a MiniOzoneCluster environment, this can lead to multiple container on different datanodes to use the same rocksdb leading to inconsistency.
",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Blocker,2019-04-07 10:47:16,18
13226462,Convert all OM Key related operations to HA model,"In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like volume not found or some conditions which  are not satisfied like when Key not found during rename, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-05 22:29:54,13
13226433,Concurrency issue in SCMConnectionManager#getValues,"{{testStartStopDatanodeStateMachine}} is flaky, causing [occasional pre-commit build failures|https://builds.apache.org/job/hadoop-multibranch/job/PR-691/1/artifact/out/patch-unit-hadoop-hdds_container-service.txt].  HDDS-1332 added some logging to find out more about the cause.

I think the problem is not test-specific, and is caused by the following: {{SCMConnectionManager#scmMachines}} is a plain {{HashMap}}, guarded by a {{ReadWriteLock}} in most places where it's used, except {{getValues()}}.  The method also returns the values collection without any write protection (though currently none of the callers modify it).",pull-request-available,[],HDDS,Bug,Major,2019-04-05 20:16:47,1
13226328,Avoid the usage of signal handlers in datanodes of the MiniOzoneClusters,"[~arpaga] showed me a problem that TestQueryNode.testHealthyNodesCount is failed in the CI check of HDDS-1339.

According to the logs the test is timed out because only 4 datanodes are started out of the 5.

The log also contained an exception from one datanode:

{code}
2019-04-04 00:26:33,583 WARN  ozone.HddsDatanodeService (LogAdapter.java:warn(59)) - failed to register any UNIX signal loggers: 
java.lang.IllegalStateException: Can't re-install the signal handlers.
    at org.apache.hadoop.util.SignalLogger.register(SignalLogger.java:77)
    at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:718)
    at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:707)
    at org.apache.hadoop.ozone.HddsDatanodeService.createHddsDatanodeService(HddsDatanodeService.java:126)
    at org.apache.hadoop.ozone.HddsDatanodeService.createHddsDatanodeService(HddsDatanodeService.java:108)
    at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.createHddsDatanodes(MiniOzoneClusterImpl.java:552)
{code}

The code which requires the signal handler is the following (signal handler is registered in the startupShutdownMessage)

{code}
  /**
   * Create an Datanode instance based on the supplied command-line arguments.
   * <p>
   * This method is intended for unit tests only. It suppresses the
   * startup/shutdown message and skips registering Unix signal handlers.
   *
   * @param args        command line arguments.
   * @param conf        HDDS configuration
   * @param printBanner if true, then log a verbose startup message.
   * @return Datanode instance
   */
  private static HddsDatanodeService createHddsDatanodeService(
      String[] args, Configuration conf, boolean printBanner) {
    if (args.length == 0 && printBanner) {
      StringUtils
          .startupShutdownMessage(HddsDatanodeService.class, args, LOG);
      return new HddsDatanodeService(conf);
    } else {
      new HddsDatanodeService().run(args);
      return null;
   }
{code}

As you can read from the comment it's expected to be called with printBanner=false to avoid the creation of the signal handler. 

Note: In the startupShutdownMessage method a new signal handler is registered and signal handlers can be registered only once:

{code}
//SignalLogger
  void register(final LogAdapter LOG) {
   if (registered) {
      throw new IllegalStateException(""Can't re-install the signal handlers."");
    }
 ....
{code}

We have a dedicated method to create datanode service for the unit tests. The only thing what we need is to turn OFF the signal handler registration here. (The following code fragment shows the original state where the signal handler creation is requested with the true parameter value)

{code}
  @VisibleForTesting
  public static HddsDatanodeService createHddsDatanodeService(
      String[] args, Configuration conf) {
    return createHddsDatanodeService(args, conf, true);
  }
{code}",pull-request-available,[],HDDS,Bug,Major,2019-04-05 09:13:41,6
13226312,Recon start fails due to changes in Aggregate Schema definition (HDDS-1189).,Recon Server start fails due to ClassNotFound exception when looking for org.apache.hadoop.ozone.recon.ReconServer. ,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-04-05 07:34:42,30
13226304,Key write fails with BlockOutputStream has been closed exception,"Key write fails with BlockOutputStream has been closed

{code}
2019-04-05 11:24:47,770 ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(102)) - LOADGEN: Create key:pool-431-thread-9-2092651262 failed with exception, but skipping
java.io.IOException: BlockOutputStream has been closed.
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.checkOpen(BlockOutputStream.java:662)
        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:245)
        at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:131)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:325)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:287)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
        at java.io.OutputStream.write(OutputStream.java:75)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:100)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:143)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-04-05 06:15:51,16
13226268,Convert all OM Bucket related operations to HA model,"In this jira, we shall convert all OM Bucket related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like bucket not found or some conditions which i have not satisfied like when deleting bucket should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-04 21:53:59,13
13226212,Add ability in OM to serve delta updates through an API.,"Added an RPC end point to serve the set of updates in OM RocksDB from a given sequence number.
This will be used by Recon (HDDS-1105) to push the data to all the tasks that will keep their aggregate data up to date. ",pull-request-available,['Ozone Manager'],HDDS,Sub-task,Major,2019-04-04 18:06:46,30
13226206,Fix OzoneS3 Gateway server due to exclusion of hk2-api,Some hk2 transitive dependencies were mistakenly excluded in HDDS-1358 to solve maven enforcer plugin issues. This jira cleans that up. ,pull-request-available,['S3'],HDDS,Bug,Major,2019-04-04 17:43:06,30
13226205,Fix testSCMChillModeRestrictedOp,"This test failure is caused by HDDS-1207, as we use the same ChillModeHandler thread for waitTime sleep. Because of that, this is causing a test failure.

 

 ",pull-request-available,[],HDDS,Bug,Major,2019-04-04 17:41:09,19
13226203,Add a shell script to run MiniOzoneChaosCluster using mvn exec,This jira adds a shell script to run MiniOzoneChaosCluster,pull-request-available,['test'],HDDS,Improvement,Major,2019-04-04 17:38:43,18
13226202,ConcurrentModificationException in TestMiniChaosOzoneCluster,"TestMiniChaosOzoneCluster is failing with the below exception
{noformat}
[ERROR] org.apache.hadoop.ozone.TestMiniChaosOzoneCluster  Time elapsed: 265.679 s  <<< ERROR!
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)
	at java.util.ArrayList$Itr.next(ArrayList.java:859)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.stop(MiniOzoneClusterImpl.java:350)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:325)
	at org.apache.hadoop.ozone.MiniOzoneChaosCluster.shutdown(MiniOzoneChaosCluster.java:130)
	at org.apache.hadoop.ozone.TestMiniChaosOzoneCluster.shutdown(TestMiniChaosOzoneCluster.java:92)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",ozone-flaky-test pull-request-available,['test'],HDDS,Bug,Major,2019-04-04 17:35:32,6
13226144,Make the ozonesecure-mr environment definition version independent,"The MapReduce example project on branch ozone-0.4 contains 0.5.0-SNAPSHOT references in the dir:

hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozonesecure-mr

After HDDS-1333 (which introduce filtering) it will be straightforward to always use the current version.",pull-request-available,[],HDDS,Bug,Major,2019-04-04 14:55:54,6
13226129,TestBlockOutputStreamWithFailures is failing,"TestBlockOutputStreamWithFailures is failing with the following error

{noformat}
2019-04-04 18:52:43,240 INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(140)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@1f6c0e8a
2019-04-04 18:52:43,240 INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(203)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@1f6c0e8a
2019-04-04 18:52:43,241 ERROR server.GrpcService (ExitUtils.java:terminate(133)) - Terminating with exit status 1: Failed to start Grpc server
java.io.IOException: Failed to bind
  at org.apache.ratis.thirdparty.io.grpc.netty.NettyServer.start(NettyServer.java:253)
  at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:166)
  at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:81)
  at org.apache.ratis.grpc.server.GrpcService.startImpl(GrpcService.java:144)
  at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)
  at org.apache.ratis.server.impl.RaftServerRpcWithProxy.start(RaftServerRpcWithProxy.java:69)
  at org.apache.ratis.server.impl.RaftServerProxy.lambda$start$3(RaftServerProxy.java:300)
  at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)
  at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:298)
  at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:419)
  at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:186)
  at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:169)
  at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:338)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.BindException: Address already in use
  at sun.nio.ch.Net.bind0(Native Method)
  at sun.nio.ch.Net.bind(Net.java:433)
  at sun.nio.ch.Net.bind(Net.java:425)
  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
  at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:130)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
  at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1358)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
  at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:1019)
  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
  at org.apache.ratis.thirdparty.io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:366)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)
  at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
  at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
  ... 1 more
{noformat}",pull-request-available,['test'],HDDS,Bug,Major,2019-04-04 13:41:26,6
13226118,[Ozone Upgrade] Create the project skeleton with CLI interface,"Ozone In-Place upgrade tool is a tool to upgrade hdfs data to ozone data without data movement.

In this jira I will create a skeleton project with the cli interface without any business logic.",pull-request-available,['upgrade'],HDDS,Sub-task,Major,2019-04-04 13:13:09,6
13226110,Create customized CSI server for Ozone,"CSI (Container Storage Interface) is a vendor neutral storage interface specification for container orchestrators. CSI support is implemented in YARN, Kubernetes and Mesos. Implementing a CSI server makes it easy to mount disk volumes for containers.

See https://github.com/container-storage-interface/spec for more details about the spec.

Until now we used https://github.com/CTrox/csi-s3 server to support CSI specification. Using an ozone specific CSI server would have the following advantages:

 * We can provide additional functionalities (as we have access to the internal Ozone API not just the very generic s3 api).
 * Security setup can be synchronized.
 * Increased stability
 * Simplified deployment (only the minimal set of the components are required to be installed)

The CSI specification itself is very simple (https://github.com/container-storage-interface/spec/blob/master/csi.proto) at least the part which is required for Ozone.

We can use various fuse s3 driver to mount the ozone buckets via s3.",pull-request-available,[],HDDS,Sub-task,Major,2019-04-04 12:39:45,6
13225883,Convert all OM Volume related operations to HA model,"In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like volume not found or some conditions which i have not satisfied like when deleting volume should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-03 18:26:48,13
13225852,OM failed to start with incorrect hostname set as ip address in CSR,"OM failed to start after HDDS-1355.

{code}

om_1 | 2019-04-03 16:23:50 ERROR OzoneManager:865 - Failed to start the OzoneManager.
om_1 | java.lang.IllegalArgumentException: IP Address is invalid
om_1 | at org.bouncycastle.asn1.x509.GeneralName.<init>(Unknown Source)
om_1 | at org.apache.hadoop.hdds.security.x509.certificates.utils.CertificateSignRequest$Builder.addAltName(CertificateSignRequest.java:205)
om_1 | at org.apache.hadoop.hdds.security.x509.certificates.utils.CertificateSignRequest$Builder.addIpAddress(CertificateSignRequest.java:197)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.getSCMSignedCert(OzoneManager.java:1387)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1018)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:971)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:928)
om_1 | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)

{code}",pull-request-available,[],HDDS,Bug,Blocker,2019-04-03 16:31:32,28
13225784,Datanode exits while executing client command when scmId is null,"Ozone Datanode exits with the following error, this happens because DN hasn't received a scmID from the SCM after registration but is processing a client command.

{code}
2019-04-03 17:02:10,958 ERROR storage.RaftLogWorker (ExitUtils.java:terminate(133)) - Terminating with exit status 1: df6b578e-8d35-44f5-9b21-db7184dcc54e-RaftLogWorker failed.
java.io.IOException: java.lang.NullPointerException: scmId cannot be null
        at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
        at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
        at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:83)
        at org.apache.ratis.server.storage.RaftLogWorker$StateMachineDataPolicy.getFromFuture(RaftLogWorker.java:76)
        at org.apache.ratis.server.storage.RaftLogWorker$WriteLog.execute(RaftLogWorker.java:354)
        at org.apache.ratis.server.storage.RaftLogWorker.run(RaftLogWorker.java:219)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException: scmId cannot be null
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:110)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:243)
        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:165)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:350)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:224)
        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:149)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354)
        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$0(ContainerStateMachine.java:385)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run$$$capture(CompletableFuture.java:1590)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-03 11:36:21,43
13225726,ContainerStateMap cannot find container while allocating blocks.,"ContainerStateMap cannot find container while allocating blocks.

{code}
org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #14
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:542)
        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.getContainerInfo(ContainerStateMap.java:189)
        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.getContainer(ContainerStateManager.java:483)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainer(SCMContainerManager.java:195)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:466)
        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:387)
        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:201)
        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)
        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)
        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:7533)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-03 07:54:31,13
13225623,"KeyOutputStream, close after write request fails after retries, runs into IllegalArgumentException","In this code, the stream is closed via try with resource.

{code}
      try (OzoneOutputStream stream = ozoneBucket.createKey(keyName,
          bufferCapacity, ReplicationType.RATIS, ReplicationFactor.THREE,
          new HashMap<>())) {
        stream.write(buffer.array());
      } catch (Exception e) {
        LOG.error(""LOADGEN: Create key:{} failed with exception"", keyName, e);
        break;
      }
{code}

Here, the write call fails correctly as expected, However the close doesn't fail with the same exception.

The exception stack stack is as following

{code}
2019-04-03 00:52:54,116 ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(101)) - LOADGEN: Create key:pool-431-thread-9-81262222 failed with exception
java.io.IOException: Retry request failed. retries get failed due to exceeded maximum allowed retries number: 5
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:492)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)
        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:287)
        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
        at java.io.OutputStream.write(OutputStream.java:75)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:99)
        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:137)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.lang.IllegalArgumentException
                at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)
                at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:643)
                at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)
                at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:100)
                ... 5 more
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-04-02 20:03:07,16
13225618,getContainerWithPipeline for a standalone pipeline fails with ConcurrentModificationException,"The excception is hit while fetching a pipeline during read.

{code}
2019-04-03 00:52:50,125 WARN  ipc.Server (Server.java:logException(2724)) - IPC Server handler 16 on 59758, call Call#2270 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.getC
ontainerWithPipeline from 192.168.0.108:60011
java.util.ConcurrentModificationException
        at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1558)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getContainerWithPipeline(SCMClientProtocolServer.java:252)
        at org.apache.hadoop.ozone.protocolPB.StorageContainerLocationProtocolServerSideTranslatorPB.getContainerWithPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:144)
        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:16390)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}",MiniOzoneChaosCluster pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-02 19:40:29,19
13225610,Download RocksDB checkpoint from OM Leader to Follower,"If a follower OM is lagging way behind the leader OM or in case of a restart or bootstrapping, a follower OM might need RocksDB checkpoint from the leader to catch up with it. This is because the leader might have purged its logs after taking a snapshot.
 This Jira aims to add support to download a RocksDB checkpoint from leader OM to follower OM through a HTTP servlet. We reuse the DBCheckpoint servlet used by Recon server. ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-04-02 19:00:19,43
13225602,Command Execution in Datanode fails becaue of NPE,"The command execution on the datanode is failing with the following exception.

{code}
2019-04-02 23:56:30,434 ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(196)) - Unable to finish the execution.
java.lang.NullPointerException
        at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:179)
        at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.execute(RunningDatanodeState.java:89)
        at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:354)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:183)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:338)
        at java.lang.Thread.run(Thread.java:748)
{code}",MiniOzoneChaosCluster pull-request-available,['Ozone Datanode'],HDDS,Bug,Major,2019-04-02 18:31:34,13
13225469,Cleanup old ReplicationManager code from SCM,"HDDS-1205 brings in new ReplicationManager and HDDS-1207 plugs in the new code, this jira is for removing the old ReplicationManager and related code.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-04-02 11:22:18,19
13225368,Add ability in Recon to track the number of small files in an Ozone cluster.,Ozone users may want to track the number of small files they have in their cluster and where they are present. Recon can help them with the information by iterating the OM Key Table and dividing the keys into different buckets based on the data size. ,pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-04-01 19:14:01,98
13225234,ozone.metadata.dirs doesn't pick multiple dirs,"{{ozone.metadata.dirs}} doesn't pick comma(,) separated paths.
 It only picks one path as opposed to the property name _ozone.metadata.dir{color:#FF0000}s{color}_
{code:java}
   <property>
      <name>ozone.metadata.dirs</name>
      <value>/data/data1/meta,/home/hdfs/data/meta</value>
   </property>
{code}
{code:java}
2019-03-31 18:44:54,824 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
SCM initialization succeeded.Current cluster id for sd=/data/data1/meta,/home/hdfs/data/meta/scm;cid=CID-1ad502d1-0104-4055-838b-1208ab78f35c
2019-03-31 18:44:55,079 INFO server.StorageContainerManager: SHUTDOWN_MSG:
{code}
{code:java}
[hdfs@localhost ozone-0.5.0-SNAPSHOT]$ ls //data/data1/meta,/home/hdfs/data/meta/scm/current/VERSION
VERSION
{code}",pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-01 10:26:44,1
13225219,Invalid metric type due to fully qualified class name,"Prometheus reports the following error ({{ozoneperf}} compose dir):

{code}
prometheus_1 | ... msg=""append failed"" err=""invalid metric type \""apache.hadoop.hdds.scm.server._scm_container_metrics_deleted_containers gauge\""""
{code}

Metric name cannot contain dots.",pull-request-available,['SCM'],HDDS,Bug,Major,2019-04-01 09:40:28,1
13224878,Recon Server REST API not working as expected.,Guice Jetty integration that is being used for Recon Server API layer is not working as expected. Fixing that in this JIRA.,pull-request-available,[],HDDS,Sub-task,Critical,2019-03-29 17:05:07,30
13224849,ozone s3 shell command has confusing subcommands,"Let's check the potential subcommands of ozone sh:

{code}
[hadoop@om-0 keytabs]$ ozone sh
Incomplete command
Usage: ozone sh [-hV] [--verbose] [-D=<String=String>]... [COMMAND]
Shell for Ozone object store
      --verbose   More verbose output. Show the stack trace of the errors.
  -D, --set=<String=String>

  -h, --help      Show this help message and exit.
  -V, --version   Print version information and exit.
Commands:
  volume, vol  Volume specific operations
  bucket       Bucket specific operations
  key          Key specific operations
  token        Token specific operations
{code}

This is fine, but for ozone s3:

{code}
[hadoop@om-0 keytabs]$ ozone s3
Incomplete command
Usage: ozone s3 [-hV] [--verbose] [-D=<String=String>]... [COMMAND]
Shell for S3 specific operations
      --verbose   More verbose output. Show the stack trace of the errors.
  -D, --set=<String=String>

  -h, --help      Show this help message and exit.
  -V, --version   Print version information and exit.
Commands:
  getsecret    Returns s3 secret for current user
  path         Returns the ozone path for S3Bucket
  volume, vol  Volume specific operations
  bucket       Bucket specific operations
  key          Key specific operations
  token        Token specific operations
{code}

This list should contain only the getsecret/path commands and not the volume/bucket/key subcommands.",pull-request-available,[],HDDS,Bug,Major,2019-03-29 15:06:45,6
13224839,Only FQDN is accepted for OM rpc address in secure environment,"While the scm address can be a host name (relative to the current search domain) if the om address is just a hostname and not a FQDN a NPE is thrown:

{code}
  10   │   OZONE-SITE.XML_ozone.om.address: ""om-0.om""
  11   │   OZONE-SITE.XML_ozone.scm.client.address: ""scm-0.scm""
  12   │   OZONE-SITE.XML_ozone.scm.names: ""scm-0.scm""
{code} 

{code}
2019-03-29 14:37:52 ERROR OzoneManager:865 - Failed to start the OzoneManager.
java.lang.NullPointerException
    at org.apache.hadoop.ozone.om.OzoneManager.getSCMSignedCert(OzoneManager.java:1372)
    at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1018)
    at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:971)
    at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:928)
    at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)
{code}

I don't know what is the right validation rule here, but I am pretty sure that NPE should be avoided and a meaningful error should be thrown. (and the behaviour should be the same for scm and om)",pull-request-available,[],HDDS,Bug,Major,2019-03-29 14:47:07,110
13224835,Kerberos principal configuration of OzoneManager doesn't use FQDN,"In the ""*.kerberos.principal"" settings hadoop supports the _HOST variable which is replaced to the fully qualified domain name.

For example:

{code}
OZONE-SITE.XML_hdds.scm.kerberos.principal: ""scm/_HOST@EXAMPLE.COM""
{code}

It works well with scm but for om it uses the hostname instead of the FQDN. (SCM uses the HddsServerUtil.getScmBlockClientBindAddress which uses the  _bind_ address but the om uses the om rpc address).

I would suggest to use the same behaviour for both SCM and OM.",Triaged,['Security'],HDDS,Bug,Minor,2019-03-29 14:36:14,110
13224799,Metrics scm_pipeline_metrics_num_pipeline_creation_failed keeps increasing because of BackgroundPipelineCreator,"There is a {{BackgroundPipelineCreator}} thread in SCM which runs in a fixed interval and tries to create pipelines. This BackgroundPipelineCreator uses {{IOException}} as exit criteria (no more pipelines can be created). In each run of BackgroundPipelineCreator we exit when we are not able to create any more pipelines, i.e. when we get IOException while trying to create the pipeline. This means that {{scm_pipeline_metrics_num_pipeline_creation_failed}} value will get incremented in each run of BackgroundPipelineCreator.",newbie pull-request-available,['SCM'],HDDS,Improvement,Minor,2019-03-29 11:40:32,30
13224673,NoClassDefFoundError when running ozone genconf,"{{ozone genconf}} fails due to incomplete classpath.

Steps to reproduce:

# [build and run Ozone|https://cwiki.apache.org/confluence/display/HADOOP/Development+cluster+with+docker]
# run {{ozone genconf}} in one of the containers:

{code}
$ ozone genconf /tmp
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/sun/xml/bind/v2/model/annotation/AnnotationReader
  at java.lang.ClassLoader.defineClass1(Native Method)
...
  at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:242)
  at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:234)
  at javax.xml.bind.ContextFinder.find(ContextFinder.java:441)
  at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:641)
  at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:584)
  at org.apache.hadoop.hdds.conf.OzoneConfiguration.readPropertyFromXml(OzoneConfiguration.java:57)
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.generateConfigurations(GenerateOzoneRequiredConfigurations.java:103)
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.call(GenerateOzoneRequiredConfigurations.java:73)
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.call(GenerateOzoneRequiredConfigurations.java:50)
  at picocli.CommandLine.execute(CommandLine.java:919)
...
  at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.main(GenerateOzoneRequiredConfigurations.java:68)
Caused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.model.annotation.AnnotationReader
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 36 more
{code}

{{AnnotationReader}} is in {{jaxb-core}} jar, which is not in the {{hadoop-ozone-tools}} classpath.",pull-request-available,['build'],HDDS,Bug,Major,2019-03-28 18:52:50,1
13224642,Fix checkstyle issue in TestDatanodeStateMachine,"The following tests are FAILED:
 
[checkstyle]: checkstyle check is failed ([https://ci.anzix.net/job/ozone-nightly/44/checkstyle/])",pull-request-available,[],HDDS,Improvement,Minor,2019-03-28 17:00:07,28
13224467,Implement GetS3Secret to use double buffer and cache.,"In Om HA getS3Secret  should happen only leader OM.

 

 

The reason is similar to initiateMultipartUpload. For more info refer HDDS-1319 

 ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-28 01:45:46,13
13224466,Remove hard-coded version ozone-0.5.0 from ReadMe of ozonesecure-mr docker-compose,"As we are releasing ozone-0.4, we should not have hard-coded ozone-0.5 for trunk. 

The proposal is to use the following to replace it

{{cd}} {{$(git rev-parse --show-toplevel)}}{{/hadoop-ozone/dist/target/ozone-}}{{*-SNAPSHOT}}{{/compose/ozone}}",pull-request-available,[],HDDS,Improvement,Blocker,2019-03-28 01:31:33,28
13224372,TestOzoneManagerHA#testOMProxyProviderFailoverOnConnectionFailure fails intermittently,"The test fails intermittently. The link to the test report can be found below.

[https://builds.apache.org/job/PreCommit-HDDS-Build/2582/testReport/]
{code:java}
java.net.ConnectException: Call From ea902c1cb730/172.17.0.3 to localhost:10174 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:310)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createVolume(OzoneManagerProtocolClientSideTranslatorPB.java:343)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.createVolume(RpcClient.java:275)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)
	at com.sun.proxy.$Proxy86.createVolume(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy86.createVolume(Unknown Source)
	at org.apache.hadoop.ozone.client.ObjectStore.createVolume(ObjectStore.java:100)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.createVolumeTest(TestOzoneManagerHA.java:162)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testOMProxyProviderFailoverOnConnectionFailure(TestOzoneManagerHA.java:237)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 49 more
{code}",TriagePending,['test'],HDDS,Sub-task,Major,2019-03-27 17:10:17,43
13224370,TestContainerReplication#testContainerReplication fails intermittently,"The test fails intermittently. The link to the test report can be found below.

https://builds.apache.org/job/PreCommit-HDDS-Build/2582/testReport/
{code:java}
java.lang.AssertionError: Container is not replicated to the destination datanode
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.apache.hadoop.ozone.container.TestContainerReplication.testContainerReplication(TestContainerReplication.java:139)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",pull-request-available,['test'],HDDS,Bug,Major,2019-03-27 17:02:50,6
13224156,Add List Containers API for Recon,"Recon server should support ""/containers"" API that lists all the containers",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-03-26 21:59:38,37
13224110,Implement Ratis Snapshots on OM,"For bootstrapping and restarting OMs, we need to implement snapshots in OM. The OM state maintained by RocksDB will be checkpoint-ed on demand. Ratis snapshots will only preserve the last applied log index by the State Machine on disk. This index will be stored in file in the OM metadata dir.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-26 18:31:32,43
13224001,Handle GroupMismatchException in OzoneClient,"If a pipeline gets destroyed in ozone client, ozone client may hit GroupMismatchException from Ratis. In cases as such, client should exclude the pipeline and retry write to a different block.",Blocker,['Ozone Client'],HDDS,Improvement,Blocker,2019-03-26 10:23:42,16
13223996,Better block allocation policy in SCM,"In case, a client hits close container exception and asks SCM to allocate block for the next write, SCM might allocate blocks from containers which are almost full and can get closed while the client write is still on. Ozone client by default tries 5 times on hitting an exception to write Data to a different block in case it hits CloseContainerException 5 times, it will give up.

The issue was found while testing with HDDS-1295.",Triaged,['SCM'],HDDS,Improvement,Major,2019-03-26 10:04:39,16
13223732,OzoneFileSystem can't work with spark/hadoop2.7 because incompatible security classes,"The current ozonefs compatibility layer is broken by: HDDS-1299.

The spark jobs (including hadoop 2.7) can't be executed any more:

{code}
2019-03-25 09:50:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProviderTokenIssuer
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
        at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)
        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:757)
        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:724)
        at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:45)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 43 more
{code}",pull-request-available,[],HDDS,Bug,Major,2019-03-25 10:18:17,6
13223529,Add some logging for flaky test testStartStopDatanodeStateMachine,"testStartStopDatanodeStateMachine fails frequently in Jenkins. It also seems to have a timing issue which may be different from the Jenkins failure.

E.g. If I add a 10 second sleep as below I can get the test to fail 100%.

{code}
@@ -163,6 +163,7 @@ public void testStartStopDatanodeStateMachine() throws IOException,
     try (DatanodeStateMachine stateMachine =
         new DatanodeStateMachine(getNewDatanodeDetails(), conf, null)) {
       stateMachine.startDaemon();
+      Thread.sleep(10_000L);
{code}",pull-request-available,[],HDDS,Bug,Major,2019-03-23 19:41:42,113
13223462,In DatanodeStateMachine join check for not null,[https://builds.apache.org/job/PreCommit-HDDS-Build/2565/testReport/org.apache.hadoop.ozone.scm.node/TestSCMNodeMetrics/testNodeReportProcessingFailure/],pull-request-available,[],HDDS,Bug,Major,2019-03-22 23:32:05,13
13223459,Add a docker compose for Ozone deployment with Recon.,"* Add a docker compose for Ozone deployment with Recon.
* Test out Recon container key service. ",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-03-22 23:04:05,30
13223458,Update documentation for Ozone-0.4.0 release,We need to update documenation of Ozone for all the new features which is part of 0.4.0 release. This is a 0.4.0 blocker JIRA.,pull-request-available,[],HDDS,Task,Blocker,2019-03-22 22:55:48,102
13223251,TestOzoneManagerHA seems to be flaky,"TestOzoneManagerHA failed once with the following error:
{code}
[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 105.931 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOzoneManagerHA
[ERROR] testOMRetryProxy(org.apache.hadoop.ozone.om.TestOzoneManagerHA)  Time elapsed: 21.781 s  <<< FAILURE!
java.lang.AssertionError: expected:<30> but was:<10>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testOMRetryProxy(TestOzoneManagerHA.java:305)
{code}
",pull-request-available,['test'],HDDS,Bug,Major,2019-03-22 05:25:15,43
13223198,Hugo errors when building Ozone,"I see some odd hugo errors when building Ozone, even though I am not building docs.
{code}
$ mvn -B -q clean compile install -DskipTests=true -Dmaven.javadoc.skip=true -Dmaven.site.skip=true -DskipShade -Phdds

Error: unknown command ""0.4.0-SNAPSHOT"" for ""hugo""
Run 'hugo --help' for usage.
.../hadoop-hdds/docs/target
Error: unknown command ""0.4.0-SNAPSHOT"" for ""hugo""
Run 'hugo --help' for usage.
.../hadoop-hdds/docs/target
{code}",pull-request-available,[],HDDS,Bug,Major,2019-03-21 21:34:42,1
13223176,TestOzoneManagerHttpServer depends on hard-coded port numbers,TestOzoneManagerHttpServer depends on a hard-coded port number due to a bug in config initialization.,pull-request-available,[],HDDS,Bug,Major,2019-03-21 19:42:51,113
13222923,Fix MalformedTracerStateStringException on DN logs,"Have seen many warnings on DN logs. This ticket is opened to track the investigation and fix for this.

{code}

2019-03-20 19:01:33 WARN PropagationRegistry$ExceptionCatchingExtractorDecorator:60 - Error when extracting SpanContext from carrier. Handling gracefully.
io.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 2c919331-9a51-4bc4-acee-df57a8dcecf0
 at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:42)
 at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:32)
 at io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)
 at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)
 at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)
 at io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)
 at org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:96)
 at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
 at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
 at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
 at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
 at org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
 at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
 at org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46)
 at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)
 at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686)
 at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
 at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-03-20 19:19:31,28
13222921,KeyOutputStream#write throws ArrayIndexOutOfBoundsException when running RandomWrite MR examples,"Repro steps:

{code} 

jar $HADOOP_MAPRED_HOME/hadoop-mapreduce-examples-*.jar randomwriter -Dtest.randomwrite.total_bytes=10000000  o3fs://bucket1.vol1/randomwrite.out

{code}

 

Error Stack:

{code}

2019-03-20 19:02:37 INFO Job:1686 - Task Id : attempt_1553108378906_0002_m_000000_0, Status : FAILED
Error: java.lang.ArrayIndexOutOfBoundsException: -5
 at java.util.ArrayList.elementData(ArrayList.java:422)
 at java.util.ArrayList.get(ArrayList.java:435)
 at org.apache.hadoop.hdds.scm.storage.BufferPool.getBuffer(BufferPool.java:45)
 at org.apache.hadoop.hdds.scm.storage.BufferPool.allocateBufferIfNeeded(BufferPool.java:59)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:215)
 at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:130)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:311)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:273)
 at org.apache.hadoop.fs.ozone.OzoneFSOutputStream.write(OzoneFSOutputStream.java:46)
 at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
 at java.io.DataOutputStream.write(DataOutputStream.java:107)
 at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1444)
 at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:83)
 at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:670)
 at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
 at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
 at org.apache.hadoop.examples.RandomWriter$RandomMapper.map(RandomWriter.java:199)
 at org.apache.hadoop.examples.RandomWriter$RandomMapper.map(RandomWriter.java:165)
 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-03-20 19:06:18,16
13222533,remove extra logging to stdout while trying to read key when one of the datanodes is down,"steps taken :

---------------------
 # wrote of key in 3 datanode cluster.
 # shutdown a datanode 
 # read the key

observation:

---------------------

read was successful.

But , it should not print all logs in client's STDOUT.

 

client's output:

---------------------
{noformat}
2019-03-19 09:49:14,329|INFO|MainThread|ozone.py:92 - getKey()|Running get key WriteOzoneFile_1552988856 in test-volume1-1552988834/test-bucket1-1552988834
2019-03-19 09:49:14,331|INFO|MainThread|machine.py:165 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|RUNNING: ssh -l root -i /tmp/hw-qe-keypair.pem -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ctr-e139-1542663976389-88627-01-000002.hwx.site ""sudo su - -c \""/usr/hdp/current/hadoop-ozone/bin/ozone sh key get /test-volume1-1552988834/test-bucket1-1552988834/WriteOzoneFile_1552988856 /tmp/OzonePipelineTest/WriteOzoneFileGet_1552988856\"" root""
2019-03-19 09:49:17,915|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|SLF4J: Class path contains multiple SLF4J bindings.
2019-03-19 09:49:17,916|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:17,916|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop-ozone/share/ozone/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:17,917|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2019-03-19 09:49:17,993|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2019-03-19 09:49:21,371|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:21 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: GetBlock
2019-03-19 09:49:21,372|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:21,372|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:21,372|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:21,372|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|getBlock {
2019-03-19 09:49:21,372|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:21,373|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:21,373|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:21,373|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:21,373|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,374|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,374|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:21,374|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:21,375|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:21,375|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:21,375|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:21,375|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:204)
2019-03-19 09:49:21,376|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
2019-03-19 09:49:21,376|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.getFromOmKeyInfo(KeyInputStream.java:305)
2019-03-19 09:49:21,376|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.rpc.RpcClient.getKey(RpcClient.java:608)
2019-03-19 09:49:21,376|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.OzoneBucket.readKey(OzoneBucket.java:284)
2019-03-19 09:49:21,377|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:95)
2019-03-19 09:49:21,377|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:21,377|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:21,378|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:21,378|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:21,378|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:21,378|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:21,379|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:21,379|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:21,379|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:21,380|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:21,380|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:21,380|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:21,381|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:21,381|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:21,382|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:21,382|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:21,382|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:21,383|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:21,383|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:21,384|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:21,384|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:21,384|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:21,385|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:21,385|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:21,385|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:21,386|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:21,386|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:21,386|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:21,387|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:21,387|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:21,387|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:21,387|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:21,388|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:21,388|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:21,388|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:21,388|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:21,389|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:21,389|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:21,389|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:21,390|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:21,390|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:21,390|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:21,391|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:21,391|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:21,391|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:21,392|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:21,392|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:21,564|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:21 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: GetBlock
2019-03-19 09:49:21,564|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:21,565|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:21,565|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:21,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|getBlock {
2019-03-19 09:49:21,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:21,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:21,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:21,567|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:21,567|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,568|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,568|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:21,568|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:21,569|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:21,569|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:21,569|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:21,570|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:204)
2019-03-19 09:49:21,570|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:118)
2019-03-19 09:49:21,570|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.getFromOmKeyInfo(KeyInputStream.java:305)
2019-03-19 09:49:21,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.rpc.RpcClient.getKey(RpcClient.java:608)
2019-03-19 09:49:21,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.OzoneBucket.readKey(OzoneBucket.java:284)
2019-03-19 09:49:21,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:95)
2019-03-19 09:49:21,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:21,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:21,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:21,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:21,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:21,573|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:21,573|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:21,573|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:21,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:21,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:21,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:21,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:21,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:21,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:21,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:21,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:21,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:21,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:21,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:21,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:21,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:21,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:21,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:21,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:21,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:21,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:21,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:21,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:21,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:21,579|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:21,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:21,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:21,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:21,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:21,581|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:21,581|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:21,581|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:21,582|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:21,582|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:21,583|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:21,583|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:21,583|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:21,584|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:21,584|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:21,584|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:21,585|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:21,585|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:21,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:21 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:21,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:21,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:21,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:21,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:21,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:21,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:21,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:21,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:21,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:21,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_1""
2019-03-19 09:49:21,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:21,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:21,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:21,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:21,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:21,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\261]\364\310\n\\\275\300\335\335\0008\314\236\332\241\273\360Qhbr\247@g\277\263i\275\224]\273""
2019-03-19 09:49:21,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""b!\034e\2616/\324&=\234\022\324m\327\305\304\313\304:\227P\035br~x<M\217tk""
2019-03-19 09:49:21,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""ttm `m\306Ed%\r\326\035c\362\372YF\275\032\302s\316d\024\177\034n\237\346\t""
2019-03-19 09:49:21,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\330\316\3634J\265\t\275\362\251%HG\245I\'\357\277\322Lr?5MI\vL\177\032\341\201\023""
2019-03-19 09:49:21,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:21,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""]\""\234\027\340\000Y<\257\004\271+1\233S2\274\226Ww \325>\030X\240\344\314\004\314\344\262""
2019-03-19 09:49:21,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""%\302\222\025\217\234\237\306\367\003B\001\324,\220p\037\3225\237\215\023a\222\272:\272\346\361\3044\370""
2019-03-19 09:49:21,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""-\f\v\020\223\030S\321M\334.I\277<\370r\353\362O\371\354v\311\v#\306\200+\245\336{\312""
2019-03-19 09:49:21,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\336u\335\277a\363\216\025\375\034\242\004F\205.&(f\246Nj\027\354\v\203ZF\245&b\356\f""
2019-03-19 09:49:21,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\242T\356\306b\220X\232\356\3318<\035\005s9\004V:\370S\fg=\221\035\217\253\342\235\266m""
2019-03-19 09:49:21,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:21,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:21,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:21,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:21,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:21,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:21,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:21,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:21,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:21,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:21,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:21,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:21,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:21,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:21,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:21,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)
2019-03-19 09:49:21,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:21,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:21,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:21,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:21,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:21,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:21,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:21,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:21,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:21,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:21,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:21,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:21,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:21,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:21,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:21,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:21,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:21,828|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:21,828|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:21,828|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:21,829|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:21,829|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:21,829|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:21,830|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:21,830|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:21,831|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:21,831|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:21,831|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:21,832|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:21,832|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:21,832|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:21,832|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:21,832|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:21,832|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:21,833|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:21,833|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:21,833|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:21,833|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:21,834|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:21,834|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:21,834|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:21,834|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:21,834|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:21,835|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:21,835|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:21,835|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:21,835|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:21,835|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:22,115|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:21 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:22,115|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:22,115|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,116|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:22,116|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:22,116|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:22,116|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,117|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:22,117|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:22,117|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,117|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:22,117|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_2""
2019-03-19 09:49:22,118|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:22,118|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:22,118|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:22,118|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:22,119|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:22,119|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\327^\207\361~\207\245\321\273\327\a\300.\337\344\""^{U\000\261\334\256|`\223\227\341I\233~\201""
2019-03-19 09:49:22,119|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""e|g\357\222]\\\353\203\375\230\032\351\266\274]\331\246\362X1!\035\214\2766\222\2321\235\370f""
2019-03-19 09:49:22,120|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\333()a8!l\271\325\273d\v\272\214>!\315\354\245|3S(\001\235\324\221\356\216\372\356\324""
2019-03-19 09:49:22,120|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\254\265\254)\232h\224\320\n\261D\352\033[\245\263\3011\211/|5@\367\23485p\227\315jX""
2019-03-19 09:49:22,120|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\021\aye_X\206R+t(\037\345\272e\314\314\275\261k\360,ff\376n\210R\356\027\237\026""
2019-03-19 09:49:22,121|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\341\237\237\t\251\217\""v\202\231~F\244\355[\vK\b\221\235\n\277\373\343\331\375S\365\237+J\357""
2019-03-19 09:49:22,121|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\304\363l\017\006\315\030\023\246\274\tO\0035+J\353#\034v\350\3345\bvo\367J\361\361\201\302""
2019-03-19 09:49:22,122|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""`\275\034r\3157I7\272\017r\343\0230>\250R&\253\371\347\266k\245d\b(\335=\v\021\177""
2019-03-19 09:49:22,122|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\375\213\224$\301\004$\230\312Vd\'\265\336OT\211e\v\""\303\'\303\032\034\\\201I\032\313\363\352""
2019-03-19 09:49:22,122|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""[\344\271\376\347\200\207\271\204\253\371t\2401\324\304\245\277\236\332\326\362\365\327\374\226\2046t<kT""
2019-03-19 09:49:22,123|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\344MJ\354b[\202\002u~\n\220\2703s\004~\211B?\036\373\0178;\247\250\311\214\310\241\361""
2019-03-19 09:49:22,123|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\205\247\220\216On\262\227\324\261`R@\v\221X\236\021\243\316{\232\320\032\212\207\344\355\377M\325E""
2019-03-19 09:49:22,124|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\361\2017\217\347v\233|\267\346\325\236\270s\214\326\204I\222\334\212^\266t\325\361y\211\213\303F\342""
2019-03-19 09:49:22,124|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\322\313q\365\252\025\376\201q\356\216\311\226\rrX\311>\v\361\021m\337\254v{\201\244]i\225\225""
2019-03-19 09:49:22,124|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\222\332C\270\371\'6\017D\037y\200\321\354|\230TP\\\243\352\274\003\fr\031\223\371!H\025\""""
2019-03-19 09:49:22,125|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\252\231D&\025\005\335\037o=2\205\026\277\234u\033\251\350\373j\321\\}\222\351f\306\035YH\246""
2019-03-19 09:49:22,125|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,125|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,126|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,126|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:22,126|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,127|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:22,127|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:22,127|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:22,128|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:22,128|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:22,128|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:22,129|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:22,129|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:22,129|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:22,130|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:22,130|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:22,131|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:22,131|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:22,131|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:22,132|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:22,132|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:22,132|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:22,133|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:22,133|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:22,133|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:22,134|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:22,134|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:22,135|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:22,135|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:22,135|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:22,136|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:22,136|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,136|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:22,137|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:22,137|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,138|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,138|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,138|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:22,139|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,139|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,139|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,140|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:22,140|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:22,140|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:22,140|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:22,141|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:22,141|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:22,142|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:22,142|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:22,142|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:22,143|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:22,143|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:22,143|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:22,144|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:22,144|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:22,144|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:22,145|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:22,145|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:22,145|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:22,146|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:22,146|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:22,146|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:22,147|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:22,147|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:22,147|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:22,148|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:22,424|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:22 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:22,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:22,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:22,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:22,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:22,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:22,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:22,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:22,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_3""
2019-03-19 09:49:22,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:22,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:22,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:22,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:22,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:22,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\vg\214\271\335\357v\312A\'\311\025\002\313G\3471\204\233\274\1778\307\222I.\340\1771Nxu""
2019-03-19 09:49:22,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""U\006e9\246\2118lm{\024\360j\033/\206q\372\235\376\246\224$\273m\252\240\246\033\273\201\003""
2019-03-19 09:49:22,431|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\205?=\274~\336\356\\\252\nL\220\252\352\016\266\370\025\033\352\205L\330\320\333\032\371\264\272\371\330\223""
2019-03-19 09:49:22,431|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\357x\217N\224\232\323\206k\037\227\262L\224\211\360\004t\235\225\2155q*\342q\024\360Lf\250\311""
2019-03-19 09:49:22,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\257_Z\022g\214O\343Y9,v\372\256\233\037\212\030\314:\315\247[\300V6]%\203\377%n""
2019-03-19 09:49:22,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""I;y\002p\307U\037F \034\027\230,\1775\311(\344\250\246\343\321\363%srR\301=\303.""
2019-03-19 09:49:22,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""<5\357R\027\312G\005\252s\332\230\332\001YO\206\350\266\262\244\222\371\3706\212\021\373\351\244\235/""
2019-03-19 09:49:22,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\2602\343_\3417\267=/\351\310\3404\373\371L\327\272\264\334\004!\323\a%E\336\324\376\212\256G""
2019-03-19 09:49:22,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""i\332\212\355\346\364O\256\236PT\316\375\224\250\034\314\224\001\357\243\316o\376\223\241\261\036\326\2025-""
2019-03-19 09:49:22,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\340\247\277\355\350\214\027\321\036\301\246\362&^\233\262\344&tP\217\234\301:\364\202\233[\237\020\255\377""
2019-03-19 09:49:22,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\276XE\344\240Hp\0042\'\263\300[x<\355\252\t\250\332\255\017F\260\2041L23\222\302C""
2019-03-19 09:49:22,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""Z\205\331\371\215\t\201~`\352z\315\351I\251\235\341\325\272\364X\f\353\310B\371\341{~)\377\305""
2019-03-19 09:49:22,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:22,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""{\235\035o\230Fy\355\276\246\273\210q;\2372\2049\347\337rdK\215/\326#~\356\336\035\317""
2019-03-19 09:49:22,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\000\245\336\307!\371R\202\343q\335\240\231\247i\343\027\240\341\333U\211D\315\305\235\253\026z\345\360\254""
2019-03-19 09:49:22,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\235\217\223\355\275\341_`\017\340\333\006\251S\372\352\336=\261\317V\331\200\304\366\324\264|m\316%\023""
2019-03-19 09:49:22,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:22,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:22,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:22,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:22,439|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:22,439|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:22,439|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:22,439|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:22,440|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:22,440|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:22,440|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:22,441|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:22,441|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:22,441|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:22,442|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:22,442|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:22,442|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:22,443|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:22,443|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:22,443|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:22,444|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:22,444|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:22,444|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:22,445|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:22,445|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:22,445|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:22,445|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:22,446|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,446|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:22,447|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:22,447|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,447|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,448|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,448|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:22,449|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,449|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,449|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,450|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:22,450|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:22,450|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:22,451|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:22,451|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:22,452|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:22,452|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:22,452|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:22,452|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:22,453|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:22,453|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:22,453|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:22,454|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:22,454|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:22,454|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:22,455|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:22,455|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:22,455|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:22,456|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:22,456|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:22,456|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:22,457|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:22,457|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:22,457|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:22,457|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:22,670|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:22 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:22,670|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:22,670|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,671|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:22,671|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:22,671|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:22,672|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,672|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:22,672|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:22,673|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,673|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:22,673|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_4""
2019-03-19 09:49:22,673|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:22,674|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:22,674|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:22,674|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:22,675|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:22,675|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\344\032\331\0304[,\365b\247V\252mzH\301k,\371\347;\377h\023\332*\216\252\203{r\342""
2019-03-19 09:49:22,676|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\261k7s)\177BHOM`\274\215y\2610\033\370\260I\372\020+\303F?=Gn\230\322\340""
2019-03-19 09:49:22,676|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""y\304\3100\243va\363\244YD\217c\034.\227=\365\350t\366\220\027\325-M\226qU>f\362""
2019-03-19 09:49:22,676|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\0279\r]\034\314\353\200`8u)\n\310\324j)B\335\335g\3266\307\246)\374\v\320\351\224\241""
2019-03-19 09:49:22,677|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\026\367o\266_=\350C(s\v\277vBB\241\210\342O\253\324V\302\217\241!\234\327o\315\304\343""
2019-03-19 09:49:22,677|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""=\331@}G\332f\334\247\216\222\357\376\223\320^\264\261iC\023\214O &\227\322\025\306(i\347""
2019-03-19 09:49:22,677|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\367W\230\343j\032\031\357\261\024\277\317\317g\326<\255\034\t\177\n\177U\341\353\'\026\362\020\250K~""
2019-03-19 09:49:22,678|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\360m\227\362\241\231\n)\242\022J\222=\324\230b\n\020\340\316^&\006\234;J/\250\037\006)\264""
2019-03-19 09:49:22,678|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\035\221\240\025h\t\f\025~\b\374\345\330\027\324\263\025\277\266\030\253g\2513t\264Y\277\243\247\325\242""
2019-03-19 09:49:22,678|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""6\370\370\037A\203\245@\207Y\203\277\303\3769-\331\\h\271\356\323\016$Y}K\307 \347N\350""
2019-03-19 09:49:22,679|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\241\265\206.\317p\316Q2\337I(\2550\303`\322\253i\256\310\222\272\027}\263Ky\336\242\357n""
2019-03-19 09:49:22,679|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\237\313C\251\353\236\352s]mo\267\364\344^u\021D\240{l\v\203g\362\2051\224\bK\364\217""
2019-03-19 09:49:22,679|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:22,680|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""1\247\\\374\036\217\034`]\366\374\325\022\213U\341\214^iT\205z9\312d\275\204[(\b\221\235""
2019-03-19 09:49:22,680|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""ti\006\\\366\031\222\343\300\353\227\251\a\033\177\301 k\304v3_\272\252t\361\273\\6\301}\n""
2019-03-19 09:49:22,680|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\362\202\f5\020%4\370\322\016\036\310\017C[\250\220L\3131\005{m\321\361\233\267\235:\252\231\206""
2019-03-19 09:49:22,681|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,681|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,681|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,681|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:22,682|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,682|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:22,682|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:22,683|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:22,683|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:22,683|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:22,684|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:22,684|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:22,684|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:22,685|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:22,685|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:22,686|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:22,686|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:22,686|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:22,687|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:22,687|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:22,687|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:22,687|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:22,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:22,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:22,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:22,689|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:22,689|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:22,690|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:22,690|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:22,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:22,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:22,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:22,692|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:22,692|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,693|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,693|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:22,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,695|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,695|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:22,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:22,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:22,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:22,697|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:22,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:22,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:22,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:22,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:22,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:22,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:22,700|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:22,700|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:22,701|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:22,701|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:22,702|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:22,702|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:22,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:22,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:22,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:22,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:22,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:22,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:22,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:22,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:22,852|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:22 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:22,853|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:22,853|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,853|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:22,854|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:22,854|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:22,855|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:22,856|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:22,857|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:22,857|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,857|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:22,858|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_5""
2019-03-19 09:49:22,859|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:22,860|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:22,860|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:22,860|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:22,861|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:22,862|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\316\030\312$|\243\232\324\325\022+\235B\233g\260T\244\247h$\266&\370\252\310\237\021\315k\302r""
2019-03-19 09:49:22,863|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\241:4\027\216,\340\200\327\3018\245\033<\2411\351R]\364\rS\003\313\355\r\244\304\223\353\245g""
2019-03-19 09:49:22,864|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\235n\307LM\t9\203\303\2755\354\342\320\204\260&t\277\352\""\t\241\321p\217\305\211\316\340\026\020""
2019-03-19 09:49:22,864|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\302EI\b,\363\355\375\031\030\b\006\037\243a\306\202e\n&?\251%+mB5\310\224\304u\307""
2019-03-19 09:49:22,865|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\313\243\020p\202\347R&\317\256<_\024\037>S\022\320e\ni\227,\264\250,&Qg>\353.""
2019-03-19 09:49:22,865|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\325\206\202\365\016\377\253\002\000\206,\217ic\365\031^\321h\256\317O\324\364s\3228\334p>\310\230""
2019-03-19 09:49:22,865|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\332%\230\2374M\316Z\271]\215\221\203\215Q\""\243kd\267\344\242\312\307\377Pkd+x1\276""
2019-03-19 09:49:22,866|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\347\021)\347\033\331\232?\326\t\273\256\017\352\3526\247\241\022\244\230y|\346\235\225\336\245\376\334\363\252""
2019-03-19 09:49:22,866|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\025\247B\325\'\371\230 \362\263K0d\322\272\264b\241:N\211\351\022\322\r\031F\276?\267x\303""
2019-03-19 09:49:22,866|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""-[\356\343.C\030\221\262*\025\244\006\317f\2540\250wjb)8\347\f\357%\177QJ;N""
2019-03-19 09:49:22,866|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\300\321\205\304>n\200\246\251\227\2341kF\004\362\340Tz\322_\334abP;H\205\316\211\030q""
2019-03-19 09:49:22,867|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\261\216\245\212\031\345B\317\270\255x\001\231\376nwE\002\301s\3571T\325-V\026\313\022\217\373u""
2019-03-19 09:49:22,867|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:22,867|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\373\344\226[\316\f\232S\025\177{\276\370\r\202x\226Gb\346L\237!?\343\222l\236a\374k\353""
2019-03-19 09:49:22,867|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""v\204\265,\003\340\b\375\252C\252C\327\212\003\324\312\374L\352\223\363\005\t\320\264\362\355[st\257""
2019-03-19 09:49:22,868|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""F\177/.\035\024\374\325}\025;\271\v>I={\372p\273\016\311\023\225\322\366\233J\022K\370#""
2019-03-19 09:49:22,868|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,868|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,868|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:22,868|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:22,869|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,869|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:22,870|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:22,870|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:22,870|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:22,870|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:22,871|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:22,871|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:22,871|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:22,871|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:22,871|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:22,872|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:22,872|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:22,872|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:22,872|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:22,873|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:22,873|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:22,873|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:22,873|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:22,873|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:22,874|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:22,874|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:22,874|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:22,874|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:22,874|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:22,875|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:22,875|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:22,875|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:22,875|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:22,876|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:22,876|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,876|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,876|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,877|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:22,877|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:22,877|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:22,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:22,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:22,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:22,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:22,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:22,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:22,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:22,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:22,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:22,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:22,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:22,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:22,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:22,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:22,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:22,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:22,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:22,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:22,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:22,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:22,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:22,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:22,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:22,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:22,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:22,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:23,266|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:23 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:23,267|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:23,267|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:23,267|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:23,267|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:23,267|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:23,268|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:23,268|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:23,268|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:23,268|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,268|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:23,268|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_6""
2019-03-19 09:49:23,269|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:23,269|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:23,269|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:23,269|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:23,270|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:23,270|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\300,G$\272\337\177K\311\226b\307\270x0\315h\315\001\351\327\316_\016\313Y7\331U\215\333`""
2019-03-19 09:49:23,270|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: "">\245N\375\247\365Vk\311 \\\356\325@\336\213* \273Z5\373\312Z\352\276\003\205\313w\255\315""
2019-03-19 09:49:23,271|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\032\206\257\367\214\232\256\222\017o\202E\203\215\355Sp\n-\302\361\301\3734\001F\262\327\031\335\336\'""
2019-03-19 09:49:23,271|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\242k^?\241\2114\327d\0029\256\2030\371\241\317\205g\231\323\027v\235\370\252-\266\374\303W\264""
2019-03-19 09:49:23,271|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""B\243\fhv-\aRE~2\247u\263\302J.p0\f\3623\317\273\022*K\242}\205\326e""
2019-03-19 09:49:23,272|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\205\003\363n\236\226\177\327U\316@|\333\236\342\351\230\371\223|\034\377i\214N\025\344\252\241t\005\320""
2019-03-19 09:49:23,272|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\302\334]Q\354Y;\363\352f\350\304\023^\t\214\211\234l\357\201\230\300)2\245Q\r\365\001\243}""
2019-03-19 09:49:23,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""@\256\307dY!\364;j\227E\255$3\216{\355r\211_\251_\203t|\030W\nA\234o\221""
2019-03-19 09:49:23,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: "":\n\262\344H3\034\306\244\036Z\242\004\b\252\264\241\356a\220iy\264\376g\226r\363\200m\204f""
2019-03-19 09:49:23,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\'\215[mB\363\243\235\002\2731\230e!fP\034\375\006\325\244\031\200\361K\202\020\315*\276\353\355""
2019-03-19 09:49:23,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""|^\273\\\340#It\263\365=\242\245\\\265\357\206\017\004\346r\306\370=Bbf1F`<\375""
2019-03-19 09:49:23,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""n\343\233s]oG\""\222\204_E\370\nb5\tPJ\274\232\033\356U\370\324\205\255\250\230\344\201""
2019-03-19 09:49:23,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\352d\316\363)\303k\305\036\341b\373\261\225:C\a\\\026)\312X\002\305\373\333~\002\306\225\366\231""
2019-03-19 09:49:23,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\202\355\002\021$!\333t2\336\250TS\243\306D\372\305\310sQ\233+\a\312V3\206\234\2274\370""
2019-03-19 09:49:23,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\000\206\200gA\314#J\354\016$<\211\v\235^\231\355\211f\257\364\243\327Mn\031\2477\017\367\370""
2019-03-19 09:49:23,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""5x\212\237\305|]\361~\3336$\""\f+UG\257\232o0\242\027\322\000\252\257\264\207\212\276\275""
2019-03-19 09:49:23,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:23,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:23,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:23,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:23,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:23,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:23,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:23,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:23,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:23,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:23,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:23,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:23,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:23,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:23,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:23,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:23,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:23,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:23,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:23,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:23,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:23,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:23,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:23,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:23,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:23,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:23,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:23,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:23,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:23,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:23,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:23,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:23,283|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:23,283|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:23,283|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:23,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:23,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:23,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:23,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:23,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:23,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:23,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:23,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:23,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:23,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:23,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:23,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:23,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:23,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:23,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:23,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:23,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:23,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:23,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:23,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:23,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:23,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:23,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:23,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:23,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:23,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:23,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:23,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:23,557|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:23 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:23,557|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:23,558|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:23,558|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:23,558|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:23,559|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:23,559|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:23,559|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:23,560|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:23,560|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,560|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:23,561|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_7""
2019-03-19 09:49:23,561|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:23,561|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:23,562|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:23,562|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:23,562|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:23,562|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:23,563|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""t\353\224_\357\351\372:\377Ik\242\266\004\307~n\245+$\243%.\351x\305\205\315\271\375\352\\""
2019-03-19 09:49:23,563|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\327^\207\361~\207\245\321\273\327\a\300.\337\344\""^{U\000\261\334\256|`\223\227\341I\233~\201""
2019-03-19 09:49:23,563|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""e|g\357\222]\\\353\203\375\230\032\351\266\274]\331\246\362X1!\035\214\2766\222\2321\235\370f""
2019-03-19 09:49:23,564|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\333()a8!l\271\325\273d\v\272\214>!\315\354\245|3S(\001\235\324\221\356\216\372\356\324""
2019-03-19 09:49:23,564|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\254\265\254)\232h\224\320\n\261D\352\033[\245\263\3011\211/|5@\367\23485p\227\315jX""
2019-03-19 09:49:23,564|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\354\336bw\034\334\221\330\321LQ\317 p\305\265\356\304\317O\237\337|\225+\322\'\t}^=\016""
2019-03-19 09:49:23,565|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\204}\217X\236\240Z\001\271a6\234\272:\'\271\000\267\205:\032\""qX\344N\315\214O\277Tu""
2019-03-19 09:49:23,565|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\210\307\200\200\027\255\257t\306lw\v\267\216\251\260]\304:\367#\027%R_h&\033\003\233\022F""
2019-03-19 09:49:23,565|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: "" Dw\331\226\3056\232\017(\266~\243\375\342\272\360\225\217\363\344\351\367\272\0308\340v\001\222\3639""
2019-03-19 09:49:23,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\270z=\235\030\267L\033-UL\300\357\305\302\2560\260\241\272\307\024\265!\277\346\343U\230\277\317\201""
2019-03-19 09:49:23,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\332\036:>\004r\361\216\372?y\t\341\026Ik<\241\207\370\000\231\356.\246{7\235\210\360\325\265""
2019-03-19 09:49:23,566|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:23,567|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\343\363d\275$]\b\311\232\235wl\036\005\236\216\240\265V\263\350mb\261\261\344;\264D\300\000B""
2019-03-19 09:49:23,568|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\363\252\312\233\2252\325\375\215T\222\311\035\005_\245\262\361W\367[\201$\037}\353?m\363/y\224""
2019-03-19 09:49:23,568|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""5J\331\364\267\214%\302\341<_\344\333V\214\325mL=\253+\035\215\a\243\310i\v\300\356\250\227""
2019-03-19 09:49:23,568|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,569|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,569|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,569|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:23,570|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:23,570|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:23,570|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:23,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:23,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:23,571|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:23,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:23,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:23,572|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:23,573|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:23,573|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:23,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:23,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:23,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:23,574|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:23,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:23,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:23,575|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:23,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:23,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:23,576|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:23,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:23,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:23,577|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:23,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:23,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:23,578|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:23,579|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:23,579|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:23,579|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:23,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:23,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:23,580|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:23,581|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:23,581|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:23,581|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:23,582|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:23,582|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:23,582|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:23,583|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:23,583|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:23,583|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:23,584|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:23,584|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:23,584|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:23,585|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:23,585|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:23,585|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:23,585|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:23,586|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:23,586|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:23,586|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:23,587|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:23,587|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:23,587|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:23,588|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:23,588|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:23,588|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:23,589|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:23,589|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:23,589|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:23,589|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:23,798|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:23 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:23,798|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:23,799|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:23,799|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:23,799|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:23,800|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:23,800|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 1
2019-03-19 09:49:23,800|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776679626145792
2019-03-19 09:49:23,801|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 23
2019-03-19 09:49:23,801|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,801|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:23,801|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_dfa8c5a9-80e8-4c2a-a371-058de7f11f64_chunk_8""
2019-03-19 09:49:23,802|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:23,802|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:23,802|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:23,803|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:23,803|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:23,804|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:23,804|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""H\304\304dy\351=\350\307F\224\275\256\026&\300\244\233\302j\301)H\303MCW\031\336]CI""
2019-03-19 09:49:23,804|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\207\\\277\333\032[N\227\234\314\356\316\340\330\032\320\303l\037\333\001\305\345\342\364l\345\003\021;?P""
2019-03-19 09:49:23,804|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""x\323\031\317\214\237z\304g>\356\375\374{\301\324}y\035\373\325\266\313\003\223\301\231\317\232a\236\212""
2019-03-19 09:49:23,805|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""z\026\374HI\373Y$Q\360\271T\253\276\241\276h/\370L\3527D\345\004\2767\256\340\364\020\230""
2019-03-19 09:49:23,805|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\r\345n\363\005\017C\311\036M\304p\272\330\302z\354]8m\324\025}\207\330\026\315\3768\361l9""
2019-03-19 09:49:23,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\3516\312\350\260\177eU\244\020\'\022rT\034\320\363\3663\332\322\204e\235\225\356J\332\330L\303\253""
2019-03-19 09:49:23,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\311\241\255\354\261\244\036\003\366\354y\321b\2501\331\fc.\273\214c8\r\377*\254,f\327\275/""
2019-03-19 09:49:23,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\376!\276\341Y\312\374\252\003w;\215Sx\030\\G>\2521\362\325A\027\034\315\361:\251\034R\321""
2019-03-19 09:49:23,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\3323\301+\034<\341\226\264\251\330\255\000\261\037\232\226y\264\257^\357^\006\262\000\200\200\033tt\346""
2019-03-19 09:49:23,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\373\330\250o\374b\274iD\332\270\002\005\216T\032\271\005}ju\357\031\264L\233\f\335\267\b\335\203""
2019-03-19 09:49:23,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\0233a\246\305e\265\326\243\270y1z\226\250#\215\032\305\260Z\354o\211\027\327G\v\346X\202\022""
2019-03-19 09:49:23,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\212>\2455\231\270yU\255Dv\320_\320\330A&8\243\330d\027@\362\024\241\004E\212\255!\351""
2019-03-19 09:49:23,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""L\n\205N\256#\t\327j\366\023B.\2139\303\225e+\255wD\357\326w\'\362C\270\035\034\240""
2019-03-19 09:49:23,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""e\310U\374\265\201\257 ?q!\022\364P9\244B\""\n\326\352J\263Y\034Q\3460\tt\261F""
2019-03-19 09:49:23,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""B\177\r-\004\353$=[c\037\n\302E\034\335\365\321]\302m\370 \027\320\207\233-\375q\226^""
2019-03-19 09:49:23,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:23,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:23,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:23,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:23,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:23,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:23,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:23,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:23,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:23,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:23,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:23,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:23,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:23,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:23,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:23,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:23,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:23,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:23,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:23,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:23,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:23,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:23,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:23,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:23,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:23,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:23,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:23,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:23,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:23,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:23,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:23,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:23,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:23,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:23,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:23,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:23,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:23,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:23,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:23,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:23,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:23,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:23,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:23,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:23,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:23,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:23,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:23,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:23,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:23,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:23,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:23,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:23,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:23,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:23,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:23,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:23,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:23,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:23,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:23,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:23,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:23,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:23,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:23,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:24,064|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:23 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:24,064|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:24,064|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,065|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:24,066|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:24,066|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:24,066|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,067|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:24,067|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:24,068|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,068|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:24,068|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_1""
2019-03-19 09:49:24,069|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:24,069|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:24,069|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:24,070|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:24,070|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:24,071|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\321\002n\027\255\307,\242\232\275\365*\266\""\v\370\020\275\334\357\301d\2259\300*\353\254\2711\032\030""
2019-03-19 09:49:24,071|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""n?\375\351=l0+\276\037\033\227J\274\255\005\357\335\300yG\346\324\323<Y\312\003\330\\y\275""
2019-03-19 09:49:24,071|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\034\226M)#1*\324\233i\310\303\345cP\267jU\211\f\266\216*Q\345\340\262\312q\211\3122""
2019-03-19 09:49:24,072|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\312\302\360\177Y\346\031\223\351Ks\212u\330p\202=\204p\032\230\000GA\272R\307\2662\235Q\276""
2019-03-19 09:49:24,072|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,073|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,073|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,074|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\321\372G\203_\256nbB\256w\334\201C\342/\006\234\263i\357\220\252\370\""vEB.od\232""
2019-03-19 09:49:24,074|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""O\342nb\330\303?\005\354\264\347\003\246\377\205\t\313\336\235\306\232\017*\244\353\036\211\270\b8J\024""
2019-03-19 09:49:24,074|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,075|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,075|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\350\325 p\342\272f\200\313\257\377y\223<6$\2170\274\036T\332a\b\r\215`\373\341@\262X""
2019-03-19 09:49:24,076|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""*n\320\f<\347OV\316\323@\351rx\211\037\330\242j/V\205\352\340\376 \3076\333\004\033\365""
2019-03-19 09:49:24,076|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\v\262\036\025t6\257\0250\367\a\315\235\233\211&\344\231\336\325B\345\246,\313EX\265\240=v#""
2019-03-19 09:49:24,077|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,077|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,077|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,077|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,078|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,078|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:24,079|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,079|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:24,079|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:24,080|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:24,080|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:24,081|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:24,081|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:24,081|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:24,082|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:24,082|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:24,083|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:24,083|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:24,083|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:24,084|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:24,084|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:24,085|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:24,085|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:24,085|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:24,086|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:24,086|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:24,086|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:24,087|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:24,087|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:24,087|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:24,088|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:24,088|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:24,088|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:24,089|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,089|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:24,090|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:24,090|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,090|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,091|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,091|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:24,092|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,092|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,092|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,093|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:24,093|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:24,093|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:24,094|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:24,094|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:24,095|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:24,095|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:24,095|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:24,096|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:24,096|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:24,096|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:24,097|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:24,097|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:24,097|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:24,098|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:24,098|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:24,098|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:24,099|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:24,099|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:24,099|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:24,100|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:24,100|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:24,100|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:24,101|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:24,101|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:24,271|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:24 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:24,271|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:24,271|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,272|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:24,272|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:24,272|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:24,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:24,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:24,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,273|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:24,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_2""
2019-03-19 09:49:24,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:24,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:24,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:24,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:24,274|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:24,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,275|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:24,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""3\365V\301\305\035\337\3058\361\031\203\3604\034\305{\253\252b\2055\261\223N5sYA\025\271\242""
2019-03-19 09:49:24,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\002(\320\245\262\226\233G\211\356\331\324EU%\027\276\245\335\327\267+\241\352\314D@\205\271Fu&""
2019-03-19 09:49:24,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\3106\260\201\017\362\360\317\363\264g\271\347f\354C\343m\234\026\230\303\274\t\372\356\370\204\006\326]\345""
2019-03-19 09:49:24,276|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\343\212\330W\255\222w@\261\322Y\307\247\337\017\321\366\b\000\213\233\234\245Us<\nW\274\005\373\341""
2019-03-19 09:49:24,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\000\342\352\262,|\302\026\275\302\374}\324\264\354\'!\271\371l\350\304\320K\003\300\225\023\264\251\246\274""
2019-03-19 09:49:24,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""L\352[4U\252y\310c\334s/\366\361\r$\203\225\357\202\210\343\273\340\024s\022k\313\336S\261""
2019-03-19 09:49:24,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\200\322\f\272\aF\266\0056\212\270W\002\354\257\204\350\312\257_\206Fi\265E*<U\307Ze\023""
2019-03-19 09:49:24,277|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\b\354\277.\374\3771Nn$\277#\202j\324CY\337\vC\231\245Z\250\037OO\365\317\207\177\212""
2019-03-19 09:49:24,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\3474Q\025\353.\205\315[\221\216\276\344:\325\227\265\331q\355\356?\230}\334QQ|\314\215\347\321""
2019-03-19 09:49:24,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\037\\\3536U\360\333\252/^T\357\267#\237)8\363S\347v\034S\354c\207\233\303\366\336\026z""
2019-03-19 09:49:24,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\n\205\367\256>\205\242\331\234Fn\203.\304%\333\022\236\367\201AN\336\227\207+m\367\231\226-\304""
2019-03-19 09:49:24,278|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\200ztd\211\276\2479\356^\316\036^\370\326:\206\306\a[<\317\002\331\376[\300\354l\""\231\\""
2019-03-19 09:49:24,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\272z\264\275\000\021\236A\021\336&\344z\t\263\002\035`\200\250p%\322$\263\276\n\301\345\""A\241""
2019-03-19 09:49:24,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,279|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:24,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:24,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:24,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:24,280|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:24,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:24,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:24,281|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:24,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:24,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:24,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:24,282|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:24,283|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:24,283|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:24,283|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:24,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:24,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:24,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:24,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:24,284|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:24,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:24,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:24,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:24,285|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:24,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:24,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:24,286|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:24,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:24,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:24,287|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:24,288|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:24,289|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:24,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:24,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:24,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:24,290|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:24,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:24,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:24,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:24,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:24,291|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:24,292|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:24,292|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:24,292|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:24,292|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:24,292|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:24,293|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:24,293|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:24,293|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:24,293|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:24,293|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:24,294|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:24,294|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:24,294|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:24,294|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:24,682|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:24 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:24,683|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:24,683|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,684|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:24,684|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:24,684|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:24,685|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,685|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:24,685|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:24,686|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,686|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:24,686|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_3""
2019-03-19 09:49:24,687|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:24,687|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:24,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:24,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:24,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:24,688|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\366\200\253\212$(?]r\303\321P\273JxX\273\023\333R\223\230\374D\270\203\263i\254\017\027;""
2019-03-19 09:49:24,689|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\f\200\241\216\261:6\334\037!*\314\231\r)\000\314s\207W1\266\202v\315\003\235\210\232\365|1""
2019-03-19 09:49:24,689|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""l\313\352?\304\234\374i\255u\032\343\022\v\377\245\177\317\200\343[\270\203Gb\200\315\204\366~\376\026""
2019-03-19 09:49:24,690|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\021*~\004\247P\231\216\206q\256\323\343p\365\207\031\001\321@\224\335\240\""\221bUw\2546-9""
2019-03-19 09:49:24,690|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""!c\037?\300\242\034\265\210D8\245\200\225\216W\'\304\370\230\263\027q\204f9\204+\274\356R\267""
2019-03-19 09:49:24,690|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\256J\335\370\331^\211\274]\223\372]\314\340\345\254bli\225\232\222`\020\326O9\255X\341\333s""
2019-03-19 09:49:24,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\231\231\375\317\335\246\212\243\333\272K\0054\340\0377l\324\0164OK\363|:k\334Q\024\232\005R""
2019-03-19 09:49:24,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\""Q\343&\366?\231<\226,\245\235,\241p\367!\003\246\277\244\362\252,\330o\277\210\344B\262\354""
2019-03-19 09:49:24,691|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\234\001\263\251#\023\321-\2779\311/\341U-~g\306\271\000\233\201L\340\245\245XE\\\335\244\330""
2019-03-19 09:49:24,692|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\365;\200\022\037=\rmHl\0035\021C\236f\300amTsU\244\241\301[\346\026K7\300\373""
2019-03-19 09:49:24,692|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\031\241\021&\240\305\210\215\365=\277u\220U\n \274?P\275\320\aDB\001\321\363Gb\262\232!""
2019-03-19 09:49:24,692|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\361n=\343\3720\212`/\026\371\217\240\301GZ\341`\235\315\256\342(\337\032\215X@J3\275U""
2019-03-19 09:49:24,693|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""^\344\204\354\3018\355 U;\335\230\271\234\377\257K\262\005\250\025K\267_g\363\002\313\332\236JB""
2019-03-19 09:49:24,693|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\032\224o\351\211\261y\260\303\234\261?\031Rza6D\202\024\335m\272e\331x\177r\212`\'\342""
2019-03-19 09:49:24,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""V\316\227\033\255\355\351\304d\2608m\273\213\247\216h1\222\212\215\230\2745B\204\230`\024\272C\024""
2019-03-19 09:49:24,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\037\220\376\a\235;\236\036`0\217\354\327d(\267Wp\324d\313\225=\261hu-L\337j46""
2019-03-19 09:49:24,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,694|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,695|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:24,695|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,695|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:24,695|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:24,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:24,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:24,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:24,696|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:24,697|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:24,697|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:24,697|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:24,697|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:24,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:24,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:24,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:24,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:24,698|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:24,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:24,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:24,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:24,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:24,699|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:24,700|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:24,700|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:24,700|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:24,700|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:24,701|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:24,701|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:24,701|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,701|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:24,702|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:24,702|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,702|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,702|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:24,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,703|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:24,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:24,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:24,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:24,704|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:24,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:24,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:24,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:24,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:24,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:24,705|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:24,706|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:24,706|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:24,706|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:24,706|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:24,707|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:24,707|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:24,707|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:24,707|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:24,708|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:24,708|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:24,708|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:24,708|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:24,708|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:24,708|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:24,876|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:24 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:24,877|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:24,877|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:24,878|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:24,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:24,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:24,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:24,879|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:24,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:24,880|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_4""
2019-03-19 09:49:24,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:24,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:24,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:24,881|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:24,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:24,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""Q&(\2329a\031\363\350\ro`NHL\177\302\252\220\233.a4\234X\313\214\317\345vl\264""
2019-03-19 09:49:24,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\\2\027\327\375=\353\304\257\345\345n\364\325\023\367\203\301%\250\035\005n\315\245\3736\005]\226\311G""
2019-03-19 09:49:24,882|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\016l\353\022&\023\232\0001\031\242\225E\006\356\262\370J)/\2108J\370\006_\264\2131i\006\022""
2019-03-19 09:49:24,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\307\374\005\231\306\376\347\r\027\253\352\305\275\342\267\""\f\300\177\206\241Z\361\256\324VIi#\337\035.""
2019-03-19 09:49:24,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""p\341\333\330?X\247\213\206\257l\320v\315?\262\241\277`H\347\320E\205\033\215\211geu\333=""
2019-03-19 09:49:24,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\027\vz\004\247%\222\377\362\354\316`\333\377\241b\347\273j\""\301\357\206\342\321\b\331\027\227\223\264\365""
2019-03-19 09:49:24,883|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\254\335\316\277@h\334L\232\332\376\355\370\330f\247\037\003-\005$\201\247v\323IE\261ua\263\027""
2019-03-19 09:49:24,884|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\2517\354\226\366\025\317H|.z\221\375\266\300\251\220+WvTd\366\300\331\310\217O\021[\213\362""
2019-03-19 09:49:24,884|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\265\247\301\202\361\303\346b<\000\034X\311L\211wSS\342 \217+\227\273\353I\345\361M\017\322\312""
2019-03-19 09:49:24,884|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\314\315\230\263-\333sB\225\274Dd\321J\367\240\245[\236\347W\373B\305\326\v\343lR;\371&""
2019-03-19 09:49:24,884|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\b\261\345{s\266\353F\366\362\005}\300[\235\245\224\313z\267\005K\023\304\fK%Jq\346\266Z""
2019-03-19 09:49:24,885|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""X&w?Rnh\342\266$JK\355\234\232\200\030\227\236]\227\'B\240\334\rx\331.\344k\270""
2019-03-19 09:49:24,885|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\277\357\200\326(\201D\271wi\227\320\002\\\330\2438c\325\a~\311\364\f\201\\p\326\274Y\263\255""
2019-03-19 09:49:24,885|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\272\200\213\324\005 \267cd\357G\227\245B\\-a#\362V\264\315\310?\350\032\316\032\036@\226P""
2019-03-19 09:49:24,885|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\001\360&\272;\302#\246\301v\266:\0212F\314\341\322$\326\326A\212\3621\370f\021\231\'\261T""
2019-03-19 09:49:24,886|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\021|S\323\000T~HL\230\005j\370 \371\325\216\231\366lk\322\350\347\263\267\211\317U\250\200\213""
2019-03-19 09:49:24,886|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,886|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,886|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:24,886|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:24,886|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,887|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:24,887|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:24,887|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:24,887|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:24,888|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:24,888|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:24,888|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:24,888|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:24,889|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:24,889|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:24,889|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:24,889|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:24,889|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:24,890|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:24,890|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:24,890|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:24,890|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:24,890|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:24,891|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:24,891|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:24,891|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:24,891|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:24,892|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:24,892|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:24,892|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:24,892|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:24,892|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:24,892|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:24,893|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:24,893|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,893|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,893|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,894|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:24,894|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:24,894|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:24,894|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:24,895|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:24,895|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:24,895|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:24,896|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:24,896|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:24,896|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:24,896|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:24,897|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:24,897|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:24,897|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:24,897|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:24,897|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:24,898|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:24,898|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:24,898|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:24,898|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:24,899|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:24,899|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:24,899|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:24,899|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:24,899|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:24,900|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:24,900|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:24,900|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:24,900|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:25,173|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:25 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:25,173|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:25,173|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,174|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:25,174|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:25,174|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:25,174|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,175|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:25,175|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:25,175|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,175|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:25,176|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_5""
2019-03-19 09:49:25,176|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:25,176|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:25,176|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:25,176|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:25,176|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:25,177|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: "",\302\247\203\372>Q\031\272\322L\233\327A\022\322\371N\000\365VM\263g\272\rLo\32703C""
2019-03-19 09:49:25,177|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\216=n\245\242\3274\025\306\3576\341u\034\240\016`7\225:5\272\267\\\251Pz\202\1772\357\215""
2019-03-19 09:49:25,177|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\201\347\342\320V\351\207\252\362,\022/>\a\f\222\341\355\234\331\177[c\356\""\356\366\253\017\'+<""
2019-03-19 09:49:25,178|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\373\312d\355M\016\276+w\321\3635]\364\217\265/\371\274\333V]\347\367\001\330\246`\371\036\037\221""
2019-03-19 09:49:25,178|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\334\346\271#\343\034\222\241x\270\272\327\330\253\331\224,\235\035\\\005\177\360\354\334p\353\342\271\036\360\020""
2019-03-19 09:49:25,178|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\326\004\331\233\233\272f\366P+7\305\334\'\032\032\003\355I\020\231E=\277\331)\212\202*`\371\351""
2019-03-19 09:49:25,178|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\270\240^\273\327\245\245\311\025\233\362\242\355\376\016P\332\313\272\260\234\211\a\2724\203\273\036\346?H\031""
2019-03-19 09:49:25,179|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\003\256/\257\316\352\033v\265\'\027\037\354O\377\211\240\220\320\024\247\236}u\031q1`\353\204\277\230""
2019-03-19 09:49:25,179|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\020\340Rl\vk\316\021y\316\006\b\027R\312\243<;\255\356\245hi\351\314\354\tC,!\246\321""
2019-03-19 09:49:25,179|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\275\246\003h\317\207\370\271\313\225.\2014p\335\307\t\322,\'\221\357\345F\300>\302\365\377\214\033\332""
2019-03-19 09:49:25,180|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\333\201\205\207\366oL\335e\0204\003\0208\355\207T\211\206\305\232Z\316\207\274[P\340e\246\254\270""
2019-03-19 09:49:25,180|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\344DA\rR\275\031\256%<\255C(\0340\332\251Ou\r\357\323\3307\220\257\354Z\227\025\213Q""
2019-03-19 09:49:25,180|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:25,180|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\026\006\371\033n\220w\310\304\271G5qZ\177M9\310\330\024\335\035\006\""(\030/\322\t\301\225W""
2019-03-19 09:49:25,181|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""?\213SA\235\235\""\252-WU*\020\2616]MK\0261:_G\261p\372\3646\242\3135@""
2019-03-19 09:49:25,181|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""?\365=\262\020}\255\267B\257\271M\2555\206\363@\310x\255!\257\256\244\354\260\306\367\215z4\364""
2019-03-19 09:49:25,181|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,181|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,182|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,182|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:25,182|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,182|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:25,183|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:25,183|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:25,183|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:25,183|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:25,184|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:25,184|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:25,184|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:25,184|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:25,184|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:25,185|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:25,185|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:25,185|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:25,185|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:25,186|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:25,186|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:25,186|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:25,186|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:25,186|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:25,186|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:25,187|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:25,187|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:25,187|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:25,187|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:25,188|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:25,188|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:25,188|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,188|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:25,188|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:25,188|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,189|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,189|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,189|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:25,190|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,190|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,190|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,190|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:25,191|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:25,191|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:25,191|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:25,191|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:25,192|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:25,192|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:25,192|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:25,193|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:25,193|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:25,193|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:25,193|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:25,193|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:25,194|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:25,194|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:25,194|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:25,194|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:25,195|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:25,195|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:25,195|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:25,195|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:25,196|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:25,196|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:25,196|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:25,196|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:25,418|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:25 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:25,418|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:25,418|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,418|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:25,419|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:25,419|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:25,419|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,419|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:25,419|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:25,420|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,420|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:25,420|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_6""
2019-03-19 09:49:25,420|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:25,420|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:25,421|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:25,421|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:25,421|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:25,421|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\020\261\363\207\220\304_@3C&\350i\274\225\034\0275\310\343xF\n\202&\252+\3361\2241T""
2019-03-19 09:49:25,421|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""}\314\243\235,\306\022\246\357\31289&\326\3030\303w\277\333@Y\032Z\366\244Q\3358.\027\334""
2019-03-19 09:49:25,422|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\347a\305\204\264\330\332\253\""\322\213DN6VUG\2253\274\022gLg\362\322\215I>\300\314\322""
2019-03-19 09:49:25,422|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\226\202\344\002\257\364\255\371\242F\314PF\fr\301\226P\037\302\362\207\302\021\376\254q\377\215\242:\303""
2019-03-19 09:49:25,422|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""k\242c\n\230c\320\235$w2\314\255\273\335\313\250\304lD\246\324\032\316\276\022\276%\341F\276\016""
2019-03-19 09:49:25,422|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\262|\'\353\213_\356{\341n\333\376\023\212^\000\255\205R\246\243\300\213\275\002\036* \177\356\332r""
2019-03-19 09:49:25,423|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\203\270\215\214\347$\341\004\363#\214\262\322\366;\336\336\253\027/\206\037E\247\027\303\'\260\2309\360\034""
2019-03-19 09:49:25,423|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: "":\023\213\236\364\210\v\'^?\322\227\232\321}\347$6\313o\256C6\331\341\""!\365\364\255\222\257""
2019-03-19 09:49:25,423|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""Z\226}NM\223G\370\362\216\365 [\002V\210\vD7_z.\266\327\033\021\366$\233\223Z\243""
2019-03-19 09:49:25,423|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\t\200\225\271\350\204L\221\343m\256\264\215[\343J\271f@\253\002\214V\240\002w\r\361k\020?^""
2019-03-19 09:49:25,423|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\245)-\005 \241\t\317\236\222\rt\""OJLM\355\353\352[\376NkrA\032\374\227\322\2471""
2019-03-19 09:49:25,424|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\316!+\3636#\315\262&\205\031\226\330\036\347\252\213S6w\371\337Bk>\301UGT\202{\363""
2019-03-19 09:49:25,424|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""0\341IU\353\3615\""f\334/\370\006~h\020F\a\347P\253\271\323\263e\202\270\257\220\237\313X""
2019-03-19 09:49:25,424|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\334Xz>(\2725@\320\214\233V\340J\242hO\344\264\217\260\""\305\200\203EgS\241\356\351/""
2019-03-19 09:49:25,424|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\204G\223\305\374v\233&\362\027\003~7\225\243\212D\270\211\2250Y$\321\0230Z\207\025\276\341|""
2019-03-19 09:49:25,424|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\020]\204k\336\323\311\227,\215h\237~\363F\236\r\230\201\323h\316\233\034\207\335\313f\024\005B\314""
2019-03-19 09:49:25,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:25,425|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:25,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:25,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:25,426|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:25,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:25,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:25,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:25,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:25,427|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:25,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:25,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:25,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:25,428|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:25,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:25,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:25,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:25,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:25,429|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:25,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:25,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:25,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:25,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:25,430|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:25,431|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:25,431|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:25,431|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:25,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:25,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:25,432|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:25,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,433|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:25,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:25,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:25,434|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:25,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:25,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:25,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:25,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:25,435|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:25,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:25,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:25,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:25,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:25,436|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:25,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:25,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:25,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:25,437|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:25,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:25,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:25,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:25,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:25,438|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:25,439|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:25,439|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:25,805|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:25 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:25,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:25,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:25,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:25,806|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:25,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:25,807|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:25,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:25,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_7""
2019-03-19 09:49:25,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:25,808|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 16777216
2019-03-19 09:49:25,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:25,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:25,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:25,809|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\210\257\216\346Y\336\260\326m\r\267-\317\205.\214\231\300\317zzjo\355\347\246,\370\370)J\205""
2019-03-19 09:49:25,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\245A\362yn\343d\273\313\370\355m\341\237\306SS-ch\222\b\366^\035\230n\233\224$\243L""
2019-03-19 09:49:25,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""!3\016\216\302\0051\350@&\200\251\343|\027\357\252\233\326\226\270\3010Y\004\027C\310\337a\352\231""
2019-03-19 09:49:25,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\251\304\005\301W\266f\225?\276\344W\f\016\235\344\213\b\300\276\200_\310!\375M\305\307\021\337cB""
2019-03-19 09:49:25,810|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\242*K\253\320V?\355\251\033\002l\226\002\030\351\256\243\337\377\242x(\f\266\375\""\rb\313\270G""
2019-03-19 09:49:25,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\365q1\037\312\a\322\262`\211\252\204\342U#\341\271[\023;\323F\270_\222p*3\371\235\f\247""
2019-03-19 09:49:25,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\321\244\321\256\036\314.\322\327\366\323\217\034!\247\264K\004D\250\214\213\202\257\370k\317v\225a\266K""
2019-03-19 09:49:25,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\227m\311\270\214`\345\352|\254\212\336B:\244ix\212\032bFr\273\253\313\231\307\237\312\263\271\270""
2019-03-19 09:49:25,811|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\376pi\033*k\261m\366r3jy\255\203\271\376\217\205\346\225{\203\207\027\265\t\225:2\030A""
2019-03-19 09:49:25,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""k\233\277\020\275K\221x]i\237ZIoGt\240RSv\201\003\227D\357\020\005\213\264\236\006\030""
2019-03-19 09:49:25,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""H\325\271\222n<\202\250\020sR\241{HD\320\377\f\330\004\320\231N\314\320j\204\365ah\227\204""
2019-03-19 09:49:25,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\344\263\246s\2554\237q\356\326\210\347\314\035\0022\020\005\033l\'\215(\225\376=\025\301tmlW""
2019-03-19 09:49:25,812|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""X\323\206\326P\341\257\216,o\324\354@:Fz\361\222\217\350\006\265 \021\030 \240\355\'\3414\362""
2019-03-19 09:49:25,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""~\337\270\261(\247\365\225\306\310\020\240\271X\375\035\364ZO=(\233G\031K\032T\335\003\356B\374""
2019-03-19 09:49:25,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""(\225\307\202\023%\201\365]a\336\241\371\276\025\376\260\310~\315\202\306\355\240v<\272%N\271\230:""
2019-03-19 09:49:25,813|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\204/\313\366\242\230\201\033\343\353\326\244\2317 7\367\264\027\035\225\314\000ld\342\347\364\n/y\303""
2019-03-19 09:49:25,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,814|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:25,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:25,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:25,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:25,815|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:25,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:25,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:25,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:25,816|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:25,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:25,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:25,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:25,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:25,817|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:25,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:25,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:25,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:25,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:25,818|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:25,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:25,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:25,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:25,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:25,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:25,819|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:25,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:25,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:25,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:25,820|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:25,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,821|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:25,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,822|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:25,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:25,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:25,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:25,823|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:25,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:25,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:25,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:25,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:25,824|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:25,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:25,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:25,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:25,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:25,825|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:25,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:25,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:25,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:25,826|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:25,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:25,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:25,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:25,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:25,827|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:25,828|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:25,959|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|19/03/19 09:49:25 WARN scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk
2019-03-19 09:49:25,959|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|traceID: ""f5bb707b-b0ef-4c21-a6ea-910b72221cfd""
2019-03-19 09:49:25,960|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,960|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|datanodeUuid: ""1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c""
2019-03-19 09:49:25,960|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|readChunk {
2019-03-19 09:49:25,960|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockID {
2019-03-19 09:49:25,961|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|containerID: 2
2019-03-19 09:49:25,961|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|localID: 101776681938518017
2019-03-19 09:49:25,961|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|blockCommitSequenceId: 43
2019-03-19 09:49:25,962|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,962|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkData {
2019-03-19 09:49:25,962|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|chunkName: ""f42c4a33a0b766f6a39981411a77be83_stream_fce2ae16-39ce-4f43-bf29-3db715ea2ced_chunk_8""
2019-03-19 09:49:25,962|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|offset: 0
2019-03-19 09:49:25,963|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|len: 15728640
2019-03-19 09:49:25,963|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksumData {
2019-03-19 09:49:25,963|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|type: SHA256
2019-03-19 09:49:25,963|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|bytesPerChecksum: 1048576
2019-03-19 09:49:25,963|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\221\371\346\003r\367\203\r\271o\243q\313dw}_\363\307\372\256^\362\361z\260\212V\206!\264\005""
2019-03-19 09:49:25,964|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""G\236\205@AF\357\006,V>>\262Hd\365\256loi\214\232\20260\316@~\376\360\222{""
2019-03-19 09:49:25,964|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\245\272\2037\266va\026z\342\vX|^\210\302\223\250|\243M\343\307-B!\313\366\'[\277]""
2019-03-19 09:49:25,964|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""c\243w\203/\310\036\210\364\034\262\353\211s\211W\251\303\362\231\230\336\a\362\254\307\3303\301\311\207%""
2019-03-19 09:49:25,965|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""M]\223-\277\333O\326\352\331\221{\311\221Y\317\314*\347\312\236\254\""\253H\\\016\207\342\312U\213""
2019-03-19 09:49:25,965|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""ag}\253\267@\377{\275a\277r\312\226\243\246\222\177c\337r\256\022\204\273@c\f0\367i\214""
2019-03-19 09:49:25,965|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\035\203SE\034\224L\253\353J`\'\b\241 FsY\273]\246\353\261\233=A\375\302\261\261\254\335""
2019-03-19 09:49:25,965|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""5@\221\005o\022\315\352\334\001\244\353_\003M\250h\364\361QxV\037\300\2137\305\205\335\220|\350""
2019-03-19 09:49:25,966|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""5\366\032\023\236\020\271Z2`q\345\n\316R4\316\306\2534\373\366\241N\223\222\341\302\330\362\260~""
2019-03-19 09:49:25,966|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\244_n\364\262[\255,\016\251{l\231N\365i>\244dg\250q\021C\300\321\304\355|\211\327\273""
2019-03-19 09:49:25,966|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""!\000\020\006A\372\3265\026h\363\373\375\224\261\264\200\241X\315vOE{a\214OBW\255_m""
2019-03-19 09:49:25,967|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""(y{\366 \327\215\017\000\367Ib\034UJ\347\312\037\202\203\005\352moA\2166\023;\243\316J""
2019-03-19 09:49:25,967|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\217\210\021(\203\t\2046\336\323\323\315\335\a\r\n\271\334\337\357\347c\263-7r\232\033T\317\271\344""
2019-03-19 09:49:25,967|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: ""\257\242\031\204\340\235,[\274\323\216\260\367\277\333\030*X\245)<%\241\332\344\274LX\005\302\275A""
2019-03-19 09:49:25,968|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|checksums: "";\n\222\316\275C\021J}\274p\313X#\266\024M\347nkuW5\023r\242\027\205\370\261\305\300""
2019-03-19 09:49:25,968|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,968|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,968|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|}
2019-03-19 09:49:25,968|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|on datanode 1b0f65e6-2ac8-4c5f-b552-e96f3a262f8c
2019-03-19 09:49:25,969|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,969|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2019-03-19 09:49:25,969|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
2019-03-19 09:49:25,970|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:247)
2019-03-19 09:49:25,970|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:217)
2019-03-19 09:49:25,970|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.readChunk(ContainerProtocolCalls.java:256)
2019-03-19 09:49:25,971|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.readChunkFromContainer(BlockInputStream.java:292)
2019-03-19 09:49:25,971|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.prepareRead(BlockInputStream.java:226)
2019-03-19 09:49:25,971|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:150)
2019-03-19 09:49:25,971|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream$ChunkInputStreamEntry.read(KeyInputStream.java:232)
2019-03-19 09:49:25,972|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:126)
2019-03-19 09:49:25,972|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.client.io.OzoneInputStream.read(OzoneInputStream.java:47)
2019-03-19 09:49:25,972|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.io.InputStream.read(InputStream.java:101)
2019-03-19 09:49:25,973|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)
2019-03-19 09:49:25,973|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:98)
2019-03-19 09:49:25,973|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.keys.GetKeyHandler.call(GetKeyHandler.java:48)
2019-03-19 09:49:25,973|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.execute(CommandLine.java:919)
2019-03-19 09:49:25,973|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.access$700(CommandLine.java:104)
2019-03-19 09:49:25,974|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
2019-03-19 09:49:25,974|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
2019-03-19 09:49:25,974|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
2019-03-19 09:49:25,974|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
2019-03-19 09:49:25,974|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
2019-03-19 09:49:25,975|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
2019-03-19 09:49:25,975|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
2019-03-19 09:49:25,975|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
2019-03-19 09:49:25,975|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95)
2019-03-19 09:49:25,976|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-03-19 09:49:25,976|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)
2019-03-19 09:49:25,976|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
2019-03-19 09:49:25,977|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,978|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,978|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,980|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
2019-03-19 09:49:25,981|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
2019-03-19 09:49:25,981|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
2019-03-19 09:49:25,982|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
2019-03-19 09:49:25,983|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
2019-03-19 09:49:25,984|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
2019-03-19 09:49:25,984|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
2019-03-19 09:49:25,984|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
2019-03-19 09:49:25,986|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
2019-03-19 09:49:25,986|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
2019-03-19 09:49:25,987|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2019-03-19 09:49:25,988|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2019-03-19 09:49:25,988|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2019-03-19 09:49:25,989|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2019-03-19 09:49:25,989|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at java.lang.Thread.run(Thread.java:748)
2019-03-19 09:49:25,990|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.89.128:9859
2019-03-19 09:49:25,990|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
2019-03-19 09:49:25,990|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
2019-03-19 09:49:25,990|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
2019-03-19 09:49:25,991|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
2019-03-19 09:49:25,991|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
2019-03-19 09:49:25,991|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
2019-03-19 09:49:25,991|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
2019-03-19 09:49:25,991|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
2019-03-19 09:49:25,992|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
2019-03-19 09:49:25,992|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
2019-03-19 09:49:25,992|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 1 more
2019-03-19 09:49:25,992|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Caused by: java.net.ConnectException: Connection refused
2019-03-19 09:49:25,993|INFO|MainThread|machine.py:180 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|... 11 more
2019-03-19 09:49:26,621|INFO|MainThread|machine.py:207 - run()||GUID=4bc13d85-dd87-4806-b3cc-7cc12fce8fdc|Exit Code: 0
2019-03-19 09:49:26,732|INFO|MainThread|machine.py:165 - run()||GUID=bf8a74f6-73bb-47e4-b572-47bbf824c543|RUNNING: ssh -l root -i /tmp/hw-qe-keypair.pem -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ctr-e139-1542663976389-88627-01-000002.hwx.site ""sudo su - -c \""md5sum /tmp/OzonePipelineTest/WriteOzoneFileGet_1552988856\"" root""
2019-03-19 09:49:30,213|INFO|MainThread|machine.py:180 - run()||GUID=bf8a74f6-73bb-47e4-b572-47bbf824c543|665facf9b10136e2b228ed64aaee80f2 /tmp/OzonePipelineTest/WriteOzoneFileGet_1552988856
2019-03-19 09:49:30,214|INFO|MainThread|machine.py:207 - run()||GUID=bf8a74f6-73bb-47e4-b572-47bbf824c543|Exit Code: 0
2019-03-19 09:49:30,214|INFO|MainThread|machine.py:642 - find_checksum()|Checksum of /tmp/OzonePipelineTest/WriteOzoneFileGet_1552988856 is : 665facf9b10136e2b228ed64aaee80f2
2019-03-19 09:49:30,216|INFO|MainThread|machine.py:165 - run()||GUID=c6f6b76e-4b17-4bdd-a595-a038a8e3262d|RUNNING: ssh -l root -i /tmp/hw-qe-keypair.pem -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ctr-e139-1542663976389-88627-01-000002.hwx.site ""sudo su - -c \""ls /tmp/OzonePipelineTest/WriteOzoneFileGet_1552988856\"" root""
2019-03-19 09:49:33,129|INFO|MainThread|machine.py:180 - run()||GUID=c6f6b76e-4b17-4bdd-a595-a038a8e3262d|/tmp/OzonePipelineTest/WriteOzoneFileGet_1552988856
2019-03-19 09:49:33,132|INFO|MainThread|machine.py:207 - run()||GUID=c6f6b76e-4b17-4bdd-a595-a038a8e3262d|Exit Code: 0
2019-03-19 09:49:33,133|INFO|MainThread|ambari.py:408 - getAdminPassword()|NoSectionError Exception raise. setting hdc = no by default.
2019-03-19 09:49:34,265|INFO|MainThread|ambari.py:408 - getAdminPassword()|NoSectionError Exception raise. setting hdc = no by default.
2019-03-19 09:49:34,265|INFO|MainThread|ambari.py:550 - startComponent()|Initiating start of DATANODE at host ctr-e139-1542663976389-88627-01-000002.hwx.site, request url is: https://172.27.89.128/ambari/api/v1/clusters/ozone-190317-133635-l3p/hosts/ctr-e139-1542663976389-88627-01-000002.hwx.site/host_components/DATANODE
2019-03-19 09:49:35,678|INFO|MainThread|ambari.py:561 - startComponent()|Completed start of DATANODE at host ctr-e139-1542663976389-88627-01-000002.hwx.site
2019-03-19 09:49:35,679|INFO|MainThread|conftest.py:345 - pytest_runtest_makereport()|call: <CallInfo when='call' result: []>
2019-03-19 09:49:35,680|INFO|MainThread|conftest.py:346 - pytest_runtest_makereport()|item: <Function 'test_ShutdownOneDatanodeWhileWrite'>
2019-03-19 09:49:35,681|INFO|MainThread|conftest.py:337 - pytest_report_teststatus()|TEST ""test_ShutdownOneDatanodeWhileWrite"" PASSED in 119.31 seconds
.2019-03-19 09:49:35,682|INFO|MainThread|ozone.py:109 - listKey()|Running list key in test-volume1-1552988834/test-bucket1-1552988834
2019-03-19 09:49:35,683|INFO|MainThread|machine.py:165 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|RUNNING: ssh -l root -i /tmp/hw-qe-keypair.pem -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ctr-e139-1542663976389-88627-01-000002.hwx.site ""sudo su - -c \""/usr/hdp/current/hadoop-ozone/bin/ozone sh key list /test-volume1-1552988834/test-bucket1-1552988834/\"" root""
2019-03-19 09:49:39,338|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|SLF4J: Class path contains multiple SLF4J bindings.
2019-03-19 09:49:39,339|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:39,340|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop-ozone/share/ozone/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:39,340|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2019-03-19 09:49:39,366|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2019-03-19 09:49:41,357|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|[ {
2019-03-19 09:49:41,359|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""version"" : 0,
2019-03-19 09:49:41,359|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""md5hash"" : null,
2019-03-19 09:49:41,360|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""createdOn"" : ""Tue, 19 Mar 2019 09:48:05 GMT"",
2019-03-19 09:49:41,360|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""modifiedOn"" : ""Tue, 19 Mar 2019 09:49:13 GMT"",
2019-03-19 09:49:41,360|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""size"" : 267386880,
2019-03-19 09:49:41,361|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""keyName"" : ""WriteOzoneFile_1552988856"",
2019-03-19 09:49:41,361|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|""type"" : null
2019-03-19 09:49:41,361|INFO|MainThread|machine.py:180 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|} ]
2019-03-19 09:49:41,385|INFO|MainThread|machine.py:207 - run()||GUID=6da81118-ca4f-457c-a80b-c0a07a89e341|Exit Code: 0
2019-03-19 09:49:41,386|INFO|MainThread|ozone.py:112 - listKey()|output=[[ {
 ""version"" : 0,
 ""md5hash"" : null,
 ""createdOn"" : ""Tue, 19 Mar 2019 09:48:05 GMT"",
 ""modifiedOn"" : ""Tue, 19 Mar 2019 09:49:13 GMT"",
 ""size"" : 267386880,
 ""keyName"" : ""WriteOzoneFile_1552988856"",
 ""type"" : null
} ]]
2019-03-19 09:49:41,386|INFO|MainThread|ozone.py:123 - deleteKey()|Running delete key WriteOzoneFile_1552988856 in test-volume1-1552988834/test-bucket1-1552988834
2019-03-19 09:49:41,388|INFO|MainThread|machine.py:165 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|RUNNING: ssh -l root -i /tmp/hw-qe-keypair.pem -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ctr-e139-1542663976389-88627-01-000002.hwx.site ""sudo su - -c \""/usr/hdp/current/hadoop-ozone/bin/ozone sh key delete /test-volume1-1552988834/test-bucket1-1552988834/WriteOzoneFile_1552988856\"" root""
2019-03-19 09:49:44,943|INFO|MainThread|machine.py:180 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|SLF4J: Class path contains multiple SLF4J bindings.
2019-03-19 09:49:44,949|INFO|MainThread|machine.py:180 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:44,949|INFO|MainThread|machine.py:180 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop-ozone/share/ozone/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:44,950|INFO|MainThread|machine.py:180 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2019-03-19 09:49:45,015|INFO|MainThread|machine.py:180 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2019-03-19 09:49:46,793|INFO|MainThread|machine.py:207 - run()||GUID=11f71a4d-2ca0-4f04-ba4d-318123b72d78|Exit Code: 0
2019-03-19 09:49:46,795|INFO|MainThread|ozone.py:70 - deleteBucket()|Running delete bucket of test-volume1-1552988834/test-bucket1-1552988834
2019-03-19 09:49:46,796|INFO|MainThread|machine.py:165 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|RUNNING: ssh -l root -i /tmp/hw-qe-keypair.pem -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ctr-e139-1542663976389-88627-01-000002.hwx.site ""sudo su - -c \""/usr/hdp/current/hadoop-ozone/bin/ozone sh bucket delete /test-volume1-1552988834/test-bucket1-1552988834\"" root""
2019-03-19 09:49:50,237|INFO|MainThread|machine.py:180 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|SLF4J: Class path contains multiple SLF4J bindings.
2019-03-19 09:49:50,237|INFO|MainThread|machine.py:180 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:50,238|INFO|MainThread|machine.py:180 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|SLF4J: Found binding in [jar:file:/usr/hdp/3.0.100.0-348/hadoop-ozone/share/ozone/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2019-03-19 09:49:50,238|INFO|MainThread|machine.py:180 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2019-03-19 09:49:50,278|INFO|MainThread|machine.py:180 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2019-03-19 09:49:52,231|INFO|MainThread|machine.py:207 - run()||GUID=5477cd27-e759-4a0d-8ac1-3cb2e8c28359|Exit Code: 0{noformat}",pull-request-available,['Ozone Client'],HDDS,Bug,Major,2019-03-19 10:07:44,20
13222490,Fix asf license errors,"HDDS-1250 added a few new files. In few of them, it is missing adding asf license header.

[https://github.com/apache/hadoop/pull/591]

 

Yetus has not reported about them. I think Yetus is broken in warning asf license errors.",pull-request-available,[],HDDS,Bug,Major,2019-03-19 05:11:14,13
13222489,Test ScmChillMode testChillModeOperations failed,"[https://ci.anzix.net/job/ozone-nightly/35/testReport/junit/org.apache.hadoop.ozone.om/TestScmChillMode/testChillModeOperations/]

 ",pull-request-available,[],HDDS,Bug,Major,2019-03-19 04:58:54,13
13222357,Fix SCM CLI does not list container with id 1,"In HDDS-1263 it is changed to handle the list containers with containerID 1 by changing the actual logic of listContainers in ScmContainerManager.java. But now with this change, it is contradicting with the javadoc.

From [~nandakumar131] comments

https://issues.apache.org/jira/browse/HDDS-1263?focusedCommentId=16794865&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16794865

 

I agree this will be the way to fix it.",pull-request-available,[],HDDS,Bug,Minor,2019-03-18 16:16:40,37
13222242,Support TokenIssuer interface for running jobs with OzoneFileSystem,"This ticket is opened to add TokenIssuer interface support to OzoneFileSystem so that MR and Spark jobs can run with OzoneFileSystem in secure mode. 

 

 ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-03-18 06:54:48,28
13222221,Fix checkstyle issue from Nightly run,"[https://ci.anzix.net/job/ozone-nightly/32/checkstyle/moduleName.588460772/fileName.-1184872187/]

 ",pull-request-available,[],HDDS,Sub-task,Major,2019-03-18 05:14:09,28
13222136,ExcludeList shoud be a RPC Client config so that multiple streams can avoid the same error.,"ExcludeList right now is a per BlockOutPutStream value, this can result in multiple keys created out of the same client to run into same exception",MiniOzoneChaosCluster,['Ozone Client'],HDDS,Bug,Major,2019-03-17 10:59:36,16
13222134,ExcludeList#getProtoBuf throws ArrayIndexOutOfBoundsException,"ExcludeList#getProtoBuf throws ArrayIndexOutOfBoundsException because getProtoBuf uses parallelStreams

{code}
2019-03-17 16:24:35,774 INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 3
	at java.util.ArrayList.add(ArrayList.java:463)
	at org.apache.hadoop.hdds.protocol.proto.HddsProtos$ExcludeListProto$Builder.addContainerIds(HddsProtos.java:12904)
	at org.apache.hadoop.hdds.scm.container.common.helpers.ExcludeList.lambda$getProtoBuf$3(ExcludeList.java:89)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool.helpComplete(ForkJoinPool.java:1870)
	at java.util.concurrent.ForkJoinPool.externalHelpComplete(ForkJoinPool.java:2467)
	at java.util.concurrent.ForkJoinTask.externalAwaitDone(ForkJoinTask.java:324)
	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:405)
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
	at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
	at org.apache.hadoop.hdds.scm.container.common.helpers.ExcludeList.getProtoBuf(ExcludeList.java:89)
	at org.apache.hadoop.hdds.scm.protocolPB.ScmBlockLocationProtocolClientSideTranslatorPB.allocateBlock(ScmBlockLocationProtocolClientSideTranslatorPB.java:100)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy22.allocateBlock(Unknown Source)
	at org.apache.hadoop.ozone.om.KeyManagerImpl.allocateBlock(KeyManagerImpl.java:275)
	at org.apache.hadoop.ozone.om.KeyManagerImpl.allocateBlock(KeyManagerImpl.java:246)
	at org.apache.hadoop.ozone.om.OzoneManager.allocateBlock(OzoneManager.java:2023)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.allocateBlock(OzoneManagerRequestHandler.java:631)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:231)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
, while invoking $Proxy28.submitRequest over null(localhost:59024). Trying to failover immediately.
2019-03-17 16:24:35,783 INFO  om.KeyManagerImpl (KeyManagerImpl.java:allocateBlock(271)) - allocate block key:pool-9-thread-7-1581351327 exclude:datanodes:containers:#6#1#9#5pipelines:
{code}",MiniOzoneChaosCluster,['SCM'],HDDS,Bug,Major,2019-03-17 10:56:19,16
13221966,Set OmKeyArgs#refreshPipeline flag properly to avoid reading from stale pipeline,"After HDDS-1138, the OM client will not talk to SCM directly to fetch the pipeline info. Instead the pipeline info is returned as part of the keyLocation cached by OM. 

 

In case SCM pipeline is changed such as closed, the client may get invalid pipeline exception. In this case, the client need to getKeyLocation with OmKeyArgs#refreshPipeline = true to force OM update its pipeline cache for this key. 

 

An optimization could be queue a background task to update all the keyLocations that is affected when OM does a refreshPipeline. (This part can be done in 0.5)
{code:java}
oldpipeline->newpipeline{code}
 ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-03-15 17:26:04,28
13221718,Implement actions need to be taken after chill mode exit wait time,"# Destroy and close the pipelines
 # Close all the containers on the pipeline.
 # trigger for pipeline creation",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-03-14 16:36:02,13
13221705,Adjust default values of pipline recovery for more resilient service restart,"As of now we have a following algorithm to handle node failures:

1. In case of a missing node the leader of the pipline or the scm can detected the missing heartbeats.
2. SCM will start to close the pipeline (CLOSING state) and try to close the containers with the remaining nodes in the pipeline
3. After 5 minutes the pipeline will be destroyed (CLOSED) and a new pipeline can be created from the healthy nodes (one node can be part only one pipwline in the same time).

While this algorithm can work well with a big cluster it doesn't provide very good usability on small clusters:

Use case1:

Given 3 nodes, in case of a service restart, if the restart takes more than 90s, the pipline will be moved to the CLOSING state. For the next 5 minutes (ozone.scm.pipeline.destroy.timeout) the container will remain in the CLOSING state. As there are no more nodes and we can't assign the same node to two different pipeline, the cluster will be unavailable for 5 minutes.

Use case2:

Given 90 nodes and 30 pipelines where all the pipelines are spread across 3 racks. Let's stop one rack. As all the pipelines are affected, all the pipelines will be moved to the CLOSING state. We have no free nodes, therefore we need to wait for 5 minutes to write any data to the cluster.

These problems can be solved in multiple ways:

1.) Instead of waiting 5 minutes, destroy the pipeline when all the containers are reported to be closed. (Most of the time it's enough, but some container report can be missing)
2.) Support multi-raft and open a pipeline as soon as we have enough nodes (even if the nodes already have a CLOSING pipelines).

Both the options require more work on the pipeline management side. For 0.4.0 we can adjust the following parameters to get better user experience:

{code}
  <property>
    <name>ozone.scm.pipeline.destroy.timeout</name>
    <value>60s</value>
    <tag>OZONE, SCM, PIPELINE</tag>
    <description>
      Once a pipeline is closed, SCM should wait for the above configured time
      before destroying a pipeline.
    </description>

  <property>
    <name>ozone.scm.stale.node.interval</name>
    <value>90s</value>
    <tag>OZONE, MANAGEMENT</tag>
    <description>
      The interval for stale node flagging. Please
      see ozone.scm.heartbeat.thread.interval before changing this value.
    </description>
  </property>
 {code}

First of all, we can be more optimistic and mark node to stale only after 5 mins instead of 90s. 5 mins should be enough most of the time to recover the nodes.

Second: we can decrease the time of ozone.scm.pipeline.destroy.timeout. Ideally the close command is sent by the scm to the datanode with a HB. Between two HB we have enough time to close all the containers via ratis. With the next HB, datanode can report the successful datanode. (If the containers can be closed the scm can manage the QUASI_CLOSED containers)

We need to wait 29 seconds (worst case) for the next HB, and 29+30 seconds for the confirmation. --> 66 seconds seems to be a safe choice (assuming that 6 seconds is enough to process the report about the successful closing)",pull-request-available,[],HDDS,Bug,Critical,2019-03-14 16:02:48,6
13221693,Fix the dynamic documentation of basic s3 client usage,"S3 gateway has a default web page to display a generic message if you open the endpoint in the browser:

http://localhost:9878/static/

It also contains a simple example to use the endpoint:

{code}
This is an endpoint of Apache Hadoop Ozone S3 gateway. Use it with any AWS S3 compatible tool with setting this url as an endpoint

For example with aws-cli:

aws s3api --endpoint http://localhost:9878/static/ create-bucket --bucket=wordcount

For more information, please check the documentation. 
{code}

Unfortunately the endpoint is wrong here, the static should be removed from the url.

The trivial fix is to move the ) in the js code>  

",pull-request-available,['S3'],HDDS,Bug,Major,2019-03-14 14:59:41,6
13221592,Fix the findbug issue caused by HDDS-1163,https://ci.anzix.net/job/ozone-nightly/30/findbugs/,newbie,[],HDDS,Bug,Minor,2019-03-14 05:38:51,30
13221547,[Ozone upgrade] Support Upgrading HDFS clusters to use Ozone,"This is the master JIRA to support upgrading existing HDFS clusters to have Ozone running concurrently. One of the requirements is that we support upgrading from HDFS to Ozone, without a full data copy. This requirement is called ""In Place upgrade"", the end result of such an upgrade would be to have the HDFS data appear in Ozone as if Ozone has taken a snap-shot of the HDFS data. Once upgrade is complete, Ozone and HDFS will act as independent systems. I will post a design document soon.",Triaged,[],HDDS,New Feature,Major,2019-03-14 00:03:51,102
13221544,"""ozone sh s3 getsecret"" throws Null Pointer Exception for unsecured clusters","{code:java}
hadoop@e72da2270499:~$ ozone sh s3 getsecret {code}
{code:java}
2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862). Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 1 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 2 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 3 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 4 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 5 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 6 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 7 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 8 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 9 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 ERROR OMFailoverProxyProvider:235 - Failed to connect to OM. Attempted 10 retries and 10 failovers java.lang.NullPointerException
{code}",pull-request-available,['Ozone CLI'],HDDS,Bug,Blocker,2019-03-13 23:42:29,37
13221530,Remove Parametrized in TestOzoneShell,"HDDS-1068 removed RestClient from the TestOzoneShell.java.

So now we don't need to be parameterized in the test anymore. We can directly test with RpcClient.",newbie pull-request-available,[],HDDS,Bug,Minor,2019-03-13 22:06:30,37
13221527,SCM CLI does not list container with id 1,"Steps to reproduce
 # Create two containers 

{code:java}
ozone scmcli create
ozone scmcli create{code}

 # Try to list containers

{code:java}
hadoop@7a73695402ae:~$ ozone scmcli list --start=0
 Container ID should be a positive long. 0
hadoop@7a73695402ae:~$ ozone scmcli list --start=1 
{ 
""state"" : ""OPEN"",
""replicationFactor"" : ""ONE"",
""replicationType"" : ""STAND_ALONE"",
""usedBytes"" : 0,
""numberOfKeys"" : 0,
""lastUsed"" : 274660388,
""stateEnterTime"" : 274646481,
""owner"" : ""OZONE"",
""containerID"" : 2,
""deleteTransactionId"" : 0,
""sequenceId"" : 0,
""open"" : true 
}{code}

There is no way to list the container with containerID 1.",pull-request-available,['Ozone CLI'],HDDS,Bug,Major,2019-03-13 21:44:12,37
13221524,In OM HA OpenKey call Should happen only leader OM,"In OM HA, currently, when openKey is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB and also clientID will be generated by each OM node.

 

The proposed approach is:

1. In startTransaction, call openKey and the response returned should be used to create a new OmRequest object and used in setting the transaction context. And also modify the ozoneManager and KeymanagerImpl to handle the code with and without ratis.

 

This Jira also implements HDDS-1319. ",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-13 21:39:47,13
13221491,Create Recon Server lifecyle integration with Ozone.,"* Create the lifecycle scripts (start/stop) for Recon Server along with Shell interface like the other components.
 * Verify configurations are being picked up by Recon Server on startup.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Critical,2019-03-13 18:27:57,37
13221419,OzoneFS classpath separation is broken by the token validation,"hadoop-ozone-filesystem-lib-legacy-0.4.0-SNAPSHOT.jar can't work any more together with older hadoop version, after the change of HDDS-1183.

{code}
2019-03-13 13:48:51 WARN  FileSystem:3170 - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.ozone.OzoneFileSystem could not be instantiated
2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration
2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2332)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2297)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3208)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3240)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3291)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3259)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:470)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:245)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:228)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:175)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:378)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 21 more
{code}

And Ozone file system current jar is compatible only with hadoop 3.2 after the latest HA change. It means that ozonefs is broken everywhere where the hadoop version is older than 3.2.",pull-request-available,[],HDDS,Improvement,Blocker,2019-03-13 14:21:57,6
13221390,Fix error propagation for SCM protocol,"HDDS-1068 fixed the error propagation between the OM client and OM server.

By default the Server.java transforms all the IOExceptions to one string (message + stack trace) and this is returned to the client.

But for business exception (eg. volume not found, chill mode is active, etc.) this is not what we need.

In the OM side we fixed this behaviour. In the ServerSideTranslator classes we catch (server) the business (OMException) exceptions and serialize them to the response object.

The exception (and the status code) is stored in message/status field of the OMResponse (hadoop-ozone/common/src/main/proto/OzoneManagerProtocol.proto)

Here I propose to do the same for the ScmBlockLocationProtocol.proto.

Unfortunately there is no common parent object (like OMRequest) in this protocol, but we can easily add one as only the Serverside/Clientside translator should be changed for that. ",pull-request-available,[],HDDS,Improvement,Critical,2019-03-13 12:15:06,11
13221208,Refactor ozone acceptance test to allow run in secure mode,Refactor ozone acceptance test to allow run in secure mode.,pull-request-available,[],HDDS,Sub-task,Major,2019-03-12 19:38:47,110
13221177,Fix failure in TestOzoneManagerHttpServer & TestStorageContainerManagerHttpServer,"Investigate failure of Fix failure in TestOzoneManagerHttpServer & TestStorageContainerManagerHttpServer in jenkins run.

",blocker pull-request-available security,[],HDDS,Sub-task,Major,2019-03-12 16:40:40,110
13221108,Fix checkstyle issue from Nightly run,https://ci.anzix.net/job/ozone-nightly/28/checkstyle/,pull-request-available,[],HDDS,Sub-task,Major,2019-03-12 12:37:30,28
13220954,In OM HA AllocateBlock call where connecting to SCM from OM should not happen on Ratis,"In OM HA, currently when allocateBlock is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB. The problem with this is, every OM allocateBlock and appends new BlockInfo into OMKeyInfom and also this a correctness issue. (As all OM's should have the same block information for a key, even though eventually this might be changed during key commit)

 

The proposed approach is:

1. Calling SCM for allocation of block will happen outside of ratis, and this block information is passed and writing to DB will happen via Ratis.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-11 22:23:03,13
13220906,Fix TestOzoneManagerHttpServer & TestStorageContainerManagerHttpServer,"Fix the following unit test failures
{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

and


{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",TriagePending,"['Ozone Manager', 'SCM']",HDDS,Sub-task,Major,2019-03-11 18:19:06,19
13220795,TestSecureOzoneRpcClient fails intermittently," 

TestSecureOzoneRpcClient fails intermittently with the following exception.
{code:java}
java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:338)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:238)
	at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:131)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:310)
	at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:271)
	at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.uploadPart(TestOzoneRpcClientAbstract.java:2188)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.doMultipartUpload(TestOzoneRpcClientAbstract.java:2131)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testMultipartUpload(TestOzoneRpcClientAbstract.java:1721)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.waitOnFlushFutures(BlockOutputStream.java:543)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:333)
	... 35 more
Caused by: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:417)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.
	at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:573)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:556)
	at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:415)
	... 6 more
{code}
cc : [~ajayydv] [~xyao]

 ",Triaged,['test'],HDDS,Bug,Major,2019-03-11 09:47:10,28
13220793,Bump trunk ozone version to 0.5.0,"ozone-0.4 branch is working, we need to update the trunk version.",pull-request-available,[],HDDS,Improvement,Major,2019-03-11 09:36:32,6
13220753,Add ozone delegation token utility subcmd for Ozone CLI,"This allow running dtutil with integration test and dev test for demo of Ozone security.

 

 

 

 ",pull-request-available,[],HDDS,Sub-task,Major,2019-03-11 06:34:44,28
13220749,OM delegation expiration time should use Time.now instead of Time.monotonicNow,"Otherwise, we will set incorrect the exp date of OM delegation like below: 

{code}
ozone dtutil print /tmp/om.dt
 
File: /tmp/om.dt
Token kind               Service              Renewer         Exp date     URL enc token
--------------------------------------------------------------------------------
OzoneToken               om:9862              yarn            *1/8/70 12:03 PM*
{code}",pull-request-available,[],HDDS,Sub-task,Major,2019-03-11 06:18:10,28
13220659,"In s3 when bucket already exists, it should just return location ","In S3 for a create bucket request, when bucket already exists it should just return the location.

This was broken by HDDS-1068.",pull-request-available,[],HDDS,Bug,Major,2019-03-10 04:16:31,13
13220506,Fix check style issues caused by HDDS-1196,"Jenkins has not reported any issues, but found when working on another jira.

https://github.com/apache/hadoop/pull/567",pull-request-available,[],HDDS,Bug,Minor,2019-03-08 17:27:01,13
13220245,Fix incorrect Ozone ClientProtocol KerberosInfo annotation,The serverPrincipal should be OMConfigKeys.OZONE_OM_KERBEROS_PRINCIPAL_KEY instead of ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,pull-request-available,[],HDDS,Sub-task,Major,2019-03-07 15:59:32,28
13220243,BaseHttpServer NPE is HTTP policy is HTTPS_ONLY,This needs to be fixed when Ozone is running inside DN as plugin and DN is running using non-privilege HTTPS port. ,pull-request-available,[],HDDS,Sub-task,Major,2019-03-07 15:55:21,28
13220097,Recon Container DB service definition,"* Define the Ozone Recon container DB service definition. 
",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-03-07 05:12:55,30
13220061,Add ChillMode metrics,"This Jira is to add few of the chill mode metrics:
 # NumberofHealthyPipelinesThreshold
 # currentHealthyPipelinesCount
 # NumberofPipelinesWithAtleastOneReplicaThreshold
 # CurrentPipelinesWithAtleastOneReplicaCount
 # ChillModeContainerWithOneReplicaReportedCutoff
 # CurrentContainerCutoff

 ",pull-request-available,[],HDDS,Improvement,Major,2019-03-06 23:55:08,13
13219830,Avoid extra buffer copy during checksum computation in write Path,"The code here does a buffer copy to to compute checksum. This needs to be avoided.
{code:java}
/**
 * Computes checksum for give data.
 * @param byteString input data in the form of ByteString.
 * @return ChecksumData computed for input data.
 */
public ChecksumData computeChecksum(ByteString byteString)
    throws OzoneChecksumException {
  return computeChecksum(byteString.toByteArray());
}

{code}",pushed-to-craterlake,['Ozone Client'],HDDS,Improvement,Major,2019-03-06 07:53:18,16
13219689,ozone-filesystem jar missing in hadoop classpath,hadoop-ozone-filesystem-lib-*.jar is missing in hadoop classpath.,pull-request-available,"['Ozone Filesystem', 'Ozone Manager']",HDDS,Bug,Major,2019-03-05 22:59:02,37
13219661,Provide docker-compose for OM HA,**This Jira proposes to add docker-compose file to run local pseudo cluster with OM HA (3 OM nodes).,pull-request-available,"['docker', 'OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-03-05 21:35:41,43
13219614,Restructure code to validate the response from server in the Read path,"In the read path, the validation of the response while reading the data from the datanodes happen in XceiverClientGrpc as well as additional  Checksum verification happens in Ozone client to verify the read chunk response. The aim of this Jira is to modify the function call to take a validator function as a part of reading data so all validation can happen in a single unified place.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-03-05 17:22:37,16
13219584,Remove TestContainerSQLCli unit test stub,"In HDDS-447 we removed the support the 'ozone noz' cli tool which was a rocksdb/leveldb to sql exporter.

But still we have the unit test for the tool (in fact only the skeleton of the unit test, as the main logic is removed). Even worse this unit test is failing as it calls System.exit:

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-tools: There are test failures.
[ERROR] 
[ERROR] Please refer to /testptch/hadoop/hadoop-ozone/tools/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
{code}

I think this test can be deleted.",pull-request-available,[],HDDS,Sub-task,Major,2019-03-05 15:33:59,6
13219550,TestContainerActionsHandler.testCloseContainerAction has an intermittent failure,"It's failed multiple times during the CI builds:

{code}
Error Message

Wanted but not invoked:
closeContainerEventHandler.onMessage(
    #1,
    org.apache.hadoop.hdds.server.events.EventQueue@3d3fcdb0
);
-> at org.apache.hadoop.hdds.scm.container.TestContainerActionsHandler.testCloseContainerAction(TestContainerActionsHandler.java:64)
Actually, there were zero interactions with this mock.
{code}

The fix is easy: we should call queue.processAll(1000L) to wait for the processing of all the events.
",pull-request-available,[],HDDS,Sub-task,Major,2019-03-05 12:44:36,6
13219415,Refactor ChillMode rules and chillmode manager,"# Make the chillmodeExitRule abstract class and move common logic for all rules into this.
 # Update test's for chill mode accordingly.",pull-request-available,[],HDDS,Improvement,Major,2019-03-04 22:29:36,13
13219377,Change name of ozoneManager service in docker compose files to om,"Change name of ozoneManager service in docker compose files to om for consistency. (secure ozone compose file will use ""om""). ",pull-request-available,[],HDDS,Sub-task,Major,2019-03-04 19:55:13,110
13219376,Change hadoop-runner and apache/hadoop base image to use Java8,"{code}

kms_1           | Exception in thread ""main"" java.lang.NoClassDefFoundError: javax/activation/DataSource

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.<clinit>(RuntimeBuiltinLeafInfoImpl.java:457)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.<init>(RuntimeTypeInfoSetImpl.java:65)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85)

kms_1           | at com.sun.xml.bind.v2.model.impl.ModelBuilder.<init>(ModelBuilder.java:156)

kms_1           | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.<init>(RuntimeModelBuilder.java:93)

kms_1           | at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473)

kms_1           | at com.sun.xml.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:319)

kms_1           | at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170)

kms_1           | at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145)

kms_1           | at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:236)

kms_1           | at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

kms_1           | at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

kms_1           | at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

kms_1           | at java.base/java.lang.reflect.Method.invoke(Method.java:566)

kms_1           | at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:186)

kms_1           | at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:146)

kms_1           | at javax.xml.bind.ContextFinder.find(ContextFinder.java:350)

kms_1           | at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:446)

kms_1           | at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:409)

kms_1           | at com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.<init>(WadlApplicationContextImpl.java:103)

kms_1           | at com.sun.jersey.server.impl.wadl.WadlFactory.init(WadlFactory.java:100)

kms_1           | at com.sun.jersey.server.impl.application.RootResourceUriRules.initWadl(RootResourceUriRules.java:169)

kms_1           | at com.sun.jersey.server.impl.application.RootResourceUriRules.<init>(RootResourceUriRules.java:106)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1359)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)

kms_1           | at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)

{code}",pull-request-available,[],HDDS,Sub-task,Blocker,2019-03-04 19:51:21,6
13219300,Enable tracing for the datanode read/write path,"HDDS-1150 introduced distributed for ozone components. But we have no trace context propagation between the clients and Ozone Datanodes.

As we use Grpc and Ratis on this RPC path the full tracing could be quite complex: we should propagate the trace id in Ratis and include it in all the log entries.

I propose a simplified solution here: to trace only the StateMachine operations.

As Ratis is a library we provide the implementation of the appropriate Raft elements especially the StateMachine and the raft messages. We can add the tracing information to the raft messages (in fact, we already have this field) and we can restore the tracing context during the StateMachine operations.

This approach is very simple (only a few lines of codes) and can show the time of the real write/read operations, but can't see the internals of the Ratis operations.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-03-04 14:23:10,6
13219278,Support plain text S3 MPU initialization request,"S3 Multi-Part-Upload (MPU) is implemented recently in the Ozone s3 gateway. We have extensive testing with using 'aws s3api' application which is passed.

But it turned out that the more simple `aws s3 cp` command fails with _405 Media type not supported error_ message

The root cause of this issue is the JAXRS implementation of the multipart upload method:

{code}
  @POST
  @Produces(MediaType.APPLICATION_XML)
  public Response multipartUpload(
      @PathParam(""bucket"") String bucket,
      @PathParam(""path"") String key,
      @QueryParam(""uploads"") String uploads,
      @QueryParam(""uploadId"") @DefaultValue("""") String uploadID,
      CompleteMultipartUploadRequest request) throws IOException, OS3Exception {
    if (!uploadID.equals("""")) {
      //Complete Multipart upload request.
      return completeMultipartUpload(bucket, key, uploadID, request);
    } else {
      // Initiate Multipart upload request.
      return initiateMultipartUpload(bucket, key);
    }
  }
{code}

Here we have a CompleteMultipartUploadRequest parameter which is created by the JAXRS framework based on the media type and the request body. With _Content-Type: application/xml_ it's easy: the JAXRS framework uses the built-in JAXB serialization. But with plain/text content-type it's not possible as there is no serialization support for CompleteMultipartUploadRequest from plain/text.

",pull-request-available,['S3'],HDDS,Bug,Blocker,2019-03-04 13:25:21,6
13218984,Test SCMChillMode failing randomly in Jenkins run,java.lang.Thread.State: TIMED_WAITING at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460) at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362) at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389) at org.apache.hadoop.ozone.om.TestScmChillMode.testSCMChillMode(TestScmChillMode.java:286) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74),pull-request-available pushed-to-craterlake,[],HDDS,Sub-task,Major,2019-03-01 19:12:07,13
13218860,Refactor Container Report Processing logic and plugin new Replication Manager,"HDDS-1205 brings in new ReplicationManager, this Jira is to refactor ContainerReportProcessing logic in SCM so that it complements ReplicationManager and plugin the new ReplicationManager code. ",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-03-01 10:49:45,19
13218850,Handle Datanode volume out of space,"steps taken :

--------------------
 # create 40 datanode cluster.
 # one of the datanodes has less than 5 GB space.
 # Started writing key of size 600MB.

operation failed:

Error on the client:

----------------------------
{noformat}
Fri Mar 1 09:05:28 UTC 2019 Ruuning /root/hadoop_trunk/ozone-0.4.0-SNAPSHOT/bin/ozone sh key put testvol172275910-1551431122-1/testbuck172275910-1551431122-1/test_file24 /root/test_files/test_file24
original md5sum a6de00c9284708585f5a99b0490b0b23
2019-03-01 09:05:39,142 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:39,578 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,368 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,450 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,457 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 does not exist
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:393)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,535 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,617 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,741 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,814 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-03-01 09:05:40,815 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 does not exist
 at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:393)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
java.nio.BufferOverflowException
 at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:189)
 at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:213)
 at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:128)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:307)
 at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:268)
 at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)
 at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
 at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:111)
 at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:53)
 at picocli.CommandLine.execute(CommandLine.java:919)
 at picocli.CommandLine.access$700(CommandLine.java:104)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)
 at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)
 at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)
 at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)
 at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)
 at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)
 at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)
 at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95){noformat}
 

ozone.log

-----------------

 
{noformat}
2019-03-01 09:05:33,248 [IPC Server handler 17 on 9889] DEBUG (OzoneManagerRequestHandler.java:137) - Received OMRequest: cmdType: CreateKey
traceID: ""5f169cde0a4c8a4e:79f0b64c3329c0ba:5f169cde0a4c8a4e:0""
clientId: ""client-86810A76C95E""
createKeyRequest {
 keyArgs {
 volumeName: ""testvol172275910-1551431122-1""
 bucketName: ""testbuck172275910-1551431122-1""
 keyName: ""test_file24""
 dataSize: 629145600
 type: RATIS
 factor: THREE
 isMultipartKey: false
 }
}
,
2019-03-01 09:05:33,255 [IPC Server handler 17 on 9889] DEBUG (KeyManagerImpl.java:465) - Key test_file24 allocated in volume testvol172275910-1551431122-1 bucket testbuck172275910-1551431122-1
2019-03-01 09:05:38,229 [IPC Server handler 8 on 9889] DEBUG (OzoneManagerRequestHandler.java:137) - Received OMRequest: cmdType: AllocateBlock
traceID: ""5f169cde0a4c8a4e:fe6c4bdb75978062:5f169cde0a4c8a4e:0""
clientId: ""client-86810A76C95E""
allocateBlockRequest {
 keyArgs {
 volumeName: ""testvol172275910-1551431122-1""
 bucketName: ""testbuck172275910-1551431122-1""
 keyName: ""test_file24""
 dataSize: 629145600
 }
 clientID: 20622763490697872
}
,
2019-03-01 09:05:38,739 [grpc-default-executor-17] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:38,790 [grpc-default-executor-17] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:38,800 [grpc-default-executor-17] DEBUG (ContainerStateMachine.java:358) - writeChunk writeStateMachineData : blockId containerID: 79
localID: 101674591075108132
blockCommitSequenceId: 0
 logIndex 3 chunkName f6508b585fbd0b834b2139939467ac03_stream_8101b9db-a724-4690-abe1-c7daa2630326_chunk_1
2019-03-01 09:05:38,801 [grpc-default-executor-17] DEBUG (ContainerStateMachine.java:365) - writeChunk writeStateMachineData completed: blockId containerID: 79
localID: 101674591075108132
blockCommitSequenceId: 0
 logIndex 3 chunkName f6508b585fbd0b834b2139939467ac03_stream_8101b9db-a724-4690-abe1-c7daa2630326_chunk_1
2019-03-01 09:05:38,978 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:0 detected, adding dummy entries
2019-03-01 09:05:38,979 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:1 detected, adding dummy entries
2019-03-01 09:05:38,980 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:2 detected, adding dummy entries
2019-03-01 09:05:38,981 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:38,981 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:39,357 [grpc-default-executor-18] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:896673a239485fcd:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE
2019-03-01 09:05:39,358 [grpc-default-executor-18] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:896673a239485fcd:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE
 
{noformat}
 ",Triaged,['Ozone Client'],HDDS,Bug,Major,2019-03-01 09:41:22,43
13218825,Refactor ReplicationManager to handle QUASI_CLOSED containers,"This Jira is for refactoring the ReplicationManager code to handle all the scenarios that are possible with the introduction of QUASI_CLOSED state of a container.

The new ReplicationManager will go through the complete set of containers in SCM to find out under/over replicated and unhealthy containers and takes appropriate action.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-03-01 06:50:41,19
13218808,Fix ClassNotFound issue with javax.xml.bind.DatatypeConverter used by DefaultProfile,"The ozonesecure docker-compose has been changed to use hadoop-runner image based on Java 11. Several packages/classes have been removed from Java 8 such as 

javax.xml.bind.DatatypeConverter.parseHexBinary

 This ticket is opened to fix issues running ozonesecure docker-compose on java 11.",pull-request-available,[],HDDS,Sub-task,Major,2019-03-01 05:12:30,28
13218805,Reporting Corruptions in Containers to SCM,"Add protocol message and handling to report container corruptions to the SCM.
Also add basic recovery handling in SCM.",pull-request-available,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Critical,2019-03-01 04:35:23,106
13218804,Ozone Data Scrubbing : Checksum verification for chunks,Background scrubber should read each chunk and verify the checksum.,pull-request-available,[],HDDS,Sub-task,Critical,2019-03-01 04:31:10,106
13218790,In Healthy Pipeline rule consider pipelines with all replicationType and replicationFactor,"In the current HealthyPipelineRule, we considered only pipeline type ratis and replication factor 3 pipelines for 10%.

 

This Jira is to consider all the pipelines with all replication factor for the 10% threshold. (Means each pipeline-type with factor should meet 10%)",TriagePending,['SCM'],HDDS,Bug,Major,2019-03-01 01:23:38,13
13218782,Rename chill mode to safe mode,"Let's go back to calling it safe mode. HDFS admins already understand what it means.

 ",newbie pull-request-available,[],HDDS,Improvement,Major,2019-03-01 00:40:55,86
13218769,Add a ChillMode handler class ,"Condition for to Start Replication monitor thread.
 # Exit Chill mode
 # Time out (configurable value) default to 5 minutes. Additional time out is added to give some additional time for datanodes to report and make pipelines healthy.

So, once we are out of chillmode, we fire ChillModeStatus, this ReplicationTimer Class will listen to that event, and wait for a configurable time, and then emit replicationEnabled.

 

The current code, when we are out of chill mode, we set replication enabled.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-28 23:25:00,13
13218762,Ozone Security Phase -2 ,This is sub-task that tracks work items in the phase-2 of security work items.,Triaged,['Security'],HDDS,Bug,Major,2019-02-28 22:18:17,102
13218720,Refactor ContainerChillModeRule and DatanodeChillMode rule,"The main intention of this Jira is to have all rules look in a similar way of handling events.

In this Jira, did the following changes:
 # Both DatanodeRule and ContainerRule implements EventHandler and listen for NodeRegistrationContainerReport
 # Update ScmChillModeManager not to handle any events. (As each rule need to handle an event, and work on that rule)",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-28 18:50:50,13
13218647,Support -conf command line argument in GenericCli,"org.apache.hadoop.hdds.GenericCli is the based class for all the Ozone related command line application. It supports to define custom configuration variables (-D or --set) but doesn't support the '--conf ozone-site.xml' argument to load an external xml file to the configuration.

Configuration and OzoneConfiguration classes load the ozone-site.xml from the classpath. But it makes very hard to start Ozone components in IDE as we can't modify the classpath easily. 

One option here is to support the --conf everywhere to make it possible to start ozone cluster in the IDE. 



Note: It's a nice to have for 0.4.0. I marked it as 0.5.0 but safe to commit at anytime to 0.4.0",newbie pull-request-available,[],HDDS,Improvement,Major,2019-02-28 14:39:37,115
13218566,Replace Ozone Rest client with S3 client in smoketests and docs,"As it's discussed in the the parent jira the rest support for Ozone Client protocol can be removed to use S3 Rest API instead of that.

Some of the unit tests are already disabled, so it seems to be better to remove it from the documentation (and from the smoketests).",pull-request-available,['test'],HDDS,Sub-task,Major,2019-02-28 08:28:56,6
13218453,Healthy pipeline Chill Mode rule to consider only pipelines with replication factor three,"Few offline comments from [~nandakumar131]
 # We should not process pipeline report from datanode again during calculations.
 # We should consider only replication factor 3 ratis pipelines.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-27 21:29:09,13
13218270,Support getDelegationToken API for OzoneFileSystem,This includes addDelegationToken/renewDelegationToken/cancelDelegationToken so that MR jobs can collect tokens correctly upon job submission time. ,pull-request-available,[],HDDS,Sub-task,Major,2019-02-27 06:59:55,28
13218198,Pipeline Rule where atleast one datanode is reported in the pipeline,"h2. Pipeline Rule with configurable percentage of pipelines with at least one datanode reported:

In this rule we consider when at least  90% of pipelines have at least one datanode reported. 

 

This rule satisfies, when we exit chill mode, Ozone clients will have at least one open replica for reads to succeed. (We can increase this threshold default from 90%, if we want to see fewer failures during reads after exit chill mode.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-26 23:44:49,13
13217960,Healthy pipeline Chill Mode Rule,"This Jira is to implement one of the chill mode rule.

*Pipeline Rule with configurable percentage of open pipelines with all datanodes reported*: In this rule, we consider when at least 10% of the pipelines have all 3 data node's reported (if it is a 3 node ratis ring), and one node reported if it is a one node ratis ring. (The percentage is configurable via ozone configuration parameter).

This rule satisfies, once the SCM is restarted, and after exiting chill mode, we have enough pipelines to give for clients for writes to succeed.

 ",pull-request-available,['SCM'],HDDS,Bug,Major,2019-02-26 01:14:14,13
13217900,Serve read requests directly from RocksDB,"We can directly server read requests from the OM's RocksDB instead of going through the Ratis server. OM should first check its role and only if it is the leader can it server read requests. 

There can be a scenario where an OM can lose its Leader status but not know about the new election in the ring. This OM could server stale reads for the duration of the heartbeat timeout but this should be acceptable (similar to how Standby Namenode could possibly server stale reads till it figures out the new status).",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-02-25 19:45:08,43
13217188,Disable failing test which are tracked by a separated jira,"As I wrote in the description of the parent Jira I propose to disable (@Ignore) the unit tests which are failing while we are fixing them to get clean Jira response from the PreCommit builds.

All the tests are tracked in a separated jira.",pull-request-available,['test'],HDDS,Sub-task,Major,2019-02-21 12:42:03,6
13217185,TestOzoneManagerHA.testTwoOMNodesDown is failing with ratis error,"h3. Error Message
{code:java}
org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-4D77D2A8F653->omNode-3@group-523986131536, cid=9, seq=0 RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$373/2067504307@6afa0221 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms){code}
Stacktrace
{code:java}
INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-4D77D2A8F653->omNode-3@group-523986131536, cid=9, seq=0 RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$373/2067504307@6afa0221 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:586) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createVolume(OzoneManagerProtocolClientSideTranslatorPB.java:230) at org.apache.hadoop.ozone.web.storage.DistributedStorageHandler.createVolume(DistributedStorageHandler.java:179) at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testCreateVolume(TestOzoneManagerHA.java:153) at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testTwoOMNodesDown(TestOzoneManagerHA.java:138) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}
 ",pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 12:32:24,43
13217183,testDelegationToken is failing in TestSecureOzoneCluster ,"TestSecureOzoneCluster.testDelegationTokenRenewal is failing:
{code:java}
java.lang.IllegalArgumentException: Invalid argument: token is null
	at org.apache.hadoop.ozone.protocolPB.OMPBHelper.convertToTokenProto(OMPBHelper.java:139)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.renewDelegationToken(OzoneManagerProtocolClientSideTranslatorPB.java:1062)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testDelegationToken(TestSecureOzoneCluster.java:438)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}
TestSecureOzoneCluster.testDelegationToken is failing:
{code:java}
java.lang.IllegalArgumentException: Invalid argument: token is null
	at org.apache.hadoop.ozone.protocolPB.OMPBHelper.convertToTokenProto(OMPBHelper.java:139)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.renewDelegationToken(OzoneManagerProtocolClientSideTranslatorPB.java:1062)
	at org.apache.hadoop.ozone.TestSecureOzoneCluster.testDelegationToken(TestSecureOzoneCluster.java:438)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}",pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 12:26:26,110
13217143,Add tracing to the client side of StorageContainerLocationProtocol and OzoneManagerProtocol,"The best way to display the network performance in the tracing server is to start a new span (=save tracing information) on both of the client and and server side.

This issue is about improving the client side tracing of OzoneManager and StorageLocationManager.

The easiest way to turn on tracing for a class which implements an interface is to create a java proxy with TracingUtil.createProxy. With this utility we can execute the required tracing methods (create span/close span) around the original methods without adding any boilerplate code.

Thanks to the current design hadoop rpc calls are create by the *ClientSideTranslatorPB classes which implements the original protocol interface (eg. StorageContainerLocationProtocolClientSideTranslatorPB implements StorageContainerLocationProtocol) which means that it can easily instrumented by TracingUtil.createProxy.

The only thing what we need is to use the interface everywhere (StorageContainerLocationProtocol) instead of the implementation (ClientSideTranslator) as a client.

The only required method which is not published in the ClientSideTranslator is the close method. With adding the close method to the interface (extends Closable) we are able to use the interface everywhere which can be instrumented to send the tracing information.",pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 09:28:20,6
13217130,Make tracing instrumentation configurable,"TracingUtil.createProxy is a helper method to create a proxy instance with tracing support.

The proxy instance implements the same interface as the original class and delegates all the method calls to the original instance but it also sends tracing information to the tracing server.

By default it's not a big overhead as the tracing libraries can be configured to send tracing only with some low probability.

But to make it more safe we can make it optional. With a global 'hdds.tracing.enabled' configuration variable (can be true by default) we can adjust the behavior of TracingUtil.createProxy.

If the tracing is disabled the TracingUtil.createProxy should return with the 'delegate' parameter instead of a proxy.",newbie pull-request-available,[],HDDS,Bug,Major,2019-02-21 08:38:24,115
13217127,Add trace information for the client side of the datanode writes,The XCeiverClients can be traced on the client side to get some information about the time of chunk writes / block puts.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 08:32:18,6
13217126,Propagate the tracing id in ScmBlockLocationProtocol,The tracing propagation is not yet enabled for the ScmBlockLocationProtocol. We can't see the internal calls between OM and SCM. We need to propagate it at least for the allocateBlock call.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-21 08:29:38,6
13217050,"After allocating container, we are not adding to container DB.","If we don't do that, we get an error when handling container report for open containers.

As they don't exist in container DB.

 
{code:java}
scm_1           | at java.lang.Thread.run(Thread.java:748)
scm_1           | 2019-02-21 00:00:32 ERROR ContainerReportHandler:173 - Received container report for an unknown container 1 from datanode e2733c00-162b-4993-a986-f6104f5008d8{ip: 172.18.0.2, host: 4f4e683d86c3} {}
scm_1           | org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #1
scm_1           | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:543)
scm_1           | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.updateContainerReplica(ContainerStateMap.java:230)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerStateManager.updateContainerReplica(ContainerStateManager.java:565)
scm_1           | at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerReplica(SCMContainerManager.java:393)
scm_1           | at org.apache.hadoop.hdds.scm.container.ReportHandlerHelper.processContainerReplica(ReportHandlerHelper.java:74)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:159)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:110)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:51)
scm_1           | at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm_1           | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
scm_1           | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
{code}
 

 ",pull-request-available,['SCM'],HDDS,Bug,Major,2019-02-21 01:04:01,13
13217011,Adding container related metrics in SCM,"This jira aims to add more container related metrics to SCM.
 Following metrics will be added as part of this jira:
 * Number of containers
 * Number of open containers
 * Number of closed containers
 * Number of quasi closed containers
 * Number of closing containers

Above are already handled in HDDS-918.
 * Number of successful create container calls
 * Number of failed create container calls
 * Number of successful delete container calls
 * Number of failed delete container calls

Handled in HDDS-2193.
 * Number of successful container report processing
 * Number of failed container report processing
 * Number of successful incremental container report processing
 * Number of failed incremental container report processing

These will be handled in this jira.",pull-request-available,['SCM'],HDDS,Improvement,Major,2019-02-20 19:25:12,13
13216977,Add optional web server to the Ozone freon test tool,"Recently we improved the default HttpServer to support prometheus monitoring and java profiling.

It would be very useful to enable the same options for freon testing:

 1. We need a simple way to profile freon and check the problems

 2. Long running freons should be monitored

We can create a new optional FreonHttpServer which includes all the required servlets by default.",pull-request-available,['Tools'],HDDS,Improvement,Major,2019-02-20 16:49:29,6
13216932,Ensure stateMachineData to be evicted only after writeStateMachineData completes in ContainerStateMachine cache,"Currently, when we write StateMachineData, we first write to cache followed by write to disk. The entry in the cache can get evicted while the actual write is happening in case write is very slow. The purpose of this Jira is to ensure the cache eviction only after writeChunk completes.",TriagePending,['Ozone Datanode'],HDDS,Improvement,Major,2019-02-20 13:57:07,16
13216742,Update DBCheckpointSnapshot to DBCheckpoint,"Instead of DBCheckpointSnapshot to DBCheckpoint, as internally we used RocksDBCheckpoint. It is little confusing to have both checkpoint and snapshot. And update similarly in other places.",pull-request-available,[],HDDS,Sub-task,Minor,2019-02-19 20:19:05,13
13216737,Fix findbugs issues caused by HDDS-1085.,The list of issues can be found here - https://ci.anzix.net/job/ozone/106/findbugs/.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-19 20:07:09,30
13216728,OzoneManager should return the pipeline info of the allocated block along with block info,"Currently, while a block is allocated from OM, the request is forwarded to SCM. However, even though the pipeline information is present with the OM for block allocation, this information is passed through to the client.

This optimization will help in reducing the number of hops for the client by reducing 1 RPC round trip for each block allocated.",pull-request-available,"['Ozone Client', 'Ozone Manager']",HDDS,Bug,Major,2019-02-19 19:13:19,28
13216724,Add metric counters to capture the RocksDB checkpointing statistics.,"As per the discussion with [~anu] on HDDS-1085, this JIRA tracks the effort to add metric counters to capture ROcksDB checkpointing performance. 

From [~anu]'s comments, it might be interesting to have 3 counters – or a map of counters.
* How much time are we taking for each CheckPoint
* How much time are we taking for each Tar operation – along with sizes
* How much time are we taking for the transfer.",pull-request-available,['Ozone Recon'],HDDS,Sub-task,Major,2019-02-19 19:09:13,30
13216623,OzoneFileSystem#create should allocate alteast one block for future writes.,"While opening a new key, OM should at least allocate one block for the key, this should be done in case the client is not sure about the number of block. However for users of OzoneFS, if the key is being created for a directory, then no blocks should be allocated.",TriagePending,['Ozone Manager'],HDDS,Bug,Major,2019-02-19 12:46:01,18
13216571,Create robot test for Ozone TDE support,"HDDS-1041 implemented TDE for Ozone and added the KMS server to the compose/ozonesecure cluster definition.

We need a simple robot framework based test to try out TDE from command line.

This task requires a working ozonesecure docker-compose cluster first.",beta1 newbie,[],HDDS,Test,Major,2019-02-19 09:39:11,28
13216570,Ozone serialization codec for Ozone S3 secret table,"HDDS-748/HDDS-864 introduced an option to use strongly typed metadata tables and separated the serialization/deserialization logic to separated codec implementation

HDDS-937 introduced a new S3 secret table which is not codec based.

I propose to use codecs for this table.

In OzoneMetadataManager the return value of getS3SecretTable() should be changed from Table<byte[],byte[]> to Table<S3SecretKey,S3SecretValue>. 

The encoding/decoding logic of S3SecretValue should be registered in ~OzoneMetadataManagerImpl:L204

As the codecs are type based we may need a wrapper class to encode the String kerberos id with md5: class S3SecretKey(String name = kerberodId). Long term we can modify the S3SecretKey to support multiple keys for the same kerberos id.

 

 ",newbie,"['Ozone Manager', 'S3']",HDDS,Sub-task,Major,2019-02-19 09:36:09,116
13216440,Fix findbug/checkstyle errors in hadoop-hdds projects,"HDDS-1114 fixed all the findbug/checkstyle problems but in the mean time new patches are committed with newer error.

Here I would like to cleanup the projects again.

(Except the static field in RatisPipelineProvider which will be ignored in this patch and tracked in HDDS-1128)",pull-request-available,[],HDDS,Bug,Major,2019-02-18 17:39:16,6
13216066,Add a config to disable checksum verification during read even though checksum data is present in the persisted data,"Currently, if the checksum is computed during data write and persisted in the disk, we will always end up verifying it while reading. This Jira aims to selectively disable checksum verification during reads even though checksum info is present in the data stored.",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2019-02-15 19:57:20,13
13216051,DN get OM certificate from SCM CA for block token validation,This is needed when the DN received block token signed by OM and it does not have the certificate that OM.,pull-request-available,[],HDDS,Sub-task,Blocker,2019-02-15 18:41:34,110
13216050,OM get the certificate from SCM CA for token validation,This is needed when the OM received delegation token signed by other OM instances and it does not have the certificate for foreign OM.,newbie,[],HDDS,Sub-task,Major,2019-02-15 18:40:00,28
13216000,Add async profiler to the hadoop-runner base container image,"HDDS-1116 provides a simple servlet to execute async profiler (https://github.com/jvm-profiling-tools/async-profiler) thanks to the Hive developers.

To run it in the docker-composed based example environments we should add it to the apache/hadoop-runner base image. 

Note: The size is not significant, the downloadable package is 102k.

",pull-request-available,['docker'],HDDS,Improvement,Major,2019-02-15 14:45:40,6
13215998,Add java profiler servlet to the Ozone web servers,"Thanks to [~gopalv] we learned that [~prasanth_j] implemented a helper servlet in Hive to initialize new [async profiler|https://github.com/jvm-profiling-tools/async-profiler] sessions and provide the svg based flame graph over HTTP. (see HIVE-20202)

It seems to very useful as with this approach the profiling could be very easy.

This patch imports the servlet from the Hive code base to the Ozone code base with minor modification (to make it work with our servlet containers)

 * The two servlets are unified to one
 * Streaming the svg to the browser based on IOUtils.copy 
 * Output message is improved

By default the profile servlet is turned off, but you can enable it with 'hdds.profiler.endpoint.enabled=true' ozone-site.xml settings. In that case you can access the /prof endpoint from scm,om,s3g. 

You should upload the async profiler first (https://github.com/jvm-profiling-tools/async-profiler) and set the ASYNC_PROFILER_HOME environment variable to find it. ",pull-request-available,[],HDDS,Improvement,Major,2019-02-15 14:40:03,6
13215974,Provide ozone specific top-level pom.xml,"Ozone build process doesn't require the pom.xml in the top level hadoop directory as we use hadoop 3.2 artifacts as parents of hadoop-ozone and hadoop-hdds. The ./pom.xml is used only to include the hadoop-ozone/hadoop-hdds projects in the maven reactor.

From command line, it's easy to build only the ozone artifacts:

{code}
mvn clean install -Phdds  -am -pl :hadoop-ozone-dist  -Danimal.sniffer.skip=true  -Denforcer.skip=true
{code}

Where: '-pl' defines the build of the hadoop-ozone-dist project
and '-am' defines to build all of the dependencies from the source tree (hadoop-ozone-common, hadoop-hdds-common, etc.)

But this filtering is available only from the command line.

With providing a lightweight pom.ozone.xml we can achieve the same:

 * We can open only hdds/ozone projects in the IDE/intellij. It makes the development faster as IDE doesn't need to reindex all the sources all the time + it's easy to execute checkstyle/findbugs plugins of the intellij to the whole project.
 * Longer term we should create an ozone specific source artifact (currently the source artifact for hadoop and ozone releases are the same) which also requires a simplified pom.

In this patch I also added the .mvn directory to the .gitignore file.

With 
{code}
mkdir -p .mvn && echo ""-f ozone.pom.xml"" > .mvn/maven.config"" you can persist the usage of the ozone.pom.xml for all the subsequent builds (in the same dir)

How to test?

Just do a 'mvn -f ozonze.pom.xml clean install -DskipTests'",pull-request-available,['build'],HDDS,Improvement,Major,2019-02-15 12:35:25,6
13215973,Fix findbugs/checkstyle/accepteance errors in Ozone,"Unfortunately as the previous two big commits (error handling HDDS-1068, checkstyle HDDS-1103) are committed in the same time a few new errors are introduced during the rebase.

This patch will fix the remaining 5 issues (+ a type in the acceptance test executor) ",pull-request-available,[],HDDS,Bug,Major,2019-02-15 12:25:07,6
13215917,Remove default dependencies from hadoop-ozone project,"There are two ways to define common dependencies with maven:

  1.) put all the dependencies to the parent project and inherit them
  2.) get all the dependencies via transitive dependencies

TLDR; I would like to switch from 1 to 2 in hadoop-ozone

My main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.

Using transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them. ",pull-request-available,['build'],HDDS,Improvement,Major,2019-02-15 08:57:06,6
13215851,OzoneManager NPE reading private key file.,"{code}

ozoneManager_1  | 2019-02-14 23:21:51 ERROR OzoneManager:596 - Unable to read key pair for OM.

ozoneManager_1  | org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:638)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:594)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManagerIfNecessary(OzoneManager.java:1216)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.start(OzoneManager.java:1007)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:768)

ozoneManager_1  | Caused by: java.lang.NullPointerException

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:635)

ozoneManager_1  | ... 4 more

ozoneManager_1  | 2019-02-14 23:21:51 ERROR OzoneManager:772 - Failed to start the OzoneManager.

ozoneManager_1  | java.lang.RuntimeException: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:597)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManagerIfNecessary(OzoneManager.java:1216)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.start(OzoneManager.java:1007)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:768)

ozoneManager_1  | Caused by: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:638)

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:594)

ozoneManager_1  | ... 3 more

ozoneManager_1  | Caused by: java.lang.NullPointerException

ozoneManager_1  | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:635)

ozoneManager_1  | ... 4 more

ozoneManager_1  | 2019-02-14 23:21:51 INFO  ExitUtil:210 - Exiting with status 1: java.lang.RuntimeException: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager

ozoneManager_1  | 2019-02-14 23:21:51 INFO  OzoneManager:51 - SHUTDOWN_MSG: 

{code}",test-badlands,[],HDDS,Sub-task,Major,2019-02-14 23:26:15,28
13215751,Add mechanism in Recon to obtain DB snapshot 'delta' updates from Ozone Manager.,"*Some context*

The FSCK server will periodically invoke this OM API passing in the most recent sequence number of its own RocksDB instance. The OM will use the RockDB getUpdateSince() API to answer this query. Since the getUpdateSince API only works against the RocksDB WAL, we have to configure OM RocksDB WAL (https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log) with sufficient max size to make this API useful. If the OM cannot get all transactions since the given sequence number (due to WAL flushing), it can error out. In that case the FSCK server can fall back to getting the entire checkpoint snapshot implemented in HDDS-1085.",pull-request-available,[],HDDS,Sub-task,Major,2019-02-14 14:21:35,30
13215748,Use picocli with Ozone genesis tool,"Ozone genesis is a command line microbenchmark tool for ozone.

With other ozone tools (eg with freon) we already switched to use picocli for command line argument parsing.

It would be great to use picocli from org.apache.hadoop.ozone.genesis.Genesis as well.

We need at least a ""–filter"" option to run only the filtered tests (this is a generic use case and the users usually comment out the unnecessary tests and the commented lines can be accidentally committed)

Freon can be used as an example, but here we don't need any subcommands.",newbie,['Tools'],HDDS,Improvement,Major,2019-02-14 14:00:48,35
13215734,Fix rat/findbug/checkstyle errors in ozone/hdds projects,"Due to the partial Yetus checks (see HDDS-891) recent patches and merge introduced many new checkstyle/rat/findbugs errors.

I would like to fix them all.",pull-request-available,[],HDDS,Bug,Major,2019-02-14 12:57:53,6
13215716,Confusing error log when datanode tries to connect to a destroyed pipeline,"steps taken:

--------------------
 # created 5 datanode cluster.
 # shutdown 2 datanodes
 # started the datanodes again.

One of the datanodes was shut down.

exception seen :

 
{noformat}
2019-02-14 07:37:26 INFO LeaderElection:230 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8 got exception when requesting votes: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.
 at java.util.concurrent.FutureTask.report(FutureTask.java:122)
 at java.util.concurrent.FutureTask.get(FutureTask.java:192)
 at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:214)
 at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:146)
 at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:102)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
 at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
 at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:83)
 at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:187)
 at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$0(LeaderElection.java:188)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2019-02-14 07:37:26 INFO LeaderElection:46 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: Election PASSED; received 1 response(s) [6a0522ba-019e-4b77-ac1f-a9322cd525b8<-61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5#0:OK-t7] and 1 exception(s); 6a0522ba-019e-4b77-ac1f-a9322cd525b8:t7, leader=null, voted=6a0522ba-019e-4b77-ac1f-a9322cd525b8, raftlog=6a0522ba-019e-4b77-ac1f-a9322cd525b8-SegmentedRaftLog:OPENED, conf=3: [61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5:172.20.0.8:9858, 6a0522ba-019e-4b77-ac1f-a9322cd525b8:172.20.0.6:9858, 0f377918-aafa-4d8a-972a-6ead54048fba:172.20.0.3:9858], old=null
2019-02-14 07:37:26 INFO LeaderElection:52 - 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.
2019-02-14 07:37:26 INFO RoleInfo:130 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: shutdown LeaderElection
2019-02-14 07:37:26 INFO RaftServerImpl:161 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8 changes role from CANDIDATE to LEADER at term 7 for changeToLeader
2019-02-14 07:37:26 INFO RaftServerImpl:258 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: change Leader from null to 6a0522ba-019e-4b77-ac1f-a9322cd525b8 at term 7 for becomeLeader, leader elected after 1066ms
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-02-14 07:37:26 INFO GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-02-14 07:37:26 INFO GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)
2019-02-14 07:37:26 INFO RoleInfo:139 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: start LeaderState
2019-02-14 07:37:26 INFO RaftLogWorker:303 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8-RaftLogWorker: Rolling segment log-3_4 to index:4
2019-02-14 07:37:26 INFO RaftLogWorker:403 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8-RaftLogWorker: Rolled log segment from /data/metadata/ratis/134f574e-c1b0-4556-a206-fd6fa533f1fb/current/log_inprogress_3 to /data/metadata/ratis/134f574e-c1b0-4556-a206-fd6fa533f1fb/current/log_3-4
2019-02-14 07:37:26 INFO RaftServerImpl:354 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: set configuration 5: [61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5:172.20.0.8:9858, 6a0522ba-019e-4b77-ac1f-a9322cd525b8:172.20.0.6:9858, 0f377918-aafa-4d8a-972a-6ead54048fba:172.20.0.3:9858], old=null at 5
/opt/starter.sh: line 162: 13 Killed $@
 
 
{noformat}
 ",TriagePending newbie pushed-to-craterlake test-badlands,['Ozone Datanode'],HDDS,Bug,Critical,2019-02-14 10:32:29,16
13215281,Performance test infrastructure : skip writing user data on Datanode,"Goal:
It can be useful to exercise the IO and control paths in Ozone for simulated large datasets without having huge disk capacity at hand. For example, this will allow us to get things like container reports and incremental container reports, while not needing huge cluster capacity. The [SimulatedFsDataset|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java] does something similar in HDFS. It has been an invaluable tool to simulate large data stores.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-02-12 16:01:46,88
13215258,Configuration tab in OM/SCM ui is not displaying the correct values,"Configuration tab in OM/SCM ui is not displaying the correct/configured values, rather it is displaying the default values.

!image-2019-02-12-19-47-18-332.png!
{code:java}
[hdfs@freonnode10 hadoop]$ curl -s http://freonnode10:9874/conf | grep ozone.om.handler.count.key
<property><name>ozone.om.handler.count.key</name><value>40</value><final>false</final><source>ozone-site.xml</source></property>
{code}",pull-request-available,"['Ozone Manager', 'SCM']",HDDS,Bug,Critical,2019-02-12 14:19:31,37
13215248,Use Java 11 JRE to run Ozone in containers,"As of now we use opendk 1.8.0 in the Ozone containers.

Java 9 and Java 10 introduces advanced support for the resource management of the containers and not all of them are available from the latest release of 1.8.0. (see this blog for more details: https://medium.com/adorsys/jvm-memory-settings-in-a-container-environment-64b0840e1d9e)

I propose to switch to use Java 11 in the containers and test everything with Java 11 at runtime.

Note: this issue is just about the runtime jdk not about the compile time JDK.",pull-request-available,[],HDDS,Sub-task,Major,2019-02-12 13:23:14,6
13215227,Disable OzoneFSStorageStatistics for hadoop versions older than 2.8,"HDDS-1033 introduced OzoneFSStorageStatistics for OzoneFileSystem. It uses the StorageStatistics which is introduced in HADOOP-13065 (available from the hadoop2.8/3.0).

Using older hadoop (for example hadoop-2.7 which is included in the spark distributions) is not possible any more even with using the isolated class loader (introduced in HDDS-922).

Fortunately it can be fixed:
 # We can support null in storageStatistics field with checking everywhere before call it.
 # We can create a new constructor of OzoneClientAdapterImpl without using OzoneFSStorageStatistics): If OzoneFSStorageStatistics is not in the method/constructor signature we don't need to load it.
 # We can check the availability of HADOOP-13065 and if the classes are not in the classpath we can skip the initialization of the OzoneFSStorageStatistics",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2019-02-12 12:35:26,6
13215141,Fix TestDefaultCertificateClient#testSignDataStream,"Investigate failure of TestDefaultCertificateClient#testSignDataStream in jenkins run. 

https://builds.apache.org/job/PreCommit-HDDS-Build/2217/testReport/org.apache.hadoop.hdds.security.x509.certificate.client/TestDefaultCertificateClient/testSignDataStream/

",blocker pull-request-available security,[],HDDS,Sub-task,Major,2019-02-12 03:05:59,28
13215120,Create an OM API to serve snapshots to Recon server,"We need to add an API to OM so that we can serve snapshots from the OM server.
 - The snapshot should be streamed to fsck server with the ability to throttle network utilization (like TransferFsImage)",pull-request-available,[],HDDS,Sub-task,Major,2019-02-11 23:45:23,30
13214494,Implement RetryProxy and FailoverProxy for OM client,"RPC Client should implement a retry and failover proxy provider to failover between OM Ratis clients. The failover should occur in two scenarios:
# When the client is unable to connect to the OM (either because of network issues or because the OM is down). The client retry proxy provider should failover to next OM in the cluster.
# When OM Ratis Client receives a response from the Ratis server for its request, it also gets the LeaderId of server which processed this request (the current Leader OM nodeId). This information should be propagated back to the client. The Client failover Proxy provider should failover to the leader OM node. This helps avoid an extra hop from Follower OM Ratis Client to Leader OM Ratis server for every request.",pull-request-available,"['OM HA', 'Ozone Manager']",HDDS,Sub-task,Major,2019-02-08 00:06:52,43
13214365,Temporarily disable the security acceptance tests by default in Ozone,"Because HDDS-1019 and HDDS-1018 (and HDDS-1038?) the security acceptance tests are not working. 

I propose to remove them from daily tests as currently we can't get any meaningful results from Jenkins.

Security tests can be run manually with the test.sh script.",pull-request-available,"['Security', 'test']",HDDS,Improvement,Major,2019-02-07 13:19:31,6
13214364,Improve the error propagation for ozone sh,"As of now the server side (om, scm) errors are not propagated to the client.

For example if ozone is started with one single datanode:

{code}
docker-compose exec ozoneManager ozone sh key  put -r THREE /vol1/bucket1/test2 NOTICE.txt             
Create key failed, error:KEY_ALLOCATION_ERROR
{code}

There is no information here about the missing datanodes, or missing pipelines.

There are multiple problems which should be fixed:

1. type safety

In ScmBlockLocationProtocolClientSideTranslatorPB the server (om) side exceptions are transformed to IOException where the original status is added to the message: 

For example:

{code}
 throw new IOException(""Volume quota change failed, error:"" + resp.getStatus());
{code}

In s3 gateway it's very hard to handle the different errors in a proper way. The current code:

{code}
if (!ex.getMessage().contains(""KEY_NOT_FOUND"")) {
            result.addError(
                new Error(keyToDelete.getKey(), ""InternalError"",
                    ex.getMessage()));
{code}

2. message

The exception message is not propagated in the om response just the status code

3. status code and error message are handled in a different way

To propagate error code and status code to the client we need to handle them in the same way.  But the Status field is part of the specific response objects (CreateVolumeRequest) and not the OMRequest. I propose to put both StatusCode and error message to the OMRequest.

4. The status codes in OzoneManagerProtocol.proto/Status enum is not in sync with OmException.ResultCodes.

It would be easy to use the same strings for both enums. With a unit test we can ensure that they have the same names in the same order.",pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2019-02-07 13:11:21,6
13214290,OM and DN should persist SCM certificate as the trust root,OM and DN should persist SCM certificate as the trust root.,pull-request-available,[],HDDS,Sub-task,Major,2019-02-07 01:12:54,110
13214247,Add API to get OM certificate from SCM CA,"Datanodes/OM need OM certificate to validate block tokens and delegation tokens. 
Add API for:
1. getCertificate(String certSerialId): To get certificate from SCM based on certificate serial id.
2. getCACertificate(): To get CA certificate.

",Blocker Security,[],HDDS,Sub-task,Major,2019-02-06 20:39:46,110
13214065,Refactor om token db as column family.,"Currently OM stores delegation tokens in a separate db. i.e ""om-token.db"". We should refactor this to be column family storing data in existing om db file.",Security blocker,[],HDDS,Sub-task,Major,2019-02-06 02:51:50,102
13214063,List Multipart uploads in a bucket,"This Jira is to implement in ozone to list of in-progress multipart uploads in a bucket.

[https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html]

 ",pull-request-available,[],HDDS,Sub-task,Blocker,2019-02-06 02:39:09,6
13213646,TestCloseContainerByPipeline#testIfCloseContainerCommandHandlerIsInvoked fails intermittently," 
{code:java}
java.lang.StackOverflowError
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.getSubject(Subject.java:297)
at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:569)
at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getEncodedBlockToken(ContainerProtocolCalls.java:578)
at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:318)
at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:602)
at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:464)
at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:480)
at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:137)
at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:489)
at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:501)
at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:501)
{code}
The failure is happening because, ozone client receives a CONTAINER_NOT+OPEN exception from daranode, and it allocates a new and retries to write. But every allocate block call to SCM allocates a block on the same quasi closed container and hence client retries indefinitely and ultimately runs out of stack space.

Logs below indicate 3 successive block allocations from SCM in quasi closed container.
{code:java}
15:15:26.812 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189852894070 bcId: 0} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state

15:15:26.818 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189853352823 bcId: 0} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state

15:15:26.825 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189853746040 bcId: 0} | ret=FAILURE
org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state
{code}
 

 

 ",newbie,['SCM'],HDDS,Improvement,Major,2019-02-04 09:57:07,16
13213434,Enable token based authentication for S3 api,Ozone has a  S3 api and mechanism to create S3 like secrets for user. This jira proposes hadoop compatible token based authentication for S3 api which utilizes S3 secret stored in OM.,pull-request-available security,[],HDDS,Sub-task,Blocker,2019-02-02 00:45:20,110
13213172,OzoneManager fails to connect with secure SCM,"In a secure Ozone cluster. OzoneManager fail to connect to SCM on {{SCMBlockLocationProtocol}}. 
",Security,[],HDDS,Sub-task,Major,2019-01-31 20:18:15,110
13213168,Support Service Level Authorization for Ozone,In a secure Ozone cluster. Datanodes fail to connect to SCM on {{StorageContainerDatanodeProtocol}}. ,Security,[],HDDS,Sub-task,Major,2019-01-31 20:04:43,28
13212686,Allow option for force in DeleteContainerCommand,"Right now, we check container state if it is not open, and then we delete container.

We need a way to delete the containers which are open, so adding a force flag will allow deleting a container without any state checks. (This is required for delete replica's when SCM detects over-replicated, and that container to delete can be in open state)",pull-request-available,[],HDDS,Bug,Major,2019-01-30 00:56:16,13
13212367,Handle replication of closed containers in DeadNodeHanlder,"This Jira is to do one of the TODO mentioned in the DeadNodeHandler

// TODO: Check replica count and call replication manager.

 

As Right now, when a node is dead, replication for the closed containers is not triggered.",pull-request-available,[],HDDS,Bug,Major,2019-01-29 01:11:29,13
13212335,Handle DeleteContainerCommand in the SCMDatanodeProtocolServer,"Right now, in the SCMDatanodeProtocolServer getCommandResponse() deleteContainerCommand is not handled, so deleteContainerCommand is not sent to Datanode.

 

The deletecontainercommand request is sent for over replicated containers, so this over replication is currently broken because of this.

 

Because of this we see below error:

 
{code:java}
java.lang.IllegalArgumentException: Not implemented
 at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.getCommandResponse(SCMDatanodeProtocolServer.java:345)
 at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.sendHeartbeat(SCMDatanodeProtocolServer.java:272)
 at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolServerSideTranslatorPB.sendHeartbeat(StorageContainerDatanodeProtocolServerSideTranslatorPB.java:88)
 at org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos$StorageContainerDatanodeProtocolService$2.callBlockingMethod(StorageContainerDatanodeProtocolProtos.java:27753)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}
 ",SCM,[],HDDS,Bug,Major,2019-01-28 21:50:39,13
13212235,Update the base image of krb5 container for the secure ozone cluster ,"The krb5 image based on a base image which is no longer available from dockerhub

See: hadoop-ozone/dist/src/main/compose/ozonesecure/docker-image/docker-krb5/Dockerfile-krb5

Dockerfile:
FROM frolvlad/alpine-oraclejdk8:slim

https://hub.docker.com/r/frolvlad/alpine-oraclejdk8

I propose to switch to openjdk:8u191-jdk-alpine3.8 but it should be tested with the acceptance test suite (A java base image is required as the issuer application creates keystores/truststores on the fly with the help of java keytool.",newbie,[],HDDS,Bug,Major,2019-01-28 13:14:59,28
13212232,Use distributed tracing to indentify performance problems in Ozone,"In the recent months multiple performance issues are resolved in OM/SCM and datanode sides. To identify the remaining problems a distributed tracing framework would help a lot.

In HADOOP-15566 there is an ongoing discussion to remove the discontinued HTrace and use something else instead. Until now without any conclusion, but

 1). There is one existing poc in the jira which uses opentracing
 2). It was suggested to ""evaluating all the options"" before a final decision 

As an evaluation step we would like to investigate the performance of ozone components with opentracing. This patch can help us to find the performance problem but can be reverted when we will have a final solution in HADOOP-15566 about the common tracing library.

To make it lightweight we can use the ozone message level tracing identifier for context propagation instead of modifying the existing hadoop rpc framework. 

",pull-request-available,[],HDDS,Sub-task,Major,2019-01-28 12:51:22,6
13211906,Add Default CertificateClient implementation,Add Default CertificateClient implementation,Blocker,[],HDDS,Sub-task,Major,2019-01-25 18:51:22,110
13211686,Invalidate closed container replicas on a failed volume,"When a volume is detected as failed, all closed containers on the volume should be marked as invalid.

Open containers will be handled separately.",pull-request-available,['Ozone Datanode'],HDDS,Improvement,Major,2019-01-25 00:48:19,35
13211430,Switch Hadoop version to 3.2.0,"Now that Hadoop 3.2.0 is released, we should be able to switch the hadoop.version from 3.2.1-SNAPSHOT to 3.2.0.

We should run all unit/acceptance tests manually once after making the change. Not sure Jenkins will do that.",pull-request-available,[],HDDS,Improvement,Major,2019-01-24 02:02:56,88
13211384,Write a tool to dump DataNode RocksDB in human-readable format,"It would be good to have a command-line tool that can dump the contents of a DataNode RocksDB file in human-readable format e.g. YAML.
",Triaged,[],HDDS,Improvement,Major,2019-01-23 21:21:32,88
13211331,Make the DNS resolution in OzoneManager more resilient,"If the OzoneManager is started before scm the scm dns may not be available. In this case the om should retry and re-resolve the dns, but as of now it throws an exception:
{code:java}
2019-01-23 17:14:25 ERROR OzoneManager:593 - Failed to start the OzoneManager.
java.net.SocketException: Call From om-0.om to null:0 failed on socket exception: java.net.SocketException: Unresolved address; For more details see:  http://wiki.apache.org/hadoop/SocketException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:798)
    at org.apache.hadoop.ipc.Server.bind(Server.java:566)
    at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1042)
    at org.apache.hadoop.ipc.Server.<init>(Server.java:2815)
    at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:994)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:421)
    at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:342)
    at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:804)
    at org.apache.hadoop.ozone.om.OzoneManager.startRpcServer(OzoneManager.java:563)
    at org.apache.hadoop.ozone.om.OzoneManager.getRpcServer(OzoneManager.java:927)
    at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:265)
    at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:674)
    at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:587)
Caused by: java.net.SocketException: Unresolved address
    at sun.nio.ch.Net.translateToSocketException(Net.java:131)
    at sun.nio.ch.Net.translateException(Net.java:157)
    at sun.nio.ch.Net.translateException(Net.java:163)
    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:76)
    at org.apache.hadoop.ipc.Server.bind(Server.java:549)
    ... 11 more
Caused by: java.nio.channels.UnresolvedAddressException
    at sun.nio.ch.Net.checkAddress(Net.java:101)
    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:218)
    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
    ... 12 more{code}
It should be fixed. (See also HDDS-421 which fixed the same problem in datanode side and HDDS-907 which is the workaround while this issue is not resolved).",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2019-01-23 17:19:12,86
13210490,MultipartUpload: S3API for list parts of a object,"https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html
",pull-request-available,[],HDDS,Sub-task,Major,2019-01-18 22:52:40,13
13209734,Fix typo in doc : Client > S3 section,"Noticed a typo where _mapped_ has been misspelt as _maped_ in Ozone Docs > Client > S3 page.
This is not a functional issue but was bothering me :)",ozoneDoc,[],HDDS,Bug,Minor,2019-01-15 21:41:34,81
13208957,Exclude dependency-reduced-pom.xml from ozone rat check,"As of now we have one (false positive) rat violation:

 
{code:java}
hadoop-ozone/ozonefs/target/rat.txt: !????? /home/elek/projects/hadoop/hadoop-ozone/ozonefs/dependency-reduced-pom.xml
{code}
As it's generated during the build, it could be safely ignored.",pull-request-available,[],HDDS,Bug,Trivial,2019-01-11 09:32:59,6
13208874,Add getServiceAddress method to ServiceInfo and use it in TestOzoneShell,"This jira has been filed based on [~ajayydv]'s [review comment |https://issues.apache.org/jira/browse/HDDS-960?focusedCommentId=16739807&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16739807]on HDDS-960

1. Add a method getServiceAddress(ServicePort port) in ServiceInfo
2. Use this method in TestOzoneShell in place of following snippet:

{code:java}
String omHostName = services.stream().filter(
        a -> a.getNodeType().equals(HddsProtos.NodeType.OM))
        .collect(Collectors.toList()).get(0).getHostname();
{code}
",newbie,[],HDDS,Improvement,Major,2019-01-10 22:12:25,1
13207970,Ozone: checkstyle improvements and code quality scripts,Experimental scripts to test github pr capabilities after the github url move. The provided scripts are easier to use locally and provides more strict/focused checks then the existing pre-commit scripts. But this is not a replacements of the existing yetus build as it adds additional (more strict) checks. ,pull-request-available,[],HDDS,New Feature,Major,2019-01-07 08:36:16,6
13207339,MultipartUpload: List Parts for a Multipart upload key,"This Jira is to implement backend to support API in S3 for list parts for an object.

https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html",pull-request-available,[],HDDS,Sub-task,Major,2019-01-02 19:54:24,13
13207020,Improve KeyHandler#putKey readability,"Remove magic number
Simplify logic to write key & update md5 digest by using libraries",TriagePending,['Ozone Manager'],HDDS,Improvement,Minor,2018-12-31 06:29:22,81
13205286,Remove dead store to local variable in OmMetadataManagerImpl,"OmMetadataManagerImpl#getExpiredOpenKeys creates a dead store local variable in following line:
{{long now = Time.now();}}

This jira aims to remove the dead store.",findbugs,['Ozone Manager'],HDDS,Improvement,Major,2018-12-19 05:48:37,81
13205206,Add S3 access check to Ozone manager,"Add the mapping from S3 User Identity to UGI inside Ozone Manager.  Also add the access check permission, that is call into the checkAccess, which will be intercepted by Ranger or Ozone access check.",pull-request-available,"['Ozone Manager', 'S3']",HDDS,Bug,Blocker,2018-12-18 18:57:08,110
13204285,Use Timeout rule for the the test methods in TestOzoneManager,In the review of HDDS-911 [~ajakumar] suggested to set the timeout method for all the methods in TestOzoneManager. AFAIK it could be done with a junit Timeout rule,newbie,"['Ozone Manager', 'test']",HDDS,Task,Major,2018-12-13 16:08:40,81
13204046,Create isolated classloder to use ozonefs with any older hadoop versions,"As of now we create a shaded ozonefs artifact which includes all the required class files to use ozonefs (Hadoop compatible file system for Ozone)

But the shading process of this artifact is very easy, it includes all the class files but no relocation rules (package name renaming) are configured. With this approach ozonefs can be used from the compatible hadoop version (this is hadoop 3.1 only, I guess) but can't be used with any older hadoop version as it requires the newer version of hadoop-common.

I tried to configure a full shading (with relocation) but it's not a simple task. For example a pure (non-relocated) Configuration is required by the ozonefs itself, but an other, newer Configuration class is required by the ozone client code which is a dependency of OzoneFileSystem So we need a relocated and a non-relocated class in the same time.

I tried out a different approach: I moved out all of the ozone specific classes from the OzoneFileSystem to an adapter class (OzoneClientAdapter). In case of an older hadoop version the adapter class itself can be loaded with an isolated classloader. The isolated classloader can load all the required classes from the jar file from a specific path. It doesn't require any specific package relocation as the default class loader doesn't load these classes. 

The OzoneFileSystem (in case of older hadoop version) can load the adapter with the isolated classloader and only a few classes should be shared between the normal and isolated classloader (the interface of the adapter and the types in the method signatures). All of the other ozone classes and the newer hadoop dependencies will be hidden by the isolated classloader.

This patch is more like a proof of concept, I would like to start a discussion about this approach. I successfully used the generated artifact to use ozonefs from spark 2.4 default distribution (which includes hadoop 2.7). 

For a final patch I would add some check to use the ozonefs without any classpath separation by default. (could be configured or chosen by automatically)


For using spark (+ hadoop 2.7 + kubernetes scheduler) together with ozone, you can check this screencast: https://www.youtube.com/watch?v=cpRJcSHIEdM&t=8s
",pull-request-available,['Ozone Filesystem'],HDDS,Improvement,Major,2018-12-12 17:05:11,6
13203964,Enable prometheus endpoint for Ozone S3 gateway,"HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.

Unfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone S3G daemon in the default web server. ",newbie,[],HDDS,Bug,Major,2018-12-12 10:57:28,30
13203962,Enable prometheus endpoints for Ozone datanodes,"HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.

Unfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone/HDDS datanodes on the web server of the HDDS Rest endpoint. ",pull-request-available,[],HDDS,Bug,Major,2018-12-12 10:56:19,6
13203889,Expose SCMMXBean as a MetricsSource,"Implement MetricsSource interface, so that external metrics can collect the SCMMXBean metrics information.

 

From *MetricsSource.java:*

It registers with \{@link MetricsSystem}, which periodically polls it to collect \{@link MetricsRecord} and passes it to \{@link MetricsSink}.",newbie,[],HDDS,Bug,Major,2018-12-12 03:28:41,86
13203888,Expose NodeManagerMXBean as a MetricsSource,"Implement MetricsSource interface, so that external metrics can collect the NodeManagerMXBean metrics information.

 

From *MetricsSource.java:*

It registers with \{@link MetricsSystem}, which periodically polls it to collect \{@link MetricsRecord} and passes it to \{@link MetricsSink}.",newbie,['SCM'],HDDS,Bug,Major,2018-12-12 03:27:47,86
13203787,Ozonefs defaultFs example is wrong in the documentation,"[https://hadoop.apache.org/ozone/docs/0.3.0-alpha/ozonefs.html]

According to this doc the defaultfs should be {{o3fs://localhost:9864/volume/bucket}}{{}}

IMHO it should be o3fs://bucket.volume

Can be checked with compose/ozonefs cluster definition in the distribution package.",newbie,['documentation'],HDDS,Bug,Major,2018-12-11 18:10:06,88
13202784,Use WAITFOR environment variable to handle dependencies between ozone containers,"Until HDDS-839 we had a hard-coded 15 seconds sleep before we started ozoneManager with the docker-compose files (hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose).

For initialization of the OzoneManager we need the scm. Om will retry the connection if scm is not available but the dns resolution is cached: if the dns of scm is not available at the startup of om, it can't be initialized.

Before HDDS-839 we handled this dependency with the 15 seconds sleep, which was usually slower what we need.

Now we can use the WAITFOR environment variables from HDDS-839 to handle this dependency (like WAITFOR:scmL9876) which can be added to all the docker-compose files.


 ",newbie,['docker'],HDDS,Improvement,Major,2018-12-06 11:50:07,88
13202780,Display the ozone version on SCM/OM web ui instead of Hadoop version,"SCM and OM web uis (http://localhost:9876 and http://localhost:9874) display the actual version but the displayed version is the version of the hadoop dependencies:

This is provided by the org.apache.hadoop.hdds.server.ServiceRuntimeInfoImpl which is a default implementation of ServiceRuntimeInfo. (Both OzoneManager and StorageContainerManager extend this class).

We need to use OzoneVersionInfo and HddsVersionInfo classes to display the actual version instead of org.apache.hadoop.util.VersionInfo.

",newbie,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2018-12-06 11:32:46,1
13202769,Create informative landing page for Ozone S3 gateway ,"{color:#FF0000}{color}

As of now the main s3g endpoint (such as [http://localhost:9878|http://localhost:9878/]) returns with HTTP 500 if it's opened from the browser.

The main endpoint is used to get all the available buckets, but amazon returns with a redirect IF the Authorization header is missing:
{code:java}
 curl -v s3.us-east-2.amazonaws.com
*   Trying 52.219.88.59...
* TCP_NODELAY set
* Connected to s3.us-east-2.amazonaws.com (52.219.88.59) port 80 (#0)
> GET / HTTP/1.1
> Host: s3.us-east-2.amazonaws.com
> User-Agent: curl/7.62.0
> Accept: */*
> 
< HTTP/1.1 307 Temporary Redirect
< x-amz-id-2: fq8RXJdSlVo8PqidHaP8XXczMfLSEAt5Tm4JP98atilWRjalMvqtPa6mwq6rEIXz4cCPrPqJkO4=
< x-amz-request-id: 5C6ACE6D6FC273B9
< Date: Thu, 06 Dec 2018 11:16:36 GMT
< Location: https://aws.amazon.com/s3/
< Content-Length: 0
{code}
I propose to do the same for Ozone:

1.) If the authorization header is missing on the root URL, redirect to an internal page.
 2.) Create an internal landing page at [http://localhost:9878/_ozone] with the following content:
 a) A very short introduction to use the endpoint (with aws client)
 b) The actual documentation of ozone (which is also included in the scm/om ui)

Note: we need an url schema which is not conflicting with the real REST requests. As the bucket and volume names should not contain underscore in ozone, we can use it to prefix all the urls:
 * [http://localhost:9878/_ozone] --> landing page
 * [http://localhost:9878/_ozone/(css]|js) --> required resources
 * [http://localhost:9878/_ozone/docs] --> Documentation with the required resources.",newbie pull-request-available,['S3'],HDDS,Sub-task,Major,2018-12-06 11:23:20,6
13199715,Fix NPE ServerUtils#getOzoneMetaDirPath,"This can be reproed with ""mvn test"" under hadoop-ozone project but not with individual test run under IntelliJ.

 
{code:java}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.33 s <<< FAILURE! - in org.apache.hadoop.ozone.TestOmUtils

testNoOmDbDirConfigured(org.apache.hadoop.ozone.TestOmUtils)  Time elapsed: 0.028 s  <<< FAILURE!

java.lang.AssertionError:

 

Expected: an instance of java.lang.IllegalArgumentException

     but: <java.lang.NullPointerException> is a java.lang.NullPointerException

Stacktrace was: java.lang.NullPointerException

        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)

        at org.apache.hadoop.hdds.server.ServerUtils.getOzoneMetaDirPath(ServerUtils.java:130)

        at org.apache.hadoop.ozone.OmUtils.getOmDbDir(OmUtils.java:141)

        at org.apache.hadoop.ozone.TestOmUtils.testNoOmDbDirConfigured(TestOmUtils.java:89)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:498)

        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)

        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)

        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)

        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

 

{code}",test-badlands,[],HDDS,Bug,Minor,2018-11-20 23:47:39,28
13199305,Provide official apache docker image for Ozone,"Similar to the apache/hadoop:2 and apache/hadoop:3 images I propose to provide apache/ozone docker images which includes the voted release binaries.

The image can follow all the conventions from HADOOP-14898

1. BRANCHING

I propose to create new docker branches:

docker-ozone-0.3.0-alpha
docker-ozone-latest

And ask INFRA to register docker-ozone-(.*) in the dockerhub to create apache/ozone: images

2. RUNNING

I propose to create a default runner script which starts om + scm + datanode + s3g all together. With this approach you can start a full ozone cluster as easy as

{code}
docker run -p 9878:9878 -p 9876:9876 -p 9874:9874 -d apache/ozone
{code}

That's all. This is an all-in-one docker image which is ready to try out.

3. RUNNING with compose

I propose to include a default docker-compose + config file in the image. To start a multi-node pseudo cluster it will be enough to execute:

{code}
docker run apache/ozone cat docker-compose.yaml > docker-compose.yaml
docker run apache/ozone cat docker-config > docker-config
docker-compose up -d
{code}

That's all, and you have a multi-(pseudo)node ozone cluster which could be scaled up and down with ozone.

4. k8s

Later we can also provide k8s resource files with the same approach:

{code}
docker run apache/ozone cat k8s.yaml | kubectl apply -f -
{code}",Triaged,[],HDDS,Bug,Major,2018-11-19 11:47:52,6
13198029,"Update javadoc in StorageContainerManager, NodeManager, PipelineManager and ContainerManager","The javadoc in following interface/classes has to be updated
* StorageContainerManager
* NodeManager
* NodeStateManager
* PipelineManager
* PipelineStateManager
* ContainerManager
* ContainerStateManager",newbie,['SCM'],HDDS,Improvement,Minor,2018-11-13 11:41:22,19
13197703,TestStorageContainerManagerHttpServer should use dynamic port,"Most of the time {{TestStorageContainerManagerHttpServer}} is failing with 
{code}
java.net.BindException: Port in use: 0.0.0.0:9876
...
Caused by: java.net.BindException: Address already in use
{code}

TestStorageContainerManagerHttpServer should use a port which is free (dynamic), instead of trying to bind with default 9876.",newbie,['test'],HDDS,Improvement,Major,2018-11-12 05:58:36,97
13196658,Rename Ozone/HDDS config keys prefixed with 'dfs',"The following Ozone config keys are prefixed with dfs which is the prefix used by HDFS. Instead we should prefix them with either HDDS or Ozone.

{code}
dfs.container.ipc
dfs.container.ipc.random.port
dfs.container.ratis.datanode.storage.dir
dfs.container.ratis.enabled
dfs.container.ratis.ipc
dfs.container.ratis.ipc.random.port
dfs.container.ratis.num.container.op.executors
dfs.container.ratis.num.write.chunk.threads
dfs.container.ratis.replication.level
dfs.container.ratis.rpc.type
dfs.container.ratis.segment.preallocated.size
dfs.container.ratis.segment.size
dfs.container.ratis.statemachinedata.sync.timeout
dfs.ratis.client.request.max.retries
dfs.ratis.client.request.retry.interval
dfs.ratis.client.request.timeout.duration
dfs.ratis.leader.election.minimum.timeout.duration
dfs.ratis.server.failure.duration
dfs.ratis.server.request.timeout.duration
dfs.ratis.server.retry-cache.timeout.duration
dfs.ratis.snapshot.threshold
{code}

Additionally, _dfs.container.ipc_ should be changed to _dfs.container.ipc.port_.",newbie,[],HDDS,Improvement,Major,2018-11-06 18:55:40,81
13196656,dfs.ratis.leader.election.minimum.timeout.duration should not be read by client,"dfs.ratis.leader.election.minimum.timeout.duration is read by client for the following assertion.
{code}
    Preconditions
        .assertTrue(maxRetryCount * retryInterval > 5 * leaderElectionTimeout,
            ""Please make sure dfs.ratis.client.request.max.retries * ""
                + ""dfs.ratis.client.request.retry.interval > ""
                + ""5 * dfs.ratis.leader.election.minimum.timeout.duration"");
{code}

This does not guarantee that the leader is using the same value as the client. We should probably just ensure that the defaults are sane and remove this assert.",newbie,[],HDDS,Improvement,Major,2018-11-06 18:53:41,114
13196528,[JDK11] mvn javadoc:javadoc -Phdds fails,"{{mvn javadoc:javadoc -Phdds}} fails on Java 11
{noformat}
[ERROR] /Users/aajisaka/git/hadoop/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/client/ScmClient.java:107: error: bad use of '>'
[ERROR]    * @param count count must be > 0.
[ERROR] /Users/aajisaka/git/hadoop/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocol/LocatedContainer.java:85: error: unknown tag: DatanodeInfo
[ERROR]   * @return Set<DatanodeInfo> nodes that currently host the container
[ERROR] /Users/aajisaka/git/hadoop/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocol/ScmLocatedBlock.java:71: error: unknown tag: DatanodeInfo
[ERROR]   * @return List<DatanodeInfo> nodes that currently host the block
[ERROR] /Users/aajisaka/git/hadoop/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/Auditable.java:28: error: malformed HTML
[ERROR]   * @return Map<String, String> with values to be logged in audit.
[ERROR]                 ^
[ERROR] /Users/aajisaka/git/hadoop/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/Auditable.java:28: error: bad use of '>'
[ERROR]   * @return Map<String, String> with values to be logged in audit.
{noformat}",javadoc,['documentation'],HDDS,Bug,Major,2018-11-06 12:45:25,81
13196414,Simplify OMAction and DNAction classes used for AuditLogging,"While reviewing HDDS-120, [~ajayydv] suggested to simplify these class by removing the constructor and the getAction().

Refer review comment: https://issues.apache.org/jira/browse/HDDS-120?focusedCommentId=16670495&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16670495",logging,"['Ozone Datanode', 'Ozone Manager']",HDDS,Improvement,Minor,2018-11-06 05:28:04,81
13196305,Period should be an invalid character in bucket names,"ozonefs paths use the following syntax:

- o3fs://bucket.volume/..

The OM host and port are read from configuration. We cannot specify a target filesystem with a fully qualified path. E.g. _o3fs://bucket.volume.om-host.example.com:9862/. Hence we cannot hand a fully qualified URL with OM hostname to a client without setting up config files beforehand. This is inconvenient. It also means there is no way to perform a distcp from one Ozone cluster to another.

We need a way to support fully qualified paths with OM hostname and port _bucket.volume.om-host.example.com_. If we allow periods in bucketnames, then such fully qualified paths cannot be parsed unambiguously. However if er disallow periods, then we can support all of the following paths unambiguously.
 # *o3fs://bucket.volume/key* - The authority has only two period-separated components. These must be bucket and volume name respectively.
 # *o3fs://bucket.volume.om-host.example.com/key* - The authority has more than two components. The first two must be bucket and volume, the rest must be the hostname.
 # *o3fs://bucket.volume.om-host.example.com:5678/key* - Similar to #2, except with a port number.

 

Open question is around HA support. I believe for HA we will have to introduce the notion of a _nameservice_, similar to HDFS nameservice. This will allow a fourth kind of Ozone URL:
 - *o3fs://bucket.volume.ns1/key* - How do we distinguish this from #3 above? One way could be to find if _ns1_ is known as an Ozone nameservice via configuration. If so then treat it as the name of an HA service. Else treat it as a hostname.

 ",newbie,[],HDDS,Improvement,Critical,2018-11-05 17:58:31,86
13196091,Avoid ByteString to byte array conversion cost by using ByteBuffer,"As noticed in HDDS-799, protobuf bytestring to byte[] array conversion has significant performance overhead in the read and write path, This jira proposes to use ByteBuffer in place to byte buffer to negate the performance overhead.",TriagePending performance,['Ozone Client'],HDDS,Bug,Major,2018-11-04 07:25:52,18
13195865,Support custom key/value annotations on volume/bucket/key,"I propose to add a custom Map<String,String> annotation field to objects/buckets and keys in Ozone Manager.

It would enable to build any extended functionality on top of the OM's generic interface. For example:

 * Support tags in Ozone S3 gateway (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html)
 * Support md5 based ETags in s3g
 * Store s3 related authorization data (ACLs, policies) together with the parent objects

As an optional feature (could be implemented later) the client can defined the exposed annotations. For example s3g can defined which annotations should be read from rocksdb on OM side and sent the the client (s3g)",pull-request-available,['Ozone Manager'],HDDS,New Feature,Major,2018-11-02 10:22:55,6
13195759,Reduce usage of Guava Preconditions ,"In the spirit of minimizing Guava dependencies, we can replace most usage of Preconditions with the [Objects|https://docs.oracle.com/javase/8/docs/api/java/util/Objects.html] class.",newbie,[],HDDS,Improvement,Major,2018-11-01 21:49:51,117
13195297,OM should not search for STDOUT root logger for audit logging,"When we start ozone, the .out file shows the following line:
{noformat}
2018-10-31 00:48:04,141 main ERROR Unable to locate appender ""STDOUT"" for logger config ""root""{noformat}
This is because the console appender has been disabled by default however incorrect log4j2 config is still trying to find the console appender.

 

This Jira aims to comment the following config lines to avoid this issue:
{code:java}
rootLogger.appenderRefs=stdout
rootLogger.appenderRef.stdout.ref=STDOUT
{code}",logging,['Ozone Manager'],HDDS,Bug,Minor,2018-10-31 04:52:42,81
13195237,Ozone shell create volume fails if volume name does not have a leading slash,"After HDDS-682, volume creation through shell fails if the volume name does not have leading slash.
{code:java}
$ ./ozone sh volume create volume1
Volume name is required
$ ./ozone sh volume create /volume1
2018-10-30 14:07:58,078 INFO rpc.RpcClient: Creating Volume: volume1, with hdds as owner and quota set to 1152921504606846976 bytes.{code}
In {{OzoneAddress#stringToUri}}, when creating a new URI, the path parameter is expected to have a leading slash. Otherwise, the path gets mixed with the authority.

To fix this, we should add a leading slash to the path variable, if it does not exist, before constructing the URI object.",newbie,[],HDDS,Bug,Major,2018-10-30 21:32:56,114
13195193,Run S3 smoke tests with replication STANDARD.,"This Jira is created from the comment from [~elek]

1. I think sooner or later we need to run ozone tests with real replication. We can add a 'scale up' to the hadoop-ozone/dist/src/main/smoketest/test.sh
{code:java}
docker-compose -f ""$COMPOSE_FILE"" down
docker-compose -f ""$COMPOSE_FILE"" up -d
docker-compose -f ""$COMPOSE_FILE"" scale datanode=3
{code}
And with this modification we don't need the '--storage-class REDUCED_REDUNDANCY'. (But we can do it in separated jira)",newbie pull-request-available,['test'],HDDS,Sub-task,Major,2018-10-30 18:25:46,6
13195020,Create S3 subcommand to run S3 related operations,"This Jira is added to create S3 subcommand, which will be used for all S3 related operations.
Under this jira, move the command ozone sh bucket <<bucketname>> to ozone s3 bucket <<bucketname>>",pull-request-available,[],HDDS,Sub-task,Major,2018-10-30 03:21:28,37
13194995,Separate client/server configuration settings,"Ozone should have separate config files for client and server. E.g.
- ozone-site-client.xml
- ozone-site-server.xml

Clients should never load ozone-site-server.xml. And vice versa i.e. servers should never load ozone-site-client.xml.

This may require duplicating a very small number of settings like OM and SCM address. ",beta1,[],HDDS,Improvement,Major,2018-10-29 23:06:38,31
13194978,Functionality to handle key rotation in DN.,"Functionality to handle key rotation in SCM, OM and DN.",backlog,[],HDDS,Sub-task,Major,2018-10-29 21:35:28,110
13194977,Functionality to handle key rotation in OM,"Functionality to handle key rotation in SCM, OM and DN.",backlog,[],HDDS,Sub-task,Major,2018-10-29 21:35:12,110
13194939,Functionality to handle key rotation in SCM,"Functionality to handle key rotation in SCM, OM and DN.",backlog,[],HDDS,Sub-task,Major,2018-10-29 18:41:43,110
13194933,Replace usage of Guava Optional with Java Optional,"Ozone and HDDS code uses {{com.google.common.base.Optional}} in multiple places.

Let's replace with the Java Optional since we only target JDK8+.",newbie,[],HDDS,Improvement,Critical,2018-10-29 18:28:13,68
13194922,Write Security audit entry to track activities related to Private Keys and  certificates,"Write Security Audit entry to track security tasks performed on SCM, OM and DN.
Tasks:
* Private Keys: bootstrap/rotation
* Certificates: CSR submission, rotation",backlog,[],HDDS,Sub-task,Major,2018-10-29 18:10:11,81
13194398,all containers are in 'CLOSING' state after service restart,"all containers are in closing state after service restart. None of the writes are working after restart.

The cluster contains 11 live datanodes.

**
{noformat}
[
 {
 ""nodeType"": ""OM"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000008.hwx.site"",
 ""ports"": {
 ""RPC"": 9889,
 ""HTTP"": 9874
 }
 },
 {
 ""nodeType"": ""SCM"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000003.hwx.site"",
 ""ports"": {
 ""RPC"": 9860
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-541661-01-000003.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-541661-01-000007.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000003.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-541661-01-000004.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000004.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000008.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-541661-01-000002.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000005.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-541661-01-000006.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000007.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 },
 {
 ""nodeType"": ""DATANODE"",
 ""hostname"": ""ctr-e138-1518143905142-544443-01-000006.hwx.site"",
 ""ports"": {
 ""HTTP"": 9880
 }
 }
]{noformat}
error thrown while write :

 
{noformat}
[root@ctr-e138-1518143905142-541661-01-000007 test_files]# ozone fs -put /etc/passwd /testdir5/
2018-10-26 12:09:43,822 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-10-26 12:09:47,882 ERROR io.ChunkGroupOutputStream: Try to allocate more blocks for write failed, already allocated 0 blocks for this write.
put: Allocate block failed, error:INTERNAL_ERROR{noformat}
 

 

pipelines in the cluster :

 
{noformat}
[root@ctr-e138-1518143905142-541661-01-000007 test_files]# ozone scmcli listPipelines
Pipeline[ Id: 29b68cc2-2d18-4db0-a11a-587ae4abc715, Nodes: e3d89961-fe38-4ed0-8a32-cd1849c58e0c{ip: 172.27.20.96, host: ctr-e138-1518143905142-544443-01-000008.hwx.site}b33a30d9-f1e2-448e-aabb-61a970445cea{ip: 172.27.85.64, host: ctr-e138-1518143905142-541661-01-000007.hwx.site}, Type:RATIS, Factor:THREE, State:CLOSING]
Pipeline[ Id: 05061f87-4c68-443b-ae27-984da2d0a2cd, Nodes: dc002a73-fc63-4e76-be3e-3c6d16ede5f6{ip: 172.27.38.9, host: ctr-e138-1518143905142-544443-01-000004.hwx.site}4e6bd2a2-6802-4e67-9710-612a2cdb9dc1{ip: 172.27.24.90, host: ctr-e138-1518143905142-544443-01-000005.hwx.site}be3f0db4-3a19-44a5-bd6e-0da47d2ed92e{ip: 172.27.20.91, host: ctr-e138-1518143905142-544443-01-000003.hwx.site}, Type:RATIS, Factor:THREE, State:CLOSING]
Pipeline[ Id: 80893f87-5e73-49a2-8f38-2adb2b13140a, Nodes: 63833540-bf93-410c-b081-243a56f93c88{ip: 172.27.10.199, host: ctr-e138-1518143905142-544443-01-000007.hwx.site}6e8b7129-8615-45fe-81e0-848a2e0ba520{ip: 172.27.15.139, host: ctr-e138-1518143905142-544443-01-000006.hwx.site}aab1f2e5-1cf0-430d-b1bf-04be8630a8ee{ip: 172.27.57.0, host: ctr-e138-1518143905142-541661-01-000003.hwx.site}, Type:RATIS, Factor:THREE, State:CLOSING]
Pipeline[ Id: f0a14cb9-d37a-4c7c-b3e6-b7e3830dfd5f, Nodes: 61e271bf-68ad-435e-8a6e-582be90ebb6f{ip: 172.27.19.74, host: ctr-e138-1518143905142-541661-01-000006.hwx.site}3622352c-b136-4c74-b952-34e938cbda94{ip: 172.27.15.131, host: ctr-e138-1518143905142-541661-01-000002.hwx.site}cb2b1e95-e803-48d3-bdf2-bf878cae62cf{ip: 172.27.23.139, host: ctr-e138-1518143905142-541661-01-000004.hwx.site}, Type:RATIS, Factor:THREE, State:CLOSING]

{noformat}
 

datanode.log :

-----------------------
{noformat}
2018-10-26 12:17:23,697 INFO org.apache.ratis.server.impl.LeaderElection: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: Election REJECTED; received 1 response(s) [e3d89961-fe38-4ed0-8a32-cd1849c58e0c<-b33a30d9-f1e2-448e-aabb-61a970445cea#0:FAIL-t1019] and 1 exception(s); e3d89961-fe38-4ed0-8a32-cd1849c58e0c:t1019, leader=null, voted=e3d89961-fe38-4ed0-8a32-cd1849c58e0c, raftlog=e3d89961-fe38-4ed0-8a32-cd1849c58e0c-SegmentedRaftLog:OPENED, conf=531: [e3d89961-fe38-4ed0-8a32-cd1849c58e0c:172.27.20.96:9858, b33a30d9-f1e2-448e-aabb-61a970445cea:172.27.85.64:9858, 0d7f5327-df16-40fe-ac88-7ed06e76a20f:172.27.68.65:9858], old=null
2018-10-26 12:17:23,697 INFO org.apache.ratis.server.impl.LeaderElection: 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2018-10-26 12:17:23,697 INFO org.apache.ratis.server.impl.RaftServerImpl: e3d89961-fe38-4ed0-8a32-cd1849c58e0c changes role from CANDIDATE to FOLLOWER at term 1019 for changeToFollower
2018-10-26 12:17:23,697 INFO org.apache.ratis.server.impl.RoleInfo: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: shutdown LeaderElection
2018-10-26 12:17:23,698 INFO org.apache.ratis.server.impl.RoleInfo: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: start FollowerState
2018-10-26 12:17:24,403 WARN org.apache.ratis.grpc.server.GrpcLogAppender: GrpcLogAppender(e3d89961-fe38-4ed0-8a32-cd1849c58e0c -> b33a30d9-f1e2-448e-aabb-61a970445cea): appendEntries Timeout, request=e3d89961-fe38-4ed0-8a32-cd1849c58e0c->b33a30d9-f1e2-448e-aabb-61a970445cea#0
2018-10-26 12:17:24,859 INFO org.apache.ratis.server.impl.FollowerState: e3d89961-fe38-4ed0-8a32-cd1849c58e0c changes to CANDIDATE, lastRpcTime:1161, electionTimeout:1161ms
2018-10-26 12:17:24,860 INFO org.apache.ratis.server.impl.RoleInfo: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: shutdown FollowerState
2018-10-26 12:17:24,860 INFO org.apache.ratis.server.impl.RaftServerImpl: e3d89961-fe38-4ed0-8a32-cd1849c58e0c changes role from FOLLOWER to CANDIDATE at term 1020 for changeToCandidate
2018-10-26 12:17:24,860 INFO org.apache.ratis.server.impl.RoleInfo: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: start LeaderElection
2018-10-26 12:17:24,864 INFO org.apache.ratis.server.impl.LeaderElection: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: begin an election in Term 1021
2018-10-26 12:17:24,869 INFO org.apache.ratis.server.impl.LeaderElection: e3d89961-fe38-4ed0-8a32-cd1849c58e0c got exception when requesting votes: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
 at java.util.concurrent.FutureTask.report(FutureTask.java:122)
 at java.util.concurrent.FutureTask.get(FutureTask.java:192)
 at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:214)
 at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:146)
 at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:102)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:222)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:203)
 at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:132)
 at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
 at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:61)
 at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:150)
 at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$0(LeaderElection.java:188)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.27.68.65:9858
 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
 at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
 at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:325)
 at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
 at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:634)
 at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:581)
 at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498)
 at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
 at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)
 at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 ... 1 more
Caused by: java.net.ConnectException: Connection refused
 ... 11 more
2018-10-26 12:17:24,882 INFO org.apache.ratis.server.impl.LeaderElection: e3d89961-fe38-4ed0-8a32-cd1849c58e0c: Election PASSED; received 1 response(s) [e3d89961-fe38-4ed0-8a32-cd1849c58e0c<-b33a30d9-f1e2-448e-aabb-61a970445cea#0:OK-t1021] and 1 exception(s); e3d89961-fe38-4ed0-8a32-cd1849c58e0c:t1021, leader=null, voted=e3d89961-fe38-4ed0-8a32-cd1849c58e0c, raftlog=e3d89961-fe38-4ed0-8a32-cd1849c58e0c-SegmentedRaftLog:OPENED, conf=531: [e3d89961-fe38-4ed0-8a32-cd1849c58e0c:172.27.20.96:9858, b33a30d9-f1e2-448e-aabb-61a970445cea:172.27.85.64:9858, 0d7f5327-df16-40fe-ac88-7ed06e76a20f:172.27.68.65:9858], old=null{noformat}
 ",TriagePending,['SCM'],HDDS,Bug,Critical,2018-10-26 12:19:39,19
13194160,Removing REST protocol support from OzoneClient,"Since we have functional {{S3Gateway}} for Ozone which works on REST protocol, having REST protocol support in OzoneClient feels redundant and it will take a lot of effort to maintain it up to date.
As S3Gateway is in a functional state now, I propose to remove REST protocol support from OzoneClient.

Once we remove REST support from OzoneClient, the following will be the interface to access Ozone cluster
 * OzoneClient (RPC Protocol)
 * OzoneFS (RPC Protocol)
 * S3Gateway (REST Protocol)",pull-request-available,['Ozone Client'],HDDS,Improvement,Major,2018-10-25 14:46:06,6
13193846,ozone fs cli prints hadoop fs in usage,"ozone fs cli help/usage page contains Usage: hadoop fs [ generic options ] 

I believe the usage should be updated.

Check line 3 of screenshot.

!image-2018-10-24-17-15-39-097.png|width=1693,height=1512!",newbie pull-request-available,['Ozone Filesystem'],HDDS,Bug,Major,2018-10-24 11:48:07,31
13193841,OzoneFileSystem doesn't support modifyAclEntries,"Hive service while starting does modifyAcl operation and as the same isn't supported it fails to start.
{code:java}
hdfs dfs -setfacl -m default:user:hive:rwx /warehouse/tablespace/external/hive{code}
Exception encountered :
{code:java}
[hdfs@ctr-e138-1518143905142-541600-02-000002 ~]$ hdfs dfs -setfacl -m default:user:hive:rwx /warehouse/tablespace/external/hive
18/10/24 08:39:35 INFO conf.Configuration: Removed undeclared tags:
18/10/24 08:39:37 INFO conf.Configuration: Removed undeclared tags:
-setfacl: Fatal internal error
java.lang.UnsupportedOperationException: OzoneFileSystem doesn't support modifyAclEntries
at org.apache.hadoop.fs.FileSystem.modifyAclEntries(FileSystem.java:2926)
at org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand.processPath(AclCommands.java:256)
at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)
at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)
at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)
at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)
at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)
at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
18/10/24 08:39:37 INFO conf.Configuration: Removed undeclared tags:
{code}",TriagePending,['Ozone Filesystem'],HDDS,Bug,Major,2018-10-24 11:41:08,63
13193821,ozone.log is not getting created in logs directory,"ozone.log is no more present in logs directory of datanodes.

Need to be added back.",newbie,['Ozone Manager'],HDDS,Bug,Major,2018-10-24 10:28:37,20
13192396,Incorrect creation time for files created by o3fs.,Files created by o3fs show creation timestamp as unix epoch.,app-compat,['Ozone Filesystem'],HDDS,Improvement,Blocker,2018-10-17 23:13:45,43
13192060,Spark shell throws OzoneFileSystem not found,"Spark shell throws OzoneFileSystem not found, if the ozone jars are not specified in the --jars options",app-compat,[],HDDS,Bug,Major,2018-10-16 22:06:55,102
13192046,Fix OzoneFS directory rename," 

Renaming a directory within the same parent directory fails with the exception:
{code:java}
Unable to move: o3://bucket2.volume2/testo3/.hive-staging_hive_2018-10-16_21-09-35_130_1001829123585250245-1/_tmp.-ext-10000 to: o3://bucket2.volume2/testo3/.hive-staging_hive_2018-10-16_21-09-35_130_1001829123585250245-1/_tmp.-ext-10000.moved
{code}
Detailed exception in comment below.",app-compat,[],HDDS,Bug,Blocker,2018-10-16 21:19:36,43
13192034,Consider removing duplication of StorageType,The StorageType class is currently duplicated in hadoop-hdds. We can just use the version in hadoop-common.,beta1,[],HDDS,Improvement,Major,2018-10-16 20:37:55,31
13191737,Creating hive table on Ozone fails,"Modified HIVE_AUX_JARS_PATH to include Ozone jars. Tried creating Hive external table on Ozone. It fails with ""Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)""
{code:java}
-bash-4.2$ beeline
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
Enter username for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:
Enter password for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:
18/10/15 21:36:55 [main]: INFO jdbc.HiveConnection: Connected to ctr-e138-1518143905142-510793-01-000004.hwx.site:10000
Connected to: Apache Hive (version 3.1.0.3.0.3.0-63)
Driver: Hive JDBC (version 3.1.0.3.0.3.0-63)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.0.3.0.3.0-63 by Apache Hive
0: jdbc:hive2://ctr-e138-1518143905142-510793> create external table testo3 ( i int, s string, d float) location ""o3://bucket2.volume2/testo3"";
Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)
0: jdbc:hive2://ctr-e138-1518143905142-510793> {code}
 ",Triaged app-compat,['documentation'],HDDS,Bug,Major,2018-10-15 21:50:45,43
13191261,Rename dfs.container.ratis.num.container.op.threads,"public static final String DFS_CONTAINER_RATIS_NUM_CONTAINER_OP_EXECUTORS_KEY
 = ""dfs.container.ratis.num.container.op.threads"";

This should be changed to dfs.container.ratis.num.container.op.executors

 

HDDS-550 has added this in OzoneConfigKeys.java, but they have named differently in ozone-default.xml and ScmConfigKeys.java

 

Because of this TestOzoneConfigurationFields.java is failing

[https://builds.apache.org/job/PreCommit-HDDS-Build/1378/testReport/]

 ",newbie,[],HDDS,Bug,Minor,2018-10-12 17:35:27,13
13190798,ozone.metadata.dirs should be tagged as REQUIRED,ozone.metadata.dirs is a required config but is missing the REQUIRED tag.,newbie,[],HDDS,Bug,Major,2018-10-11 01:07:02,81
13190786,"On SCM UI, Node Manager info is empty","Fields like below are empty

Node Manager: Minimum chill mode nodes 
Node Manager: Out-of-node chill mode 
Node Manager: Chill mode status 
Node Manager: Manual chill mode

Please see attached screenshot !Screen Shot 2018-10-10 at 4.19.59 PM.png!",pull-request-available,[],HDDS,Bug,Major,2018-10-10 23:22:45,6
13190766,ozone genconf improvements,"A few potential improvements to genconf:
 # -Path should be optional :default to current config directory _etc/hadoop_.-
 # genconf silently overwrites existing _ozone-site.xml_. It should never do so.
 # The generated config file should have _ozone.enabled = true_.
 # -Have a {{pseudo}} option to generate configs for starting pseudo-cluster. This should be useful for quick dev-testing.-",newbie,[],HDDS,Improvement,Major,2018-10-10 21:47:09,81
13190765,ozone.scm.client.address should be an optional setting,{{ozone.scm.client.address}} should be an optional setting. Clients can fallback to {{ozone.scm.names}} if the former is unspecified.,newbie,[],HDDS,Improvement,Major,2018-10-10 21:42:50,114
13190762,hdds.db.profile should not be tagged as a required setting & should default to DISK,"hdds.db.profile is tagged as a required setting and defaults to SSD. It should default to DISK instead.
{code:java}
<property>
  <name>hdds.db.profile</name>
  <value>SSD</value>
  <tag>OZONE, OM, PERFORMANCE, REQUIRED</tag>
  <description>
    This property allows user to pick a configuration
    that tunes the RocksDB settings for the hardware it is running
    on. Right now, we have SSD and DISK as profile options.
  </description>
</property>{code}",newbie,[],HDDS,Improvement,Major,2018-10-10 21:40:49,81
13190441,Correct Ozone getOzoneConf description ,"The {{./ozone getozoneconf}} subcommand description mentions the subcommand as {{getconf}}. We should consistently call it either {{getozoneconf}} or {{getconf}} at both places.
{code:java}
$ bin/ozone getozoneconf
ozone getconf is utility for getting configuration information from the config file.

ozone getconf
	[-includeFile]			gets the include file path that defines the datanodes that can join the cluster.
	[-excludeFile]			gets the exclude file path that defines the datanodes that need to decommissioned.
	[-ozonemanagers]			gets list of Ozone Manager nodes in the cluster
	[-storagecontainermanagers]			gets list of ozone storage container manager nodes in the cluster
	[-confKey [key]]			gets a specific key from the configuration
{code}",newbie,[],HDDS,Bug,Minor,2018-10-09 18:02:15,81
13190304,Mapreduce example fails with java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character,"Set up a hadoop cluster where ozone is also installed. Ozone can be referenced via o3://xx.xx.xx.xx:9889
{code:java}
[root@ctr-e138-1518143905142-510793-01-000002 ~]# ozone sh bucket list o3://xx.xx.xx.xx:9889/volume1/
2018-10-09 07:21:24,624 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[ {
""volumeName"" : ""volume1"",
""bucketName"" : ""bucket1"",
""createdOn"" : ""Tue, 09 Oct 2018 06:48:02 GMT"",
""acls"" : [ {
""type"" : ""USER"",
""name"" : ""root"",
""rights"" : ""READ_WRITE""
}, {
""type"" : ""GROUP"",
""name"" : ""root"",
""rights"" : ""READ_WRITE""
} ],
""versioning"" : ""DISABLED"",
""storageType"" : ""DISK""
} ]
[root@ctr-e138-1518143905142-510793-01-000002 ~]# ozone sh key list o3://xx.xx.xx.xx:9889/volume1/bucket1
2018-10-09 07:21:54,500 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[ {
""version"" : 0,
""md5hash"" : null,
""createdOn"" : ""Tue, 09 Oct 2018 06:58:32 GMT"",
""modifiedOn"" : ""Tue, 09 Oct 2018 06:58:32 GMT"",
""size"" : 0,
""keyName"" : ""mr_job_dir""
} ]
[root@ctr-e138-1518143905142-510793-01-000002 ~]#{code}
Hdfs is also set fine as below
{code:java}
[root@ctr-e138-1518143905142-510793-01-000002 ~]# hdfs dfs -ls /tmp/mr_jobs/input/
Found 1 items
-rw-r--r-- 3 root hdfs 215755 2018-10-09 06:37 /tmp/mr_jobs/input/wordcount_input_1.txt
[root@ctr-e138-1518143905142-510793-01-000002 ~]#{code}
Now try to run Mapreduce example job against ozone o3:
{code:java}
[root@ctr-e138-1518143905142-510793-01-000002 ~]# /usr/hdp/current/hadoop-client/bin/hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /tmp/mr_jobs/input/ o3://xx.xx.xx.xx:9889/volume1/bucket1/mr_job_dir/output
18/10/09 07:15:38 INFO conf.Configuration: Removed undeclared tags:
java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : :
at org.apache.hadoop.hdds.scm.client.HddsClientUtils.verifyResourceName(HddsClientUtils.java:143)
at org.apache.hadoop.ozone.client.rpc.RpcClient.getVolumeDetails(RpcClient.java:231)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)
at com.sun.proxy.$Proxy16.getVolumeDetails(Unknown Source)
at org.apache.hadoop.ozone.client.ObjectStore.getVolume(ObjectStore.java:92)
at org.apache.hadoop.fs.ozone.OzoneFileSystem.initialize(OzoneFileSystem.java:121)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3354)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(FileOutputFormat.java:178)
at org.apache.hadoop.examples.WordCount.main(WordCount.java:85)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
18/10/09 07:15:39 INFO conf.Configuration: Removed undeclared tags:
[root@ctr-e138-1518143905142-510793-01-000002 ~]#
{code}",app-compat test-badlands,['documentation'],HDDS,Bug,Blocker,2018-10-09 07:25:25,43
13190283,Fix TestOzoneConfiguration TestOzoneConfigurationFields,"java.lang.AssertionError: ozone-default.xml has 2 properties missing in class org.apache.hadoop.ozone.OzoneConfigKeys class org.apache.hadoop.hdds.scm.ScmConfigKeys class org.apache.hadoop.ozone.om.OMConfigKeys class org.apache.hadoop.hdds.HddsConfigKeys class org.apache.hadoop.ozone.s3.S3GatewayConfigKeys Entries: hdds.lock.suppress.warning.interval.ms hdds.write.lock.reporting.threshold.ms expected:<0> but was:<2>

 

hdds.lock.suppress.warning.interval.ms and hdds.write.lock.reporting.threshold.ms should be removed from ozone-default.xml 

This is caused by HDDS-354, which has missed removing these properties from ozone-default.xml",newbie,[],HDDS,Task,Major,2018-10-09 05:30:33,97
13190162,Add new classes for pipeline management,This Jira adds new classes and corresponding unit tests for pipeline management in SCM. The old classes will be removed in a subsequent jira.,recovery,['SCM'],HDDS,Bug,Major,2018-10-08 17:55:28,35
13189703,ContainerStateMachine should fail subsequent transactions per container in case one fails,"ContainerStateMachine will keep of track of the last successfully applied transaction index and on restart inform Ratis the index, so that the subsequent transactions can be reapplied from here.

Moreover, in case one transaction fails, all the subsequent transactions on the container should fail in the containerStateMachine and a container close action to SCM needs to be initiated to close the container.",recovery,[],HDDS,Bug,Major,2018-10-05 12:32:59,16
13189647,om-audit-log4j2.properties must be packaged in ozone-dist ,"After HDDS-447, it appears the om-audit-log4j2.properties file is not available ozone tar ball.

On decompressing the ozone tar, we must see this file in ozone-<version>/etc/hadoop directory.

This Jira aims to fix this so that audit logging configurations are available and logs are generated.",regression,['Ozone Manager'],HDDS,Bug,Major,2018-10-05 06:31:56,81
13189569,Make VirtualHostStyleFilter port agnostic,"Based on the discussion in HDDS-525

The host HTTP header sometimes contains the port, sometimes not (with aws cli we have the port, with mitm proxy we doesn't). Would be easier to remove it anyway to make it easier to configure the s3 gateway.",newbie,[],HDDS,Sub-task,Major,2018-10-04 20:55:51,117
13189331,Add proto changes required for CopyKey support in ozone,"This Jira is the starter Jira to make changes required for copy key request in S3 to support copy key across the bucket. In ozone world, this is just a metadata change. This Jira is created to just change .proto file for copy key request support.

 

This Jira is created to provide a copy object API in RpcClient to copy an object which already exists in ozone to same/another bucket.

[https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html]",S3,[],HDDS,New Feature,Major,2018-10-04 00:05:14,13
13188626,TestContainerPersistence fails regularly in Jenkins,"TestContainerPersistence tests are regularly failing in Jenkins with the error - ""\{{Unable to create directory /dfs/data}}"". 
In \{{#init()}}, we are setting HDDS_DATANODE_DIR_KEY to a test dir location. But in {{#setupPaths}}, we are using DFS_DATANODE_DATA_DIR_KEY as the data dir location. ",newbie,['test'],HDDS,Bug,Minor,2018-10-01 19:24:55,81
13188594,Remove volume name parsing from VirtualHostStyleFilter,"UPDATE:  Since the creation of this jira the volume name is removed from the url. So the original goal is simplified: the only task is to remove the volume from the VirtualHostStyleFilter parsing part.

 ORIGINAL description:

In VirtualHost Style filter parse bucketname from the HttpHeader Host if it is virtual host style request. Remove the volume parsing code.

This Jira is created from [~elek] comments on HDDS-525 jira.",newbie,['S3'],HDDS,Sub-task,Major,2018-10-01 17:19:31,13
13187734,fs.default.name is deprecated,{{fs.default.name}} is deprecated. Docs should be updated to use {{fs.defaultFS}}.,newbie,['documentation'],HDDS,Bug,Major,2018-09-26 21:16:34,81
13187726,"When creating keys, the creationTime and modificationTime should ideally be the same","Steps to replicate:
 # Start ozone
 # Create Volume and Bucket or use existing ones
 # Create Key
 # List Keys for that bucket or just get key info

We will see that the creationTime and ModificationTime has a minor difference.

 
{noformat}
hadoop@fdaf56d9e9d8:~$ ./bin/ozone sh key put /rvol/rbucket/rkey sample.orc
hadoop@fdaf56d9e9d8:~$ ./bin/ozone sh key list /rvol/rbucket
[ {
""version"" : 0,
""md5hash"" : null,
""createdOn"" : ""Wed, 26 Sep 2018 20:29:10 GMT"",
""modifiedOn"" : ""Wed, 26 Sep 2018 20:29:12 GMT"",
""size"" : 2262690,
""keyName"" : ""rkey""
} ]{noformat}
Potential fix area : KeyManagerImpl#commitKey
{code:java}
keyInfo = new OmKeyInfo.Builder()
    .setVolumeName(args.getVolumeName())
    .setBucketName(args.getBucketName())
    .setKeyName(args.getKeyName())
    .setOmKeyLocationInfos(Collections.singletonList(
        new OmKeyLocationInfoGroup(0, locations)))
    .setCreationTime(Time.now())
    .setModificationTime(Time.now())
    .setDataSize(size)
    .setReplicationType(type)
    .setReplicationFactor(factor)
    .build();
{code}
For setting, both these values, we are getting current time and thus the minor difference.",newbie,"['Ozone Client', 'Ozone Manager']",HDDS,Bug,Major,2018-09-26 20:55:59,118
13187717,Update the acceptance test location mentioned in ozone document,"This is found during the release verification of 0.2.1. In the ""Building from Souces"" page:

"" please follow the instructions in the *README.md* in the 

{{$hadoop_src/hadoop-ozone/acceptance-test""}}

 

{{The correct location should be }}

{{""$hadoop_src/}}hadoop-ozone/dist/target/$hdds_version/smoketest""",newbie,[],HDDS,Bug,Major,2018-09-26 20:28:49,28
13186851,Ozone Quota support.,"Create a volume with just 1 MB as quota
{code:java}
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone sh volume create --quota=1MB --user=root /hive
2018-09-23 02:10:11,283 [main] INFO - Creating Volume: hive, with root as owner and quota set to 1048576 bytes.
{code}
Now create a bucket and put a big key greater than 1MB in the bucket
{code:java}
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone sh bucket create /hive/bucket1
2018-09-23 02:10:38,003 [main] INFO - Creating Bucket: hive/bucket1, with Versioning false and Storage Type set to DISK
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ls -l ../../ozone-0.3.0-SNAPSHOT.tar.gz
-rw-r--r-- 1 root root 165903437 Sep 21 13:16 ../../ozone-0.3.0-SNAPSHOT.tar.gz
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone sh key put /hive/ozone-0.3.0-SNAPSHOT.tar.gz ../../ozone-0.3.0-SNAPSHOT.tar.gz
volume/bucket/key name required in putKey
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone sh key put /hive/bucket1/ozone-0.3.0-SNAPSHOT.tar.gz ../../ozone-0.3.0-SNAPSHOT.tar.gz
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone sh key info /hive/bucket1/ozone-0.3.0-SNAPSHOT.tar.gz
{
""version"" : 0,
""md5hash"" : null,
""createdOn"" : ""Sun, 23 Sep 2018 02:13:02 GMT"",
""modifiedOn"" : ""Sun, 23 Sep 2018 02:13:08 GMT"",
""size"" : 165903437,
""keyName"" : ""ozone-0.3.0-SNAPSHOT.tar.gz"",
""keyLocations"" : [ {
""containerID"" : 2,
""localID"" : 100772661343420416,
""length"" : 134217728,
""offset"" : 0
}, {
""containerID"" : 3,
""localID"" : 100772661661007873,
""length"" : 31685709,
""offset"" : 0
} ]
}{code}
It was able to put a 165 MB file on a volume with just 1MB quota.

 

Currently Ozone haven't support Quota, So I think this should be a new feature .
 The design document can be referred to the attachment. ([design google docs|https://docs.google.com/document/d/1ohbGn5N7FN6OD15xMShHH2SrtZRYx0-zUf9vjatn_OM/edit])",Triaged,[],HDDS,New Feature,Major,2018-09-23 02:18:21,4
13186776,Ozone datanode command ignores the invalid options,"ozone datanode command starts datanode and ignores the invalid option, apart from help
{code:java}
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone datanode -help
Starts HDDS Datanode
{code}
For all the other invalid options, it just ignores and starts the DN like below:
{code:java}
[root@ctr-e138-1518143905142-481027-01-000002 bin]# ./ozone datanode -ABC
2018-09-22 00:59:34,462 [main] INFO - STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting HddsDatanodeService
STARTUP_MSG: host = ctr-e138-1518143905142-481027-01-000002.hwx.site/172.27.54.20
STARTUP_MSG: args = [-ABC]
STARTUP_MSG: version = 3.2.0-SNAPSHOT
STARTUP_MSG: classpath = /root/ozone-0.3.0-SNAPSHOT/etc/hadoop:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-cli-1.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/guava-11.0.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-3.2.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsp-api-2.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/gson-2.2.4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/dnsjava-2.1.7.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/avro-1.7.7.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-json-1.19.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/log4j-1.2.17.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-lang3-3.7.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-server-1.19.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/hadoop-annotations-3.2.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/re2j-1.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-databind-2.9.5.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-core-1.19.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsch-0.1.54.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/asm-5.0.4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-2.9.5.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/json-smart-2.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-io-2.5.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-codec-1.11.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-text-1.4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/paranamer-2.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-net-3.6.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/xz-1.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jettison-1.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-annotations-2.9.5.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.2.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/auto-service-1.0-rc4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/bcpkix-jdk15on-1.54.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/netty-all-4.0.52.Final.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/netty-tcnative-2.0.8.Final-osx-x86_64.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/aalto-xml-1.0.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/hadoop-hdfs-client-3.2.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/ratis-netty-0.3.0-eca3531-20180918.052321-2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/okhttp-2.7.5.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/bcprov-jdk15on-1.54.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/snakeyaml-1.8.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/jzlib-1.1.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/jctools-core-2.1.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/jboss-marshalling-1.4.11.Final.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/auto-value-annotations-1.6.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/compress-lzf-1.0.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/sqlite-jdbc-3.8.7.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/javapoet-1.10.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/animal-sniffer-annotations-1.16.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/hadoop-hdds-client-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/rocksdbjni-5.14.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/hamcrest-all-1.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/error_prone_annotations-2.2.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/auto-value-1.6.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/ratis-grpc-0.3.0-eca3531-20180918.052238-2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/log4j-api-2.11.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/snakeyaml-1.16.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/log4j-core-2.11.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/hadoop-hdds-container-service-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/okio-1.6.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/picocli-3.5.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/j2objc-annotations-1.3.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/leveldbjni-all-1.8.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/metrics-core-3.2.4.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/ratis-proto-shaded-0.3.0-eca3531-20180918.051334-1.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/commons-pool2-2.6.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/ratis-common-0.3.0-eca3531-20180918.052022-2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/hadoop-hdfs-3.2.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/ratis-server-0.3.0-eca3531-20180918.052141-2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/commons-daemon-1.0.13.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/lz4-1.3.0.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/disruptor-3.4.2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/ratis-client-0.3.0-eca3531-20180918.052108-2.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/lib/auto-common-0.8.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/hadoop-hdds-server-scm-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/hadoop-hdds-tools-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/hadoop-hdds-server-framework-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/hdds/hadoop-hdds-common-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/lib/jmh-generator-annprocess-1.19.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/lib/jmh-core-1.19.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/lib/hadoop-ozone-client-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/lib/jopt-simple-4.6.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/hadoop-ozone-tools-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/hadoop-ozone-objectstore-service-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/hadoop-ozone-ozone-manager-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozone/hadoop-ozone-common-0.3.0-SNAPSHOT.jar:/root/ozone-0.3.0-SNAPSHOT/share/hadoop/ozonefs/hadoop-ozone-filesystem-0.3.0-SNAPSHOT.jar
STARTUP_MSG: build = git@github.com:apache/hadoop.git -r a2752779ac1545f5e0a52fce3cff02a7007e95fb; compiled by 'nmaheshwari' on 2018-09-21T10:48Z
STARTUP_MSG: java = 1.8.0_112
************************************************************/
2018-09-22 00:59:34,490 [main] INFO - registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-22 00:59:34,719 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2018-09-22 00:59:34,875 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-22 00:59:34,875 INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
2018-09-22 00:59:34,925 INFO Configuration.deprecation: No unit for ozone.scm.heartbeat.rpc-timeout(1000) assuming MILLISECONDS
2018-09-22 00:59:34,941 INFO Configuration.deprecation: No unit for hdds.write.lock.reporting.threshold.ms(5000) assuming MILLISECONDS
{code}",newbie,[],HDDS,Bug,Major,2018-09-22 01:01:51,119
13186743,Fix Ozone fs valid uri pattern,"The valid uri pattern for an Ozone fs uri should be {{o3fs://<hostname>:<port>/<volume>/<bucket>}}.

But OzoneFileSystem accepts uri's of the form {{o3fs://<bucket>.<volume>}} only.
{code:java}
# In OzoneFileSyste.java
private static final Pattern URL_SCHEMA_PATTERN =
    Pattern.compile(""(.+)\\.([^\\.]+)"");
if (!matcher.matches()) {
  throw new IllegalArgumentException(""Ozone file system url should be ""
      + ""in the form o3fs://bucket.volume"");
}{code}
",alpha2,['documentation'],HDDS,Bug,Blocker,2018-09-21 22:26:10,102
13186738,Some Ozone DataNode logs go to a separate ozone.log file,"Some, but not all DataNode logs go to a separate ozone.log file. Couple of things to fix here:
# The behavior should be consistent. All log messages should go to the new log file.
# The new log file name should follow the Hadoop log file convention e.g. _hadoop-<user>-<service>-<hostname>.log_",beta1,['Ozone Datanode'],HDDS,Bug,Blocker,2018-09-21 21:58:58,31
13186327,log4j is added with root to apache/hadoop:2 and apache/hadoop:3 images,"{code}
docker run -it apache/hadoop:2 ls -lah  /opt/hadoop/etc/hadoop

total 152K
drwxr-xr-x 1 hadoop users 4.0K Aug 13 17:08 .
drwxr-xr-x 1 hadoop users 4.0K Nov 13  2017 ..
-rw-r--r-- 1 hadoop users 7.7K Nov 13  2017 capacity-scheduler.xml
...
-rw-r--r-- 1 hadoop users 5.8K Nov 13  2017 kms-site.xml
-rw-r--r-- 1 root   root  1023 Aug 13 17:04 log4j.properties
-rw-r--r-- 1 hadoop users 1.1K Nov 13  2017 mapred-env.cmd
...
{code}

The owner of the log4j is root instead of hadoop. For this reason we can't use the images for acceptance tests as the launcher script can't overwrite log4j properties based on the environment variables.

Same is true with 

{code}
docker run -it apache/hadoop:2 ls -lah  /opt/hadoop/etc/hadoop
{code}
",newbie,[],HDDS,Bug,Major,2018-09-20 12:38:06,81
13186260,Implement DeleteObject REST endpoint,"Simple delete Object call.

Implemented by HDDS-444 without the acceptance tests.

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:44:43,6
13186258,Implement PutBucket REST endpoint,"The create bucket creates a bucket using createS3Bucket which has been added as part of HDDS-577.

[https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html]

Stub implementation is created as part of HDDS-444. Need to finalize, check the missing headers, add acceptance tests.",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:38:32,13
13186257,Implement DeleteBucket REST endpoint,"The delete bucket will do the opposite of the create bucket call. It will locate the volume via the username in the delete call.

Reference is here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html

This is implemented as part of HDDS-444 but we need the double check the headers and add acceptance tests.",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:35:16,13
13186256,Implement HeadBucket REST endpoint,"This operation is useful to determine if a bucket exists and you have permission to access it. The operation returns a 200 OK if the bucket exists and you have permission to access it. Otherwise, the operation might return responses such as 404 Not Found and 403 Forbidden.  

See the reference here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:34:05,13
13186254,Implement ListBucket REST endpoint,"You can also name it as GetService.

See te AWS reference:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html

The List Bucket API needs the call to be handled at the root resource (“/{volume}”).  
 
This implementation of the GET operation returns a list of all buckets owned by the authenticated sender of the request.

",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:31:48,120
13186253,Implement PutObject Rest endpoint,"The Put Object call allows users to add an Object to an S3 bucket.

The aws reference is here:

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html

We have an initial implementation in HDDS-444, but we need add the configurable chunk size (what we have in the upload from 'ozone sh' command), and support replication, replication type parameters.

We also need to support Content-MD5 header",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:29:54,114
13186251,Implement HeadObject REST endpoint,"The HEAD operation retrieves metadata from an object without returning the object itself. This operation is useful if you are interested only in an object's metadata. To use HEAD, you must have READ access to the object.

Steps:
 1. Look up the volume
 2. Read the key and return to the user.

The AWS reference is here:

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html

We have a simple version of this call in HDDS-444 but without Range support.",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:27:22,120
13186250,Implement CopyObject REST endpoint,"The Copy object is a simple call to Ozone Manager.  This API can only be done after the PUT OBJECT Call.

This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.

If the Put Object call has this header, then Put Object call will issue a rename. 

Work Items or JIRAs
Detect the presence of the extra header - x-amz-copy-source
Make sure that destination bucket exists.

The AWS reference is here:

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html

(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody would be interested, I can be more specific, explain what we need or help).",newbie,['S3'],HDDS,Sub-task,Major,2018-09-20 07:24:10,13
13186248,Implement GetObject REST endpoint,"The goal is implement the GetObject endpoint for the S3 Gateway:

There reference doc is here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html

This is mostly just a simple stream copy between the OzoneClient and the output, but we also need to:

1. support rest-response-* headers
2. Support Range headers (Jetty may support it out-of the box)

(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody interested I can be more specific).",newbie,[],HDDS,Sub-task,Major,2018-09-20 07:21:40,6
13186027,Add robot framework to the apache/hadoop-runner baseimage,"In HDDS-352 we moved the acceptance tests to the dist folder. Currently the framework is not part of the base image we need to install it all the time.

See the following lines in the [test.sh|https://github.com/apache/hadoop/blob/trunk/hadoop-dist/src/main/smoketest/test.sh]:

{code}
docker-compose -f ""$COMPOSE_FILE"" exec datanode sudo apt-get update
docker-compose -f ""$COMPOSE_FILE"" exec datanode sudo apt-get install -y python-pip
docker-compose -f ""$COMPOSE_FILE"" exec datanode sudo pip install robotframework
{code}

This could be removed after we add these lines to the [docker file|https://github.com/apache/hadoop/blob/docker-hadoop-runner/Dockerfile]:
",alpha2 newbie,['test'],HDDS,Improvement,Major,2018-09-19 10:35:26,81
13185918,OzoneManager HA,"OzoneManager can be a single point of failure in an Ozone cluster. We propose an HA implementation for OM using Ratis (Raft protocol).

Attached the design document for the proposed implementation.",Triaged,"['OM HA', 'Ozone Manager']",HDDS,New Feature,Major,2018-09-18 22:08:32,43
13185905,Ozone UI: Configuration page does not show description.,The Config page on OzoneManager UI does not show the Description. ,newbie,['Ozone Manager'],HDDS,Bug,Minor,2018-09-18 21:09:51,81
13185888,Suppress license warnings for error log files,"Let's suppress ASF license warnings for JVM error files. e.g.
{code}
Lines that start with ????? in the ASF License  report indicate files that do not have an Apache license header:
 !????? /testptch/hadoop/hadoop-ozone/integration-test/hs_err_pid4508.log
{code}",newbie,['Tools'],HDDS,Bug,Blocker,2018-09-18 20:01:04,113
13185878,Ozone docs and ozonefs packages have undefined hadoop component,"When building the ozone package, the docs and ozonefs packages create an UNDEF hadoop component in the share folder:
 * ./hadoop-ozone/ozonefs/target/hadoop-ozone-filesystem-0.3.0-SNAPSHOT/share/hadoop/UNDEF/lib
 * ./hadoop-ozone/docs/target/hadoop-ozone-docs-0.3.0-SNAPSHOT/share/hadoop/UNDEF/lib",newbie,[],HDDS,Bug,Major,2018-09-18 18:56:28,81
13185675,Minor typos in README.md in smoketest,"File:hadoop-dist/src/main/smoketest/README.md 

Line 23: robot smoketest/bascic should be change to robot smoketest/basic.

Line 30: ozone standalon should be changed to ozone standalone",newbie,[],HDDS,Bug,Trivial,2018-09-18 00:57:37,114
13185670,Improve om and scm start up options ,"I propose the following changes:
 # Rename _createObjectStore_ to _init_
 # Use double-dash for all long options (Unix convention)
 # Format existing object store should be a no-op. If a user runs:
{code:java}
ozone om --init{code}
And OM is already initialized, it should give a warning message and exit the process with success.",alpha2 incompatible,[],HDDS,Bug,Major,2018-09-18 00:13:11,118
13185636,Doc files are missing ASF license headers,"The following doc files are missing ASF license headers:
{code}
Lines that start with ????? in the ASF License  report indicate files that do not have an Apache license header:
 !????? /testptch/hadoop/hadoop-ozone/docs/content/BuildingSources.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/KeyCommands.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/Hdds.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/OzoneManager.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/BucketCommands.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/OzoneFS.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/VolumeCommands.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/JavaApi.md
 !????? /testptch/hadoop/hadoop-ozone/docs/content/RunningWithHDFS.md
{code}
",newbie,['documentation'],HDDS,Bug,Blocker,2018-09-17 20:58:13,118
13185464,Add more ozone fs tests in the robot integration framework,"Currently , we have few number of ozone fs tests in robot integration framework.

Need to add more.",alpha2,[],HDDS,Test,Minor,2018-09-17 10:15:25,20
13185438,Log files related to each daemon doesn't have proper startup and shutdown logs,"All the logs (startup/shutdown messages) go into ozone.log. We have a separate log file for each daemon and that log file doesn't contain these logs. 
{noformat}
[root@ctr-e138-1518143905142-468367-01-000002 logs]# cat ozone.log.2018-09-16 | head -20
2018-09-16 05:29:59,638 [main] INFO (LogAdapter.java:51) - STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting OzoneManager
STARTUP_MSG: host = ctr-e138-1518143905142-468367-01-000002.hwx.site/172.27.68.129
STARTUP_MSG: args = [-createObjectStore]
STARTUP_MSG: version = 3.2.0-SNAPSHOT
STARTUP_MSG: classpath = /root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/etc/hadoop:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-lang3-3.7.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-databind-2.9.5.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/hadoop-annotations-3.2.0-SNAPSHOT.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-3.2.0-SNAPSHOT.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-2.9.5.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share/hadoop/common/lib/commons-text-1.4.jar:/root/hadoop_trunk/ozone-0.3.0-SNAPSHOT/share{noformat}",alpha2,['Ozone Manager'],HDDS,Bug,Major,2018-09-17 08:41:19,81
13185257,Rename 'ozone oz' to 'ozone sh',"Ozone shell volume/bucket/key sub-commands are invoked using _ozone oz._ 

_ozone oz_ sounds repetitive. Instead we can replace it with _ozone sh_.",Incompatible,[],HDDS,Improvement,Blocker,2018-09-14 21:44:33,113
13184981,numberofKeys is 0 for all containers even when keys are present," 

numberofKeys field is 0 for all containers even when keys are present

 
{noformat}
[root@ctr-e138-1518143905142-459606-01-000005 bin]# ./ozone scmcli list --count=40 --start=1 | grep numberOfKeys
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,
 ""numberOfKeys"" : 0,{noformat}
 

 

 
{noformat}
[root@ctr-e138-1518143905142-459606-01-000005 bin]# ./ozone oz key list /fs-volume/fs-bucket/ | grep keyName
2018-09-13 19:10:33,502 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 ""keyName"" : ""15GBFILE""
 ""keyName"" : ""15GBFILE1""
 ""keyName"" : ""1GB1""
 ""keyName"" : ""1GB10""
 ""keyName"" : ""1GB11""
 ""keyName"" : ""1GB12""
 ""keyName"" : ""1GB13""
 ""keyName"" : ""1GB14""
 ""keyName"" : ""1GB15""
 ""keyName"" : ""1GB2""
 ""keyName"" : ""1GB3""
 ""keyName"" : ""1GB4""
 ""keyName"" : ""1GB5""
 ""keyName"" : ""1GB6""
 ""keyName"" : ""1GB7""
 ""keyName"" : ""1GB8""
 ""keyName"" : ""1GB9""
 ""keyName"" : ""1GBsecond1""
 ""keyName"" : ""1GBsecond10""
 ""keyName"" : ""1GBsecond11""
 ""keyName"" : ""1GBsecond12""
 ""keyName"" : ""1GBsecond13""
 ""keyName"" : ""1GBsecond14""
 ""keyName"" : ""1GBsecond15""
 ""keyName"" : ""1GBsecond2""
 ""keyName"" : ""1GBsecond3""
 ""keyName"" : ""1GBsecond4""
 ""keyName"" : ""1GBsecond5""
 ""keyName"" : ""1GBsecond6""
 ""keyName"" : ""1GBsecond7""
 ""keyName"" : ""1GBsecond8""
 ""keyName"" : ""1GBsecond9""
 ""keyName"" : ""2GBFILE""
 ""keyName"" : ""2GBFILE2""
 ""keyName"" : ""50GBFILE2""
 ""keyName"" : ""passwd1""{noformat}
 ",newbie,['SCM Client'],HDDS,Bug,Minor,2018-09-13 19:12:28,120
13184950,TestChunkStreams#testErrorReadGroupInputStream & TestChunkStreams#testReadGroupInputStream are failing,"TestChunkStreams.testErrorReadGroupInputStream & TestChunkStreams.testReadGroupInputStream test-cases are failing with the below error.

{code}
[ERROR] testErrorReadGroupInputStream(org.apache.hadoop.ozone.om.TestChunkStreams)  Time elapsed: 0.058 s  <<< ERROR!
java.lang.UnsupportedOperationException
	at org.apache.hadoop.ozone.om.TestChunkStreams$2.getPos(TestChunkStreams.java:188)
	at org.apache.hadoop.ozone.client.io.ChunkGroupInputStream$ChunkInputStreamEntry.getPos(ChunkGroupInputStream.java:245)
	at org.apache.hadoop.ozone.client.io.ChunkGroupInputStream$ChunkInputStreamEntry.getRemaining(ChunkGroupInputStream.java:217)
	at org.apache.hadoop.ozone.client.io.ChunkGroupInputStream.read(ChunkGroupInputStream.java:118)
	at org.apache.hadoop.ozone.om.TestChunkStreams.testErrorReadGroupInputStream(TestChunkStreams.java:214)

[ERROR] testReadGroupInputStream(org.apache.hadoop.ozone.om.TestChunkStreams)  Time elapsed: 0.001 s  <<< ERROR!
java.lang.UnsupportedOperationException
	at org.apache.hadoop.ozone.om.TestChunkStreams$1.getPos(TestChunkStreams.java:134)
	at org.apache.hadoop.ozone.client.io.ChunkGroupInputStream$ChunkInputStreamEntry.getPos(ChunkGroupInputStream.java:245)
	at org.apache.hadoop.ozone.client.io.ChunkGroupInputStream$ChunkInputStreamEntry.getRemaining(ChunkGroupInputStream.java:217)
	at org.apache.hadoop.ozone.client.io.ChunkGroupInputStream.read(ChunkGroupInputStream.java:118)
	at org.apache.hadoop.ozone.om.TestChunkStreams.testReadGroupInputStream(TestChunkStreams.java:159)
{code}",newbie,['Ozone Client'],HDDS,Improvement,Major,2018-09-13 16:22:52,114
13184931,OM and SCM should use picocli to parse arguments,"SCM and OM can use the picocli to parse command-line arguments.

Suggested in HDDS-415 by [~anu].",alpha2 newbie,"['Ozone Manager', 'SCM']",HDDS,Improvement,Major,2018-09-13 14:52:09,30
13184885,"PutKey failed due to error ""Rejecting write chunk request. Chunk overwrite without explicit request""","steps taken :

------------------
 # Ran Put Key command to write 50GB data. Put Key client operation failed after 17 mins.

error seen  ozone.log :

------------------------------------

 
{code}
2018-09-13 12:11:53,734 [ForkJoinPool.commonPool-worker-20] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_1 chunk stage:COMMIT_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_1 tmp chunk file
2018-09-13 12:11:56,576 [pool-3-thread-60] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 chunk stage:WRITE_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 tmp chunk file
2018-09-13 12:11:56,739 [ForkJoinPool.commonPool-worker-20] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 chunk stage:COMMIT_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 tmp chunk file
2018-09-13 12:12:21,410 [Datanode State Machine Thread - 0] DEBUG (DatanodeStateMachine.java:148) - Executing cycle Number : 206
2018-09-13 12:12:51,411 [Datanode State Machine Thread - 0] DEBUG (DatanodeStateMachine.java:148) - Executing cycle Number : 207
2018-09-13 12:12:53,525 [BlockDeletingService#1] DEBUG (TopNOrderedContainerDeletionChoosingPolicy.java:79) - Stop looking for next container, there is no pending deletion block contained in remaining containers.
2018-09-13 12:12:55,048 [Datanode ReportManager Thread - 1] DEBUG (ContainerSet.java:191) - Starting container report iteration.
2018-09-13 12:13:02,626 [pool-3-thread-1] ERROR (ChunkUtils.java:244) - Rejecting write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216}
2018-09-13 12:13:03,035 [pool-3-thread-1] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 54834b29-603d-4ba9-9d68-0885215759d8 : Message: Rejecting write chunk request. OverWrite flag required.ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216} : Result: OVERWRITE_FLAG_REQUIRED
2018-09-13 12:13:03,037 [ForkJoinPool.commonPool-worker-11] ERROR (ChunkUtils.java:244) - Rejecting write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216}
2018-09-13 12:13:03,037 [ForkJoinPool.commonPool-worker-11] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 54834b29-603d-4ba9-9d68-0885215759d8 : Message: Rejecting write chunk request. OverWrite flag required.ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216} : Result: OVERWRITE_FLAG_REQUIRED
 
{code}
 ",alpha2,['Ozone Client'],HDDS,Bug,Blocker,2018-09-13 12:35:29,16
13184864,Add a NULL check to protect DeadNodeHandler#onMessage,"Add a NULL check to protect the situation below(may only happened in the case of unit test):
 1.A new datanode register to SCM.
 2. There is no container allocated in the new datanode temporarily.
 3.The new datanode dead and an event was fired to {{DeadNodeHandler}}
 4.In function {{DeadNodeHandler#onMessage}}, there will get nothing in {{node2ContainerMap}} and {{containers}} will be {{NULL}}
 5.NullPointerException will be throwen in the following iterate of {{containers}} like:
{noformat}
[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.535 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.node.TestDeadNodeHandler
[ERROR] testStatisticsUpdate(org.apache.hadoop.hdds.scm.node.TestDeadNodeHandler)  Time elapsed: 0.33 s  <<< ERROR!
java.lang.NullPointerException
        at org.apache.hadoop.hdds.scm.node.DeadNodeHandler.onMessage(DeadNodeHandler.java:68)
        at org.apache.hadoop.hdds.scm.node.TestDeadNodeHandler.testStatisticsUpdate(TestDeadNodeHandler.java:179)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
{noformat}",EasyFix,[],HDDS,Improvement,Minor,2018-09-13 11:09:27,120
13184843,Add rest service to the s3gateway,"The next step is after HDDS-441 is to add a rest server to the s3gateway service.

For the http server the obvious choice is to use org.apache.hadoop.http.HttpServer2. We also have a org.apache.hadoop.hdds.server.BaseHttpServer which helps to create the HttpServer2.

In hadoop usually the jersey 1.19 is used. I prefer to exclude jersey dependency from the s3gateway and add the latest jersey2. Hopefully it also could be initialized easily, similar to HttpServer2.addJerseyResourcePackage

The trickiest part is the resource handling. By default the input parameter of the jersey is the JAX-RS resource class and jersey creates new instances from the specified resource classes.

But with this approach we can't inject other components (such as the OzoneClient) to the resource classes. In Hadoop usually a singleton is used or the reference object is injected to the ServletContext. Both of these are just workaround and make the testing harder.

I propose to use some lightweight managed dependency injection:
 # If we can use and JettyApi to instantiate the resource classes, that would be the easiest one.
 # Using a simple CDI framework like dagger, also would help. Dagger is very lightweight, it doesn't support request scoped objects just simple @Inject annotations, but hopefully we won't need fancy new features.
 # The most complex solution would be to use CDI or Guice. CDI seems to be more nature choice for the JAX-RS. It can be checked how easy is to integrate Weld to the Jetty + Jersey combo.

The expected end result of this task is a new HttpServer subcomponent inside the s3gateway which could be started/stopped. We need an example simple service (for exampe a /health endpoint which returns with an 'OK' string) which can demonstrate how our own utilitites (such as OzoneClient) could be injected to the REST resources.",newbie,[],HDDS,Sub-task,Major,2018-09-13 09:22:42,6
13184836,Create reusable ProgressBar utility for freon tests,"Since HDDS-398 we can support multiple type of freon tests. But to add more test we need common utilities for generic task.

One of the most important is to provide a reusable Progressbar utility.

Currently the ProgressBar class is part the RandomKeyGenerator. It should be moved out from the class and all the thread start/stop logic should be moved to the ProgressBar.

{{ProgressBar bar = new ProgressBar(System.out, () ->  ... , 200);}}
{{bar.start(); // thred should be started here}}{{bar.stop(); // thread should be stopped.}}

 ",newbie,['test'],HDDS,Improvement,Major,2018-09-13 09:04:11,121
13184803,Create new s3gateway daemon,"The first element what we need is a new command line application to start the s3 gateway.

1. A new project should be introduced: hadoop-ozone/s3-gateway

2. A new command line application (eg. org.apache.hadoop.ozone.s3.Gateway should be added with a simple main and start/stop method which just prints out a starting/stopping log message

3. hadoop-ozone/common/src/main/bin/ozone should be modified to manage the new service (eg. ozone s3g start, ozone s3g stop)

4. to make it easier to test a new docker-compose based test cluster should be added to the hadoop-dist/src/main/compose (the normal ./ozone could be copied but we need to add the new s3g component)",newbie,[],HDDS,Sub-task,Major,2018-09-13 06:56:32,6
13184761,Datanode loops forever if it cannot create directories,"Datanode starts but runs in a tight loop forever if it cannot create the DataNode ID directory e.g. due to permissions issues. I encountered this by having a typo in my ozone-site.xml for {{ozone.scm.datanode.id}}.

In just a few minutes the DataNode had generated over 20GB of log+out files with the following exception:
{code:java}
2018-09-12 17:28:20,649 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 2
63:
java.io.IOException: Unable to create datanode ID directories.
at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
2018-09-12 17:28:20,648 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Execution exception when running task in Datanode State Mach
ine Thread - 160
2018-09-12 17:28:20,650 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 1
60:
java.io.IOException: Unable to create datanode ID directories.
at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){code}

We should just exit since this is a fatal issue.",newbie,['Ozone Datanode'],HDDS,Bug,Blocker,2018-09-13 00:32:35,13
13184758,'ozone oz volume create' should default to current logged in user,"Currently the user parameter appears to be mandatory. It should just default to the current Unix user if missing.

E.g.
{code:java}
$ ozone oz volume create vol32
Missing required option '--user=<userName>'{code}",newbie,['Tools'],HDDS,Improvement,Blocker,2018-09-13 00:07:41,81
13184750,'ozone oz' should print usage when command or sub-command is missing,"When invoked without the command or sub-command, _ozone oz_ prints the following error:
{code:java}
$ ozone oz
Please select a subcommand
{code}
and
{code:java}
$ ozone oz volume
Please select a subcommand
{code}
For most familiar with Unix utilities it is obvious they should rerun the command with _--help._ However we can just print the usage instead to avoid guesswork",newbie usability,['Tools'],HDDS,Improvement,Major,2018-09-12 23:16:53,81
13184113,Add field modificationTime for Volume and Bucket,"There are update operations that can be performed for Volume, Bucket and Key.

While Key records the modification time, Volume and & Bucket do not capture this.

 

This Jira proposes to add the required field to Volume and Bucket in order to capture the modficationTime.
(The modification time of this Jira only applies to volume/bucket metadata modification itself, *not covering key changes under the volume bucket*.)
 

Current Status:
{noformat}
hadoop@1987b5de4203:~$ ./bin/ozone oz -infoVolume /dummyvol
2018-09-10 17:16:12 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{
""owner"" : {
""name"" : ""bilbo""
},
""quota"" : {
""unit"" : ""TB"",
""size"" : 1048576
},
""volumeName"" : ""dummyvol"",
""createdOn"" : ""Mon, 10 Sep 2018 17:11:32 GMT"",
""createdBy"" : ""bilbo""
}

hadoop@1987b5de4203:~$ ./bin/ozone oz -infoBucket /dummyvol/mybuck
2018-09-10 17:15:25 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{
""volumeName"" : ""dummyvol"",
""bucketName"" : ""mybuck"",
""createdOn"" : ""Mon, 10 Sep 2018 17:12:09 GMT"",
""acls"" : [ {
""type"" : ""USER"",
""name"" : ""hadoop"",
""rights"" : ""READ_WRITE""
}, {
""type"" : ""GROUP"",
""name"" : ""users"",
""rights"" : ""READ_WRITE""
}, {
""type"" : ""USER"",
""name"" : ""spark"",
""rights"" : ""READ_WRITE""
} ],
""versioning"" : ""DISABLED"",
""storageType"" : ""DISK""
}

hadoop@1987b5de4203:~$ ./bin/ozone oz -infoKey /dummyvol/mybuck/myk1
2018-09-10 17:19:43 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{
""version"" : 0,
""md5hash"" : null,
""createdOn"" : ""Mon, 10 Sep 2018 17:19:04 GMT"",
""modifiedOn"" : ""Mon, 10 Sep 2018 17:19:04 GMT"",
""size"" : 0,
""keyName"" : ""myk1"",
""keyLocations"" : [ ]
}{noformat}",Triaged backlog newbie pull-request-available,['Ozone Manager'],HDDS,Improvement,Major,2018-09-10 17:51:54,31
13184004,ContainerStateMachine.readStateMachineData throws OverlappingFileLockException," 
{code:java}
2018-09-06 23:11:41,386 ERROR org.apache.ratis.server.impl.LogAppender: GRpcLogAppender(d95c60fd-0e23-4237-8135-e05a326b952d_9858 -> 954e7a3b-b20e-43a5-8f82-4381872aa7bb_9858) hit IOException while loadin
g raft log
org.apache.ratis.server.storage.RaftLogIOException: d95c60fd-0e23-4237-8135-e05a326b952d_9858: Failed readStateMachineData for (t:39, i:667)SMLOGENTRY, client-CD988394E416, cid=90
at org.apache.ratis.server.storage.RaftLog$EntryWithData.getEntry(RaftLog.java:360)
at org.apache.ratis.server.impl.LogAppender$LogEntryBuffer.getAppendRequest(LogAppender.java:165)
at org.apache.ratis.server.impl.LogAppender.createRequest(LogAppender.java:214)
at org.apache.ratis.grpc.server.GRpcLogAppender.appendLog(GRpcLogAppender.java:148)
at org.apache.ratis.grpc.server.GRpcLogAppender.runAppenderImpl(GRpcLogAppender.java:92)
at org.apache.ratis.server.impl.LogAppender.runAppender(LogAppender.java:101)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.OverlappingFileLockException
at sun.nio.ch.SharedFileLockTable.checkList(FileLockTable.java:255)
at sun.nio.ch.SharedFileLockTable.add(FileLockTable.java:152)
at sun.nio.ch.AsynchronousFileChannelImpl.addToFileLockTable(AsynchronousFileChannelImpl.java:178)
at sun.nio.ch.SimpleAsynchronousFileChannelImpl.implLock(SimpleAsynchronousFileChannelImpl.java:185)
at sun.nio.ch.AsynchronousFileChannelImpl.lock(AsynchronousFileChannelImpl.java:118)
at org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils.readData(ChunkUtils.java:176)
at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerImpl.readChunk(ChunkManagerImpl.java:161)
at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleReadChunk(KeyValueHandler.java:598)
at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:201)
at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:142)
at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:217)
at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:289)
at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$readStateMachineData$3(ContainerStateMachine.java:359)
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
... 1 more
{code}",Arches-Deferral-Candidate,['Ozone Datanode'],HDDS,Bug,Major,2018-09-10 10:34:47,35
13183718,OzoneCLI: clean up ozone oz help messages,"hdfs o3 should be ozone oz

 

{code}

 -createVolume <arg>        creates a volumefor the specified user.

                            For example : hdfs o3  -createVolume <volumeURI>

                            -root -user <userName>

{code}

 

 ",newbie,['Ozone CLI'],HDDS,Bug,Major,2018-09-07 19:34:19,81
13183670,Add Ozone submodule to the hadoop.apache.org,"The current hadoop.apache.org doesn't mention Ozone in the ""Modules"" section.

We can add something like this (or better):
{quote}Hadoop Ozone is an object store for Hadoop on top of the Hadoop HDDS which provides low-level binary storage layer.
{quote}
We can also link to the [http://ozone.hadoop.apache.org|http://ozone.hadoop.apache.org/]",site,[],HDDS,Improvement,Trivial,2018-09-07 16:32:40,81
13183463,Ozone acceptance-test and integration-test packages have undefined hadoop component,"When building the ozone package, the acceptance-test and integration-test packages create an UNDEF hadoop component in the share folder:
 * ./hadoop-ozone/acceptance-test/target/hadoop-ozone-acceptance-test-3.2.0-SNAPSHOT/share/hadoop/UNDEF/lib
 * ./hadoop-ozone/integration-test/target/hadoop-ozone-integration-test-0.2.1-SNAPSHOT/share/hadoop/UNDEF/lib ",newbie,[],HDDS,Bug,Major,2018-09-06 17:45:58,81
13183289,Implement toString() in OmKeyLocationInfo,"OmKeyLocationInfo does not have an overridden toString().

As such, the information is not captured appropriately in Audit Logging.

Example:

{{2018-09-06 01:57:22,950 | INFO  | OMAudit | user=hadoop | ip=172.18.0.4 | op=COMMIT_KEY | \{volume=vol-0-16241, bucket=bucket-0-61479, key=key-257-04655, dataSize=10240, replicationType=null, replicationFactor=null, keyLocationInfo=[org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo@5fb8c959], clientID=61945301411} | ret=SUCCESS |}}

 

This Jira proposes to implement the required toString() so that audit log contains appropriate data.",Logging,['Ozone Manager'],HDDS,Improvement,Major,2018-09-06 04:12:25,81
13183094,"Fix createdOn and modifiedOn timestamp for volume, bucket, key","Example for Key shown below. Similar observations can be made for Bucket and Volume.
 # ran putKey command for a file

{noformat}
[root@ctr-e138-1518143905142-459606-01-000003 bin]# ./ozone oz -putKey /test-vol1/test-bucket1/file1 -file /etc/passwd -v
2018-09-05 10:25:11,498 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Volume Name : test-vol1
Bucket Name : test-bucket1
Key Name : file1
File Hash : 8164cc3d5b05c44b73a6277661aa4645
2018-09-05 10:25:12,377 INFO conf.ConfUtils: raft.rpc.type = GRPC (default)
2018-09-05 10:25:12,390 INFO conf.ConfUtils: raft.grpc.message.size.max = 33554432 (custom)
2018-09-05 10:25:12,402 INFO conf.ConfUtils: raft.client.rpc.retryInterval = 300 ms (default)
2018-09-05 10:25:12,407 INFO conf.ConfUtils: raft.client.async.outstanding-requests.max = 100 (default)
2018-09-05 10:25:12,407 INFO conf.ConfUtils: raft.client.async.scheduler-threads = 3 (default)
2018-09-05 10:25:12,518 INFO conf.ConfUtils: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2018-09-05 10:25:12,518 INFO conf.ConfUtils: raft.grpc.message.size.max = 33554432 (custom)
2018-09-05 10:25:12,866 INFO conf.ConfUtils: raft.client.rpc.request.timeout = 3000 ms (default)
2018-09-05 10:25:13,644 INFO conf.ConfUtils: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2018-09-05 10:25:13,644 INFO conf.ConfUtils: raft.grpc.message.size.max = 33554432 (custom)
2018-09-05 10:25:13,645 INFO conf.ConfUtils: raft.client.rpc.request.timeout = 3000 ms (default)
[root@ctr-e138-1518143905142-459606-01-000003 bin]# ./ozone oz -getKey /test-vol1/test-bucket1/file1 -file getkey3
2018-09-05 10:25:22,020 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-09-05 10:25:22,778 INFO conf.ConfUtils: raft.rpc.type = GRPC (default)
2018-09-05 10:25:22,790 INFO conf.ConfUtils: raft.grpc.message.size.max = 33554432 (custom)
2018-09-05 10:25:22,800 INFO conf.ConfUtils: raft.client.rpc.retryInterval = 300 ms (default)
2018-09-05 10:25:22,804 INFO conf.ConfUtils: raft.client.async.outstanding-requests.max = 100 (default)
2018-09-05 10:25:22,805 INFO conf.ConfUtils: raft.client.async.scheduler-threads = 3 (default)
2018-09-05 10:25:22,890 INFO conf.ConfUtils: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2018-09-05 10:25:22,890 INFO conf.ConfUtils: raft.grpc.message.size.max = 33554432 (custom)
2018-09-05 10:25:23,250 INFO conf.ConfUtils: raft.client.rpc.request.timeout = 3000 ms (default)
2018-09-05 10:25:24,066 INFO conf.ConfUtils: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2018-09-05 10:25:24,067 INFO conf.ConfUtils: raft.grpc.message.size.max = 33554432 (custom)
2018-09-05 10:25:24,067 INFO conf.ConfUtils: raft.client.rpc.request.timeout = 3000 ms (default){noformat}
2. Ran infoKey on that key
{noformat}
[root@ctr-e138-1518143905142-459606-01-000003 bin]# ./ozone oz -infoKey /test-vol1/test-bucket1/file1 -v
2018-09-05 10:54:42,053 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Volume Name : test-vol1
Bucket Name : test-bucket1
Key Name : file1
{
 ""version"" : 0,
 ""md5hash"" : null,
 ""createdOn"" : ""Sat, 14 Dec +114522267 00:51:17 GMT"",
 ""modifiedOn"" : ""Fri, 09 Jun +50648 04:30:12 GMT"",
 ""size"" : 4659,
 ""keyName"" : ""file1"",
 ""keyLocations"" : [ {
 ""containerID"" : 16,
 ""localID"" : 1536143112267,
 ""length"" : 4659,
 ""offset"" : 0
 } ]
}{noformat}
""createdOn"" and ""modifiedOn"" metadata are incorrect.

Here is the current date:
{noformat}
[root@ctr-e138-1518143905142-459606-01-000003 bin]# date
Wed Sep 5 10:54:52 UTC 2018{noformat}
Also , the ""md5hash"" for the key is showing as null.",newbie,['Ozone Manager'],HDDS,Bug,Blocker,2018-09-05 10:57:10,81
13182564,Remove openContainers.db from SCM,openContainers.db(OPEN_CONTAINERS_DB) is not being used anywhere in the code right now. It can be removed from the code as well.,newbie,['SCM'],HDDS,Bug,Major,2018-09-03 04:06:36,81
13182562,"TestOzoneRestWithMiniCluster fails with ""Unable to read ROCKDB config""","Ozone datanode initialization is failing with the following exception.

This was noted in the following precommit build result.
https://builds.apache.org/job/PreCommit-HDDS-Build/935/testReport/org.apache.hadoop.ozone.web/TestOzoneRestWithMiniCluster/org_apache_hadoop_ozone_web_TestOzoneRestWithMiniCluster_2/

{code}
2018-09-02 20:56:33,501 INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(176)) - Unable to read ROCKDB config
java.io.IOException: Unable to find the configuration directory. Please make sure that HADOOP_CONF_DIR is setup correctly 
	at org.apache.hadoop.utils.db.DBConfigFromFile.getConfigLocation(DBConfigFromFile.java:62)
	at org.apache.hadoop.utils.db.DBConfigFromFile.readFromFile(DBConfigFromFile.java:118)
	at org.apache.hadoop.utils.db.DBStoreBuilder.getDbProfile(DBStoreBuilder.java:170)
	at org.apache.hadoop.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:122)
	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.<init>(OmMetadataManagerImpl.java:133)
	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:146)
	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:295)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.createOM(MiniOzoneClusterImpl.java:357)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:304)
	at org.apache.hadoop.ozone.web.TestOzoneRestWithMiniCluster.init(TestOzoneRestWithMiniCluster.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)
{code}",pull-request-available,['Ozone Manager'],HDDS,Bug,Major,2018-09-03 03:44:07,81
13182453,Audit Parser tool for processing ozone audit logs,"This jira propses to create audit parser tool to process ozone audit logs.
This tool should give a command line feature to load and query audit logs.",alpha2,['Tools'],HDDS,New Feature,Major,2018-08-31 23:15:10,81
13182435,Add method to check for valid key name based on URI characters,"As per design, key names composed of all valid characters in URI set must be treated as valid key name.

For URI character set: [https://tools.ietf.org/html/rfc2396#appendix-A]

This Jira proposes to define validateKeyName() similar to validateResourceName() that validates bucket/volume name

 

Valid Key name must:
 * conform to URI Character set
 * must allow /

TBD whether key names must impose other rules similar to volume/bucket names like  -
 * should not start with period or dash
 * should not end with period or dash
 * should not have contiguous periods
 * should not have period after dash and vice versa

etc

 ",Triaged,['Ozone Manager'],HDDS,Improvement,Major,2018-08-31 21:34:17,81
13181721,Add api to remove event handler in EventQueue,Add api to remove event handler in EventQueue.,alpha2,[],HDDS,New Feature,Major,2018-08-28 20:51:43,110
13181326,Remove dependencies between hdds/ozone and hdfs proto files,"It would be great to make the hdds/ozone proto files independent from hdfs proto files. It would help as to start ozone with multiple version of hadoop version.

Also helps to make artifacts from the hdds protos:  HDDS-220

 Currently we have a few unused ""hdfs.proto"" import in the proto files and we use the StorageTypeProto from hdfs:

{code}
cd hadoop-hdds
grep -r ""hdfs"" --include=""*.proto""
common/src/main/proto/ScmBlockLocationProtocol.proto:import ""hdfs.proto"";
common/src/main/proto/StorageContainerLocationProtocol.proto:import ""hdfs.proto"";

 cd ../hadoop-ozone
grep -r ""hdfs"" --include=""*.proto""
common/src/main/proto/OzoneManagerProtocol.proto:import ""hdfs.proto"";
common/src/main/proto/OzoneManagerProtocol.proto:    required hadoop.hdfs.StorageTypeProto storageType = 5 [default = DISK];
common/src/main/proto/OzoneManagerProtocol.proto:    optional hadoop.hdfs.StorageTypeProto storageType = 6;
{code}

I propose to 

1.) remove the hdfs import statements from the proto files
2.) Copy the StorageTypeProto and create a Hdds version from it (without PROVIDED)",newbie,[],HDDS,Improvement,Major,2018-08-27 12:25:59,6
13181074,Create custom message structure for use in AuditLogging,"In HDDS-198 we introduced a framework for AuditLogging in Ozone.

We had used StructuredDataMessage for formatting the messages to be logged.

 

Based on discussion with [~jnp] and [~anu], this Jira proposes to create a custom message structure to generate audit messages in the following format:

user=xxx ip=xxx op=XXXX_XXXX \{key=val, key1=val1..} ret=XXXXXX",audit logging,[],HDDS,Improvement,Major,2018-08-24 22:13:48,81
13180876,Genconf tool must generate ozone-site.xml with sample values,"As discussed with [~anu], currently, the genconf tool generates a template ozone-site.xml. This is not very useful for new users as they would have to understand what values should be set for the minimal configuration properties.

This Jira proposes to modify the ozone-default.xml which is leveraged by genconf tool to generate ozone-site.xml

 

Further, as suggested by [~arpitagarwal], we must add a {{--pseudo}} option to generate configs for starting pseudo-cluster. This should be useful for quick dev-testing.",pull-request-available,['Tools'],HDDS,Improvement,Major,2018-08-24 06:04:24,81
13180787,Add RetriableException class in Ozone,"Certain Exception thrown by a server can be because server is in a state
where request cannot be processed temporarily.
 Ozone Client may retry the request. If the service is up, the server may be able to
 process a retried request. This Jira aims to introduce notion of RetriableException in Ozone.",TriagePending,['Ozone Client'],HDDS,Bug,Major,2018-08-23 17:58:28,16
13180534,Add and implement following functions in SCMClientProtocolServer,"Add and implement following functions in SCMClientProtocolServer
# isScmInChillMode
# forceScmExitChillMode",alpha2,[],HDDS,New Feature,Major,2018-08-22 18:22:45,110
13179303,Separate install and testing phases in acceptance tests.,"In the current acceptance tests (hadoop-ozone/acceptance-test) the robot files contain two kind of commands:

1) starting and stopping clusters
2) testing the basic behaviour with client calls

It would be great to separate the two functionality and include only the testing part in the robot files.

1. Ideally the tests could be executed in any environment. After a kubernetes install I would like to do a smoke test. It could be a different environment but I would like to execute most of the tests (check ozone cli, rest api, etc.)

2. There could be multiple ozone environment (standlaone ozone cluster, hdfs + ozone cluster, etc.). We need to test all of them with all the tests.

3. With this approach we can collect the docker-compose files just in one place (hadoop-dist project). After a docker-compose up there should be a way to execute the tests with an existing cluster. Something like this:

{code}
docker run -it apache/hadoop-runner -v ./acceptance-test:/opt/acceptance-test -e SCM_URL=http://scm:9876 --network=composenetwork start-all-tests.sh
{code}

4. It also means that we need to execute the tests from a separated container instance. We need a configuration parameter to define the cluster topology. Ideally it could be just one environment variables with the url of the scm and the scm could be used to discovery all of the required components + download the configuration files from there.

5. Until now we used the log output of the docker-compose files to do some readiness probes. They should be converted to poll the jmx endpoints and check if the cluster is up and running. If we need the log files for additional testing we can create multiple implementations for different type of environments (docker-compose/kubernetes) and include the right set of functions based on an external parameters.

6. Still we need a generic script under the ozone-acceptance test project to run all the tests (starting the docker-compose clusters, execute tests in a different container, stop the cluster) 
",test,[],HDDS,Improvement,Major,2018-08-16 08:57:13,6
13177721,ContainerStateMachine#readStateMachinedata should read from temporary chunk file if the data is not present as committed chunk,"ContainerStateMachine#readStateMachinedata currently only reads data from a committed chunk right now. However for leader, it might be necessary to read the chunk data from the temporary chunk.",TriagePending,['Ozone Datanode'],HDDS,Bug,Major,2018-08-08 16:28:08,18
13177616,Print out container location information for a specific ozone key ,"In the protobuf protocol we have all the containerid/localid(=blockid) information for a specific ozone key.

It would be a big help to print out this information to the command line with the ozone cli.

It requires to improve the REST and RPC interface with additionalOzone KeyLocation information.

It would help a very big help during the test of the current scm behaviour.",newbie,['SCM'],HDDS,Improvement,Major,2018-08-08 08:24:05,120
13177555,Update GettingStarted page to mention details about Ozone GenConf tool,Add description about Ozone GenConf tool in GettingStarted page,documentation,['documentation'],HDDS,Bug,Minor,2018-08-08 00:02:10,81
13176406,ratis INFO logs should not shown during ozoneFs command-line execution,"ratis INFOs should not be shown during ozoneFS CLI execution.

Please find the snippet from one othe execution :

 
{noformat}
hadoop@08315aa4b367:~/bin$ ./ozone fs -put /etc/passwd /p2
2018-08-02 12:17:18 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.rpc.type = GRPC (default)
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.grpc.message.size.max = 33554432 (custom)
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.client.rpc.retryInterval = 300 ms (default)
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.client.async.outstanding-requests.max = 100 (default)
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.client.async.scheduler-threads = 3 (default)
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2018-08-02 12:17:19 INFO ConfUtils:41 - raft.grpc.message.size.max = 33554432 (custom)
2018-08-02 12:17:20 INFO ConfUtils:41 - raft.client.rpc.request.timeout = 3000 ms (default)
Aug 02, 2018 12:17:20 PM org.apache.ratis.shaded.io.grpc.internal.ProxyDetectorImpl detectProxy
WARNING: Failed to construct URI for proxy lookup, proceeding without proxy
..
..
..
 
{noformat}
 ",newbie,['Ozone Filesystem'],HDDS,Bug,Blocker,2018-08-02 12:19:03,9
13176401,Use new StorageSize API for reading ozone.scm.container.size.gb,"Container size is configured using property {{ozone.scm.container.size.gb}}. This can be renamed to {{ozone.scm.container.size}} and use new StorageSize API to read the value.

The property is defined in
 1. ozone-default.xml
 2. ScmConfigKeys#OZONE_SCM_CONTAINER_SIZE_GB

The default value is defined in
 1. ozone-default.xml
 2. {{ScmConfigKeys#OZONE_SCM_CONTAINER_SIZE_DEFAULT}}",newbie,['SCM'],HDDS,Improvement,Major,2018-08-02 12:10:03,107
13175286,Add InterfaceAudience/InterfaceStability annotations,This Jira is to add IntefaceAudience for datanode code in the container-service module.,Triaged,[],HDDS,Improvement,Major,2018-07-27 22:58:09,13
13174178,Volume and Bucket name should not contain a delimiter ('/'),"Please see below example. Here the user issues command to create bucket like below. Where /namit is the volume. 
{code}
hadoop@288c0999be17:~$ ozone oz -createBucket /namit/hjk/fgh
2018-07-24 00:30:52 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-07-24 00:30:52 INFO  RpcClient:337 - Creating Bucket: namit/hjk, with Versioning false and Storage Type set to DISK
{code}
As seen above it just ignored '/fgh'
There should be a Warning / Error message instead of just ignoring everything after a '/' 
",newbie,['Ozone Client'],HDDS,Bug,Major,2018-07-24 18:50:50,114
13174002,Fix NodeReportPublisher.getReport NPE,"This can be reproed with TestKeys#testPutKey

{code}
2018-07-23 21:33:55,598 WARN  concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread Datanode ReportManager Thread - 0: 
java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.common.volume.VolumeInfo.getScmUsed(VolumeInfo.java:107)
	at org.apache.hadoop.ozone.container.common.volume.VolumeSet.getNodeReport(VolumeSet.java:350)
	at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.getNodeReport(OzoneContainer.java:260)
	at org.apache.hadoop.ozone.container.common.report.NodeReportPublisher.getReport(NodeReportPublisher.java:64)
	at org.apache.hadoop.ozone.container.common.report.NodeReportPublisher.getReport(NodeReportPublisher.java:39)
	at org.apache.hadoop.ozone.container.common.report.ReportPublisher.publishReport(ReportPublisher.java:86)
	at org.apache.hadoop.ozone.container.common.report.ReportPublisher.run(ReportPublisher.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}",newbie,[],HDDS,Bug,Major,2018-07-24 05:02:35,107
13173770,Recon: Add container size distribution visualization,"It would be good if we have some metric/histogram in OzoneManager UI indicating the different container size range and corresponding percentages for the same created in the cluster.

For example :

0-2 GB           10%

2-4 GB .         20%

4-5 GB           70%

5+ GB            0%

 ",TriagePending,['Ozone Recon'],HDDS,Improvement,Minor,2018-07-23 10:42:26,37
13173212,'oz' subcommand reference is not present in 'ozone' command help,"'oz' subcommand is not present in ozone help.

 

ozone help:

------------------------

 
{noformat}
hadoop@8ceb8dfccb36:~/bin$ ./ozone
Usage: ozone [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
OPTIONS is none or any of:
--buildpaths attempt to add class files from build tree
--config dir Hadoop config directory
--daemon (start|status|stop) operate on a daemon
--debug turn on shell script debug mode
--help usage information
--hostnames list[,of,host,names] hosts to use in worker mode
--hosts filename list of hosts to use in worker mode
--loglevel level set the log4j level for this command
--workers turn on worker mode
SUBCOMMAND is one of:

 Admin Commands:
jmxget get JMX exported values from NameNode or DataNode.
Client Commands:
classpath prints the class path needed to get the hadoop jar and the
 required libraries
envvars display computed Hadoop environment variables
freon runs an ozone data generator
genconf generate minimally required ozone configs and output to
 ozone-site.xml in specified path
genesis runs a collection of ozone benchmarks to help with tuning.
getozoneconf get ozone config values from configuration
noz ozone debug tool, convert ozone metadata into relational data
o3 command line interface for ozone
scmcli run the CLI of the Storage Container Manager
version print the version
Daemon Commands:
datanode run a HDDS datanode
om Ozone Manager
scm run the Storage Container Manager service
SUBCOMMAND may print help when invoked w/o parameters or with -h.
{noformat}
 

'oz' subcommand example :

----------------------------------------

 
{noformat}
hadoop@8ceb8dfccb36:~/bin$ ./ozone oz -listVolume /
2018-07-19 14:51:25 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[ {
 ""owner"" : {
 ""name"" : ""hadoop""
 },
 ""quota"" : {
 ""unit"" : ""TB"",
 ""size"" : 1048576
 },
 ""volumeName"" : ""vol-0-01597"",
 ""createdOn"" : ""Sat, 20 Feb +50517 10:11:35 GMT"",
 ""createdBy"" : ""hadoop""
}, {
 ""owner"" : {
 ""name"" : ""hadoop""
 },
 ""quota"" : {
 ""unit"" : ""TB"",
 ""size"" : 1048576
 },
 ""volumeName"" : ""vol-0-19478"",
 ""createdOn"" : ""Thu, 03 Jun +50517 22:23:12 GMT"",
 ""createdBy"" : ""hadoop""
}, {
 ""owner"" : {
 ""name"" : ""hadoop""
 },
 ""quota"" : {
 ""unit"" : ""TB"",
 ""size"" : 1048576
 }
 
{noformat}
 

 ",newbie,['Ozone Client'],HDDS,Bug,Minor,2018-07-19 14:56:17,97
13171982,Fix TestOzoneConfigurationFields for missing hdds.command.status.report.interval in config classes ,"{{TestOzoneConfigurationFields}} is failing with the below error
{noformat}
TestOzoneConfigurationFields>TestConfigurationFieldsBase.testCompareXmlAgainstConfigurationClass:540 
ozone-default.xml has 1 properties missing in  class org.apache.hadoop.ozone.OzoneConfigKeys  
class org.apache.hadoop.hdds.scm.ScmConfigKeys  class org.apache.hadoop.ozone.om.OMConfigKeys 
Entries:   hdds.command.status.report.interval expected:<0> but was:<1>
{noformat}",newbie,['test'],HDDS,Sub-task,Minor,2018-07-13 11:28:58,97
13171796,Eliminate the datanode ID file,"This Jira is to remove the datanodeID file. After ContainerIO  work (HDDS-48 branch) is merged, we have a version file in each Volume which stores datanodeUuid and some additional fields in that file.

And also if this disk containing datanodeId path is removed, that DN will now be unusable with current code.",alpha2,[],HDDS,Improvement,Major,2018-07-12 18:25:06,13
13171012,Update docs to reflect the new name for Ozone Manager,KSM has been renamed to Ozone Manager. The docs still refer to KSM. This JIRA is to track that docs issue.,newbie,['Ozone Manager'],HDDS,Improvement,Major,2018-07-09 23:45:37,97
13170083,Create acceptance test for using datanode plugin,"In the current docker-compose files (both in the hadoop-dist and acceptance-test) we use  simplified ozone clusters: there is no namenode and we use standalone hdds datanode processes.

To test ozone/hdds as a datanode plugin we need to create separated acceptance tests which uses hadoop:3.1 and hadoop:3.0 + ozone hdds datanode plugin artifact",alpha2 newbie,[],HDDS,Improvement,Major,2018-07-04 13:56:31,97
13170081,Remove hdfs command line from ozone distribution.,"As the ozone release artifact doesn't contain a stable namenode/datanode code the hdfs command should be removed from the ozone artifact.

ozone-dist-layout-stitching also could be simplified to copy only the required jar files (we don't need to copy the namenode/datanode server side jars, just the common artifacts",newbie,[],HDDS,Sub-task,Major,2018-07-04 13:54:13,6
13170080,Create acceptance test to test ./start-ozone.sh and ./stop-ozone.sh for ozone/hdds,"Usually use the 'ozone' shell command to test our ozone/hdds cluster.

We need to create different acceptance test compose files to test the ./start-all.sh and ./hadoop-daemon.sh functionality.",newbie,[],HDDS,Improvement,Major,2018-07-04 13:52:37,97
13170078,Genearate version-info.properties for hadoop and ozone,"org.apache.hadoop.util.VersionInfo provides an api to show the actual version information.

We need to generate hdds-version-info.properties and ozone-version-info.properties as part of the build process(most probably in hdds/common, ozone/common projects)  and print out the available versions in case of 'ozone version' command",newbie,[],HDDS,Sub-task,Major,2018-07-04 13:48:08,97
13170075,add existing docker-compose files to the ozone release artifact,"Currently we use docker-compose files to run ozone pseudo cluster locally. After a full build, they can be found under hadoop-dist/target/compose.

As they are very useful, I propose to make them part of the ozone release to make it easier to try out ozone locally. 

I propose to create a new folder (docker/) in the ozone.tar.gz which contains all the docker-compose subdirectories + some basic README how they could be used.

We should explain in the README that the docker-compose files are not for production just for local experiments.",newbie,[],HDDS,Sub-task,Minor,2018-07-04 13:43:23,6
13169940,hadoop-hdds unit tests should use randomized ports,"MiniOzoneCluster should use randomized ports by default, so individual tests don't have to do anything to avoid port conflicts at runtime. e.g. TestStorageContainerManagerHttpServer fails if port 9876 is in use.

{code}
[INFO] Running org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.084 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer
[ERROR] testHttpPolicy[0](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 0.401 s  <<< ERROR!
java.net.BindException: Port in use: 0.0.0.0:9876
{code}",alpha2 newbie test,['test'],HDDS,Bug,Major,2018-07-03 22:08:29,97
13168747,Add name for LeaseManager,"During the review of HDDS-195 we realised that one server could have multiple LeaseManagers (for example one for the watchers one for the container creation).

To make it easier to monitor it would be good to use some specific names for the release manager.

This jira is about adding a new field (name) to the release manager which should be defined by a constructor parameter and should be required.

It should be used in the name of the Threads and all the log message (Something like ""Starting CommandWatcher LeasManager...."")",newbie,['SCM'],HDDS,Improvement,Minor,2018-06-28 01:21:33,97
13168662,"Create AuditLogger mechanism to be used by OM, SCM and Datanode","This Jira tracks the work to create a custom AuditLogger which can be used by OM, SCM, Datanode for auditing read/write events.

The AuditLogger will be designed using log4j2 and leveraging the MarkerFilter approach to be able to turn on/off audit of read/write events by simply changing the log config.",audit log4j2,[],HDDS,New Feature,Major,2018-06-27 16:03:13,81
13167767,Improve shell error message for unrecognized option,"The error message with an unrecognized option is unfriendly. E.g.
{code}
$ ozone oz -badOption
Unrecognized option: -badOptionERROR: null
{code}",newbie,[],HDDS,Improvement,Blocker,2018-06-22 22:52:00,6
13166445,Shell error messages are often cryptic,"Error messages in the Ozone shell are often too cryptic. e.g.
{code}
$ ozone oz -putKey /vol1/bucket1/key1 -file foo.txt
Command Failed : Create key failed, error:INTERNAL_ERROR
{code}",newbie,[],HDDS,Improvement,Blocker,2018-06-15 18:43:42,19
13166316,Revisit skipped tests in HDDS and Ozone and enable/remove them,There are quite a few tests which have been disabled for HDDS and Ozone. This jira proposes to re-visit the jira's and re-enable or remove them of not needed.,alpha2,['SCM'],HDDS,Sub-task,Major,2018-06-15 10:30:00,18
13166045,Add unit test for OzoneHddsDatanodeService,We have to add unit-test for {{OzoneHddsDatanodeService}} class.,newbie pull-request-available test,['Ozone Datanode'],HDDS,Sub-task,Major,2018-06-14 09:05:57,104
13166044,Add unit test for HddsDatanodeService,We have to add unit-test for {{HddsDatanodeService}} class.,newbie test,['Ozone Datanode'],HDDS,Sub-task,Major,2018-06-14 09:05:48,68
13163922,Update Ozone site docs,Ozone site docs need a few updates to the command syntax.,documentation,['documentation'],HDDS,Improvement,Major,2018-06-04 17:22:25,113
13163590,Freon times out because of because of wrong ratis port number in datanode details,"TestFreon is timing out with the following error because the ratis host is assigned as ""e0500982-ee67-476a-b0a8-eaaede737ad7_org.apache.hadoop.hdds.protocol.DatanodeDetails$Port@63eef88a""


{code}
2018-06-02 11:15:06,338 INFO  storage.RaftLogWorker (RaftLogWorker.java:<init>(86)) - new e0500982-ee67-476a-b0a8-eaaede737ad7_org.apache.hadoop.hdds.protocol.DatanodeDetails$Port@63eef88a-RaftLogWorker for Storage Directory /Users/msingh/code/work/apache/ozone/oz_review/hadoop-ozone/integration-test/target/test/data/MiniOzoneClusterImpl-669dfcd6-1f16-4993-b870-62571efb1e46/datanode-0/data/ratis/e0500982-ee67-476a-b0a8-eaaede737ad7/group-7347726F7570

WARNING: [org.apache.ratis.shaded.io.grpc.internal.ManagedChannelImpl-11] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host 63eef88a, cause=java.net.UnknownHostExc
eption: 63eef88a: nodename nor servname provided, or not known
        at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
        at java.net.InetAddress.getAllByName(InetAddress.java:1192)
        at java.net.InetAddress.getAllByName(InetAddress.java:1126)
        at org.apache.ratis.shaded.io.grpc.internal.DnsNameResolver$JdkResolver.resolve(DnsNameResolver.java:361)
        at org.apache.ratis.shaded.io.grpc.internal.DnsNameResolver$1.run(DnsNameResolver.java:172)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
}
{code}",reviewed,[],HDDS,Sub-task,Blocker,2018-06-02 05:47:10,18
13163312,Output of createVolume can be improved,"The output of {{createVolume}} includes a huge number (1 Exabyte) when the quota is not specified. This number can either be specified in a friendly format or omitted when the user did not use the \{{-quota}} option.
{code:java}
    2018-05-31 20:35:56 INFO  RpcClient:210 - Creating Volume: vol2, with hadoop as owner and quota set to 1152921504606846976 bytes.{code}",newbie usability,[],HDDS,Improvement,Major,2018-05-31 23:43:05,98
13163311,createVolume verbose warning with non-existent user,"When createVolume is invoked for a non-existent user, it logs a verbose warning for {{PartialGroupNameException}}.
{code:java}
    hadoop@9a70d9aa6bf9:~$ ozone oz volume create --user=nosuchuser vol4
    2018-05-31 20:40:17 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user nosuchuser
    PartialGroupNameException The user name 'nosuchuser' is not found. id: ‘nosuchuser’: no such user
    id: ‘nosuchuser’: no such user

      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)
      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)
      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)
      at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)
      at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:384)
      at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:319)
      at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:269)
      at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
      at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
      at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
      at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
      at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
      at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
      at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)
      at org.apache.hadoop.security.Groups.getGroups(Groups.java:227)
      at org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1545)
      at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1533)
      at org.apache.hadoop.ozone.client.rpc.RpcClient.createVolume(RpcClient.java:190)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      at java.lang.reflect.Method.invoke(Method.java:498)
      at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)
      at com.sun.proxy.$Proxy11.createVolume(Unknown Source)
      at org.apache.hadoop.ozone.client.ObjectStore.createVolume(ObjectStore.java:77)
      at org.apache.hadoop.ozone.web.ozShell.volume.CreateVolumeHandler.execute(CreateVolumeHandler.java:98)
      at org.apache.hadoop.ozone.web.ozShell.Shell.dispatch(Shell.java:395)
      at org.apache.hadoop.ozone.web.ozShell.Shell.run(Shell.java:135)
      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
      at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:114)
    2018-05-31 20:40:17 INFO  RpcClient:210 - Creating Volume: vol4, with nosuchuser as owner and quota set to 1152921504606846976 bytes.
{code}
However the volume is created:
{code}
$ ozone oz volume list --user=nosuchuser
[ {
  ""owner"" : {
    ""name"" : ""nosuchuser""
  },
  ""quota"" : {
    ""unit"" : ""TB"",
    ""size"" : 1048576
  },
  ""volumeName"" : ""vol4"",
  ""createdOn"" : ""Thu, 31 May 2018 20:40:17 GMT"",
  ""createdBy"" : ""nosuchuser""
} ]
{code}",newbie usability,[],HDDS,Bug,Blocker,2018-05-31 23:39:00,16
13163195,SCM CA: OM sends CSR and uses certificate issued by SCM,Initialize OM keypair and get SCM signed certificate.,pull-request-available,[],HDDS,Sub-task,Major,2018-05-31 16:25:45,110
13162446,TestGenerateOzoneRequiredConfigurations should use GenericTestUtils#getTempPath to set output directory,{{TestGenerateOzoneRequiredConfigurations}} uses current directory (.) as its output location which generates {{ozone-site.xml}} file in the directory from where the test-cases is executed. Insead we should use {{GenericTestUtils#getTempPath}} to get the output directory for test-cases.,newbie,['Tools'],HDDS,Bug,Minor,2018-05-28 19:39:03,97
13162361,Fix findbugs warning in MetadataKeyFilters.java,"{noformat}
module:hadoop-hdds/common 
       Found reliance on default encoding in org.apache.hadoop.utils.MetadataKeyFilters$KeyPrefixFilter.filterKey(byte[], byte[], byte[]):in org.apache.hadoop.utils.MetadataKeyFilters$KeyPrefixFilter.filterKey(byte[], byte[], byte[]): String.getBytes() At MetadataKeyFilters.java:[line 97] 
{noformat}",newbie reviewed,[],HDDS,Bug,Major,2018-05-28 10:15:12,122
13161905,Adding HDDS datanode Audit Log,This can be useful to find users who overload the DNs. ,alpha2 logging,[],HDDS,New Feature,Major,2018-05-24 21:55:14,81
13161631,"Wrapper for set/get Standalone, Ratis and Rest Ports in DatanodeDetails.","It will be very helpful to have a wrapper for set/get Standalone, Ratis and Rest Ports in DatanodeDetails.

Search and Replace usage of DatanodeDetails#newPort directly in current code. ",newbie,['Ozone Datanode'],HDDS,Improvement,Major,2018-05-23 23:47:00,98
13161589,Implement VolumeSet to manage disk volumes,"VolumeSet would be responsible for managing volumes in the Datanode. Some of its functions are:
 # Initialize volumes on startup
 # Provide APIs to add/ remove volumes
 # Choose and return volume to calling service based on the volume choosing policy (currently implemented Round Robin choosing policy)",ContainerIO,[],HDDS,Sub-task,Major,2018-05-23 21:09:05,43
13161275,Adding SCM Audit log,This ticket is opened to add SCM audit log.,alpha2,['SCM'],HDDS,New Feature,Major,2018-05-23 02:10:21,81
13161274,Adding Ozone Manager Audit Log,This ticket is opened to add ozone manager's audit log. ,Logging audit,[],HDDS,Sub-task,Major,2018-05-23 02:09:55,81
13161114,Change ozone datanode command to start the standalone datanode plugin,"The current ozone datanode command starts the regular hdfs datanode with an enabled HddsDatanodeService as a datanode plugin.

The goal is to start only the HddsDatanodeService.java (main function is already there but GenericOptionParser should be adopted). ",newbie,['Ozone Datanode'],HDDS,Bug,Major,2018-05-22 15:59:07,97
13159933,Merge ContainerData and ContainerStatus classes,"According to refactoring of containerIO, ContainerData has common fields for different kinds of containerTypes, and each Container will extend ConatinerData to add its fields. So, for this merging ContainerStatus fields to ConatinerData.",reviewed,[],HDDS,Sub-task,Major,2018-05-17 05:56:09,13
13159796,Remove SendContainerCommand from SCM,After the removal of {{ReportState}} from heartbeat we no longer need {{SendContainerCommand}} which is used by SCM to ask datanode to send container report.,reviewed,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2018-05-16 18:08:48,19
13159781,Remove ReportState from SCMHeartbeatRequestProto,"Since datanode will be sending container report in the configured interval there is no need to send {{ReportState}} in heartbeat. {{ReportState}} is only useful in pull model implementation of container report, this change can be reverted in future if we also want to support pull model.",reviewed,"['Ozone Datanode', 'SCM']",HDDS,Sub-task,Major,2018-05-16 16:53:25,19
13159689,Key replication factor and type should be stored per key by Ozone Manager,"Currently for a key, a client requests for multiple blocks through allocate block calls. However it is possible for the allocate block call to have a different replication type and factor than the blocks allocated during create key.

This jira proposes to store the replication factor and type values inside the OzoneManager and re-use the values for the subsequent block allocation calls.",reviewed,['Ozone Manager'],HDDS,Bug,Major,2018-05-16 11:44:47,18
13159514,Rename name of properties related to configuration tags,"We have two properties in {{ozone-default.xml}} for configuration tags.
 * {{hadoop.custom.tags}}
 * {{ozone.system.tags}}

For better readability, these properties can be renamed to
 * {{hadoop.tags.custom}}
 * {{ozone.tags.system}}",newbie,[],HDDS,Bug,Major,2018-05-15 20:06:49,97
13159167,Fix config names for secure ksm and scm,There are some inconsistencies in ksm and scm config for kerberos. Jira intends to correct them.,reviewed,[],HDDS,Sub-task,Major,2018-05-14 18:14:02,110
13078680,"Add Audit Logs to OzoneManager, SCM and Datanodes.",Add audit logs for ozone components.,alpha2,['Ozone Manager'],HDDS,Bug,Major,2017-06-09 16:58:17,123
13081231, Cleanup error messages,"Many error messages thrown from ozone are written for developers by developers. We need to review all publicly visible error messages to make sure it correct, includes enough context (stack traces do not count) and makes sense for the reader.",OzonePostMerge,['SCM'],HDDS,Sub-task,Major,2017-06-20 20:53:09,102
13152822,Fix Ozone related doc links in hadoop-project/src/site/site.xml,"Because Hdds profile https://issues.apache.org/jira/browse/HDDS-61#is off by default, the links in the generated site will be invalid without specifying maven profile -Phdds. 

 

 ",newbie,[],HDDS,Bug,Major,2018-04-16 20:49:54,28
13096955,Ozone: C/C++ implementation of ozone client using curl,"This Jira is introduced for implementation of ozone client in C/C++ using curl library.

All these calls will make use of HTTP protocol and would require libcurl. The libcurl API are referenced from here:
https://curl.haxx.se/libcurl/

Additional details would be posted along with the patches.",OzonePostMerge,['Native'],HDDS,Bug,Major,2017-08-23 08:35:12,16
13157736,Fix Ozone Unit Test Failures,This is an umbrellas JIRA to fix unit test failures related or unrelated HDDS-1.,alpha2,[],HDDS,Test,Major,2018-05-07 23:08:29,28
13157569,Ozone: Rename directory in ozonefs should be atomic,Currently rename in ozonefs is not atomic. While rename takes place another client might be adding a new file into the directory. Further if rename fails midway the directory will be in an inconsistent state.,alpha2,[],HDDS,Improvement,Major,2018-05-07 12:10:34,35
13119919,Remove Pipeline from Datanode Container Protocol protobuf definition.,"The current Ozone code passes pipeline information to datanodes as well. However datanodes do not use this information.

Hence Pipeline should be removed from ozone datanode commands.",reviewed,"['Native', 'Ozone Datanode']",HDDS,Improvement,Major,2017-11-21 12:17:45,18
13151737,Send NodeReport and ContainerReport when datanodes register,"From chillmode Deisgn Notes:

As part of this Jira, will update register to send NodeReport and ContaineReport.

Current Datanodes, send one heartbeat per 30 seconds. That means that even if the datanode is ready it will take around a 1 min or longer before the SCM sees the datanode container reports. We can address this partially by making sure that Register call contains both NodeReport and ContainerReport.

 

 ",reviewed,"['Ozone Datanode', 'SCM']",HDDS,Task,Major,2018-04-11 19:04:30,13
