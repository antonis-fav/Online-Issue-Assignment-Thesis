id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13326258,Don't adjust nodeCount when setting node id topology in in-jvm dtests,"We update the node count when setting the node id topology in in-jvm dtests, this should only happen if node count is smaller than the node id topology, otherwise bootstrap tests error out.",pull-request-available,['Test/dtest/java'],CASSANDRA,Improvement,Low,2020-09-07 07:49:30,0
13325867,Make sure we don't throw any uncaught exceptions during in-jvm dtests,We should assert that we don't throw any uncaught exceptions when running in-jvm dtests,pull-request-available,['Test/dtest/java'],CASSANDRA,Improvement,Normal,2020-09-03 07:55:11,0
13325605,Avoid marking shutting down nodes as up after receiving gossip shutdown message,"We have two recent failures for this test on trunk: 

1.) https://app.circleci.com/pipelines/github/maedhroz/cassandra/102/workflows/37ed8dab-9da4-4730-a883-20b7a99d88b4/jobs/518/tests (CASSANDRA-15909)
2.) https://app.circleci.com/pipelines/github/jolynch/cassandra/6/workflows/41e080e0-d7ff-4256-899e-b4010c6ef5ab/jobs/716/tests (CASSANDRA-15379)

The test expects there to be mismatches and then read repair executed on a following SELECT, but either those mismatches aren’t there, read repair isn’t happening, or both.",dtest incremental_repair repair,['Test/dtest/python'],CASSANDRA,Bug,Normal,2020-09-01 20:20:44,0
13309380,Add bytebuddy support for in-jvm dtests,"Old python dtests support byteman, but that is quite horrible to work with, [bytebuddy|https://bytebuddy.net/#/] is much better, so we should add support for that in the in-jvm dtests.",pull-request-available,['Test/dtest/java'],CASSANDRA,Improvement,Normal,2020-06-04 07:46:47,0
13279543,Run in-jvm upgrade dtests in circleci,We should run the in-jvm upgrade dtests in circleci,CI,"['CI', 'Test/dtest/java']",CASSANDRA,Improvement,Normal,2020-01-15 09:50:09,0
13265329,Use multiple data directories in the in-jvm dtests,We should default to using 3 data directories when running the in-jvm dtests.,pull-request-available,"['Local/Compaction', 'Test/dtest/java']",CASSANDRA,Improvement,Normal,2019-10-30 15:59:58,0
13194386,Make it possible to connect with user/pass + port in fqltool replay,We also need to close the executor service,fqltool security,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2018-10-26 11:24:57,0
13183061,Add dtests for fqltool replay/compare,We should add some basic round-trip dtests for {{fqltool replay}} and {{compare}},fqltool,[],CASSANDRA,Improvement,Normal,2018-09-05 07:34:28,0
13181958,Log the actual (if server-generated) timestamp and nowInSeconds used by queries in FQL,"FQL doesn't currently log the actual timestamp - in microseconds - if it's been server generated, nor the nowInSeconds value. It needs to, to allow for - in conjunction with CASSANDRA-14664 and CASSANDRA-14671 - deterministic playback tests.",fqltool,[],CASSANDRA,Improvement,Normal,2018-08-29 17:41:40,1
13181627,Use consistent nowInSeconds and timestamps values within a request,"We don't currently use consistent values of {{nowInSeconds}} and {{timestamp}} in the codebase, and sometimes generate several server-side timestamps for each in the same request. {{QueryState}} should cache the values it generated so that the same values are used for the duration of write/read.",fqltool,[],CASSANDRA,Improvement,Low,2018-08-28 14:56:08,1
13181056,Allow providing and overriding nowInSeconds via native protocol,"For FQL replay testing, to allow for deterministic and repeatable workload replay comparisons, we need to be able to set custom nowInSeconds via native protocol - primarily to control TTL expiration, both on read and write paths.",fqltool protocolv5,[],CASSANDRA,New Feature,Low,2018-08-24 20:57:29,1
13179969,Full query log needs to log the keyspace,"If the full query log is enabled and a set of clients have already executed ""USE <ks>"" we can't figure out which keyspace the following queries are executed against.

We need this for CASSANDRA-14618",fqltool,[],CASSANDRA,New Feature,Normal,2018-08-20 14:12:52,0
13177705,Clean up cache-related metrics,"{{ChunkCache}} added {{CacheMissMetrics}} which is an almost exact duplicate of pre-existing {{CacheMetrics}}. I believe it was done initially because the authors thought there was no way to register hits with {{Caffeine}}, only misses, but that's not quite true. All we need is to provide a {{StatsCounter}} object when building the cache and update our metrics from there.

The patch removes the redundant code and streamlines chunk cache metrics to use more idiomatic tracking.",virtual-tables,[],CASSANDRA,Improvement,Low,2018-08-08 15:39:38,1
13177410,Expose buffer cache metrics in caches virtual table,"As noted by [~blerer] in CASSANDRA-14538, we should expose buffer cache metrics in the caches virtual table.",virtual-tables,[],CASSANDRA,New Feature,Low,2018-08-07 13:43:52,1
13176473,Create fqltool compare command,"We need a {{fqltool compare}} command that can take the recorded runs from CASSANDRA-14618 and compares them, it should output any differences and potentially all queries against the mismatching partition up until the mismatch",fqltool,[],CASSANDRA,New Feature,Normal,2018-08-02 17:06:02,0
13176472,Create fqltool replay command,"Make it possible to replay the full query logs from CASSANDRA-13983 against one or several clusters. The goal is to be able to compare different runs of production traffic against different versions/configurations of Cassandra.

* It should be possible to take logs from several machines and replay them in ""order"" by the timestamps recorded
* Record the results from each run to be able to compare different runs (against different clusters/versions/etc)
* If {{fqltool replay}} is run against 2 or more clusters, the results should be compared as we go",fqltool,[],CASSANDRA,New Feature,Normal,2018-08-02 17:04:29,0
13175351,Flaky dtest: nodetool_test.TestNodetool.test_describecluster_more_information_three_datacenters,"@jay zhuang observed nodetool_test.TestNodetool.test_describecluster_more_information_three_datacenters being flaky in Apache Jenkins. I ran locally and got a different flaky behavior:

{noformat}
        out_node1_dc3, err, _ = node1_dc3.nodetool('describecluster')
        assert 0 == len(err), err
>       assert out_node1_dc1 == out_node1_dc3
E       AssertionError: assert 'Cluster Info...1=3, dc3=1}\n' == 'Cluster Infor...1=3, dc3=1}\n'
E           Cluster Information:
E           	Name: test
E           	Snitch: org.apache.cassandra.locator.PropertyFileSnitch
E           	DynamicEndPointSnitch: enabled
E           	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
E           	Schema versions:
E           		fc9ec7cd-80ba-3f27-87af-fc0bafcf7a03: [127.0.0.6, 127.0.0.5, 127.0.0.4, 127.0.0.3, 127.0.0.2, 127.0.0.1]...
E         
E         ...Full output truncated (26 lines hidden), use '-vv' to show


09:58:14,357 ccm DEBUG Log-watching thread exiting.
===Flaky Test Report===

test_describecluster_more_information_three_datacenters failed and was not selected for rerun.
	<class 'AssertionError'>
	assert 'Cluster Info...1=3, dc3=1}\n' == 'Cluster Infor...1=3, dc3=1}\n'
    Cluster Information:
    	Name: test
    	Snitch: org.apache.cassandra.locator.PropertyFileSnitch
    	DynamicEndPointSnitch: enabled
    	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
    	Schema versions:
    		fc9ec7cd-80ba-3f27-87af-fc0bafcf7a03: [127.0.0.6, 127.0.0.5, 127.0.0.4, 127.0.0.3, 127.0.0.2, 127.0.0.1]...
  
  ...Full output truncated (26 lines hidden), use '-vv' to show
	[<TracebackEntry /opt/orig/1/opt/dev/cassandra-dtest/nodetool_test.py:373>]

===End Flaky Test Report===
{noformat}

As this test is for a patch that was introduced for 4.0, this dtest (should) only be failing on trunk.",dtest,"['Test/dtest/python', 'Tool/nodetool']",CASSANDRA,Task,Low,2018-07-28 17:48:36,0
13174881,[dtest] test_sstableofflinerelevel - offline_tools_test.TestOfflineTools,"consistently failing dtest on 3.0 (no other branches). Output from pytest:

{noformat}
        output, _, rc = node1.run_sstableofflinerelevel(""keyspace1"", ""standard1"")
>       assert re.search(""L0=1"", output)
E       AssertionError: assert None
E        +  where None = <function search at 0x7f99afffbe18>('L0=1', 'New leveling: \nL0=0\nL1 10\n')
E        +    where <function search at 0x7f99afffbe18> = re.search

offline_tools_test.py:160: AssertionError
{noformat}
",dtest,['Test/dtest/python'],CASSANDRA,Bug,Low,2018-07-26 14:59:46,0
13174868,[dtest] test_functional - global_row_key_cache_test.TestGlobalRowKeyCache,"dtest fails all the time on 3.0, but not other branches. Error from pytest output:

{code}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [WARN  [main] 2018-07-23 18:53:10,075 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:53:56,966 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:55:54,508 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:56:42,688 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:53:10,075 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class)]
{code}",dtest,['Test/dtest/python'],CASSANDRA,Bug,Normal,2018-07-26 14:32:40,0
13173740,[DTEST] fix write_failures_test.py::TestWriteFailures::test_thrift,"seems it needs a {{WITH COMPACT STORAGE}} to avoid failing like this:
{code}
write_failures_test.py::TestWriteFailures::test_thrift swapoff: Not superuser.
01:23:57,245 ccm DEBUG Log-watching thread starting.

INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 178, in wrap_session
INTERNALERROR>     session.exitstatus = doit(config, session) or 0
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 215, in _main
INTERNALERROR>     config.hook.pytest_runtestloop(session=session)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 236, in pytest_runtestloop
INTERNALERROR>     item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 81, in pytest_runtest_protocol
INTERNALERROR>     self.runner.pytest_runtest_protocol(item, nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 64, in pytest_runtest_protocol
INTERNALERROR>     runtestprotocol(item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 79, in runtestprotocol
INTERNALERROR>     reports.append(call_and_report(item, ""call"", log))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 120, in call_and_report
INTERNALERROR>     report = hook.pytest_runtest_makereport(item=item, call=call)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 196, in _multicall
INTERNALERROR>     gen.send(outcome)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/skipping.py"", line 123, in pytest_runtest_makereport
INTERNALERROR>     rep = outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 331, in pytest_runtest_makereport
INTERNALERROR>     longrepr = item.repr_failure(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 675, in repr_failure
INTERNALERROR>     return self._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 668, in _repr_failure_py
INTERNALERROR>     return super(FunctionMixin, self)._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/nodes.py"", line 295, in _repr_failure_py
INTERNALERROR>     tbfilter=tbfilter,
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 476, in getrepr
INTERNALERROR>     return fmt.repr_excinfo(self)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 717, in repr_excinfo
INTERNALERROR>     reprtraceback = self.repr_traceback(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 664, in repr_traceback
INTERNALERROR>     reprentry = self.repr_traceback_entry(entry, einfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 624, in repr_traceback_entry
INTERNALERROR>     s = self.get_source(source, line_index, excinfo, short=short)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 568, in get_source
INTERNALERROR>     lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 575, in get_exconly
INTERNALERROR>     exlines = excinfo.exconly(tryshort=True).split(""\n"")
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 426, in exconly
INTERNALERROR>     lines = format_exception_only(self.type, self.value)
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 136, in format_exception_only
INTERNALERROR>     return list(TracebackException(etype, value, None).format_exception_only())
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 462, in __init__
INTERNALERROR>     _seen.add(exc_value)
INTERNALERROR> TypeError: unhashable type: 'InvalidRequestException'
{code}",dtest,[],CASSANDRA,Bug,Normal,2018-07-23 08:02:33,0
13161703,Add option to sanity check tombstones on reads/compaction,We should add an option to do a quick sanity check of tombstones on reads + compaction. It should either log the error or throw an exception.,pull-request-available,[],CASSANDRA,Improvement,Low,2018-05-24 08:56:33,0
13156112,Run ant eclipse-warnings in circleci,We should run ant eclipse-warnings in circle-ci,CI,['CI'],CASSANDRA,Improvement,Normal,2018-04-30 13:58:30,0
13152896,Fix setting min/max compaction threshold with LCS,To be able to actually set max/min_threshold in compaction options we need to remove it from the options map when validating.,lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2018-04-17 05:43:58,0
13138972,Flaky Unittest: org.apache.cassandra.db.compaction.BlacklistingCompactionsTest,"The unittest is flaky
{noformat}
    [junit] Testcase: testBlacklistingWithSizeTieredCompactionStrategy(org.apache.cassandra.db.compaction.BlacklistingCompactionsTest): FAILED
    [junit] expected:<8> but was:<25>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<25>
    [junit]     at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklisting(BlacklistingCompactionsTest.java:170)
    [junit]     at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklistingWithSizeTieredCompactionStrategy(BlacklistingCompactionsTest.java:71)
{noformat}",testing,['Legacy/Testing'],CASSANDRA,Bug,Low,2018-02-16 04:54:28,0
13133040,[DTEST] repair_tests/repair_test.py:TestRepair.simple_sequential_repair_test,Getting all rows from a node times out.,dtest,['Test/dtest/python'],CASSANDRA,Bug,Normal,2018-01-23 12:53:45,0
13129753,"[DTEST] [TRUNK] TestTopology.movement_test is flaky; fails assert ""values not within 16.00% of the max: (851.41, 713.26)""","DTest* TestTopology.test_movement* is flaky. All of the testing so far (and thus all of the current known observed failures) have been when running against trunk. When the test fails, it always due to the assert_almost_equal assert.

{code}
AssertionError: values not within 16.00% of the max: (851.41, 713.26) ()
{code}

The following CircleCI runs are 2 examples with dtests runs that failed due to this test failing it's assert:
[https://circleci.com/gh/mkjellman/cassandra/487]
[https://circleci.com/gh/mkjellman/cassandra/526]

*p.s.* assert_almost_equal has a comment ""@params error Optional margin of error. Default 0.16"". I don't see any obvious notes for why the default is this magical 16% number. It looks like it was committed as part of a big bulk commit by Sean McCarthy (who I can't find on JIRA). If anyone has any history on the magic 16% allowed delta please share!
",dtest,['Test/dtest/python'],CASSANDRA,Bug,Normal,2018-01-10 04:39:35,0
13108030,Infinite compaction of L0 SSTables in JBOD,"I recently upgraded from 2.2.6 to 3.11.0.

I am seeing Cassandra loop infinitely compacting the same data over and over. Attaching logs.

It is compacting two tables, one on /srv/disk10, the other on /srv/disk1. It does create new SSTables but immediately recompacts again. Note that I am not inserting anything at the moment, there is no flushing happening on this table (Memtable switch count has not changed).

My theory is that it somehow thinks those should be compaction candidates. But they shouldn't be, they are on different disks and I ran nodetool relocatesstables as well as nodetool compact. So, it tries to compact them together, but the compaction results in the exact same 2 SSTables on the 2 disks, because the keys are split by data disk.

This is pretty serious, because all our nodes right now are consuming CPU doing this for multiple tables, it seems.",jbod-aware-compaction,['Local/Compaction'],CASSANDRA,Bug,Normal,2017-10-09 17:23:02,0
13099785,dtest failure: batch_test.TestBatch.batchlog_replay_compatibility_?_test,"batch_test.TestBatch.batchlog_replay_compatibility_1_test and batch_test.TestBatch.batchlog_replay_compatibility_4_test are failing:
http://cassci.datastax.com/view/cassandra-3.11/job/cassandra-3.11_dtest/160/testReport/batch_test/TestBatch/batchlog_replay_compatibility_1_test/
http://cassci.datastax.com/view/cassandra-3.11/job/cassandra-3.11_dtest/160/testReport/batch_test/TestBatch/batchlog_replay_compatibility_4_test/",dtest,['Test/dtest/python'],CASSANDRA,Bug,Low,2017-09-05 09:32:13,1
13097635,CircleCI fix - only collect the xml file from containers where it exists,"Followup from CASSANDRA-13775 - my fix with {{ant eclipse-warnings}} obviously does not work since it doesn't generate any xml files

Push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers
Test running here: https://circleci.com/gh/krummas/cassandra/86",CI,['CI'],CASSANDRA,Bug,Normal,2017-08-25 15:21:48,0
13097299,Fix short read protection logic for querying more rows,"Discovered by [~benedict] while reviewing CASSANDRA-13747:

{quote}
While reviewing I got a little suspicious of the modified line {{DataResolver}} :479, as it seemed that n and x were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.

This is probably a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.

I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.

I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value >= 0 before setting to 1.
{quote}",Correctness,['Legacy/Coordination'],CASSANDRA,Bug,Normal,2017-08-24 12:36:26,1
13097281,"Regression in 3.0, breaking the fix from CASSANDRA-6069","The goal of the fix of CASSANDRA-6069 was to make sure that collection tombstones in an update in CAS were using {{t-1}} because at least in {{INSERT}} collection tombstones are used to delete data prior to the update but shouldn't delete the newly inserted data itself. Because in 2.x the collection tombstones are normal range tombstones and thus part of the {{DeletionInfo}}, we went with the easy solution of using {{t-1}} for all of {{DeletionInfo}}.

When moving that code to 3.0, this was migrated too literally however and only the {{DeletionInfo}} got the {{t -1}}. But in 3.0, range tombstones are not part of {{DeletionInfo}} anymore, and so this is broken.

Thanks to [~aweisberg] for noticing this.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2017-08-24 09:56:40,2
13092855,Fix AssertionError in short read protection,"{{ShortReadRowProtection.moreContents()}} expects that by the time we get to that method, the global post-reconciliation counter was already applied to the current partition. However, sometimes it won’t get applied, and the global counter continues counting with {{rowInCurrentPartition}} value not reset from previous partition, which in the most obvious case would trigger the assertion we are observing - {{assert !postReconciliationCounter.isDoneForPartition();}}. In other cases it’s possible because of this lack of reset to query a node for too few extra rows, causing unnecessary SRP data requests.

Why is the counter not always applied to the current partition?

The merged {{PartitionIterator}} returned from {{DataResolver.resolve()}} has two transformations applied to it, in the following order:
{{Filter}} - to purge non-live data from partitions, and to discard empty partitions altogether (except for Thrift)
{{Counter}}, to count and stop iteration

Problem is, {{Filter}} ’s {{applyToPartition()}} code that discards empty partitions ({{closeIfEmpty()}} method) would sometimes consume the iterator, triggering short read protection *before* {{Counter}} ’s {{applyToPartition()}} gets called and resets its {{rowInCurrentPartition}} sub-counter.

We should not be consuming iterators until all transformations are applied to them. For transformations it means that they cannot consume iterators unless they are the last transformation on the stack.

The linked branch fixes the problem by splitting {{Filter}} into two transformations. The original - {{Filter}} - that does filtering within partitions - and a separate {{EmptyPartitionsDiscarder}}, that discards empty partitions from {{PartitionIterators}}. Thus {{DataResolve.resolve()}}, when constructing its {{PartitionIterator}}, now does merge first, then applies {{Filter}}, then {{Counter}}, and only then, as its last (third) transformation - the {{EmptyPartitionsDiscarder}}. Being the last one applied, it’s legal for it to consume the iterator, and triggering {{moreContents()}} is now no longer a problem.

Fixes: [3.0|https://github.com/iamaleksey/cassandra/commits/13747-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13747-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13747-4.0]. dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13747].",Correctness,['Legacy/Coordination'],CASSANDRA,Bug,Normal,2017-08-07 11:15:20,1
13087066,Fix incorrect [2.1 <— 3.0] serialization of counter cells with pre-2.1 local shards,"We stopped generating local shards in C* 2.1, after CASSANDRA-6504 (Counters 2.0). But it’s still possible to have counter cell values
around, remaining from 2.0 times, on 2.1, 3.0, 3.11, and even trunk nodes, if they’ve never been overwritten.

In 2.1, we used two classes for two kinds of counter columns:
{{CounterCell}} class to store counters - internally as collections of {{CounterContext}} blobs, encoding collections of (host id, count, clock) tuples
{{CounterUpdateCell}} class to represent unapplied increments - essentially a single long value; this class was never written to commit log, memtables, or sstables, and was only used inside {{Mutation}} object graph - in memory, and marshalled over network in cases when counter write coordinator and counter write leader were different nodes
3.0 got rid of {{CounterCell}} and {{CounterUpdateCell}}, among other {{Cell}} classes. In order to represent these unapplied increments - equivalents of 2.1 {{CounterUpdateCell}} - in 3.0 we encode them as regular counter columns, with a ‘special’ {{CounterContext}} value. I.e. a counter context with a single local shard. We do that so that we can reuse local shard reconcile logic (summing up) to seamlessly support counters with same names collapsing to single increments in batches. See {{UpdateParameters.addCounter()}} method comments [here|https://github.com/apache/cassandra/blob/cassandra-3.0.14/src/java/org/apache/cassandra/cql3/UpdateParameters.java#L157-L171] for details. It also assumes that nothing else can generate a counter with local shards.

It works fine in pure 3.0 clusters, and in mixed 2.1/3.0 clusters, assuming that there are no counters with legacy local shards remaining from 2.0 era. It breaks down badly if there are.

{{LegacyLayout.serializeAsLegacyPartition()}} and consequently {{LegacyCell.isCounterUpdate()}} - classes responsible for serializing and deserialising in 2.1 format for compatibility - use the following logic to tell if a cell of {{COUNTER}} kind is a regular final counter or an unapplied increment:

{code}
private boolean isCounterUpdate()
{
    // See UpdateParameters.addCounter() for more details on this
    return isCounter() && CounterContext.instance().isLocal(value);
}
{code}

{{CounterContext.isLocal()}} method here looks at the first shard of the collection of tuples and returns true if it’s a local one.

This method would correctly identify a cell generated by {{UpdateParameters.addCounter()}} as a counter update and serialize it correctly as a 2.1 {{CounterUpdateCell}}. However, it would also incorrectly flag any regular counter cell that just so happens to have a local shard as the first tuple of the counter context as a counter update. If a 2.1 node as a coordinator of a read requests fetches such a value from a 3.0 node, during a rolling upgrade, instead of the expected {{CounterCell}} object it will receive a {{CounterUpdateCell}}, breaking all the things. In the best case scenario it will cause an assert in {{AbstractCell.reconcileCounter()}} to be raised.

To fix the problem we must find an unambiguous way, without false positives or false negatives, to represent and identify unapplied counter updates on 3.0 side. ",counters upgrade,['Legacy/Coordination'],CASSANDRA,Bug,Normal,2017-07-14 02:04:09,1
13079423,Implement short read protection on partition boundaries,"It seems that short read protection doesn't work when the short read is done at the end of a partition in a range query. The final assertion of this dtest fails:
{code}
def short_read_partitions_delete_test(self):
        cluster = self.cluster
        cluster.set_configuration_options(values={'hinted_handoff_enabled': False})
        cluster.set_batch_commitlog(enabled=True)
        cluster.populate(2).start(wait_other_notice=True)
        node1, node2 = self.cluster.nodelist()

        session = self.patient_cql_connection(node1)
        create_ks(session, 'ks', 2)
        session.execute(""CREATE TABLE t (k int, c int, PRIMARY KEY(k, c)) WITH read_repair_chance = 0.0"")

        # we write 1 and 2 in a partition: all nodes get it.
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (1, 1)"", consistency_level=ConsistencyLevel.ALL))
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (2, 1)"", consistency_level=ConsistencyLevel.ALL))

        # we delete partition 1: only node 1 gets it.
        node2.flush()
        node2.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 1""))
        node2.start(wait_other_notice=True)

        # we delete partition 2: only node 2 gets it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node2, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 2""))
        node1.start(wait_other_notice=True)

        # read from both nodes
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ALL)
        assert_none(session, ""SELECT * FROM t LIMIT 1"")
{code}
However, the dtest passes if we remove the {{LIMIT 1}}.

Short read protection [uses a {{SinglePartitionReadCommand}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/DataResolver.java#L484], maybe it should use a {{PartitionRangeReadCommand}} instead?",Correctness,['Legacy/Coordination'],CASSANDRA,Bug,Normal,2017-06-13 09:20:32,1
13078100,test failure in rebuild_test.TestRebuild.disallow_rebuild_from_nonreplica_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/524/testReport/rebuild_test/TestRebuild/disallow_rebuild_from_nonreplica_test

{noformat}
Error Message

ToolError not raised
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: Python driver version in use: 3.10
dtest: DEBUG: cluster ccm directory: /tmp/dtest-0tUjhX
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 DC1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 DC1> discovered
--------------------- >> end captured logging << ---------------------
{noformat}


{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrappedtestrebuild
    f(obj)
  File ""/home/automaton/cassandra-dtest/rebuild_test.py"", line 357, in disallow_rebuild_from_nonreplica_test
    node1.nodetool('rebuild -ks ks1 -ts (%s,%s] -s %s' % (node3_token, node1_token, node3_address))
  File ""/usr/lib/python2.7/unittest/case.py"", line 116, in __exit__
    ""{0} not raised"".format(exc_name))
{noformat}",dtest test-failure,[],CASSANDRA,Bug,Normal,2017-06-07 20:06:48,0
13077785,test failure in bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/445/testReport/bootstrap_test/TestBootstrap/consistent_range_movement_false_with_rf1_should_succeed_test

{noformat}
Error Message
31 May 2017 04:28:09 [node3] Missing: ['Starting listening for CQL clients']:
INFO  [main] 2017-05-31 04:18:01,615 YamlConfigura.....
See system.log for remainder
{noformat}

{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 236, in consistent_range_movement_false_with_rf1_should_succeed_test
    self._bootstrap_test_with_replica_down(False, rf=1)
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 278, in _bootstrap_test_with_replica_down
    jvm_args=[""-Dcassandra.consistent.rangemovement={}"".format(consistent_range_movement)])
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 696, in start
    self.wait_for_binary_interface(from_mark=self.mark)
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 514, in wait_for_binary_interface
    self.watch_log_for(""Starting listening for CQL clients"", **kwargs)
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 471, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""31 May 2017 04:28:09 [node3] Missing: ['Starting listening for CQL clients']:\nINFO  [main] 2017-05-31 04:18:01,615 YamlConfigura.....\n
{noformat}

{noformat}
-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-PKphwD\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.protocol: WARNING: Server warning: When increasing replication factor you need to run a full (-full) repair to distribute the data.\ncassandra.connection: WARNING: Heartbeat failed for connection (139927174110160) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 32.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 64.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 128.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------""
{noformat}",dtest test-failure,[],CASSANDRA,Bug,Normal,2017-06-06 20:18:09,0
13061719,Run more test targets on CircleCI,"Currently we only run {{ant test}} on circleci, we should use all the (free) containers we have and run more targets in parallel.",CI,['CI'],CASSANDRA,Bug,Normal,2017-04-05 12:33:18,0
13058179,"Cqlsh COPY fails importing Map<String,List<String>>, ParseError unhashable type list","When importing data with the _COPY_ command into a column family that has a _map<text, frozen<list<text>>>_ field, I get a _unhashable type: 'list'_ error. Here is how to reproduce:

{code}
CREATE TABLE table1 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<list<text>>>
);

insert into table1 (col1, col2map) values (1, {'key': ['value1']});

cqlsh:ks> copy table1 to 'table1.csv';


table1.csv file content:
1,{'key': ['value1']}


cqlsh:ks> copy table1 from 'table1.csv';
...
Failed to import 1 rows: ParseError - Failed to parse {'key': ['value1']} : unhashable type: 'list',  given up without retries
Failed to process 1 rows; failed rows written to kv_table1.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.420 seconds (0 skipped).
{code}

But it works fine for Map<String, Set<String>>.

{code}
CREATE TABLE table2 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<set<text>>>
);

insert into table2 (col1, col2map) values (1, {'key': {'value1'}});

cqlsh:ks> copy table2 to 'table2.csv';


table2.csv file content:
1,{'key': {'value1'}}


cqlsh:ks> copy table2 from 'table2.csv';
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.417 seconds (0 skipped).
{code}

The exception seems to arrive in _convert_map_ function in _ImportConversion_ class inside _copyutil.py_.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2017-03-22 10:54:31,3
13058159,Fix racy read command serialization,"Constantly see this error in the log without any additional information or a stack trace.

{code}
Exception in thread Thread[MessagingService-Incoming-/10.0.1.26,5,main]
{code}

{code}
java.lang.ArrayIndexOutOfBoundsException: null
{code}

Logger: org.apache.cassandra.service.CassandraDaemon
Thrdead: MessagingService-Incoming-/10.0.1.12
Method: uncaughtException
File: CassandraDaemon.java
Line: 229",pull-request-available,[],CASSANDRA,Bug,Normal,2017-03-22 09:22:14,1
13029162,dtest failures in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_trunk.cql3_insert_thrift_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/lastCompletedBuild/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_trunk/cql3_insert_thrift_test_2/

{code}
Unexpected error in node1 log, error: 
ERROR [main] 2016-12-17 14:48:28,471 CassandraDaemon.java:708 - Exception encountered during startup: Invalid yaml. Please remove properties [start_rpc] from your cassandra.yaml
{code}

Related failures:
http://cassci.datastax.com/job/trunk_dtest_upgrade/lastCompletedBuild/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_trunk/cql3_non_compound_range_tombstones_test_2/
http://cassci.datastax.com/job/trunk_dtest_upgrade/lastCompletedBuild/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_trunk/rename_test_2/

These failures are also happening in the different # node & RF variants, as well as in 3.0.X -> trunk upgrades.",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-12-19 15:07:11,4
13027048,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x.static_columns_with_distinct_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.X_dtest_upgrade/28/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/static_columns_with_distinct_test

{code}
Error Message

<Error from server: code=0000 [Server error] message=""java.io.IOError: java.io.IOException: Corrupt empty row found in unfiltered partition"">
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 46, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 4010, in static_columns_with_distinct_test
    rows = list(cursor.execute(""SELECT DISTINCT k, s1 FROM test2""))
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
{code}{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:7eac22dd41cb09e6d64fb5ac48b2cca3c8840cc8
Unexpected error in node2 log, error: 
ERROR [Native-Transport-Requests-2] 2016-12-08 03:20:04,861 Message.java:617 - Unexpected exception during request; channel = [id: 0xf4c13f2c, L:/127.0.0.2:9042 - R:/127.0.0.1:52112]
java.io.IOError: java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:224) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:212) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:779) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:741) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:408) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:273) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:232) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:76) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:219) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:204) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.9.jar:3.9]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_51]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
Caused by: java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:430) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:219) ~[apache-cassandra-3.9.jar:3.9]
	... 23 common frames omitted
{code}

Related failures: (~25)
http://cassci.datastax.com/job/cassandra-3.X_dtest_upgrade/28/testReport/",dtest test-failure,['Test/dtest/python'],CASSANDRA,Bug,Urgent,2016-12-09 17:04:45,2
13025644,Use timestamp from ClientState by default in AlterTableStatement,"example failure:

http://cassci.datastax.com/job/trunk_testall/1298/testReport/org.apache.cassandra.cql3.validation.operations/AlterTest/testDropListAndAddListWithSameName

{code}
Error Message

Invalid value for row 0 column 2 (mycollection of type list<text>), expected <null> but got <[first element]>
{code}{code}Stacktrace

junit.framework.AssertionFailedError: Invalid value for row 0 column 2 (mycollection of type list<text>), expected <null> but got <[first element]>
	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:908)
	at org.apache.cassandra.cql3.validation.operations.AlterTest.testDropListAndAddListWithSameName(AlterTest.java:87)
{code}",test-failure testall,[],CASSANDRA,Bug,Low,2016-12-05 14:20:50,2
13023763,dtest failure in snapshot_test.TestSnapshot.test_basic_snapshot_and_restore,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_jdk8/321/testReport/snapshot_test/TestSnapshot/test_basic_snapshot_and_restore

{code}
Error Message

sstableloader command '/home/automaton/cassandra/bin/sstableloader -d 127.0.0.1 /tmp/tmpdtJ9V7/0/ks/cf' failed; exit status: 1'; stdout: Established connection to initial hosts
Opening sstables and calculating sections to stream
; stderr: Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.format.big.BigTableReader.getPosition(BigTableReader.java:240)
	at org.apache.cassandra.io.sstable.format.SSTableReader.getPosition(SSTableReader.java:1572)
	at org.apache.cassandra.io.sstable.format.SSTableReader.getPositionsForRanges(SSTableReader.java:1503)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:128)
	at java.io.File.list(File.java:1161)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:79)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:161)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:99)
Caused by: java.lang.RuntimeException: java.net.UnknownHostException: openstack-cassci-external-df85c4d-jenkins-cassandra-2: openstack-cassci-external-df85c4d-jenkins-cassandra-2: unknown error
	at org.apache.cassandra.utils.FBUtilities.getLocalAddress(FBUtilities.java:138)
	at org.apache.cassandra.tracing.Tracing.<init>(Tracing.java:82)
	at org.apache.cassandra.tracing.Tracing.<clinit>(Tracing.java:88)
	... 8 more
Caused by: java.net.UnknownHostException: openstack-cassci-external-df85c4d-jenkins-cassandra-2: openstack-cassci-external-df85c4d-jenkins-cassandra-2: unknown error
	at java.net.InetAddress.getLocalHost(InetAddress.java:1484)
	at org.apache.cassandra.utils.FBUtilities.getLocalAddress(FBUtilities.java:133)
	... 10 more
Caused by: java.net.UnknownHostException: openstack-cassci-external-df85c4d-jenkins-cassandra-2: unknown error
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:907)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1302)
	at java.net.InetAddress.getLocalHost(InetAddress.java:1479)
	... 11 more
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 100, in test_basic_snapshot_and_restore
    self.restore_snapshot(snapshot_dir, node1, 'ks', 'cf')
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 72, in restore_snapshot
    ("" "".join(args), exit_status, stdout, stderr))
{code}",dtest test-failure,[],CASSANDRA,Improvement,Normal,2016-11-28 15:32:39,5
13021601,dtest failure in batch_test.TestBatch.logged_batch_doesnt_throw_uae_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.X_dtest/37/testReport/batch_test/TestBatch/logged_batch_doesnt_throw_uae_test

{noformat}
Error Message

Error from server: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={'required_replicas': 3, 'alive_replicas': 2, 'consistency': 'ALL'}
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-Ysb5Cf
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
dtest: DEBUG: Creating schema...
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 193, in logged_batch_doesnt_throw_uae_test
    cl=ConsistencyLevel.ALL)
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 164, in assert_all
    res = session.execute(simple_query)
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
'Error from server: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={\'required_replicas\': 3, \'alive_replicas\': 2, \'consistency\': \'ALL\'}\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-Ysb5Cf\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.policies: INFO: Using datacenter \'datacenter1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ndtest: DEBUG: Creating schema...\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\n--------------------- >> end captured logging << ---------------------'
{noformat}",dtest test-failure,[],CASSANDRA,Improvement,Normal,2016-11-17 21:46:08,4
13021561,dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_3_0_x_To_indev_3_x.bootstrap_test,"example failure:

http://cassci.datastax.com/job/trunk_large_dtest/40/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_3_0_x_To_indev_3_x/bootstrap_test

[^upgrade_test.consoleout.txt]",dtest test-failure,[],CASSANDRA,Improvement,Normal,2016-11-17 19:12:23,5
13021172,Increase error margin in SplitterTest,"SplitterTest is a randomized test - it generates random tokens and splits the ranges in equal parts. Since it is random we sometimes get very big vnodes right where we want a split and that makes the split unbalanced

Bumping the error margin a bit will avoid these false positives.",lhf,[],CASSANDRA,Bug,Normal,2016-11-16 15:31:37,0
13019612,dtest failure in disk_balance_test.TestDiskBalance.disk_balance_stress_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1418/testReport/disk_balance_test/TestDiskBalance/disk_balance_stress_test

{noformat}
Error Message

'float' object has no attribute '2f'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-lxr8Vr
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/disk_balance_test.py"", line 31, in disk_balance_stress_test
    self.assert_balanced(node)
  File ""/home/automaton/cassandra-dtest/disk_balance_test.py"", line 120, in assert_balanced
    assert_almost_equal(*sums, error=0.1, error_message=node.name)
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 187, in assert_almost_equal
    assert vmin > vmax * (1.0 - error) or vmin == vmax, ""values not within {.2f}% of the max: {} ({})"".format(error * 100, args, error_message)
""'float' object has no attribute '2f'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-lxr8Vr\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
{noformat}",dtest test-failure,[],CASSANDRA,Bug,Normal,2016-11-09 19:01:36,0
13019606,testall failure in org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest-compression,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/602/testReport/org.apache.cassandra.db.compaction/NeverPurgeTest/minorNeverPurgeTombstonesTest_compression

{noformat}
Error Message

Memory was freed by Thread[NonPeriodicTasks:1,5,main]
Stacktrace

junit.framework.AssertionFailedError: Memory was freed by Thread[NonPeriodicTasks:1,5,main]
	at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:103)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:260)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:223)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBufferMmap(CompressedRandomAccessReader.java:168)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:226)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:303)
	at org.apache.cassandra.io.util.AbstractDataInput.readInt(AbstractDataInput.java:202)
	at org.apache.cassandra.io.util.AbstractDataInput.readLong(AbstractDataInput.java:264)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:131)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:169)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.verifyContainsTombstones(NeverPurgeTest.java:114)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest(NeverPurgeTest.java:85)
Standard Output

WARN  20:06:47 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:47 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You a
...[truncated 2456 chars]...
 this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
{noformat}",test-failure,[],CASSANDRA,Improvement,Normal,2016-11-09 18:38:33,0
13019604,testall failure in org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetadata,"This failed in both 'test' and 'test-compression' targets.

example failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/602/testReport/org.apache.cassandra.db/ColumnFamilyStoreTest/testSliceByNamesCommandOldMetadata
http://cassci.datastax.com/job/cassandra-2.2_testall/602/testReport/org.apache.cassandra.db/ColumnFamilyStoreTest/testSliceByNamesCommandOldMetadata_compression/


{noformat}
Stacktrace

junit.framework.AssertionFailedError
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:171)
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:166)
	at org.apache.cassandra.io.sstable.format.SSTableWriter.rename(SSTableWriter.java:266)
	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:791)
	at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetadata(ColumnFamilyStoreTest.java:1158)
{noformat}",test-failure,[],CASSANDRA,Improvement,Normal,2016-11-09 18:32:12,2
13017692,dtest failure in materialized_views_test.TestMaterializedViews.populate_mv_after_insert_wide_rows_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/316/testReport/materialized_views_test/TestMaterializedViews/populate_mv_after_insert_wide_rows_test

{code}
Error Message

Expected [[0, 0]] from SELECT * FROM t_by_v WHERE id = 0 AND v = 0, but got []
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 211, in populate_mv_after_insert_wide_rows_test
    assert_one(session, ""SELECT * FROM t_by_v WHERE id = {} AND v = {}"".format(i, j), [j, i])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 130, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
{code}",dtest test-failure,[],CASSANDRA,Improvement,Normal,2016-11-03 15:27:53,4
13017430,Fix short read protection when more than one row is missing,"We are seeing an issue with paging reads missing some small number of columns when we do paging/limit reads. We get this on a single DC cluster itself when both reads and writes are happening with QUORUM. Paging/limit reads see this issue. I have attached the ccm based script which reproduces the problem.

* Keyspace RF - 3
* Table (id int, course text, marks int, primary key(id, course))
* replicas for partition key 1 - r1, r2 and r3
* insert (1, '1', 1) ,  (1, '2', 2),  (1, '3', 3),  (1, '4', 4),  (1, '5', 5) - succeeded on all 3 replicas
* insert (1, '6', 6) succeeded on r1 and r3, failed on r2
* delete (1, '2'), (1, '3'), (1, '4'), (1, '5') succeeded on r1 and r2, failed on r3
* insert (1, '7', 7) succeeded on r1 and r2, failed on r3

Local data on 3 nodes looks like as below now

r1: (1, '1', 1), tombstone(2-5 records), (1, '6', 6), (1, '7', 7)
r2: (1, '1', 1), tombstone(2-5 records), (1, '7', 7)
r3: (1, '1', 1),  (1, '2', 2),  (1, '3', 3),  (1, '4', 4),  (1, '5', 5), (1, '6', 6)

If we do a paging read with page_size 2, and if it gets data from r2 and r3, then it will only get the data (1, '1', 1) and (1, '7', 7) skipping record 6. This problem would happen if the same query is not doing paging but limit set to 2 records.

Resolution code for reads works same for paging queries and normal queries. Co-ordinator shouldn't respond back to client with records/columns that it didn't have complete visibility on all required replicas (in this case 2 replicas). In above case, it is sending back record (1, '7', 7) back to client, but its visibility on r3 is limited up to (1, '2', 2) and it is relying on just r2 data to assume (1, '6', 6) doesn't exist, which is wrong. End of the resolution all it can conclusively say any thing about is (1, '1',  and the other one is that we  and and and and and and the and the and the and d and the other is and 1), which exists and (1, '2', 2), which is deleted.

Ideally we should have different resolution implementation for paging/limit queries.

We could reproduce this on 2.0.17, 2.1.16 and 3.0.9.

Seems like 3.0.9 we have ShortReadProtection transformation on list queries. I assume that is to protect against the cases like above. But, we can reproduce the issue in 3.0.9 as well.",Correctness,['Legacy/Coordination'],CASSANDRA,Bug,Urgent,2016-11-02 21:50:45,1
13016172,dtest failure in replication_test.SnitchConfigurationUpdateTest.test_cannot_restart_with_different_rack,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/280/testReport/replication_test/SnitchConfigurationUpdateTest/test_cannot_restart_with_different_rack

{code}
Error Message

Problem stopping node node1
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/replication_test.py"", line 630, in test_cannot_restart_with_different_rack
    node1.stop(wait_other_notice=True)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 727, in stop
    raise NodeError(""Problem stopping node %s"" % self.name)
{code}",dtest test-failure,[],CASSANDRA,Bug,Normal,2016-10-28 14:24:59,3
13015430,testall failure in org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest-compression,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/597/testReport/org.apache.cassandra.db.compaction/NeverPurgeTest/minorNeverPurgeTombstonesTest_compression/

{code}
Error Message

Memory was freed by Thread[NonPeriodicTasks:1,5,main]
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: Memory was freed by Thread[NonPeriodicTasks:1,5,main]
	at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:103)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:260)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:223)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBufferMmap(CompressedRandomAccessReader.java:168)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:226)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:303)
	at org.apache.cassandra.io.util.AbstractDataInput.readInt(AbstractDataInput.java:202)
	at org.apache.cassandra.io.util.AbstractDataInput.readLong(AbstractDataInput.java:264)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:131)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:169)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.verifyContainsTombstones(NeverPurgeTest.java:114)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest(NeverPurgeTest.java:85)
{code}

Related failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/598/testReport/org.apache.cassandra.db.compaction/NeverPurgeTest/minorNeverPurgeTombstonesTest/",test-failure testall,[],CASSANDRA,Bug,Normal,2016-10-26 14:16:15,0
13015321,Extend native protocol flags and add supported versions to the SUPPORTED response,"We already use 7 bits for the flags of the QUERY message, and since they are encoded with a fixed size byte, we may be forced to change the structure of the message soon, and I'd like to do this in version 5 but without wasting bytes on the wire. Therefore, I propose to convert fixed flag's bytes to unsigned vints, as defined in CASSANDRA-9499. The only exception would be the flags in the frame, which should stay as fixed size.

Up to 7 bits, vints are encoded the same as bytes are, so no immediate change would be required in the drivers, although they should plan to support vint flags if supporting version 5. Moving forward, when a new flag is required for the QUERY message, and eventually when other flags reach 8 bits in other messages too, the flag's bitmaps would be automatically encoded with a size that is big enough to accommodate all flags, but no bigger than required. We can currently support up to 8 bytes with unsigned vints.

The downside is that drivers need to implement unsigned vint encoding for version 5, but this is already required by CASSANDRA-11873, and will most likely be required by CASSANDRA-11622 as well.

I would also like to add the list of versions to the SUPPORTED message, in order to simplify the handshake for drivers that prefer to send an OPTION message, rather than rely on receiving an error for an unsupported version in the STARTUP message. Said error should also contain the full list of supported versions, not just the min and max, for clarity, and because the latest version is now a beta version.

Finally, we currently store versions as integer constants in {{Server.java}}, and we still have a fair bit of hard-coded numbers in the code, especially in tests. I plan to clean this up by introducing a {{ProtocolVersion}} enum.",protocolv5,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2016-10-26 06:31:33,3
13014742,testall failure in org.apache.cassandra.index.internal.CassandraIndexTest.indexOnFirstClusteringColumn,"example failure:

http://cassci.datastax.com/job/trunk_testall/1250/testReport/org.apache.cassandra.index.internal/CassandraIndexTest/indexOnFirstClusteringColumn/

{code}
Error Message

Error setting schema for test (query was: CREATE INDEX c_index ON cql_test_keyspace.table_20(c))
{code}{code}
Stacktrace

java.lang.RuntimeException: Error setting schema for test (query was: CREATE INDEX c_index ON cql_test_keyspace.table_20(c))
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:705)
	at org.apache.cassandra.cql3.CQLTester.createIndex(CQLTester.java:627)
	at org.apache.cassandra.index.internal.CassandraIndexTest.access$400(CassandraIndexTest.java:56)
	at org.apache.cassandra.index.internal.CassandraIndexTest$TestScript.run(CassandraIndexTest.java:626)
	at org.apache.cassandra.index.internal.CassandraIndexTest.indexOnFirstClusteringColumn(CassandraIndexTest.java:86)
Caused by: org.apache.cassandra.exceptions.InvalidRequestException: Index c_index already exists
	at org.apache.cassandra.cql3.statements.CreateIndexStatement.validate(CreateIndexStatement.java:133)
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:696)
{code}",test-failure,[],CASSANDRA,Bug,Normal,2016-10-24 14:31:46,2
13014242,testall failure in org.apache.cassandra.db.compaction.CompactionsCQLTest.testTriggerMinorCompactionDTCS-compression,"example failure:
http://cassci.datastax.com/job/trunk_testall/1243/testReport/org.apache.cassandra.db.compaction/CompactionsCQLTest/testTriggerMinorCompactionDTCS_compression/

{code}
Error Message

No minor compaction triggered in 5000ms
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: No minor compaction triggered in 5000ms
	at org.apache.cassandra.db.compaction.CompactionsCQLTest.waitForMinor(CompactionsCQLTest.java:247)
	at org.apache.cassandra.db.compaction.CompactionsCQLTest.testTriggerMinorCompactionDTCS(CompactionsCQLTest.java:72)
{code}

Related failure:
http://cassci.datastax.com/job/cassandra-3.X_testall/47/testReport/org.apache.cassandra.db.compaction/CompactionsCQLTest/testTriggerMinorCompactionDTCS/",test-failure,[],CASSANDRA,Bug,Normal,2016-10-21 13:42:52,0
13013513,testall failure in org.apache.cassandra.dht.tokenallocator.RandomReplicationAwareTokenAllocatorTest.testExistingCluster,"example failure:
http://cassci.datastax.com/job/trunk_testall/1239/testReport/org.apache.cassandra.dht.tokenallocator/RandomReplicationAwareTokenAllocatorTest/testExistingCluster/

{code}
Error Message

Expected max unit size below 1.2500, was 1.2564
{code}
{code}
Stacktrace

junit.framework.AssertionFailedError: Expected max unit size below 1.2500, was 1.2564
	at org.apache.cassandra.dht.tokenallocator.AbstractReplicationAwareTokenAllocatorTest.grow(AbstractReplicationAwareTokenAllocatorTest.java:657)
	at org.apache.cassandra.dht.tokenallocator.RandomReplicationAwareTokenAllocatorTest.grow(RandomReplicationAwareTokenAllocatorTest.java:26)
	at org.apache.cassandra.dht.tokenallocator.AbstractReplicationAwareTokenAllocatorTest.testExistingCluster(AbstractReplicationAwareTokenAllocatorTest.java:545)
	at org.apache.cassandra.dht.tokenallocator.AbstractReplicationAwareTokenAllocatorTest.testExistingCluster(AbstractReplicationAwareTokenAllocatorTest.java:518)
	at org.apache.cassandra.dht.tokenallocator.RandomReplicationAwareTokenAllocatorTest.testExistingCluster(RandomReplicationAwareTokenAllocatorTest.java:38)
{code}",test-failure,['Legacy/Testing'],CASSANDRA,Bug,Normal,2016-10-19 12:56:50,3
13013504,testall failure in org.apache.cassandra.db.ColumnFamilyStoreCQLHelperTest.testDynamicComposite,"example failure:
http://cassci.datastax.com/job/cassandra-3.0_testall/706/testReport/org.apache.cassandra.db/ColumnFamilyStoreCQLHelperTest/testDynamicComposite/

{code}
Stacktrace

junit.framework.AssertionFailedError: 
	at org.apache.cassandra.db.ColumnFamilyStoreCQLHelperTest.testDynamicComposite(ColumnFamilyStoreCQLHelperTest.java:636)
{code}",test-failure,['Legacy/Testing'],CASSANDRA,Bug,Normal,2016-10-19 12:47:37,3
13013500,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x.boolean_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/64/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x/boolean_test

{code}
Error Message

Problem starting node node1 due to [Errno 2] No such file or directory: '/tmp/dtest-QXmxBV/test/node1/cassandra.pid'
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 2206, in boolean_test
    for is_upgraded, cursor in self.do_upgrade(cursor):
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 153, in do_upgrade
    node1.start(wait_for_binary_proto=True, wait_other_notice=True)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 648, in start
    self._update_pid(process)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1780, in _update_pid
    raise NodeError('Problem starting node %s due to %s' % (self.name, e), process)
{code}",dtest test-failure,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-10-19 12:44:36,4
13012889,dtest failure in json_test.FromJsonSelectTests.select_using_secondary_index_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1396/testReport/json_test/FromJsonSelectTests/select_using_secondary_index_test

{code}
Error Message

Doctest failed! Captured output:
**********************************************************************
Line 25, in select_using_secondary_index_test
Failed example:
    cqlsh_print('''
    SELECT * from person_likes where name = fromJson('{""first"":""test"", ""middle"":""guy"", ""last"":""jones""}')
    ''')
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python2.7/doctest.py"", line 1315, in __run
        compileflags, 1) in test.globs
      File ""<doctest select_using_secondary_index_test[4]>"", line 3, in <module>
        ''')
      File ""/home/automaton/cassandra-dtest/json_test.py"", line 105, in cqlsh_print
        output = cqlsh(cmds, supress_err=supress_err)
      File ""/home/automaton/cassandra-dtest/json_test.py"", line 95, in cqlsh
        raise RuntimeError(""Unexpected cqlsh error: {}"".format(err))
    RuntimeError: Unexpected cqlsh error: <stdin>:4:'ResultSet' object has no attribute 'column_types'
{code}

There are more tests which have failed with this error on this build.",dtest,['Feature/2i Index'],CASSANDRA,Improvement,Normal,2016-10-17 15:12:10,5
13011692,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test_not_intersecting_all_ranges,"example failure:

http://cassci.datastax.com/job/cassandra-3.X_dtest/6/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test_not_intersecting_all_ranges

{code}
Error Message

Subprocess sstablemetadata on keyspace: keyspace1, column_family: None exited with non-zero status; exit status: 1; 
stdout: 
usage: Usage: sstablemetadata [--gc_grace_seconds n] <sstable filenames>
Dump contents of given SSTable to standard output in JSON format.
    --gc_grace_seconds <arg>   The gc_grace_seconds to use when
                               calculating droppable tombstones
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 369, in sstable_marking_test_not_intersecting_all_ranges
    for out in (node.run_sstablemetadata(keyspace='keyspace1').stdout for node in cluster.nodelist()):
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 369, in <genexpr>
    for out in (node.run_sstablemetadata(keyspace='keyspace1').stdout for node in cluster.nodelist()):
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1021, in run_sstablemetadata
    return handle_external_tool_process(p, ""sstablemetadata on keyspace: {}, column_family: {}"".format(keyspace, column_families))
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1983, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-10-12 14:57:25,5
13011689,Disable RPC_READY gossip flag when shutting down client servers,"example failure:

http://cassci.datastax.com/job/cassandra-3.X_dtest/4/testReport/pushed_notifications_test/TestPushedNotifications/restart_node_test

{code}
Error Message

[{'change_type': u'DOWN', 'address': ('127.0.0.2', 9042)}, {'change_type': u'UP', 'address': ('127.0.0.2', 9042)}, {'change_type': u'DOWN', 'address': ('127.0.0.2', 9042)}]
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/pushed_notifications_test.py"", line 181, in restart_node_test
    self.assertEquals(expected_notifications, len(notifications), notifications)
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
{code}",dtest,['Legacy/Distributed Metadata'],CASSANDRA,Bug,Normal,2016-10-12 14:52:38,3
13011682,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x.in_order_by_without_selecting_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/13/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x/in_order_by_without_selecting_test

{code}
Error Message

Expected [[3], [4], [5], [0], [1], [2]] from SELECT v FROM test WHERE k IN (1, 0), but got [[0], [1], [2], [3], [4], [5]]
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 46, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 4200, in in_order_by_without_selecting_test
    assert_all(cursor, ""SELECT v FROM test WHERE k IN (1, 0)"", [[3], [4], [5], [0], [1], [2]])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 169, in assert_all
    assert list_res == expected, ""Expected {} from {}, but got {}"".format(expected, query, list_res)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-10-12 14:35:49,5
13011680,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x.limit_multiget_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/13/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x/limit_multiget_test

{code}
Error Message

Expected [[48, 'http://foo.com', 42]] from SELECT * FROM clicks WHERE userid IN (48, 2) LIMIT 1, but got [[2, u'http://foo.com', 42]]
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 362, in limit_multiget_test
    assert_one(cursor, ""SELECT * FROM clicks WHERE userid IN (48, 2) LIMIT 1"", [48, 'http://foo.com', 42])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 130, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-10-12 14:32:12,5
13009153,cqlsh copy tests hang in case of no answer from the server or driver,"-If we bundle the driver to cqlsh using the 3.6.0 tag or cassandra_test head, some cqlsh copy tests hang, for example {{test_bulk_round_trip_blogposts}}. See CASSANDRA-12736 and CASSANDRA-11534 for some sample failures.-

If the driver fails to invoke a callback (either error or success), or if the server never answers to the driver, then the copy parent process will wait forever to receive an answer from child processes. We should put a cap to this. We should also use a very high timeout rather than None, so that the driver will notify us if there is no answer from the server.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-10-03 02:49:44,3
13008909,Update the bundled python driver on trunk,"Trunk was recently updated to be version 4.0. The bundled python driver that cqlsh uses has a bug where it fails to connect to a C* server with a major version number greater than 3. The driver was fixed upstream, and we should pull in the fix.

[~pauloricardomg] or [~stefania]?",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-09-30 15:27:51,3
13008288,dtest failure in upgrade_tests.paging_test.TestPagingWithModifiersNodes2RF1_Upgrade_current_3_x_To_indev_3_x.test_with_allow_filtering,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/58/testReport/upgrade_tests.paging_test/TestPagingWithModifiersNodes2RF1_Upgrade_current_3_x_To_indev_3_x/test_with_allow_filtering

This is happening on many trunk upgrade tests. See : http://cassci.datastax.com/job/trunk_dtest_upgrade/58/testReport/

{code}
Standard Output
http://git-wip-us.apache.org/repos/asf/cassandra.git git:d45f323eb972c6fec146e5cfa84fdc47eb8aa5eb
Unexpected error in node2 log, error: 
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-09-28 04:30:11,223 CassandraDaemon.java:217 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.RuntimeException: Unknown column cdc during deserialization
	at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:192) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:668) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:656) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:341) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:350) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:610) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:593) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:114) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:190) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.7.jar:3.7]
Unexpected error in node2 log, error: 
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-09-28 04:30:11,270 CassandraDaemon.java:217 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.RuntimeException: Unknown column cdc during deserialization
	at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:192) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:668) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:656) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:341) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:350) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:610) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:593) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:114) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:190) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.7.jar:3.7]
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-28 16:38:09,5
13008285,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test_not_intersecting_all_ranges,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/406/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test_not_intersecting_all_ranges

{code}
Error Message

Subprocess sstablemetadata on keyspace: keyspace1, column_family: None exited with non-zero status; exit status: 1; 
stdout: 
usage: Usage: sstablemetadata [--gc_grace_seconds n] <sstable filenames>
Dump contents of given SSTable to standard output in JSON format.
    --gc_grace_seconds <arg>   The gc_grace_seconds to use when
                               calculating droppable tombstones
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 366, in sstable_marking_test_not_intersecting_all_ranges
    for out in (node.run_sstablemetadata(keyspace='keyspace1').stdout for node in cluster.nodelist()):
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 366, in <genexpr>
    for out in (node.run_sstablemetadata(keyspace='keyspace1').stdout for node in cluster.nodelist()):
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1021, in run_sstablemetadata
    return handle_external_tool_process(p, ""sstablemetadata on keyspace: {}, column_family: {}"".format(keyspace, column_families))
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1983, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-28 16:31:54,4
13008281,dtest failure in materialized_views_test.TestMaterializedViews.clustering_column_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/818/testReport/materialized_views_test/TestMaterializedViews/clustering_column_test

{code}
Error Message

'TestMaterializedViews' object has no attribute '_settled_stages'
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 367, in clustering_column_test
    self._insert_data(session)
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 96, in _insert_data
    self._settle_nodes()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 85, in _settle_nodes
    while attempts > 0 and not self._settled_stages(node):
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-28 16:22:24,4
13008278,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x.in_order_by_without_selecting_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x/in_order_by_without_selecting_test

{code}
Error Message

Expected [[3], [4], [5], [0], [1], [2]] from SELECT v FROM test WHERE k IN (1, 0), but got [[0], [1], [2], [3], [4], [5]]
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 46, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 4200, in in_order_by_without_selecting_test
    assert_all(cursor, ""SELECT v FROM test WHERE k IN (1, 0)"", [[3], [4], [5], [0], [1], [2]])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 169, in assert_all
    assert list_res == expected, ""Expected {} from {}, but got {}"".format(expected, query, list_res)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-28 16:18:50,5
13008275,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x.limit_multiget_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x/limit_multiget_test

{code}
Error Message

Expected [[48, 'http://foo.com', 42]] from SELECT * FROM clicks WHERE userid IN (48, 2) LIMIT 1, but got [[2, u'http://foo.com', 42]]
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 359, in limit_multiget_test
    assert_one(cursor, ""SELECT * FROM clicks WHERE userid IN (48, 2) LIMIT 1"", [48, 'http://foo.com', 42])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 130, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-28 16:15:14,4
13007588,dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.bloomfilter_size_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/485/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 153, in bloomfilter_size_test
    self.assertLessEqual(bfSize, size_factor * max_bf_size)
  File ""/usr/lib/python2.7/unittest/case.py"", line 936, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""457864 not less than or equal to 400000.0
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-26 14:13:20,0
13007219,Repair history tables should have TTL and TWCS,Some tools schedule a lot of small subrange repairs which can lead to a lot of repairs constantly being run. These partitions can grow pretty big in theory. I dont think much reads from them which might help but its still kinda wasted disk space. I think a month TTL (longer than gc grace) and maybe a 1 day twcs window makes sense to me.,lhf,['Legacy/Core'],CASSANDRA,Improvement,Normal,2016-09-23 18:31:13,0
13006548,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x.conditional_update_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/9/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x/conditional_update_test

{code}
Error Message

<Error from server: code=2000 [Syntax error in CQL query] message=""line 1:35 no viable alternative at input 'IN'"">
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 3028, in conditional_update_test
    assert_one(cursor, ""DELETE FROM test WHERE k = 0 IF v1 IN (null)"", [True])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 128, in assert_one
    res = session.execute(simple_query)
  File ""cassandra/cluster.py"", line 1998, in cassandra.cluster.Session.execute (cassandra/cluster.c:34869)
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""cassandra/cluster.py"", line 3781, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:73073)
    raise self._final_exception
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-21 15:52:45,4
13005923,dtest failure in json_tools_test.TestJson.json_tools_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/508/testReport/json_tools_test/TestJson/json_tools_test/

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/json_tools_test.py"", line 23, in json_tools_test
    debug(""Version: "" + cluster.version())
""cannot concatenate 'str' and 'instance' objects
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-19 14:27:56,4
13005445,dtest failure in nose.failure.Failure.runTest,"example failure:

http://cassci.datastax.com/job/trunk_dtest-skipped-with-require/434/testReport/nose.failure/Failure/runTest

{code}
Stacktrace

Traceback (most recent call last):
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/usr/local/lib/python2.7/dist-packages/nose/loader.py"", line 418, in loadTestsFromName
    addr.filename, addr.module)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/paging_test.py"", line 18, in <module>
    from upgrade_base import UpgradeTester
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 33, in <module>
    class UpgradeTester(Tester):
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 47, in UpgradeTester
    if LooseVersion(CASSANDRA_VERSION_FROM_BUILD) < '2.2':
  File ""/usr/lib/python2.7/distutils/version.py"", line 265, in __init__
    self.parse(vstring)
  File ""/usr/lib/python2.7/distutils/version.py"", line 274, in parse
    self.component_re.split(vstring))
TypeError: expected string or buffer
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-16 13:49:19,4
13005008,cqlsh NoHostsAvailable/AuthenticationFailure when sourcing a file with COPY commands,"In {{cqlsh}}, with authentication enabled, when sourcing a file with {{COPY}} commands in it:
{noformat}
test.cql:2:Error for (None, None): Failed to connect to all replicas ['127.0.0.1'] for (None, None), errors: [""NoHostAvailable - ('Unable to connect to any servers', {'127.0.0.1': AuthenticationFailed('Remote end requires authentication.',)})""] (permanently given up after 0 rows and 5 attempts)
{noformat}

{{cqlsh}} creates a new {{Shell}} without passing all pertinent arguments. When {{copyutil}} creates new cluster connections, they are not initialized correctly.

This is only for the {{source}} command. As a workaround,  {{cqlsh -f <script>}}  works, since it does not create a new {{Shell}} instance.

Repro:

{code}
ccm create -v 3.7 -n 1 test
ccm updateconf ""authenticator: PasswordAuthenticator"" ""authorizer: CassandraAuthorizer""
ccm start

echo ""copy system.local to 'something';"" > test.cql

echo ""source 'test.cql'"" | ccm node1 cqlsh
{code}",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-09-14 21:29:30,3
13004628,dtest failure in repair_tests.repair_test.TestRepairDataSystemTable.repair_parent_table_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/478/testReport/repair_tests.repair_test/TestRepairDataSystemTable/repair_parent_table_test

{code}
stderr: WARN  02:48:18,362 No schema agreement from live replicas after 10 s. The schema may not be up to date on some nodes.
java.lang.RuntimeException: Encountered exception creating schema
	at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpacesNative(SettingsSchema.java:101)
	at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpaces(SettingsSchema.java:69)
	at org.apache.cassandra.stress.settings.StressSettings.maybeCreateKeyspaces(StressSettings.java:228)
	at org.apache.cassandra.stress.StressAction.run(StressAction.java:59)
	at org.apache.cassandra.stress.Stress.run(Stress.java:143)
	at org.apache.cassandra.stress.Stress.main(Stress.java:62)
Caused by: com.datastax.driver.core.exceptions.InvalidQueryException: Keyspace 'keyspace1' does not exist
	at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:50)
	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)
	at org.apache.cassandra.stress.util.JavaDriverClient.execute(JavaDriverClient.java:183)
	at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpacesNative(SettingsSchema.java:86)
	... 5 more
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 320, in run
    self.setUp()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 32, in wrapped_setUp
    orig_setUp(obj, *args, **kwargs)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 1082, in setUp
    self.node1.stress(stress_options=['write', 'n=5K', 'no-warmup', 'cl=ONE', '-schema', 'replication(factor=3)'])
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 1256, in stress
    return handle_external_tool_process(p, ['stress'] + stress_options)
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 1985, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}

There are no logs.",dtest,[],CASSANDRA,Improvement,Normal,2016-09-13 15:02:41,4
13004611,dtest failure in upgrade_tests.storage_engine_upgrade_test.TestBootstrapAfterUpgrade.upgrade_with_range_tombstone_eoc_0_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/31/testReport/upgrade_tests.storage_engine_upgrade_test/TestBootstrapAfterUpgrade/upgrade_with_range_tombstone_eoc_0_test

{code}
Error Message

Expected [] to have length 2, but instead is of length 0

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/storage_engine_upgrade_test.py"", line 421, in upgrade_with_range_tombstone_eoc_0_test
    assert_length_equal(ret, 2)
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 249, in assert_length_equal
    expected_length, len(object_with_length)))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
{code}

Related failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/31/testReport/upgrade_tests.storage_engine_upgrade_test/TestStorageEngineUpgrade/upgrade_with_range_tombstone_eoc_0_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-09-13 14:47:11,4
13004610,dtest failure in replace_address_test.TestReplaceAddress.insert_data_during_replace_same_address_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testReport/replace_address_test/TestReplaceAddress/insert_data_during_replace_same_address_test

{code}
Error Message

13 Sep 2016 05:42:44 [replacement] Missing: ['Writes will not be forwarded to this node during replacement']:
INFO  [main] 2016-09-13 05:41:07,618 YamlConfigura.....
See system.log for remainder

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 422, in insert_data_during_replace_same_address_test
    self._test_insert_data_during_replace(same_address=True)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 217, in _test_insert_data_during_replace
    self._do_replace(same_address=same_address, extra_jvm_args=[""-Dcassandra.write_survey=true""])
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 112, in _do_replace
    timeout=60)
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 450, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-13 14:42:31,4
13004607,dtest failure in replace_address_test.TestReplaceAddress.insert_data_during_replace_different_address_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testReport/replace_address_test/TestReplaceAddress/insert_data_during_replace_different_address_test

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7400', ['join']] exited with non-zero status; exit status: 1; 
stdout: nodetool: This node has already joined the ring.

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 430, in insert_data_during_replace_different_address_test
    self._test_insert_data_during_replace(same_address=False)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 228, in _test_insert_data_during_replace
    self.replacement_node.nodetool(""join"")
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 756, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 1985, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-13 14:36:41,4
13004603,dtest failure in thrift_tests.TestMutations.test_range_tombstone_eoc_0,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testReport/thrift_tests/TestMutations/test_range_tombstone_eoc_0

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/thrift_tests.py"", line 2588, in test_range_tombstone_eoc_0
    self.assertEquals(2, len(ret))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
'2 != 0
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-13 14:32:19,4
13002396,dtest failure in auth_test.TestAuth.auth_metrics_test,"This failure is happening on many different tests in [trunk_offheap_dtest #389|http://cassci.datastax.com/job/trunk_offheap_dtest/389/].

One example:

http://cassci.datastax.com/job/trunk_offheap_dtest/389/testReport/auth_test/TestAuth/auth_metrics_test/

From the logs:
{code}
ERROR [main] 2016-09-02 01:13:43,688 CassandraDaemon.java:752 - Local host name unknown: java.net.UnknownHostException: openstack-cassci-external-df85c4d-jenkins-trunk-offheap-dtest-3: openstack-cassci-external-df85c4d-jenkins-trunk-offheap-dtest-3: unknown error
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-02 15:56:17,5
13002387,dtest failure in cqlsh_tests.cqlsh_tests.TestCqlsh.test_pep8_compliance,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/687/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_pep8_compliance

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py"", line 67, in test_pep8_compliance
    p = subprocess.Popen(cmds, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
  File ""/usr/lib/python2.7/subprocess.py"", line 710, in __init__
    errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1335, in _execute_child
    raise child_exception
""[Errno 2] No such file or directory
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-09-02 15:31:06,5
13001795,Removing static column results in ReadFailure due to CorruptSSTableException,"We ran into an issue on production where reads began to fail for certain queries, depending on the range within the relation for those queries. Cassandra system log showed an unhandled {{CorruptSSTableException}} exception.

CQL read failure:
{code}
ReadFailure: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}

Cassandra exception:
{code}
WARN  [SharedPool-Worker-2] 2016-08-31 12:49:27,979 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-2,5,main]: {}
java.lang.RuntimeException: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /usr/local/apache-cassandra-3.0.8/data/data/issue309/apples_by_tree-006748a06fa311e6a7f8ef8b642e977b/mb-1-big-Data.db
  at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2453) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_72]
  at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.8.jar:3.0.8]
  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72]
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /usr/local/apache-cassandra-3.0.8/data/data/issue309/apples_by_tree-006748a06fa311e6a7f8ef8b642e977b/mb-1-big-Data.db
  at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:343) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.isReverseOrder(LazilyInitializedUnfilteredRowIterator.java:65) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.isReverseOrder(LazilyInitializedUnfilteredRowIterator.java:66) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:62) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:24) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:295) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:134) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:127) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:123) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:65) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:289) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1796) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2449) ~[apache-cassandra-3.0.8.jar:3.0.8]
  ... 5 common frames omitted
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /usr/local/apache-cassandra-3.0.8/data/data/issue309/apples_by_tree-006748a06fa311e6a7f8ef8b642e977b/mb-1-big-Data.db
  at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:130) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:46) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:69) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:338) ~[apache-cassandra-3.0.8.jar:3.0.8]
  ... 19 common frames omitted
Caused by: java.io.IOException: Corrupt (negative) value length encountered
  at org.apache.cassandra.db.marshal.AbstractType.readValue(AbstractType.java:399) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.BufferCell$Serializer.deserialize(BufferCell.java:302) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.UnfilteredSerializer.readSimpleColumn(UnfilteredSerializer.java:462) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeRowBody(UnfilteredSerializer.java:440) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeStaticRow(UnfilteredSerializer.java:381) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.readStaticRow(AbstractSSTableIterator.java:179) ~[apache-cassandra-3.0.8.jar:3.0.8]
  at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:103) ~[apache-cassandra-3.0.8.jar:3.0.8]
  ... 22 common frames omitted
{code}

After debugging, it appears that a previously dropped static column (weeks prior) was the instigator of the issue. As a workaround we added back the column, restarted all cassandra processes within the cluster, and the read error and corruption exception went away.

Attached is a script to reproduce with a simple schema.

Also noteworthy (and shown in the script) is that when in this state, compaction silently failed (exit 0) to remove the dropped static columns from the ""corrupted"" sstable.
",compaction corruption drop read static,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Urgent,2016-08-31 19:12:10,3
13001685,dtest failure in cql_tracing_test.TestCqlTracing.tracing_default_impl_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/50/testReport/cql_tracing_test/TestCqlTracing/tracing_default_impl_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 163, in tracing_default_impl_test
    errs[0][1])
'list index out of range
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-31 14:45:12,4
13001682,dtest failure in repair_tests.repair_test.TestRepair.nonexistent_table_repair_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/50/testReport/repair_tests.repair_test/TestRepair/nonexistent_table_repair_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 210, in nonexistent_table_repair_test
    self.assertFalse(t.isAlive(), 'Repair thread on inexistent table is still running')
  File ""/usr/lib/python2.7/unittest/case.py"", line 416, in assertFalse
    raise self.failureException(msg)
""Repair thread on inexistent table is still running
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-31 14:42:59,4
13001679,dtest failure in materialized_views_test.TestMaterializedViews.add_dc_after_mv_simple_replication_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_simple_replication_test/

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 389, in add_dc_after_mv_simple_replication_test
    self._add_dc_after_mv_test(1)
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 363, in _add_dc_after_mv_test
    node5.start(jvm_args=[""-Dcassandra.migration_task_wait_in_seconds={}"".format(MIGRATION_WAIT)])
  File ""/home/automaton/ccm/ccmlib/node.py"", line 636, in start
    self._update_pid(process)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 1770, in _update_pid
    raise NodeError('Problem starting node %s due to %s' % (self.name, e), process)
""Problem starting node node5 due to [Errno 2] No such file or directory: '/tmp/dtest-S4bmF0/test/node5/cassandra.pid'
{code}

Related failure:
http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_network_replication_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-08-31 14:37:15,4
13001382,cqlsh lost the ability to have a request wait indefinitely,"In commit c7f0032912798b5e53b64d8391e3e3d7e4121165, when client_timeout became request_timeout, the logic was changed so that you can no longer use a timeout of None, despite the docs saying that you can:

https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshUsingCqlshrc.html#cqlshUsingCqlshrc__request-timeout",cqlsh doc-impacting lhf,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-08-30 16:34:53,3
13001054,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_with_backoff,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/385/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_with_backoff

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 1123, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2565, in test_bulk_round_trip_with_backoff
    copy_from_options={'MAXINFLIGHTMESSAGES': 64, 'MAXPENDINGCHUNKS': 1})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2454, in _test_bulk_round_trip
    sum(1 for _ in open(tempfile2.name)))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""250000 != 249714
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-29 17:41:46,5
13000352,dtest failure in paging_test.TestPagingDatasetChanges.test_cell_TTL_expiry_during_paging,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/49/testReport/paging_test/TestPagingDatasetChanges/test_cell_TTL_expiry_during_paging

{code}
Error Message

Error from server: code=2200 [Invalid query] message=""unconfigured table paging_test""
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-V_YoOr
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------

{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 2660, in test_cell_TTL_expiry_during_paging
    session, 'paging_test', cl=CL.ALL, format_funcs={'id': int, 'mytext': random_txt}
  File ""/home/automaton/cassandra-dtest/datahelp.py"", line 130, in create_rows
    vals=', '.join('?' for k in dicts[0].keys()), postfix=postfix)
  File ""cassandra/cluster.py"", line 2162, in cassandra.cluster.Session.prepare (cassandra/cluster.c:37231)
    raise
  File ""cassandra/cluster.py"", line 2159, in cassandra.cluster.Session.prepare (cassandra/cluster.c:37087)
    query_id, bind_metadata, pk_indexes, result_metadata = future.result()
  File ""cassandra/cluster.py"", line 3665, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:70216)
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""unconfigured table paging_test""\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-V_YoOr\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\n--------------------- >> end captured logging << ---------------------'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-26 12:27:12,5
13000022,dtest failure in cdc_test.TestCDC.test_cdc_data_available_in_cdc_raw,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1350/testReport/cdc_test/TestCDC/test_cdc_data_available_in_cdc_raw/

{code}
Error Message

25 Aug 2016 04:01:25 [node2] Missing: ['Starting listening for CQL clients']:
INFO  [main] 2016-08-25 03:51:25,259 YamlConfigura.....
See system.log for remainder

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cdc_test.py"", line 515, in test_cdc_data_available_in_cdc_raw
    loading_node.start(wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 655, in start
    self.wait_for_binary_interface(from_mark=self.mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 493, in wait_for_binary_interface
    self.watch_log_for(""Starting listening for CQL clients"", **kwargs)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 450, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
{code}",dtest,[],CASSANDRA,Bug,Normal,2016-08-25 13:55:10,5
12999380,"For LCS, single SSTable up-level is handled inefficiently","I'm using the latest trunk (as of August 2016, which probably is going to be 3.10) to run some experiments on LeveledCompactionStrategy and noticed this inefficiency.

The test data is generated using cassandra-stress default parameters (keyspace1.standard1), so as you can imagine, it consists of a ton of newly inserted partitions that will never merge in compactions, which is probably the worst kind of workload for LCS (however, I'll detail later why this scenario should not be ignored as a corner case; for now, let's just assume we still want to handle this scenario efficiently).

After the compaction test is done, I scrubbed debug.log for patterns that match  the ""Compacted"" summary so that I can see how long each individual compaction took and how many bytes they processed. The search pattern is like the following:

{noformat}
grep 'Compacted.*standard1' debug.log
{noformat}

Interestingly, I noticed a lot of the finished compactions are marked as having *only one* SSTable involved. With the workload mentioned above, the ""single SSTable"" compactions actually consist of the majority of all compactions (as shown below), so its efficiency can affect the overall compaction throughput quite a bit.

{noformat}
automaton@0ce59d338-1:~/cassandra-trunk/logs$ grep 'Compacted.*standard1' debug.log-test1 | wc -l
243
automaton@0ce59d338-1:~/cassandra-trunk/logs$ grep 'Compacted.*standard1' debug.log-test1 | grep "") 1 sstable"" | wc -l
218
{noformat}

By looking at the code, it appears that there's a way to directly edit the level of a particular SSTable like the following:

{code}
sstable.descriptor.getMetadataSerializer().mutateLevel(sstable.descriptor, targetLevel);
sstable.reloadSSTableMetadata();
{code}

To be exact, I summed up the time spent for these single-SSTable compactions (the total data size is 60GB) and found that if each compaction only needs to spend 100ms for only the metadata change (instead of the 10+ second they're doing now), it can already achieve 22.75% saving on total compaction time.

Compared to what we have now (reading the whole single-SSTable from old level and writing out the same single-SSTable at the new level), the only difference I could think of by using this approach is that the new SSTable will have the same file name (sequence number) as the old one's, which could break some assumptions on some other part of the code. However, not having to go through the full read/write IO, and not having to bear the overhead of cleaning up the old file, creating the new file, creating more churns in heap and file buffer, it seems the benefits outweigh the inconvenience. So I'd argue this JIRA belongs to LHF and should be made available in 3.0.x as well.

As mentioned in the 2nd paragraph, I'm also going to address why this kind of all-new-partition workload should not be ignored as a corner case. Basically, for the main use case of LCS where you need to frequently merge partitions to optimize read and eliminate tombstones and expired data sooner, LCS can be perfectly happy and efficiently perform the partition merge and tombstone elimination for a long time. However, as soon as the node becomes a bit unhealthy for various reasons (could be a bad disk so it's missing a whole bunch of mutations and need repair, could be the user chooses to ingest way more data than it usually takes and exceeds its capability, or god-forbidden, some DBA chooses to run offline sstablelevelreset), you will have to handle this kind of ""all-new-partition with a lot of SSTables in L0"" scenario, and once all L0 SSTables finally gets up-leveled to L1, you will likely see a lot of such single-SSTable compactions, which is the situation this JIRA is intended to address.

Actually, when I think more about this, to make this kind of single SSTable up-level more efficient will not only help the all-new-partition scenario, but also help in general any time when there is a big backlog of L0 SSTables due to too many flushes or excessive repair streaming with vnode. In those situations, by default STCS_in_L0 will be triggered, and you will end up getting a bunch of much bigger L0 SSTables after STCS is done. When it's time to up-level those much bigger L0 SSTables most likely they will overlap among themselves and you will add them all into your compaction session (along with all overlapped L1 SSTables). For these much bigger L0 SSTables, they have gone through a few rounds of STCS compactions, so if there's partition merge that needs to be done because fragments of the same partition are dispersed in smaller L0 SSTables earlier, after those STCS rounds, what you end up having in those much bigger L0 SSTables (generated by STCS) will not have much more opportunity for partition merge to happen, so we're in a scenario very similar to L0 data ""consists of a ton of newly inserted partitions that will never merge in compactions"" mentioned earlier.",compaction lcs performance,['Local/Compaction'],CASSANDRA,Improvement,Normal,2016-08-23 22:03:34,0
12998917,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x.select_with_alias_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/30/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x/select_with_alias_test

{code}
Error Message

Regexp didn't match: ""Aliases aren't allowed in the where clause"" not found in 'Error from server: code=2200 [Invalid query] message=""Undefined column name user_id""'
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 3189, in select_with_alias_test
    assert_invalid(cursor, 'SELECT id AS user_id, name AS user_name FROM users WHERE user_id = 0', matching=error_msg)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 92, in assert_invalid
    assert_exception(session, query, matching=matching, expected=expected)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 65, in assert_exception
    _assert_exception(session.execute, query, matching=matching, expected=expected)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 54, in _assert_exception
    assert_regexp_matches(str(e), matching)
  File ""/usr/lib/python2.7/unittest/case.py"", line 1002, in assertRegexpMatches
    raise self.failureException(msg)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-22 14:59:50,5
12998879,dtest failure in cql_tests.SlowQueryTester.remote_query_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/680/testReport/cql_tests/SlowQueryTester/remote_query_test

{code}
Error Message

22 Aug 2016 04:05:15 [node1] Missing: ['Starting listening for CQL clients']:
ERROR [main] 2016-08-22 03:55:13,422 CassandraDaem.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-qeHpYA
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tests.py"", line 879, in remote_query_test
    node1.start(wait_for_binary_proto=True, join_ring=False)  # ensure other node executes queries
  File ""/home/automaton/ccm/ccmlib/node.py"", line 655, in start
    self.wait_for_binary_interface(from_mark=self.mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 493, in wait_for_binary_interface
    self.watch_log_for(""Starting listening for CQL clients"", **kwargs)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 450, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""22 Aug 2016 04:05:15 [node1] Missing: ['Starting listening for CQL clients']:\nERROR [main] 2016-08-22 03:55:13,422 CassandraDaem.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-qeHpYA\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-22 12:53:34,5
12998874,dtest failure in cql_tests.SlowQueryTester.disable_slow_query_log_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/680/testReport/cql_tests/SlowQueryTester/disable_slow_query_log_test

http://cassci.datastax.com/job/cassandra-2.2_dtest/680/testReport/cql_tests/SlowQueryTester/local_query_test/

{code}
Error Message

Error starting node1.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-rDW1JQ
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tests.py"", line 945, in disable_slow_query_log_test
    ""-Dcassandra.test.read_iteration_delay_ms=50""])
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 414, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-rDW1JQ\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
Standard Output

[node1 ERROR] org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [slow_query_log_timeout_in_ms] from your cassandra.yaml
	at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:146)
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:113)
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:85)
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:135)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:119)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:507)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:641)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-22 12:38:54,4
12998217,dtest failure in topology_test.TestTopology.crash_during_decommission_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/376/testReport/topology_test/TestTopology/crash_during_decommission_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 673, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout
{code}

{code}
Standard Output

Unexpected error in node1 log, error: 
ERROR [RMI TCP Connection(2)-127.0.0.1] 2016-08-18 02:15:31,444 StorageService.java:3719 - Error while decommissioning node 
org.apache.cassandra.streaming.StreamException: Stream failed
	at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:215) ~[main/:na]
	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:191) ~[main/:na]
	at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:448) ~[main/:na]
	at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:551) ~[main/:na]
	at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:249) ~[main/:na]
	at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:263) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-18 14:43:59,4
12998216,dtest failure in auth_test.TestAuth.conditional_create_drop_user_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/458/testReport/auth_test/TestAuth/conditional_create_drop_user_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/auth_test.py"", line 348, in conditional_create_drop_user_test
    self.prepare()
  File ""/home/automaton/cassandra-dtest/auth_test.py"", line 978, in prepare
    n = self.wait_for_any_log(self.cluster.nodelist(), 'Created default superuser', 25)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 760, in wait_for_any_log
    found = node.grep_log(pattern, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 347, in grep_log
    with open(os.path.join(self.get_path(), 'logs', filename)) as f:
""[Errno 2] No such file or directory: '/tmp/dtest-XmnSYI/test/node1/logs/system.log'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-18 14:40:52,4
12998100,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x.cql3_non_compound_range_tombstones_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/25/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x/cql3_non_compound_range_tombstones_test/

It looks like this is failing with a TimedOutException.",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-08-18 05:36:12,5
12997932,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x.select_with_alias_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/29/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x/select_with_alias_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 3184, in select_with_alias_test
    assert_invalid(cursor, 'SELECT id AS user_id, name AS user_name FROM users WHERE user_id = 0', matching=error_msg)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 92, in assert_invalid
    assert_exception(session, query, matching=matching, expected=expected)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 65, in assert_exception
    _assert_exception(session.execute, query, matching=matching, expected=expected)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 54, in _assert_exception
    assert_regexp_matches(str(e), matching)
  File ""/usr/lib/python2.7/unittest/case.py"", line 1002, in assertRegexpMatches
    raise self.failureException(msg)
'Regexp didn\'t match: ""Aliases aren\'t allowed in the where clause"" not found in \'Error from server: code=2200 [Invalid query] message=""Undefined column name user_id""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-17 15:17:36,4
12997931,dtest failure in cqlshlib.test.test_cqlsh_output.TestCqlshOutput.test_describe_keyspace_output,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_cqlsh_tests/29/testReport/cqlshlib.test.test_cqlsh_output/TestCqlshOutput/test_describe_keyspace_output

{code}
Error Message

errors={'127.0.0.1': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1

{code}

http://cassci.datastax.com/job/cassandra-3.0_cqlsh_tests/lastCompletedBuild/cython=no,label=ctool-lab/testReport/cqlshlib.test.test_cqlsh_output/TestCqlshOutput/test_describe_keyspace_output/",dtest,['Test/dtest/python'],CASSANDRA,Bug,Normal,2016-08-17 15:14:53,3
12997928,dtest failure in rebuild_test.TestRebuild.simple_rebuild_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/382/testReport/rebuild_test/TestRebuild/simple_rebuild_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/rebuild_test.py"", line 75, in simple_rebuild_test
    session.execute(""ALTER KEYSPACE system_auth WITH REPLICATION = {'class':'NetworkTopologyStrategy', 'dc1':1, 'dc2':1};"")
  File ""cassandra/cluster.py"", line 1972, in cassandra.cluster.Session.execute (cassandra/cluster.c:34423)
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile).result()
  File ""cassandra/cluster.py"", line 3665, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:70216)
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""Unknown keyspace system_auth""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-17 14:52:23,4
12997924,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_non_prepared_statements,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/447/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_non_prepared_statements

{code}
Error Message

100000 != 96848
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-BryYNs
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Running stress without any user profile
dtest: DEBUG: Generated 100000 records
dtest: DEBUG: Exporting to csv file: /tmp/tmpREOhBZ
dtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpREOhBZ' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000
dtest: DEBUG: COPY TO took 0:00:04.598829 to export 100000 records
dtest: DEBUG: Truncating keyspace1.standard1...
dtest: DEBUG: Importing from csv file: /tmp/tmpREOhBZ
dtest: DEBUG: COPY keyspace1.standard1 FROM '/tmp/tmpREOhBZ' WITH PREPAREDSTATEMENTS = False
dtest: DEBUG: COPY FROM took 0:00:10.348123 to import 100000 records
dtest: DEBUG: Exporting to csv file: /tmp/tmpeXLPtz
dtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpeXLPtz' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000
dtest: DEBUG: COPY TO took 0:00:11.681829 to export 100000 records
--------------------- >> end captured logging << ---------------------
{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2482, in test_bulk_round_trip_non_prepared_statements
    copy_from_options={'PREPAREDSTATEMENTS': False})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2461, in _test_bulk_round_trip
    sum(1 for _ in open(tempfile2.name)))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""100000 != 96848\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-BryYNs\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Running stress without any user profile\ndtest: DEBUG: Generated 100000 records\ndtest: DEBUG: Exporting to csv file: /tmp/tmpREOhBZ\ndtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpREOhBZ' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000\ndtest: DEBUG: COPY TO took 0:00:04.598829 to export 100000 records\ndtest: DEBUG: Truncating keyspace1.standard1...\ndtest: DEBUG: Importing from csv file: /tmp/tmpREOhBZ\ndtest: DEBUG: COPY keyspace1.standard1 FROM '/tmp/tmpREOhBZ' WITH PREPAREDSTATEMENTS = False\ndtest: DEBUG: COPY FROM took 0:00:10.348123 to import 100000 records\ndtest: DEBUG: Exporting to csv file: /tmp/tmpeXLPtz\ndtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpeXLPtz' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000\ndtest: DEBUG: COPY TO took 0:00:11.681829 to export 100000 records\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-08-17 14:43:24,3
12997888,SyntaxException when COPY FROM Counter Table with Null value,"I have a simple counter table 

{noformat}
CREATE TABLE test (
    a int PRIMARY KEY,
    b counter,
    c counter
) ;
{noformat}

I have updated b column value with 

{noformat}
UPDATE test SET b = b + 1 WHERE a = 1;
{noformat}

Now I have export the data with 

{noformat}
COPY test TO 'test.csv';
{noformat}

And Import it with 

{noformat}
COPY test FROM 'test.csv';
{noformat}

I get this Error

{noformat}
Failed to import 1 rows: SyntaxException - line 1:34 no viable alternative at input 'WHERE' (...=b+1,c=c+ [WHERE]...) -  will retry later, attempt 1 of 5
{noformat}",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-08-17 11:46:11,3
12997623,dtest failure in upgrade_tests.repair_test.TestUpgradeRepair.repair_after_upgrade_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/24/testReport/upgrade_tests.repair_test/TestUpgradeRepair/repair_after_upgrade_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-16 15:57:41,4
12997598,dtest failure in pending_range_test.TestPendingRangeMovements.pending_range_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/456/testReport/pending_range_test/TestPendingRangeMovements/pending_range_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/pending_range_test.py"", line 63, in pending_range_test
    self.assertRegexpMatches(out, '127\.0\.0\.1.*?Down.*?Moving')
  File ""/usr/lib/python2.7/unittest/case.py"", line 1002, in assertRegexpMatches
    raise self.failureException(msg)
""Regexp didn't match: '127\\\\.0\\\\.0\\\\.1.*?Down.*?Moving' not found in '\\nDatacenter: datacenter1\\n==========\\nAddress    Rack        Status State   Load            Owns                Token                                       \\n                                                                          5534023222112865484                         \\n127.0.0.2  rack1       Up     Normal  200.01 KiB      73.44%              -5534023222112865485                        \\n127.0.0.3  rack1       Up     Normal  162.37 KiB      80.00%              -1844674407370955162                        \\n127.0.0.1  rack1       Down   Normal  147.05 KiB      66.56%              -634023222112864484                         \\n127.0.0.4  rack1       Up     Normal  164.85 KiB      40.00%              1844674407370955161                         \\n127.0.0.5  rack1       Up     Normal  164.99 KiB      40.00%              5534023222112865484  
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-16 14:33:45,4
12997596,dtest failure in jmx_test.TestJMX.netstats_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/285/testReport/jmx_test/TestJMX/netstats_test

{code}
Error Message

""ConnectException: 'Connection refused'."" does not match ""Subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['netstats']] exited with non-zero status; exit status: 1; 
stdout: Starting NodeTool
; 
stderr: nodetool: Failed to connect to 'localhost:7100' - ConnectException: 'Connection refused: connect'.
""
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: d:\temp\2\dtest-dbbq3u
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------

{code}

{code}
Stacktrace

  File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\jmx_test.py"", line 35, in netstats_test
    node1.nodetool('netstats')
  File ""C:\tools\python2\lib\unittest\case.py"", line 127, in __exit__
    (expected_regexp.pattern, str(exc_value)))
'""ConnectException: \'Connection refused\'."" does not match ""Subprocess [\'nodetool\', \'-h\', \'localhost\', \'-p\', \'7100\', [\'netstats\']] exited with non-zero status; exit status: 1; \nstdout: Starting NodeTool\n; \nstderr: nodetool: Failed to connect to \'localhost:7100\' - ConnectException: \'Connection refused: connect\'.\n""\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\2\\dtest-dbbq3u\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\n--------------------- >> end captured logging << ---------------------'
Standard Error

Started: node1 with pid: 6288
Started: node3 with pid: 2280
Started: node2 with pid: 6980
{code}",dtest windows,[],CASSANDRA,Improvement,Normal,2016-08-16 14:27:31,5
12997307,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_3_x.bug_5732_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/lastCompletedBuild/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_3_x/bug_5732_test

{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:2143805eef1fc93d9cd53f4415158e469c473730
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-15 02:42:18,863 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@147fec06) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@448270585:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-15 02:42:18,864 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1e878776) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1535449794:/mnt/tmp/dtest-mrsh87/test/node1/data2/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-15 02:42:18,866 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3071c268) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@1779704647:/mnt/tmp/dtest-mrsh87/test/node1/data2/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-15 02:42:18,877 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@480aa544) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@1001055629:/mnt/tmp/dtest-mrsh87/test/node1/data2/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-15 02:42:18,877 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5760212) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1320355808:[[OffHeapBitSet]] was not released before the reference was garbage collected
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-15 14:55:19,5
12997291,dtest failure in TestMutations:setUpClass.TestMutations:setUpClass,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/290/testReport/junit/(root)/TestMutations_setUpClass/TestMutations_setUpClass/",dtest,[],CASSANDRA,Improvement,Normal,2016-08-15 13:52:45,4
12997284,dtest failure in batch_test.TestBatch.logged_batch_compatibility_3_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/792/testReport/batch_test/TestBatch/logged_batch_compatibility_3_test

http://cassci.datastax.com/job/cassandra-3.0_dtest/792/testReport/batch_test/TestBatch/logged_batch_compatibility_2_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest/792/testReport/batch_test/TestBatch/logged_batch_compatibility_5_test/

{code}
Error Message

('Unable to connect to any servers', {'127.0.0.1': DriverException('ProtocolError returned from server while using explicitly set client protocol_version 4',)})
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-Ob2TdU
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Testing with 1 node(s) at version 'git:cassandra-2.1', 2 node(s) at current version
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set cassandra dir for node1 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set cassandra dir for node2 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set cassandra dir for node3 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1
--------------------- >> end captured logging << ---------------------
{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 290, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 318, in logged_batch_compatibility_3_test
    self._logged_batch_compatibility_test(0, 2, 'git:cassandra-2.1', 1)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 340, in _logged_batch_compatibility_test
    session = self.prepare_mixed(coordinator_idx, current_nodes, previous_version, previous_nodes)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 417, in prepare_mixed
    self.prepare(previous_nodes + current_nodes, compression, previous_version)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 383, in prepare
    session = self.patient_cql_connection(node1)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 545, in patient_cql_connection
    bypassed_exception=NoHostAvailable
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 219, in retry_till_success
    return fun(*args, **kwargs)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 478, in cql_connection
    protocol_version, port=port, ssl_opts=ssl_opts)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 506, in _create_session
    session = cluster.connect()
  File ""cassandra/cluster.py"", line 1156, in cassandra.cluster.Cluster.connect (cassandra/cluster.c:17387)
    with self._lock:
  File ""cassandra/cluster.py"", line 1189, in cassandra.cluster.Cluster.connect (cassandra/cluster.c:17208)
    raise
  File ""cassandra/cluster.py"", line 1176, in cassandra.cluster.Cluster.connect (cassandra/cluster.c:16911)
    self.control_connection.connect()
  File ""cassandra/cluster.py"", line 2521, in cassandra.cluster.ControlConnection.connect (cassandra/cluster.c:45182)
    self._set_new_connection(self._reconnect_internal())
  File ""cassandra/cluster.py"", line 2558, in cassandra.cluster.ControlConnection._reconnect_internal (cassandra/cluster.c:46079)
    raise NoHostAvailable(""Unable to connect to any servers"", errors)
""('Unable to connect to any servers', {'127.0.0.1': DriverException('ProtocolError returned from server while using explicitly set client protocol_version 4',)})\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-Ob2TdU\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Testing with 1 node(s) at version 'git:cassandra-2.1', 2 node(s) at current version\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set cassandra dir for node1 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set cassandra dir for node2 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set cassandra dir for node3 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1\n--------------------- >> end captured logging << ---------------------""
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-2.1
http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-2.1
http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-2.1
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-15 13:21:00,4
12997278,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x.bug_5732_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/16/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x/bug_5732_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 216, in tearDown
    super(UpgradeTester, self).tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 666, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 2.1.15, and jdk to 8. (Prior values will be restored after test).\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-D8UF3i\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: [[Row(table_name=u'ks', index_name=u'test.testindex')], [Row(table_name=u'ks', index_name=u'test.testindex')]]\ndtest: DEBUG: upgrading node1 to git:91f7387e1f785b18321777311a5c3416af0663c2\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Querying upgraded node\ndtest: DEBUG: Querying old node\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-D8UF3i\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-D8UF3i] directory\n--------------------- >> end captured logging << ---------------------""
{code}

{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:91f7387e1f785b18321777311a5c3416af0663c2
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@73deb57f) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@2098812276:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7926de0f) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1009016655:[[OffHeapBitSet]] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3a5760f9) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@223486002:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@42cb4131) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1544265728:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5dda43d0) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1100327913:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@59cfa823) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1480923322:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,601 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@570e14a1) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@992487242:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,602 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1f021ebc) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1878148398:[Memory@[0..4), Memory@[0..e)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,604 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@48feef6d) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@848724815:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,605 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3dee8c5f) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@1078490617:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,614 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7f726f1a) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@2037913408:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,615 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@303df044) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@861514759:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5fbf0fc9) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1715786089:[[OffHeapBitSet]] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3924b235) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1197672578:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@600596e0) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@545967120:[[OffHeapBitSet]] was not released before the reference was garbage collected
{code}",dtest,['Local/Startup and Shutdown'],CASSANDRA,Bug,Normal,2016-08-15 12:50:26,3
12997262,dtest failure in compaction_test.TestCompaction_with_SizeTieredCompactionStrategy.bloomfilter_size_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/lastCompletedBuild/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/bloomfilter_size_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-15 10:00:31,4
12996966,dtest failure in compaction_test.TestCompaction_with_SizeTieredCompactionStrategy.bloomfilter_size_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1335/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/bloomfilter_size_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 155, in bloomfilter_size_test
    self.assertGreaterEqual(bfSize, size_factor * min_bf_size)
  File ""/usr/lib/python2.7/unittest/case.py"", line 948, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""125672 not greater than or equal to 300000
{code}

related failure:
http://cassci.datastax.com/job/trunk_dtest/1335/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-08-12 15:20:33,5
12996921,dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.bloomfilter_size_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/282/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test

{code}
Stacktrace

  File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\compaction_test.py"", line 155, in bloomfilter_size_test
    self.assertLessEqual(bfSize, size_factor * max_bf_size)
  File ""C:\tools\python2\lib\unittest\case.py"", line 936, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""C:\tools\python2\lib\unittest\case.py"", line 410, in fail
    raise self.failureException(msg)
""125456 not less than or equal to 50000.0\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\2\\dtest-kfylxp\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: sstable_count is: 1\ndtest: DEBUG: dir_count is: 3\ndtest: DEBUG: bloom filter size is: 125456\ndtest: DEBUG: size factor = 0.333333333333\n--------------------- >> end captured logging << ---------------------""

{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-12 11:15:51,5
12996629,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x.cas_simple_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/20/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/cas_simple_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 216, in tearDown
    super(UpgradeTester, self).tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 666, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 3.7, and jdk to 8. (Prior values will be restored after test).\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-jGuOLx\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 3.7, and jdk to 8. (Prior values will be restored after test).\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-jGuOLx\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-jGuOLx] directory\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-7LSAP2\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: upgrading node1 to git:15fd71f9a3b07bbac7a1182f2e6bffd32e79b955\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Querying upgraded node\ndtest: DEBUG: Querying old node\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-7LSAP2\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-7LSAP2] directory\n--------------------- >> end captured logging << ---------------------""
{code}

{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:15fd71f9a3b07bbac7a1182f2e6bffd32e79b955
Unexpected error in node1 log, error: 
ERROR [CompactionExecutor:1] 2016-08-11 02:36:24,280 CassandraDaemon.java:217 - Exception in thread Thread[CompactionExecutor:1,1,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) ~[na:1.8.0_51]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_51]
	at org.apache.cassandra.db.compaction.CompactionManager.submitBackground(CompactionManager.java:184) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:270) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-11 14:17:50,5
12996625,dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_2_x_To_indev_3_0_x.rolling_upgrade_with_internode_ssl_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_large_dtest/20/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_2_x_To_indev_3_0_x/rolling_upgrade_with_internode_ssl_test

{code}
Error Message

Ran out of time waiting for queue size (1) to be 'le' to 0. Aborting.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 2.2.7, and jdk to 8. (Prior values will be restored after test).
dtest: DEBUG: cluster ccm directory: /tmp/dtest-U_C4o2
ccm: DEBUG: Log-watching thread starting.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Setting extra configuration options:
{   'partitioner': 'org.apache.cassandra.dht.Murmur3Partitioner'}
dtest: DEBUG: Versions to test (<class 'upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_2_x_To_indev_3_0_x'>): ['2.2.7', 'git:465def8e295b453dc22430dc4bb7039f6921151e']
dtest: DEBUG: ***using internode ssl***
dtest: DEBUG: generating keystore.jks in [/tmp/dtest-U_C4o2]
dtest: DEBUG: exporting cert from keystore.jks in [/tmp/dtest-U_C4o2]
dtest: DEBUG: importing cert into truststore.jks in [/tmp/dtest-U_C4o2]
dtest: DEBUG: Creating cluster (2.2.7)
dtest: DEBUG: Current upgrade path: ['***2.2.7***', 'git:465def8e295b453dc22430dc4bb7039f6921151e']
dtest: DEBUG: rows written (but not verified) queue size is at 417, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 442, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 474, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 503, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 536, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 565, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 597, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 634, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 663, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size is at 690, target is to reach 'ge' 5000
dtest: DEBUG: rows written (but not verified) queue size (5044) is 'ge' to 5000. Continuing.
dtest: DEBUG: counters incremented (but not verified) queue size (5042) is 'ge' to 5000. Continuing.
dtest: DEBUG: Upgrading ['node1'] to git:465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: JAVA_HOME: /usr/lib/jvm/jdk1.8.0_51
dtest: DEBUG: Shutting down node: node1
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set new cassandra dir for node1: /home/automaton/.ccm/repository/gitCOLON465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: Starting node1 on new version (git:465def8e295b453dc22430dc4bb7039f6921151e)
dtest: DEBUG: Successfully upgraded 1 of 3 nodes to git:465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: Upgrading ['node2'] to git:465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: JAVA_HOME: /usr/lib/jvm/jdk1.8.0_51
dtest: DEBUG: Shutting down node: node2
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set new cassandra dir for node2: /home/automaton/.ccm/repository/gitCOLON465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: Starting node2 on new version (git:465def8e295b453dc22430dc4bb7039f6921151e)
dtest: DEBUG: Successfully upgraded 2 of 3 nodes to git:465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: Upgrading ['node3'] to git:465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: JAVA_HOME: /usr/lib/jvm/jdk1.8.0_51
dtest: DEBUG: Shutting down node: node3
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set new cassandra dir for node3: /home/automaton/.ccm/repository/gitCOLON465def8e295b453dc22430dc4bb7039f6921151e
dtest: DEBUG: Starting node3 on new version (git:465def8e295b453dc22430dc4bb7039f6921151e)
dtest: DEBUG: Successfully upgraded 3 of 3 nodes to git:465def8e295b453dc22430dc4bb7039f6921151e
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: writes pending verification queue size is at 46041, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 46013, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45983, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45952, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45922, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45893, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45862, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45829, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45801, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 45771, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36705, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36675, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36649, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36620, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36592, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36564, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36536, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36507, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36474, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 36442, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27707, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27679, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27650, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27623, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27591, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27559, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27532, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27504, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27474, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 27441, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 18630, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 18601, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 18571, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 18543, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 18512, target is to reach 'le' 0
dtest: DEBUG: writes pending verification queue size is at 18484, target is to reach 'le' 0

{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 298, in rolling_upgrade_with_internode_ssl_test
    self.upgrade_scenario(rolling=True, internode_ssl=True)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 366, in upgrade_scenario
    self._wait_until_queue_condition('counters pending verification', incr_verify_queue, operator.le, 0, max_wait_s=1200)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 551, in _wait_until_queue_condition
    raise RuntimeError(""Ran out of time waiting for queue size ({}) to be '{}' to {}. Aborting."".format(qsize, opfunc.__name__, required_len))
""Ran out of time waiting for queue size (1) to be 'le' to 0. Aborting.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 2.2.7, and jdk to 8. (Prior values will be restored after test).\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-U_C4o2\nccm: DEBUG: Log-watching thread starting.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Setting extra configuration options:\n{   'partitioner': 'org.apache.cassandra.dht.Murmur3Partitioner'}\ndtest: DEBUG: Versions to test (<class 'upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_2_x_To_indev_3_0_x'>): ['2.2.7', 'git:465def8e295b453dc22430dc4bb7039f6921151e']\ndtest: DEBUG: ***using internode ssl***\ndtest: DEBUG: generating keystore.jks in [/tmp/dtest-U_C4o2]\ndtest: DEBUG: exporting cert from keystore.jks in [/tmp/dtest-U_C4o2]\ndtest: DEBUG: importing cert into truststore.jks in [/tmp/dtest-U_C4o2]\ndtest: DEBUG: Creating cluster (2.2.7)\ndtest: DEBUG: Current upgrade path: ['***2.2.7***', 'git:465def8e295b453dc22430dc4bb7039f6921151e']\ndtest: DEBUG: rows written (but not verified) queue size is at 417, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 442, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 474, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 503, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 536, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 565, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 597, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 634, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 663, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size is at 690, target is to reach 'ge' 5000\ndtest: DEBUG: rows written (but not verified) queue size (5044) is 'ge' to 5000. Continuing.\ndtest: DEBUG: counters incremented (but not verified) queue size (5042) is 'ge' to 5000. Continuing.\ndtest: DEBUG: Upgrading ['node1'] to git:465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: JAVA_HOME: /usr/lib/jvm/jdk1.8.0_51\ndtest: DEBUG: Shutting down node: node1\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set new cassandra dir for node1: /home/automaton/.ccm/repository/gitCOLON465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: Starting node1 on new version (git:465def8e295b453dc22430dc4bb7039f6921151e)\ndtest: DEBUG: Successfully upgraded 1 of 3 nodes to git:465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: Upgrading ['node2'] to git:465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: JAVA_HOME: /usr/lib/jvm/jdk1.8.0_51\ndtest: DEBUG: Shutting down node: node2\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set new cassandra dir for node2: /home/automaton/.ccm/repository/gitCOLON465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: Starting node2 on new version (git:465def8e295b453dc22430dc4bb7039f6921151e)\ndtest: DEBUG: Successfully upgraded 2 of 3 nodes to git:465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: Upgrading ['node3'] to git:465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: JAVA_HOME: /usr/lib/jvm/jdk1.8.0_51\ndtest: DEBUG: Shutting down node: node3\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set new cassandra dir for node3: /home/automaton/.ccm/repository/gitCOLON465def8e295b453dc22430dc4bb7039f6921151e\ndtest: DEBUG: Starting node3 on new version (git:465def8e295b453dc22430dc4bb7039f6921151e)\ndtest: DEBUG: Successfully upgraded 3 of 3 nodes to git:465def8e295b453dc22430dc4bb7039f6921151e\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: writes pending verification queue size is at 46041, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 46013, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45983, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45952, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45922, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45893, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45862, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45829, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45801, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 45771, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36705, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36675, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36649, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36620, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36592, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36564, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36536, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36507, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36474, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 36442, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27707, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27679, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27650, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27623, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27591, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27559, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27532, target is to reach 'le' 0\ndtest: DEBUG: writes pending verification queue size is at 27504, target is to reach 'le' 0\ndtest: DEBUG
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-11 13:56:12,5
12996617,dtest failure in consistency_test.TestConsistency.readrepair_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/789/testReport/consistency_test/TestConsistency/readrepair_test

{code}
Error Message

Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 1 responses."" info={'received_responses': 1, 'required_responses': 2, 'consistency': 'QUORUM'}
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-ola0t8
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
--------------------- >> end captured logging << ---------------------
{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 904, in readrepair_test
    query_c1c2(session, n, ConsistencyLevel.QUORUM)
  File ""/home/automaton/cassandra-dtest/tools.py"", line 83, in query_c1c2
    rows = list(session.execute(query))
  File ""cassandra/cluster.py"", line 1941, in cassandra.cluster.Session.execute (cassandra/cluster.c:33653)
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile).result()
  File ""cassandra/cluster.py"", line 3629, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:69380)
    raise self._final_exception
'Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\' responses] message=""Operation timed out - received only 1 responses."" info={\'received_responses\': 1, \'required_responses\': 2, \'consistency\': \'QUORUM\'}\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-ola0t8\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\n--------------------- >> end captured logging << ---------------------'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-11 13:36:00,4
12996610,dtest failure in repair_tests.repair_test.TestRepair.test_multiple_concurrent_repairs,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/318/testReport/repair_tests.repair_test/TestRepair/test_multiple_concurrent_repairs

{code}
Error Message

ERROR 15:43:07 Error creating pool to /127.0.0.3:9042
com.datastax.driver.core.TransportException: [/127.0.0.3:9042] Cannot connect
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
Caused by: java.net.ConnectException: Connection refused: /127.0.0.3:9042
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]
	at com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	... 6 common frames omitted
ERROR 15:43:07 Error creating pool to /127.0.0.1:9042
com.datastax.driver.core.TransportException: [/127.0.0.1:9042] Cannot connect
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
Caused by: java.net.ConnectException: Connection refused: /127.0.0.1:9042
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]
	at com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	... 6 common frames omitted
Failed to connect over JMX; not collecting these stats

-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-b0pWB9
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------

{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 886, in test_multiple_concurrent_repairs
    self.assertTrue(len(stderr) == 0, stderr)
  File ""/usr/lib/python2.7/unittest/case.py"", line 422, in assertTrue
    raise self.failureException(msg)
""ERROR 15:43:07 Error creating pool to /127.0.0.3:9042\ncom.datastax.driver.core.TransportException: [/127.0.0.3:9042] Cannot connect\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]\nCaused by: java.net.ConnectException: Connection refused: /127.0.0.3:9042\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]\n\tat com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\t... 6 common frames omitted\nERROR 15:43:07 Error creating pool to /127.0.0.1:9042\ncom.datastax.driver.core.TransportException: [/127.0.0.1:9042] Cannot connect\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]\nCaused by: java.net.ConnectException: Connection refused: /127.0.0.1:9042\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]\n\tat com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\t... 6 common frames omitted\nFailed to connect over JMX; not collecting these stats\n\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-b0pWB9\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""

{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-11 13:19:09,4
12996303,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_writing_with_token_boundaries,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/787/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_writing_with_token_boundaries

Failed on CassCI build cassandra-3.0_dtest build #787

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 1022, in test_writing_with_token_boundaries
    self._test_writing_with_token_boundaries(10000, None, 2000000000000000000)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 1059, in _test_writing_with_token_boundaries
    self.assertItemsEqual(csv_values, result)
  File ""/usr/lib/python2.7/unittest/case.py"", line 901, in assertItemsEqual
    self.fail(msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""Element counts were not equal:\nFirst has 0, Second has 1:  ('130', -4364617663693876050L)\nFirst has 0, Second has 1:  ('1504', -4313346088993828066L)\nFirst has 0, Second has 1:  ('1657', -4298243044528711865L)\nFirst has 0, Second has 1:  ('1908', -4357762565998238890L)\nFirst has 0, Second has 1:  ('196', -4311842292754600676L)\nFirst has 0, Second has 1:  ('2069', -4364398944370882217L)\nFirst has 0, Second has 1:  ('2840', -4341639477649832153L)\nFirst has 0, Second has 1:  ('2887', -4318016824479819783L)\nFirst has 0, Second has 1:  ('2899', -4302748366908469185L)\nFirst has 0, Second has 1:  ('2928', -4320094196758787736L)\nFirst has 0, Second has 1:  ('2985', -4314356124534988584L)\nFirst has 0, Second has 1:  ('3684', -4338074463992249966L)\nFirst has 0, Second has 1:  ('371', -4314424123257001171L)\nFirst has 0, Second has 1:  ('3726', -4327342039280507889L)\nFirst has 0, Second has 1:  ('3767', -4314615789624913427L)\nFirst has 0, Second has 1:  ('3837', -4345782419910891107L)\nFirst has 0, Second has 1:  ('3917', -4288469607605675346L)\nFirst has 0, Second has 1:  ('4023', -4327319429102869913L)\nFirst has 0, Second has 1:  ('4340', -4364719196309290555L)\nFirst has 0, Second has 1:  ('4775', -4334399295585005795L)\nFirst has 0, Second has 1:  ('480', -4297721626756162038L)\nFirst has 0, Second has 1:  ('4927', -4363012199808638126L)\nFirst has 0, Second has 1:  ('5227', -4322405738833807588L)\nFirst has 0, Second has 1:  ('564', -4294201317243228473L)\nFirst has 0, Second has 1:  ('585', -4359001293509999319L)\nFirst has 0, Second has 1:  ('5869', -4350305245827564608L)\nFirst has 0, Second has 1:  ('6907', -4350623491924194304L)\nFirst has 0, Second has 1:  ('709', -4304008865600291097L)\nFirst has 0, Second has 1:  ('7415', -4315752378065264743L)\nFirst has 0, Second has 1:  ('7476', -4300546270541034340L)\nFirst has 0, Second has 1:  ('7805', -4344641724309508742L)\nFirst has 0, Second has 1:  ('7922', -4363605089028496367L)\nFirst has 0, Second has 1:  ('8026', -4319008002233878821L)\nFirst has 0, Second has 1:  ('8180', -4361912691055780971L)\nFirst has 0, Second has 1:  ('8371', -4309172311179179912L)\nFirst has 0, Second has 1:  ('8988', -4326093437666683541L)\nFirst has 0, Second has 1:  ('9492', -4347264403260361686L)\nFirst has 0, Second has 1:  ('9783', -4329297319597600121L)\nFirst has 0, Second has 1:  ('9911', -4320490295580904236L)\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-2v8O54\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Exporting to csv file: /tmp/tmpDS47Vq\ndtest: DEBUG: Exporting to csv file: /tmp/tmpQvnl6e\ndtest: DEBUG: Exporting to csv file: /tmp/tmpfEEiAz\ndtest: DEBUG: Exporting to csv file: /tmp/tmpc7T8av\ndtest: DEBUG: Exporting to csv file: /tmp/tmplxBTNa\ndtest: DEBUG: Exporting to csv file: /tmp/tmpb5VYGq\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,['Test/dtest/python'],CASSANDRA,Bug,Normal,2016-08-10 14:51:18,3
12996296,dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.bloomfilter_size_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/669/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test

Failed on CassCI build cassandra-2.2_dtest build #669
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 147, in bloomfilter_size_test
    self.assertLessEqual(bfSize, size_factor * max_bf_size)
  File ""/usr/lib/python2.7/unittest/case.py"", line 936, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""125456 not less than or equal to 0\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-XJ96MC\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: bloom filter size is: 125456\ndtest: DEBUG: size factor = 0\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-10 14:28:31,0
12996274,dtest failure in consistency_test.TestAvailability.test_network_topology_strategy,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/669/testReport/consistency_test/TestAvailability/test_network_topology_strategy

Failed on CassCI build cassandra-2.2_dtest build #669
{code}

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 319, in test_network_topology_strategy
    self._start_cluster()
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 96, in _start_cluster
    cluster.start(wait_for_binary_proto=True, wait_other_notice=True)
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 414, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node9.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-an_vc5\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-10 13:38:37,4
12995962,dtest failure in bootstrap_test.TestBootstrap.local_quorum_bootstrap_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/31/testReport/bootstrap_test/TestBootstrap/local_quorum_bootstrap_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-09 14:08:55,4
12995772,dtest failure in ttl_test.TestDistributedTTL.ttl_is_respected_on_delayed_replication_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/31/testReport/ttl_test/TestDistributedTTL/ttl_is_respected_on_delayed_replication_test

{code}
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/ttl_test.py"", line 439, in ttl_is_respected_on_delayed_replication_test
    self.assertLessEqual(abs(ttl_session1[0][0] - ttl_session2[0][0]), 1)
""unsupported operand type(s) for -: 'NoneType' and 'int'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-08 22:25:16,4
12995764,CompactionsCQLTest.testTriggerMinorCompactionDTCS fails,"[Link|http://cassci.datastax.com/job/cassandra-3.9_testall/lastCompletedBuild/testReport/org.apache.cassandra.db.compaction/CompactionsCQLTest/testTriggerMinorCompactionDTCS/]

Error Message
No minor compaction triggered in 5000ms

Stacktrace
{noformat}
junit.framework.AssertionFailedError: No minor compaction triggered in 5000ms
	at org.apache.cassandra.db.compaction.CompactionsCQLTest.waitForMinor(CompactionsCQLTest.java:247)
	at org.apache.cassandra.db.compaction.CompactionsCQLTest.testTriggerMinorCompactionDTCS(CompactionsCQLTest.java:72)
{noformat}",unittest,['Legacy/Testing'],CASSANDRA,Bug,Normal,2016-08-08 21:37:35,0
12995711,dtest failure in upgrade_tests.paging_test.TestPagingDataNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x.test_paging_using_secondary_indexes,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/22/testReport/upgrade_tests.paging_test/TestPagingDataNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/test_paging_using_secondary_indexes

{code}
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-08-06 02:34:06,595 CassandraDaemon.java:201 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer.deserialize(ReadCommand.java:1042) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer.deserialize(ReadCommand.java:964) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.8.jar:3.0.8]

{code}",dtest,['Feature/2i Index'],CASSANDRA,Improvement,Normal,2016-08-08 17:28:13,5
12995677,dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_2_x_To_indev_3_x.bootstrap_multidc_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_large_dtest/7/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_2_x_To_indev_3_x/bootstrap_multidc_test

{code}
node5: ERROR [main] 2016-08-06 18:38:38,187 MigrationManager.java:164 - Migration task failed to complete
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-08 15:25:09,5
12995674,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_2_2_x.select_with_alias_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/11/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_2_2_x/select_with_alias_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-08 15:19:07,5
12995671,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x.select_with_alias_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/6/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x/select_with_alias_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 3187, in select_with_alias_test
    assert_invalid(cursor, 'SELECT id AS user_id, name AS user_name FROM users WHERE id IN (0) ORDER BY user_name', matching=error_msg)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 92, in assert_invalid
    assert_exception(session, query, matching=matching, expected=expected)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 65, in assert_exception
    _assert_exception(session.execute, query, matching=matching, expected=expected)
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 54, in _assert_exception
    assert_regexp_matches(str(e), matching)
  File ""/usr/lib/python2.7/unittest/case.py"", line 1002, in assertRegexpMatches
    raise self.failureException(msg)
'Regexp didn\'t match: ""Aliases aren\'t allowed in the where clause"" not found in \'Error from server: code=2200 [Invalid query] message=""Aliases are not allowed in order by clause (\\\'user_name\\\')""\'
{code}

Related failures:
http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/6/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_0_x_To_indev_2_1_x/select_with_alias_test/
http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/6/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x/select_with_alias_test/
http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/6/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_2_1_x/select_with_alias_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-08-08 15:11:31,4
12995667,dtest failure in cql_tracing_test.TestCqlTracing.tracing_simple_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/381/testReport/cql_tracing_test/TestCqlTracing/tracing_simple_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 102, in tracing_simple_test
    self.trace(session)
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 74, in trace
    self.assertIn('/127.0.0.1', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 803, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
'\'/127.0.0.1\' not found in ""Consistency level set to ALL.
{code}",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-08-08 14:54:38,4
12995287,dtest failure in cqlsh_tests.cqlsh_tests.TestCqlsh.test_pep8_compliance,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1326/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_pep8_compliance

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 290, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py"", line 69, in test_pep8_compliance
    self.assertEqual(len(stdout), 0, stdout)
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""/home/automaton/cassandra/pylib/cqlshlib/cql3handling.py:796:1: E302 expected 2 blank lines, found 1
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-05 17:45:56,5
12995237,dtest failure in bootstrap_test.TestBootstrap.local_quorum_bootstrap_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/666/testReport/bootstrap_test/TestBootstrap/local_quorum_bootstrap_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-05 15:15:55,4
12995234,dtest failure in read_repair_test.TestReadRepair.test_gcable_tombstone_resurrection_on_range_slice_query,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/666/testReport/read_repair_test/TestReadRepair/test_gcable_tombstone_resurrection_on_range_slice_query",dtest,[],CASSANDRA,Improvement,Normal,2016-08-05 15:07:15,5
12995126,List Append order is wrong,"""INSERT INTO collection_type(key,normal_column,list_column) VALUES ('k','value',[ '#293847','#323442' ]);""

""UPDATE collection_type SET list_column=list_column+'#611987' WHERE key='k`;""

Using 2.1.7.1 java driver to run Update query, the output is: '#611987', '#293847','#323442'

Using DevCenter 1.3.1 to execute Update query, result is in correct order: '#293847','#323442', '#611987'

The error happened in 3 node cluster. In local, one node is working properly.
(all Cassandra 2.1.13. )

Is it related to internal message processing?",remove-reopen,[],CASSANDRA,Bug,Normal,2016-08-05 07:07:45,2
12994919,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_reading_with_multiple_files,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/783/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_reading_with_multiple_files",dtest,[],CASSANDRA,Improvement,Normal,2016-08-04 15:03:15,3
12994680,dtest failure in upgrade_tests.upgrade_through_versions_test.ProtoV1Upgrade_AllVersions_EndsAt_indev_2_2_x.parallel_upgrade_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/276/testReport/upgrade_tests.upgrade_through_versions_test/ProtoV1Upgrade_AllVersions_EndsAt_indev_2_2_x/parallel_upgrade_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-03 19:43:17,5
12994675,dtest failure in read_repair_test.TestReadRepair.test_gcable_tombstone_resurrection_on_range_slice_query,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_jdk8/291/testReport/read_repair_test/TestReadRepair/test_gcable_tombstone_resurrection_on_range_slice_query",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-08-03 19:27:21,2
12994269,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x.list_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/5/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x/list_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 1375, in list_test
    assert_one(cursor, ""SELECT tags FROM user"", [['foo', 'bar', 'foo', 'foobar']])
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 124, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
""Expected [[['foo', 'bar', 'foo', 'foobar']]] from SELECT tags FROM user, but got [[[u'foo', u'foo', u'bar', u'foo', u'foobar']]]
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-08-02 12:20:02,5
12994072,dtest failure in materialized_views_test.TestMaterializedViews.add_dc_after_mv_simple_replication_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/24/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_simple_replication_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-01 19:45:06,5
12994067,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_2_2_x.bug_10652_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/7/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_2_2_x/bug_10652_test",dtest,[],CASSANDRA,Improvement,Normal,2016-08-01 19:25:09,5
12994004,dtest failure in upgrade_tests.storage_engine_upgrade_test.TestLoadLaCompactSStables.sstableloader_compression_snappy_to_snappy_test,"These are all similar looking test failures, which appear to be timeout issues of some kind.
{noformat}('Unable to connect to any servers', {'127.0.0.1': OperationTimedOut('errors=Timed out creating connection (10 seconds), last_host=None',)}){noformat}


http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.storage_engine_upgrade_test/TestLoadLaCompactSStables/sstableloader_compression_snappy_to_snappy_test

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.storage_engine_upgrade_test/TestLoadLaSStables/sstableloader_compression_deflate_to_deflate_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDataNodes2RF1_Upgrade_current_2_1_x_To_indev_3_0_x/test_paging_across_multi_wide_rows/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDataNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/test_paging_across_multi_wide_rows/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingSizeNodes2RF1_Upgrade_current_2_1_x_To_indev_3_0_x/test_undefined_page_size_default/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/test_cell_TTL_expiry_during_paging/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingSizeNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x/test_undefined_page_size_default/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes2RF1_Upgrade_current_3_0_x_To_indev_3_0_x/test_cell_TTL_expiry_during_paging/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes2RF1_Upgrade_current_2_1_x_To_indev_3_0_x/test_row_TTL_expiry_during_paging/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.storage_engine_upgrade_test/TestBootstrapAfterUpgrade/upgrade_with_unclustered_CQL_table_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/range_tombstones_compaction_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_3_0_x_To_indev_3_0_x/bug_5732_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/collection_flush_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_3_0_x_To_indev_3_0_x/large_count_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/conditional_delete_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_3_0_x/range_tombstones_compaction_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingWithDeletionsNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x/test_failure_threshold_deletions/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingWithDeletionsNodes2RF1_Upgrade_current_3_0_x_To_indev_3_0_x/test_ttl_deletions/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/limit_multiget_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/limit_multiget_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/test_data_change_impacting_later_page/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/expanded_list_item_conditional_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/no_range_ghost_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/limit_compact_table_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/counters_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/bug_6612_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/static_columns_with_2i_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/bug_6115_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/cas_and_ttl_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/order_by_multikey_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDataNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x/test_paging_using_secondary_indexes/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/indexes_composite_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/multi_in_compact_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/more_user_types_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingWithModifiersNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/test_with_order_by_reversed/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingWithModifiersNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/test_with_order_by/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/key_index_with_reverse_clustering_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/limit_sparse_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/select_distinct_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/dense_cf_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/list_item_conditional_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/bug_5240_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_3_0_x/bug7105_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/10/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_2_x_To_indev_3_0_x/composite_index_with_pk_test/

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/7/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x/batch_test/

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/7/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_2_1_x_To_indev_2_2_x/static_with_limit_test/

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/7/testReport/upgrade_tests.paging_test/TestPagingWithDeletionsNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x/test_multiple_cell_deletions/

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/7/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_2_x_To_indev_2_2_x/range_query_2ndary_test/
",dtest,[],CASSANDRA,Improvement,Normal,2016-08-01 15:46:29,5
12993560,dtest failure in hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_enabled_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/440/testReport/hintedhandoff_test/TestHintedHandoffConfig/hintedhandoff_enabled_test

{code}
Error Message

29 Jul 2016 00:56:17 [node1] Missing: ['Finished hinted']:
INFO  [HANDSHAKE-/127.0.0.2] 2016-07-29 00:54:14,4.....
See system.log for remainder
{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/hintedhandoff_test.py"", line 125, in hintedhandoff_enabled_test
    self._do_hinted_handoff(node1, node2, True)
  File ""/home/automaton/cassandra-dtest/hintedhandoff_test.py"", line 61, in _do_hinted_handoff
    node1.watch_log_for([""Finished hinted""], from_mark=log_mark, timeout=120)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""29 Jul 2016 00:56:17 [node1] Missing: ['Finished hinted']:\nINFO  [HANDSHAKE-/127.0.0.2] 2016-07-29 00:54:14,4.....\nSee system.log for remainder
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-29 15:08:44,5
12993539,dtest failure in cql_tracing_test.TestCqlTracing.tracing_unknown_impl_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/21/testReport/cql_tracing_test/TestCqlTracing/tracing_unknown_impl_test",dtest,[],CASSANDRA,Improvement,Normal,2016-07-29 13:15:46,5
12992973,dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.bloomfilter_size_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/19/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test

500352 not less than or equal to 150000",dtest,[],CASSANDRA,Bug,Normal,2016-07-27 15:52:03,0
12992959,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_round_trip_random,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/16/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_round_trip_random",dtest,[],CASSANDRA,Improvement,Normal,2016-07-27 15:15:41,5
12992950,dtest failure in nose.failure.Failure.runTest,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_cqlsh_tests/17/testReport/nose.failure/Failure/runTest

{code}
Stack Trace
Traceback (most recent call last):
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/usr/local/lib/python2.7/dist-packages/nose/loader.py"", line 418, in loadTestsFromName
    addr.filename, addr.module)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/__init__.py"", line 17, in <module>
    from .cassconnect import create_test_db, remove_test_db
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/cassconnect.py"", line 22, in <module>
    from .basecase import cql, cqlsh, cqlshlog, TEST_HOST, TEST_PORT, rundir
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/basecase.py"", line 46, in <module>
    import cqlsh
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/cqlsh.py"", line 109, in <module>
    from cassandra.cluster import Cluster, PagedResult
ImportError: cannot import name PagedResult
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-27 14:57:37,4
12992417,SSTablesIteratedTest.testDeletionOnOverlappingIndexedSSTable-compression is flaky,"[History|https://cassci.datastax.com/view/cassandra-3.9/job/cassandra-3.9_testall/lastCompletedBuild/testReport/org.apache.cassandra.cql3.validation.miscellaneous/SSTablesIteratedTest/testDeletionOnOverlappingIndexedSSTable_compression/history/]

Error Message
expected:<2> but was:<3>

Stacktrace
{noformat}
junit.framework.AssertionFailedError: expected:<2> but was:<3>
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.executeAndCheck(SSTablesIteratedTest.java:45)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnOverlappingIndexedSSTable(SSTablesIteratedTest.java:435)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnOverlappingIndexedSSTable(SSTablesIteratedTest.java:361)
{noformat}",unittest,[],CASSANDRA,Improvement,Low,2016-07-25 22:14:54,3
12992308,dtest failure in json_test.JsonFullRowInsertSelect.pkey_requirement_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/354/testReport/json_test/JsonFullRowInsertSelect/pkey_requirement_test

{code}
Error Message

Doctest failed! Captured output:
**********************************************************************
Line 25, in pkey_requirement_test
Failed example:
    cqlsh_err_print('''INSERT INTO primitive_type_test JSON '{""col1"": ""bar""}' ''')
Expected:
    <stdin>:2:InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid null value in condition for column key1""
    <BLANKLINE>
Got:
    <stdin>:2:InvalidRequest: code=2200 [Invalid query] message=""Invalid null value in condition for column key1""
    <BLANKLINE>
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-25 15:15:11,5
12992306,dtest failure in cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/354/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_describe

{code}
Error Message

Lists differ: [""CREATE KEYSPACE test WITH re... != [""CREATE KEYSPACE test WITH re...

First differing element 9:
AND cdc = false
{code}

Related Failure: 

http://cassci.datastax.com/job/trunk_offheap_dtest/354/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_describe_mv/",dtest,[],CASSANDRA,Improvement,Normal,2016-07-25 15:11:33,5
12992304,dtest failure in cqlshlib.test.remove_test_db,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_cqlsh_tests/2/testReport/cqlshlib/test/remove_test_db

http://cassci.datastax.com/job/cassandra-3.9_cqlsh_tests/3/cython=yes,label=ctool-lab/testReport/cqlshlib.test.test_cqlsh_completion/TestCqlshCompletion/test_complete_in_create_columnfamily/

http://cassci.datastax.com/job/cassandra-3.9_cqlsh_tests/3/cython=yes,label=ctool-lab/testReport/cqlshlib.test.test_cqlsh_completion/TestCqlshCompletion/test_complete_in_create_table/",dtest,[],CASSANDRA,Improvement,Normal,2016-07-25 15:10:41,5
12992303,dtest failure in secondary_indexes_test.TestSecondaryIndexes.test_query_indexes_with_vnodes,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/347/testReport/secondary_indexes_test/TestSecondaryIndexes/test_query_indexes_with_vnodes

{code}
Standard Output

Unexpected error in node2 log, error: 
ERROR [ReadStage-1] 2016-07-20 04:58:27,391 MessageDeliveryTask.java:74 - The secondary index 'composites_index' is not yet available
{code}",dtest,['Feature/2i Index'],CASSANDRA,Improvement,Normal,2016-07-25 15:02:56,4
12992275,dtest failure in user_functions_test.TestUserFunctions.test_migration,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/17/testReport/user_functions_test/TestUserFunctions/test_migration",dtest,[],CASSANDRA,Improvement,Normal,2016-07-25 13:32:50,4
12991974,SSTablesIteratedTest.testDeletionOnIndexedSSTableASC-compression failure,"Error Message
expected:<3> but was:<4>

Stacktrace
junit.framework.AssertionFailedError: expected:<3> but was:<4>
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.executeAndCheck(SSTablesIteratedTest.java:45)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnIndexedSSTableASC(SSTablesIteratedTest.java:348)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnIndexedSSTableASC(SSTablesIteratedTest.java:312)

[Failure|http://cassci.datastax.com/job/cassandra-3.9_testall/lastCompletedBuild/testReport/org.apache.cassandra.cql3.validation.miscellaneous/SSTablesIteratedTest/testDeletionOnIndexedSSTableASC_compression/]
",unittest,[],CASSANDRA,Improvement,Normal,2016-07-22 22:47:05,3
12991836,dtest failure in offline_tools_test.TestOfflineTools.sstableofflinerelevel_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/271/testReport/offline_tools_test/TestOfflineTools/sstableofflinerelevel_test",dtest windows,[],CASSANDRA,Improvement,Normal,2016-07-22 14:43:58,5
12991526,dtest failure in materialized_views_test.TestMaterializedViews.add_dc_after_mv_simple_replication_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/432/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_simple_replication_test

{code}
Standard Output

Unexpected error in node4 log, error: 
ERROR [main] 2016-07-21 01:57:17,951 MigrationManager.java:164 - Migration task failed to complete
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-21 16:33:10,5
12991524,dtest failure in bootstrap_test.TestBootstrap.consistent_range_movement_false_with_two_replicas_down_should_fail_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/431/testReport/bootstrap_test/TestBootstrap/consistent_range_movement_false_with_two_replicas_down_should_fail_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 180, in consistent_range_movement_false_with_two_replicas_down_should_fail_test
    self._bootstrap_test_with_replica_down(False, stop_two_replicas=True)
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 233, in _bootstrap_test_with_replica_down
    node4.watch_log_for(""Unable to find sufficient sources for streaming range"")
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""20 Jul 2016 00:58:03 [node4] Missing: ['Unable to find sufficient sources for streaming range']:\nINFO  [main] 2016-07-20 00:48:02,304 YamlConfigura.....\nSee system.log for remainder
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-21 16:30:58,5
12991522,dtest failure in user_functions_test.TestUserFunctions.test_migration,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1311/testReport/user_functions_test/TestUserFunctions/test_migration

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/user_functions_test.py"", line 92, in test_migration
    [2, 2.0, math.sin(2.0), math.cos(2.0), math.tan(2.0)])
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 122, in assert_one
    res = session.execute(simple_query)
  File ""cassandra/cluster.py"", line 1941, in cassandra.cluster.Session.execute (cassandra/cluster.c:33653)
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile).result()
  File ""cassandra/cluster.py"", line 3629, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:69380)
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""Unknown function \'x_tan\'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-21 16:27:22,4
12991519,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/15/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test

{code}
Error Message

'Repaired at: 0' unexpectedly found in 'SSTable: 
{code}

Related failure:

http://cassci.datastax.com/job/trunk_dtest/1315/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test/",dtest,[],CASSANDRA,Bug,Normal,2016-07-21 16:21:53,0
12991488,dtest failure in write_failures_test.TestWriteFailures.test_thrift,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/14/testReport/write_failures_test/TestWriteFailures/test_thrift

Failure is
{code}
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,127 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-2-big-Data.db as it does not exist
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,334 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-15-big-Data.db as it does not exist
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,337 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-31-big-Data.db as it does not exist
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,339 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-18-big-Data.db as it does not exist
{code}",dtest,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2016-07-21 14:31:19,3
12989873,cqlshlib test failure: cqlshlib.test.remove_test_db,"[~Stefania]  
http://cassci.datastax.com/job/cassandra-3.9_cqlsh_tests/lastCompletedBuild/testReport/

Hello, these three tests are failing:
cqlshlib.test.remove_test_db
cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_create_columnfamily
cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_create_table

Can you look at them, please?  Thank you!",cqlsh,['Legacy/Testing'],CASSANDRA,Bug,Normal,2016-07-15 17:51:50,3
12989861,dtest failure in write_failures_test.TestWriteFailures.test_paxos_any,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/10/testReport/write_failures_test/TestWriteFailures/test_paxos_any

and:

http://cassci.datastax.com/job/cassandra-3.9_dtest/10/testReport/write_failures_test/TestWriteFailures/test_mutation_v3/

Failed on CassCI build cassandra-3.9_dtest #10

",dtest,['Legacy/Distributed Metadata'],CASSANDRA,Bug,Normal,2016-07-15 17:20:27,3
12989184,dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_1_x_To_indev_3_x.bootstrap_test,"example failure:

http://cassci.datastax.com/job/upgrade_tests-all/59/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_1_x_To_indev_3_x/bootstrap_test

Failed on CassCI build upgrade_tests-all #59

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 707, in bootstrap_test
    self.upgrade_scenario(after_upgrade_call=(self._bootstrap_new_node,))
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 383, in upgrade_scenario
    call()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 688, in _bootstrap_new_node
    nnode.start(use_jna=True, wait_other_notice=True, wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 634, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 481, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""13 Jul 2016 02:23:05 [node2] Missing: ['127.0.0.4.* now UP']:\nINFO  [HANDSHAKE-/127.0.0.4] 2016-07-13 02:21:00,2.....\nSee system.log for remainder
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-13 17:11:30,4
12989179,dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_2_x_To_indev_3_0_x.rolling_upgrade_with_internode_ssl_test,"example failure:

http://cassci.datastax.com/job/upgrade_tests-all/59/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_2_x_To_indev_3_0_x/rolling_upgrade_with_internode_ssl_test

Failed on CassCI build upgrade_tests-all #59

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 295, in rolling_upgrade_with_internode_ssl_test
    self.upgrade_scenario(rolling=True, internode_ssl=True)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 352, in upgrade_scenario
    self._check_on_subprocs(self.subprocs)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 409, in _check_on_subprocs
    raise RuntimeError(message)
""A subprocess has terminated early. Subprocess statuses: Process-13 (is_alive: False), Process-14 (is_alive: True), Process-15 (is_alive: True), Process-16 (is_alive: True), attempting to terminate remaining subprocesses now.
{code}

node2_debug.log is too large to attach.",dtest,[],CASSANDRA,Improvement,Normal,2016-07-13 17:04:27,5
12988256,dtest failure in materialized_views_test.TestMaterializedViews.add_dc_after_mv_network_replication_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/309/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_network_replication_test

Failed on CassCI build trunk_offheap_dtest #309

{code}
Standard Output

Unexpected error in node4 log, error: 
ERROR [main] 2016-07-06 19:21:26,631 MigrationManager.java:164 - Migration task failed to complete
{code}

Related failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/423/testReport/materialized_views_test/TestMaterializedViews/add_node_after_mv_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-07-11 12:08:06,5
12988243,dtest failure in cql_tracing_test.TestCqlTracing.tracing_simple_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/493/testReport/cql_tracing_test/TestCqlTracing/tracing_simple_test

Failed on CassCI build cassandra-2.1_dtest #493

Seems related to CASSANDRA-12007.

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 102, in tracing_simple_test
    self.trace(session)
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 73, in trace
    self.assertIn('/127.0.0.1', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 803, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
'\'/127.0.0.1\' not found in ""Consistency level set to ALL....
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-11 11:31:05,5
12988240,dtest failure in thrift_tests.TestMutations.test_describe_keyspace,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/492/testReport/thrift_tests/TestMutations/test_describe_keyspace

Failed on CassCI build cassandra-2.1_dtest #492

{code}
Stacktrace

Traceback (most recent call last):
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/thrift_tests.py"", line 1507, in test_describe_keyspace
    assert len(kspaces) == 4, [x.name for x in kspaces]  # ['Keyspace2', 'Keyspace1', 'system', 'system_traces']
AssertionError: ['Keyspace2', 'system', 'Keyspace1', 'ValidKsForUpdate', 'system_traces']
{code}

Related failures:
http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/304/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/cassandra-3.0_dtest/767/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/264/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/trunk_dtest/1301/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/trunk_novnode_dtest/421/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/cassandra-3.9_dtest/6/testReport/thrift_tests/TestMutations/test_describe_keyspace/",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-07-11 11:22:24,4
12987565,Improve determinism of CDC data availability,"The latency with which CDC data becomes available has a known limitation due to our reliance on CommitLogSegments being discarded to have the data available in cdc_raw: if a slowly written table co-habitates a CommitLogSegment with CDC data, the CommitLogSegment won't be flushed until we hit either memory pressure on memtables or CommitLog limit pressure. Ultimately, this leaves a non-deterministic element to when data becomes available for CDC consumption unless a consumer parses live CommitLogSegments.

To work around this limitation and make semi-realtime CDC consumption more friendly to end-users, I propose we extend CDC as follows:
h6. High level:
* Consumers parse hard links of active CommitLogSegments in cdc_raw instead of waiting for flush/discard and file move
* C* stores an offset of the highest seen CDC mutation in a separate idx file per commit log segment in cdc_raw. Clients tail this index file, delta their local last parsed offset on change, and parse the corresponding commit log segment using their last parsed offset as min
* C* flags that index file with an offset and DONE when the file is flushed so clients know when they can clean up

h6. Details:
* On creation of a CommitLogSegment, also hard-link the file in cdc_raw
* On first write of a CDC-enabled mutation to a segment, we:
** Flag it as {{CDCState.CONTAINS}}
** Set a long tracking the {{CommitLogPosition}} of the 1st CDC-enabled mutation in the log
** Set a long in the CommitLogSegment tracking the offset of the end of the last written CDC mutation in the segment if higher than the previously known highest CDC offset
* On subsequent writes to the segment, we update the offset of the highest known CDC data
* On CommitLogSegment fsync, we write a file in cdc_raw as <segment_name>_cdc.idx containing the min offset and end offset fsynced to disk per file
* On segment discard, if CDCState == {{CDCState.PERMITTED}}, delete both the segment in commitlog and in cdc_raw
* On segment discard, if CDCState == {{CDCState.CONTAINS}}, delete the segment in commitlog and update the <segment_name>_cdc.idx file w/end offset and a DONE marker
* On segment replay, store the highest end offset of seen CDC-enabled mutations from a segment and write that to <segment_name>_cdc.idx on completion of segment replay. This should bridge the potential correctness gap of a node writing to a segment and then dying before it can write the <segment_name>_cdc.idx file.

This should allow clients to skip the beginning of a file to the 1st CDC mutation, track an offset of how far they've parsed, delta against the _cdc.idx file end offset, and use that as a determinant on when to parse new CDC data. Any existing clients written to the initial implementation of CDC need only add the <segment_name>_cdc.idx logic and checking for DONE marker to their code, so the burden on users to update to support this should be quite small for the benefit of having data available as soon as it's fsynced instead of at a non-deterministic time when potentially unrelated tables are flushed.

Finally, we should look into extending the interface on CommitLogReader to be more friendly for realtime parsing, perhaps supporting taking a CommitLogDescriptor and RandomAccessReader and resuming readSection calls, assuming the reader is at the start of a SyncSegment. Would probably also need to rewind to the start of the segment before returning so subsequent calls would respect this contract. This would skip needing to deserialize the descriptor and all completed SyncSegments to get to the root of the desired segment for parsing.

One alternative we discussed offline - instead of just storing the highest seen CDC offset, we could instead store an offset per CDC mutation (potentially delta encoded) in the idx file to allow clients to seek and only parse the mutations with CDC enabled. My hunch is that the performance delta from doing so wouldn't justify the complexity given the SyncSegment deserialization and seeking restrictions in the compressed and encrypted cases as mentioned above.

The only complication I can think of with the above design is uncompressed mmapped CommitLogSegments on Windows being undeletable, but it'd be pretty simple to disallow configuration of CDC w/uncompressed CommitLog on that environment.

And as a final note: while the above might sound involved, it really shouldn't be a big change from where we are with v1 of CDC from a C* complexity nor code perspective, or from a client implementation perspective.",native_protocol,[],CASSANDRA,Improvement,Normal,2016-07-07 18:21:03,6
12986893,dtest failure in snapshot_test.TestArchiveCommitlog.dont_test_archive_commitlog,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/409/testReport/snapshot_test/TestArchiveCommitlog/dont_test_archive_commitlog

Failed on CassCI build trunk_novnode_dtest #409

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 168, in dont_test_archive_commitlog
    self.run_archive_commitlog(restore_point_in_time=False, restore_archived_commitlog=False)
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 279, in run_archive_commitlog
    set())
  File ""/usr/lib/python2.7/unittest/case.py"", line 522, in assertNotEqual
    raise self.failureException(msg)
""set([]) == set([])
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-05 18:01:09,4
12986878,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_round_trip_random,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/764/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_round_trip_random

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 928, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2334, in test_round_trip_random
    self._test_round_trip(nodes=3, partitioner=""random"")
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2296, in _test_round_trip
    self.prepare(nodes=nodes, partitioner=partitioner)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 109, in prepare
    self.cluster.populate(nodes, tokens=tokens).start(wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 412, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.
{code}

Failed on CassCI build cassandra-3.0_dtest #764",dtest,[],CASSANDRA,Improvement,Normal,2016-07-05 16:48:55,5
12986872,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_jdk8/240/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test

Failed on CassCI build cassandra-2.1_dtest_jdk8 #240

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 42, in sstable_marking_test
    node3.stop(gently=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 701, in stop
    raise NodeError(""Problem stopping node %s"" % self.name)
""Problem stopping node node3
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-07-05 16:29:05,5
12986866,dtest failure in repair_tests.repair_test.TestRepair.dc_repair_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/488/testReport/repair_tests.repair_test/TestRepair/dc_repair_test

Failed on CassCI build cassandra-2.1_dtest #488

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 441, in dc_repair_test
    cluster = self._setup_multi_dc()
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 540, in _setup_multi_dc
    self.check_rows_on_node(node2, 2000, missings=[1000])
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 70, in check_rows_on_node
    self.assertEqual(len(result), rows)
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""2001 != 2000
{code}
",dtest,[],CASSANDRA,Improvement,Normal,2016-07-05 16:18:04,5
12986267,CAS Reads Inconsistencies ,"While looking at the CAS code in Cassandra, I found a potential issue with CAS Reads. Here is how it can happen with RF=3

1) You issue a CAS Write and it fails in the propose phase. A machine replies true to a propose and saves the commit in accepted filed. The other two machines B and C does not get to the accept phase. 

Current state is that machine A has this commit in paxos table as accepted but not committed and B and C does not. 

2) Issue a CAS Read and it goes to only B and C. You wont be able to read the value written in step 1. This step is as if nothing is inflight. 

3) Issue another CAS Read and it goes to A and B. Now we will discover that there is something inflight from A and will propose and commit it with the current ballot. Now we can read the value written in step 1 as part of this CAS read.

If we skip step 3 and instead run step 4, we will never learn about value written in step 1. 

4. Issue a CAS Write and it involves only B and C. This will succeed and commit a different value than step 1. Step 1 value will never be seen again and was never seen before. 



If you read the Lamport “paxos made simple” paper and read section 2.3. It talks about this issue which is how learners can find out if majority of the acceptors have accepted the proposal. 

In step 3, it is correct that we propose the value again since we dont know if it was accepted by majority of acceptors. When we ask majority of acceptors, and more than one acceptors but not majority has something in flight, we have no way of knowing if it is accepted by majority of acceptors. So this behavior is correct. 

However we need to fix step 2, since it caused reads to not be linearizable with respect to writes and other reads. In this case, we know that majority of acceptors have no inflight commit which means we have majority that nothing was accepted by majority. I think we should run a propose step here with empty commit and that will cause write written in step 1 to not be visible ever after. 

With this fix, we will either see data written in step 1 on next serial read or will never see it which is what we want. 
",LWT pull-request-available,"['Feature/Lightweight Transactions', 'Legacy/Coordination']",CASSANDRA,Bug,Normal,2016-07-01 17:46:09,2
12985848,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_next_2_1_x_To_head_trunk.select_with_alias_test,"example failure:

http://cassci.datastax.com/job/upgrade_tests-all-custom_branch_runs/37/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_next_2_1_x_To_head_trunk/select_with_alias_test

Failed on CassCI build upgrade_tests-all-custom_branch_runs #37

This is just a problem with different error messages across C* versions. Someone needs to do the legwork of figuring out what is required where, and filtering. The query is failing correctly.",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 19:02:30,2
12985826,read_repair_test.TestReadRepair.alter_rf_and_run_read_repair_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1292/testReport/read_repair_test/TestReadRepair/alter_rf_and_run_read_repair_test/

Failed on CassCI build trunk_dtest #1292",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 17:12:19,4
12985806,dtest failure in compaction_test.TestCompaction_with_SizeTieredCompactionStrategy.compaction_throughput_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/compaction_throughput_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 251, in compaction_throughput_test
    avgthroughput = re.match(throughput_pattern, stringline).group(1).strip()
""'NoneType' object has no attribute 'group'
{code}

Related failures:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/compaction_throughput_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/compaction_throughput_test/

Failed on CassCI build trunk_dtest #1290",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 15:24:03,5
12985805,dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.large_compaction_warning_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/large_compaction_warning_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 330, in large_compaction_warning_test
    node.watch_log_for('{} large partition ks/large:user \({}\)'.format(verb, sizematcher), from_mark=mark, timeout=180)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 448, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""28 Jun 2016 15:16:51 [node1] Missing: ['Writing large partition ks/large:user \\\\(\\\\d+ bytes\\\\)']:\nINFO  [Native-Transport-Requests-5] 2016-06-28 15:.....\nSee system.log for remainder
{code}

Related failures:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/large_compaction_warning_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/large_compaction_warning_test/

Failed on CassCI build trunk_dtest #1290",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 15:22:46,4
12985802,dtest failure in cqlsh_tests.cqlsh_tests.TestCqlsh.test_past_and_future_dates,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_past_and_future_dates

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py"", line 149, in test_past_and_future_dates
    self.assertIn(""2143-04-19 11:21:01+0000"", output)
  File ""/usr/lib/python2.7/unittest/case.py"", line 803, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""'2143-04-19 11:21:01+0000' not found in '\\n id | value\\n----+---------------------------------\\n  1 | 2143-04-19 11:21:01.000000+0000\\n  2 | 1943-04-19 11:21:01.000000+0000\\n\\n(2 rows)\\n'
{code}

Failed on CassCI build trunk_dtest #1290",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 15:20:41,5
12985798,dtest failure in jmx_test.TestJMX.table_metric_mbeans_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/jmx_test/TestJMX/table_metric_mbeans_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/jmx_test.py"", line 77, in table_metric_mbeans_test
    with JolokiaAgent(node1) as jmx:
  File ""/home/automaton/cassandra-dtest/jmxutils.py"", line 231, in __enter__
    self.start()
  File ""/home/automaton/cassandra-dtest/jmxutils.py"", line 122, in start
    subprocess.check_output(args, stderr=subprocess.STDOUT)
  File ""/usr/lib/python2.7/subprocess.py"", line 573, in check_output
    raise CalledProcessError(retcode, cmd, output=output)
""Command '('/usr/lib/jvm/jdk1.8.0_45/bin/java', '-cp', '/usr/lib/jvm/jdk1.8.0_45/lib/tools.jar:lib/jolokia-jvm-1.2.3-agent.jar', 'org.jolokia.jvmagent.client.AgentLauncher', '--host', '127.0.0.1', 'start', '32367')' returned non-zero exit status 1
{code}

Other related failures:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/configuration_test/TestConfiguration/change_durable_writes_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/repair_tests.deprecated_repair_test/TestDeprecatedRepairAPI/force_repair_async_1_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/disk_balance_test/TestDiskBalance/blacklisted_directory_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/repair_tests.deprecated_repair_test/TestDeprecatedRepairAPI/force_repair_async_2_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/thrift_hsha_test/ThriftHSHATest/test_closing_connections/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/jmx_test/TestJMX/test_compactionstats/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/repair_tests.deprecated_repair_test/TestDeprecatedRepairAPI/force_repair_range_async_1_test/



Failed on CassCI build trunk_dtest #1290",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 15:11:36,5
12985792,dtest failure in snapshot_test.TestSnapshot.test_basic_snapshot_and_restore,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/260/testReport/snapshot_test/TestSnapshot/test_basic_snapshot_and_restore

Failed on CassCI build cassandra-2.1_novnode_dtest #260

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 96, in test_basic_snapshot_and_restore
    self.restore_snapshot(snapshot_dir, node1, 'ks', 'cf')
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 68, in restore_snapshot
    ("" "".join(args), exit_status, stdout, stderr))
'sstableloader command \'/home/automaton/cassandra/bin/sstableloader -d 127.0.0.1 /tmp/tmpGgR_dT/0/ks/cf\' failed; exit status: 1\'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-30 14:49:14,5
12985134,dtest failure in rebuild_test.TestRebuild.rebuild_ranges_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/647/testReport/rebuild_test/TestRebuild/rebuild_ranges_test

Failed on CassCI build cassandra-2.2_dtest #647",dtest,[],CASSANDRA,Improvement,Normal,2016-06-29 18:33:02,4
12984398,DESCRIBE INDEX: missing quotes for case-sensitive index name,"Create a custom index with a case-sensitive name.
The result of the DESCRIBE INDEX command does not have quotes around the index name. As a result, the index cannot be recreated with this output.",cqlsh lhf,[],CASSANDRA,Bug,Low,2016-06-28 14:44:20,3
12983836,dtest failure in snitch_test.TestGossipingPropertyFileSnitch.test_prefer_local_reconnect_on_listen_address,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/274/testReport/snitch_test/TestGossipingPropertyFileSnitch/test_prefer_local_reconnect_on_listen_address

Failed on CassCI build trunk_offheap_dtest #274

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 761, in tearDown
    ['\n'.join(msg) for msg in node.grep_log_for_errors()]))
  File ""/home/automaton/ccm/ccmlib/node.py"", line 357, in grep_log_for_errors
    return self.grep_log_for_errors_from(seek_start=getattr(self, 'error_mark', 0))
  File ""/home/automaton/ccm/ccmlib/node.py"", line 360, in grep_log_for_errors_from
    with open(os.path.join(self.get_path(), 'logs', filename)) as f:
""[Errno 2] No such file or directory: '/mnt/tmp/dtest-Pa4WH7/test/node2/logs/system.log'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-27 16:05:00,5
12983820,dtest failure in consistency_test.TestAccuracy.test_simple_strategy_each_quorum_users,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/407/testReport/consistency_test/TestAccuracy/test_simple_strategy_each_quorum_users

Failed on CassCI build trunk_novnode_dtest #407

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 591, in test_simple_strategy_each_quorum_users
    self._run_test_function_in_parallel(TestAccuracy.Validation.validate_users, [self.nodes], [self.rf], combinations)
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 543, in _run_test_function_in_parallel
    assert False, err.message
'Error from server: code=2200 [Invalid query] message=""unconfigured table users""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-27 15:25:46,4
12983808,dtest failure in consistency_test.TestAccuracy.test_simple_strategy_counters,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/484/testReport/consistency_test/TestAccuracy/test_simple_strategy_counters

Failed on CassCI build cassandra-2.1_dtest #484

{code}
Standard Error

Traceback (most recent call last):
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 514, in run
    valid_fcn(v)
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 497, in validate_counters
    check_all_sessions(s, n, c)
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 490, in check_all_sessions
    ""value of %s at key %d, instead got these values: %s"" % (write_nodes, val, n, results)
AssertionError: Failed to read value from sufficient number of nodes, required 2 nodes to have a counter value of 1 at key 200, instead got these values: [0, 0, 1]
{code}",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-06-27 14:50:25,3
12983804,dtest failure in pushed_notifications_test.TestPushedNotifications.add_and_remove_node_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/483/testReport/pushed_notifications_test/TestPushedNotifications/add_and_remove_node_test

Failed on CassCI build cassandra-2.1_dtest #483

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/pushed_notifications_test.py"", line 242, in add_and_remove_node_test
    self.assertEquals(2, len(notifications), notifications)
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""[{'change_type': u'NEW_NODE', 'address': ('127.0.0.2', 9042)}]
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-27 14:40:24,5
12982723,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_writing_with_max_output_size,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/260/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_writing_with_max_output_size

Failed on CassCI build cassandra-3.0_dtest_win32 #260",dtest windows,[],CASSANDRA,Improvement,Normal,2016-06-24 15:28:03,5
12982046,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_copy_options_from_config_file,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/262/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_copy_options_from_config_file

Failed on CassCI build trunk_offheap_dtest #262

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2203, in test_copy_options_from_config_file
    [('header', 'True'), ('maxattempts', '9')])
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2194, in do_test
    check_options(out, expected_options)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2176, in check_options
    d = json.loads(opts)
  File ""/usr/lib/python2.7/json/__init__.py"", line 338, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.7/json/decoder.py"", line 366, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.7/json/decoder.py"", line 384, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
'No JSON object could be decoded
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-22 18:40:23,5
12982042,dtest failure in paging_test.TestPagingData.static_columns_paging_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/261/testReport/paging_test/TestPagingData/static_columns_paging_test

Failed on CassCI build trunk_offheap_dtest #261

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 715, in static_columns_paging_test
    self.assertEqual(16, len(results))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""16 != 6
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-22 18:21:53,4
12982031,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_round_trip_with_different_number_precision,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/405/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_round_trip_with_different_number_precision

Failed on CassCI build trunk_novnode_dtest #405

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2049, in test_round_trip_with_different_number_precision
    do_test(0, 0)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2044, in do_test
    self.assertItemsEqual(sorted(list(csv_rows(tempfile1.name))), sorted(list(csv_rows(tempfile2.name))))
  File ""/usr/lib/python2.7/unittest/case.py"", line 901, in assertItemsEqual
    self.fail(msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""Element counts were not equal:\nFirst has 1, Second has 0:  ['1', '1', '1']
{code}

Logs are attached.",dtest,[],CASSANDRA,Improvement,Normal,2016-06-22 18:04:22,5
12981983,dtest failure in repair_tests.repair_test.TestRepair.repair_after_upgrade_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/259/testReport/repair_tests.repair_test/TestRepair/repair_after_upgrade_test

Failed on CassCI build cassandra-3.0_dtest_win32 #259",dtest windows,[],CASSANDRA,Improvement,Normal,2016-06-22 17:23:46,5
12981945,dtest failure in auth_test.TestAuth.login_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1284/testReport/auth_test/TestAuth/login_test

Failed on CassCI build trunk_dtest #1284

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/auth_test.py"", line 82, in login_test
    assert isinstance(e.errors.values()[0], AuthenticationFailed)
{code}

Logs are attached.",dtest,[],CASSANDRA,Improvement,Normal,2016-06-22 16:08:07,5
12981910,dtest failure in pushed_notifications_test.TestPushedNotifications.add_and_remove_node_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/482/testReport/pushed_notifications_test/TestPushedNotifications/add_and_remove_node_test

Failed on CassCI build cassandra-2.1_dtest #482

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/pushed_notifications_test.py"", line 242, in add_and_remove_node_test
    self.assertEquals(2, len(notifications), notifications)
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""[{'change_type': u'NEW_NODE', 'address': ('127.0.0.2', 9042)}]
{code}

Logs are attached.",dtest,[],CASSANDRA,Improvement,Normal,2016-06-22 15:04:49,5
12981429,dtest failure in batch_test.TestBatch.logged_batch_compatibility_3_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/252/testReport/batch_test/TestBatch/logged_batch_compatibility_3_test

Failed on CassCI build cassandra-3.0_novnode_dtest #252",dtest,[],CASSANDRA,Improvement,Normal,2016-06-21 17:42:52,5
12981178,Syncing most recent commit in CAS across replicas can cause all CAS queries in the CQL partition to fail,"We update the most recent commit on requiredParticipant replicas if out of sync during the prepare round in beginAndRepairPaxos method. We keep doing this in a loop till the requiredParticipant replicas have the same most recent commit or we hit timeout. 

Say we have 3 machines A,B and C and gc grace on the table is 10 days. We do a CAS write at time 0 and it went to A and B but not to C.  C will get the hint later but will not update the most recent commit in paxos table. This is how CAS hints work. 
In the paxos table whose gc_grace=0, most_recent_commit in A and B will be inserted with timestamp 0 and with a TTL of 10 days. After 10 days, this insert will become a tombstone at time 0 till it is compacted away since gc_grace=0.

Do a CAS read after say 1 day on the same CQL partition and this time prepare phase involved A and C. most_recent_commit on C for this CQL partition is empty. A sends the most_recent_commit to C with a timestamp of 0 and with a TTL of 10 days. This most_recent_commit on C will expire on 11th day since it is inserted after 1 day. 

most_recent_commit are now in sync on A,B and C, however A and B most_recent_commit will expire on 10th day whereas for C it will expire on 11th day since it was inserted one day later. 

Do another CAS read after 10days when most_recent_commit on A and B have expired and is treated as tombstones till compacted. In this CAS read, say A and C are involved in prepare phase. most_recent_commit will not match between them since it is expired in A and is still there on C. This will cause most_recent_commit to be applied to A with a timestamp of 0 and TTL of 10 days. If A has not compacted away the original most_recent_commit which has expired, this new write to most_recent_commit wont be visible on reads since there is a tombstone with same timestamp(Delete wins over data with same timestamp). 

Another round of prepare will follow and again A would say it does not know about most_recent_write(covered by original write which is not a tombstone) and C will again try to send the write to A. This can keep going on till the request timeouts or only A and B are involved in the prepare phase. 

When A’s original most_recent_commit which is now a tombstone is compacted, all the inserts which it was covering will come live. This will in turn again get played to another replica. This ping pong can keep going on for a long time. 

The issue is that most_recent_commit is expiring at different times across replicas. When they get replayed to a replica to make it in sync, we again set the TTL from that point.  
During the CAS read which timed out, most_recent_commit was being sent to another replica in a loop. Even in successful requests, it will try to loop for a couple of times if involving A and C and then when the replicas which respond are A and B, it will succeed. So this will have impact on latencies as well. 

These timeouts gets worse when a machine is down as no progress can be made as the machine with unexpired commit is always involved in the CAS prepare round. Also with range movements, the new machine gaining range has empty most recent commit and gets the commit at a later time causing same issue. 

Repro steps:
1. Paxos TTL is max(3 hours, gc_grace) as defined in SystemKeyspace.paxosTtl(). Change this method to not put a minimum TTL of 3 hours. 
Method  SystemKeyspace.paxosTtl() will look like return metadata.getGcGraceSeconds();   instead of return Math.max(3 * 3600, metadata.getGcGraceSeconds());
We are doing this so that we dont need to wait for 3 hours. 

Create a 3 node cluster with the code change suggested above with machines A,B and C
CREATE KEYSPACE  test WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };
use test;
CREATE TABLE users (a int PRIMARY KEY,b int);
alter table users WITH gc_grace_seconds=120;
consistency QUORUM;
bring down machine C
INSERT INTO users (user_name, password ) VALUES ( 1,1) IF NOT EXISTS;
Nodetool flush on machine A and B
Bring up the down machine B 
consistency SERIAL;
tracing on;
wait 80 seconds
Bring up machine C
select * from users where user_name = 1;
Wait 40 seconds 
select * from users where user_name = 1;  //All queries from this point forward will timeout. 

One of the potential fixes could be to set the TTL based on the remaining time left on another replicas. This will be TTL-timestamp of write. This timestamp is calculated from ballot which uses server time.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2016-06-21 03:04:33,2
12980886,dtest failure in batch_test.TestBatch.logged_batch_compatibility_3_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/252/testReport/batch_test/TestBatch/logged_batch_compatibility_3_test

Failed on CassCI build cassandra-3.0_novnode_dtest #252",dtest,[],CASSANDRA,Improvement,Normal,2016-06-20 15:08:10,5
12980883,dtest failure in repair_tests.repair_test.TestRepair.repair_after_upgrade_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/257/testReport/repair_tests.repair_test/TestRepair/repair_after_upgrade_test

Failed on CassCI build cassandra-3.0_dtest_win32 #257",dtest,[],CASSANDRA,Improvement,Normal,2016-06-20 14:59:23,5
12980230,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_copy_to_with_child_process_crashing,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/360/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_copy_to_with_child_process_crashing

Failed on CassCI build cassandra-2.1_offheap_dtest #360

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 889, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2701, in test_copy_to_with_child_process_crashing
    self.assertIn('some records might be missing', err)
  File ""/usr/lib/python2.7/unittest/case.py"", line 803, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)

Error Message

'some records might be missing' not found in ''
{code}

Logs are attached.",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-06-17 15:57:10,3
12978834,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_jdk8/229/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test

Failed on CassCI build cassandra-2.1_dtest_jdk8 #229

Logs are attached.

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 226, in sstable_repairedset_test
    self.assertNotIn('Repaired at: 0', finaloutput)
  File ""/usr/lib/python2.7/unittest/case.py"", line 810, in assertNotIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""'Repaired at: 0' unexpectedly found in 'No such file: /mnt/tmp/dtest-DoN5MO/test/node1/data0/keyspace1/standard1-0d92e6402f7c11e6bac8356bf83fc3ce/keyspace1-standard1-ka-81-Data.db
{code}",dtest,[],CASSANDRA,Bug,Normal,2016-06-14 15:35:56,0
12978198,dtest failure in cqlsh_tests.cqlsh_tests.TestCqlsh.test_refresh_schema_on_timeout_error,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/745/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_refresh_schema_on_timeout_error

Failed on CassCI build cassandra-3.0_dtest #745",dtest,[],CASSANDRA,Improvement,Normal,2016-06-13 15:45:43,3
12977666,"On clock skew, paxos may ""corrupt"" the node clock","W made a mistake in CASSANDRA-9649 so that a temporal clock skew on one node can ""corrupt"" other node clocks through Paxos. That wasn't intended and we should fix that. I'll attach a patch later.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2016-06-10 16:17:15,2
12976900,cqlsh copyutil should get host metadata by connected address,"pylib.copyutil presently accesses cluster metadata using {{shell.hostname}} which could be an unresolved hostname.
https://github.com/apache/cassandra/blob/58d3b9a90461806d44dd85bf4aa928e575d5fb6c/pylib/cqlshlib/copyutil.py#L207

Cluster metadata normally refers to hosts in terms of numeric host address, not hostname. This works in the current integration because the driver allows hosts with unresolved names into metadata during the initial control connection. In a future version of the driver, that anomaly is removed, and no duplicate hosts-by-name are present in the metadata.

We will need to update copyutil to refer to hosts by address when accessing metadata. This can be accomplished by one of two methods presently:

# shell.conn.control_connection.host (gives the current connected host address)
# scan metadata.all_hosts() for the one that {{is_up}} and use host.address/host.datacenter",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-06-08 15:24:20,3
12976694,dtest failure in json_tools_test.TestJson.json_tools_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/252/testReport/json_tools_test/TestJson/json_tools_test

Failed on CassCI build cassandra-2.2_dtest_win32 #252",dtest windows,[],CASSANDRA,Improvement,Normal,2016-06-08 03:10:18,5
12975533,dtest failure in user_functions_test.TestUserFunctions.test_migration,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/234/testReport/user_functions_test/TestUserFunctions/test_migration

Failed on CassCI build trunk_offheap_dtest #234

Logs are attached.

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/user_functions_test.py"", line 50, in test_migration
    session2.execute(""use ks"")
  File ""cassandra/cluster.py"", line 1706, in cassandra.cluster.Session.execute (cassandra/cluster.c:28532)
    return self.execute_async(query, parameters, trace, custom_payload, timeout).result()
  File ""cassandra/cluster.py"", line 3339, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:62978)
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-1nvamN\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   \'enable_scripted_user_defined_functions\': \'true\',\n    \'enable_user_defined_functions\': \'true\',\n    \'initial_token\': None,\n    \'memtable_allocation_type\': \'offheap_objects\',\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5}\n--------------------- >> end captured logging << ---------------------'
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-06-03 16:35:33,4
12975530,dtest failure in consistency_test.TestAvailability.test_network_topology_strategy_each_quorum,"example failure:

http://cassci.datastax.com/job/trunk_large_dtest/10/testReport/consistency_test/TestAvailability/test_network_topology_strategy_each_quorum

Failed on CassCI build trunk_large_dtest #10

Logs are attached.

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 719, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')

Standard Output

Unexpected error in node3 log, error: 
ERROR [SharedPool-Worker-1] 2016-06-03 14:25:27,460 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-2] 2016-06-03 14:25:27,460 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-3] 2016-06-03 14:25:27,462 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-2] 2016-06-03 14:25:27,464 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-3] 2016-06-03 14:25:27,464 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-1] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-4] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-5] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-7] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-6] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
{code}",dtest,[],CASSANDRA,Bug,Normal,2016-06-03 16:13:30,3
12974235,dtest failure in secondary_indexes_test.TestSecondaryIndexes.test_manual_rebuild_index,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1245/testReport/secondary_indexes_test/TestSecondaryIndexes/test_manual_rebuild_index

Failed on CassCI build trunk_dtest #1245

Logs are attached.

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/secondary_indexes_test.py"", line 305, in test_manual_rebuild_index
    self.assertEqual(1, len(list(session.execute(stmt, [lookup_value]))))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""1 != 0\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-HSQXU3\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,['Feature/2i Index'],CASSANDRA,Improvement,Normal,2016-05-31 14:29:53,5
12973436,dtest failure in cql_tests.AbortedQueriesTester.remote_query_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1243/testReport/cql_tests/AbortedQueriesTester/remote_query_test

Failed on CassCI build trunk_dtest #1243

{code}
ERROR [SharedPool-Worker-1] 2016-05-27 10:08:49,471 Keyspace.java:504 - Attempting to mutate non-existant table 01855840-23f3-11e6-912e-c5dc3b68cc6d (ks.test2)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-05-27 15:21:37,5
12973105,dtest failure in cql_tests.AbortedQueriesTester.materialized_view_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1236/testReport/cql_tests/AbortedQueriesTester/materialized_view_test

Failed on CassCI build trunk_dtest #1236",dtest,[],CASSANDRA,Improvement,Normal,2016-05-26 15:42:50,5
12973085,dtest failure in sstable_generation_loading_test.TestSSTableGenerationAndLoading.sstableloader_compression_snappy_to_none_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/243/testReport/sstable_generation_loading_test/TestSSTableGenerationAndLoading/sstableloader_compression_snappy_to_none_test

Failed on CassCI build cassandra-3.0_dtest_win32 #243

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/243/testReport/sstable_generation_loading_test/TestSSTableGenerationAndLoading/sstableloader_compression_none_to_deflate_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/243/testReport/sstable_generation_loading_test/TestSSTableGenerationAndLoading/sstableloader_compression_none_to_deflate_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/243/testReport/sstable_generation_loading_test/TestSSTableGenerationAndLoading/sstableloader_compression_none_to_none_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/243/testReport/sstable_generation_loading_test/TestSSTableGenerationAndLoading/sstableloader_compression_snappy_to_deflate_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/243/testReport/sstable_generation_loading_test/TestSSTableGenerationAndLoading/sstableloader_compression_snappy_to_deflate_test/

",dtest windows,[],CASSANDRA,Improvement,Normal,2016-05-26 14:40:14,5
12972422,Streaming will miss sections for early opened sstables during compaction,"Once validation compaction has been finished, all mismatching sstable sections for a token range will be used for streaming as return by {{StreamSession.getSSTableSectionsForRanges}}. Currently 2.1 will try to restrict the sstable candidates by checking if they can be found in {{CANONICAL_SSTABLES}} and will ignore them otherwise. At the same time {{IntervalTree}} in the {{DataTracker}} will be build based on replaced non-canonical sstables as well. In case of early opened sstables this becomes a problem, as the tree will be update with {{OpenReason.EARLY}} replacements that cannot be found in canonical. But whenever {{getSSTableSectionsForRanges}} will get a early instance from the view, it will fail to retrieve the corresponding canonical version from the map, as the different generation will cause a hashcode mismatch. Please find a test attached.

As a consequence not all sections for a range are streamed. In our case this has caused deleted data to reappear, as sections holding tombstones were left out due to this behavior.",correctness repair streaming,[],CASSANDRA,Bug,Urgent,2016-05-24 15:48:03,0
12972142,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x.select_key_in_test,"example failure:

http://cassci.datastax.com/job/upgrade_tests-all/47/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x/select_key_in_test

Failed on CassCI build upgrade_tests-all #47

Attached logs for test failure.

{code}
ERROR [CompactionExecutor:2] 2016-05-21 23:10:35,678 CassandraDaemon.java:195 - Exception in thread Thread[CompactionExecutor:2,1,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.5.jar:3.5]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1364) ~[na:1.8.0_51]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.5.jar:3.5]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_51]
	at org.apache.cassandra.db.compaction.CompactionManager.submitBackground(CompactionManager.java:184) ~[apache-cassandra-3.5.jar:3.5]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:270) ~[apache-cassandra-3.5.jar:3.5]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}",dtest,[],CASSANDRA,Bug,Normal,2016-05-23 18:24:33,0
12971376,cannot use cql since upgrading python to 2.7.11+,"OS: Debian GNU/Linux stretch/sid 
Kernel: 4.5.0-2-amd64 #1 SMP Debian 4.5.4-1 (2016-05-16) x86_64 GNU/Linux
Python version: 2.7.11+ (default, May  9 2016, 15:54:33)
[GCC 5.3.1 20160429]

cqlsh --version: cqlsh 5.0.1
cassandra -v: 3.5 (also occurs with 3.0.6)

Issue:
when running cqlsh, it returns the following error:

cqlsh -u dbarpt_usr01
Password: *****

Connection error: ('Unable to connect to any servers', {'odbasandbox1': TypeError('ref() does not take keyword arguments',)})

I cleared PYTHONPATH:

python -c ""import json; print dir(json); print json.__version__""
['JSONDecoder', 'JSONEncoder', '__all__', '__author__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', '__version__', '_default_decoder', '_default_encoder', 'decoder', 'dump', 'dumps', 'encoder', 'load', 'loads', 'scanner']
2.0.9

Java based clients can connect to Cassandra with no issue. Just CQLSH and Python clients cannot.

nodetool status also works.

Thank you for your help.


",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-05-20 02:03:27,3
12970865,dtest failure in topology_test.TestTopology.simple_decommission_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1223/testReport/topology_test/TestTopology/simple_decommission_test

Failed on CassCI build trunk_dtest #1223

The problem is that node3 detected node2 as down before the stop call was made, so the wait_other_notice check fails. The fix here is almost certainly as simple as just changing that line to {{node2.stop()}}",dtest,[],CASSANDRA,Improvement,Normal,2016-05-18 18:10:51,4
12970593,If repair fails no way to run repair again,"I have a test that disables gossip and runs repair at the same time. 

{quote}
WARN  [RMI TCP Connection(15)-54.67.121.105] 2016-05-17 16:57:21,775 StorageService.java:384 - Stopping gossip by operator request
INFO  [RMI TCP Connection(15)-54.67.121.105] 2016-05-17 16:57:21,775 Gossiper.java:1463 - Announcing shutdown
INFO  [RMI TCP Connection(15)-54.67.121.105] 2016-05-17 16:57:21,776 StorageService.java:1999 - Node /172.31.31.1 state jump to shutdown
INFO  [HANDSHAKE-/172.31.17.32] 2016-05-17 16:57:21,895 OutboundTcpConnection.java:514 - Handshaking version with /172.31.17.32
INFO  [HANDSHAKE-/172.31.24.76] 2016-05-17 16:57:21,895 OutboundTcpConnection.java:514 - Handshaking version with /172.31.24.76
INFO  [Thread-25] 2016-05-17 16:57:21,925 RepairRunnable.java:125 - Starting repair command #1, repairing keyspace keyspace1 with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 3)
INFO  [Thread-26] 2016-05-17 16:57:21,953 RepairRunnable.java:125 - Starting repair command #2, repairing keyspace stresscql with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 3)
INFO  [Thread-27] 2016-05-17 16:57:21,967 RepairRunnable.java:125 - Starting repair command #3, repairing keyspace system_traces with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 2)
{quote}

This ends up failing:

{quote}
16:54:44.844 INFO  serverGroup-node-1-574 - STDOUT: [2016-05-17 16:57:21,933] Starting repair command #1, repairing keyspace keyspace1 with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 3)
[2016-05-17 16:57:21,943] Did not get positive replies from all endpoints. List of failed endpoint(s): [172.31.24.76, 172.31.17.32]
[2016-05-17 16:57:21,945] null
{quote}

Subsequent calls to repair with all nodes up still fails:

{quote}
ERROR [ValidationExecutor:3] 2016-05-17 18:58:53,460 CompactionManager.java:1193 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:3] 2016-05-17 18:58:53,460 Validator.java:261 - Failed creating a merkle tree for [repair #66425f10-1c61-11e6-83b2-0b1fff7a067d on keyspace1/standard1, 
{quote}",fallout,[],CASSANDRA,Bug,Normal,2016-05-18 01:13:55,0
12970447,dtest failure in replication_test.SnitchConfigurationUpdateTest.test_rf_expand_gossiping_property_file_snitch_multi_dc,"Test is failing on trunk. Example failure is here:

http://cassci.datastax.com/job/trunk_novnode_dtest/378/testReport/replication_test/SnitchConfigurationUpdateTest/test_rf_expand_gossiping_property_file_snitch_multi_dc/

It appears to be running out of memory to allocate, inside the dtest python code, when starting nodetool. This may need moved to large test, but it's odd that it just started happening

{code}
[node3 ERROR] Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f25b9fd0000, 65536, 1) failed; error='Cannot allocate memory' (errno=12)
[node2 ERROR] Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f591e0c0000, 65536, 1) failed; error='Cannot allocate memory' (errno=12)
[node5 ERROR] Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fabca130000, 65536, 1) failed; error='Cannot allocate memory' (errno=12)
[node6 ERROR] Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fd0b2080000, 65536, 1) failed; error='Cannot allocate memory' (errno=12)
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-05-17 17:36:58,5
12969234,dtest failure in upgrade_tests.upgrade_through_versions_test.ProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD.rolling_upgrade_test,"No cassci link, as jenkins is failing to show failures for this test, but I'm seeing detected leaks.

Relevant log section:
{code}
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,616 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@41f74411) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@1328587359:/mnt/tmp/dtest-X5kTWw/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/la-12-big-Data.db was not released before the reference was garbage collected
DEBUG [Reference-Reaper:1] 2016-05-11 16:17:47,617 FileCacheService.java:177 - Invalidating cache for /mnt/tmp/dtest-X5kTWw/test/node1/data2/system/local-7ad54392bcdd35a684174e047860b377/tmplink-la-13-big-Data.db
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4b0f2f61) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@235776100:/mnt/tmp/dtest-X5kTWw/test/node1/data2/system/local-7ad54392bcdd35a684174e047860b377/tmplink-la-13-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@75a1b278) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1061799113:[Memory@[0..8), Memory@[0..50)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@d0f8066) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1847744299:/mnt/tmp/dtest-X5kTWw/test/node1/data2/system/local-7ad54392bcdd35a684174e047860b377/tmplink-la-13-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@16680ae5) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1744239469:/mnt/tmp/dtest-X5kTWw/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/la-12-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,618 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@160b4b45) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1785515424:[[OffHeapBitSet]] was not released before the reference was garbage collected
{code}

Logs are attached. node1 is the one experiencing the issue.",dtest,[],CASSANDRA,Bug,Low,2016-05-12 17:17:50,3
12968662,dtest failure in upgrade_tests.upgrade_through_versions_test.ProtoV3Upgrade_AllVersions_Skips_3_0_x_EndsAt_Trunk_HEAD.rolling_upgrade_test,"Example failure:

http://cassci.datastax.com/view/Parameterized/job/upgrade_tests-all-custom_branch_runs/12/testReport/upgrade_tests.upgrade_through_versions_test/ProtoV3Upgrade_AllVersions_Skips_3_0_x_EndsAt_Trunk_HEAD/rolling_upgrade_test_2/

The test is seeing a corrupt hint sstable after upgrade from 2.2.5 to 3.6. Relevant stack trace is

{code}
ERROR [main] 2016-05-11 16:22:25,180 CassandraDaemon.java:727 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at org.apache.cassandra.hints.LegacyHintsMigrator.forceCompaction(LegacyHintsMigrator.java:119) ~[main/:na]
	at org.apache.cassandra.hints.LegacyHintsMigrator.compactLegacyHints(LegacyHintsMigrator.java:108) ~[main/:na]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrate(LegacyHintsMigrator.java:92) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:321) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:581) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:710) [main/:na]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_51]
	at org.apache.cassandra.hints.LegacyHintsMigrator.forceCompaction(LegacyHintsMigrator.java:115) ~[main/:na]
	... 5 common frames omitted
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:351) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:265) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner.hasNext(BigTableScanner.java:245) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:463) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:150) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:72) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:182) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:82) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:805) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_51]
Caused by: java.io.EOFException: null
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:68) ~[main/:na]
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:60) ~[main/:na]
	at org.apache.cassandra.io.util.TrackedDataInputPlus.readFully(TrackedDataInputPlus.java:93) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:400) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:375) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:109) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:89) ~[main/:na]
	at org.apache.cassandra.io.sstable.IndexInfo$Serializer.deserialize(IndexInfo.java:135) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:651) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:577) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$LegacyShallowIndexedEntry.deserialize(RowIndexEntry.java:508) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$Serializer.deserialize(RowIndexEntry.java:321) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:310) ~[main/:na]
	... 19 common frames omitted
ERROR [CompactionExecutor:2] 2016-05-11 16:22:25,183 CassandraDaemon.java:213 - Exception in thread Thread[CompactionExecutor:2,1,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:351) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:265) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner.hasNext(BigTableScanner.java:245) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:463) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:150) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:72) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:182) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:82) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:805) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
Caused by: java.io.EOFException: null
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:68) ~[main/:na]
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:60) ~[main/:na]
	at org.apache.cassandra.io.util.TrackedDataInputPlus.readFully(TrackedDataInputPlus.java:93) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:400) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:375) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:109) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:89) ~[main/:na]
	at org.apache.cassandra.io.sstable.IndexInfo$Serializer.deserialize(IndexInfo.java:135) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:651) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:577) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$LegacyShallowIndexedEntry.deserialize(RowIndexEntry.java:508) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$Serializer.deserialize(RowIndexEntry.java:321) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:310) ~[main/:na]
	... 19 common frames omitted
{code}

Logs are attached",dtest,['Local/Compaction'],CASSANDRA,Bug,Normal,2016-05-12 15:07:43,3
12967925,cqlsh show sessions truncates time_elapsed values > 999999,"Output from show session in cqlsh:
{quote}
Submit hint for /10.255.227.20 [EXPIRING-MAP-REAPER:1] | 2016-05-11 15:57:53.730000 | 10.255.226.163 |         283246
{quote}
Output from select * from trace_events where session_id=(same as above):
{quote}
 1bbce5c0-1791-11e6-9598-3b9ec975a2e6 | 1ee37a20-1791-11e6-9598-3b9ec975a2e6 |                         Submit hint for /10.255.227.20 | 10.255.226.163 |        5283246 |                     EXPIRING-MAP-REAPER:1
{quote}
Notice that the 5 (seconds) part is being truncated in the output.
",lhf,"['Legacy/CQL', 'Legacy/Observability', 'Legacy/Testing', 'Legacy/Tools']",CASSANDRA,Bug,Normal,2016-05-11 20:28:55,3
12965767,dtest failure in pushed_notifications_test.TestPushedNotifications.move_single_node_test,"one recent failure (no vnode job)

{noformat}
'MOVED_NODE' != u'NEW_NODE'
{noformat}

http://cassci.datastax.com/job/trunk_novnode_dtest/366/testReport/pushed_notifications_test/TestPushedNotifications/move_single_node_test

Failed on CassCI build trunk_novnode_dtest #366",dtest,[],CASSANDRA,Improvement,Normal,2016-05-06 23:01:26,4
12965715,Incremental repair fails with vnodes+lcs+multi-dc,"Produced on 2.1.12

We are seeing incremental repair fail with an error regarding creating multiple repair sessions on overlapping sstables. This is happening in the following setup

* 6 nodes
* 2 Datacenters
* Vnodes enabled
* Leveled compaction on the relevant tables

When STCS is used instead, we don't hit an issue. This is slightly related to https://issues.apache.org/jira/browse/CASSANDRA-11461, except in this case OpsCenter repair service is running all repairs sequentially. Let me know what other information we can provide. ",lcs,[],CASSANDRA,Bug,Normal,2016-05-06 20:20:03,0
12964485,dtest failure in pushed_notifications_test.TestPushedNotifications.restart_node_localhost_test,"from offheap test job, failure. looks like it could be a routine timeout, but I think I saw the exact same timeout for this test on another job.

{noformat}
('Unable to connect to any servers', {})
{noformat}
http://cassci.datastax.com/job/trunk_offheap_dtest/178/testReport/pushed_notifications_test/TestPushedNotifications/restart_node_localhost_test

Failed on CassCI build trunk_offheap_dtest #178",dtest,[],CASSANDRA,Improvement,Normal,2016-05-02 23:52:08,4
12964484,[windows] dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_reading_with_skip_and_max_rows,"looks to be an assertion problem, so could be test or cassandra related:

e.g.:
{noformat}
10000 != 331
{noformat}

http://cassci.datastax.com/job/trunk_dtest_win32/404/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_reading_with_skip_and_max_rows

Failed on CassCI build trunk_dtest_win32 #404",dtest windows,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2016-05-02 23:47:00,3
12964481,dtest failure in upgrade_crc_check_chance_test.TestCrcCheckChanceUpgrade.crc_check_chance_upgrade_test,"single failure, unsure if test or cassandra issue:
{noformat}
ERROR [BatchlogTasks:1] 2016-05-02 20:23:08,488 CassandraDaemon.java:195 - Exception in thread Thread[BatchlogTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:122) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.utils.JVMStabilityInspector
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_45]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_45]
	... 8 common frames omitted
ERROR [BatchlogTasks:1] 2016-05-02 20:23:08,490 CassandraDaemon.java:195 - Exception in thread Thread[BatchlogTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:199) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.handleOrLog(DebuggableThreadPoolExecutor.java:244) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logExceptionsAfterExecute(DebuggableThreadPoolExecutor.java:227) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:89) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1150) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.utils.JVMStabilityInspector
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_45]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_45]
	... 7 common frames omitted
ERROR [OptionalTasks:1] 2016-05-02 20:23:08,577 CassandraDaemon.java:195 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:122) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
ERROR [OptionalTasks:1] 2016-05-02 20:23:08,588 CassandraDaemon.java:195 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:199) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.handleOrLog(DebuggableThreadPoolExecutor.java:244) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logExceptionsAfterExecute(DebuggableThreadPoolExecutor.java:227) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:89) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1150) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
WARN  [OptionalTasks:2] 2016-05-02 20:23:08,588 CassandraRoleManager.java:344 - CassandraRoleManager skipped default role setup: some nodes were not ready
{noformat}
http://cassci.datastax.com/job/trunk_dtest/1190/testReport/upgrade_crc_check_chance_test/TestCrcCheckChanceUpgrade/crc_check_chance_upgrade_test

Failed on CassCI build trunk_dtest #1190",dtest,[],CASSANDRA,Improvement,Normal,2016-05-02 23:39:41,4
12963999,dtest failure in sstableutil_test.SSTableUtilTest.abortedcompaction_test,"example failure:

{noformat}
Lists differ: ['/mnt/tmp/dtest-hXZ_VA/test/n... != ['/mnt/tmp/dtest-hXZ_VA/test/n...

First differing element 16:
/mnt/tmp/dtest-hXZ_VA/test/node1/data2/keyspace1/standard1-483ee2700d5911e6b19a879d803a6aae/ma-3-big-CRC.db
/mnt/tmp/dtest-hXZ_VA/test/node1/data2/keyspace1/standard1-483ee2700d5911e6b19a879d803a6aae/ma-5-big-CRC.db

Diff is 5376 characters long. Set self.maxDiff to None to see it.
{noformat}

http://cassci.datastax.com/job/trunk_novnode_dtest/360/testReport/sstableutil_test/SSTableUtilTest/abortedcompaction_test

Failed on CassCI build trunk_novnode_dtest #360",dtest,[],CASSANDRA,Improvement,Normal,2016-04-29 22:21:55,3
12963264,multiple dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest,"these appear to be related, all failed on the same build (but appear to be passing now).

http://cassci.datastax.com/job/trunk_dtest/1165/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_copy_from_with_brackets_in_UDT/

http://cassci.datastax.com/job/trunk_dtest/1165/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_undefined_as_null_indicator/

http://cassci.datastax.com/job/trunk_dtest/1165/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_round_trip_with_sub_second_precision/

http://cassci.datastax.com/job/trunk_dtest/1165/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_null_as_null_indicator/

http://cassci.datastax.com/job/trunk_dtest/1165/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_default_null_indicator/

http://cassci.datastax.com/job/trunk_dtest/1165/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_all_datatypes_write/",dtest,[],CASSANDRA,Improvement,Normal,2016-04-27 18:57:45,5
12963234,dtest failure in materialized_views_test.TestMaterializedViews.clustering_column_test,"single failure, but might be worth looking into to see if it repros at all.

http://cassci.datastax.com/job/cassandra-3.0_dtest/669/testReport/materialized_views_test/TestMaterializedViews/clustering_column_test

Failed on CassCI build cassandra-3.0_dtest #669",dtest,[],CASSANDRA,Improvement,Low,2016-04-27 18:23:14,4
12963227,(2.1) dtest failure in bootstrap_test.TestBootstrap.test_cleanup,"This test was originally waiting on CASSANDRA-11179, which I recently removed the 'require' annotation from (since 11179 is committed). Not sure why failing on 2.1 now, perhaps didn't get committed.

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/339/testReport/bootstrap_test/TestBootstrap/test_cleanup

Failed on CassCI build cassandra-2.1_offheap_dtest #339",dtest,[],CASSANDRA,Improvement,Normal,2016-04-27 18:01:15,0
12962805,dtest failure in pushed_notifications_test.TestPushedNotifications.move_single_node_test,"single test flap, so could be a fluke. happened on the trunk no-vnode test:

http://cassci.datastax.com/job/trunk_novnode_dtest/354/testReport/pushed_notifications_test/TestPushedNotifications/move_single_node_test

Failed on CassCI build trunk_novnode_dtest #354",dtest,[],CASSANDRA,Improvement,Normal,2016-04-26 23:13:30,4
12962790,dtest failure in topology_test.TestTopology.movement_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/353/testReport/topology_test/TestTopology/movement_test

Failed on CassCI build trunk_novnode_dtest #353",dtest,[],CASSANDRA,Improvement,Normal,2016-04-26 22:44:00,5
12962775,dtest failure in topology_test.TestTopology.decommissioned_node_cant_rejoin_test,"intermittent failure, example failure:

failed on trunk no-vnodes job

""True is not false""

http://cassci.datastax.com/job/trunk_novnode_dtest/351/testReport/topology_test/TestTopology/decommissioned_node_cant_rejoin_test

Failed on CassCI build trunk_novnode_dtest #351",dtest,[],CASSANDRA,Improvement,Normal,2016-04-26 22:31:48,4
12962510,LongLeveledCompactionStrategyTest broken since CASSANDRA-10099,Seems CASSANDRA-10099 broke LongLeveledCompactionStrategyTest,lcs,['Legacy/Testing'],CASSANDRA,Bug,Normal,2016-04-26 07:38:22,0
12962423,dtest failure in sstableutil_test.SSTableUtilTest.abortedcompaction_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/660/testReport/sstableutil_test/SSTableUtilTest/abortedcompaction_test

Failed on CassCI build cassandra-3.0_dtest #660

Looks likely to be a test problem, with error message ""0 not greater than 0""",dtest,[],CASSANDRA,Improvement,Normal,2016-04-25 23:30:26,4
12962387,dtest failure in json_test.ToJsonSelectTests.basic_data_types_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/585/testReport/json_test/ToJsonSelectTests/basic_data_types_test

Failed on CassCI build cassandra-2.2_dtest #585",dtest,[],CASSANDRA,Improvement,Normal,2016-04-25 20:38:56,5
12962386,dtest failure in json_test.ToJsonSelectTests.complex_data_types_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/585/testReport/json_test/ToJsonSelectTests/complex_data_types_test

Failed on CassCI build cassandra-2.2_dtest #585",dtest,[],CASSANDRA,Improvement,Normal,2016-04-25 20:38:48,5
12962385,dtest failure in json_test.JsonFullRowInsertSelect.simple_schema_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/585/testReport/json_test/JsonFullRowInsertSelect/simple_schema_test

Failed on CassCI build cassandra-2.2_dtest #585",dtest,[],CASSANDRA,Improvement,Normal,2016-04-25 20:38:42,5
12962384,dtest failure in json_test.FromJsonInsertTests.basic_data_types_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/585/testReport/json_test/FromJsonInsertTests/basic_data_types_test

Failed on CassCI build cassandra-2.2_dtest #585",dtest,[],CASSANDRA,Improvement,Normal,2016-04-25 20:38:35,5
12961585,cqlsh COPY FROM fails for null values with non-prepared statements,"cqlsh's {{COPY FROM ... WITH PREPAREDSTATEMENTS = False}} fails if the row contains null values. Reason is that the {{','.join(r)}} in {{make_non_prepared_batch_statement}} doesn't seem to handle {{None}}, which results in this error message.
{code}
Failed to import 1 rows: TypeError - sequence item 2: expected string, NoneType found,  given up without retries
{code}

Attached patch should fix the problem.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-04-22 09:27:40,3
12960321,"Removing an element from map<any type, tinyint/smallint> corrupts commitlog","2.2.6 has no this bug.
I've tried 3.0 alpha 1, 3.0 beta 1, 3.0 beta 2, 3.0.0, 3.0.6, 3.5, datastax-ddc 3.5.0 (from repo), and trunk (3.6) - all of them have this bug. 
I've found that the error is thrown since d12d2d496540c698f30e9b528b66e8f6636842d3, which is included in 3.0 beta 1 (but *not* in the alpha 1).
Cassandra 3.0 alpha 1 does not throw the error, but forgets about the changes after shutting down.


Only after rm ./data/commitlog/* , Cassandra starts fine.
By the way, map<int, boolean> works fine.

Steps to reproduce:
{code}
$ ant build
$ ./bin/cassandra
$ ./bin/cqlsh
{code}
{code:sql}
CREATE KEYSPACE bugs
    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}
    AND durable_writes = true;

CREATE TABLE bugs.bug1 (
    id int,
    m  map<int, tinyint or smallint>, -- key can be any type
    PRIMARY KEY (id)
);

INSERT INTO bugs.bug1 (id, m) VALUES (1, {0: 4, 4: 3});

UPDATE bugs.bug1 SET m[0]=NULL WHERE id=1;
-- and/or UPDATE bugs.bug1 SET m[1]=NULL WHERE id=1;

SELECT * FROM bugs.bug1;
{code}
{code}
 id | m
----+--------
  1 | {4: 3}

(1 rows)
{code}
{code}
$ ./bin/nodetool stopdaemon
$ ./bin/cassandra
{code}",commitlog regression serializers,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Urgent,2016-04-20 12:35:36,2
12960065,dtest failure in replace_address_test.TestReplaceAddress.replace_first_boot_test,"This looks like a timeout kind of flap. It's flapped once. Example failure:

http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/344/testReport/replace_address_test/TestReplaceAddress/replace_first_boot_test

Failed on CassCI build cassandra-2.2_offheap_dtest #344 - 2.2.6-tentative

{code}
Error Message

15 Apr 2016 16:23:41 [node3] Missing: ['127.0.0.4.* now UP']:
INFO  [main] 2016-04-15 16:21:32,345 Config.java:4.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4i5qkE
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
dtest: DEBUG: Starting cluster with 3 nodes.
dtest: DEBUG: 32
dtest: DEBUG: Inserting Data...
dtest: DEBUG: Stopping node 3.
dtest: DEBUG: Testing node stoppage (query should fail).
dtest: DEBUG: Retrying read after timeout. Attempt #0
dtest: DEBUG: Retrying read after timeout. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Starting node 4 to replace node 3
dtest: DEBUG: Verifying querying works again.
dtest: DEBUG: Verifying tokens migrated sucessfully
dtest: DEBUG: ('WARN  [main] 2016-04-15 16:21:21,068 TokenMetadata.java:196 - Token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4\n', <_sre.SRE_Match object at 0x7fd21c0e2370>)
dtest: DEBUG: Try to restart node 3 (should fail)
dtest: DEBUG: [('WARN  [GossipStage:1] 2016-04-15 16:21:22,942 StorageService.java:1962 - Host ID collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner\n', <_sre.SRE_Match object at 0x7fd1f83555e0>)]
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 212, in replace_first_boot_test
    node4.start(wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 610, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 457, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 425, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""15 Apr 2016 16:23:41 [node3] Missing: ['127.0.0.4.* now UP']:\nINFO  [main] 2016-04-15 16:21:32,345 Config.java:4.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4i5qkE\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: Starting cluster with 3 nodes.\ndtest: DEBUG: 32\ndtest: DEBUG: Inserting Data...\ndtest: DEBUG: Stopping node 3.\ndtest: DEBUG: Testing node stoppage (query should fail).\ndtest: DEBUG: Retrying read after timeout. Attempt #0\ndtest: DEBUG: Retrying read after timeout. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Starting node 4 to replace node 3\ndtest: DEBUG: Verifying querying works again.\ndtest: DEBUG: Verifying tokens migrated sucessfully\ndtest: DEBUG: ('WARN  [main] 2016-04-15 16:21:21,068 TokenMetadata.java:196 - Token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4\\n', <_sre.SRE_Match object at 0x7fd21c0e2370>)\ndtest: DEBUG: Try to restart node 3 (should fail)\ndtest: DEBUG: [('WARN  [GossipStage:1] 2016-04-15 16:21:22,942 StorageService.java:1962 - Host ID collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner\\n', <_sre.SRE_Match object at 0x7fd1f83555e0>)]\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-04-19 17:38:22,4
12960056,dtest failure in user_types_test.TestUserTypes.test_nested_user_types,"This is a single flap:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/217/testReport/user_types_test/TestUserTypes/test_nested_user_types

Failed on CassCI build cassandra-2.2_dtest_win32 #217

{code}
Error Message

Lists differ: [None] != [[u'test', u'test2']]

First differing element 0:
None
[u'test', u'test2']

- [None]
+ [[u'test', u'test2']]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: d:\temp\dtest-vgkgwi
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\user_types_test.py"", line 289, in test_nested_user_types
    self.assertEqual(listify(primary_item), [[u'test', u'test2']])
  File ""C:\tools\python2\lib\unittest\case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""C:\tools\python2\lib\unittest\case.py"", line 742, in assertListEqual
    self.assertSequenceEqual(list1, list2, msg, seq_type=list)
  File ""C:\tools\python2\lib\unittest\case.py"", line 724, in assertSequenceEqual
    self.fail(msg)
  File ""C:\tools\python2\lib\unittest\case.py"", line 410, in fail
    raise self.failureException(msg)
""Lists differ: [None] != [[u'test', u'test2']]\n\nFirst differing element 0:\nNone\n[u'test', u'test2']\n\n- [None]\n+ [[u'test', u'test2']]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-vgkgwi\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
Standard Error

Started: node1 with pid: 4328
Started: node3 with pid: 7568
Started: node2 with pid: 7504
{code}",dtest windows,[],CASSANDRA,Improvement,Normal,2016-04-19 17:10:30,4
12958863,clqsh: COPY FROM throws TypeError with Cython extensions enabled,"Any COPY FROM command in cqlsh is throwing the following error:

""get_num_processes() takes no keyword arguments""

Example command: 

COPY inboxdata (to_user_id,to_user_network,created_time,attachments,from_user_id,from_user_name,from_user_network,id,message,to_user_name,updated_time) FROM 'inbox.csv';

Similar commands worked parfectly in the previous versions such as 3.0.4",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-04-14 14:34:10,3
12958509,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_non_prepared_statements,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/329/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_non_prepared_statements

Failed on CassCI build cassandra-2.1_offheap_dtest #329

{noformat}
Error Message

'int' object has no attribute 'on_read_timeout'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-LNfFyy
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Running stress without any user profile
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2325, in test_bulk_round_trip_non_prepared_statements
    copy_from_options={'PREPAREDSTATEMENTS': False})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2283, in _test_bulk_round_trip
    num_records = create_records()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2258, in create_records
    ret = rows_to_list(self.session.execute(count_statement))[0][0]
  File ""cassandra/cluster.py"", line 1581, in cassandra.cluster.Session.execute (cassandra/cluster.c:27046)
    return self.execute_async(query, parameters, trace, custom_payload, timeout).result()
  File ""cassandra/cluster.py"", line 3145, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:59905)
    raise self._final_exception
""'int' object has no attribute 'on_read_timeout'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-LNfFyy\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Running stress without any user profile\n--------------------- >> end captured logging << ---------------------""
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-04-13 15:44:51,4
12958506,dtest failure in secondary_indexes_test.TestSecondaryIndexes.test_query_indexes_with_vnodes,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/344/testReport/secondary_indexes_test/TestSecondaryIndexes/test_query_indexes_with_vnodes

Failed on CassCI build trunk_novnode_dtest #344

Test does not appear to configure single-token cluster correctly:
{noformat}
Error Message

Error starting node1.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4pEIhy
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/secondary_indexes_test.py"", line 436, in test_query_indexes_with_vnodes
    cluster.populate(2, use_vnodes=True).start()
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 360, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4pEIhy\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
Standard Output

[node1 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node2 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
{noformat}",dtest,['Feature/2i Index'],CASSANDRA,Improvement,Normal,2016-04-13 15:35:27,4
12958505,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test_not_intersecting_all_ranges,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/344/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test_not_intersecting_all_ranges

Failed on CassCI build trunk_novnode_dtest #344

Test does not appear to deal with single-token cluster testing correctly:
{noformat}
Error Message

Error starting node1.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-I164Fa
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None, 'phi_convict_threshold': 5, 'start_rpc': 'true'}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 369, in sstable_marking_test_not_intersecting_all_ranges
    cluster.populate(4, use_vnodes=True).start()
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 360, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-I164Fa\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None, 'phi_convict_threshold': 5, 'start_rpc': 'true'}\n--------------------- >> end captured logging << ---------------------""
Standard Output

[node1 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node3 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node2 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node4 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-04-13 15:33:49,4
12957218,cqlsh fails to format collections when using aliases,"Given is a simple table. Selecting the columns without an alias works fine. However, if the map is selected using an alias, cqlsh fails to format the value.

{code}
create keyspace foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE foo.foo (id int primary key, m map<int, text>);
insert into foo.foo (id, m) VALUES ( 1, {1: 'one', 2: 'two', 3:'three'});
insert into foo.foo (id, m) VALUES ( 2, {1: '1one', 2: '2two', 3:'3three'});

cqlsh> select id, m from foo.foo;

 id | m
----+-------------------------------------
  1 |    {1: 'one', 2: 'two', 3: 'three'}
  2 | {1: '1one', 2: '2two', 3: '3three'}

(2 rows)
cqlsh> select id, m as ""weofjkewopf"" from foo.foo;

 id | weofjkewopf
----+-----------------------------------------------------------------------
  1 |    OrderedMapSerializedKey([(1, u'one'), (2, u'two'), (3, u'three')])
  2 | OrderedMapSerializedKey([(1, u'1one'), (2, u'2two'), (3, u'3three')])

(2 rows)
Failed to format value OrderedMapSerializedKey([(1, u'one'), (2, u'two'), (3, u'three')]) : 'NoneType' object has no attribute 'sub_types'
Failed to format value OrderedMapSerializedKey([(1, u'1one'), (2, u'2two'), (3, u'3three')]) : 'NoneType' object has no attribute 'sub_types'
{code}
",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-04-08 14:09:24,3
12956737,Implement streaming for bulk read requests,"Allow clients to stream data from a C* host, bypassing the coordination layer and eliminating the need to query individual pages one by one.",protocolv5,['Legacy/Local Write-Read Paths'],CASSANDRA,Sub-task,Normal,2016-04-07 03:14:19,3
12956586,C* won't launch with whitespace in path on Windows,"In a directory named 'test space', I see the following on launch:

{noformat}
Error: Could not find or load main class space\cassandra.logs.gc.log
{noformat}",windows,[],CASSANDRA,Bug,Low,2016-04-06 18:16:40,6
12956245,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_reading_max_parse_errors,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/197/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_reading_max_parse_errors

Failed on CassCI build cassandra-3.0_novnode_dtest #197

{noformat}
Error Message

False is not true
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-c2AJlu
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Importing csv file /mnt/tmp/tmp2O43PH with 10 max parse errors
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 943, in test_reading_max_parse_errors
    self.assertTrue(num_rows_imported < (num_rows / 2))  # less than the maximum number of valid rows in the csv
  File ""/usr/lib/python2.7/unittest/case.py"", line 422, in assertTrue
    raise self.failureException(msg)
""False is not true\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-c2AJlu\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Importing csv file /mnt/tmp/tmp2O43PH with 10 max parse errors\n--------------------- >> end captured logging << ---------------------""
Standard Output

(EE)  Using CQL driver: <module 'cassandra' from '/home/automaton/cassandra/bin/../lib/cassandra-driver-internal-only-3.0.0-6af642d.zip/cassandra-driver-3.0.0-6af642d/cassandra/__init__.py'>(EE)  Using connect timeout: 5 seconds(EE)  Using 'utf-8' encoding(EE)  <stdin>:2:Failed to import 2500 rows: ParseError - could not convert string to float: abc,  given up without retries(EE)  <stdin>:2:Exceeded maximum number of parse errors 10(EE)  <stdin>:2:Failed to process 2500 rows; failed rows written to import_ks_testmaxparseerrors.err(EE)  <stdin>:2:Exceeded maximum number of parse errors 10(EE)  
{noformat}",dtest,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2016-04-05 19:04:45,3
12955973,dtest failure in sstableutil_test.SSTableUtilTest.abortedcompaction_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/637/testReport/sstableutil_test/SSTableUtilTest/abortedcompaction_test

Failed on CassCI build cassandra-3.0_dtest #637

Next run passed, so this could be a flaky test.

{noformat}
Error Message

0 not greater than 0
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-gbo1Uc
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: Comparing all files...
dtest: DEBUG: Comparing final files...
dtest: DEBUG: Comparing tmp files...
dtest: DEBUG: Comparing op logs...
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: Comparing all files...
dtest: DEBUG: Comparing final files...
dtest: DEBUG: Comparing tmp files...
dtest: DEBUG: Comparing op logs...
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/sstableutil_test.py"", line 79, in abortedcompaction_test
    self.assertGreater(len(tmpfiles), 0)
  File ""/usr/lib/python2.7/unittest/case.py"", line 942, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""0 not greater than 0\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-gbo1Uc\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: Comparing all files...\ndtest: DEBUG: Comparing final files...\ndtest: DEBUG: Comparing tmp files...\ndtest: DEBUG: Comparing op logs...\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: Comparing all files...\ndtest: DEBUG: Comparing final files...\ndtest: DEBUG: Comparing tmp files...\ndtest: DEBUG: Comparing op logs...\n--------------------- >> end captured logging << ---------------------""
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-04-04 23:07:58,3
12955942,dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_default,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/327/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_default

Failed on CassCI build cassandra-2.1_offheap_dtest #327

Looks like a test error in dtest.py FlakyRetryPolicy() getting in an odd state. This test runs OK for me locally.

{noformat}
Error Message

'int' object has no attribute 'on_read_timeout'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-SrTCbF
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-SrTCbF
dtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-SrTCbF] directory
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-smdNhH
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Running stress without any user profile
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 829, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2306, in test_bulk_round_trip_default
    self._test_bulk_round_trip(nodes=3, partitioner=""murmur3"", num_operations=100000)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2277, in _test_bulk_round_trip
    num_records = create_records()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2252, in create_records
    ret = rows_to_list(self.session.execute(count_statement))[0][0]
  File ""cassandra/cluster.py"", line 1581, in cassandra.cluster.Session.execute (cassandra/cluster.c:27107)
    return self.execute_async(query, parameters, trace, custom_payload, timeout).result()
  File ""cassandra/cluster.py"", line 3145, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:60227)
    raise self._final_exception
""'int' object has no attribute 'on_read_timeout'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-SrTCbF\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-SrTCbF\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-SrTCbF] directory\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-smdNhH\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Running stress without any user profile\n--------------------- >> end captured logging << ---------------------""
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-04-04 21:53:46,4
12955927,dtest failure in consistency_test.TestAccuracy.test_simple_strategy_users,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/326/testReport/consistency_test/TestAccuracy/test_simple_strategy_users

Failed on CassCI build cassandra-2.1_offheap_dtest #326",dtest,[],CASSANDRA,Improvement,Normal,2016-04-04 21:13:13,5
12955796,Bug or not?: coordinator using SimpleSnitch may query other nodes for copies of local data ,"As [~Stefania] explains [in this JIRA comment|https://issues.apache.org/jira/browse/CASSANDRA-11225?focusedCommentId=15221059&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221059], {{SimpleSnitch}} does not implement {{IEndpointSnitch.sortByProximity(localhost, liveendpoints)}}, so a query for data on the coordinator may query other nodes. That seems like unnecessary work to me, and on that note, Stefania woonders [in this JIRA comment|https://issues.apache.org/jira/browse/CASSANDRA-11225?focusedCommentId=15223598&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15223598] - should this be considered a bug?

Stefania, I'm assigning you here -- could you find the right people to involve in this discussion?",doc-impacting,['Legacy/Coordination'],CASSANDRA,Bug,Low,2016-04-04 15:29:39,3
12955322,dtest failure in pushed_notifications_test.TestVariousNotifications.tombstone_failure_threshold_message_test,"The test was unable to find the tombstone failure threshold error in the logs

example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/635/testReport/pushed_notifications_test/TestVariousNotifications/tombstone_failure_threshold_message_test

Failed on CassCI build cassandra-3.0_dtest #635",dtest,[],CASSANDRA,Improvement,Normal,2016-04-01 15:08:23,4
12955158,cqlsh: COPY FROM should use regular inserts for single statement batches,"I haven't reproduced it with a test yet but, from code inspection, if CQL rows are larger than {{batch_size_fail_threshold_in_kb}} and this parameter cannot be changed, then data import will fail.

Users can control the batch size by setting MAXBATCHSIZE.

If a batch contains a single statement, there is no need to use a batch and we should use normal inserts instead or, alternatively, we should skip the batch size check for unlogged batches with only one statement.",lhf,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-04-01 00:38:12,3
12955019,dtest failure in materialized_views_test.TestMaterializedViews.base_replica_repair_test,"base_replica_repair_test has failed on trunk with the following exception in the log of node2:

{code}
ERROR [main] 2016-03-31 08:48:46,949 CassandraDaemon.java:708 - Exception encountered during startup
java.lang.RuntimeException: Failed to list files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:53) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:547) ~[main/:na]
        at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:725) ~[main/:na]
        at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:690) ~[main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:567) ~[main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:555) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:383) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:320) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:130) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:107) ~[main/:na]
        at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:139) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepareRestrictions(SelectStatement.java:864) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:811) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:799) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:505) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:242) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:286) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:294) ~[main/:na]
        at org.apache.cassandra.schema.SchemaKeyspace.query(SchemaKeyspace.java:1246) ~[main/:na]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:875) ~[main/:na]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:867) ~[main/:na]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:134) ~[main/:na]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:124) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:229) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:562) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:691) [main/:na]
Caused by: java.lang.RuntimeException: Failed to list directory files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985, inconsistent disk state for transaction [ma_txn_flush_58db56b0-f71d-11e5-bf68-03a01adb9f11.log in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:149) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:103) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$48/35984028.accept(Unknown Source) ~[na:na]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ~[na:1.8.0_45]
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[na:1.8.0_45]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[na:1.8.0_45]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512) ~[na:1.8.0_45]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502) ~[na:1.8.0_45]
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) ~[na:1.8.0_45]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) ~[na:1.8.0_45]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_45]
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) ~[na:1.8.0_45]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:71) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:49) ~[main/:na]
        ... 25 common frames omitted
{code}

example failure:

http://cassci.datastax.com/job/trunk_dtest/1092/testReport/materialized_views_test/TestMaterializedViews/base_replica_repair_test

Failed on CassCI build trunk_dtest #1092

I've attached the logs from the failure in build 1092.",dtest,[],CASSANDRA,Bug,Normal,2016-03-31 16:59:54,3
12954731,dtest failure in cql_tracing_test.TestCqlTracing.tracing_unknown_impl_test,"Failing on the following assert, on trunk only: {{self.assertEqual(len(errs[0]), 1)}}

Is not failing consistently.

example failure:

http://cassci.datastax.com/job/trunk_dtest/1087/testReport/cql_tracing_test/TestCqlTracing/tracing_unknown_impl_test

Failed on CassCI build trunk_dtest #1087",dtest,['Legacy/Observability'],CASSANDRA,Bug,Normal,2016-03-30 19:46:38,3
12954704,dtest failure in compaction_test.TestCompaction_with_LeveledCompactionStrategy.bloomfilter_size_test,"The final assertion {{self.assertGreaterEqual(bfSize, min_bf_size)}} is failing with {{44728 not greater than or equal to 50000}} on 2.1, pretty consistently.

example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/439/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/bloomfilter_size_test

Failed on CassCI build cassandra-2.1_dtest #439",dtest,[],CASSANDRA,Improvement,Normal,2016-03-30 18:23:30,0
12953974,dtest failure in sslnodetonode_test.TestNodeToNodeSSLEncryption,"{{Invalid yaml. Please remove properties [require_endpoint_verification] from your cassandra.yaml}}

example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/556/testReport/sslnodetonode_test/TestNodeToNodeSSLEncryption/ssl_wrong_hostname_no_validation_test

Failed on CassCI build cassandra-2.2_dtest #556

All of the sslnodetonodetests are failing on 2.2 and 3.0. I assume a ticket was committed that broke these tests by changing the appropriate yaml key?",dtest,[],CASSANDRA,Improvement,Normal,2016-03-28 15:04:53,4
12953635,dtest failure in consistency_test.TestAccuracy.test_network_topology_strategy_users,"This test and consistency_test.TestAvailability.test_network_topology_strategy have begun failing now that we dropped the instance size we run CI with. The tests should be altered to reflect the constrained resources. They are ambitious for dtests, regardless.

example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/221/testReport/consistency_test/TestAccuracy/test_network_topology_strategy_users

Failed on CassCI build cassandra-2.1_novnode_dtest #221",dtest,[],CASSANDRA,Improvement,Normal,2016-03-25 18:13:10,4
12953620,Make number of cores used by cqlsh COPY visible to testing code,"As per this conversation with [~Stefania]:

https://github.com/riptano/cassandra-dtest/pull/869#issuecomment-200597829

we don't currently have a way to verify that the test environment variable {{CQLSH_COPY_TEST_NUM_CORES}} actually affects the behavior of {{COPY}} in the intended way. If this were added, we could make our tests of the one-core edge case a little stricter.",lhf,['Legacy/Testing'],CASSANDRA,Improvement,Low,2016-03-25 16:47:49,3
12953601,dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test,"sstable_repairedset_test is failing on 2.1 and 2.2, but not on trunk.

In the final assertion, after running sstablemetadata on both nodes, we see unrepaired sstables, when we expect all sstables to be repaired.

example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/220/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test

Failed on CassCI build cassandra-2.1_novnode_dtest #220",dtest,[],CASSANDRA,Improvement,Normal,2016-03-25 15:44:30,0
12952901,dtest failure in cql_tests.AbortedQueriesTester.remote_query_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1076/testReport/cql_tests/AbortedQueriesTester/remote_query_test

Failed on CassCI build trunk_dtest #1076

Also breaking:
cql_tests.AbortedQueriesTester.materialized_view_test
topology_test.TestTopology.do_not_join_ring_test

Broken by https://github.com/pcmanus/ccm/pull/479",dtest,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2016-03-23 16:54:46,4
12951608,dtest failure in materialized_views_test.TestMaterializedViews.complex_mv_select_statements_test,"We've got a single flap on the 3.0 novnode job:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/189/testReport/materialized_views_test/TestMaterializedViews/complex_mv_select_statements_test

{code}

Error Message

Expected [[1, 0, 1, 0], [1, 1, 1, 0], [1, 2, 1, 0]] from SELECT a, b, c, d FROM mv, but got [[1, 0, 1, 0], [1, 1, 1, 0]]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-d0ZZ9_
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Creating keyspace
dtest: DEBUG: Testing MV primary key: ((a, b), c)
dtest: DEBUG: Testing MV primary key: ((b, a), c)
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 1193, in complex_mv_select_statements_test
    cl=ConsistencyLevel.QUORUM
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 67, in assert_all
    assert list_res == expected, ""Expected %s from %s, but got %s"" % (expected, query, list_res)
""Expected [[1, 0, 1, 0], [1, 1, 1, 0], [1, 2, 1, 0]] from SELECT a, b, c, d FROM mv, but got [[1, 0, 1, 0], [1, 1, 1, 0]]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-d0ZZ9_\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Creating keyspace\ndtest: DEBUG: Testing MV primary key: ((a, b), c)\ndtest: DEBUG: Testing MV primary key: ((b, a), c)\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-03-18 20:05:21,4
12951341,dtest failure in topology_test.TestTopology.decommissioned_node_cant_rejoin_test,"This fails hard on 3.0:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/186/testReport/topology_test/TestTopology/decommissioned_node_cant_rejoin_test

Here's the changeset from the first failure on that job:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/186/changes

(just this commit: https://github.com/apache/cassandra/commit/719caa67649bf6f27cdd99dd7d6055d2aa8546ae)

It has failed on trunk as well:

http://cassci.datastax.com/view/trunk/job/trunk_novnode_dtest/lastCompletedBuild/testReport/topology_test/TestTopology/decommissioned_node_cant_rejoin_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-03-17 21:17:08,4
12951333,dtest failure in topology_test.TestTopology.crash_during_decommission_test,"This has flapped once on 3.0:

http://cassci.datastax.com/job/cassandra-3.0_dtest/614/testReport/topology_test/TestTopology/crash_during_decommission_test

and more frequently on trunk:

http://cassci.datastax.com/job/trunk_dtest/1057/testReport/junit/topology_test/TestTopology/crash_during_decommission_test/

Message and trace:

{code}
Error Message

15 Mar 2016 10:48:55 [node1] Missing: ['127.0.0.2.* now UP']:
.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-EJfTo3
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Status as reported by node 127.0.0.2
dtest: DEBUG: Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  112.85 KB  32           64.1%             d7d836bd-a0ff-474c-b04c-2c00bd2fc636  rack1
UN  127.0.0.2  127.25 KB  32           61.7%             68c5a8c4-8986-40df-afb2-ee0849814618  rack1
UN  127.0.0.3  112.68 KB  32           74.2%             6dd52315-53f4-444d-bace-a8a0d7c2b34e  rack1


dtest: DEBUG: Restarting node2
dtest: DEBUG: 
dtest: DEBUG: 
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 253, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/topology_test.py"", line 230, in crash_during_decommission_test
    node2.start(wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 597, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 417, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""15 Mar 2016 10:48:55 [node1] Missing: ['127.0.0.2.* now UP']:\n.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-EJfTo3\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Status as reported by node 127.0.0.2\ndtest: DEBUG: Datacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  112.85 KB  32           64.1%             d7d836bd-a0ff-474c-b04c-2c00bd2fc636  rack1\nUN  127.0.0.2  127.25 KB  32           61.7%             68c5a8c4-8986-40df-afb2-ee0849814618  rack1\nUN  127.0.0.3  112.68 KB  32           74.2%             6dd52315-53f4-444d-bace-a8a0d7c2b34e  rack1\n\n\ndtest: DEBUG: Restarting node2\ndtest: DEBUG: \ndtest: DEBUG: \n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-03-17 20:46:56,4
12951315,dtest failure in replace_address_test.TestReplaceAddress.resumable_replace_test,"example failures:

http://cassci.datastax.com/job/cassandra-2.2_dtest/lastCompletedBuild/testReport/replace_address_test/TestReplaceAddress/resumable_replace_test/history/

http://cassci.datastax.com/job/cassandra-3.0_dtest/612/testReport/replace_address_test/TestReplaceAddress/resumable_replace_test/

Seems to fail hard with the following error:

{code}
16 Mar 2016 18:42:00 [node1] Missing: ['127.0.0.4.* now UP']:
INFO  [HANDSHAKE-/127.0.0.4] 2016-03-16 18:40:03,1.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-JdjudW
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
dtest: DEBUG: Starting node 4 to replace node 3
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 253, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 253, in resumable_replace_test
    node4.start(jvm_args=[""-Dcassandra.replace_address_first_boot=127.0.0.3""])
  File ""/home/automaton/ccm/ccmlib/node.py"", line 597, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 417, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""16 Mar 2016 18:42:00 [node1] Missing: ['127.0.0.4.* now UP']:\nINFO  [HANDSHAKE-/127.0.0.4] 2016-03-16 18:40:03,1.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-JdjudW\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: Starting node 4 to replace node 3\n--------------------- >> end captured logging << ---------------------""
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-03-17 20:00:49,4
12951245,Cancelled compaction leading to infinite loop in compaction strategy getNextBackgroundTask,"Our test is basically running *nodetool repair* on specific keyspaces (such as keyspace1) and the test is also triggering *nodetool compact keyspace1 standard1* in the background. 
And so it looks like running major compactions & repairs lead to that issue when using *LCS*.


Below is an excerpt from the *thread dump* (the rest is attached)
{code}
""CompactionExecutor:2"" #33 daemon prio=1 os_prio=4 tid=0x00007f5363e64f10 nid=0x3c4e waiting for monitor entry [0x00007f53340d8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:252)
	- waiting to lock <0x00000006c9362c80> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.lifecycle.Tracker.notifySSTableRepairedStatusChanged(Tracker.java:434)
	at org.apache.cassandra.db.compaction.CompactionManager.performAnticompaction(CompactionManager.java:550)
	at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:465)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- <0x00000006c9362ca8> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""CompactionExecutor:1"" #32 daemon prio=1 os_prio=4 tid=0x00007f5363e618b0 nid=0x3c4d runnable [0x00007f5334119000]
   java.lang.Thread.State: RUNNABLE
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:650)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at com.google.common.collect.Iterators.addAll(Iterators.java:361)
	at com.google.common.collect.Iterables.addAll(Iterables.java:354)
	at org.apache.cassandra.db.compaction.LeveledManifest.getCandidatesFor(LeveledManifest.java:589)
	at org.apache.cassandra.db.compaction.LeveledManifest.getCompactionCandidates(LeveledManifest.java:349)
	- locked <0x00000006d0f7a6a8> (a org.apache.cassandra.db.compaction.LeveledManifest)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:98)
	- locked <0x00000006d0f7a568> (a org.apache.cassandra.db.compaction.LeveledCompactionStrategy)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:95)
	- locked <0x00000006c9362c80> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:257)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

*CPU usage is at 100%*
{code}
top -p 15386
top - 12:12:40 up  1:28,  1 user,  load average: 1.08, 1.11, 1.16
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.3 us,  0.0 sy, 12.4 ni, 87.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  16433792 total,  8947336 used,  7486456 free,    89552 buffers
KiB Swap:        0 total,        0 used,        0 free.  3326796 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
15386 automat+  20   0 7891448 5.004g 290184 S 102.9 31.9  80:07.06 java

{code}

*ttop shows that the compaction thread consumes all the CPU*
{code}
$ java -jar sjk.jar ttop -p 15386
Monitoring threads ...

2016-03-17T12:17:13.514+0000 Process summary
  process cpu=126.34%
  application cpu=102.81% (user=102.46% sys=0.35%)
  other: cpu=23.53%
  heap allocation rate 375mb/s
[000002] user= 0.00% sys= 0.00% alloc=     0b/s - Reference Handler
[000003] user= 0.00% sys= 0.00% alloc=     0b/s - Finalizer
[000005] user= 0.00% sys= 0.00% alloc=     0b/s - Signal Dispatcher
[000012] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Accept-7199
[000013] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Accept-0
[000015] user= 0.00% sys= 0.00% alloc=   476b/s - AsyncAppender-Worker-ASYNCDEBUGLOG
[000016] user= 0.00% sys= 0.05% alloc=  1070b/s - ScheduledTasks:1
[000017] user= 0.00% sys= 0.00% alloc=    33b/s - EXPIRING-MAP-REAPER:1
[000018] user= 0.00% sys= 0.02% alloc=  1932b/s - Background_Reporter:1
[000022] user= 0.00% sys= 0.00% alloc=     0b/s - MemtablePostFlush:1
[000023] user= 0.00% sys= 0.00% alloc=     0b/s - MemtableReclaimMemory:1
[000026] user= 0.00% sys= 0.00% alloc=     0b/s - SlabPoolCleaner
[000027] user= 0.00% sys= 0.00% alloc=     0b/s - PERIODIC-COMMIT-LOG-SYNCER
[000028] user= 0.00% sys= 0.00% alloc=     0b/s - COMMIT-LOG-ALLOCATOR
[000029] user= 0.00% sys= 0.01% alloc=  7086b/s - OptionalTasks:1
[000030] user= 0.00% sys= 0.00% alloc=     0b/s - Reference-Reaper:1
[000031] user= 0.00% sys= 0.00% alloc=     0b/s - Strong-Reference-Leak-Detector:1
[000032] user=99.45% sys= 0.07% alloc=  374mb/s - CompactionExecutor:1
[000033] user= 0.00% sys= 0.00% alloc=     0b/s - CompactionExecutor:2
[000036] user= 0.00% sys= 0.00% alloc=     0b/s - NonPeriodicTasks:1
[000037] user= 0.00% sys= 0.00% alloc=     0b/s - LocalPool-Cleaner:1
[000041] user= 0.00% sys= 0.00% alloc=     0b/s - IndexSummaryManager:1
[000043] user= 0.00% sys= 0.01% alloc=  2705b/s - GossipTasks:1
[000044] user= 0.00% sys= 0.00% alloc=     0b/s - ACCEPT-/10.200.182.146
[000045] user= 0.00% sys= 0.01% alloc=  2283b/s - BatchlogTasks:1
[000055] user= 0.00% sys= 0.02% alloc=  9494b/s - GossipStage:1
[000056] user= 0.00% sys= 0.00% alloc=     0b/s - AntiEntropyStage:1
[000057] user= 0.00% sys= 0.00% alloc=     0b/s - MigrationStage:1
[000058] user= 0.00% sys= 0.00% alloc=     0b/s - MiscStage:1
[000067] user= 0.00% sys= 0.02% alloc=  2445b/s - MessagingService-Incoming-/10.200.182.144
[000068] user= 0.00% sys= 0.01% alloc=   968b/s - MessagingService-Outgoing-/10.200.182.144
[000069] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.144
[000070] user= 0.00% sys= 0.02% alloc=   512b/s - MessagingService-Outgoing-/10.200.182.144
[000072] user= 0.00% sys= 0.00% alloc=     0b/s - NanoTimeToCurrentTimeMillis updater
[000073] user= 0.00% sys= 0.02% alloc=  3113b/s - MessagingService-Incoming-/10.200.182.144
[000074] user= 0.00% sys= 0.00% alloc=     0b/s - PendingRangeCalculator:1
[000075] user= 0.00% sys= 0.41% alloc=   66kb/s - SharedPool-Worker-1
[000076] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-2
[000077] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-3
[000078] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-4
[000079] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-5
[000080] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-6
[000081] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-7
[000082] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-8
[000084] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-2
[000085] user= 0.00% sys= 0.00% alloc=   181b/s - HintsWriteExecutor:1
[000091] user= 0.00% sys= 0.00% alloc=     0b/s - PO-thread-0
[000092] user= 0.00% sys= 0.00% alloc=     0b/s - NodeHealthPlugin-Scheduler-thread-0
[000093] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-1
[000094] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-2
[000097] user= 0.00% sys= 0.00% alloc=     0b/s - Lease RemoteMessageServer acceptor-2-1
[000104] user= 0.00% sys= 0.00% alloc=     0b/s - RemoteMessageClient worker-4-1
[000120] user= 0.00% sys= 0.00% alloc=     0b/s - RemoteMessageClient connection limiter - 0
[000121] user= 0.00% sys= 0.00% alloc=     0b/s - threadDeathWatcher-5-1
[000122] user= 0.00% sys= 0.00% alloc=     0b/s - PO-thread scheduler
[000123] user= 0.00% sys= 0.00% alloc=     0b/s - JOB-TRACKER
[000124] user= 0.00% sys= 0.01% alloc=  1276b/s - TASK-TRACKER
[000127] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-1
[000128] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-2
[000129] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-3
[000130] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-4
[000131] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-5
[000132] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-6
[000133] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-7
[000134] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-8
[000135] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-9
[000136] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-10
[000137] user= 0.19% sys=-0.18% alloc=     0b/s - epollEventLoopGroup-6-11
[000138] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-12
[000139] user= 0.19% sys=-0.19% alloc=     0b/s - epollEventLoopGroup-6-13
[000140] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-14
[000141] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-15
[000142] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-16
[000143] user= 0.19% sys=-0.04% alloc=   13kb/s - Thread-7
[000144] user= 0.00% sys= 0.00% alloc=     0b/s - taskCleanup
[000145] user= 0.00% sys= 0.00% alloc=     0b/s - DseGossipStateUpdater
[000146] user= 0.00% sys= 0.00% alloc=     0b/s - DestroyJavaVM
[000149] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-10
[000150] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-11
[000151] user= 0.00% sys= 0.00% alloc=     0b/s - Directory/File cleanup thread
[000153] user= 0.00% sys= 0.00% alloc=     0b/s - pool-15-thread-1
[000190] user= 0.00% sys= 0.00% alloc=     0b/s - pool-18-thread-1
[000215] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-3
[000217] user= 0.00% sys= 0.00% alloc=     0b/s - RMI Scheduler(0)
[000220] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Connection(335)-10.200.182.146
[000222] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-4
[000223] user= 0.00% sys= 0.00% alloc=     0b/s - taskCleanup
[000224] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-69
[000225] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-70
[000227] user= 0.00% sys= 0.00% alloc=     0b/s - pool-19-thread-1
[000254] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-9
[000255] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-11
[000256] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-10
[000269] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-13
[000270] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-12
[000272] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-14
[000273] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-15
[000274] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-18
[000275] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-19
[000276] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-20
[000277] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-17
[000278] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-16
[000279] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-22
[000280] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-21
[000281] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-23
[000282] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-24
[000283] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-25
[000284] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-26
[000285] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-27
[000286] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-28
[000287] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-29
[000288] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-30
[000289] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-31
[000290] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-32
[000296] user= 0.00% sys= 0.00% alloc=  1970b/s - pool-2-thread-1
[000297] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-33
[000298] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-34
[000302] user= 0.00% sys= 0.01% alloc=  1576b/s - MessagingService-Incoming-/10.200.182.145
[000303] user= 0.00% sys= 0.00% alloc=   451b/s - MessagingService-Outgoing-/10.200.182.145
[000304] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.145
[000305] user= 0.00% sys= 0.01% alloc=   206b/s - MessagingService-Outgoing-/10.200.182.145
[000308] user= 0.00% sys= 0.00% alloc=   424b/s - MessagingService-Incoming-/10.200.182.145
[000314] user= 0.00% sys= 0.00% alloc=     0b/s - StreamingTransferTaskTimeouts:1
[000324] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000325] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000326] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000328] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.146
[000329] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-35
[000330] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-37
[000331] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-36
[000332] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-39
[000333] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-38
[000334] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-42
[000335] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-41
[000336] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-40
[000337] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-46
[000338] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-44
[000339] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-45
[000340] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-43
[000341] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-47
[000342] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-48
[000343] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-50
[000344] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-49
[000375] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:1
[000376] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:2
[000406] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.145
[000408] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.146
[000409] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.144
[000415] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:3
[000418] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:4
[000435] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:5
[000438] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:6
[000439] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:7
[000444] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:8
[000687] user= 0.00% sys= 0.00% alloc=     0b/s - JMX server connection timeout 687
[000688] user= 2.44% sys= 0.16% alloc= 1380kb/s - RMI TCP Connection(401)-10.200.182.146
[000694] user= 0.00% sys= 0.00% alloc=     0b/s - Attach Listener
[000726] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Connection(400)-10.200.182.146
[000743] user=-0.00% sys=-0.16% alloc=-109800b/s - MemtableFlushWriter:112
[000745] user= 0.00% sys= 0.00% alloc=     0b/s - MemtableFlushWriter:113
[000746] user= 0.00% sys= 0.03% alloc=  4295b/s - JMX server connection timeout 746
{code}

",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2016-03-17 15:36:09,0
12950910,dtest failure in snitch_test.TestGossipingPropertyFileSnitch.test_prefer_local_reconnect_on_listen_address,"This is a weird one:

http://cassci.datastax.com/job/cassandra-2.2_dtest/540/testReport/snitch_test/TestGossipingPropertyFileSnitch/test_prefer_local_reconnect_on_listen_address

Not sure why the keyspace isn't getting created. At first I thought it was expecting {{keyspace1}} but finding {{Keyspace1}}, but then the test would just never pass.",dtest,[],CASSANDRA,Improvement,Normal,2016-03-16 17:31:44,4
12950855,dtest failure in repair_tests.repair_test.TestRepair.partitioner_range_repair_test,"This new(ish) test failed on the 2.1 novnodes job. I don't know if it's a flap, or if it never would have passed; the test is relatively new, and I think this may be one of the first commits to 2.1 since it was committed. Here's the failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/216/testReport/repair_tests.repair_test/TestRepair/partitioner_range_repair_test

Failed on CassCI build cassandra-2.1_novnode_dtest #216. Assigning [~philipthompson] for initial triaging, since he wrote this test.",dtest,[],CASSANDRA,Improvement,Normal,2016-03-16 14:39:57,4
12950195,ERROR [CompactionExecutor] CassandraDaemon.java Exception in thread,"Hey. Please help me with a problem. Recently I updated to 3.3.0 and this problem appeared in the logs.

ERROR [CompactionExecutor:2458] 2016-03-10 12:41:15,127 CassandraDaemon.java:195 - Exception in thread Thread[CompactionExecutor:2458,1,main]
java.lang.AssertionError: null
at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:49) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:88) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:83) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BufferCell.purge(BufferCell.java:175) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.lambda$purge$107(ComplexColumnData.java:165) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$68/1224572667.apply(Unknown Source) ~[na:na]
at org.apache.cassandra.utils.btree.BTree$FiltrationTracker.apply(BTree.java:650) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:693) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:668) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.transformAndFilter(ComplexColumnData.java:170) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.purge(ComplexColumnData.java:165) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.purge(ComplexColumnData.java:43) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow.lambda$purge$102(BTreeRow.java:333) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow$$Lambda$67/1968133513.apply(Unknown Source) ~[na:na]
at org.apache.cassandra.utils.btree.BTree$FiltrationTracker.apply(BTree.java:650) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:693) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:668) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow.transformAndFilter(BTreeRow.java:338) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow.purge(BTreeRow.java:333) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.partitions.PurgeFunction.applyToRow(PurgeFunction.java:88) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:116) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:38) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:64) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:24) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:76) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:78) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264) ~[apache-cassandra-3.3.0.jar:3.3.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]",error,"['Legacy/Local Write-Read Paths', 'Local/Compaction']",CASSANDRA,Bug,Normal,2016-03-15 06:17:21,2
12949031,Fix bloom filter sizing with LCS,Since CASSANDRA-7272 we most often over allocate the bloom filter size with LCS,lcs,[],CASSANDRA,Bug,Normal,2016-03-11 09:18:17,0
12948874,dtest failure in consistency_test.TestConsistency.quorum_available_during_failure_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/68/testReport/consistency_test/TestConsistency/quorum_available_during_failure_test

Failed on CassCI build trunk_offheap_dtest #68

This seems to be failing after merging this CCM PR:

https://github.com/pcmanus/ccm/pull/461

I'm not sure why it would fail with that error-checking code but not without it. On this test run, it ran after these two tests:

{code}
test_simple_strategy_each_quorum (consistency_test.TestAvailability) ... ok
test_simple_strategy_each_quorum_counters (consistency_test.TestAccuracy) ... ok
{code}

so, maybe processes are hanging around after one of those tests. [~philipthompson] can you have a first look here?",dtest,[],CASSANDRA,Improvement,Normal,2016-03-10 18:51:56,4
12948625,cqlsh: COPY FROM should check that explicit column names are valid,"If an invalid column is specified in the COPY FROM command, then it fails without an appropriate error notification.

For example using this schema:

{code}
CREATE TABLE bulk_read.value500k_cluster1 (
    pk int,
    c1 int,
    v1 text,
    v2 text,
    PRIMARY KEY (pk, c1)
);
{code}

and this COPY FROM command (note the third column name is wrong:

{code}
COPY bulk_read.value500k_cluster1 (pk, c1, vv, v2) FROM 'test.csv';
{code}

we get the following error:

{code}
Starting copy of bulk_read.value500k_cluster1 with columns ['pk', 'c1', 'vv', 'v2'].
1 child process(es) died unexpectedly, aborting
Processed: 0 rows; Rate:       0 rows/s; Avg. rate:       0 rows/s
0 rows imported from 0 files in 0.109 seconds (0 skipped).
{code}

Running cqlsh with {{--debug}} reveals where the problem is:

{code}
Starting copy of bulk_read.value500k_cluster1 with columns ['pk', 'c1', 'vv', 'v2'].
Traceback (most recent call last):
  File ""/home/automaton/cassandra-src/bin/../pylib/cqlshlib/copyutil.py"", line 2005, in run
    self.inner_run(*self.make_params())
  File ""/home/automaton/cassandra-src/bin/../pylib/cqlshlib/copyutil.py"", line 2027, in make_params
    is_counter = (""counter"" in [table_meta.columns[name].cql_type for name in self.valid_columns])
{code}

The parent process should check that all column names are valid and output an appropriate error message rather than letting worker processes crash.",lhf,['Legacy/Tools'],CASSANDRA,Bug,Low,2016-03-10 01:44:40,3
12948481,dtest failure in ttl_test.TestTTL.remove_column_ttl_with_default_ttl_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1043/testReport/ttl_test/TestTTL/remove_column_ttl_with_default_ttl_test

This test started failing on build #1041, when a change to TTL behavior was introduced:

https://github.com/apache/cassandra/commit/e017f9494844234fa73848890347f59c622cea40

[~blerer] Looks like I failed to review this properly, sorry. Could you have a look at this, please?",dtest,[],CASSANDRA,Improvement,Normal,2016-03-09 18:30:57,5
12948418,dtest failure in compaction_test.TestCompaction_with_LeveledCompactionStrategy.data_size_test,"This dtest has failed once:

http://cassci.datastax.com/job/cassandra-3.0_dtest/597/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/data_size_test

Here's the history for this test:

http://cassci.datastax.com/job/cassandra-3.0_dtest/597/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/data_size_test/history/

It failed at this line:

https://github.com/riptano/cassandra-dtest/blob/88a74d7/compaction_test.py#L86

Basically, it ran compaction over the default stress tables, but timed out waiting to see the line {{Compacted }} in the log.",dtest,[],CASSANDRA,Improvement,Normal,2016-03-09 15:02:12,4
12948253,Improve backoff policy for cqlsh COPY FROM,"Currently we have an exponential back-off policy in COPY FROM that kicks in when timeouts are received. However there are two limitations:

* it does not cover new requests and therefore we may not back-off sufficiently to give time to an overloaded server to recover
* the pause is performed in the receiving thread and therefore we may not process server messages quickly enough

There is a static throttling mechanism in rows per second from feeder to worker processes (the INGESTRATE) but the feeder has no idea of the load of each worker process. However it's easy to keep track of how many chunks a worker process has yet to read by introducing a bounded semaphore.

The idea is to move the back-off pauses to the worker processes main thread so as to include all messages, new and retries, not just the retries that timed out. The worker process will not read new chunks during the back-off pauses, and the feeder process can then look at the number of pending chunks before sending new chunks to a worker process.

[~aholmber], [~aweisberg] what do you think?  ",doc-impacting,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2016-03-09 02:39:22,3
12948166,dtest failure in replication_test.SnitchConfigurationUpdateTest.test_rf_collapse_property_file_snitch,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/592/testReport/replication_test/SnitchConfigurationUpdateTest/test_rf_collapse_property_file_snitch

Failed on CassCI build cassandra-3.0_dtest #592

The test failed on this line:

https://github.com/riptano/cassandra-dtest/blob/7a331cda807c96ae107b58017854f0e57996d8c3/replication_test.py#L567

So, a node's expected move from one rack to another doesn't happen in the allotted timeout time. This is the only flap I've seen. Maybe the thing to do here is increase the timeout and keep an eye on it?",dtest,[],CASSANDRA,Improvement,Normal,2016-03-08 21:29:55,4
12945984,dtest failure in upgrade_tests.regression_test.TestForRegressions.test_10822,"example failure:

http://cassci.datastax.com/job/upgrade_tests-all/2/testReport/upgrade_tests.regression_test/TestForRegressions/test_10822

Failed on CassCI build upgrade_tests-all #2

looks like it fails every time, likely to be a test code problem:
{noformat}
'NoneType' object has no attribute 'starting_version'
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-03-01 23:32:31,4
12945655,(windows) dtest failure in compaction_test.TestCompaction_with_LeveledCompactionStrategy.compaction_strategy_switching_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_win32/337/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/compaction_strategy_switching_test

Failed on CassCI build trunk_dtest_win32 #337

Failing pretty regularly:
{noformat}
[Error 5] Access is denied: u'\\\\?\\d:\\temp\\dtest-wigqqn\\test\\node1\\data0\\ks\\cf-293dd6c0c58c11e58fa05d04d8558111\\ma-1-big-Data.db'
{noformat}

Error looks very similar to that seen on CASSANDRA-11281, so perhaps the same root cause.",dtest,[],CASSANDRA,Improvement,Normal,2016-02-29 22:54:51,5
12945653,(windows) dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.compaction_strategy_switching_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_win32/337/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/compaction_strategy_switching_test

Failed on CassCI build trunk_dtest_win32 #337

this test appears to be flaky:
{noformat}
[Error 5] Access is denied: u'\\\\?\\d:\\temp\\dtest-ha5dpw\\test\\node1\\data0\\ks\\cf-ef8f1030c58911e5a56c87d8e427fe34\\ma-7-big-Data.db'
{noformat}

Error looks very similar to that seen on CASSANDRA-11281, so perhaps the same root cause.
",dtest,[],CASSANDRA,Improvement,Normal,2016-02-29 22:53:41,5
12945616,dtest failure in disk_balance_test.TestDiskBalance.disk_balance_bootstrap_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1011/testReport/disk_balance_test/TestDiskBalance/disk_balance_bootstrap_test

Failed on CassCI build trunk_dtest #1011

This looks likely to be a test issue, perhaps we need to relax the assertion here a bit:
{noformat}
values not within 20.00% of the max: (474650, 382235, 513385) (node1)
{noformat}

This is flaky with several flaps in the last few weeks.",dtest,[],CASSANDRA,Improvement,Normal,2016-02-29 21:15:45,0
12945545,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1.select_distinct_with_deletions_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/155/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1/select_distinct_with_deletions_test

Failed on CassCI build cassandra-3.0_novnode_dtest #155

looks to be an assertion error, so could be test code or possibly cassandra:
{noformat}
9 != 8
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-02-29 18:10:43,4
12945088,(windows) dtest failure in upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/167/testReport/upgrade_internal_auth_test/TestAuthUpgrade/upgrade_to_30_test

Failed on CassCI build cassandra-3.0_dtest_win32 #167

this test is flapping pretty frequently. not certain yet on failure cause, might vary across builds.",dtest windows,[],CASSANDRA,Improvement,Normal,2016-02-26 22:28:20,5
12945014,dtest failure in materialized_views_test.TestMaterializedViews.view_tombstone_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/564/testReport/materialized_views_test/TestMaterializedViews/view_tombstone_test

Failed on CassCI build cassandra-3.0_dtest #564

intermittent failure, error:
{noformat}
Expected [[1, 1, 'b', 3.0]] from SELECT * FROM t_by_v WHERE v = 1, but got []
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-02-26 19:31:24,4
12944756,COPY TO should have higher double precision,"At the moment COPY TO uses the same float precision as cqlsh, which by default is 5 but it can be changed in cqlshrc. However, typically people want to preserve precision when exporting data and so this default is too low for COPY TO.

I suggest adding a new COPY FROM option to specify floating point precision with a much higher default value, for example 12.",doc-impacting lhf,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2016-02-26 03:22:49,3
12944729,(windows) dtest failure in auth_test.TestAuth.restart_node_doesnt_lose_auth_data_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/194/testReport/auth_test/TestAuth/restart_node_doesnt_lose_auth_data_test

Failed on CassCI build cassandra-2.2_dtest_win32 #194

looks like a problem with dtest code:
{noformat}
code=2200 [Invalid query] message=""Keyspace ks does not exist""
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-02-26 00:55:43,4
12944689,(windows) dtest failure in upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/174/testReport/upgrade_internal_auth_test/TestAuthUpgrade/upgrade_to_22_test

Failed on CassCI build cassandra-2.2_dtest_win32 #174

looks like there could be multiple causes for this intermittent failure.",dtest windows,[],CASSANDRA,Improvement,Normal,2016-02-25 22:41:53,5
12944664,(windows) dtest failure in replace_address_test.TestReplaceAddress.replace_with_reset_resume_state_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/165/testReport/replace_address_test/TestReplaceAddress/replace_with_reset_resume_state_test

Failed on CassCI build cassandra-2.2_dtest_win32 #165

2 flaps of this test in recent history, looks like a possible test issue, perhaps with invalid yaml at startup (somehow).
",dtest windows,[],CASSANDRA,Improvement,Normal,2016-02-25 21:02:26,4
12944657,Add repair options to repair_history table,"It would be nice if options to trigger a repair (-pr, -local, -parallelism, incremental, etc) are also included in the {{system_distributed.parent_repair_history}} and {{system_distributed.repair_history}} tables. 

The simplest way would be to add it as a map to allow for new options in the future.",lhf,['Legacy/Coordination'],CASSANDRA,Improvement,Low,2016-02-25 20:38:37,0
12944327,dtest failure in bootstrap_test.TestBootstrap.simple_bootstrap_test_nodata,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/526/testReport/bootstrap_test/TestBootstrap/simple_bootstrap_test_nodata

Failed on CassCI build cassandra-2.2_dtest #526",dtest,[],CASSANDRA,Improvement,Normal,2016-02-25 00:07:02,4
12944325,dtest failure in repair_test.TestRepair.local_dc_repair_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/519/testReport/repair_test/TestRepair/local_dc_repair_test

Failed on CassCI build cassandra-2.2_dtest #519

Appears to be flapping occasionally, most recent error:
{noformat}
[Unavailable exception] message=""Cannot achieve consistency level ALL"" info={'required_replicas': 4, 'alive_replicas': 3, 'consistency': 'ALL'
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-02-24 23:47:55,4
12944001,dtest failure in replication_test.SnitchConfigurationUpdateTest.test_failed_snitch_update_property_file_snitch,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/301/testReport/replication_test/SnitchConfigurationUpdateTest/test_failed_snitch_update_property_file_snitch

Failed on CassCI build cassandra-2.1_offheap_dtest #301
Failed on CassCI build cassandra-2.1_offheap_dtest #286

Looks to be a timeout issue on both historical failures:
{noformat}
Ran out of time waiting for topology to change on node 0
{noformat}",dtest,[],CASSANDRA,Improvement,Normal,2016-02-24 22:42:14,4
12943092,nodetool tablestats' keyspace-level metrics are wrong/misleading,"In the nodetool tablestats output (formerly cfstats), we display ""keyspace"" level metrics before the table-level metrics:

{noformat}
Keyspace: testks
        Read Count: 14772528
        Read Latency: 0.14456651623879135 ms.
        Write Count: 4761283
        Write Latency: 0.062120404521218336 ms.
        Pending Flushes: 0
                Table: processes
                SSTable count: 7
                Space used (live): 496.76 MB
                Space used (total): 496.76 MB
                Space used by snapshots (total): 0 bytes
                Off heap memory used (total): 285.76 KB
                SSTable Compression Ratio: 0.2318241570710227
                Number of keys (estimate): 3027
                Memtable cell count: 2140
                Memtable data size: 1.66 MB
                Memtable off heap memory used: 0 bytes
                Memtable switch count: 967
                Local read count: 14772528
                Local read latency: 0.159 ms
                Local write count: 4761283
                Local write latency: 0.068 ms
{noformat}

However, the keyspace-level metrics are misleading, at best.  They are aggregate metrics for every table in the keyspace _that is included in the command line filters_.  So, if you run {{tablestats}} for a single table, the keyspace-level stats will only reflect that table's stats.

I see two possible fixes:
# If the command line options don't include the entire keyspace, skip the keyspace-level stats
# Ignore the command line options, and always make the keyspace-level stats an aggregate of all tables in the keyspace

My only concern with option 2 is that performance may suffer a bit on keyspaces with many tables.  However, this is a command line tool, so as long as the response time is reasonable, I don't think it's a big deal.",lhf,['Tool/nodetool'],CASSANDRA,Bug,Low,2016-02-24 21:59:52,2
12941589,replication_test.ReplicationTest.network_topology_test flaps,"Test intermittently failing with set comparison errors that differ from one failure to the next. Looks a bit more stable recently since #203 failed, but probably worth keeping an eye on, and check if there's a problem with the test code.

most recent failure:
http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/203/testReport/replication_test/ReplicationTest/network_topology_test/",dtest,[],CASSANDRA,Improvement,Normal,2016-02-24 00:29:06,4
12941537,repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test failing,"recent occurence:
http://cassci.datastax.com/job/cassandra-2.1_dtest/427/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test/

last 2 runs failed:
http://cassci.datastax.com/job/cassandra-2.1_dtest/427/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test/history/",dtest,[],CASSANDRA,Improvement,Normal,2016-02-23 20:52:24,4
12940642,upgrade bootstrap tests flap when migration tasks fail,"I've seen these tests flap:

{code}
upgrade_tests/upgrade_through_versions_test.py:ProtoV4Upgrade_3_1_UpTo_3_2_HEAD.bootstrap_test
upgrade_tests/upgrade_through_versions_test.py:ProtoV4Upgrade_3_3_UpTo_Trunk_HEAD.bootstrap_test

upgrade_tests/upgrade_through_versions_test.py:ProtoV3Upgrade_3_0_UpTo_3_1_HEAD.bootstrap_multidc_test
upgrade_tests/upgrade_through_versions_test.py:ProtoV3Upgrade_3_2_UpTo_3_3_HEAD.bootstrap_multidc_test
upgrade_tests/upgrade_through_versions_test.py:ProtoV3Upgrade_3_3_UpTo_Trunk_HEAD.bootstrap_multidc_test
upgrade_tests/upgrade_through_versions_test.py:ProtoV4Upgrade_3_3_UpTo_Trunk_HEAD.bootstrap_multidc_test
{code}

There may be more upgrade paths that flap, I'm not sure. All the failures I've seen look like this:

{code}
Unexpected error in node5 node log: ['ERROR [main] 2016-02-18 20:05:13,012 MigrationManager.java:164 - Migration task failed to complete\nERROR [main] 2016-02-18 20:05:14,012 MigrationManager.java:164 - Migration task failed to complete']
{code}

[~rhatch] Do these look familiar at all?",dtest,[],CASSANDRA,Bug,Normal,2016-02-19 20:51:27,4
12939988,Parallel cleanup can lead to disk space exhaustion,"In CASSANDRA-5547, we made cleanup (among other things) run in parallel across multiple sstables.  There have been reports on IRC of this leading to disk space exhaustion, because multiple sstables are (almost entirely) rewritten at the same time.  This seems particularly problematic because cleanup is frequently run after a cluster is expanded due to low disk space.

I'm not really familiar with how we perform free disk space checks now, but it sounds like we can make some improvements here.  It would be good to reduce the concurrency of cleanup operations if there isn't enough free disk space to support this.",doc-impacting,"['Legacy/Tools', 'Local/Compaction']",CASSANDRA,Improvement,Normal,2016-02-17 23:24:04,0
12939330,Infinite loop bug adding high-level SSTableReader in compaction,"Observed that after a large repair on LCS that sometimes the system will enter an infinite loop with vast amounts of logs lines recording, ""Adding high-level (L${LEVEL}) SSTableReader(path='${TABLE}') to candidates""

This results in an outage of the node and eventual crashing. The log spam quickly rotates out possibly useful earlier debugging.",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2016-02-15 17:43:47,0
12938255,"SOURCE command in CQLSH 3.2 requires that ""use keyspace"" is in the cql file that you are sourcing","a difference in behaviour between SOURCE command in CQLSH 3.1 and 3.2. 
In CQLSH 3.1 SOURCE will NOT require ""use keyspace"" in the cql file that you execute: the ""keyspace"" directive in the qlshrc file will work and the cql file will be executed.

In CQLSH 3.2.1, SOURCE command requires that ""use keyspace"" is in the cql file that you are sourcing, otherwise it throws this error:
""No keyspace has been specified. USE a keyspace, or explicitly specify keyspace.tablename"". 
The ""keyspace"" directive in cqlshrc is overridden by source command.

steps to reproduce:
create a file called select.cql in your home directory:
{noformat}
echo ""CONSISTENCY ONE;"" > select.cql
echo ""select * from tab;"" >> select.cql
{noformat}

in cqlsh:
{noformat}
create KEYSPACE kspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
create TABLE tab ( id int primary key);
insert into tab (id) VALUES ( 1);
{noformat}

Add this to cqlsgrc:
{noformat}
[authentication]
keyspace = kspace
{noformat}

Then exit cqlsh and rerun cqlsh using the cqlshrc just modified.
Note that you are in keyspace ""kspace"".
execute:
{noformat}
source 'select.cql' 
{noformat}

this will have different behaviour in CQLSH 3.2 and 3.1",lhf,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-02-10 17:44:33,3
12937142,select_distinct_with_deletions_test failing on non-vnode environments,"Looks like this was fixed in CASSANDRA-10762, but not for non-vnode environments:

{code}
$ DISABLE_VNODES=yes KEEP_TEST_DIR=yes CASSANDRA_VERSION=git:cassandra-3.0 PRINT_DEBUG=true nosetests -s -v upgrade_tests/cql_tests.py:TestCQLNodes2RF1.select_distinct_with_deletions_test
select_distinct_with_deletions_test (upgrade_tests.cql_tests.TestCQLNodes2RF1) ... cluster ccm directory: /tmp/dtest-UXb0un
http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-3.0
Custom init_config not found. Setting defaults.
Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
getting default job version for 3.0.3
UpgradePath(starting_version='binary:2.2.3', upgrade_version=None)
starting from 2.2.3
upgrading to {'install_dir': '/home/ryan/.ccm/repository/gitCOLONcassandra-3.0'}
Querying upgraded node
FAIL

======================================================================
FAIL: select_distinct_with_deletions_test (upgrade_tests.cql_tests.TestCQLNodes2RF1)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/ryan/git/datastax/cassandra-dtest/upgrade_tests/cql_tests.py"", line 3360, in select_distinct_with_deletions_test
    self.assertEqual(9, len(rows))
AssertionError: 9 != 8
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-UXb0un
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: getting default job version for 3.0.3
dtest: DEBUG: UpgradePath(starting_version='binary:2.2.3', upgrade_version=None)
dtest: DEBUG: starting from 2.2.3
dtest: DEBUG: upgrading to {'install_dir': '/home/ryan/.ccm/repository/gitCOLONcassandra-3.0'}
dtest: DEBUG: Querying upgraded node
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 56.022s

FAILED (failures=1)
{code}",dtest,[],CASSANDRA,Bug,Normal,2016-02-05 20:29:13,2
12936865,cqlsh_copy_test failing on 2.1,"See [here|http://cassci.datastax.com/job/cassandra-2.1_dtest/415/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_datetimeformat_round_trip/] for a failure of {{cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_datetimeformat_round_trip}} on 2.1 against the SHA 165f586e6f5e7.

The test is seeing different timestamps than expected.",dtest,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-02-04 21:32:46,3
12934503,upgrade_supercolumns_test dtests failing on 2.1,"This tests in this module fail [here|https://github.com/riptano/cassandra-dtest/blob/18647a3e167f127795e2fe63d73305dddf103716/upgrade_supercolumns_test.py#L213] and [here|https://github.com/riptano/cassandra-dtest/blob/529cd71ad5ac4c2f28ccb5560ddc068f604c7b28/upgrade_supercolumns_test.py#L106] when a call to {{start}} with {{wait_other_notice=True}} times out. It happens consistently on the upgrade path from cassandra-2.1 to 2.2. I haven't seen clear evidence as to whether this is a test failure or a C* bug, so I'll mark it as a test error for the TE team to debug.

I don't have a CassCI link for this failure - the changes to the tests haven't been merged yet.

EDIT: changing the title of this ticket since there are multiple similar failures. The failing tests are

{code}
upgrade_supercolumns_test.py:TestSCUpgrade.upgrade_with_counters_test failing
upgrade_supercolumns_test.py:TestSCUpgrade.upgrade_with_index_creation_test
{code}",dtest,[],CASSANDRA,Improvement,Normal,2016-01-27 15:57:59,4
12933106,largecolumn_test.TestLargeColumn.cleanup_test is failing,"This is absolutely a test issue. {{largecolumn_test.TestLargeColumn.cleanup_test}} fails on a few versions, as seen [here|http://cassci.datastax.com/job/cassandra-2.2_dtest/488/testReport/largecolumn_test/TestLargeColumn/cleanup_test/]. The nominal complaint is 
{{Expected numeric from fields from nodetool gcstats}}
except as we can see from the new debug output I added, gcstats is printing out numeric fields. So the regex is wrong somehow.",dtest,['Legacy/Testing'],CASSANDRA,Bug,Normal,2016-01-21 16:15:52,2
12933073,C*2.1 cqlsh DESCRIBE KEYSPACE ( or TABLE ) returns 'NoneType' object has no attribute 'replace',"C* 2.1 cqlsh DESCRIBE KEYSPACE ( or TABLE ) returns:

{code}
 'NoneType' object has no attribute 'replace' 
{code}

for thrift CF's originally created in C* 1.2.

Repro:

1. Create cf in cassandra-cli on C* 1.2.x  (1.2.9 was used here)

{code}
[default@ks1] CREATE COLUMN FAMILY t1
...	WITH column_type='Standard'
...	AND comparator='CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)'
...	AND default_validation_class='UTF8Type'
...	AND key_validation_class='UTF8Type'
...	AND read_repair_chance=0.1
...	AND dclocal_read_repair_chance=0.0
...	AND gc_grace=864000
...	AND min_compaction_threshold=4
...	AND max_compaction_threshold=32
...	AND replicate_on_write=true
...	AND compaction_strategy='LeveledCompactionStrategy' AND compaction_strategy_options={sstable_size_in_mb: 32}
...	AND caching='KEYS_ONLY'
...	AND compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:64};

qlsh> describe keyspace ks1;

CREATE KEYSPACE ks1 WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'datacenter1': '1'
};

USE ks1;

CREATE TABLE t1 (
  key text,
  column1 text,
  column2 text,
  value text,
  PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.100000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'sstable_size_in_mb': '32', 'class': 'LeveledCompactionStrategy'} AND
  compression={'chunk_length_kb': '64', 'sstable_compression': 'SnappyCompressor'};


cqlsh> select keyspace_name, columnfamily_name,column_aliases,key_aliases from system.schema_columnfamilies where keyspace_name= 'ks1';

 keyspace_name | columnfamily_name | column_aliases | key_aliases
---------------+-------------------+----------------+-------------
           ks1 |                t1 |             [] |          []


2/ Upgrade -> C* 2.0.9 -> nodetool upgradesstables -a

At this stage , DESCRIBE in cqlsh is working

3/ Upgrade -> C* 2.1.12 -> nodetool upgradesstables -a

DESCRIBE now fails:

cqlsh> describe table ks1.t1;
'NoneType' object has no attribute 'replace'

cqlsh> describe keyspace ks1;
'NoneType' object has no attribute 'replace'
{code}

You can workaround by manually updating {{system.schema_columnfamilies}}

{code}
 UPDATE system.schema_columnfamilies SET column_aliases ='[""column1"",""column2""]' WHERE keyspace_name = 'ks1' AND columnfamily_name = 't1';
{code}

Once you exit and restart cqlsh, {{DESCRIBE}} is not working as per C* 1.2

{code}
cqlsh> describe keyspace ks1;

CREATE KEYSPACE ks1 WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '1'}  AND durable_writes = true;

CREATE TABLE ks1.t1 (
    key text,
    column1 text,
    column2 text,
    value text,
    PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (column1 ASC, column2 ASC)
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'sstable_size_in_mb': '32', 'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'chunk_length_kb': '64', 'sstable_compression': 'org.apache.cassandra.io.compress.SnappyCompressor'}
    AND dclocal_read_repair_chance = 0.0
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.1
    AND speculative_retry = '99.0PERCENTILE';
{code}


",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-01-21 13:56:49,3
12932942,COPY FROM on large datasets: fix progress report and optimize performance part 4,"h5. Description

Running COPY from on a large dataset (20G divided in 20M records) revealed two issues:

* The progress report is incorrect, it is very slow until almost the end of the test at which point it catches up extremely quickly.

* The performance in rows per second is similar to running smaller tests with a smaller cluster locally (approx 35,000 rows per second). As a comparison, cassandra-stress manages 50,000 rows per second under the same set-up, therefore resulting 1.5 times faster. 

See attached file _copy_from_large_benchmark.txt_ for the benchmark details.

h5. Doc-impacting changes to COPY FROM options

* A new option was added: PREPAREDSTATEMENTS - it indicates if prepared statements should be used; it defaults to true.
* The default value of CHUNKSIZE changed from 1000 to 5000.
* The default value of MINBATCHSIZE changed from 2 to 10.",doc-impacting,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-01-21 01:56:27,3
12932643,Make it clear what timestamp_resolution is used for with DTCS,"We have had a few cases lately where users misunderstand what timestamp_resolution does, we should;

* make the option not autocomplete in cqlsh
* update documentation
* log a warning",dtcs,[],CASSANDRA,Improvement,Normal,2016-01-20 07:42:19,0
12932522,consistent_reads_after_move_test is failing on trunk,"The novnode dtest {{consistent_bootstrap_test.TestBootstrapConsistency.consistent_reads_after_move_test}} is failing on trunk. See an example failure [here|http://cassci.datastax.com/job/trunk_novnode_dtest/274/testReport/consistent_bootstrap_test/TestBootstrapConsistency/consistent_reads_after_move_test/].

On trunk I am getting an OOM of one of my C* nodes [node3], which is what causes the nodetool move to fail. Logs are attached.",dtest,['Legacy/Testing'],CASSANDRA,Bug,Urgent,2016-01-19 20:52:18,0
12930060,DateTieredCompactionStrategy not compacting  sstables in 2.1.12,"The following CF is never compacting from day one

CREATE TABLE globaldb.""DynamicParameter"" (
    dp_id bigint PRIMARY KEY,
    dp_advertiser_id int,
    dp_application_id int,
    dp_application_user_id bigint,
    dp_banner_id int,
    dp_campaign_id int,
    dp_click_timestamp timestamp,
    dp_country text,
    dp_custom_parameters text,
    dp_flags bigint,
    dp_ip int,
    dp_machine_id text,
    dp_string text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'max_sstable_age_days': '30', 'base_time_seconds': '3600', 'timestamp_resolution': 'MILLISECONDS', 'enabled': 'true', 'min_threshold': '2', 'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.2
    AND default_time_to_live = 10713600
    AND gc_grace_seconds = 1209600
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
",dtcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2016-01-14 06:58:34,0
12929250,cqlsh_copy_tests failing en mass when vnodes are disabled,"Check out [an example cassci failure|http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/186/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_list_data/] as well as the [full novnode report page|http://cassci.datastax.com/userContent/cstar_report/index.html?jobs=cassandra-2.1_novnode_dtest,cassandra-3.0_novnode_dtest,cassandra-2.2_novnode_dtest&show_known=true].

Many COPY TO tests are failing when the cluster only has one token. The message {{Found no ranges to query, check begin and end tokens: None - None}} is printed, and it appears to be coming from cqlsh, specfically in pylib/cqlshlib/copyutil.py",dtest,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-01-11 18:27:06,3
12928084,disk_balance_bootstrap_test is failing on trunk,"http://cassci.datastax.com/job/trunk_dtest/891/testReport/junit/disk_balance_test/TestDiskBalance/disk_balance_bootstrap_test/

{code}
======================================================================
FAIL: disk_balance_bootstrap_test (disk_balance_test.TestDiskBalance)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 51, in disk_balance_bootstrap_test
    self.assert_balanced(node)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 127, in assert_balanced
    assert_almost_equal(*sums, error=0.2, error_message=node.name)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/assertions.py"", line 65, in assert_almost_equal
    assert vmin > vmax * (1.0 - error) or vmin == vmax, ""values not within %.2f%% of the max: %s (%s)"" % (error * 100, args, kwargs['error_message'])
AssertionError: values not within 20.00% of the max: (529955, 386060, 473640) (node4)
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-nNoQzp
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 114.862s

FAILED (failures=1)

{code}",dtest,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-01-06 18:07:31,0
12928082,disk_balance_decommission_test is failing on trunk,"http://cassci.datastax.com/job/trunk_dtest/891/testReport/junit/disk_balance_test/TestDiskBalance/disk_balance_decommission_test/

{code}
======================================================================
FAIL: disk_balance_decommission_test (disk_balance_test.TestDiskBalance)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 74, in disk_balance_decommission_test
    self.assert_balanced(node)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 127, in assert_balanced
    assert_almost_equal(*sums, error=0.2, error_message=node.name)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/assertions.py"", line 65, in assert_almost_equal
    assert vmin > vmax * (1.0 - error) or vmin == vmax, ""values not within %.2f%% of the max: %s (%s)"" % (error * 100, args, kwargs['error_message'])
AssertionError: values not within 20.00% of the max: (482095, 477840, 612940) (node2)
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-SLbi3e
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 121.295s

FAILED (failures=1)
{code}",dtest,['Legacy/Tools'],CASSANDRA,Bug,Normal,2016-01-06 18:04:44,0
12924894,[Regression] Error when removing list element with UPDATE statement,"Steps to reproduce:

{code:sql}
CREATE TABLE simple(
  id int PRIMARY KEY,
  int_list list<int>
);

INSERT INTO simple(id, int_list) VALUES(10, [1,2,3]);
SELECT * FROM simple;

 id | int_list
----+-----------
 10 | [1, 2, 3]

UPDATE simple SET int_list[0]=null WHERE id=10;
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.AssertionError"">
{code}

 Per CQL semantics, setting a column to NULL == deleting it.

 When using debugger, below is the Java stack trace on server side:

{noformat}
 ERROR o.apache.cassandra.transport.Message - Unexpected exception during request; channel = [id: 0x6dbc33bd, /192.168.51.1:57723 => /192.168.51.1:9473]
java.lang.AssertionError: null
	at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:49) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:88) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.UpdateParameters.addTombstone(UpdateParameters.java:141) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.UpdateParameters.addTombstone(UpdateParameters.java:136) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.Lists$SetterByIndex.execute(Lists.java:362) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:94) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:666) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:606) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:413) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:401) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:472) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:449) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [cassandra-all-3.1.1.jar:3.1.1]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_60-ea]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [cassandra-all-3.1.1.jar:3.1.1]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60-ea]
{noformat}

The root cause seems to be located at *org.apache.cassandra.cql3.Lists:362* :

{code:java}
            CellPath elementPath = existingRow.getComplexColumnData(column).getCellByIndex(idx).path();
            if (value == null)
            {
                params.addTombstone(column);
            }
            else if (value != ByteBufferUtil.UNSET_BYTE_BUFFER)
            {
                params.addCell(column, elementPath, value);
            }
{code}

 In the if block, it seems we do not pass the CellPath as it should be and it makes the asertion _assert column.isComplex() == (path != null);_ fails at 
*org.apache.cassandra.db.rows:49*

{code:java}
    public BufferCell(ColumnDefinition column, long timestamp, int ttl, int localDeletionTime, ByteBuffer value, CellPath path)
    {
        super(column);
        assert column.isComplex() == (path != null);
        ....
   }
{code}

 Another remark about the code block in *org.apache.cassandra.cql3.Lists:362*, there is an *if/else if* but there is no final *else* block to catch all other alternatives, is it *intended* or just an oversight ?",regression,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2015-12-30 14:14:33,2
12924450,ERROR [CompactionExecutor] CassandraDaemon.java  Exception in thread,"Hey. Please help me with a problem. Recently I updated to 3.0.1 and this problem appeared in the logs.

ERROR [CompactionExecutor:2596] 2015-12-28 08:30:27,733 CassandraDaemon.java:195 - Exception in thread Thread[CompactionExecutor:2596,1,main]
java.lang.AssertionError: null
	at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:49) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:88) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:83) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BufferCell.purge(BufferCell.java:175) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.ComplexColumnData.lambda$purge$100(ComplexColumnData.java:165) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$53/1339741213.apply(Unknown Source) ~[na:na]
	at org.apache.cassandra.utils.btree.BTree$FiltrationTracker.apply(BTree.java:614) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:657) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:632) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.ComplexColumnData.transformAndFilter(ComplexColumnData.java:170) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.ComplexColumnData.purge(ComplexColumnData.java:165) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.ComplexColumnData.purge(ComplexColumnData.java:43) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BTreeRow.lambda$purge$95(BTreeRow.java:333) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BTreeRow$$Lambda$52/1236900032.apply(Unknown Source) ~[na:na]
	at org.apache.cassandra.utils.btree.BTree$FiltrationTracker.apply(BTree.java:614) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:657) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:632) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BTreeRow.transformAndFilter(BTreeRow.java:338) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.rows.BTreeRow.purge(BTreeRow.java:333) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToRow(PurgeFunction.java:88) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:116) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:38) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:64) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:24) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:76) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:78) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:253) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
",error,"['Legacy/Local Write-Read Paths', 'Local/Compaction']",CASSANDRA,Bug,Normal,2015-12-28 05:54:33,2
12923645,sstableutil_test.py:SSTableUtilTest.abortedcompaction_test flapping on 3.0,"{{sstableutil_test.py:SSTableUtilTest.abortedcompaction_test}} flaps on 3.0:

http://cassci.datastax.com/job/cassandra-3.0_dtest/438/testReport/junit/sstableutil_test/SSTableUtilTest/abortedcompaction_test/

It also flaps on the CassCI job running without vnodes:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/110/testReport/junit/sstableutil_test/SSTableUtilTest/abortedcompaction_test/history/

",dtest,[],CASSANDRA,Sub-task,Normal,2015-12-22 02:41:51,0
12923606,"netstats_test dtest fails on Windows, flaps on Linux","jmx_test.py:TestJMX.netstats_test started failing hard on Windows about a month ago:

http://cassci.datastax.com/job/cassandra-3.0_dtest_win32/140/testReport/junit/jmx_test/TestJMX/netstats_test/history/?start=25

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/156/testReport/jmx_test/TestJMX/netstats_test/history/

It fails when it is unable to connect to a node via JMX. I don't know if this problem has any relationship to CASSANDRA-10913.",dtest windows,[],CASSANDRA,Sub-task,Normal,2015-12-21 23:51:13,5
12922766,Fix skipping logic on upgrade tests in dtest,"This will be a general ticket for upgrade dtests that fail because of bad logic surrounding skipping tests. We need a better system in place for skipping tests that are not intended to work on certain versions of Cassandra; at present, we run the upgrade tests with {{SKIP=false}} because, again, the built-in skipping logic is bad.

One such test is test_v2_protocol_IN_with_tuples:

http://cassci.datastax.com/job/storage_engine_upgrade_dtest-22_tarball-311/lastCompletedBuild/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3/test_v2_protocol_IN_with_tuples/

This shouldn't be run on clusters that include nodes running 3.0.",dtest,[],CASSANDRA,Sub-task,Normal,2015-12-17 16:16:18,5
12922490,test_refresh_schema_on_timeout_error dtest flapping on CassCI,"These tests create keyspaces and tables through cqlsh, then runs {{DESCRIBE}} to confirm they were successfully created. These tests flap under the novnode dtest runs:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/lastCompletedBuild/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_refresh_schema_on_timeout_error/history/
http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/lastCompletedBuild/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_refresh_schema_on_timeout_error/history/

I have not reproduced this locally on Linux.",dtest,[],CASSANDRA,Sub-task,Normal,2015-12-16 20:33:31,5
12922479,network_topology_test dtest still failing,"It looks like CASSANDRA-8158 may not have been properly resolved:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/176/testReport/replication_test/ReplicationTest/network_topology_test/history/
http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/lastCompletedBuild/testReport/replication_test/ReplicationTest/network_topology_test/history/
http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/lastCompletedBuild/testReport/replication_test/ReplicationTest/network_topology_test/history/

[~philipthompson] Can you have a look?",dtest,[],CASSANDRA,Sub-task,Normal,2015-12-16 20:04:28,4
12922252,Alter behavior of batch WARN and fail on single partition batches,"In an attempt to give operator insight into potentially harmful batch usage, Jiras were created to log WARN or fail on certain batch sizes. This ignores the single partition batch, which doesn't create the same issues as a multi-partition batch. 

The proposal is to ignore size on single partition batch statements. 

Reference:
[CASSANDRA-6487|https://issues.apache.org/jira/browse/CASSANDRA-6487]
[CASSANDRA-8011|https://issues.apache.org/jira/browse/CASSANDRA-8011]",lhf,[],CASSANDRA,Improvement,Low,2015-12-16 05:31:46,2
12921817,pushed_notifications_test.py:TestPushedNotifications.restart_node_test flapping on C* 2.1,"This test flaps on CassCI on 2.1. [~aboudreault] Do I remember correctly that you did some work on these tests in the past few months? If so, could you have a look and see if there's some assumption the test makes that don't hold for 2.1?

Oddly, it fails frequently under JDK8:

http://cassci.datastax.com/job/cassandra-2.1_dtest_jdk8/lastCompletedBuild/testReport/pushed_notifications_test/TestPushedNotifications/restart_node_test/history/

but less frequently on JDK7:

http://cassci.datastax.com/job/cassandra-2.1_dtest/lastCompletedBuild/testReport/pushed_notifications_test/TestPushedNotifications/restart_node_test/history/
",dtest,[],CASSANDRA,Sub-task,Normal,2015-12-14 23:17:35,4
12921799,thrift_tests.py:TestCQLAccesses.test_range_tombstone_and_static failing on C* 2.1,"http://cassci.datastax.com/job/cassandra-2.1_dtest/376/testReport/thrift_tests/TestCQLAccesses/test_range_tombstone_and_static/history/

I haven't had enough experience with thrift or the thrift tests to debug this. It passes on 2.2+. I've reproduced this failure locally.",dtest,[],CASSANDRA,Sub-task,Normal,2015-12-14 22:33:30,2
12921122,jmxmetrics_test.TestJMXMetrics.begin_test is failing,"This test is failing on 2.1-head. There appear to be structural issues with the test, and no C* bug to be fixed.",dtest,['Legacy/Testing'],CASSANDRA,Sub-task,Low,2015-12-11 17:56:44,5
12920407,Fix the way we replace sstables after anticompaction,"We have a bug when we replace sstables after anticompaction, we keep adding duplicates which causes leveled compaction to fail after. Reason being that LCS does not keep its sstables in a {{Set}}, so after first compaction, we will keep around removed sstables in the leveled manifest and that will put LCS in an infinite loop as it tries to mark non-existing sstables as compacting
",lcs,[],CASSANDRA,Bug,Normal,2015-12-09 12:34:27,0
12916743,Allow literal value as parameter of UDF & UDA,"I have defined the following UDF

{code:sql}
CREATE OR REPLACE FUNCTION  maxOf(current int, testValue int) RETURNS NULL ON NULL INPUT 
RETURNS int 
LANGUAGE java 
AS  'return Math.max(current,testValue);'

CREATE TABLE maxValue(id int primary key, val int);
INSERT INTO maxValue(id, val) VALUES(1, 100);

SELECT maxOf(val, 101) FROM maxValue WHERE id=1;
{code}

I got the following error message:

{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:19 no viable alternative at input '101' (SELECT maxOf(val1, [101]...)"">
{code}

 It would be nice to allow literal value as parameter of UDF and UDA too.

 I was thinking about an use-case for an UDA groupBy() function where the end user can *inject* at runtime a literal value to select which aggregation he want to display, something similar to GROUP BY ... HAVING <filter clause>",CQL3 UDF doc-impacting,['Legacy/CQL'],CASSANDRA,Improvement,Low,2015-11-28 16:49:38,2
12912405,Don't remove level info when doing upgradesstables,Seems we blow away the level info when doing upgradesstables. Introduced in  CASSANDRA-8004,lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-11-12 07:11:38,0
12905520,Make cqlsh tests work when authentication is configured,"cqlsh tests break if the runner has an authentication section in their ~/.cassandra/cqlshrc, because cqlsh changes the prompt and the tests scan output for a prompt. It manifests as read timeouts while waiting for a prompt in test/run_cqlsh.py.
[This pattern|https://github.com/mambocab/cassandra/blob/1c27f9be1ba8ea10dbe843d513e23de6238dede8/pylib/cqlshlib/test/run_cqlsh.py#L30] could be generalized to match the ""<username>@cqlsh..."" prompt that arises with this config.",cqlsh test,"['Legacy/Testing', 'Legacy/Tools']",CASSANDRA,Improvement,Low,2015-10-16 13:09:17,3
12905450,RangeAwareCompaction,"Broken out from CASSANDRA-6696, we should split sstables based on ranges during compaction.

Requirements;
* dont create tiny sstables - keep them bunched together until a single vnode is big enough (configurable how big that is)
* make it possible to run existing compaction strategies on the per-range sstables

We should probably add a global compaction strategy parameter that states whether this should be enabled or not.",compaction lcs vnodes,['Local/Compaction'],CASSANDRA,New Feature,Normal,2015-10-16 09:05:54,0
12904919,Add ability to skip TIME_WAIT sockets on port check on Windows startup,"C* sockets are often staying TIME_WAIT for up to 120 seconds (2x max segment lifetime) for me in my dev environment on Windows. This is rather obnoxious since it means I can't launch C* for up to 2 minutes after stopping it.

Attaching a patch that adds a simple -a for aggressive startup to the launch scripts to ignore duplicate port check from netstat if it's TIME_WAIT. Also snuck in some more liberal interpretation of help strings in the .ps1.",Windows,['Packaging'],CASSANDRA,Improvement,Low,2015-10-14 16:25:01,6
12903904,Failure to start up Cassandra when temporary compaction files are not all renamed after kill/crash (FSReadError),"We have seen an issue intermittently but repeatedly over the last few months where, after exiting the Cassandra process, it fails to start with an FSReadError (stack trace below). The FSReadError refers to a 'statistics' file for a  that doesn't exist, though a corresponding temporary file does exist (eg. there is no /media/data/cassandraDB/data/clusteradmin/singleton_token-01a92ed069b511e59b2c53679a538c14/clusteradmin-singleton_token-ka-9-Statistics.db file, but there is a /media/data/cassandraDB/data/clusteradmin/singleton_token-01a92ed069b511e59b2c53679a538c14/clusteradmin-singleton_token-tmp-ka-9-Statistics.db file.)

We tracked down the issue to the fact that the process exited with leftover compactions and some of the 'tmp' files for the SSTable had been renamed to final files, but not all of them - the issue happens if the 'Statistics' file is not renamed but others are. The scenario we've seen on the last two occurrences involves the 'CompressionInfo' file being a final file while all other files for the SSTable generation were left with 'tmp' names.

When this occurs, Cassandra cannot start until the file issue is resolved; we've worked around it by deleting the SSTable files from the same generation, both final and tmp, which at least allows Cassandra to start. Renaming all files to either tmp or final names would also work.

We've done some debugging in Cassandra and have been unable to cause the issue without renaming the files manually. The rename code at SSTableWriter.rename() looks like it could result in this if the process exits in the middle of the rename, but in every occurrence we've debugged through, the Set of components is ordered and Statistics is the first file renamed.

However the comments in SSTableWriter.rename() suggest that the 'Data' file is meant to be used as meaning the files were completely renamed. The method ColumnFamilyStore. removeUnfinishedCompactionLeftovers(), however, will proceed assuming the compaction is complete if any of the component files has a final name, and will skip temporary files when reading the list. If the 'Statistics' file is temporary then it won't be read, and the defaults does not include a list of ancestors, leading to the NullPointerException.

It appears that ColumnFamilyStore. removeUnfinishedCompactionLeftovers() should perhaps either ensure that all 'tmp' files are properly renamed before it uses them, or skip SSTable files that don't have either the 'Data' or 'Statistics' file in final form.

Stack trace: 
{code}
FSReadError in Failed to remove unfinished compaction leftovers (file: /media/data/cassandraDB/data/clusteradmin/singleton_token-01a92ed069b511e59b2c53679a538c14/clusteradmin-singleton_token-ka-9-Statistics.db).  See log for details.
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:617)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:302)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:536)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:625)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:609)
        ... 3 more
Exception encountered during startup: java.lang.NullPointerException
{code}
",compaction triage,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2015-10-09 21:28:26,0
12902842,Fix sstableverify_test dtest,"The dtest for sstableverify is failing:

http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/lastCompletedBuild/testReport/offline_tools_test/TestOfflineTools/sstableverify_test/

It fails in the same way when I run it on OpenStack, so I don't think it's just a CassCI problem.

[~slebresne] Looks like you made changes to this test recently:

https://github.com/riptano/cassandra-dtest/commit/51ab085f21e01cc8e5ad88a277cb4a43abd3f880

Could you have a look at the failure? I'm assigning you for triage, but feel free to reassign.",test,"['Legacy/Tools', 'Test/dtest/python']",CASSANDRA,Sub-task,Normal,2015-10-06 22:34:27,3
12902784,Fix pep8 compliance in cqlsh.py,"cqlsh has fallen out of pep8 compliance:

http://cassci.datastax.com/view/trunk/job/cassandra-3.0_dtest/lastCompletedBuild/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_pep8_compliance/

[~philipthompson] has said off-JIRA that he'll fix it.",cqlsh,[],CASSANDRA,Sub-task,Normal,2015-10-06 19:09:33,4
12902141,Remove offheap_objects option until 9472 re-introduces them,We need to send a meaningful error message if user try to use {{offheap_objects}} in 3.0 since it's not supported currently (pending CASSANDRA-9472). And document the current removal in the NEWS file.,doc-impacting,[],CASSANDRA,Bug,Low,2015-10-02 15:34:51,2
12901713,cqlsh: Include sub-second precision in timestamps by default,"Query with >= timestamp works. But the exact timestamp value is not working.

{noformat}
NCHAN-M-D0LZ:bin nchan$ ./cqlsh
Connected to CCC Multi-Region Cassandra Cluster at <host>:<port>.
[cqlsh 5.0.1 | Cassandra 2.1.7 | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh>
{noformat}

{panel:title=Schema|borderStyle=dashed|borderColor=#ccc|titleBGColor=#F7D6C1|bgColor=#FFFFCE}
cqlsh:ccc> desc COLUMNFAMILY ez_task_result ;

CREATE TABLE ccc.ez_task_result (
    submissionid text,
    ezid text,
    name text,
    time timestamp,
    analyzed_index_root text,
    ...
    ...
    PRIMARY KEY (submissionid, ezid, name, time)
{panel}

{panel:title=Working|borderStyle=dashed|borderColor=#ccc|titleBGColor=#F7D6C1|bgColor=#FFFFCE}
cqlsh:ccc> select submissionid, ezid, name, time, state, status, translated_criteria_status from ez_task_result where submissionid='760dd154670811e58c04005056bb6ff0' and ezid='760dd6de670811e594fc005056bb6ff0' and name='run-sanities' and time>='2015-09-29 20:54:23-0700';

 submissionid                     | ezid                             | name         | time                     | state     | status      | translated_criteria_status
----------------------------------+----------------------------------+--------------+--------------------------+-----------+-------------+----------------------------
 760dd154670811e58c04005056bb6ff0 | 760dd6de670811e594fc005056bb6ff0 | run-sanities | 2015-09-29 20:54:23-0700 | EXECUTING | IN_PROGRESS |       run-sanities started

(1 rows)
cqlsh:ccc>
{panel}
{panel:title=Not working|borderStyle=dashed|borderColor=#ccc|titleBGColor=#F7D6C1|bgColor=#FFFFCE}
cqlsh:ccc> select submissionid, ezid, name, time, state, status, translated_criteria_status from ez_task_result where submissionid='760dd154670811e58c04005056bb6ff0' and ezid='760dd6de670811e594fc005056bb6ff0' and name='run-sanities' and time='2015-09-29 20:54:23-0700';

 submissionid | ezid | name | time | analyzed_index_root | analyzed_log_path | clientid | end_time | jenkins_path | log_file_path | path_available | path_to_task | required_for_overall_status | start_time | state | status | translated_criteria_status | type
--------------+------+------+------+---------------------+-------------------+----------+----------+--------------+---------------+----------------+--------------+-----------------------------+------------+-------+--------+----------------------------+------

(0 rows)
cqlsh:ccc>
{panel}


",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2015-09-30 21:58:53,3
12901458,Windows dtest 3.0: cqlsh CLEAR/CLS tests fail,[cqlsh_tests.cqlsh_tests.TestCqlsh.test_clear|http://cassci.datastax.com/view/win32/job/cassandra-3.0_dtest_win32/72/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_clear/] and [cqlsh_tests.cqlsh_tests.TestCqlsh.test_cls|http://cassci.datastax.com/view/win32/job/cassandra-3.0_dtest_win32/72/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_cls/] both fail when the regex looking for a screen-clearing character fails. This is probably a Linux-only test at this point.,Windows cqlsh,['Legacy/Tools'],CASSANDRA,Sub-task,Normal,2015-09-29 23:15:59,6
12901440,Fix cqlsh bugs,"This is followup to CASSANDRA-10289

The tests currently failing should be:

* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_create_columnfamily}}
** uses {{create_columnfamily_table_template}}. Stefania says ""the {{(}} after {{CREATE ... IF}} does not look valid to me.""
* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_create_table}}
** uses {{create_columnfamily_table_template}}, see above.
* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_delete}}
** Stefania says: ""I don't think keyspaces are a valid completion after {{DELETE a [}} and after {{DELETE FROM twenty_rows_composite_table USING TIMESTAMP 0 WHERE TOKEN(a) >=}}. From a quick analysis of {{cqlhandling.py}} I think it comes from {{<term>}}, which picks up {{<functionName>}}, which was changed to include {{ks.}} by CASSANDRA-7556.
* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_drop_keyspace}}
** Stefania says: ""the {{;}} after {{DROP KEYSPACE IF}} is not valid.
* {{cqlshlib.test.test_cqlsh_output.TestCqlshOutput.test_timestamp_output}}
** already documented with CASSANDRA-10313 and CASSANDRA-10397

I'm happy to break these out into separate tickets if necessary. 

To run the tests locally, I cd to {{cassandra/pylib/cqlshlib}} and run the following:

{code}
ccm create -n 1 --install-dir=../.. test
ccm start --wait-for-binary-proto
nosetests test 2>&1
ccm remove
{code}

This requires nose and ccm. Until CASSANDRA-10289 is resolved, you'll have to use my branch here: https://github.com/mambocab/cassandra/tree/fix-cqlsh-tests

Tests for this branch are run (non-continuously) here:

http://cassci.datastax.com/job/scratch_mambocab-fix_cqlsh/

Assigning [~Stefania] for now, since she's already looked at 10289, but feel free to reassign.",cqlsh,['Legacy/Tools'],CASSANDRA,Sub-task,Normal,2015-09-29 21:49:17,3
12901266,Minor random optimizations,"Sorry for the somewhat vague summary, but while doing some quick profiling on the plane, I noticed 3 places which could be slightly and trivially improved (I don't mean by that that they are the only 3 places that can be improved, I just semi-randomly looked at those), so I've pushed 3 commits for those [here|https://github.com/pcmanus/cassandra/commits/minor_optims]. There is no particular link between the 3 commits but they are trivial enough that I don't have the courage to open 3 tickets. Each commit description should provide enough informations on what each commit is about so I won't repeat it here.",performance,[],CASSANDRA,Improvement,Low,2015-09-29 09:36:22,2
12901262,Specialize MultiCBuilder when building a single clustering,"{{MultiCBuilder}} is used to build the {{Clustering}} and {{Slice.Bound}} used by  queries. As the name implies, it's able to build multiple {{Clustering}}/{{Slice.Bound}} for when we have {{IN}}, but most queries don't use {{IN}} and in this (frequent) case, {{MultiCBuilder}} creates quite a bit more objects that would be necessary (it creates 2 lists for its {{elementsList}}, then a {{CBuilder}} and a {{BTreeSet.Builder}} (even though we know the resulting set will have only one element in this case)). Without being huge, this does show up as non entirely negligible when profiling some simple stress.

We can easily know if the query has a {{IN}} and so we can know when only a single {{Clustering}}/{{Slice.Bound}} is built, and we can specialize the implementation in that case to be less wasteful.",performance,[],CASSANDRA,Improvement,Low,2015-09-29 09:12:07,2
12895551,Windows dtest 3.0: replication_test fails on Windows,"Looks like there's a regex failure taking place: [link|https://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/69/testReport/junit/replication_test/SnitchConfigurationUpdateTest/test_rf_collapse_gossiping_property_file_snitch_multi_dc/]

Rack changes appear to be working but not being picked up by test checking.",windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-09-22 19:01:41,3
12864258,Stress should exit with non-zero status after failure,"Currently, stress always exits with sucess status, even if after a failure. In order to be able to rely on stress exit status during dtests it would be nice if it exited with a non-zero status after failures.",lhf stress,['Legacy/Tools'],CASSANDRA,Improvement,Low,2015-09-15 13:21:58,3
12863719,cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_all_datatypes_read fails locally,"I get this failure on my box with TZ at GMT+08:
{code}
======================================================================
FAIL: test_all_datatypes_read (cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/stefi/git/cstar/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 674, in test_all_datatypes_read
    self.assertCsvResultEqual(self.tempfile.name, results)
  File ""/home/stefi/git/cstar/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 137, in assertCsvResultEqual
    raise e
AssertionError: Element counts were not equal:
First has 1, Second has 0:  ['ascii', '1099511627776', '0xbeef', 'True', '3.140000000000000124344978758017532527446746826171875', '2.444', '1.1', '127.0.0.1', '25', '\xe3\x83\xbd(\xc2\xb4\xe3\x83\xbc\xef\xbd\x80)\xe3\x83\x8e', '2005-07-14 12:30:00', '30757c2c-584a-11e5-b2d0-9cebe804ecbe', '2471e7de-41e4-478f-a402-e99ed779be76', 'asdf', '36893488147419103232']
First has 0, Second has 1:  ['ascii', '1099511627776', '0xbeef', 'True', '3.140000000000000124344978758017532527446746826171875', '2.444', '1.1', '127.0.0.1', '25', '\xe3\x83\xbd(\xc2\xb4\xe3\x83\xbc\xef\xbd\x80)\xe3\x83\x8e', '2005-07-14 04:30:00', '30757c2c-584a-11e5-b2d0-9cebe804ecbe', '2471e7de-41e4-478f-a402-e99ed779be76', 'asdf', '36893488147419103232']
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-8VAvBl
dtest: DEBUG: Importing from csv file: /tmp/tmpGwW8yB
dtest: WARNING: Mismatch at index: 10
dtest: WARNING: Value in csv: 2005-07-14 12:30:00
dtest: WARNING: Value in result: 2005-07-14 04:30:00
--------------------- >> end captured logging << ---------------------
{code}",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2015-09-13 23:53:07,3
12862261,Make DTCS work well with old data,"Operational tasks become incredibly expensive if you keep around a long timespan of data with DTCS - with default settings and 1 year of data, the oldest window covers about 180 days. Bootstrapping a node with vnodes with this data layout will force cassandra to compact very many sstables in this window.

We should probably put a cap on how big the biggest windows can get. We could probably default this to something sane based on max_sstable_age (ie, say we can reasonably handle 1000 sstables per node, then we can calculate how big the windows should be to allow that)",dtcs,['Local/Compaction'],CASSANDRA,Sub-task,Normal,2015-09-07 16:19:41,0
12862172,Do STCS in DTCS-windows,"To avoid constant recompaction of files in big ( > max threshold) DTCS windows, we should do STCS of those files.

Patch here: https://github.com/krummas/cassandra/commits/marcuse/dtcs_stcs",dtcs,['Local/Compaction'],CASSANDRA,Sub-task,Normal,2015-09-07 08:46:52,0
12862005,BATCH statement is broken in cqlsh,"BEGIN BATCH .... APPLY BATCH is not parsed correctly.

Steps:
{code}
CREATE KEYSPACE Excelsior  WITH REPLICATION={'class':'SimpleStrategy','replication_factor':1};
CREATE TABLE excelsior.data (id int primary key);
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0); APPLY BATCH ;
{code}
Error
{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 0:-1 mismatched input '<EOF>' expecting K_APPLY"">
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:0 no viable alternative at input 'APPLY' ([APPLY]...)"">
{code}
While 
{code}
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0)  APPLY BATCH ;
{code}
without *;* after insert works.

Consequently neither
{code}
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0);INSERT INTO excelsior.data (id) VALUES (0); APPLY BATCH ;
{code}
Error:
{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 0:-1 mismatched input '<EOF>' expecting K_APPLY"">
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:0 no viable alternative at input 'APPLY' ([APPLY]...)"">
{code}
nor
{code}
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0);INSERT INTO excelsior.data (id) VALUES (0) APPLY BATCH ;
{code}
Error
{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 0:-1 mismatched input '<EOF>' expecting K_APPLY"">
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:43 missing EOF at 'APPLY' (...(id) VALUES (0) [APPLY] BATCH...)"">
{code}
works.

It was OK in 2.2.0 and 3.0 beta 1.
3.0-beta2-tentative also affected.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2015-09-05 06:03:48,3
12861398,Incremental repairs not working as expected with DTCS,"HI,
we are ingesting data 6 million records every 15 mins into one DTCS table and relaying on Cassandra for purging the data.Table Schema given below, Issue 1: we are expecting to see table sstable created on day d1 will not be compacted after d1 how we are not seeing this, how ever i see some data being purged at random intervals
Issue 2: when we run incremental repair using ""nodetool repair keyspace table -inc -pr"" each sstable is splitting up to multiple smaller SStables and increasing the total storage.This behavior is same running repairs on any node and any number of times
There are mutation drop's in the cluster

Table:
{code}
CREATE TABLE TableA (
    F1 text,
    F2 int,
    createts bigint,
    stats blob,
    PRIMARY KEY ((F1,F2), createts)
) WITH CLUSTERING ORDER BY (createts DESC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'min_threshold': '12', 'max_sstable_age_days': '1', 'base_time_seconds': '50', 'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.0
    AND default_time_to_live = 93600
    AND gc_grace_seconds = 3600
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
{code}

Thanks",dtcs,[],CASSANDRA,Bug,Normal,2015-09-02 21:16:31,0
12860295,Periodically attempt to delete failed snapshot deletions on Windows,"The changes in CASSANDRA-9658 leave us in a position where a node on Windows will have to be restarted to clear out snapshots that cannot be deleted at request time due to sstables still being mapped, thus preventing deletions of hard links. A simple periodic task to categorize failed snapshot deletions and retry them would help prevent node disk utilization from growing unbounded by snapshots as compaction will eventually make these snapshot files deletable.

Given that hard links to files in NTFS don't take up any extra space on disk so long as the original file still exists, the only limitation for users from this approach will be the inability to 'move' a snapshot file to another drive share. They will be copyable, however, so it's a minor platform difference.

This goes directly against the goals of CASSANDRA-8271 and will likely be built on top of that code. Until such time as we get buffered performance in-line with memory-mapped, this is an interim necessity for production roll-outs.",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2015-08-28 15:24:34,6
12859554,decommissioned_wiped_node_can_join_test fails on Jenkins,"This test passes locally but reliably fails on Jenkins. It seems after we restart node4, it is unable to Gossip with other nodes:

{code}
INFO  [HANDSHAKE-/127.0.0.2] 2015-08-27 06:50:42,778 OutboundTcpConnection.java:494 - Handshaking version with /127.0.0.2
INFO  [HANDSHAKE-/127.0.0.1] 2015-08-27 06:50:42,778 OutboundTcpConnection.java:494 - Handshaking version with /127.0.0.1
INFO  [HANDSHAKE-/127.0.0.3] 2015-08-27 06:50:42,778 OutboundTcpConnection.java:494 - Handshaking version with /127.0.0.3
ERROR [main] 2015-08-27 06:51:13,785 CassandraDaemon.java:635 - Exception encountered during startup
java.lang.RuntimeException: Unable to gossip with any seeds
        at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1342) ~[main/:na]
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:518) ~[main/:na]
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:763) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:687) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:570) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:320) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:516) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:622) [main/:na]
WARN  [StorageServiceShutdownHook] 2015-08-27 06:51:13,799 Gossiper.java:1453 - No local state or state is in silent shutdown, not announcing shutdown
{code}

It seems both the addresses and port number of the seeds are correct so I don't think the problem is the Amazon private addresses but I might be wrong. 

It's also worth noting that the first time the node starts up without problems. The problem only occurs during a restart.",CI,"['CI', 'Legacy/Distributed Metadata', 'Legacy/Testing']",CASSANDRA,Sub-task,Normal,2015-08-27 09:28:38,3
12858744,Windows dtest 3.0: cqlsh_tests\cqlsh_tests.py:TestCqlsh.test_describe fails,Lists differ error. Looks to be \r vs. \r\n related.,Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-25 20:30:48,6
12858740,Windows dtest 3.0: offline_tools_test.py:TestOfflineTools.sstablelevelreset_test fails,"CI error:
{noformat}
  File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-3.0_dtest_win32\cassandra-dtest\tools.py"", line 252, in wrapped
    f(obj)
  File ""D:\jenkins\workspace\cassandra-3.0_dtest_win32\cassandra-dtest\offline_tools_test.py"", line 75, in sstablelevelreset_test
    self.assertTrue(max(final_levels) == 0)
  File ""C:\tools\python2\lib\unittest\case.py"", line 422, in assertTrue
    raise self.failureException(msg)
'False is not true\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-epkti6\ndtest: DEBUG: \ndtest: DEBUG: [1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1]\ndtest: DEBUG: [1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1]\n--------------------- >> end captured logging << ---------------------'
{noformat}

Local failure:
{noformat}
FAIL: sstablelevelreset_test (offline_tools_test.TestOfflineTools)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""c:\src\cassandra-dtest\tools.py"", line 252, in wrapped
    f(obj)
  File ""c:\src\cassandra-dtest\offline_tools_test.py"", line 32, in sstablelevelreset_test
    self.assertIn(""ColumnFamily not found: keyspace1/standard1"", error)
AssertionError: 'ColumnFamily not found: keyspace1/standard1' not found in 'Exception in thread ""main"" java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.config.DatabaseDescriptor\r\n\tat org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:57)\r\n\tat org.apache.cassandra.tools.SSTableLevelResetter.main(SSTableLevelResetter.java:104)\r\n'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: c:\temp\dtest-3kgttu
{noformat}

Failure is consistent.",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-25 20:20:23,6
12857231,Windows dtest 3.0: consistent_reads_after_write_test (materialized_views_test.TestMaterializedViewsConsistency) timing out,"Test times out and also throws errors, ending in:
{noformat}
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Python27\lib\multiprocessing\forking.py"", line 380, in main
On 0; match: 0; extra: 0; missing: 0    prepare(preparation_data)
  File ""C:\Python27\lib\multiprocessing\forking.py"", line 488, in prepare
    assert main_name not in sys.modules, main_name
AssertionError: __main__
{noformat}

Reproducible locally, happening on CI as well",Windows,"['Feature/Materialized Views', 'Test/dtest/python']",CASSANDRA,Sub-task,Normal,2015-08-19 14:56:43,6
12856956,Windows utest 2.2: LeveledCompactionStrategyTest.testGrouperLevels flaky,"{noformat}
junit.framework.AssertionFailedError
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testGrouperLevels(LeveledCompactionStrategyTest.java:131)
{noformat}

[Test is flaky on Windows|http://cassci.datastax.com/view/cassandra-2.2/job/cassandra-2.2_utest_win32/lastCompletedBuild/testReport/org.apache.cassandra.db.compaction/LeveledCompactionStrategyTest/testGrouperLevels/history/]

[Test is consistent on linux|http://cassci.datastax.com/view/cassandra-2.2/job/cassandra-2.2_utest/lastCompletedBuild/testReport/org.apache.cassandra.db.compaction/LeveledCompactionStrategyTest/testGrouperLevels/history/]

Doesn't repro locally.",Windows,"['Legacy/Testing', 'Local/Compaction']",CASSANDRA,Bug,Normal,2015-08-18 16:48:45,6
12856787,Refuse to start and print txn log information in case of disk corruption,"Transaction logs were introduced by CASSANDRA-7066 and are read during start-up. In case of file system errors, such as disk corruption, we currently log a panic error and leave the sstable files and transaction logs as they are; this is to avoid rolling back a transaction (i.e. deleting files) by mistake.

We should instead look at the {{disk_failure_policy}} and refuse to start unless the failure policy is {{ignore}}. 

We should also consider stashing files that cannot be read during startup, either transaction logs or sstables, by moving them to a dedicated sub-folder. 

",doc-impacting,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2015-08-18 00:42:20,3
12856742,Windows dtest 3.0: ttl_test.py failures,"ttl_test.py:TestTTL.update_column_ttl_with_default_ttl_test2
ttl_test.py:TestTTL.update_multiple_columns_ttl_test
ttl_test.py:TestTTL.update_single_column_ttl_test

Errors locally are different than CI from yesterday. Yesterday on CI we have timeouts and general node hangs. Today on all 3 tests when run locally I see:
{noformat}
Traceback (most recent call last):
  File ""c:\src\cassandra-dtest\dtest.py"", line 532, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
AssertionError: Unexpected error in node1 node log: ['ERROR [main] 2015-08-17 16:53:43,120 NoSpamLogger.java:97 - This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr']
{noformat}

This traces back to the commit for CASSANDRA-7066 today by [~Stefania] and [~benedict].  Stefania - care to take this ticket and also look further into whether or not we're going to have issues with 7066 on Windows? That error message certainly *sounds* like it's not a good thing.",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Sub-task,Normal,2015-08-17 20:59:01,3
12856736,Windows dtest 3.0: TestScrub and TestScrubIndexes failures,"scrub_test.py:TestScrub.test_standalone_scrub
scrub_test.py:TestScrub.test_standalone_scrub_essential_files_only
scrub_test.py:TestScrubIndexes.test_standalone_scrub

Somewhat different messages between CI and local, but consistent on env. Locally, I see:
{noformat}
dtest: DEBUG: ERROR 20:41:20 This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr
{noformat}

Consistently fails, both on CI and locally.

",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Sub-task,Normal,2015-08-17 20:45:05,6
12856729,Windows dtest 3.0: TestRepair multiple failures,"repair_test.py:TestRepair.dc_repair_test
repair_test.py:TestRepair.local_dc_repair_test
repair_test.py:TestRepair.simple_parallel_repair_test
repair_test.py:TestRepair.simple_sequential_repair_test

All failing w/the following error:
{noformat}
File ""D:\Python27\lib\unittest\case.py"", line 358, in run
    self.tearDown()
  File ""D:\jenkins\workspace\cassandra-3.0_dtest_win32\cassandra-dtest\dtest.py"", line 532, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
""Unexpected error in node3 node log: ['ERROR [STREAM-IN-/127.0.0.1] 2015-08-17 00:41:09,426 StreamSession.java:520 - [Stream #a69fc140-4478-11e5-a8ae-4f8718583077] Streaming error occurred java.io.IOException: An existing connection was forcibly closed by the remote host \\tat sun.nio.ch.SocketDispatcher.read0(Native Method) ~[na:1.8.0_45] \\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43) ~[na:1.8.0_45] \\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.8.0_45] \\tat sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.8.0_45] \\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[na:1.8.0_45] \\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:53) ~[main/:na] \\tat org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:261) ~[main/:na] \\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]']\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-3kmbjb\ndtest: DEBUG: Starting cluster..\ndtest: DEBUG: Inserting data...\ndtest: DEBUG: Checking data on node3...\ndtest: DEBUG: Checking data on node1...\ndtest: DEBUG: Checking data on node2...\ndtest: DEBUG: starting repair...\ndtest: DEBUG: Repair time: 5.37800002098\ndtest: DEBUG: removing ccm cluster test at: d:\\temp\\dtest-3kmbjb\ndtest: DEBUG: clearing ssl stores from [d:\\temp\\dtest-3kmbjb] directory\n--------------------- >> end captured logging << ---------------------""
{noformat}

Failure history: [consistent|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/17/testReport/repair_test/TestRepair/dc_repair_test/history/]

Env: ci and local",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-17 20:21:35,6
12856716,Windows dtest 3.0: TestOfflineTools failures,"offline_tools_test.py:TestOfflineTools.sstablelevelreset_test
offline_tools_test.py:TestOfflineTools.sstableofflinerelevel_test

Both tests fail with the following:
{noformat}
Traceback (most recent call last):
  File ""c:\src\cassandra-dtest\dtest.py"", line 532, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
AssertionError: Unexpected error in node1 node log: ['ERROR [main] 2015-08-17 15:55:05,060 NoSpamLogger.java:97 - This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr']
{noformat}

Failure history: [consistent|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/17/testReport/junit/jmx_test/TestJMX/netstats_test/history/]

Env: ci and local",Windows,['Legacy/Tools'],CASSANDRA,Sub-task,Normal,2015-08-17 19:56:50,6
12856697,Windows dtest 3.0: incremental_repair_test.py:TestIncRepair.sstable_repairedset_test fails,"{noformat}
File ""D:\Python27\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-3.0_dtest_win32\cassandra-dtest\incremental_repair_test.py"", line 165, in sstable_repairedset_test
    self.assertGreaterEqual(len(uniquematches), 2)
  File ""D:\Python27\lib\unittest\case.py"", line 948, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""D:\Python27\lib\unittest\case.py"", line 410, in fail
    raise self.failureException(msg)
'0 not greater than or equal to 2\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-pq7lpx\ndtest: DEBUG: []\n--------------------- >> end captured logging << ---------------------'
{noformat}

Failure history: [consistent|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/17/testReport/junit/hintedhandoff_test/TestHintedHandoffConfig/hintedhandoff_dc_disabled_test/history/]

Env: both CI and local",Windows,['Legacy/Streaming and Messaging'],CASSANDRA,Sub-task,Normal,2015-08-17 18:42:09,6
12856691,Windows dtest 3.0: HintedHandoff tests failing,"hintedhandoff_test.py:TestHintedHandoffConfig.hintedhandoff_dc_disabled_test
hintedhandoff_test.py:TestHintedHandoffConfig.hintedhandoff_dc_reenabled_test
hintedhandoff_test.py:TestHintedHandoffConfig.hintedhandoff_disabled_test
hintedhandoff_test.py:TestHintedHandoffConfig.hintedhandoff_enabled_test
hintedhandoff_test.py:TestHintedHandoffConfig.nodetool_test

All are failing with some variant of the following:
{noformat}
File ""D:\Python27\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-3.0_dtest_win32\cassandra-dtest\hintedhandoff_test.py"", line 130, in hintedhandoff_dc_disabled_test
    self.assertEqual('Hinted handoff is running\nData center dc1 is disabled', res.rstrip())
  File ""D:\Python27\lib\unittest\case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""D:\Python27\lib\unittest\case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""'Hinted handoff is running\\nData center dc1 is disabled' != 'Starting NodeTool\\r\\nHinted handoff is running\\r\\nData center dc1 is disabled'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-pddrcf\n--------------------- >> end captured logging << ---------------------""
{noformat}

Failure history: consistent for all jobs

Env: Both ci and local",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-17 18:25:31,6
12856683,Improve concurrency in CompactionStrategyManager,"Continue discussion from CASSANDRA-9882.

CompactionStrategyManager(WrappingCompactionStrategy for <3.0) tracks SSTable changes mainly for separating repaired / unrepaired SSTables (+ LCS manages level).

This is blocking operation, and can lead to block of flush etc. when determining next background task takes longer.

Explore the way to mitigate this concurrency issue.",compaction lcs,[],CASSANDRA,Improvement,Normal,2015-08-17 18:06:51,0
12856659,Fix dtests on 3.0 branch on Windows,Parent ticket to track subtasks for dtest failures on Windows on the 3.0 branch,Windows,['Test/dtest/python'],CASSANDRA,Bug,Normal,2015-08-17 17:08:39,6
12856209,Windows dtest 2.2: rebuild_test.py:TestRebuild.simple_rebuild_test,"Error text:
{noformat}
concurrent rebuild should not be allowed
File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\rebuild_test.py"", line 87, in simple_rebuild_test
    self.fail(""concurrent rebuild should not be allowed"")
  File ""C:\tools\python2\lib\unittest\case.py"", line 410, in fail
    raise self.failureException(msg)
'concurrent rebuild should not be allowed\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-rltloj\n--------------------- >> end captured logging << ---------------------'
{noformat}

[Failure history|http://cassci.datastax.com/view/cassandra-2.2/job/cassandra-2.2_dtest_win32/61/testReport/rebuild_test/TestRebuild/simple_rebuild_test/history/] (Consistent)

Env: Both CI and local",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-14 16:34:53,6
12856207,Windows dtest 2.2: json_test.py:JsonFullRowInsertSelect.pkey_requirement_test,"Error text: 
{noformat}
Doctest failed! Captured output:
**********************************************************************
Line 25, in pkey_requirement_test
Failed example:
    cqlsh_err_print('''INSERT INTO primitive_type_test JSON '{""col1"": ""bar""}' ''')
Expected:
    <stdin>:2:InvalidRequest: code=2200 [Invalid query] message=""Invalid null value in condition for column key1""
    <BLANKLINE>
Got:
    <stdin>:2:InvalidRequest: code=2200 [Invalid query] message=""Invalid null value in condition for column key1""
    <BLANKLINE>

-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: d:\temp\dtest-fgbxha
--------------------- >> end captured logging << ---------------------
{noformat}
and:
{noformat}
File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\json_test.py"", line 1252, in pkey_requirement_test
    run_func_docstring(tester=self, test_func=self.pkey_requirement_test)
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\json_test.py"", line 181, in run_func_docstring
    raise RuntimeError(""Doctest failed! Captured output:\n{}"".format(test_output_capturer.content))
'Doctest failed! Captured output:\n**********************************************************************\nLine 25, in pkey_requirement_test\nFailed example:\n    cqlsh_err_print(\'\'\'INSERT INTO primitive_type_test JSON \'{""col1"": ""bar""}\' \'\'\')\nExpected:\n    <stdin>:2:InvalidRequest: code=2200 [Invalid query] message=""Invalid null value in condition for column key1""\n    <BLANKLINE>\nGot:\n    <stdin>:2:InvalidRequest: code=2200 [Invalid query] message=""Invalid null value in condition for column key1""\r\n    <BLANKLINE>\n\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-fgbxha\n--------------------- >> end captured logging << ---------------------'
{noformat}

[Failure history|http://cassci.datastax.com/view/cassandra-2.2/job/cassandra-2.2_dtest_win32/61/testReport/json_test/JsonFullRowInsertSelect/pkey_requirement_test/history/] (Consistent, introduced in build 58)

Env: Both CI and Local.",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-14 16:31:59,6
12856203,Windows dtest 2.2: sstablesplit_test.py:TestSSTableSplit.split_test,"Error text:
{noformat}
 File ""C:\tools\python2\lib\unittest\case.py"", line 358, in run
    self.tearDown()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\dtest.py"", line 513, in tearDown
    self._cleanup_cluster()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\dtest.py"", line 212, in _cleanup_cluster
    self.cluster.remove()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\ccm\ccmlib\cluster.py"", line 223, in remove
    common.rmdirs(self.get_path())
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\ccm\ccmlib\common.py"", line 156, in rmdirs
    shutil.rmtree(u""\\\\?\\"" + path)
  File ""C:\tools\python2\lib\shutil.py"", line 247, in rmtree
    rmtree(fullname, ignore_errors, onerror)
  File ""C:\tools\python2\lib\shutil.py"", line 247, in rmtree
    rmtree(fullname, ignore_errors, onerror)
  File ""C:\tools\python2\lib\shutil.py"", line 252, in rmtree
    onerror(os.remove, fullname, sys.exc_info())
  File ""C:\tools\python2\lib\shutil.py"", line 250, in rmtree
    os.remove(fullname)
""[Error 5] Access is denied: u'\\\\\\\\?\\\\d:\\\\temp\\\\dtest-zsl_4c\\\\test\\\\node1\\\\commitlogs\\\\CommitLog-5-1439489973883.log'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-zsl_4c\ndtest: DEBUG: Run stress to insert data\ndtest: DEBUG: Compact sstables.\ndtest: DEBUG: Number of sstables after compaction: 1\ndtest: DEBUG: Run sstablesplit\ndtest: DEBUG: Original sstable and sizes before split: [('d:\\\\temp\\\\dtest-zsl_4c\\\\test\\\\node1\\\\data\\\\keyspace1\\\\standard1-e596155041e611e5b337390aedd65e00\\\\la-25-big-Data.db', 281000000L)]\ndtest: DEBUG: Number of sstables after split: 27. expected 26.0\ndtest: DEBUG: Compact sstables.\ndtest: DEBUG: Number of sstables after compaction: 1\ndtest: DEBUG: Run sstablesplit\ndtest: DEBUG: Original sstable and sizes before split: [('d:\\\\temp\\\\dtest-zsl_4c\\\\test\\\\node1\\\\data\\\\keyspace1\\\\standard1-e596155041e611e5b337390aedd65e00\\\\la-54-big-Data.db', 281000000L)]\ndtest: DEBUG: Number of sstables after split: 27. expected 26.0\ndtest: DEBUG: Run stress to ensure data is readable\ndtest: DEBUG: removing ccm cluster test at: d:\\temp\\dtest-zsl_4c\n--------------------- >> end captured logging << -----
{noformat}

[Failure History|http://cassci.datastax.com/view/cassandra-2.2/job/cassandra-2.2_dtest_win32/61/testReport/sstablesplit_test/TestSSTableSplit/split_test/history/]

Env: CI only. Passes locally.",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-08-14 16:25:04,6
12856188,cqlsh HELP SELECT_EXPR gives outdated incorrect information,"Within cqlsh, the HELP SELECT_EXPR states that COUNT is the only function supported by CQL.

It is missing a description of the SUM, AVG, MIN, and MAX built in functions.

It should probably also mention that user defined functions can be invoked via SELECT.

The outdated text is in pylib/cqlshlib/helptopics.py under def help_select_expr",cqlsh lhf,['Legacy/Tools'],CASSANDRA,Bug,Low,2015-08-14 15:29:04,4
12855904,Bring cqlsh into PEP8 compliance,"In order for us to begin running flake8 against cqlsh.py in CI, it would be helpful if it were already PEP8 complaint, with the exception of using 120 character lines.",cqlsh,[],CASSANDRA,Bug,Low,2015-08-13 16:31:34,4
12853943,Parse Error on CQLSH describe when describing a table with a non-reserved keyword name,"If a table with the name 'map' is created, the describe command will return ""Improper describe command"", indicating a parse error.

I believe this is because 'map' is a keyword when referring to a type, but cqlshlib's parser identifies it as a keyword in the context of a describe command, even though it is not. This same mismatch likely applies to other CQL keywords as well.

The cqlshlib and C* CQL parsers should treat keywords the same where possible.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2015-08-10 19:30:49,3
12853915,Windows utest 3.0: ColumnFamilyStoreTest.testScrubDataDirectories failing,"{noformat}
    [junit] Testcase: testScrubDataDirectories(org.apache.cassandra.db.ColumnFamilyStoreTest):  Caused an ERROR
    [junit] java.nio.file.AccessDeniedException: build\test\cassandra\data;0\ColumnFamilyStoreTest1\Standard1-a0d5fa503f8b11e58dec831ef068609c\ma-7-big-Index.db
    [junit] FSWriteError in build\test\cassandra\data;0\ColumnFamilyStoreTest1\Standard1-a0d5fa503f8b11e58dec831ef068609c\ma-7-big-Index.db
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:135)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:152)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:147)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:556)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testScrubDataDirectories(ColumnFamilyStoreTest.java:533)
    [junit] Caused by: java.nio.file.AccessDeniedException: build\test\cassandra\data;0\ColumnFamilyStoreTest1\Standard1-a0d5fa503f8b11e58dec831ef068609c\ma-7-big-Index.db
    [junit]     at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
    [junit]     at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
    [junit]     at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    [junit]     at java.nio.file.Files.delete(Files.java:1126)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:129)
{noformat}

Test has never passed on Windows ([history|http://cassci.datastax.com/view/trunk/job/trunk_utest_win32/lastCompletedBuild/testReport/org.apache.cassandra.db/ColumnFamilyStoreTest/testScrubDataDirectories/history/]).

[~stefania_alborghetti]: Looks like this is your test. Care to take this?",Windows,"['Legacy/Testing', 'Legacy/Tools']",CASSANDRA,Sub-task,Normal,2015-08-10 18:17:06,3
12853912,Windows utest 3.0: TransactionLogsTest failure,"{noformat}
    [junit] Testcase: testUntrack(org.apache.cassandra.db.lifecycle.TransactionLogsTest):       FAILED
    [junit] build\test\cassandra\data;0\TransactionLogsTest\mockcf4-6742d2103f8911e58737831ef068609c\ma-1-big-Data.db
    [junit] junit.framework.AssertionFailedError: build\test\cassandra\data;0\TransactionLogsTest\mockcf4-6742d2103f8911e58737831ef068609c\ma-1-big-Data.db
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest.assertFiles(TransactionLogsTest.java:575)
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest.testUntrack(TransactionLogsTest.java:215)
    [junit]
    [junit]
    [junit] Testcase: testAbortOnlyNew(org.apache.cassandra.db.lifecycle.TransactionLogsTest):  FAILED
    [junit] build\test\cassandra\data;0\TransactionLogsTest\mockcf5-67456a203f8911e58737831ef068609c\ma-0-big-Data.db
    [junit] junit.framework.AssertionFailedError: build\test\cassandra\data;0\TransactionLogsTest\mockcf5-67456a203f8911e58737831ef068609c\ma-0-big-Data.db
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest.assertFiles(TransactionLogsTest.java:575)
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest.testAbortOnlyNew(TransactionLogsTest.java:312)
    [junit]
    [junit]
    [junit] Testcase: testNoPrepare(org.apache.cassandra.db.lifecycle.TransactionLogsTest):     FAILED
    [junit] build\test\cassandra\data;0\TransactionLogsTest\mockcf13-675681203f8911e58737831ef068609c\ma-1-big-Data.db
    [junit] junit.framework.AssertionFailedError: build\test\cassandra\data;0\TransactionLogsTest\mockcf13-675681203f8911e58737831ef068609c\ma-1-big-Data.db
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest.assertFiles(TransactionLogsTest.java:575)
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest.access$200(TransactionLogsTest.java:56)
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest$TxnTest$Transaction.assertAborted(TransactionLogsTest.java:143)
    [junit]     at org.apache.cassandra.db.lifecycle.TransactionLogsTest$TxnTest.assertAborted(TransactionLogsTest.java:191)
    [junit]     at org.apache.cassandra.utils.concurrent.AbstractTransactionalTest.testNoPrepare(AbstractTransactionalTest.java:40)
{noformat}

taking the testPrepare case as an example: looks like it never passed on Windows ([history|http://cassci.datastax.com/view/trunk/job/trunk_utest_win32/lastCompletedBuild/testReport/org.apache.cassandra.db.lifecycle/TransactionLogsTest/testPrepare/history/])

[~stefania_alborghetti]: annotate points to you on these tests. Care to take a look at this?",Windows,"['Legacy/Local Write-Read Paths', 'Legacy/Testing']",CASSANDRA,Sub-task,Normal,2015-08-10 18:04:37,3
12853898,Windows 3.0 utest parity,"21 failures on Windows, 10 on linux as of 2015-08-10. Ticket to track subtasks.",Windows,['Legacy/Testing'],CASSANDRA,Bug,Normal,2015-08-10 17:27:28,6
12851491,Deprecated Repair tests fail on windows,"After fixing the jolokia agent problem, we can now see that these tests are genuinely failing. Every test follows the same basic pattern:

{code}

    def force_repair_async_1_test(self, ):
        """"""
        test forceRepairAsync(String keyspace, boolean isSequential,
                              Collection<String> dataCenters,
                              Collection<String> hosts,
                              boolean primaryRange, boolean fullRepair, String... columnFamilies)
        """"""
        opt = self._deprecated_repair_jmx(""forceRepairAsync(java.lang.String,boolean,java.util.Collection,java.util.Collection,boolean,boolean,[Ljava.lang.String;)"",
                                          ['ks', True, [], [], False, False, [""cf""]])
        self.assertEqual(opt[""parallelism""], ""sequential"", opt)
        self.assertEqual(opt[""primary_range""], ""false"", opt)
        self.assertEqual(opt[""incremental""], ""true"", opt)
        self.assertEqual(opt[""job_threads""], ""1"", opt)
        self.assertEqual(opt[""data_centers""], ""[]"", opt)
        self.assertEqual(opt[""hosts""], ""[]"", opt)
        self.assertEqual(opt[""column_families""], ""[cf]"", opt)
{code}
In each test, the response for {{opt[""parallelism""]}} is incorrect. [~yukim] wrote these tests, and may be able to shed light on what's going on.",windows,[],CASSANDRA,Sub-task,Normal,2015-08-04 20:14:06,4
12851126,cqlsh should have DESCRIBE MATERIALIZED VIEW,"cqlsh doesn't currently produce describe output that can be used to recreate a MV. Needs to add a new {{DESCRIBE MATERIALIZED VIEW}} command, and also add to {{DESCRIBE KEYSPACE}}.",doc-impacting materializedviews,"['Feature/Materialized Views', 'Legacy/Tools']",CASSANDRA,Improvement,Normal,2015-08-03 15:22:22,3
12849577,Potential race caused by async cleanup of transaction log files,"There seems to be a potential race in the cleanup of transaction log files, introduced in CASSANDRA-7066
It's pretty hard to trigger on trunk, but it's possible to hit it via {{o.a.c.db.SecondaryIndexTest#testCreateIndex}} 

That test creates an index, then removes it to check that the removal is correctly recorded, then adds the index again to assert that it gets rebuilt from the existing data. 
The removal causes the SSTables of the index CFS to be dropped, which is a transactional operation and so writes a transaction log. When the drop is completed and the last reference to an SSTable is released, the cleanup of the transaction log is scheduled on the periodic tasks executor. The issue is that re-creating the index re-creates the index CFS. When this happens, it's possible for the cleanup of the txn log to have not yet happened. If so, the initialization of the CFS attempts to read the log to identify any orphaned temporary files. The cleanup can happen between the finding the log file and reading it's contents, which results in a {{NoSuchFileException}}

{noformat}
[junit] java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit] java.lang.RuntimeException: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:620)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionFile.getTrackedFiles(TransactionLogs.java:190)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionData.getTemporaryFiles(TransactionLogs.java:338)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs.getTemporaryFiles(TransactionLogs.java:739)
[junit]     at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getTemporaryFiles(LifecycleTransaction.java:541)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.getFilter(Directories.java:652)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:641)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:606)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:351)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:313)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:511)
[junit]     at org.apache.cassandra.index.internal.CassandraIndexer.addIndexedColumn(CassandraIndexer.java:115)
[junit]     at org.apache.cassandra.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:265)
[junit]     at org.apache.cassandra.db.SecondaryIndexTest.testIndexCreate(SecondaryIndexTest.java:467)
[junit] Caused by: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
[junit]     at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:361)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:407)
[junit]     at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
[junit]     at java.nio.file.Files.newInputStream(Files.java:152)
[junit]     at java.nio.file.Files.newBufferedReader(Files.java:2784)
[junit]     at java.nio.file.Files.readAllLines(Files.java:3202)
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:616)
[junit] 
[junit] 
[junit] Test org.apache.cassandra.db.SecondaryIndexTest FAILED
{noformat}
",benedict-to-commit,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2015-07-28 10:31:43,3
12849406,Windows dtest: cqlsh_tests\cqlsh_copy_tests.py failing with Permission denied,"Last run shows failure on all the cqlsh copy dtests with messages similar to the following:
{noformat}
05:39:07 ======================================================================
05:39:07 ERROR: test_source_copy_round_trip (cqlsh_copy_tests.CqlshCopyTest)
05:39:07 ----------------------------------------------------------------------
05:39:07 Traceback (most recent call last):
05:39:07   File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\cqlsh_tests\cqlsh_copy_tests.py"", line 789, in test_source_copy_round_trip
05:39:07     with open(commandfile.name, 'w') as commands:
05:39:07 IOError: [Errno 13] Permission denied: 'd:\\temp\\tmpawtlxf'
05:39:07 -------------------- >> begin captured logging << --------------------
05:39:07 dtest: DEBUG: removing ccm cluster test at: d:\temp\dtest-9dklfv
05:39:07 dtest: DEBUG: cluster ccm directory: d:\temp\dtest-98gttb
05:39:07 dtest: DEBUG: Exporting to csv file: d:\temp\tmppok0qa
05:39:07 --------------------- >> end captured logging << ---------------------
{noformat}

Jake did quite a bit of work around this on CASSANDRA-9795 but I'm not certain this is a related issue so opening as a new ticket and linking.",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Low,2015-07-27 19:31:08,6
12849073,Anticompaction can mix old and new data with DTCS in 2.2+,"since CASSANDRA-6851 we group sstables before running anticompaction on them to avoid increasing sstable count.

We should not do this for DTCS as it can mix new and old data.",dtcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-07-25 07:15:54,0
12848945,Windows dtest: snapshot_test.py failures,"System call is linux-specific:

{noformat}
ERROR [CommitLogArchiver:1] 2015-07-24 13:26:39,622 CassandraDaemon.java:181 - Exception in thread Thread[CommitLogArchiver:1,5,main]
java.lang.RuntimeException: java.io.IOException: Exception while executing the command: cp c:\temp\dtest-phi4wd\test\node1\commitlogs\CommitLog-5-1437758795056.log c:  emp     mpkzsvkb/CommitLog-5-1437758795056.log, command error Code: 1, command output: cp: accessing `c:\temp\tmpkzsvkb/CommitLog-5-1437758795056.log': Invalid argument

        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[guava-16.0.jar:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{noformat}",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-07-24 17:28:52,6
12848660,DTCS (maybe other strategies) can block flushing when there are lots of sstables,"MemtableFlushWriter tasks can get blocked by Compaction getNextBackgroundTask.  This is in a wonky cluster with 200k sstables in the CF, but seems bad for flushing to be blocked by getNextBackgroundTask when we are trying to make these new ""smart"" strategies that may take some time to calculate what to do.

{noformat}
""MemtableFlushWriter:21"" daemon prio=10 tid=0x00007ff7ad965000 nid=0x6693 waiting for monitor entry [0x00007ff78a667000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:237)
	- waiting to lock <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1475)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1127)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- <0x0000000743b3ac38> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""MemtableFlushWriter:19"" daemon prio=10 tid=0x00007ff7ac57a000 nid=0x649b waiting for monitor entry [0x00007ff78b8ee000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:237)
	- waiting to lock <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1475)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1127)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""CompactionExecutor:14"" daemon prio=10 tid=0x00007ff7ad359800 nid=0x4d59 runnable [0x00007fecce3ea000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.io.sstable.SSTableReader.equals(SSTableReader.java:628)
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:206)
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:220)
	at com.google.common.collect.ImmutableSet.access$000(ImmutableSet.java:74)
	at com.google.common.collect.ImmutableSet$Builder.build(ImmutableSet.java:531)
	at com.google.common.collect.Sets$1.immutableCopy(Sets.java:606)
	at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:1352)
	at org.apache.cassandra.db.compaction.DateTieredCompactionStrategy.getNextBackgroundSSTables(DateTieredCompactionStrategy.java:88)
	at org.apache.cassandra.db.compaction.DateTieredCompactionStrategy.getNextBackgroundTask(DateTieredCompactionStrategy.java:65)
	- locked <0x00000006fcdbbf00> (a org.apache.cassandra.db.compaction.DateTieredCompactionStrategy)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:72)
	- locked <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

",dtcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-07-23 19:03:44,0
12848381,Windows dtest: fix commitlog_test errors,"{noformat}
======================================================================
FAIL: ignore_failure_policy_test (commitlog_test.TestCommitLog)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\commitlog_test.py"", line 251, in ignore_failure_policy_test
    """""")
AssertionError: (<class 'cassandra.OperationTimedOut'>, <class 'cassandra.WriteTimeout'>) not raised
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: c:\temp\dtest-fzrrz1
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
{noformat}",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Low,2015-07-22 21:08:19,6
12846950,windows - failed testClearEphemeralSnapshots,"Did some juggling to get the state cleared/prepped when I wrote the test but apparently it's intermittently failing with the following:

{noformat}
junit.framework.AssertionFailedError
	at org.apache.cassandra.db.WindowsFailedSnapshotTracker.handleFailedSnapshot(WindowsFailedSnapshotTracker.java:104)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:741)
	at org.apache.cassandra.db.ColumnFamilyStore.clearEphemeralSnapshots(ColumnFamilyStore.java:2343)
	at org.apache.cassandra.db.ColumnFamilyStoreTest.testClearEphemeralSnapshots(ColumnFamilyStoreTest.java:1555)
{noformat}

I assume it's something trivial (having not yet looked into it at all).",Windows,['Legacy/Testing'],CASSANDRA,Bug,Low,2015-07-22 17:24:53,6
12846943,Windows dtest: auth_test failure,"Looks like there's a failure during the abort path on SSTableWriter w/access violation. This didn't show up in CI since there was apparently a missing library on there and auth_test.py was being skipped entirely.

{noformat}
INFO  [MigrationStage:1] 2015-07-22 12:33:33,728 ColumnFamilyStore.java:328 - Initializing system_auth.permissions
ERROR [CompactionExecutor:2] 2015-07-22 12:33:33,745 SSTable.java:261 - Missing component: c:\temp\dtest-n95zao\test\node1\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-tmp-ka-5-Digest.sha1
ERROR [CompactionExecutor:2] 2015-07-22 12:33:33,745 SSTable.java:261 - Missing component: c:\temp\dtest-n95zao\test\node1\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-tmp-ka-5-CompressionInfo.dbERROR [CompactionExecutor:2] 2015-07-22 12:33:33,745 SSTable.java:261 - Missing component: c:\temp\dtest-n95zao\test\node1\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-tmp-ka-5-Summary.db
ERROR [CompactionExecutor:2] 2015-07-22 12:33:33,756 SSTableWriter.java:367 - Failed deleting temp components for c:\temp\dtest-n95zao\test\node1\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-tmp-ka-5
org.apache.cassandra.io.FSWriteError: java.nio.file.FileSystemException: c:\temp\dtest-n95zao\test\node1\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-tmp-ka-5-Data.db: The process cannot access the file because it is being used by another process.

        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:135) ~[main/:na]
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:121) ~[main/:na]
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:108) ~[main/:na]
        at org.apache.cassandra.io.sstable.SSTableWriter.abort(SSTableWriter.java:363) ~[main/:na]
        at org.apache.cassandra.io.sstable.SSTableRewriter.abort(SSTableRewriter.java:236) [main/:na]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220) [main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) [main/:na]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73) [main/:na]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) [main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:236) [main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.nio.file.FileSystemException: c:\temp\dtest-n95zao\test\node1\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-tmp-ka-5-Data.db: The process cannot access the file because it is being used by another process.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86) ~[na:1.8.0_45]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97) ~[na:1.8.0_45]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102) ~[na:1.8.0_45]
        at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269) ~[na:1.8.0_45]
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) ~[na:1.8.0_45]
        at java.nio.file.Files.delete(Files.java:1126) ~[na:1.8.0_45]
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:131) ~[main/:na]
        ... 14 common frames omitted
{noformat}",Windows,['Test/dtest/python'],CASSANDRA,Sub-task,Normal,2015-07-22 16:45:28,6
12846047,trunk pig-test fails,"{noformat}
pig-test:
    [mkdir] Created dir: /var/lib/jenkins/jobs/trunk_pigtest/workspace/build/test/cassandra
    [mkdir] Created dir: /var/lib/jenkins/jobs/trunk_pigtest/workspace/build/test/output
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/var/lib/jenkins/jobs/trunk_pigtest/workspace/build/lib/jars/ant-1.8.3.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.pig.CqlRecordReaderTest
    [junit] Testsuite: org.apache.cassandra.pig.CqlRecordReaderTest Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.799 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.pig.CqlTableDataTypeTest
    [junit] Testsuite: org.apache.cassandra.pig.CqlTableDataTypeTest Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 44.627 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.pig.CqlTableTest
    [junit] 
    [junit] Exception: java.lang.IllegalStateException thrown from the UncaughtExceptionHandler in thread ""cluster15357-connection-reaper-0""
    [junit] Testsuite: org.apache.cassandra.pig.CqlTableTest
    [junit] Testsuite: org.apache.cassandra.pig.CqlTableTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.pig.CqlTableTest:testCqlNativeStorageSingleKeyTable:	Caused an ERROR
    [junit] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.pig.CqlTableTest FAILED (crashed)
    [junit] Testsuite: org.apache.cassandra.pig.ThriftColumnFamilyDataTypeTest
    [junit] Testsuite: org.apache.cassandra.pig.ThriftColumnFamilyDataTypeTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.889 sec
    [junit] 
    [junit] Testcase: testCassandraStorageDataType(org.apache.cassandra.pig.ThriftColumnFamilyDataTypeTest):	Caused an ERROR
    [junit] Unable to open iterator for alias rows
    [junit] org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias rows
    [junit] 	at org.apache.pig.PigServer.openIterator(PigServer.java:882)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyDataTypeTest.testCassandraStorageDataType(ThriftColumnFamilyDataTypeTest.java:81)
    [junit] Caused by: java.io.IOException: Job terminated with anomalous status FAILED
    [junit] 	at org.apache.pig.PigServer.openIterator(PigServer.java:874)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.pig.ThriftColumnFamilyDataTypeTest FAILED
    [junit] Testsuite: org.apache.cassandra.pig.ThriftColumnFamilyTest
    [junit] Testsuite: org.apache.cassandra.pig.ThriftColumnFamilyTest
    [junit] Testsuite: org.apache.cassandra.pig.ThriftColumnFamilyTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.pig.ThriftColumnFamilyTest:testCqlNativeStorageCompositeKeyCF:	Caused an ERROR
    [junit] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.pig.ThriftColumnFamilyTest FAILED (crashed)
[junitreport] Processing /var/lib/jenkins/jobs/trunk_pigtest/workspace/build/test/TESTS-TestSuites.xml to /tmp/null1591595172
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 1048ms
[junitreport] Deleting: /tmp/null1591595172

BUILD FAILED
{noformat}",test-failure,['Legacy/Testing'],CASSANDRA,Bug,Normal,2015-07-18 02:11:07,1
12844931,RangeTombstonListTest sometimes fails on trunk,"I've seen random failures with {{RangeTombstoneList.addAllRandomTest}}. The problem is 2 inequalities in {{RangeTombstoneList.insertFrom}} that should be inclusive rather than strict when we deal with boundaries between range. In practice, that makes us consider range like {{[3, 3)}} during addition, which is non-sensical.

Attaching patch as well as a test that reproduce (extracted from {{addAllRandomTest}} with a failing seed).",test,[],CASSANDRA,Improvement,Normal,2015-07-14 11:31:01,2
12844908,Don't wrap byte arrays in SequentialWriter,"While profiling a simple stress write run ({{cassandra-stress write n=2000000 -rate threads=50}} to be precise) with Mission Control, I noticed that a non trivial amount of heap pressure was due to the {{ByteBuffer.wrap()}} call in {{SequentialWriter.write(byte[])}}. Basically, when writing a byte array, we wrap it in a ByteBuffer to reuse the {{SequentialWriter.write(ByteBuffer)}} method. One could have hoped this wrapping would be stack allocated, but if Mission Control isn't lying (and I was told it's fairly honest on that front), it's not. And we do use that {{write(byte[])}} method quite a bit, especially with the new vint encodings since they use a {{byte[]}} thread local buffer and call that method.

Anyway, it sounds very simple to me to have a more direct {{write(byte[])}} method, so attaching a patch to do that. A very quick local benchmark seems to show a little bit less allocation and a slight edge for the branch with this patch (on top of CASSANDRA-9705 I must add), but that local bench was far from scientific so happy if someone that knows how to use our perf service want to give that patch a shot.
",performance,[],CASSANDRA,Improvement,Low,2015-07-14 08:56:15,2
12844763,RangeTombstoneTest.testRowWithRangeTombstonesUpdatesSecondaryIndex failure,"This test failure started with the 8099 commit.
{noformat}
Error Message

expected:<10> but was:<0>

Stacktrace

junit.framework.AssertionFailedError: expected:<10> but was:<0>
	at org.apache.cassandra.db.RangeTombstoneTest.runCompactionWithRangeTombstoneAndCheckSecondaryIndex(RangeTombstoneTest.java:579)
	at org.apache.cassandra.db.RangeTombstoneTest.testRowWithRangeTombstonesUpdatesSecondaryIndex(RangeTombstoneTest.java:455)
{noformat}",test-failure,[],CASSANDRA,Sub-task,Urgent,2015-07-13 19:42:06,2
12841220,Improve batchlog write path,"Currently we allocate an on-heap {{ByteBuffer}} to serialize the batched mutations into, before sending it to a distant node, generating unnecessary garbage (potentially a lot of it).

With materialized views using the batchlog, it would be nice to optimise the write path:
- introduce a new verb ({{Batch}})
- introduce a new message ({{BatchMessage}}) that would encapsulate the mutations, expiration, and creation time (similar to {{HintMessage}} in CASSANDRA-6230)
- have MS serialize it directly instead of relying on an intermediate buffer

To avoid merely shifting the temp buffer to the receiving side(s) we should change the structure of the batchlog table to use a list or a map of individual mutations.",performance,"['Feature/Materialized Views', 'Legacy/Coordination']",CASSANDRA,Improvement,Normal,2015-06-29 01:15:39,3
12840877,Re-enable memory-mapped index file reads on Windows,"It appears that the impact of buffered vs. memory-mapped index file reads has changed dramatically since last I tested. [Here's some results on various platforms we pulled together yesterday w/2.2-HEAD|https://docs.google.com/spreadsheets/d/1JaO2x7NsK4SSg_ZBqlfH0AwspGgIgFZ9wZ12fC4VZb0/edit#gid=0].

TL;DR: On linux we see a 40% hit in performance from 108k ops/sec on reads to 64.8k ops/sec. While surprising in itself, the really unexpected result (to me) is on Windows - with standard access we're getting 16.8k ops/second on our bare-metal perf boxes vs. 184.7k ops/sec with memory-mapped index files, an over 10-fold increase in throughput. While testing w/standard access, CPU's on the stress machine and C* node are both sitting < 4%, network doesn't appear bottlenecked, resource monitor doesn't show anything interesting, and performance counters in the kernel show very little. Changes in thread count simply serve to increase median latency w/out impacting any other visible metric that we're measuring, so I'm at a loss as to why the disparity is so huge on the platform.

The combination of my changes to get the 2.1 branch to behave on Windows along with [~benedict] and [~Stefania]'s changes in lifecycle and cleanup patterns on 2.2 should hopefully have us in a state where transitioning back to using memory-mapped I/O on Windows will only cause trouble on snapshot deletion. Fairly simple runs of stress w/compaction aren't popping up any obvious errors on file access or renaming - I'm going to do some much heavier testing (ccm multi-node clusters, long stress w/repair and compaction, etc) and see if there's any outstanding issues that need to be stamped out to call mmap'ed index files on Windows safe. The one thing we'll never be able to support is deletion of snapshots while a node is running and sstables are mapped, but for a > 10x throughput increase I think users would be willing to make that sacrifice.

The combination of the powercfg profile change, the kernel timer resolution, and memory-mapped index files are giving some pretty interesting performance numbers on EC2.",Windows performance,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2015-06-26 15:07:10,6
12840391,Paxos ballot in StorageProxy could clash,"This code in {{StorageProxy.beginAndRepairPaxos()}} takes a timestamp in microseconds but divides it by 1000 before adding one. So if the summary is null, ballotMillis would be the same for up to 1000 possible state timestamp values:

{code}
    long currentTime = (state.getTimestamp() / 1000) + 1;
    long ballotMillis = summary == null
                                 ? currentTime
                                 : Math.max(currentTime, 1 +    UUIDGen.unixTimestamp(summary.mostRecentInProgressCommit.ballot));
    UUID ballot = UUIDGen.getTimeUUID(ballotMillis);
{code}

{{state.getTimestamp()}} returns the time in micro seconds and it ensures to add one microsecond to any previously used timestamp if the client sends the same or an older timestamp. 

Initially I used this code in {{ModificationStatement.casInternal()}}, introduced by CASSANDRA-9160 to support cas unit tests, but occasionally these tests were failing. It was only when I ensured uniqueness of the ballot that the tests started to pass reliably.

I wonder if we could ever have the same issue in StorageProxy?

cc [~jbellis] and [~slebresne] for CASSANDRA-7801",LWT,"['Feature/Lightweight Transactions', 'Legacy/Coordination']",CASSANDRA,Bug,Low,2015-06-25 01:09:12,3
12840351,Warn if power profile is not High Performance on Windows,"Windows' power profiles have a pretty marked impact on application performance and the CPU frequency throttling is fairly aggressive even in balanced mode. As we have a large number of threads with varying work rather than a single busy thread-per-core, the scheduler on Windows sees enough downtime to constantly struggle w/our user-space operations and the frequency on the system will jump up and down even when fully saturated from a stress.

I've done some benchmarking of the ""Balanced"" vs. ""High Performance"" power profiles - [link to performance numbers|https://docs.google.com/spreadsheets/d/1YS8VtdZAgyec-mcnSgtNhQH9LiHstOaiMtlppvEIIM8/edit#gid=0]. Note: reads are not saturating the box (or even impacting resources at all really) as the CPU's on both stress and node are sitting around 4% usage. Still have something to figure out there on 2.2.

We have a few ways we can approach this - for the 1st (warn), here's a branch with warning during startup if non-High Performance power profile detected: [here|https://github.com/apache/cassandra/compare/trunk...josh-mckenzie:check_power_plan].

Alternatively we could get more aggressive and actually attempt a powercfg /s to the GUID of the High Performance power profile or refuse to start Cassandra if we're not in the performance profile. I also briefly pursued using Sigar to query this information however the documentation for the library is no longer available (or at least I couldn't find it).",Performance Windows,['Packaging'],CASSANDRA,Improvement,Low,2015-06-24 21:53:50,6
12840061,DTCS configuration proposals for handling consequences of repairs,"This is a document bringing up some issues when DTCS is used to compact time series data in a three node cluster. The DTCS is currently configured with a few parameters that are making the configuration fairly simple, but might cause problems in certain special cases like recovering from the flood of small SSTables due to repair operation. We are suggesting some ideas that might be a starting point for further discussions. Following sections are containing:

- Description of the cassandra setup
- Feeding process of the data
- Failure testing
- Issues caused by the repair operations for the DTCS
- Proposal for the DTCS configuration parameters

Attachments are included to support the discussion and there is a separate section giving explanation for those.

Cassandra setup and data model

- Cluster is composed from three nodes running Cassandra 2.1.2. Replication factor is two and read and write consistency levels are ONE.
- Data is time series data. Data is saved so that one row contains a certain time span of data for a given metric ( 20 days in this case). The row key contains information about the start time of the time span and metrix name. Column name gives the offset from the beginning of time span. Column time stamp is set to correspond time stamp when adding together the timestamp from the row key and the offset (the actual time stamp of data point). Data model is analog to KairosDB implementation.
- Average sampling rate is 10 seconds varying significantly from metric to metric.
- 100 000 metrics are fed to the Cassandra.
- max_sstable_age_days is set to 5 days (objective is to keep SStable files in manageable size, around 50 GB)
- TTL is not in use in the test.

Procedure for the failure test.

- Data is first dumped to Cassandra for 11 days and the data dumping is stopped so that DTCS will have a change to finish all compactions. Data is dumped with ""fake timestamps"" so that column time stamp is set when data is written to Cassandra.
- One of the nodes is taken down and new data is dumped on top of the earlier data covering couple of hours worth of data (faked time stamps).
- Dumping is stopped and the node is kept down for few hours.
- Node is taken up and the ""nodetool repair"" is applied on the node that was down.

Consequences

- Repair operation will lead to massive amount of new SStables far back in the history. New SStables are covering similar time spans than the files that were created by DTCS before the shutdown of one of the nodes.
- To be able to compact the small files the max_sstable_age_days should be increased to allow compaction to handle the files. However, the in a practical case the time window will increase so large that generated files will be huge that is not desirable. The compaction also combines together one very large file with a bunch of small files in several phases that is not effective. Generating really large files may also lead to out of disc space problems.
- See the list of time graphs later in the document.

Improvement proposals for the DTCS configuration

Below is a list of desired properties for the configuration. Current parameters are mentioned if available.

- Initial window size (currently:base_time_seconds)
- The amount of similar size windows for the bucketing (currently: min_threshold)
- The multiplier for the window size when increased (currently: min_threshold). This we would like to be independent from the min_threshold parameter so that you could actually control the rate how fast the window size is increased.
- Maximum length of the time window inside which the files are assigned for a certain bucket (not currently defined). This means that expansion of time window length is restricted. When the limit is reached the window size will be same all the way back in the history (e.g. one week)
- The maximum horizon in which SStables are candidates for buckets (currently: max_sstable_age_days)
- Maximum file size of SStable allowed to be in a set of files to be compacted (not possible currently). Preventing out of disk space situations.
- Optional strategies to select the most interesting bucket:
    - Minimum amount of SStables in the time window before it is a candidate for the most interesting bucket (currently: min_threshold for the most recent window, otherwise two). Being able set this value independently would allow to put most of the efforts on those areas where a large amount of small files should be compacted together instead of few new files.
    - Optionally, the criteria for the most interesting bucket could be set: e.g. select the window with most files to be compacted.
    - Inside the bucket when the amount of files is limited by max_threshold, the compaction would select first small files instead of one huge file and a bunch of small files.

The above set of parameters allows to recover from repair operations producing large amount of small SStables.

- Maximum length of the time window for compactions would keep the compacted SStable size in reasonable range and would allow to extend the horizon far back in the history
- Combining small files together instead of combining one huge file with e.g. 31 small files again and again is more disk efficient

In addition to the previous advantages the above parameters would also allow:

- Dumping of more data in the history (e.g. new metrics) by assigning the correct timestamp for the column (fake time stamp) and proper compaction of new and existing SStables.
- Expiring reasonable size SStable with TTL even if the compactions would be intermittently executed far back in the history. In this case the new data has to fed with TTL calculated dynamically.
- Note: Being able to give the absolute time stamp for the column expiry time would be beneficial when data is dumped back in the history. This is the case when you move data from some legacy system to Cassandra with faked time stamps and would like to keep the data only a certain time period.  Currently the absolute time stamp is calculated by Cassandra from the system time and given TTL. TTL has to be calculated dynamically based on the current time and desired expiry moment making things more complex.

One interesting question is that why those duplicate SStable files are created? The duplication problem could not be produced when the data was dumped with following spec:

- 100 metrics
- 20 days of data in one row
- one year of data
- max_sstable_age_days = 15
 - memtable_offheap_space_in_mb was decreased so that small SStables were created (to create something to be compacted)
 - One node was taken down and one more day of data was dumped on top of the earlier data
- ""nodetool repair -pr"" was executed on each node => duplicates were checked in each step => no duplicates
- ""nodetool repair"" was executed on a node that was down => no duplicates were generated


--------------------------------------------------------------------------------------------------------------------

ATTACHMENTS


Time graphs of content of SSTables from different phases of the test run:

*************************************************************

Fields in the below time graphs are following:

- Order number from the  SSTable file name
- Minimum column timestamp in the SSTable file
- Timespan representation graphically
- Maximum column time stamp in SStable
- The size of the SStable in megabytes

Time graphs after dumping the 11 days of data and letting all compactions to run through

node0_20150621_1646_time_graph.txt
node1_20150621_1646_time_graph.txt
node2_20150621_1646_time_graph.txt (error: same as for node1, but the behavior is same)

Time graphs after taking one node down (node2) and dumping couple of hours of mode data

node0_20150621_2320_time_graph.txt
node1_20150621_2320_time_graph.txt
node2_20150621_2320_time_graph.txt 

Time graphs when the repair operation has finished and compactions are done. Compactions will naturally handle only the files inside the max_sstable_age_days range.

==> Now there is a large amount of small files covering pretty much same areas as the original SStables

node0_20150623_1526_time_graph.txt
node1_20150623_1526_time_graph.txt
node2_20150623_1526_time_graph.txt

-----------
Trend from the SStable count as a function of time on each node.
sstable_counts.jpg

Vertical lines:

1) Clearing the database and dumping the 11 days worth of data
2) Stopping the dumping and letting compactions run
3) Taking one node down (top bottom one in figure) and dumping few hours of new data on top of earlier data
4) Starting the repair operation
5) Repair operation finished


--------
Nodetool status prints before and after repair operation
nodetool status infos.txt

--------------
Tracing compactions

Log files were parsed to demonstrate the creation of new small SStables and the combination of one large file with a bunch of small ones. This is done from the time range where the max_sstable_age_days is able to reach (in this case 5 days). The hierarchy of the files is shown in the file ""sstable_compaction_trace.txt"". The first flushed file can be found from the line 10.

Each line represents either a flushed SStable or the SStable created by DTCS. For flushed files the timestamp indicates the time period the file represents. For compacted files (marked with C) the first timestamp represents the moment when the compaction was done (wall clock). Time stamps are faked when written to the database. The size of the file is the last field. The first field with number in parenthesis shows the level of the file. Top level files marked with (0) are those that don't have any predecessors and should be found from the disk also.

SStables that are created by the repair operation are not mentioned in the log files so they are handled as phantom files. The existence of file can be concluded from the predecessor list of compacted SStable. Those are marked with None,None in timestamps.

In the file ""sstable_compaction_trace_snipped.txt"" is one portion that shows the compaction hierarchy for the small files originating from the repair operation. max_threshold is in the default value of 32. In each step 31 tiny files are compacted together with 46 GB file.


sstable_compaction_trace.txt
sstable_compaction_trace_snipped.txt",compaction dtcs,[],CASSANDRA,Improvement,Normal,2015-06-24 07:50:12,0
12839585,Set kernel timer resolution on Windows,"In Windows 7/Server 2008 and to a similar extent Windows 8/Server 2012, the kernel's internal time is set to an interval of 15.6ms. (Use [clockres|https://technet.microsoft.com/en-us/sysinternals/bb897568.aspx] to confirm current 'tick rate' on Windows). Win8/Server2012 have a tickless kernel w/timer coalescing ([info here|http://arstechnica.com/information-technology/2012/10/better-on-the-inside-under-the-hood-of-windows-8/2/]) and the platform shows similar performance characteristics with C* to Windows 7 with a slight edge in performance to win8/server 2012 in my testing (the testing and results of which are outside the scope of this ticket).

Some arguments against lowering the system's internal timer to 1ms are [here|https://randomascii.wordpress.com/2013/07/08/windows-timer-resolution-megawatts-wasted/]. These seem largely constrained to ""it'll drain your battery"" and ""it'll prevent your processor from being as effective in sleep states"". The 2nd is somewhat of a concern as we don't want Windows users to all of a sudden have increased CPU-usage bills from virtualized environments. In the comments, one individual mentions a VirtualBox VM spinning at 10-20% cpu just from changing that flag alone which seems mathematically unlikely, but is worth keeping an eye on and testing.

A Microsoft publication that largely reinforces the cautionary tale on power consumption can be found [here|http://download.microsoft.com/download/3/0/2/3027D574-C433-412A-A8B6-5E0A75D5B237/Timer-Resolution.docx].

With the cautionary tales on our radar, the impact on throughput and latency on the 2.2 branch on Windows is [fairly dramatic|https://docs.google.com/spreadsheets/d/1nqPhNwOVt0SU7b9lt9o4Tyl0Z1yDrV2oo7LbBPaFa6A/edit#gid=0]. A couple of caveats on these #'s: I'm not completely saturating the system as the thread count is relatively low (keeping it consistent with other testing where it *was* saturating), and the read #'s from our 2012 test environment are not affected by this timer change while I see it on 3 other bare-metal installations. The testing environment is new and we haven't worked out the kinks yet, however the write / mixed illustrate the throughput and latency #'s I've mentioned above; for reads the cpu's are sitting idle at 1-5% used by stress and C* so something else clearly needs to be addressed there; I included them for completeness sake.

Some preliminary testing on OpenStack indicates kernel-space syscall saturation w/this patch that actually *degrades* performance, however the unpatched performance numbers in our OpenStack environment are low enough that I question their validity.

Opening this ticket w/attached branch to get it on the radar / conversation going, and I'm going to update this from being hard-coded to being a tunable in the .yaml.

Initial patch [available here|https://github.com/apache/cassandra/compare/trunk...josh-mckenzie:2.2_WinTimer].",Windows performance,"['Legacy/Local Write-Read Paths', 'Local/Startup and Shutdown']",CASSANDRA,Improvement,Normal,2015-06-22 16:06:43,6
12838588,Speed up Windows launch scripts,"Currently the async callback to start C* on Windows from within ccm is taking upwards of 1.5 to 2 seconds per node due to a variety of somewhat expensive process launches we're doing in there (java version check, async port open checking). Contrast this with a crisp 0-1 ms on linux...

Some of that stuff can be cleaned up and sped up which should help both speed up our testing environment and iron out an error that pops up on the port check w/IPv6 (note: node still starts, just complains).",Windows,['Packaging'],CASSANDRA,Improvement,Low,2015-06-17 21:04:54,6
12838097,Allow an initial connection timeout to be set in cqlsh,"[PYTHON-206|https://datastax-oss.atlassian.net/browse/PYTHON-206] introduced the ability to change the initial connection timeout on connections from the default of 5s.

This change was introduced because some auth providers (kerberos) can take longer than 5s to complete a first time negotiation for a connection. 

cqlsh should allow this setting to be changed. ",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2015-06-16 08:46:53,3
12837353,Make sstableofflinerelevel print stats before relevel,"The current version of sstableofflinerelevel prints the new level hierarchy. While ""nodetool cfstats ..."" will tell the current hierarchy it would be nice to have ""sstableofflinerelevel"" output the current level histograms for easy comparison of what changes will be made. Especially since sstableofflinerelevel needs to run when node isn't running and ""nodetool cfstats ..."" doesn't work because of that.",lcs lhf,['Legacy/Tools'],CASSANDRA,Improvement,Low,2015-06-12 09:04:47,0
12836815,DateTieredCompactionStrategy fails to combine SSTables correctly when TTL is used.,"DateTieredCompaction works correctly when data is dumped for a certain time period in short SSTables in time manner and then compacted together. However, if TTL is applied to the data columns the DTCS fails to compact files correctly in timely manner. In our opinion the problem is caused by two issues:

A) During the DateTieredCompaction process the getFullyExpiredSStables is called twice. First from the DateTieredCompactionStrategy class and second time from the CompactionTask class. On the first time the target is to find out fully expired SStables that are not overlapping with any non-fully expired SSTables. That works correctly. When the getFullyExpiredSSTables is called second time from CompactionTask class the selection of fully expired SSTables is modified compared to the first selection.

B) The minimum timestamp of the new SSTables created by combining together fully expired SSTable and files from the most interesting bucket is not correct.

These two issues together cause problems for the DTCS process when it combines together SSTables having overlap in time and TTL for the column. This is demonstrated by generating test data first without compactions and showing the timely distribution of files. When the compaction is enabled the DCTS combines files together, but the end result is not something to be expected. This is demonstrated in the file motivation_jira.txt

Attachments contain following material:

- Motivation_jira.txt: Practical examples how the DTCS behaves with TTL
- Explanation_jira.txt: gives more details, explains test cases and demonstrates the problems in the compaction process
- Logfile file for the compactions in the first test case (compaction_stage_test01_jira.log)
- Logfile file for the compactions in the seconnd test case (compaction_stage_test02_jira.log)
- source code zip file for version 2.1.5 with additional comment statements (src_2.1.5_with_debug.zip)
- Python script to generate test data (datagen.py)
- Python script to read metadata from SStables (cassandra_sstable_metadata_reader.py)
- Python script to generate timeline representation of SSTables (cassandra_sstable_timespan_graph.py)
",dtcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-06-10 12:01:40,0
12835710,Avoid digest mismatch storm on upgrade to 3.0,"CASSANDRA-8099, in {{UnfilteredRowIterators.digest()}}:

{code}
        // TODO: we're not computing digest the same way that old nodes. This
        // means we'll have digest mismatches during upgrade. We should pass the messaging version of
        // the node this is for (which might mean computing the digest last, and won't work
        // for schema (where we announce the version through gossip to everyone))
{code}

In a mixed 2.1(2.2) - 3.0 clusters, we need to calculate both digest at the same time and keep both results, and send the appropriate one, depending on receiving nodes' messaging versions. Do that until {{MessagingService.allNodesAtLeast30()}} is true (this is not unprecedented).",upgrade,[],CASSANDRA,Bug,Normal,2015-06-05 13:07:07,2
12834943,Sstables created with MockSchema occasionally triggers assertions,"It's extremely hard to reproduce but occasionally some Jenkins tests fail as follows:

http://cassci.datastax.com/job/trunk_utest/229/testReport/org.apache.cassandra.db.lifecycle/TrackerTest/testAddInitialSSTables/

{code}
junit.framework.AssertionFailedError
	at org.apache.cassandra.utils.concurrent.Ref$State.assertNotReleased(Ref.java:157)
	at org.apache.cassandra.utils.concurrent.Ref.ref(Ref.java:113)
	at org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy.get(SSTableReader.java:2111)
	at org.apache.cassandra.io.sstable.format.SSTableReader$DescriptorTypeTidy.<init>(SSTableReader.java:1980)
	at org.apache.cassandra.io.sstable.format.SSTableReader$DescriptorTypeTidy.get(SSTableReader.java:2017)
	at org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier.setup(SSTableReader.java:1897)
	at org.apache.cassandra.io.sstable.format.SSTableReader.setup(SSTableReader.java:1842)
	at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:530)
	at org.apache.cassandra.MockSchema.sstable(MockSchema.java:128)
	at org.apache.cassandra.MockSchema.sstable(MockSchema.java:92)
	at org.apache.cassandra.MockSchema.sstable(MockSchema.java:87)
	at org.apache.cassandra.db.lifecycle.TrackerTest.testAddInitialSSTables(TrackerTest.java:146)
{code}

I propose not to reuse cfname in MockSchema.sstable() and in fact to get rid of the shared MockSchema.cfs which has caused other test problems, see CASSANDRA-9514.",test-failure,['Legacy/Testing'],CASSANDRA,Improvement,Normal,2015-06-03 06:32:55,3
12833870,ViewTest.testSSTablesInBounds fails in trunk,"Error:
{{0(true) 1(false) expected:<1> but was:<2>}}

http://cassci.datastax.com/job/trunk_testall/125/testReport/org.apache.cassandra.db.lifecycle/ViewTest/testSSTablesInBounds/",test-failure,['Legacy/Testing'],CASSANDRA,Improvement,Normal,2015-05-29 19:44:25,3
12833868,TrackerTest.testAddSSTables fails in 2.2 and trunk,"Error:
{{expected:<1> but was:<2>}}

http://cassci.datastax.com/job/cassandra-2.2_testall/33/testReport/org.apache.cassandra.db.lifecycle/TrackerTest/testTryModify/
http://cassci.datastax.com/job/trunk_testall/125/testReport/org.apache.cassandra.db.lifecycle/TrackerTest/testDropSSTables/",test-failure,['Legacy/Testing'],CASSANDRA,Improvement,Normal,2015-05-29 19:41:10,3
12833492,ColumnFamilyStoreTest.testSliceByNamesCommandOldMetadata failing,"{noformat}
    [junit] Testcase: testSliceByNamesCommandOldMetadata(org.apache.cassandra.db.ColumnFamilyStoreTest):        FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:171)
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:166)
    [junit]     at org.apache.cassandra.io.sstable.format.SSTableWriter.rename(SSTableWriter.java:266)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:766)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetadata(ColumnFamilyStoreTest.java:1125)
{noformat}",test,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Low,2015-05-28 16:53:39,6
12833127,Need to set TTL with COPY command,"I can import a chunk of data into Cassandra table with COPY command like:

COPY my_table (name, address) FROM my_file.csv WITH option='value' ... ;

But I am not able to specify a finite TTL in COPY command with ""USING TTL 3600"", for example. ",cqlsh,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2015-05-27 18:12:26,3
12832787,SSTable leak after stress and repair,"I have a dtest that fails intermittently because of SSTable leaks. The test logic leading to the error is:

- create a 5-node cluster
- insert 5000 records with {{stress}}, RF=3 at CL=ONE
- run {{flush}} on all nodes 
- run {{repair}} on a single node.

The leak is detected on a different node than {{repair}} was run on.

The failing test is [here|https://github.com/mambocab/cassandra-dtest/blob/CASSANDRA-5839-squash/repair_test.py#L317]. The relevant error his [here|https://gist.github.com/mambocab/8aab7b03496e0b279bd3#file-node2-log-L256], along with the errors from the entire 5-node cluster. In these logs, the {{repair}} was run on {{node1}} and the leak was found on {{node2}}.

I can bisect, but I thought I'd get the ball rolling in case someone knows where to look.",stress,[],CASSANDRA,Bug,Normal,2015-05-26 17:28:33,0
12831664,Metrics should use up to date nomenclature,"There are a number of exposed metrics that currently are named using the old nomenclature of columnfamily and rows (meaning partitions).
It would be good to audit all metrics and update any names to match what they actually represent; we should probably do that in a single sweep to avoid a confusing mixture of old and new terminology. 

As we'd need to do this in a major release, I've initially set the fixver for 3.0 beta1.
",jmx,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2015-05-21 09:13:15,3
12830767,3.x should refuse to start on JVM_VERSION < 1.8,"When I was looking at CASSANDRA-9408, I noticed that {{conf/cassandra-env.sh}} and {{conf/cassandra-env.ps1}} do JVM version checking and should get updated for 3.x to refuse to start with JVM_VERSION < 1.8.",lhf,[],CASSANDRA,Task,Low,2015-05-18 18:23:02,4
12830731,Windows: deleteWithConfirm errors on various unit tests,"Failure is intermittent enough that bisect is proving to be more hassle than it's worth. Seems pretty consistent in CI.

{noformat}
    [junit] Testcase: testDeleteIfNotDirty(org.apache.cassandra.db.CommitLogTest):      Caused an ERROR
    [junit] java.nio.file.AccessDeniedException: build\test\cassandra\commitlog;0\CommitLog-5-1431965988394.log
    [junit] FSWriteError in build\test\cassandra\commitlog;0\CommitLog-5-1431965988394.log
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:131)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:148)
    [junit]     at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.recycleSegment(CommitLogSegmentManager.java:360)
    [junit]     at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:166)
    [junit]     at org.apache.cassandra.db.commitlog.CommitLog.startUnsafe(CommitLog.java:416)
    [junit]     at org.apache.cassandra.db.commitlog.CommitLog.resetUnsafe(CommitLog.java:389)
    [junit]     at org.apache.cassandra.db.CommitLogTest.testDeleteIfNotDirty(CommitLogTest.java:178)
    [junit] Caused by: java.nio.file.AccessDeniedException: build\test\cassandra\commitlog;0\CommitLog-5-1431965988394.log
    [junit]     at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
    [junit]     at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
    [junit]     at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    [junit]     at java.nio.file.Files.delete(Files.java:1126)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:125)
{noformat}",Windows,"['Legacy/Local Write-Read Paths', 'Legacy/Testing']",CASSANDRA,Bug,Low,2015-05-18 16:21:49,6
12830209,cqlsh support for native protocol v4 features,"cqlsh/python-driver need to add support for all the new 2.2 CQL features:
- {{date}} and {{time}} types - CASSANDRA-7523 - not in cqlsh yet
- {{smallint}} and {{tinyint}} types - CASSANDRA-8951 - not in the driver yet
- client warnings - CASSANDRA-8930
- tracing improvements - CASSANDRA-7807",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2015-05-15 16:07:23,3
12828427,JVM_EXTRA_OPTS not getting picked up by windows startup environment,"-    $env:JVM_OPTS=""$env:JVM_OPTS $JVM_EXTRA_OPTS""
+    $env:JVM_OPTS=""$env:JVM_OPTS $env:JVM_EXTRA_OPTS""

Missing env: on front of JVM_EXTRA_OPTS in conf\cassandra-env.ps1

Attaching trivial patch to fix this.",windows,['Packaging'],CASSANDRA,Bug,Normal,2015-05-08 17:28:19,6
12827998,Possible overlap with LCS and including non-compacting sstables,"since CASSANDRA-7414 we are including high-level sstables in lower level compactions if we have not run compactions in the high level for a while.

If the compaction candidates only contain a single partition this can cause overlap since first token in sstables == last token in sstables which we interpret as being ""entire ring"".

",lcs,['Local/Compaction'],CASSANDRA,Improvement,Normal,2015-05-07 11:54:31,0
12827319,COPY TO improvements,"COPY FROM has gotten a lot of love.  COPY TO not so much.  One obvious improvement could be to parallelize reading and writing (write one page of data while fetching the next).
",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2015-05-05 15:12:54,3
12827313,Match cassandra-loader options in COPY FROM,"https://github.com/brianmhess/cassandra-loader added a bunch of options to handle real world requirements, we should match those.",doc-impacting,['Legacy/Tools'],CASSANDRA,New Feature,Urgent,2015-05-05 15:05:40,3
12824437,"for DateTieredCompactionStrategy, TIMESTAMP_RESOLUTION_KEY sets wrong msxSSTableAge value if RESOLUTION is other than MILLISECONDS","I was trying to set 'timestamp_resolution' to MINUTES/HOURS/DAYS. it turned out maxSSTableAge was set as wrong value. In the code,
{code}
    public DateTieredCompactionStrategyOptions(Map<String, String> options)
    {
        String optionValue = options.get(TIMESTAMP_RESOLUTION_KEY);
        TimeUnit timestampResolution = optionValue == null ? DEFAULT_TIMESTAMP_RESOLUTION : TimeUnit.valueOf(optionValue);
        optionValue = options.get(MAX_SSTABLE_AGE_KEY);
        double fractionalDays = optionValue == null ? DEFAULT_MAX_SSTABLE_AGE_DAYS : Double.parseDouble(optionValue);
        maxSSTableAge = Math.round(fractionalDays * timestampResolution.convert(1, TimeUnit.DAYS));
 ...   }{code}

maxSSTableAge will be set as the value in ""timestamp_resolution"" unit, such as , with the following settings,
        'timestamp_resolution':'HOURS',
        'max_sstable_age_days':'7',
        'base_time_seconds':'3600'
and I get: 
maxSSTableAge=168,  baseTime=1

while in the following routine, it expect maxSSTableAge as milliseconds
    static Iterable<SSTableReader> filterOldSSTables(List<SSTableReader> sstables, long maxSSTableAge, long now)


",dtcs,[],CASSANDRA,Bug,Normal,2015-04-27 22:15:02,0
12823744,"Max sstable size in leveled manifest is an int, creating large sstables overflows this and breaks LCS","nodetool compactionstats
pending tasks: -222222228

I can see negative numbers in 'pending tasks' on all 8 nodes
it looks like -222222228 + real number of pending tasks
for example -222222128 for 100 real pending tasks

",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-04-24 13:24:11,0
12823670,Disable single-sstable tombstone compactions for DTCS,"We should probably disable tombstone compactions by default for DTCS for these reasons:

# users should not do deletes with DTCS
# the only way we should get rid of data is by TTL - and then we don't want to trigger a single sstable compaction whenever an sstable is 20%+ expired, we want to drop the whole thing when it is fully expired",dtcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-04-24 06:59:35,0
12823511,"""timestamp"" is considered as a reserved keyword in cqlsh completion","cqlsh seems to treat ""timestamp"" as a reserved keyword when used as an identifier:

{code}
cqlsh:ks1> create table t1 (int int primary key, ascii ascii, bigint bigint, blob blob, boolean boolean, date date, decimal decimal, double double, float float, inet inet, text text, time time, timestamp timestamp, timeuuid timeuuid, uuid uuid, varchar varchar, varint varint);
{code}

Leads to the following completion when building an {{INSERT}} statement:

{code}
cqlsh:ks1> insert into t1 (int, 
""timestamp"" ascii       bigint      blob        boolean     date        decimal     double      float       inet        text        time        timeuuid    uuid        varchar     varint
{code}

""timestamp"" is a keyword but not a reserved one and should therefore not be proposed as a quoted string. It looks like this error happens only for timestamp. Not a big deal of course, but it might be worth reviewing the keywords treated as reserved in cqlsh, especially with the many changes introduced in 3.0.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2015-04-23 18:40:18,3
12822794,Figure out a better default float precision rule for cqlsh,"We currently use a {{DEFAULT_FLOAT_PRECISION}} of 5 in cqlsh with formatting {{'%.*g' % (float_precision, val)}}.  In practice, this is way too low.  For example, 12345.5 will show up as 123456.  Since the float precision is used for cqlsh's COPY TO, it's particularly important that we maintain as much precision as is practical by default.

There are some other tricky considerations, though.  If the precision is too high, python will do something like this:

{noformat}
> '%.25g' % (12345.5555555555555555,)
'12345.55555555555474711582'
{noformat}

That's not terrible, but it would be nice to avoid if we can.",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2015-04-21 20:56:26,3
12787788,"Unit test failures, trunk + Windows","Variety of different test failures have cropped up over the past 2-3 weeks:

h6. -org.apache.cassandra.cql3.UFTest FAILED (timeout)- // No longer failing / timing out
h6. testLoadNewSSTablesAvoidsOverwrites(org.apache.cassandra.db.ColumnFamilyStoreTest):       FAILED
{noformat}
   12 SSTables unexpectedly exist
   junit.framework.AssertionFailedError: 12 SSTables unexpectedly exist
   at org.apache.cassandra.db.ColumnFamilyStoreTest.testLoadNewSSTablesAvoidsOverwrites(ColumnFamilyStoreTest.java:1896)
{noformat}

h6. org.apache.cassandra.db.KeyCacheTest FAILED
{noformat}
   expected:<4> but was:<2>
   junit.framework.AssertionFailedError: expected:<4> but was:<2>
   at org.apache.cassandra.db.KeyCacheTest.assertKeyCacheSize(KeyCacheTest.java:221)
   at org.apache.cassandra.db.KeyCacheTest.testKeyCache(KeyCacheTest.java:181)
{noformat}

h6. RecoveryManagerTest:
{noformat}
   org.apache.cassandra.db.RecoveryManagerTest FAILED
   org.apache.cassandra.db.RecoveryManager2Test FAILED
   org.apache.cassandra.db.RecoveryManager3Test FAILED
   org.apache.cassandra.db.RecoveryManagerTruncateTest FAILED
   All are the following:
      java.nio.file.AccessDeniedException: build\test\cassandra\commitlog;0\CommitLog-5-1427995105229.log
      FSWriteError in build\test\cassandra\commitlog;0\CommitLog-5-1427995105229.log
         at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:128)
         at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.recycleSegment(CommitLogSegmentManager.java:360)
         at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:156)
         at org.apache.cassandra.db.RecoveryManagerTest.testNothingToRecover(RecoveryManagerTest.java:75)
      Caused by: java.nio.file.AccessDeniedException: build\test\cassandra\commitlog;0\CommitLog-5-1427995105229.log
         at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
         at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
         at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
         at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
         at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
         at java.nio.file.Files.delete(Files.java:1079)
         at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:124)
{noformat}

h6. testScrubCorruptedCounterRow(org.apache.cassandra.db.ScrubTest):  FAILED
{noformat}
Expecting new size of 1, got 2 while replacing [BigTableReader(path='C:\src\refCassandra\build\test\cassandra\data;0\Keyspace1\Counter1-deab62b2d95c11e489c6e117fe147c1d\la-1-big-Data.db')] by [BigTableReader(path='C:\src\refCassandra\build\test\cassandra\data;0\Keyspace1\Counter1-deab62b2d95c11e489c6e117fe147c1d\la-1-big-Data.db')] in View(pending_count=0, sstables=[BigTableReader(path='C:\src\refCassandra\build\test\cassandra\data;0\Keyspace1\Counter1-deab62b2d95c11e489c6e117fe147c1d\la-3-big-Data.db')], compacting=[])
junit.framework.AssertionFailedError: Expecting new size of 1, got 2 while replacing [BigTableReader(path='C:\src\refCassandra\build\test\cassandra\data;0\Keyspace1\Counter1-deab62b2d95c11e489c6e117fe147c1d\la-1-big-Data.db')] by [BigTableReader(path='C:\src\refCassandra\build\test\cassandra\data;0\Keyspace1\Counter1-deab62b2d95c11e489c6e117fe147c1d\la-1-big-Data.db')] in View(pending_count=0, sstables=[BigTableReader(path='C:\src\refCassandra\build\test\cassandra\data;0\Keyspace1\Counter1-deab62b2d95c11e489c6e117fe147c1d\la-3-big-Data.db')], compacting=[])
   at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:767)
   at org.apache.cassandra.db.DataTracker.replaceReaders(DataTracker.java:408)
   at org.apache.cassandra.db.DataTracker.replaceWithNewInstances(DataTracker.java:312)
   at org.apache.cassandra.io.sstable.SSTableRewriter.moveStarts(SSTableRewriter.java:341)
   at org.apache.cassandra.io.sstable.SSTableRewriter.abort(SSTableRewriter.java:202)
   at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:277)
   at org.apache.cassandra.db.ScrubTest.testScrubCorruptedCounterRow(ScrubTest.java:152)
{noformat}",Windows,['Legacy/Testing'],CASSANDRA,Improvement,Normal,2015-04-02 19:29:07,6
12786278,Tombstoned SSTables are not removed past max_sstable_age_days when using DTCS,"When using DTCS, tombstoned sstables past max_sstable_age_days are not removed by minor compactions. I was able to reproduce this manually and also wrote a dtest (currently failing) which reproduces this issue: [dtcs_deletion_test|https://github.com/riptano/cassandra-dtest/blob/master/compaction_test.py#L115] in compaction_test.py. I tried applying the patch in CASSANDRA-8359 but found that the test still fails with the same issue.",compaction dtcs test,[],CASSANDRA,Bug,Normal,2015-03-27 18:12:10,0
12785472,Fix hinted_handoff_enabled yaml setting,"As discussed in CASSANDRA-6157, at the moment we have a single parameter {{hinted_handoff_enabled}} that can be either a boolean or a csv list of enabled data centers.

We should have a boolean global {{hinted_handoff_enabled}} param plus a separate yaml list for the HH DC blacklist - {{hinted_handoff_disabled_datacenters}} to be checked when the global flag is on.

Backward compatibility with the existing approach should be kept.",doc-impacting,['Local/Config'],CASSANDRA,Sub-task,Low,2015-03-25 10:24:47,3
12783395,Run stress nightly against trunk in a way that validates,"Stress has some very basic validation functionality when used without workload profiles. It found a bug on trunk when I first ran it so it has value even though the validation is basic.

As a beachhead for the kind of blackbox validation that we are missing we can start by running stress nightly or 24/7 in some rotation.

There should be two jobs. One job has inverted success criteria (C* should lose some data) and the job should only ""pass"" if the failure is detected. This is just to prove that the harness reports failure if failure occurs.

Another would be the real job that runs stress, parses and parses the output for reports of missing data.

This job is the first pass and basis of what we can point to when a developer makes a change, implements a feature, or fixes a bug, and say ""go add validation to this job.""

Follow on tickets to link to this
* Test multiple configurations
* Get stress to validate more query functionality and APIs (counters, LWT, batches)
* Parse logs and fail tests on error level logs (great way to improve log messages over time)
* ?

I am going to hold off on creating a ton of issues until we have a basic version of the job running.",LWT monthly-release,['Feature/Lightweight Transactions'],CASSANDRA,Task,Normal,2015-03-19 21:50:19,4
12781556,Fix SSTableRewriterTest on Windows,"Platform specific failures:
org.apache.cassandra.io.sstable.SSTableRewriterTest.testNumberOfFiles_truncate
org.apache.cassandra.io.sstable.SSTableRewriterTest.testSmallFiles
org.apache.cassandra.io.sstable.SSTableRewriterTest.testNumberOfFiles_dont_clean_readers
org.apache.cassandra.io.sstable.SSTableRewriterTest.testNumberOfFiles_finish_empty_new_writer
",Windows,"['Legacy/Testing', 'Legacy/Tools']",CASSANDRA,Bug,Low,2015-03-12 17:08:03,3
12781378,Add client to cqlsh SHOW_SESSION,"Once the python driver supports it, https://datastax-oss.atlassian.net/browse/PYTHON-235, add the client to cqlsh {{SHOW_SESSION}} as done in this commit:

https://github.com/apache/cassandra/commit/249f79d3718fa05347d60e09f9d3fa15059bd3d3

Also, update the bundled python driver.",lhf,['Legacy/Tools'],CASSANDRA,Improvement,Low,2015-03-12 01:15:25,3
12781304,Remove transient RandomAccessFile usage,"There are a few places within the code base where we use a RandomAccessFile transiently to either grab fd's or channels for other operations. This is prone to access violations on Windows (see CASSANDRA-4050 and CASSANDRA-8709) - while these usages don't appear to be causing issues at this time there's no reason to keep them. The less RandomAccessFile usage in the code-base the more stable we'll be on Windows.

[SSTableReader.dropPageCache|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L2021]
* Used to getFD, have FileChannel version

[FileUtils.truncate|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java#L188]
* Used to get file channel for channel truncate call. Only use is in index file close so channel truncation down-only is acceptable.

[MMappedSegmentedFile.createSegments|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/util/MmappedSegmentedFile.java#L196]
* Used to get file channel for mapping.

Keeping these in a single ticket as all three should be fairly trivial refactors.",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Low,2015-03-11 19:55:52,3
12780574,COPY command has inherent 128KB field size limit,"In using the COPY command as follows:
{{cqlsh -e ""COPY test.test1mb(pkey, ccol, data) FROM 'in/data1MB/data1MB_9.csv'""}}
the following error is thrown:
{{<stdin>:1:field larger than field limit (131072)}}

The data file contains a field that is greater than 128KB (it's more like almost 1MB).

A work-around (thanks to [~jjordan] and [~thobbs] is to modify the cqlsh script and add the line
{{csv.field_size_limit(1000000000)}}
anywhere after the line
{{import csv}}",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2015-03-09 18:44:46,4
12779756,Don't lookup maxPurgeableTimestamp unless we need to,"Currently we look up the maxPurgeableTimestamp in LazilyCompactedRow constructor, we should only do that if we have to (ie, if we know there is a tombstone to possibly drop)",performance,['Local/Compaction'],CASSANDRA,Improvement,Normal,2015-03-05 10:03:14,0
12779092,"Our default buffer size for (uncompressed) buffered reads should be smaller, and based on the expected record size","A large contributor to slower buffered reads than mmapped is likely that we read a full 64Kb at once, when average record sizes may be as low as 140 bytes on our stress tests. The TLB has only 128 entries on a modern core, and each read will touch 32 of these, meaning we are unlikely to almost ever be hitting the TLB, and will be incurring at least 30 unnecessary misses each time (as well as the other costs of larger than necessary accesses). When working with an SSD there is little to no benefit reading more than 4Kb at once, and in either case reading more data than we need is wasteful. So, I propose selecting a buffer size that is the next larger power of 2 than our average record size (with a minimum of 4Kb), so that we expect to read in one operation. I also propose that we create a pool of these buffers up-front, and that we ensure they are all exactly aligned to a virtual page, so that the source and target operations each touch exactly one virtual page per 4Kb of expected record size.",benedict-to-commit,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2015-03-03 15:00:53,3
12779090,RandomAccessReader should share its FileChannel with all instances (via SegmentedFile),"There's no good reason to open a FileChannel for each \(Compressed\)\?RandomAccessReader, and this would simplify RandomAccessReader to just a thin wrapper.",performance,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2015-03-03 14:54:02,3
12776445,single_file_split_test fails on 2.1,"In CASSANDRA-8623, we fix an issue about getting ""Data component is missing"" errors when splitting multiple sstable one at the time. I wrote a dtest for that, which work properly to test that error. However, it seems that the CompactionExecutor is failing. It's not the same error, but looks related.  

Test: https://github.com/riptano/cassandra-dtest/blob/master/sstablesplit_test.py#L68

Output: http://cassci.datastax.com/job/cassandra-2.1_dtest/726/testReport/junit/sstablesplit_test/TestSSTableSplit/single_file_split_test/",qa-resolved,['Local/Compaction'],CASSANDRA,Bug,Low,2015-02-20 13:49:13,0
12775794,LOCAL_QUORUM writes returns wrong message,"We have two DC3, each with 7 nodes.
Here is the keyspace setup:

 create keyspace test
 with placement_strategy = 'NetworkTopologyStrategy'
 and strategy_options = {DC2 : 3, DC1 : 3}
 and durable_writes = true;

We brought down two nodes in DC2 for maintenance. We only write to DC1 using local_quroum (using datastax JavaClient)
But we see this errors in the log:
Cassandra timeout during write query at consistency LOCAL_QUORUM (4 replica were required but only 3 acknowledged the write
why does it say 4 replica were required? and Why would it give error back to client since local_quorum should succeed.

Here are the output from nodetool status

Note: Ownership information does not include topology; for complete information, specify a keyspace
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.2.0.1  10.92 GB   256     7.9%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC206
UN  10.2.0.2   6.17 GB    256     8.0%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC106
UN  10.2.0.3  6.63 GB    256     7.3%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC107
DL  10.2.0.4  1.54 GB    256     7.7%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC107
UN  10.2.0.5  6.02 GB    256     6.6%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC106
UJ  10.2.0.6   3.68 GB    256     ?      XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC205
UN  10.2.0.7  7.22 GB    256     7.7%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC205
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.1.0.1   6.04 GB    256     8.6%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC10
UN  10.1.0.2   7.55 GB    256     7.4%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC8
UN  10.1.0.3   5.83 GB    256     7.0%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC9
UN  10.1.0.4    7.34 GB    256     7.9%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC6
UN  10.1.0.5   7.57 GB    256     8.0%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC7
UN  10.1.0.6   5.31 GB    256     7.3%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC10
UN  10.1.0.7   5.47 GB    256     8.6%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC9

I did a cql trace on the query and here is the trace, and it does say 
   Write timeout; received 3 of 4 required replies | 17:27:52,831 |  10.1.0.1 |        2002873

at the end. I guess that is where the client gets the error from. But the rows was inserted to Cassandra correctly. And I traced read with local_quorum and it behaves correctly and the reads don't go to DC2. The problem is only with writes on local_quorum.
{code}
Tracing session: 5a789fb0-b70d-11e4-8fca-99bff9c19890

 activity                                                                                                                                    | timestamp    | source      | source_elapsed
---------------------------------------------------------------------------------------------------------------------------------------------+--------------+-------------+----------------
                                                                                                                          execute_cql3_query | 17:27:50,828 |  10.1.0.1 |              0
 Parsing insert into test (user_id, created, event_data, event_id)values ( 123456789 , 9eab8950-b70c-11e4-8fca-99bff9c19891, 'test', '16'); | 17:27:50,828 |  10.1.0.1 |             39
                                                                                                                         Preparing statement | 17:27:50,828 |  10.1.0.1 |            135
                                                                                                           Message received from /10.1.0.1 | 17:27:50,829 |  10.1.0.5 |             25
                                                                                                              Sending message to /10.1.0.5 | 17:27:50,829 |  10.1.0.1 |            421
                                                                                                   Executing single-partition query on users | 17:27:50,829 |  10.1.0.5 |            177
                                                                                                                Acquiring sstable references | 17:27:50,829 |  10.1.0.5 |            191
                                                                                                                 Merging memtable tombstones | 17:27:50,830 |  10.1.0.5 |            208
                                                                                                           Message received from /10.1.0.5 | 17:27:50,830 |  10.1.0.1 |           1461
                                                                                                           Message received from /10.1.0.1 | 17:27:50,830 |  10.1.0.2 |             14
                                                                                                                 Key cache hit for sstable 1 | 17:27:50,830 |  10.1.0.5 |            254
                                                                                                        Processing response from /10.1.0.5 | 17:27:50,830 |  10.1.0.1 |           1514
                                                                                                   Executing single-partition query on users | 17:27:50,830 |  10.1.0.2 |             78
                                                                                                 Seeking to partition beginning in data file | 17:27:50,830 |  10.1.0.5 |            264
                                                                                                              Sending message to /10.1.0.2 | 17:27:50,830 |  10.1.0.1 |           1517
                                                                                                                Acquiring sstable references | 17:27:50,830 |  10.1.0.2 |             85
                                                                   Skipped 0/1 non-slice-intersecting sstables, included 0 due to tombstones | 17:27:50,830 |  10.1.0.5 |            453
                                                                                                           Determining replicas for mutation | 17:27:50,830 |  10.1.0.1 |           1746
                                                                                                                 Merging memtable tombstones | 17:27:50,830 |  10.1.0.2 |             97
                                                                                                  Merging data from memtables and 1 sstables | 17:27:50,830 |  10.1.0.5 |            476
                                                                                                           Message received from /10.1.0.2 | 17:27:50,830 |  10.1.0.1 |           2369
                                                                                                                 Key cache hit for sstable 2 | 17:27:50,830 |  10.1.0.2 |            120
                                                                                                          Read 1 live and 0 tombstoned cells | 17:27:50,830 |  10.1.0.5 |            506
                                                                                                 Seeking to partition beginning in data file | 17:27:50,830 |  10.1.0.2 |            123
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,830 |  10.1.0.5 |            570
                                                                   Skipped 0/1 non-slice-intersecting sstables, included 0 due to tombstones | 17:27:50,830 |  10.1.0.2 |            288
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,830 |  10.1.0.5 |            617
                                                                                                  Merging data from memtables and 1 sstables | 17:27:50,830 |  10.1.0.2 |            297
                                                                                                          Read 1 live and 0 tombstoned cells | 17:27:50,830 |  10.1.0.2 |            319
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,830 |  10.1.0.2 |            362
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,830 |  10.1.0.2 |            420
                                                                                                           Message received from /10.1.0.1 | 17:27:50,831 |   10.1.0.4 |             17
                                                                                                              Sending message to /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           2435
                                                                                                           Message received from /10.1.0.1 | 17:27:50,831 |  10.1.0.2 |              8
                                                                                                              Acquiring switchLock read lock | 17:27:50,831 |   10.1.0.4 |             61
                                                                                                        Processing response from /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           2488
                                                                                                              Acquiring switchLock read lock | 17:27:50,831 |  10.1.0.2 |             44
                                                                                                                      Appending to commitlog | 17:27:50,831 |   10.1.0.4 |             78
                                                                                    Not hinting /10.2.0.4 which has been down 364809650ms | 17:27:50,831 |  10.1.0.1 |           2503
                                                                                                                      Appending to commitlog | 17:27:50,831 |  10.1.0.2 |             62
                                                                                                                    Adding to event memtable | 17:27:50,831 |   10.1.0.4 |             96
                                                                                                               Sending message to /10.1.0.4 | 17:27:50,831 |  10.1.0.1 |           2557
                                                                                                                    Adding to event memtable | 17:27:50,831 |  10.1.0.2 |             80
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,831 |   10.1.0.4 |            177
                                                                                                              Acquiring switchLock read lock | 17:27:50,831 |  10.1.0.1 |           2669
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,831 |  10.1.0.2 |            160
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,831 |   10.1.0.4 |            333
                                                                                                                      Appending to commitlog | 17:27:50,831 |  10.1.0.1 |           2682
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,831 |  10.1.0.2 |            266
                                                                                                                    Adding to event memtable | 17:27:50,831 |  10.1.0.1 |           2720
                                                                                                             Sending message to /10.2.0.5 | 17:27:50,831 |  10.1.0.1 |           2758
                                                                                                           Message received from /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           2989
                                                                                                        Processing response from /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           3024
                                                                                                            Message received from /10.1.0.4 | 17:27:50,832 |  10.1.0.1 |           3764
                                                                                                         Processing response from /10.1.0.4 | 17:27:50,832 |  10.1.0.1 |           3805
                                                                                                           Message received from /10.1.0.1 | 17:27:50,841 | 10.2.0.5 |             24
                                                                                                   Enqueuing forwarded write to /10.2.0.7 | 17:27:50,841 | 10.2.0.5 |            255
                                                                                                   Enqueuing forwarded write to /10.2.0.3 | 17:27:50,841 | 10.2.0.5 |            283
                                                                                                    Enqueuing forwarded write to /10.2.0.6 | 17:27:50,841 | 10.2.0.5 |            307
                                                                                                              Acquiring switchLock read lock | 17:27:50,841 | 10.2.0.5 |            362
                                                                                                                      Appending to commitlog | 17:27:50,841 | 10.2.0.5 |            380
                                                                                                             Sending message to /10.2.0.7 | 17:27:50,841 | 10.2.0.5 |            382
                                                                                                                    Adding to event memtable | 17:27:50,841 | 10.2.0.5 |            411
                                                                                                              Sending message to /10.2.0.6 | 17:27:50,841 | 10.2.0.5 |            429
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,841 | 10.2.0.5 |            484
                                                                                                             Sending message to /10.2.0.3 | 17:27:50,841 | 10.2.0.5 |            561
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,841 | 10.2.0.5 |            625
                                                                                                              Acquiring switchLock read lock | 17:27:50,842 |  10.2.0.6 |           1031
                                                                                                              Acquiring switchLock read lock | 17:27:50,842 | 10.2.0.7 |            178
                                                                                                                      Appending to commitlog | 17:27:50,842 |  10.2.0.6 |           1066
                                                                                                                      Appending to commitlog | 17:27:50,842 | 10.2.0.7 |            196
                                                                                                                    Adding to event memtable | 17:27:50,842 |  10.2.0.6 |           1118
                                                                                                                    Adding to event memtable | 17:27:50,842 | 10.2.0.7 |            231
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,842 |  10.2.0.6 |           1181
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,842 | 10.2.0.7 |            286
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,842 | 10.2.0.7 |            421
                                                                                                              Acquiring switchLock read lock | 17:27:50,843 | 10.2.0.3 |           1216
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,843 |  10.2.0.6 |           1382
                                                                                                                      Appending to commitlog | 17:27:50,843 | 10.2.0.3 |           1239
                                                                                                                    Adding to event memtable | 17:27:50,843 | 10.2.0.3 |           1313
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,843 | 10.2.0.3 |           1407
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,843 | 10.2.0.3 |           1631
                                                                                                          Message received from /10.2.0.5 | 17:27:50,851 |  10.1.0.1 |          23333
                                                                                                       Processing response from /10.2.0.5 | 17:27:50,851 |  10.1.0.1 |          23380
                                                                                                          Message received from /10.2.0.7 | 17:27:50,852 |  10.1.0.1 |          23908
                                                                                                       Processing response from /10.2.0.7 | 17:27:50,852 |  10.1.0.1 |          23953
                                                                                                           Message received from /10.2.0.6 | 17:27:50,853 |  10.1.0.1 |          25143
                                                                                                        Processing response from /10.2.0.6 | 17:27:50,853 |  10.1.0.1 |          25178
                                                                                                          Message received from /10.2.0.3 | 17:27:50,854 |  10.1.0.1 |          25478
                                                                                                       Processing response from /10.2.0.3 | 17:27:50,854 |  10.1.0.1 |          25516
                                                                                             Write timeout; received 3 of 4 required replies | 17:27:52,831 |  10.1.0.1 |        2002873
                                                                                                                            Request complete | 17:27:52,833 |  10.1.0.1 |        2005989
{code}
cqlsh:XXXX> CONSISTENCY
Current consistency level is LOCAL_QUORUM.",qa-resolved,[],CASSANDRA,Bug,Normal,2015-02-18 02:05:37,2
12772443,Don't check for overlap with sstables that have had their start positions moved in LCS,"When picking compaction candidates in LCS, we check that we won't cause any overlap in the higher level. Problem is that we compare the files that have had their start positions moved meaning we can cause overlap. We need to also include the tmplink files when checking this.

Note that in 2.1 overlap is not as big problem as earlier, if adding an sstable would cause overlap, we send it back to L0 instead, meaning we do a bit more compaction but we never actually have overlap.",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-02-04 18:12:40,0
12771386,Convert SequentialWriter from using RandomAccessFile to nio channel,"For non-mmap'ed I/O on Windows, using nio channels will give us substantially more flexibility w/regards to renaming and moving files around while writing them.  This change in conjunction with CASSANDRA-4050 should allow us to remove the Windows bypass code in SSTableRewriter for non-memory-mapped I/O.

In general, migrating from instances of RandomAccessFile to nio channels will help make Windows and linux behavior more consistent.",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2015-01-30 17:58:59,6
12768797,long-test LongLeveledCompactionStrategyTest flaps in 2.0,"LongLeveledCompactionStrategyTest periodically fails with:
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 54.412 sec
    [junit] 
    [junit] Testcase: testParallelLeveledCompaction(org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest):      Caused an ERROR
    [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit]     at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:413)
    [junit]     at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:402)
    [junit]     at org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest.testParallelLeveledCompaction(LongLeveledCompactionStrategyTest.java:97)
    [junit] Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit]     at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:188)
    [junit]     at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:409)
    [junit] Caused by: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit]     at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:143)
    [junit]     at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:166)
    [junit]     at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:167)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit]     at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
    [junit]     at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
    [junit]     at org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest$1.run(LongLeveledCompactionStrategyTest.java:87)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    [junit]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    [junit]     at java.lang.Thread.run(Thread.java:745)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest FAILED
{noformat}

I would guess the failure is 10-20% of the time, looping over the test repeatedly.

----

On the 2.1 branch, the failure is different, so perhaps this could also be updated.
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.04 sec
    [junit] 
    [junit] Testcase: testParallelLeveledCompaction(org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest):      Caused an ERROR
    [junit] org.apache.cassandra.db.compaction.WrappingCompactionStrategy cannot be cast to org.apache.cassandra.db.compaction.LeveledCompactionStrategy
    [junit] java.lang.ClassCastException: org.apache.cassandra.db.compaction.WrappingCompactionStrategy cannot be cast to org.apache.cassandra.db.compaction.LeveledCompactionStrategy
    [junit]     at org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest.testParallelLeveledCompaction(LongLeveledCompactionStrategyTest.java:45)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest FAILED
{noformat}",test-failure,['Legacy/Testing'],CASSANDRA,Improvement,Low,2015-01-20 21:19:43,3
12767804,"Faster sequential IO (on compaction, streaming, etc)","When node is doing a lot of sequencial IO (streaming, compacting, etc) a lot of CPU is lost in calls to RAF's int read() and DataOutputStream's write(int).
This is because default implementations of readShort,readLong, etc as well as their matching write* are implemented with numerous calls of byte by byte read and write. 
This makes a lot of syscalls as well.

A quick microbench shows than just reimplementation of these methods in either way gives 8x speed increase.

A patch attached implements RandomAccessReader.read<Type> and SequencialWriter.write<Type> methods in more efficient way.
I also eliminated some extra byte copies in CompositeType.split and ColumnNameHelper.maxComponents, which were on my profiler's hotspot method list during tests.

A stress tests on my laptop show that this patch makes compaction 25-30% faster  on uncompressed sstables and 15% faster for compressed ones.

A deployment to production shows much less CPU load for compaction. 
(I attached a cpu load graph from one of our production, orange is niced CPU load - i.e. compaction; yellow is user - i.e. not compaction related tasks)

",compaction performance,['Legacy/Tools'],CASSANDRA,Improvement,Normal,2015-01-15 18:45:33,3
12767650,sstablesplit fails *randomly* with Data component is missing,"I'm experiencing an issue related to sstablesplit. I would like to understand if I am doing something wrong or there is an issue in the split process. The process fails randomly with the following exception:
{code}
ERROR 02:17:36 Error in ThreadPoolExecutor
java.lang.AssertionError: Data component is missing for sstable./tools/bin/../../data/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-16
{code}

See attached output.log file. The process never stops after this exception and I've also seen the dataset growing indefinitely (number of sstables).  

* I have not been able to reproduce the issue with a single sstablesplit command. ie, specifying all files with glob matching.
* I can reproduce the bug if I call multiple sstablesplit one file at the time (the way ccm does)

Here is the test case file to reproduce the bug:

https://drive.google.com/file/d/0BwZ_GPM33j6KdVh0NTdkOWV2R1E/view?usp=sharing

1. Download the split_issue.tar.gz file. It includes latest cassandra-2.1 branch binaries.
2. Extract it
3. CD inside the use case directory
4. Download the dataset (2G) just to be sure we have the same thing, and place it in the working directory.
   https://docs.google.com/uc?id=0BwZ_GPM33j6KV3ViNnpPcVFndUU&export=download
5. The first time, run ./test.sh. This will setup and run a test.
6. The next times, you can only run ./test --no-setup . This will only reset the dataset as its initial state and re-run the test. You might have to run the tests some times before experiencing it... but I'm always able with only 2-3 runs.
",qa-resolved,['Legacy/Tools'],CASSANDRA,Bug,Normal,2015-01-15 03:40:43,0
12766905,Windows - SSTableRewriterTest fails on trunk,"Right at the top of the test, we see:

{noformat}
    [junit] ERROR 18:15:05 Unable to delete build\test\cassandra\data;0\SSTableRewriterTest\Standard1-e63f49c09a8611e4bebb8ff5e6ab1035\tmplink-la-27-big-Data.db (it will be removed on server restart; we'll also retry after GC)
    [junit] ERROR 18:15:05 Unable to delete build\test\cassandra\data;0\SSTableRewriterTest\Standard1-e63f49c09a8611e4bebb8ff5e6ab1035\tmplink-la-27-big-Data.db (it will be removed on server restart; we'll also retry after GC)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testFileRemoval(org.apache.cassandra.io.sstable.SSTableRewriterTest):     FAILED
    [junit] expected:<0> but was:<2>
    [junit] junit.framework.AssertionFailedError: expected:<0> but was:<2>
    [junit]     at org.apache.cassandra.io.sstable.SSTableRewriterTest.assertFileCounts(SSTableRewriterTest.java:758)
    [junit]     at org.apache.cassandra.io.sstable.SSTableRewriterTest.testFileRemoval(SSTableRewriterTest.java:229)
{noformat}

The rest cascade after that.",Windows,"['Legacy/Local Write-Read Paths', 'Legacy/Testing']",CASSANDRA,Bug,Low,2015-01-12 18:16:26,6
12765749,AssertionErrors after activating unchecked_tombstone_compaction with leveled compaction,"During our upgrade of Cassandra from version 2.0.7 to 2.1.2 we experienced a serious problem regarding the setting unchecked_tombstone_compaction in combination with leveled compaction strategy.

In order to prevent tombstone-threshold-warnings we activated the setting for a specific table after the upgrade. Some time after that we observed new errors in our log files:
{code}
INFO  [CompactionExecutor:184] 2014-12-11 12:36:06,597 CompactionTask.java:136 - Compacting [SSTableReader(path='/data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1848-Data.db'), SSTableReader(path='/
data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1847-Data.db'), SSTableReader(path='/data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1845-Data.db'), SSTableReader
(path='/data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1846-Data.db')]
ERROR [CompactionExecutor:183] 2014-12-11 12:36:06,613 CassandraDaemon.java:153 - Exception in thread Thread[CompactionExecutor:183,1,main]
java.lang.AssertionError: /data/cassandra/data/metrigo_prod/new_user_data/metrigo_prod-new_user_data-tmplink-ka-705732-Data.db
        at org.apache.cassandra.io.sstable.SSTableReader.getApproximateKeyCount(SSTableReader.java:243) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:146) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:75) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:232) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_45]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
        {code}
Obviously that error aborted the compaction and after some time the number of pending compactions became very high on every node. Of course, this in turn had a negative impact on several other metrics.

After reverting the setting we had to restart all nodes. After that compactions could finish again and the pending compactions could be worked off.",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2015-01-08 12:03:08,0
12764841,OOM caused by large tombstone warning.,"When running with high amount of tombstones the error message generation from CASSANDRA-6117 can lead to out of memory situation with the default setting.

Attached a heapdump viewed in visualvm showing how this construct created two 777mb strings to print the error message for a read query and then crashed OOM.

{code}
        if (respectTombstoneThresholds() && columnCounter.ignored() > DatabaseDescriptor.getTombstoneWarnThreshold())
        {
            StringBuilder sb = new StringBuilder();
            CellNameType type = container.metadata().comparator;
            for (ColumnSlice sl : slices)
            {
                assert sl != null;

                sb.append('[');
                sb.append(type.getString(sl.start));
                sb.append('-');
                sb.append(type.getString(sl.finish));
                sb.append(']');
            }

            logger.warn(""Read {} live and {} tombstoned cells in {}.{} (see tombstone_warn_threshold). {} columns was requested, slices={}, delInfo={}"",
                        columnCounter.live(), columnCounter.ignored(), container.metadata().ksName, container.metadata().cfName, count, sb, container.deletionInfo());
        }
{code}

",tombstone,[],CASSANDRA,Bug,Normal,2015-01-05 05:19:50,1
12764771,deleted row still can be selected out,"first
{code}CREATE  KEYSPACE space1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
CREATE  TABLE space1.table3(a int, b int, c text,primary key(a,b));
CREATE  KEYSPACE space2 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};{code}
second
{code}CREATE  TABLE space2.table1(a int, b int, c int, primary key(a,b));
CREATE  TABLE space2.table2(a int, b int, c int, primary key(a,b));
INSERT INTO space1.table3(a,b,c) VALUES(1,1,'1');
drop table space2.table1;
DELETE FROM space1.table3 where a=1 and b=1;
drop table space2.table2;
select * from space1.table3 where a=1 and b=1;{code}

you will find that the row (a=1 and b=1)  in space1.table3 is not deleted.",qa-resolved,[],CASSANDRA,Bug,Urgent,2015-01-04 09:40:08,2
12764439,Windows - Failure to rename file during compaction - unit test only,"This occurs on both 2.1 and trunk on Windows.

The error is as follows:
{noformat}
ERROR 21:42:48 Fatal exception in thread Thread[CompactionExecutor:3,1,main]
java.lang.RuntimeException: Failed to rename build\test\cassandra\data;0\system\schema_columns-296e9c049bec3085827dc17d3df2122a\system-schema_columns-tmp-ka-10-Index.db to build\test\cassandra\data;0\system\schema_columns-296e9c049bec3085827dc17d3df2122a\system-schema_columns-ka-10-Index.db
at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:170) ~[main/:na]
at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:154) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:569) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:561) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.java:535) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableWriter.finish(SSTableWriter.java:470) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableRewriter.finishAndMaybeThrow(SSTableRewriter.java:349) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewriter.java:324) ~[main/:na]
at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewriter.java:304) ~[main/:na]
at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201) ~[main/:na]
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:75) ~[main/:na]
at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[main/:na]
at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:226) ~[main/:na]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_71]
at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_71]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_71]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]
at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]
Caused by: java.nio.file.FileSystemException: build\test\cassandra\data;0\system\schema_columns-296e9c049bec3085827dc17d3df2122a\system-schema_columns-tmp-ka-10-Index.db -> build\test\cassandra\data;0\system\schema_columns-296e9c049bec3085827dc17d3df2122a\system-schema_columns-ka-10-Index.db: The process cannot access the file because it is being used by another process.
{noformat}

The rename operation from -tmp to final sstable name fails on Windows as something still has a handle open to the file.  This occurs during unit tests only so marking as minor, but it's incredibly noisy so it's best to fix it.",Windows,"['Legacy/Testing', 'Local/Compaction']",CASSANDRA,Bug,Low,2015-01-01 00:13:16,6
12764055,RangeTombstoneList becoming bottleneck on tombstone heavy tasks,"I would like to propose a change of the data structure used in the RangeTombstoneList to store and insert tombstone ranges to something with at least O(log N) insert in the middle and at near O(1) and start AND end. Here is why:

When having tombstone heavy work-loads the current implementation of RangeTombstoneList becomes a bottleneck with slice queries.

Scanning the number of tombstones up to the default maximum (100k) can take up to 3 minutes of how addInternal() scales on insertion of middle and start elements.

The attached test shows that with 50k deletes from both sides of a range.

INSERT 1...110000
flush()
DELETE 1...50000
DELETE 110000...60000

While one direction performs ok (~400ms on my notebook):
{code}
SELECT * FROM timeseries WHERE name = 'a' ORDER BY timestamp DESC LIMIT 1
{code}

The other direction underperforms (~7seconds on my notebook)
{code}
SELECT * FROM timeseries WHERE name = 'a' ORDER BY timestamp ASC LIMIT 1
{code}
",tombstone,['Legacy/CQL'],CASSANDRA,Improvement,Normal,2014-12-29 17:51:48,6
12764043,Cassandra could not start with NPE in ColumnFamilyStore.removeUnfinishedCompactionLeftovers,"It happens sometimes after restarts caused by undeletable files under Windows.

{quote}
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:579)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:232)
    at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:377)
    at com.jetbrains.cassandra.service.CassandraServiceMain.start(CassandraServiceMain.java:81)
    ... 6 more
{quote}",windows,['Local/Startup and Shutdown'],CASSANDRA,Bug,Low,2014-12-29 16:29:27,6
12763538,java.lang.RuntimeException: Failed to rename XXX to YYY,"{code}
java.lang.RuntimeException: Failed to rename build\test\cassandra\data;0\system\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\system-schema_keyspaces-tmp-ka-5-Index.db to build\test\cassandra\data;0\system\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\system-schema_keyspaces-ka-5-Index.db
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:170) ~[main/:na]
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:154) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:569) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:561) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.java:535) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableWriter.finish(SSTableWriter.java:470) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableRewriter.finishAndMaybeThrow(SSTableRewriter.java:349) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewriter.java:324) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewriter.java:304) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:200) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:75) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:226) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
Caused by: java.nio.file.FileSystemException: build\test\cassandra\data;0\system\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\system-schema_keyspaces-tmp-ka-5-Index.db -> build\test\cassandra\data;0\system\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\system-schema_keyspaces-ka-5-Index.db: The process cannot access the file because it is being used by another process.

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86) ~[na:1.7.0_45]
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97) ~[na:1.7.0_45]
	at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0_45]
	at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287) ~[na:1.7.0_45]
	at java.nio.file.Files.move(Files.java:1345) ~[na:1.7.0_45]
	at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUtils.java:184) ~[main/:na]
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:166) ~[main/:na]
	... 18 common frames omitted
{code}",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2014-12-23 20:04:28,6
12761143,Constant compaction under LCS,"It appears that tables configured with LCS will completely re-compact themselves over some period of time after upgrading from 2.0 to 2.1 (2.0.11 -> 2.1.2, specifically). It starts out with <10 pending tasks for an hour or so, then starts building up, now with 50-100 tasks pending across the cluster after 12 hours. These nodes are under heavy write load, but were easily able to keep up in 2.0 (they rarely had >5 pending compaction tasks), so I don't think it's LCS in 2.1 actually being worse, just perhaps some different LCS behavior that causes the layout of tables from 2.0 to prompt the compactor to reorganize them?

The nodes flushed ~11MB SSTables under 2.0. They're currently flushing ~36MB SSTables due to the improved memtable setup in 2.1. Before I upgraded the entire cluster to 2.1, I noticed the problem and tried several variations on the flush size, thinking perhaps the larger tables in L0 were causing some kind of cascading compactions. Even if they're sized roughly like the 2.0 flushes were, same behavior occurs. I also tried both enabling & disabling STCS in L0 with no real change other than L0 began to back up faster, so I left the STCS in L0 enabled.

Tables are configured with 32MB sstable_size_in_mb, which was found to be an improvement on the 160MB table size for compaction performance. Maybe this is wrong now? Otherwise, the tables are configured with defaults. Compaction has been unthrottled to help them catch-up. The compaction threads stay very busy, with the cluster-wide CPU at 45% ""nice"" time. No nodes have completely caught up yet. I'll update JIRA with status about their progress if anything interesting happens.

From a node around 12 hours ago, around an hour after the upgrade, with 19 pending compaction tasks:
SSTables in each level: [6/4, 10, 105/100, 268, 0, 0, 0, 0, 0]
SSTables in each level: [6/4, 10, 106/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [1, 16/10, 105/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [5/4, 10, 103/100, 272, 0, 0, 0, 0, 0]
SSTables in each level: [4, 11/10, 105/100, 270, 0, 0, 0, 0, 0]
SSTables in each level: [1, 12/10, 105/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [1, 14/10, 104/100, 267, 0, 0, 0, 0, 0]
SSTables in each level: [9/4, 10, 103/100, 265, 0, 0, 0, 0, 0]

Recently, with 41 pending compaction tasks:
SSTables in each level: [4, 13/10, 106/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [4, 12/10, 106/100, 273, 0, 0, 0, 0, 0]
SSTables in each level: [5/4, 11/10, 106/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [4, 12/10, 103/100, 275, 0, 0, 0, 0, 0]
SSTables in each level: [2, 13/10, 106/100, 273, 0, 0, 0, 0, 0]
SSTables in each level: [3, 10, 104/100, 275, 0, 0, 0, 0, 0]
SSTables in each level: [6/4, 11/10, 103/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [4, 16/10, 105/100, 264, 0, 0, 0, 0, 0]

More information about the use case: writes are roughly uniform across these tables. The data is ""sharded"" across these 8 tables by key to improve compaction parallelism. Each node receives up to 75,000 writes/sec sustained at peak, and a small number of reads. This is a pre-production cluster that's being warmed up with new data, so the low volume of reads (~100/sec per node) is just from automatic sampled data checks, otherwise we'd just use STCS :)",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2014-12-11 19:22:16,0
12759729,Standalone Scrubber broken for LCS,"After CASSANDRA-8004, the compaction strategy for a column family will not be instanceof LeveledCompactionStrategy (StandaloneScrubber.java:100), so we don't check the manifest.",lcs,"['Legacy/Tools', 'Local/Compaction']",CASSANDRA,Bug,Low,2014-12-05 17:12:25,0
12759478,Error during start up on windows,"While using ccm with the current C* 2.1-HEAD code on Windows, I frequently see this exception.
{code}[node1 ERROR] Exception calling ""BeginConnect"" with ""4"" argument(s): ""The requested address 
is not valid in its context""
At 
D:\jenkins\workspace\cassandra-2.1_dtest_win32\cassandra\bin\cassandra.ps1:358 
char:9
+         $connect = $tcpobject.BeginConnect($listenAddress, $port, $null, 
$null)
+         
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [], MethodInvocationException
    + FullyQualifiedErrorId : SocketException
 
You cannot call a method on a null-valued expression.
At 
D:\jenkins\workspace\cassandra-2.1_dtest_win32\cassandra\bin\cassandra.ps1:359 
char:9
+         $wait = $connect.AsyncWaitHandle.WaitOne(25, $false)
+         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (:) [], RuntimeException
    + FullyQualifiedErrorId : InvokeMethodOnNull{code}

I have not yet seen this exception when psutil is not installed, but that may not be relevant, as I dont know how that could possibly matter.",windows,['Packaging'],CASSANDRA,Bug,Low,2014-12-04 17:53:01,6
12758861,Add option to set max_sstable_age in fractional days in DTCS,"Using days as the unit for max_sstable_age in DTCS might be too much, add option to set it in seconds",doc-impacting dtcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2014-12-02 10:46:05,0
12755926,LeveledCompactionStrategy should split large files across data directories when compacting,"Because we fall back to STCS for L0 when LCS gets behind, the sstables in L0 can get quite large during sustained periods of heavy writes.  This can result in large imbalances between data volumes when using JBOD support.  

Eventually these large files get broken up as L0 sstables are moved up into higher levels; however, because LCS only chooses a single volume on which to write all of the sstables created during a single compaction, the imbalance is persisted.",lcs,['Local/Compaction'],CASSANDRA,Improvement,Normal,2014-11-17 21:43:42,0
12755467,SStablesplit behavior changed,"The dtest sstablesplit_test.py has begun failing due to an incorrect number of sstables being created after running sstablesplit.

http://cassci.datastax.com/job/cassandra-2.1_dtest/559/changes#detail1
is the run where the failure began.

In 2.1.x, the test expects 7 sstables to be created after split, but instead 12 are being created. All of the data is there, and the sstables add up to the expected size, so this simply may be a change in default behavior. The test runs sstablesplit without the --size argument, and the default has not changed, so it is unexpected that the behavior would change in a minor point release.",qa-resolved,['Legacy/Tools'],CASSANDRA,Bug,Low,2014-11-14 20:02:10,0
12755154,Windows: RepairOptionTest fails on trunk,"{noformat}
    [junit] [main] WARN org.apache.cassandra.repair.messages.RepairOption - Snapshot-based repair is not yet supported on Windows.  Reverting to parallel repair.
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testParseOptions(org.apache.cassandra.repair.messages.RepairOptionTest):  FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.cassandra.repair.messages.RepairOptionTest.testParseOptions(RepairOptionTest.java:45)
{noformat}

Just need to re-enable snapshot-based repairs on trunk on Windows (since that works)",Windows,['Legacy/Testing'],CASSANDRA,Bug,Low,2014-11-13 20:34:51,6
12755148,Windows: Commitlog access violations on unit tests,"We have four unit tests failing on trunk on Windows, all with FileSystemException's related to the SchemaLoader:

{noformat}
[junit] Test org.apache.cassandra.db.compaction.DateTieredCompactionStrategyTest FAILED
[junit] Test org.apache.cassandra.cql3.ThriftCompatibilityTest FAILED
[junit] Test org.apache.cassandra.io.sstable.SSTableRewriterTest FAILED
[junit] Test org.apache.cassandra.repair.LocalSyncTaskTest FAILED
{noformat}

Example error:
{noformat}
    [junit] Caused by: java.nio.file.FileSystemException: build\test\cassandra\commitlog;0\CommitLog-5-1415908745965.log: The process cannot access the file because it is being used by another process.
    [junit]
    [junit]     at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
    [junit]     at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
    [junit]     at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    [junit]     at java.nio.file.Files.delete(Files.java:1079)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:125)
{noformat}
",Windows,"['Legacy/Local Write-Read Paths', 'Legacy/Testing']",CASSANDRA,Bug,Low,2014-11-13 20:20:01,6
12754861,"Create a tool that given a bunch of sstables creates a ""decent"" sstable leveling","In old versions of cassandra (i.e. not trunk/3.0), when bootstrapping a new node, you will end up with a ton of files in L0 and it might be extremely painful to get LCS to compact into a new leveling

We could probably exploit the fact that we have many non-overlapping sstables in L0, and offline-bump those sstables into higher levels. It does not need to be perfect, just get the majority of the data into L1+ without creating overlaps.

So, suggestion is to create an offline tool that looks at the range each sstable covers and tries to bump it as high as possible in the leveling.",lcs,[],CASSANDRA,Improvement,Normal,2014-11-12 19:41:40,0
12753195,parse_for_table_meta errors out on queries with undefined grammars,"CASSANDRA-6910 introduced changes to the cqlsh function {{parse_for_table_meta}} that cause an error to be thrown whenever a query does not have its grammar defined in cql3handling.py. However, this is affecting queries that are legitimate cql syntax and are accepted by Cassandra, but aren't defined in cqlsh. ",cqlsh,[],CASSANDRA,Bug,Normal,2014-11-05 22:05:34,4
12753125,SELECT ... TOKEN() function broken in C* 2.1.1,"{code:title=Cassandra 2.1.1, Oracle Java 1.7.0_72, Ubuntu 14.04.1 64 bit}
cqlsh:blink> show version;
[cqlsh 5.0.1 | Cassandra 2.1.1 | CQL spec 3.2.0 | Native protocol v3]
cqlsh:blink> select token(id) from users limit 1;
list index out of range
{code}

versus

{code:title=Cassandra 2.1.0, Oracle Java 1.7.0_67, Ubuntu 12.04.5 64 bit}
cqlsh:blink> show version;
[cqlsh 5.0.1 | Cassandra 2.1.0 | CQL spec 3.2.0 | Native protocol v3]
cqlsh:blink> select token(id) from users limit 1;

 token(id)
----------------------
 -9223237793432919630

(1 rows)
{code}

It also fails with C* 2.1.1, Java 1.7.0_72, Ubuntu 12.04.5.",cqlsh qa-resolved,[],CASSANDRA,Bug,Normal,2014-11-05 18:43:31,4
12751745,Make comments/default paths in cassandra-env.ps1 more Windows-centric,"When porting launch-script functionality over some of the disabled-by-default options in cassandra-env.ps1 weren't modified.  They're decidedly non-Windows:
{code}
   # Configure the following for JEMallocAllocator and if jemalloc is not available in the system
    # library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
{code}

Trivial effort - just update those to reflect sane windows defaults/paths/ecosystem.  Should have 0 effect on runtime defaults as they're all already Windows-ified.",Windows,['Packaging'],CASSANDRA,Improvement,Low,2014-10-30 17:33:34,6
12751446,Grant Permission fails if permission had been revoked previously,"The dtest auth_test.py:TestAuth.alter_cf_auth_test is failing. 

{code}
        cassandra.execute(""GRANT ALTER ON ks.cf TO cathy"")
        cathy.execute(""ALTER TABLE ks.cf ADD val int"")

        cassandra.execute(""REVOKE ALTER ON ks.cf FROM cathy"")
        self.assertUnauthorized(""User cathy has no ALTER permission on <table ks.cf> or any of its parents"",
                                cathy, ""CREATE INDEX ON ks.cf(val)"")

        cassandra.execute(""GRANT ALTER ON ks.cf TO cathy"")
        cathy.execute(""CREATE INDEX ON ks.cf(val)"")
{code}

In this section of code, the user cathy is granted ""ALTER"" permissions on 'ks.cf', then they are revoked, then granted again. Monitoring system_auth.permissions during this section of code show that the permission is added with the initial grant, and revoked properly, but the table remains empty after the second grant.

When the cathy user attempts to create an index, the following exception is thrown:

{code}
Unauthorized: code=2100 [Unauthorized] message=""User cathy has no ALTER permission on <table ks.cf> or any of its parents""
{code}",qa-resolved,[],CASSANDRA,Bug,Normal,2014-10-29 18:02:48,1
12751429,Archive Commitlog Test Failing,"The test snapshot_test.TestArchiveCommitlog.test_archive_commitlog is failing on 2.0.11, but not 2.1.1. We attempt to replay 65000 rows, but in 2.0.11 only 63000 rows succeed. URL for test output:

http://cassci.datastax.com/job/cassandra-2.0_dtest/lastCompletedBuild/testReport/snapshot_test/TestArchiveCommitlog/test_archive_commitlog/",qa-resolved,[],CASSANDRA,Improvement,Normal,2014-10-29 17:02:31,4
12751400,Overlapping sstables in L1+,"Seems we have a bug that can create overlapping sstables in L1:

{code}
WARN [main] 2014-10-28 04:09:42,295 LeveledManifest.java (line 164) At level 2, SSTableReader(path='<sstable>') [DecoratedKey(2838397575996053472, 00
10000000001111000000000000066059b200001000000000066059b200000000111100000000100000000000004000000000000000000100), DecoratedKey(5516674013223138308, 001000000000111100000000000000ff2d160000100000000000ff2d1600000
000111100000000100000000000004000000000000000000100)] overlaps SSTableReader(path='<sstable>') [DecoratedKey(2839992722300822584, 00100000000011110000
0000000000229ad20000100000000000229ad200000000111100000000100000000000004000000000000000000100), DecoratedKey(5532836928694021724, 0010000000001111000000000000034b05a600001000000000034b05a600000000111100000000100
000000000004000000000000000000100)].  This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. Sending back to L0.  If
 you didn't drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable
{code}

Which might manifest itself during compaction with this exception:
{code}
ERROR [CompactionExecutor:3152] 2014-10-28 00:24:06,134 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:3152,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(5516674013223138308, 001000000000111100000000000000ff2d160000100000000000ff2d1600000000111100000000100000000000004000000000000000000100) >= current key DecoratedKey(2839992722300822584, 001000000000111100000000000000229ad20000100000000000229ad200000000111100000000100000000000004000000000000000000100) writing into <sstable>
{code}
since we use LeveledScanner when compacting (the backing sstable scanner might go beyond the start of the next sstable scanner)",lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2014-10-29 15:33:01,0
12751333,Deleting columns breaks secondary index on clustering column,"Removing items from a set breaks index for field {{id}}:

{noformat}
cqlsh:cs> CREATE TABLE buckets (
      ...   tenant int,
      ...   id int,
      ...   items set<text>,
      ...   PRIMARY KEY (tenant, id)
      ... );
cqlsh:cs> CREATE INDEX buckets_ids ON buckets(id);
cqlsh:cs> INSERT INTO buckets (tenant, id, items) VALUES (1, 1, {'foo', 'bar'});
cqlsh:cs> SELECT * FROM buckets;

 tenant | id | items
--------+----+----------------
      1 |  1 | {'bar', 'foo'}

(1 rows)

cqlsh:cs> SELECT * FROM buckets WHERE id = 1;

 tenant | id | items
--------+----+----------------
      1 |  1 | {'bar', 'foo'}

(1 rows)

cqlsh:cs> UPDATE buckets SET items=items-{'foo'} WHERE tenant=1 AND id=1;
cqlsh:cs> SELECT * FROM buckets;

 tenant | id | items
--------+----+---------
      1 |  1 | {'bar'}

(1 rows)

cqlsh:cs> SELECT * FROM buckets WHERE id = 1;

(0 rows)
{noformat}

Re-building the index fixes the issue:

{noformat}
cqlsh:cs> DROP INDEX buckets_ids;
cqlsh:cs> CREATE INDEX buckets_ids ON buckets(id);
cqlsh:cs> SELECT * FROM buckets WHERE id = 1;

 tenant | id | items
--------+----+---------
      1 |  1 | {'bar'}

(1 rows)
{noformat}

Adding items does not cause similar failure, only delete. Also didn't test if other collections are also affected(?)",qa-resolved,['Feature/2i Index'],CASSANDRA,Bug,Urgent,2014-10-29 08:35:11,2
12751061,CQL Spec needs to be updated with DateTieredCompactionStrategy,The {{CREATE TABLE}} section of the CQL Specification isn't up to date for the latest {{DateTieredCompactionStrategy}} that has been added in 2.0.11 and 2.1.1. We need to cover all its options just like it's done for the other strategies.,dtcs,[],CASSANDRA,Task,Low,2014-10-28 11:51:48,0
12750639,Compactions stop completely because of RuntimeException in CompactionExecutor,"I have a cluster that is recovering from being overloaded with writes.  I am using the workaround from CASSANDRA-6621 to prevent the STCS fallback (which is killing the cluster - see CASSANDRA-7949). 

I have observed that after one or more exceptions like this

{code}
ERROR [CompactionExecutor:4087] 2014-10-26 22:50:05,016 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:4087,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(425124616570337476, 0010000000001111000000000000033523da00001000000000033523da000000001111000000001000000000
00004000000000000000000100) >= current key DecoratedKey(-8778432288598355336, 0010000000001111000000000000040c7a8f00001000000000040c7a8f000000001111000000001000000000
00004000000000000000000100) writing into /cassandra-data/disk2/myks/mytable/myks-mytable-tmp-jb-130379-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

the node completely stops the compactions and I end up in the state like this:

{code}
# nodetool compactionstats
pending tasks: 1288
          compaction type        keyspace           table       completed           total      unit  progress
Active compaction remaining time :        n/a
{code}

The node recovers if restarted and starts compactions - until getting more exceptions like this.
",lcs,[],CASSANDRA,Bug,Normal,2014-10-26 23:17:10,0
12750196,No error message shown when starting Cassandra on Windows if Cassandra is already running,"The output from running cassandra.bat while Cassandra is already running gives no indication that Cassandra was already running, or that the command failed. Instead it prints out {code}Starting Cassandra Server.{code}",windows,['Packaging'],CASSANDRA,Bug,Low,2014-10-23 20:29:15,6
12749501,Opening results early with leveled compactions broken,"CASSANDRA-8034 notifies the listeners whenever we replace an sstable to make sure we have track the right instance.

Problem is though that when we open early and finish a compaction, we try to re-add the same sstable to the manifest which drops it to level 0 since it overlaps with the one that is already there",lcs,['Local/Compaction'],CASSANDRA,Bug,Urgent,2014-10-21 13:10:51,0
12748955,Windows Service never finishes shutting down,"When using procrun and the -install combination on Windows and starting cassandra via services.msc, stopping the service never completes and gets stuck in ""stopping"" status forever.  Probably related to:

{code}
    public void stop()
    {
        // this doesn't entirely shut down Cassandra, just the RPC server.
        // jsvc takes care of taking the rest down
        logger.info(""Cassandra shutting down..."");
        thriftServer.stop();
        nativeServer.stop();
    }
{code}

procrun calls the StopMethod as CassandraDaemon.stop so we may need to either a) augment what procrun's doing or b) add a more comprehensive stop to be called on Windows shutdown.",windows,['Local/Startup and Shutdown'],CASSANDRA,Bug,Low,2014-10-17 19:22:00,6
12748313,Stopping a node during compaction can make already written files stay around,"In leveled compaction we generally create many files during compaction, in 2.0 we left the ones we had written as -tmp- files, in 2.1 we close and open the readers, removing the -tmp- markers.

This means that any ongoing compactions will leave the resulting files around if we restart. Note that stop:ing the compaction will cause an exception and that makes us call abort() on the SSTableRewriter which removes the files.

Guess a fix could be to keep the -tmp- marker and make -tmplink- files until we are actually done with the compaction.",triaged,['Local/Compaction'],CASSANDRA,Bug,Normal,2014-10-15 15:40:38,0
12748069,Windows install scripts fail to set logdir and datadir,"After CASSANDRA-7136, the install scripts to run Cassandra as a service fail on both the legacy and the powershell paths.  Looks like they need to have
{code}
++JvmOptions=-Dcassandra.logdir=%CASSANDRA_HOME%\logs ^
++JvmOptions=-Dcassandra.storagedir=%CASSANDRA_HOME%\data
{code}
added to function correctly.

We should take this opportunity to make sure the source of the java options is uniform for both running and installation to prevent mismatches like this in the future.",Windows,['Packaging'],CASSANDRA,Bug,Normal,2014-10-14 16:14:16,6
12747344,"cassandra-cli and cqlsh report two different values for a setting, partially update it and partially report it","cassandra-cli updates and prints out a min_compaction_threshold that is not shown by cqlsh (it shows a different min_threshold attribute)

cqlsh updates ""both"" values but only shows one of them

{code}
cassandra-cli:
UPDATE COLUMN FAMILY foo WITH min_compaction_threshold = 8;

$ echo ""describe foo;"" | cassandra-cli -h `hostname` -k bar
      Compaction min/max thresholds: 8/32

$ echo ""describe table foo;"" | cqlsh -k bar `hostname`
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
{code}

{code}
cqlsh:
ALTER TABLE foo WITH compaction = {'class' : 'SizeTieredCompactionStrategy', 'min_threshold' : 16};

cassandra-cli:
      Compaction min/max thresholds: 16/32
      Compaction Strategy Options:
        min_threshold: 16
cqlsh:
  compaction={'min_threshold': '16', 'class': 'SizeTieredCompactionStrategy'} AND
{code}

{code}
cassandra-cli:
UPDATE COLUMN FAMILY foo WITH min_compaction_threshold = 8;

cassandra-cli:
      Compaction min/max thresholds: 8/32
      Compaction Strategy Options:
        min_threshold: 16

cqlsh:
  compaction={'min_threshold': '16', 'class': 'SizeTieredCompactionStrategy'} AND
{code}
",cli cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2014-10-10 17:11:19,3
12745532,Current check for cygwin environment in cassandra.ps1 is too greedy,mingw (and other) packages comes with a version of uname so we need to check the output of the uname call rather than just check to confirm it's present.,Windows,['Packaging'],CASSANDRA,Bug,Low,2014-10-02 16:10:11,6
12745524,stop-server.bat fails to call powershell if there's a space in the directory name,"Example error:
{noformat}
C:\vm-shared\src\test space\cassandra>Processing -File 'C:\vm-shared\src\test' failed because the file does not have a '.ps1' extension. Specify a valid PowerShell script file name, and then try again.
{noformat}",Windows,['Packaging'],CASSANDRA,Bug,Low,2014-10-02 15:35:34,6
12744723,Windows Unit tests and Dtests erroring due to sstable deleting task error,"Currently a large number of dtests and unit tests are erroring on windows with the following error in the node log:
{code}
ERROR [NonPeriodicTasks:1] 2014-09-29 11:05:04,383 SSTableDeletingTask.java:89 - Unable to delete c:\\users\\username\\appdata\\local\\temp\\dtest-vr6qgw\\test\\node1\\data\\system\\local-7ad54392bcdd35a684174e047860b377\\system-local-ka-4-Data.db (it will be removed on server restart; we'll also retry after GC)\n
{code}

git bisect points to the following commit:
{code}
0e831007760bffced8687f51b99525b650d7e193 is the first bad commit
commit 0e831007760bffced8687f51b99525b650d7e193
Author: Benedict Elliott Smith <benedict@apache.org>
Date:  Fri Sep 19 18:17:19 2014 +0100

    Fix resource leak in event of corrupt sstable

    patch by benedict; review by yukim for CASSANDRA-7932

:100644 100644 d3ee7d99179dce03307503a8093eb47bd0161681 f55e5d27c1c53db3485154cd16201fc5419f32df M      CHANGES.txt
:040000 040000 194f4c0569b6be9cc9e129c441433c5c14de7249 3c62b53b2b2bd4b212ab6005eab38f8a8e228923 M  src
:040000 040000 64f49266e328b9fdacc516c52ef1921fe42e994f de2ca38232bee6d2a6a5e068ed9ee0fbbc5aaebe M  test
{code}

You can reproduce this by running simple_bootstrap_test.",windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2014-09-29 17:41:06,6
12744024,Run LCS for both repaired and unrepaired data,"If a user has leveled compaction configured, we should run that for both the unrepaired and the repaired data. I think this would make things a lot easier for end users

It would simplify migration to incremental repairs as well, if a user runs incremental repair on its nice leveled unrepaired data, we wont need to drop it all to L0, instead we can just start moving sstables from the unrepaired leveling straight into the repaired leveling

Idea could be to have two instances of LeveledCompactionStrategy and move sstables between the instances after an incremental repair run (and let LCS be totally oblivious to whether it handles repaired or unrepaired data). Same should probably apply to any compaction strategy, run two instances and remove all repaired/unrepaired logic from the strategy itself.",compaction lcs,['Local/Compaction'],CASSANDRA,Bug,Normal,2014-09-25 13:24:42,0
12742613,Changes to index_interval table properties revert after subsequent modifications,"It appears that if you want to increase the sampling in *-Summary.db files, you would change the default for {{index_interval}} table property from the {{128}} default value to {{256}} on a given CQL {{TABLE}}.

However, if you {{ALTER TABLE}} after setting the value, {{index_interval}} returns to the default, {{128}}. This is unexpected behavior. I would expect the value for {{index_interval}} to not be affected by subsequent {{ALTER TABLE}} statements.

As noted in Environment, this was seen with a 2.0.9-SNAPSHOT built w/ `ccm`. 

If I just use a table from one of DataStax documentation tutorials (musicdb as mdb):

{noformat}
cqlsh:mdb> DESC TABLE songs;

CREATE TABLE songs (
  id uuid,
  album text,
  artist text,
  data blob,
  reviews list<text>,
  tags set<text>,
  title text,
  venue map<timestamp, text>,
  PRIMARY KEY ((id))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{noformat}

We've got {{128}} as expected.

We alter it:

{noformat}
cqlsh:mdb> ALTER TABLE songs WITH index_interval = 256; 
{noformat}

And the change appears: 

{noformat}
cqlsh:mdb> DESC TABLE songs;

CREATE TABLE songs (
  id uuid,
  album text,
  artist text,
  data blob,
  reviews list<text>,
  tags set<text>,
  title text,
  venue map<timestamp, text>,
  PRIMARY KEY ((id))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=256 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{noformat}

But if do another {{ALTER TABLE}}, say, change the caching or comment, the {{index_interval}} will revert back to {{128}}.

{noformat}
cqlsh:mdb> ALTER TABLE songs WITH caching = 'none'; 
cqlsh:mdb> DESC TABLE songs; 

CREATE TABLE songs (
  id uuid,
  album text,
  artist text,
  data blob,
  reviews list<text>,
  tags set<text>,
  title text,
  venue map<timestamp, text>,
  PRIMARY KEY ((id))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='NONE' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{noformat}

It should be {{index_interval=256}}.

I know that 2.1 will replace {{index_interval}}. 

I have not confirmed any behavior with {{min_index_interval}} nor {{max_index_interval}} (which is described in resolved #6379). 
",cql3 metadata,['Local/Config'],CASSANDRA,Bug,Normal,2014-09-19 00:05:16,3
12742575,"cqlsh connect error ""member_descriptor' object is not callable""","When using cqlsh (Cassandra 2.1.0) with ssl, python 2.6.9. I get Connection error: ('Unable to connect to any servers', {...: TypeError(""'member_descriptor' object is not callable"",)}) 
I am able to connect from another machine using python 2.7.5.",cqlsh lhf,['Legacy/Tools'],CASSANDRA,Bug,Low,2014-09-18 20:57:47,3
12742573,"Add ""CREATE INDEX ... ON ..(KEYS())"" syntax to cqlsh and CQL.textile",http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/create_index_r.html?scroll=reference_ds_eqm_nmd_xj__CreatIdxCollKey,cqlsh lhf qa-resolved,['Legacy/Tools'],CASSANDRA,Sub-task,Low,2014-09-18 20:56:21,4
12739899,Implement -f functionality in stop-server.bat,"Stop-server.bat lists ""-f"" as an argument but does not handle it inside of stop-server.ps1. ",Windows qa-resolved,[],CASSANDRA,Improvement,Low,2014-09-08 16:03:17,4
12739560,Select an element inside a UDT throws an index error,"Create the following data model:
{noformat}
CREATE TYPE address (
street text,
city text,
zip_code int,
phones set<text>
 );
 
CREATE TYPE fullname (
firstname text,
lastname text
);

CREATE TABLE users (
id uuid PRIMARY KEY,
name FROZEN <fullname>,
addresses map<text, FROZEN <address>>
);

INSERT INTO users (id, name) 
VALUES (62c36092-82a1-3a00-93d1-46196ee77204, {firstname: 'Marie-Claude', lastname: 'Josset'});
{noformat}

When trying to select a sub-field in the name type:
{noformat}
SELECT name.lastname FROM users WHERE id=62c36092-82a1-3a00-93d1-46196ee77204;
{noformat}

You get the following error:
{noformat}
list index out of range
{noformat}",qa-resolved,[],CASSANDRA,Bug,Normal,2014-09-05 18:48:54,4
12737546,Archive Commitlog Tests Failings,"Four of the snapshot_test.py:TestArchiveCommitlog tests are failing on 2.1.0 and 2.1-HEAD:
http://cassci.datastax.com/job/cassandra-2.1.0_dtest/lastCompletedBuild/testReport/snapshot_test/

The tests restore archived commit logs and check how many rows exist after restoring. They are passing on 2.0-HEAD.",qa-resolved,[],CASSANDRA,Improvement,Normal,2014-08-28 19:08:11,4
12736811,recreating a counter column after dropping it leaves in unusable state,"create table counter_bug (t int, c counter, primary key (t));
update counter_bug set c = c +1 where t = 1;
select * from counter_bug ;
 
 t | c
---+---
 1 | 1
 
(1 rows)
 
alter table counter_bug drop c;
alter table counter_bug add c counter;
update counter_bug set c = c +1 where t = 1;
select * from counter_bug;
 
(0 rows)

update counter_bug set c = c +1 where t = 2;
select * from counter_bug;
 
(0 rows)",qa-resolved,[],CASSANDRA,Bug,Normal,2014-08-26 14:13:13,1
12735865,enable describe on indices,"Describe index should be supported, right now, the only way is to export the schema and find what it really is before updating/dropping the index.

verified in 
[cqlsh 3.1.8 | Cassandra 1.2.18.1 | CQL spec 3.0.0 | Thrift protocol 19.36.2]
",doc-impacting,['Legacy/Tools'],CASSANDRA,Improvement,Low,2014-08-21 18:47:10,3
12735784,UDF cleanups (#7395 follow-up),"The current code for UDF is largely not reusing the pre-existing mechanics/code for native/hardcoded functions. I don't see a good reason for that but I do see downsides: it's more code to maintain and makes it much easier to have inconsitent behavior between hard-coded and user-defined function. More concretely, {{UDFRegistery/UDFFunctionOverloads}} fundamentally do the same thing than {{Functions}}, we should just merge both. I'm also not sure there is a need for both {{UFMetadata}} and {{UDFunction}} since {{UFMetadata}} really only store infos on a given function (contrarly to what the javadoc pretends).  I suggest we consolidate all this to cleanup the code, but also as a way to fix 2 problems that the UDF code has but that the existing code for ""native"" functions don't:
* if there is multiple overloads of a function, the UDF code picks the first version whose argument types are compatible with the concrete arguments provided. This is broken for bind markers: we don't know the type of markers and so the first function match may not at all be what the user want. The only sensible choice is to detect that type of ambiguity and reject the query, asking the user to explicitly type-cast their bind marker (which is what the code for hard-coded function does).
* the UDF code builds a function signature using the CQL type names of the arguments and use that to distinguish multiple overrides in the schema. This means in particular that {{f(v text)}} and {{f(v varchar)}} are considered distinct, which is wrong since CQL considers {{varchar}} as a simple alias of {{text}}. And in fact, the function resolution does consider them aliases leading to seemingly broken behavior.

There is a few other small problems that I'm proposing to fix while doing this cleanup:
* Function creation only use the function name when checking if the function exists, which is not enough since we allow multiple over-loadings. You can bypass the check by using ""OR REPLACE"" but that's obviously broken.
* {{IF NOT EXISTS}} for function creation is broken.
* The code allows to replace a function (with {{OR REPLACE}}) by a new function with an incompatible return type. Imo that's dodgy and we should refuse it (users can still drop and re-create the method if they really want).
",cql,['Legacy/CQL'],CASSANDRA,Bug,Normal,2014-08-21 13:52:15,2
12735390,"When compaction is interrupted, it leaves locked, undeletable files","During tests of new features of 2.1 like: 
- incremental repairs 
- leveled compaction
I interrupted a compaction, which left the following ERROR in the _system.log_
{quote}
org.apache.cassandra.db.compaction.CompactionInterruptedException: Compaction interrupted: Compaction@152e6e70-1975-11e4-ba09-61f0d75c60c6(xx, xxx, 378505918/1993581634)bytes
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:174)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_09]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) ~[na:1.7.0_09]
	at java.util.concurrent.FutureTask.run(FutureTask.java:166) ~[na:1.7.0_09]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) ~[na:1.7.0_09]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [na:1.7.0_09]
	at java.lang.Thread.run(Thread.java:722) [na:1.7.0_09]
{quote}

Right after that, a cascade of reoccurring errors was being emitted till the restart:
{quote}
ERROR [NonPeriodicTasks:1] 2014-08-19 13:38:41,258 SSTableDeletingTask.java:81 - Unable to delete /grid/data04/cassandra/data/xx/xxx-152e6e70197511e4ba0961f0d75c60c6/xx-xxx-ka-55058-Data.db (it will be removed on server restart; we'll also retry after GC)
{quote}
which made this node blinking (noted from the other nodes gossiper log entries).
After restart, the node is healthy and fully operational.",compaction,['Local/Compaction'],CASSANDRA,Bug,Normal,2014-08-20 12:11:59,0
12735336,A successful INSERT with CAS does not always store data in the DB after a DELETE,"When I run a loop with CQL statements to DELETE, INSERT with CAS and then a GET.
The INSERT opertion is successful (Applied), but no data is stored in the database. I have checked the database manually after the test to verify that the DB is empty.
{code}
        for (int i = 0; i < 10000; ++i)
        {
            try
            {
                t.del();
                t.cas();
                t.select();
            }
            catch (Exception e)
            {
                System.err.println(""i="" + i);
                e.printStackTrace();
                break;
            }
        }


        myCluster = Cluster.builder().addContactPoint(""localhost"").withPort(12742).build();
        mySession = myCluster.connect();

        mySession.execute(""CREATE KEYSPACE IF NOT EXISTS castest WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };"");
        mySession.execute(""CREATE TABLE IF NOT EXISTS castest.users (userid text PRIMARY KEY, name text)"");

        myInsert = mySession.prepare(""INSERT INTO castest.users (userid, name) values ('user1', 'calle') IF NOT EXISTS"");
        myDelete = mySession.prepare(""DELETE FROM castest.users where userid='user1'"");
        myGet = mySession.prepare(""SELECT * FROM castest.users where userid='user1'"");
    }
{code}

I can reproduce the fault with the attached program on a PC with windows 7.
You need a cassandra runing and you need to set the port in the program.

",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Improvement,Normal,2014-08-20 07:01:51,2
12734252,"Windows - fsync-analog, flush data to disk",We currently use CLibrary fsync linux-native calls to flush to disk.  Given the role this plays in our SequentialWriter and data integrity in general we need some analog to this function on Windows.,Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2014-08-14 20:59:01,6
12733346,Windows cqlsh: prompt for install of pyreadline if missing during cqlsh init,Windows python doesn't come with readline functionality by default and tab completion in cqlsh is silently unavailable due to this.  Installing pyreadline is an easy fix - it would be nice to prompt users to install this dependency if it's not available on Windows.,Windows,['Legacy/Tools'],CASSANDRA,Improvement,Low,2014-08-11 21:03:31,6
12733330,Get Windows command-line flags in-line with linux,"linux was here first.  In the PowerShell launch scripts, -v is verbose right now as I missed that it's used for 'version' on linux.  Add version print functionality to Windows launch using -v and find another sane flag to use for verbose env output printing.",Windows,['Packaging'],CASSANDRA,Improvement,Low,2014-08-11 20:03:18,6
12733053,altering a table to add a static column bypasses clustering column requirement check,"cqlsh:test_ks> create TABLE foo ( bar int, primary key (bar));
cqlsh:test_ks> alter table foo add bar2 text static;
cqlsh:test_ks> describe table foo;

CREATE TABLE foo (
  bar int,
  bar2 text static,
  PRIMARY KEY ((bar))
) 

cqlsh:test_ks> select * from foo;
TSocket read 0 bytes


ERROR [Thrift:12] 2014-08-09 15:08:22,518 CassandraDaemon.java (line 199) Exception in thread Thread[Thrift:12,5,main]
java.lang.AssertionError
	at org.apache.cassandra.config.CFMetaData.getStaticColumnNameBuilder(CFMetaData.java:2142)
	at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:454)
	at org.apache.cassandra.cql3.statements.SelectStatement.getRangeCommand(SelectStatement.java:360)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:206)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)

",cql,[],CASSANDRA,Bug,Normal,2014-08-09 19:12:40,2
12732683,dtest cql_tests.py:TestCQL.cql3_insert_thrift_test fails intermittently,"This test fails about 20-25% of the time - ran about 10 times through looping the test, and it typically fails on the 4th or 5th test.
{noformat}
(master)mshuler@hana:~/git/cassandra-dtest$ ../loop_dtest.sh ""cql_tests.py:TestCQL.cql3_insert_thrift_test""
<...>

==== Run #4 ====
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
cql3_insert_thrift_test (cql_tests.TestCQL) ... cluster ccm directory: /tmp/dtest-Drwunj
[node1 ERROR] 
[node1 ERROR] 
FAIL
removing ccm cluster test at: /tmp/dtest-Drwunj

======================================================================
FAIL: cql3_insert_thrift_test (cql_tests.TestCQL)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/cql_tests.py"", line 1627, in cql3_insert_thrift_test
    assert res == [ [2, 4, 200] ], res
AssertionError: []

----------------------------------------------------------------------
Ran 1 test in 7.192s
{noformat}

loop_dtest.sh:
{noformat}
#!/bin/bash
if [ ${1} ]; then
    export MAX_HEAP_SIZE=""1G""
    export HEAP_NEWSIZE=""256M""
    export PRINT_DEBUG=true
    COUNT=0
    while true; do
        echo
        echo ""==== Run #$COUNT ====""
        nosetests --nocapture --nologcapture --verbosity=3 ${1}
        if [ $? -ne 0 ]; then
            exit 1
        fi
        ((COUNT++))
        sleep 0.5
    done
    unset MAX_HEAP_SIZE HEAP_NEWSIZE PRINT_DEBUG
else
    echo ""  ${0} needs a test to run..""
    exit 255
fi
{noformat}

I find no ERROR/WARN log entries from the failed test - attached node log anyway.",qa-resolved,['Test/dtest/python'],CASSANDRA,Improvement,Low,2014-08-07 21:29:30,4
12732568,Add new CMS GC flags to Windows startup scripts for JVM later than 1.7.0_60,"Replicate changes from CASSANDRA-7432.

Relevant patch contents:
{noformat}
 # note: bash evals '1.7.x' as > '1.7' so this is really a >= 1.7 jvm check
+if { [ ""$JVM_VERSION"" \> ""1.7"" ] && [ ""$JVM_VERSION"" \< ""1.8.0"" ] && [ ""$JVM_PATCH_VERSION"" -ge ""60"" ]; } || [ ""$JVM_VERSION"" \> ""1.8"" ] ; then
+    JVM_OPTS=""$JVM_OPTS -XX:+CMSParallelInitialMarkEnabled -XX:+CMSEdenChunksRecordAlways""
+fi
{noformat}",Windows,['Packaging'],CASSANDRA,Improvement,Normal,2014-08-07 14:54:05,6
12732028,Expected Compaction Interruption is logged as ERROR,"As seen in the attached log, occasionally a major compaction will interrupt other running compactions. This is not an error and is expected behavior. However this is logged at ERROR. ",qa-resolved,['Local/Compaction'],CASSANDRA,Bug,Low,2014-08-05 14:29:36,0
12731452,user types allow counters,"From the conversation on CASSANDRA-6312 it seems we should not allow user types to contain counters. Presently, user types can be defined with field types of counter, and these user types can also be associated with tables without error.

I'm not certain if there's a compelling case for counters within user types, but I don't think there's any syntax existing presently that would allow updating them anyway.

To repro:
{noformat}
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

USE test;

CREATE TYPE t_item (
  sub_one counter
); 

CREATE TYPE item (
  sub_one counter
); 

CREATE TABLE test.mytable (
    value1 text PRIMARY KEY,
    item t_item,
    value2 text
);

cqlsh:test> insert into mytable (value1, value2) VALUES ( 'foo', 'bar');
cqlsh:test> select * from mytable;

 value1 | item | value2
--------+------+--------
    foo | null |    bar

{noformat}",qa-resolved,[],CASSANDRA,Bug,Normal,2014-08-01 23:00:31,1
12730632,cqlsh should automatically disable tracing when selecting from system_traces,Nobody needs to trace their traces while they're tracing.,lhf qa-resolved,['Legacy/Tools'],CASSANDRA,Improvement,Low,2014-07-29 22:17:40,4
12730584,Cqlsh cannot use .csv files exported by Excel,"When performing COPY table FROM 'test.csv' on cqlsh across any of the three major branches (1.2, 2.0, 2.1), the following error is thrown:

{code}new-line character seen in unquoted field - do you need to open the file in universal-newline mode?{code}

This happens if the .csv file has windows style line endings, such as if it was created by saving a .csv from excel. This reproduces in both unix and windows environments. This is a simple enough fix. I found that running dos2unix on csv files generated by excel was not sufficient to correct the issue.",osx,['Legacy/Tools'],CASSANDRA,Bug,Low,2014-07-29 19:37:56,6
12729931,Change line-endings for all .ps1 files to be CRLF,Line endings on .ps1 files are currently LF only.  In the Windows ecosystem it's more appropriate to be CRLF.,Windows,['Packaging'],CASSANDRA,Bug,Low,2014-07-25 21:03:47,6
12729610,Normalize windows tool batch file environments,"Reference CASSANDRA-7587.  We don't currently have something analogous to cassandra.in.sh for Windows so there's no single point of insertion to add something to the environment for those tools to run.  Along with that, there's a bunch of duplicated code in those .bat files to determine classpath, home dir, etc.",Windows,['Legacy/Tools'],CASSANDRA,Improvement,Low,2014-07-24 23:07:12,6
12729191,Dtest errors should show full system log message,"Although system errors are always reported when they happen, dtest does not show the full message. You have to go to the log file yourself to see the actual error. This takes two seconds when running a test by hand, but it's more difficult when viewing results run on cassci. We should be able to parse this out and pump it into the top most exception that is spit out by nosetests.

Example of a useless error message: http://cassci.datastax.com/job/cassandra_upgrade_dtest/66/testReport/upgrade_through_versions_test/TestUpgradeThroughVersions/upgrade_test_2/",qa-resolved,[],CASSANDRA,Improvement,Normal,2014-07-23 15:52:53,4
12727108,Windows: IOException when repairing a range of tokens,Looks like we missed this in CASSANDRA-6907 - range-based repair still defaults to snapshot-based on Windows.,Windows,['Legacy/Streaming and Messaging'],CASSANDRA,Bug,Low,2014-07-14 15:53:18,6
12726245,add date and time types,"http://www.postgresql.org/docs/9.1/static/datatype-datetime.html

(we already have timestamp; interval is out of scope for now, and see CASSANDRA-6350 for discussion on timestamp-with-time-zone.  but date/time should be pretty easy to add.)",docs qa-resolved,['Legacy/CQL'],CASSANDRA,Sub-task,Low,2014-07-09 04:33:00,6
12726104,Support starting of daemon from within powershell,Currently cassandra.ps1 doesn't work if run directly from within PowerShell and the use-case seems pretty common.  Modify the script to work correctly without the .bat wrapper.,Windows qa-resolved,['Packaging'],CASSANDRA,Improvement,Low,2014-07-08 15:18:04,6
12725783,cassandra fails to start if WMI memory query fails on Windows,"{code:title=failure}
C:\src\cassandra>bin\cassandra.bat -f
Detected powershell execution permissions.  Running with enhanced startup scripts.
Setting up Cassandra environment
Starting cassandra server
Invalid initial heap size: -Xms0M
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code}",Windows qa-resolved,['Packaging'],CASSANDRA,Bug,Low,2014-07-07 17:03:04,6
12725650,Unable to update list element by index using CAS condition,"While running IT tests for *Achilles*, I ran into a strange bug:

*With CQLSH*

{code:sql}
cqlsh:test> CREATE TABLE cas_update(id int PRIMARY KEY,name text,friends list<text>);
cqlsh:test> INSERT INTO cas_update (id, name , friends ) VALUES ( 10,'John',['Paul','George']);
cqlsh:test> SELECT * FROM cas_update WHERE id=10;

 id | friends            | name
----+--------------------+------
 10 | ['Paul', 'George'] | John

cqlsh:test> UPDATE cas_update SET friends[0]='Helen' WHERE id=10 IF name='John';
Bad Request: List index 0 out of bound, list has size 0

cqlsh:test> UPDATE cas_update SET friends[0]='Helen' WHERE id=10;
cqlsh:test> SELECT * FROM cas_update WHERE id=10;

 id | friends             | name
----+---------------------+------
 10 | ['Helen', 'George'] | John
{code}

It seems that we cannot update list element by index with a CAS condition.

*With Java driver 2.0.2 or 2.0.3*

{code:java}
 ACHILLES_DML_STATEMENT@:writeDMLStatementLog Prepared statement : [INSERT INTO CompleteBean(id,followers,friends,name,preferences) VALUES (:id,:followers,:friends,:name,:preferences) USING TTL :ttl;] with CONSISTENCY LEVEL [ONE] 
 ACHILLES_DML_STATEMENT@:writeDMLStatementLog    bound values : [621309709026375591, [], [Paul, Andrew], John, {}, 0] 
 ACHILLES_DML_STATEMENT@:writeDMLStartBatch  
 ACHILLES_DML_STATEMENT@:writeDMLStartBatch  
 ACHILLES_DML_STATEMENT@:writeDMLStartBatch ****** BATCH UNLOGGED START ****** 
 ACHILLES_DML_STATEMENT@:writeDMLStartBatch  
 ACHILLES_DML_STATEMENT@:writeDMLStatementLog Parameterized statement : [UPDATE CompleteBean USING TTL 100 SET friends[0]=? WHERE id=621309709026375591 IF name=?;] with CONSISTENCY LEVEL [ONE] 
 ACHILLES_DML_STATEMENT@:writeDMLStatementLog    bound values : [100, 0, Helen, 621309709026375591, John] 
 ACHILLES_DML_STATEMENT@:writeDMLStatementLog Parameterized statement : [UPDATE CompleteBean USING TTL 100 SET friends[1]=null WHERE id=621309709026375591 IF name=?;] with CONSISTENCY LEVEL [ONE] 
 ACHILLES_DML_STATEMENT@:writeDMLStatementLog    bound values : [100, 1, null, 621309709026375591, John] 
 ACHILLES_DML_STATEMENT@:writeDMLEndBatch  
 ACHILLES_DML_STATEMENT@:writeDMLEndBatch   ****** BATCH UNLOGGED END with CONSISTENCY LEVEL [DEFAULT] ****** 
 ACHILLES_DML_STATEMENT@:writeDMLEndBatch  
 ACHILLES_DML_STATEMENT@:writeDMLEndBatch  
 ACHILLES_DML_STATEMENT@:truncateTable   Simple query : [TRUNCATE entity_with_enum] with CONSISTENCY LEVEL [ALL] 
 ACHILLES_DML_STATEMENT@:truncateTable   Simple query : [TRUNCATE CompleteBean] with CONSISTENCY LEVEL [ALL] 

com.datastax.driver.core.exceptions.InvalidQueryException: List index 0 out of bound, list has size 0
        at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:35)
        at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:256)
        at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:172)
        at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
{code}


With Cassandra *2.0.8* and Java Driver 2.0.2 or 2.0.3, *the test passed* so it seems that there is a regression somewhere in the CAS update code

",LWT,"['Feature/Lightweight Transactions', 'Legacy/CQL']",CASSANDRA,Bug,Normal,2014-07-06 22:08:21,2
12724685,Throw exception on unknown UDT field,"Currently, the following code:
{noformat}
CREATE TYPE foo (f : int);
CREATE TABLE test (k int PRIMARY KEY, v foo);

INSERT INTO test (k, v) VALUES (0, { s : ?})
{noformat}
will crash, because the {{s}} field is not part of type {{foo}} and it's not caught. The consequence being that the metadata for the bindMarker ends up being {{null}} and some NPE is thrown. We should throw a proper exception instead.",cql,['Legacy/CQL'],CASSANDRA,Bug,Low,2014-07-01 09:24:28,2
12724679,COPY FROM doesn't accept diacritical characters,The current version of the COPY command chokes on diacritical characters such as ñ ë á and ù. It would be great if the COPY command would support these characters.,cqlsh,[],CASSANDRA,Improvement,Low,2014-07-01 08:58:42,4
12724501,Dtest: Windows - various cqlsh_tests errors,"Have a few windows-specific failures in this test.

{code:title=test_eat_glass}
======================================================================
ERROR: test_eat_glass (cqlsh_tests.TestCqlsh)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 158, in test_eat_glass
    """""".encode(""utf-8""))
  File ""build\bdist.win32\egg\ccmlib\node.py"", line 613, in run_cqlsh
    p.stdin.write(cmd + ';\n')
IOError: [Errno 22] Invalid argument
{code}

{code:title=test_simple_insert}
======================================================================
ERROR: test_simple_insert (cqlsh_tests.TestCqlsh)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 35, in test_simple_insert
    cursor.execute(""select id, value from simple.simple"");
  File ""c:\src\cassandra-dbapi2\cql\cursor.py"", line 80, in execute
    response = self.get_response(prepared_q, cl)
  File ""c:\src\cassandra-dbapi2\cql\thrifteries.py"", line 77, in get_response
    return self.handle_cql_execution_errors(doquery, compressed_q, compress, cl)
  File ""c:\src\cassandra-dbapi2\cql\thrifteries.py"", line 98, in handle_cql_execution_errors
    raise cql.ProgrammingError(""Bad Request: %s"" % ire.why)
ProgrammingError: Bad Request: Keyspace simple does not exist
{code}

{code:title=test_with_empty_values}
======================================================================
ERROR: test_with_empty_values (cqlsh_tests.TestCqlsh)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 347, in test_with_empty_values
    output = self.run_cqlsh(node1, ""select intcol, bigintcol, varintcol from CASSANDRA_7196.has_all_types where num in (0, 1, 2, 3, 4)"")
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 373, in run_cqlsh
    p = subprocess.Popen([ cli ] + args, env=env, stdin=subprocess.PIPE, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
  File ""C:\Python27\lib\subprocess.py"", line 710, in __init__
    errread, errwrite)
  File ""C:\Python27\lib\subprocess.py"", line 958, in _execute_child
    startupinfo)
WindowsError: [Error 193] %1 is not a valid Win32 application
{code}",Windows qa-resolved,['Test/dtest/python'],CASSANDRA,Improvement,Low,2014-06-30 18:21:24,4
12724484,Dtest: Windows-specific failure: sc_with_row_cache_test (super_column_cache_test.TestSCCache),"Windows-specific dtest failure:

{code:title=failure message}
======================================================================
FAIL: sc_with_row_cache_test (super_column_cache_test.TestSCCache)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\super_column_cache_test.py"", line 44, in sc_with_row_cache_test
    assert_columns(cli, ['name'])
  File ""C:\src\cassandra-dtest\super_column_cache_test.py"", line 10, in assert_columns
    assert not cli.has_errors(), cli.errors()
AssertionError: 'org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:185)\r\n\tat org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)\r\n\tat org.apache.cassandra.thrift.TFramedTransportFactory.openTransport(TFramedTransportFactory.java:41)\r\n\tat org.apache.cassandra.cli.CliMain.connect(CliMain.java:65)\r\n\tat org.apache.cassandra.cli.CliMain.main(CliMain.java:237)\r\nCaused by: java.net.ConnectException: Connection refused: connect\r\n\tat java.net.DualStackPlainSocketImpl.connect0(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)\r\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\r\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\r\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\r\n\tat java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)\r\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n\tat java.net.Socket.connect(Socket.java:579)\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:180)\r\n\t... 4 more\r\nException connecting to 127.0.0.1/9160. Reason: Connection refused: connect.\r\n'
{code}",Windows,['Test/dtest/python'],CASSANDRA,Bug,Low,2014-06-30 17:02:50,4
12724070,Send source sstable level when bootstrapping or replacing nodes,When replacing or bootstrapping a new node we can keep the source sstable level to avoid doing alot of compaction after bootstrap,lcs,[],CASSANDRA,Improvement,Normal,2014-06-27 07:01:42,0
12723879,Launch scripts on Windows don't handle spaces gracefully,"There's also some .ps1 problems after we get past just the .bat.  Should be some trivial escaping to fix.

C:\src - Copy\cassandra\bin>cassandra.bat
Detected powershell execution permissions.  Running with enhanced startup scripts.
Processing -File 'C:\src' failed because the file does not have a '.ps1' extension. Specify a valid PowerShell script file name, and then try again.",Windows,['Packaging'],CASSANDRA,Bug,Low,2014-06-26 14:16:57,6
12723162,Add -D command-line parsing to Windows powershell launch scripts,"Looks like there was an undocumented ability to pass in -D params to the JVM in the linux environment I missed while porting the logic over to Windows.

{code:title=-D}
-D)
   properties=""$properties -D$2""
   shift 2
;;
{code}",Windows qa-resolved,['Packaging'],CASSANDRA,Improvement,Low,2014-06-23 17:15:43,6
12722195,After cleanup we can end up with non-compacting high level sstables,"If we run cleanup (or increase sstable size) on a node with LCS, we could end up with a bunch of sstables in higher levels that are ""never"" compacted.",lcs,[],CASSANDRA,Bug,Low,2014-06-18 11:38:17,0
12721816,Native Protocol V3 CREATE Response,"Native protocol v3 changes the EVENT (opcode 12) SCHEMA_CHANGE to include the target type that changed : <change><target><keyspace><name>.

The RESULT (opcode 8) SCHEMA_CHANGE has the old layout (<change><keyspace><table>.

Is this difference intentional or does the protocol spec needs change for RESULT/SCHEMA_CHANGE?",protocol,['Legacy/CQL'],CASSANDRA,Task,Normal,2014-06-17 23:39:52,2
12721750,Node enables vnodes when bounced,"According to cassandra.yaml, in the information for the num_tokens setting, ""Specifying initial_token will override this setting."" So if exactly one initial token is set, then vnodes are disabled, regardless of if or what num_tokens are set to. This behavior is inconsistent when a node is started, versus if it has been bounced.

From a fresh checkout of C*, if I build, then edit cassandra.yaml so that:

num_tokens: 256
initial_token: -9223372036854775808

then run bin/cassandra, C* will start correctly. I can run bin/nodetool ring and see that the node has exactly one token and it is what I set in initial_token. If I gracefully shutdown C*, then restart the node, running bin/nodetool ring shows that the node now has vnodes enabled and has 256 tokens.

I have been able to reproduce this locally on OSX using 2.0.8, 2.1 rc1, and trunk. I have not yet tested in Linux or Windows to see if it occurs there.",qa-resolved,[],CASSANDRA,Improvement,Low,2014-06-17 18:17:15,4
12721216,Missing flexibility to have file://<server>/<etc> vs. file:/// when loading config file cassandra.yaml,"The parameter in the VM options -Dcassandra.config= needs file:///
Allow the user to have optional ""file:///"" when loading the config file from the filesystem",lhf patch,['Packaging'],CASSANDRA,Bug,Low,2014-06-13 23:43:25,6
12720965,Allow selecting Map values and Set elements,"Allow ""SELECT map['key]"" and ""SELECT list[index].""  (Selecting a UDT subfield is already supported.)
",cql,['Legacy/CQL'],CASSANDRA,New Feature,Normal,2014-06-12 19:51:51,2
12719029,some compactions do not works under windows (file in use during rename),"compaction do not works under windows due to file rename fails: (Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces = process can not access file because its in use by another process). Not all compactions are broken. compactions done during server startup on system tables works fine.

INFO  18:30:27 Completed flushing c:\cassandra-2.1\data\system\compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-6-Dat.db (42 bytes) for commitlog position ReplayPosition(segmentId=1402165543361, psition=8024611)
ERROR 18:30:27 Exception in thread hread[CompactionExecutor:5,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted
INFO  18:30:27 Compacting [SSTableReader(path='c:\cassandra-2.1\data\system\com
actions_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_prog
ess-ka-3-Data.db'), SSTableReader(path='c:\cassandra-2.1\data\system\compaction
_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka
5-Data.db'), SSTableReader(path='c:\cassandra-2.1\data\system\compactions_in_pr
gress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-4-Data
db'), SSTableReader(path='c:\cassandra-2.1\data\system\compactions_in_progress-
5080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-6-Data.db')]
ERROR 18:30:27 Exception in thread Thread[CompactionExecutor:5,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted
INFO  18:30:27 Compacted 4 sstables to [].  423 bytes to 0 (~0% of original) in
39ms = 0,000000MB/s.  4 total partitions merged to 0.  Partition merge counts w
re {2:2, }
INFO  18:30:45 Enqueuing flush of compactions_in_progress: 1345 (0%) on-heap, 0
(0%) off-heap
INFO  18:30:45 Writing Memtable-compactions_in_progress@15659113(153 serialized
bytes, 10 ops, 0%/0% of on/off-heap limit)
INFO  18:30:45 Completed flushing c:\cassandra-2.1\data\system\compactions_in_p
ogress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-8-Dat
.db (173 bytes) for commitlog position ReplayPosition(segmentId=1402165543361,
osition=8025407)
INFO  18:30:45 Compacting [SSTableReader(path='c:\cassandra-2.1\data\test\sipdb
58f51090ee6511e3815625991ef2b954\test-sipdb-ka-3-Data.db'), SSTableReader(path=
c:\cassandra-2.1\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka
1-Data.db'), SSTableReader(path='c:\cassandra-2.1\data\test\sipdb-58f51090ee651
e3815625991ef2b954\test-sipdb-ka-4-Data.db'), SSTableReader(path='c:\cassandra-
.1\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-2-Data.db'),
STableReader(path='c:\cassandra-2.1\data\test\sipdb-58f51090ee6511e3815625991ef
b954\test-sipdb-ka-6-Data.db')]
ERROR 18:31:41 Unable to delete c:\cassandra-2.1\data\test\sipdb-58f51090ee6511
3815625991ef2b954\test-sipdb-ka-8-Data.db (it will be removed on server restart
 we'll also retry after GC)
ERROR 18:31:41 Missing component: c:\cassandra-2.1\data\test\sipdb-58f51090ee65
1e3815625991ef2b954\test-sipdb-tmp-ka-8-Digest.sha1
ERROR 18:31:41 Missing component: c:\cassandra-2.1\data\test\sipdb-58f51090ee65
1e3815625991ef2b954\test-sipdb-tmp-ka-8-Summary.db
ERROR 18:31:41 Missing component: c:\cassandra-2.1\data\test\sipdb-58f51090ee65
1e3815625991ef2b954\test-sipdb-tmp-ka-8-Statistics.db
INFO  18:31:41 Enqueuing flush of compactions_in_progress: 148 (0%) on-heap, 0
0%) off-heap
INFO  18:31:41 Writing Memtable-compactions_in_progress@6888852(0 serialized by
es, 1 ops, 0%/0% of on/off-heap limit)
INFO  18:31:41 Completed flushing c:\cassandra-2.1\data\system\compactions_in_p
ogress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-9-Dat
.db (42 bytes) for commitlog position ReplayPosition(segmentId=1402165543361, p
sition=8025563)
ERROR 18:31:41 Exception in thread Thread[CompactionExecutor:6,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted
ERROR 18:31:41 Exception in thread Thread[CompactionExecutor:6,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted",Windows,"['Legacy/Local Write-Read Paths', 'Local/Compaction']",CASSANDRA,Bug,Normal,2014-06-07 19:11:35,6
12718506,Java heap being set too large on Windows with 32-bit JVM,"On windows, the JVM settings for max heap size and new gen heap size are set based on the total system memory. When the system has 8G of RAM, the max heap size is set to 2048M. However, according to http://goo.gl/1ElbLm, the recommended max heap for a 32 bit JVM on Windows is 1.8G.

When cassandra is started on Windows under these conditions, the following error is seen:

Error occurred during initialization of VM
Could not reserve enough space for object heap
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Switching to a 64-bit JVM on the same machine solves the issue. If a 32-bit JVM is being used, cassandra should be started up with a smaller heap than would be normally used to prevent the error.",Windows qa-resolved,['Packaging'],CASSANDRA,Bug,Low,2014-06-04 22:56:27,6
12718454,Java heap flags are being set to invalid values on Windows,"In conf/cassandra-env.ps1 max heap size is set based on system memory. New generation heap size is then set based on max heap size. Integer division is not used, and thus floating point results are possible, which will prevent cassandra from being started up. The following error is seen:

Invalid initial eden size: -Xmn511.75M
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
",Windows qa-resolved,['Packaging'],CASSANDRA,Bug,Normal,2014-06-04 19:59:52,6
12718178,Modify system tcp keepalive settings on Windows install scripts,"Reference CASSANDRA-3569

We need to change the powershell installation scripts to also update the registry w/a 5 minute tcp keepalive timer timeout.",Windows qa-resolved,['Packaging'],CASSANDRA,Improvement,Low,2014-06-03 15:42:01,6
12717788,GossipingPropertyFileSnitchTest fails on Windows,"{code:failure message}
    [junit] Testcase: testAutoReloadConfig(org.apache.cassandra.locator.GossipingPropertyFileSnitchTest):       Caused an ERROR
    [junit] Illegal char <:> at index 2: /C:/vm-shared/src/cassandra/test/conf/cassandra-rackdc.properties
    [junit] java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/vm-shared/src/cassandra/test/conf/cassandra-rackdc.properties
    [junit]     at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182)
    [junit]     at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153)
    [junit]     at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)
    [junit]     at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)
    [junit]     at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)
    [junit]     at java.nio.file.Paths.get(Paths.java:84)
    [junit]     at org.apache.cassandra.locator.GossipingPropertyFileSnitchTest.testAutoReloadConfig(GossipingPropertyFileSnitchTest.java:40)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.locator.GossipingPropertyFileSnitchTest FAILED
{code}",Windows,['Legacy/Testing'],CASSANDRA,Sub-task,Low,2014-06-01 19:51:36,6
12717652,Infinite loop in StreamReader.read during exception condition while running repair,"InputStream.skip is returning -1 during exception conditions which leads the following logic to infinite loop:

{code:title=loop}
protected void drain(InputStream dis, long bytesRead) throws IOException
{
    long toSkip = totalSize() - bytesRead;
    toSkip = toSkip - dis.skip(toSkip);
    while (toSkip > 0)
        toSkip = toSkip - dis.skip(toSkip);
}
{code}",Core,['Legacy/Streaming and Messaging'],CASSANDRA,Bug,Low,2014-05-30 21:02:45,6
12717141,Windows: address potential JVM swapping,"Similar to mlockall() in CLibrary.java for linux, it would be nice to lock the virtual address space on Windows to prevent page faults.

One option: Reference API:  http://msdn.microsoft.com/en-us/library/windows/desktop/aa366895(v=vs.85).aspx",Windows perfomance,['Local/Startup and Shutdown'],CASSANDRA,Improvement,Low,2014-05-28 21:33:47,6
12716401,BatchlogManagerTest unit test failing in 2.1 & trunk,"Commit 1147ee3 for CASSANDRA-6975 introduced a regression for BatchlogManagerTest in 2.1 and trunk:

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.BatchlogManagerTest
    [junit] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 45.795 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] WARN  18:43:57 Changing /127.0.0.1's host ID from 1c3d6470-e2aa-11e3-beb0-9b2001e5c823 to 32f21350-e2aa-11e3-beb0-9b2001e5c823
    [junit] WARN  18:43:57 Changing /127.0.0.1's host ID from 1c3d6470-e2aa-11e3-beb0-9b2001e5c823 to 32f21350-e2aa-11e3-beb0-9b2001e5c823
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testReplay(org.apache.cassandra.db.BatchlogManagerTest):  Caused an ERROR
    [junit] Error validating SELECT count(*) FROM %s.%s
    [junit] java.lang.RuntimeException: Error validating SELECT count(*) FROM %s.%s
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:275)
    [junit]     at org.apache.cassandra.db.BatchlogManager.countAllBatches(BatchlogManager.java:102)
    [junit]     at org.apache.cassandra.db.BatchlogManagerTest.testReplay(BatchlogManagerTest.java:60)
    [junit] Caused by: org.apache.cassandra.exceptions.SyntaxException: line 1:24 no viable alternative at character '%'
    [junit]     at org.apache.cassandra.cql3.CqlLexer.throwLastRecognitionError(CqlLexer.java:201)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:455)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:430)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:211)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:252)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:262)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.BatchlogManagerTest FAILED
{noformat}

http://cassci.datastax.com/job/cassandra-2.1_utest/291/testReport/org.apache.cassandra.db/BatchlogManagerTest/testReplay/
http://cassci.datastax.com/job/trunk_utest/627/testReport/junit/org.apache.cassandra.db/BatchlogManagerTest/testReplay/",qa-resolved,['Legacy/Testing'],CASSANDRA,Bug,Low,2014-05-23 18:49:46,2
12715776,MultiSliceTest.test_with_overlap* unit tests failing in trunk,"Example:
https://cassci.datastax.com/job/trunk_utest/623/testReport/org.apache.cassandra.thrift/MultiSliceTest/",qa-resolved,['Legacy/Testing'],CASSANDRA,Bug,Low,2014-05-21 15:37:47,2
12715471,"Add ""Major"" Compaction to LCS and split sstables during STCS major compaction","LCS has a number of minor issues (maybe major depending on your perspective).

LCS is primarily used for wide rows so for instance when you repair data in LCS you end up with a copy of an entire repaired row in L0.  Over time if you repair you end up with multiple copies of a row in L0 - L5.  This can make predicting disk usage confusing.  

Another issue is cleaning up tombstoned data.  If a tombstone lives in level 1 and data for the cell lives in level 5 the data will not be reclaimed from disk until the tombstone reaches level 5.

I propose we add a ""major"" compaction for LCS that forces consolidation of data to level 5 to address these.",compaction lcs,[],CASSANDRA,Improvement,Low,2014-05-20 13:23:46,0
12714922,Error when dropping keyspace.,"created a 3 node datacenter  called existing.

ran cassandra-stress:

{{cassandra-stress -R NetworkTopologyStrategy -O existing:2 -d existing0 -n 2000000 -k}}

Added a 2nd datacenter called new with 3 nodes started it with {{auto_bootstrap: false}}
{code}
alter keyspace ""Keyspace1"" with replication = {'class':'NetworkTopologyStrategy','existing':2,'new':2};
{code}
I then discovered that cassandra-stress --operation=read failed with LOCAL_QUORUM if a node was down in the local datacenter - this occured in both, but should not have, so decided to try again.

I shut down the new datacenter and removed all 3 nodes.  I then tried to drop the Keyspace1 keyspace.  cqlsh disconnected, and the log shows the error below.
{code}
ERROR [MigrationStage:1] 2014-05-16 23:57:03,085 CassandraDaemon.java (line 198) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.IllegalStateException: One row required, 0 found
at org.apache.cassandra.cql3.UntypedResultSet.one(UntypedResultSet.java:53)
at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:263)
at org.apache.cassandra.db.DefsTables.mergeKeyspaces(DefsTables.java:227)
at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:182)
at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:303)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
{code}",schema,[],CASSANDRA,Bug,Normal,2014-05-17 01:20:21,1
12714791,testDiskFailurePolicy_best_effort assertion error,"    [junit] Testsuite: org.apache.cassandra.db.DirectoriesTest
    [junit] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.297 sec
    [junit]
    [junit] Testcase: testDiskFailurePolicy_best_effort(org.apache.cassandra.db.DirectoriesTest):       FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.cassandra.db.DirectoriesTest.testDiskFailurePolicy_best_effort(DirectoriesTest.java:207)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.DirectoriesTest FAILED
",Windows,['Legacy/Testing'],CASSANDRA,Sub-task,Low,2014-05-16 15:54:18,6
12714777,Make incremental repair default in 3.0,"To get more testing etc, we should make incremental repair default in trunk",repair,[],CASSANDRA,Bug,Low,2014-05-16 14:27:07,0
12714754,Tuple type,"For CASSANDRA-6875 we need to be able to talk about tuples values and types (for prepared variables). Since we need it there, clients will need to support them anyway and so I think it would be a lot cleaner to start supporting those more generally. Besides, having tuples is a relatively simple and natural extension to what we have. I'll note in particular that tuple have a close relationship to user type in the sense that a tuple will be really just like an anonymous with no name for the fields and in particular a tuple value will be the same than a user type value.

The syntax would simply look like that:
{noformat}
CREATE TABLE foo (
    k int PRIMARY KEY,
    v tuple<int, text, float>
)

INSERT INTO foo(k, v) VALUES(0, (3, 'bar', 2.1));
{noformat}
We can also add projections in selects if we want:
{noformat}
SELECT v[0], v[2] FROM foo WHERE k = 0;
{noformat}
but that can come later (after all, we still don't have projections for collections and it's not a big deal).",cql3,['Legacy/CQL'],CASSANDRA,New Feature,Normal,2014-05-16 11:45:17,2
12714284,Post-compaction cache preheating can result in FileNotFoundExceptions when tables are dropped,"In CompactionTask.java, after a compaction finishes, we do this:

{code}
        replaceCompactedSSTables(toCompact, sstables);
        // TODO: this doesn't belong here, it should be part of the reader to load when the tracker is wired up
        for (SSTableReader sstable : sstables)
            sstable.preheat(cachedKeyMap.get(sstable.descriptor));
{code}

The problem is that if the table was dropped, {{replaceCompactedSSTables}} will release its references on the new {{sstables}}, resulting in them being closed.",compaction,[],CASSANDRA,Bug,Low,2014-05-14 20:35:32,0
12713401,Update ccm for windows launch script changes from #7001,The new .bat launch script changes will require changes to ccm and dtests so we can get windows dtests running on cassci.,Windows qa-resolved,['Legacy/Testing'],CASSANDRA,Improvement,Normal,2014-05-09 22:15:25,4
12713178,ColumnFamilyStoreTest unit tests fails on Windows,Looks like files aren't getting deleted correctly during testLoadNewSSTablesAvoidsOverwrites during a sanity check.  Test passes on linux.,Windows,['Legacy/Testing'],CASSANDRA,Sub-task,Low,2014-05-08 20:29:35,6
12713135,CliTest unit test fails on Windows,looks like a newline mismatch error,Windows,['Legacy/Testing'],CASSANDRA,Bug,Low,2014-05-08 17:38:01,6
12712881,Wrong class type: class org.apache.cassandra.db.Column in CounterColumn.reconcile,"When migrating a cluster of 6 nodes from 1.2.11 to 2.0.7, we started to see on the first migrated node this error:
{noformat}
ERROR [ReplicateOnWriteStage:1] 2014-05-07 11:26:59,779 CassandraDaemon.java (line 198) Exception in thread Thread[ReplicateOnWriteStage:1,5,main]
java.lang.AssertionError: Wrong class type: class org.apache.cassandra.db.Column
        at org.apache.cassandra.db.CounterColumn.reconcile(CounterColumn.java:159)
        at org.apache.cassandra.db.filter.QueryFilter$1.reduce(QueryFilter.java:109)
        at org.apache.cassandra.db.filter.QueryFilter$1.reduce(QueryFilter.java:103)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:112)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:98)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.filter.NamesQueryFilter.collectReducedColumns(NamesQueryFilter.java:98)
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:122)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1540)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1369)
        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:100)
        at org.apache.cassandra.service.StorageProxy$8$1.runMayThrow(StorageProxy.java:1085)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1916)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

We then saw on the other 5 nodes, still on 1.2.x, this error:
{noformat}
ERROR [MutationStage:2793] 2014-05-07 11:46:12,301 CassandraDaemon.java (line 191) Exception in thread Thread[MutationStage:2793,5,main]
java.lang.AssertionError: Wrong class type: class org.apache.cassandra.db.Column
        at org.apache.cassandra.db.CounterColumn.reconcile(CounterColumn.java:165)
        at org.apache.cassandra.db.AtomicSortedColumns$Holder.addColumn(AtomicSortedColumns.java:378)
        at org.apache.cassandra.db.AtomicSortedColumns.addColumn(AtomicSortedColumns.java:166)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
        at org.apache.cassandra.db.SuperColumn.addColumn(SuperColumn.java:218)
        at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:229)
        at org.apache.cassandra.db.ThreadSafeSortedColumns.addColumnInternal(ThreadSafeSortedColumns.java:108)
        at org.apache.cassandra.db.ThreadSafeSortedColumns.addAllWithSizeDelta(ThreadSafeSortedColumns.java:138)
        at org.apache.cassandra.db.AbstractColumnContainer.addAllWithSizeDelta(AbstractColumnContainer.java:99)
        at org.apache.cassandra.db.Memtable.resolve(Memtable.java:205)
        at org.apache.cassandra.db.Memtable.put(Memtable.java:168)
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:742)
        at org.apache.cassandra.db.Table.apply(Table.java:388)
        at org.apache.cassandra.db.Table.apply(Table.java:353)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:280)
        at org.apache.cassandra.db.CounterMutation.apply(CounterMutation.java:137)
        at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:773)
        at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:1651)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
{noformat}
Here some other stack we also on the 5 unmigrated nodes:
{noformat}
ERROR [ReadStage:4242] 2014-05-07 11:46:12,259 CassandraDaemon.java (line 191) Exception in thread Thread[ReadStage:4242,5,main]
java.lang.AssertionError: Wrong class type: class org.apache.cassandra.db.Column
        at org.apache.cassandra.db.CounterColumn.reconcile(CounterColumn.java:165)
        at org.apache.cassandra.db.AtomicSortedColumns$Holder.addColumn(AtomicSortedColumns.java:378)
        at org.apache.cassandra.db.AtomicSortedColumns.addColumn(AtomicSortedColumns.java:166)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
        at org.apache.cassandra.db.SuperColumn.addColumn(SuperColumn.java:218)
        at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:229)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.resolveAgainst(ArrayBackedSortedColumns.java:164)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:141)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
        at org.apache.cassandra.db.filter.QueryFilter$1.reduce(QueryFilter.java:112)
        at org.apache.cassandra.db.filter.QueryFilter$1.reduce(QueryFilter.java:96)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:111)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.filter.NamesQueryFilter.collectReducedColumns(NamesQueryFilter.java:103)
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1391)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1207)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
{noformat}

And the client side, it is failing with:
{noformat}
Caused by: org.apache.cassandra.thrift.UnavailableException: null
        at org.apache.cassandra.thrift.Cassandra$get_slice_result.read(Cassandra.java:7866)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_slice(Cassandra.java:594)
        at org.apache.cassandra.thrift.Cassandra$Client.get_slice(Cassandra.java:578)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:274)
{noformat}

After seeing such errors, we just shut down the first migrated node, hoping it would avoid all these client errors. But errors continue to be logged, even if there were only the 5 1.2.x nodes in the ring.
As the usual wild guess, let's reboot a node to fix it. At our damned surprise, it would restart and would fail with:
{noformat}
 INFO 11:33:40,190 Initializing system.LocationInfo
java.lang.AssertionError
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:162)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1541)
        at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1456)
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:306)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:287)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:154)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:574)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:253)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:381)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3
{noformat}

From there we only had 4 running nodes, with errors spreading around. So we halted everything, put the first node back to 1.2.11 and restored the data which has been snapshot just before the first node was migrated. 
",qa-resolved,[],CASSANDRA,Bug,Normal,2014-05-07 16:26:38,1
12712461,o.a.c.service.RemoveTest unit test failing in 2.1,bisecting..,qa-resolved,['Legacy/Testing'],CASSANDRA,Improvement,Low,2014-05-06 00:16:25,6
12712444,dtests should reuse existing clusters where possible,"Many dtests don't require special setup, specifically the cql tests. We can reuse the clusters we setup across multiple tests to save time.

Suggestion: only share clusters across a single test suite, and don't share the cluster by default, turn it on per-class. If we share them more broadly than that we may get into weird states because some tests definitely do mess with the cluster setup.",qa-resolved,[],CASSANDRA,Improvement,Low,2014-05-05 22:25:24,4
12712426,o.a.c.db.marshal.CollectionTypeTest unit test failing in 2.1,bisecting..,qa-resolved,['Legacy/Testing'],CASSANDRA,Improvement,Low,2014-05-05 21:39:07,2
12712423,o.a.c.db.ColumnFamilyTest.testDigest failing in 2.1,bisecting...,qa-resolved,[],CASSANDRA,Improvement,Low,2014-05-05 21:13:38,7
12712386,sstablemetadata command should print some more stuff,It would be nice if the sstablemetadata command printed out some more of the stuff we track.  Like the Min/Max column names and the min/max token in the file.,lhf,['Legacy/Tools'],CASSANDRA,Improvement,Low,2014-05-05 19:04:35,0
12712320,"Followup to 6914: null handling, duplicate column in resultSet and cleanup","The patch for CASSANDRA-6914 left a few stuffs not properly handled:
# A condition like {{IF m['foo'] = null}} is not handled and throw a NPE.
# It's using ByteBuffer.equals() to compare 2 collection values which is generally incorrect (the actual comparator should be used).
# If 2 conditions on 2 elements of the same collection were provided and the CAS failed, then the collection was duplicated in the resultSet.
# The ColumnCondition.WithVariables was generally a bit inefficient/ugly: it can lead to bind multiple times the same terms which is unnecessary. It's cleaner to directly create a condition with bound values.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Low,2014-05-05 14:24:45,2
12710662,cqlsh: DESCRIBE is not case-insensitive,"Keyspaces which are named starting with capital letters (and perhaps other things) sometimes require double quotes and sometimes do not.

For example, describe works without quotes:

cqlsh> describe keyspace ProductGenomeLocal;

CREATE KEYSPACE ""ProductGenomeLocal"" WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '3'
};

USE ""ProductGenomeLocal"";
[...]

But use will not:

cqlsh> use ProductGenomeLocal;
Bad Request: Keyspace 'productgenomelocal' does not exist

It seems that qoutes should only really be necessary when there's spaces or other symbols that need to be quoted. 

At the least, the acceptance or failures of quotes should be consistent.

Other minor annoyance: tab expansion works in use and describe with quotes, but will not work in either without quotes.
",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2014-04-25 21:25:23,4
12709810,Refuse CAS batch that have a 'USING TIMESTAMP',"Cassandra must refuse  BATCHes with {{TIMESTAMP}}, if they contain a CAS statement(s). Like this one:
{code}
BEGIN BATCH USING TIMESTAMP 1111111111111111
INSERT INTO users (id, firstname, lastname) VALUES (999, 'Jack', 'Sparrow')  IF NOT EXISTS
APPLY BATCH
{code}",LWT lhf,['Feature/Lightweight Transactions'],CASSANDRA,Sub-task,Low,2014-04-22 16:51:26,2
12709760,Simplify (and unify) cleanup of compaction leftovers,"Currently we manage a list of in-progress compactions in a system table, which we use to cleanup incomplete compactions when we're done. The problem with this is that 1) it's a bit clunky (and leaves us in positions where we can unnecessarily cleanup completed files, or conversely not cleanup files that have been superceded); and 2) it's only used for a regular compaction - no other compaction types are guarded in the same way, so can result in duplication if we fail before deleting the replacements.

I'd like to see each sstable store in its metadata its direct ancestors, and on startup we simply delete any sstables that occur in the union of all ancestor sets. This way as soon as we finish writing we're capable of cleaning up any leftovers, so we never get duplication. It's also much easier to reason about.",benedict-to-commit compaction,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Low,2014-04-22 13:10:35,3
12709550,Clean up unit tests on Windows,Currently we have unit tests failing on Windows that aren't failing on Unix.  We need to have parity on unit tests between platforms for future multi-platform development.,Windows,['Legacy/Testing'],CASSANDRA,Bug,Low,2014-04-21 13:49:09,6
12707312,"sstable_generation_loading_test sstableloader_compression_* dtests fail in 1.2, 2.0, and 2.1","This looks like something fundamentally wrong with the sstable_generation_loading_test.py dtest - 1.2 and 2.0 look like:
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-sJDYKB
ok
remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-gRTcgb
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time
10000,1000,1000,11.5,80.3,280.6,5
END
ok
sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-xneocV
Testing sstableloader with pre_compression=Deflate and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-_iJ1tD
Testing sstableloader with pre_compression=Deflate and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-FZAci9
Testing sstableloader with pre_compression=Deflate and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-AsoepN
Testing sstableloader with pre_compression=None and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Cfiwh8
Testing sstableloader with pre_compression=None and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-tTuQQg
Testing sstableloader with pre_compression=None and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-lA0rXi
Testing sstableloader with pre_compression=Snappy and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-jJ5iub
Testing sstableloader with pre_compression=Snappy and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-JTMBKg
Testing sstableloader with pre_compression=Snappy and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL

======================================================================
FAIL: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 108, in sstableloader_compression_deflate_to_deflate_test
    self.load_sstable_with_configuration('Deflate', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 102, in sstableloader_compression_deflate_to_none_test
    self.load_sstable_with_configuration('Deflate', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 105, in sstableloader_compression_deflate_to_snappy_test
    self.load_sstable_with_configuration('Deflate', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 90, in sstableloader_compression_none_to_deflate_test
    self.load_sstable_with_configuration(None, 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 84, in sstableloader_compression_none_to_none_test
    self.load_sstable_with_configuration(None, None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 87, in sstableloader_compression_none_to_snappy_test
    self.load_sstable_with_configuration(None, 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 99, in sstableloader_compression_snappy_to_deflate_test
    self.load_sstable_with_configuration('Snappy', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 93, in sstableloader_compression_snappy_to_none_test
    self.load_sstable_with_configuration('Snappy', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 96, in sstableloader_compression_snappy_to_snappy_test
    self.load_sstable_with_configuration('Snappy', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

----------------------------------------------------------------------
Ran 11 tests in 336.704s

FAILED (failures=9)
{noformat}

2.1 looks like:
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-gPkstl
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Vx3Bsj
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Ap0VwZ
Testing sstableloader with pre_compression=Deflate and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Nsy44b
Testing sstableloader with pre_compression=Deflate and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-77_xIH
Testing sstableloader with pre_compression=Deflate and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-x3esB0
Testing sstableloader with pre_compression=None and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-HgJni9
Testing sstableloader with pre_compression=None and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-QkUEf5
Testing sstableloader with pre_compression=None and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-mgv9zG
Testing sstableloader with pre_compression=Snappy and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-e0mEVq
Testing sstableloader with pre_compression=Snappy and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-x3D6Bu
Testing sstableloader with pre_compression=Snappy and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR

======================================================================
ERROR: incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 25, in incompressible_data_in_compressed_table_test
    cluster.populate(1).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 55, in remove_index_file_test
    cluster.populate(1).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 108, in sstableloader_compression_deflate_to_deflate_test
    self.load_sstable_with_configuration('Deflate', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 102, in sstableloader_compression_deflate_to_none_test
    self.load_sstable_with_configuration('Deflate', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 105, in sstableloader_compression_deflate_to_snappy_test
    self.load_sstable_with_configuration('Deflate', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 90, in sstableloader_compression_none_to_deflate_test
    self.load_sstable_with_configuration(None, 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 84, in sstableloader_compression_none_to_none_test
    self.load_sstable_with_configuration(None, None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 87, in sstableloader_compression_none_to_snappy_test
    self.load_sstable_with_configuration(None, 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 99, in sstableloader_compression_snappy_to_deflate_test
    self.load_sstable_with_configuration('Snappy', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 93, in sstableloader_compression_snappy_to_none_test
    self.load_sstable_with_configuration('Snappy', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 96, in sstableloader_compression_snappy_to_snappy_test
    self.load_sstable_with_configuration('Snappy', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

----------------------------------------------------------------------
Ran 11 tests in 56.599s

FAILED (errors=11)
{noformat}

2.1 last ccm node1 log (node2 is the same):
{noformat}
INFO  [main] 2014-04-08 19:20:33,339 CassandraDaemon.java:102 - Hostname: hana.12.am
INFO  [main] 2014-04-08 19:20:33,396 YamlConfigurationLoader.java:80 - Loading settings from file:/tmp/dtest-x3D6Bu/test/node1/conf/cassandra.yaml
INFO  [main] 2014-04-08 19:20:33,514 YamlConfigurationLoader.java:123 - Node configuration:[authenticator=AllowAllAuthenticator; authorizer=AllowAllAuthorizer; auto_bootstrap=false; auto_snapshot=true; batchlog_replay_throttle_in_kb=1024; cas_contention_timeout_in_ms=1000; client_encryption_options=<REDACTED>; cluster_name=test; column_index_size_in_kb=64; commitlog_directory=/tmp/dtest-x3D6Bu/test/node1/commitlogs; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_period_in_ms=10000; compaction_preheat_key_cache=true; compaction_throughput_mb_per_sec=16; concurrent_counter_writes=32; concurrent_reads=32; concurrent_writes=32; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=[/tmp/dtest-x3D6Bu/test/node1/data]; disk_failure_policy=stop; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; endpoint_snitch=SimpleSnitch; flush_directory=/tmp/dtest-x3D6Bu/test/node1/flush; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; in_memory_compaction_limit_in_mb=64; incremental_backups=false; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=-9223372036854775808; inter_dc_tcp_nodelay=false; internode_compression=all; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=127.0.0.1; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=0.4; native_transport_port=9042; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_validity_in_ms=2000; phi_convict_threshold=5; preheat_kernel_page_cache=false; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=10000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_timeout_in_ms=10000; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=127.0.0.1; rpc_keepalive=true; rpc_port=9160; rpc_server_type=sync; saved_caches_directory=/tmp/dtest-x3D6Bu/test/node1/saved_caches; seed_provider=[{class_name=org.apache.cassandra.locator.SimpleSeedProvider, parameters=[{seeds=127.0.0.1}]}]; server_encryption_options=<REDACTED>; snapshot_before_compaction=false; ssl_storage_port=7001; start_native_transport=true; start_rpc=true; storage_port=7000; thrift_framed_transport_size_in_mb=15; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=10000; write_request_timeout_in_ms=10000]
INFO  [main] 2014-04-08 19:20:33,740 DatabaseDescriptor.java:197 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2014-04-08 19:20:33,746 DatabaseDescriptor.java:285 - Global memtable on-heap threshold is enabled at 249MB
INFO  [main] 2014-04-08 19:20:33,747 DatabaseDescriptor.java:289 - Global memtable off-heap threshold is enabled at 249MB
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:113 - JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_51
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:141 - Heap size: 1046937600/1046937600
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:143 - Code Cache Non-heap memory: init = 2555904(2496K) used = 675712(659K) committed = 2555904(2496K) max = 50331648(49152K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - Par Eden Space Heap memory: init = 214827008(209792K) used = 81668768(79754K) committed = 214827008(209792K) max = 214827008(209792K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - Par Survivor Space Heap memory: init = 26804224(26176K) used = 0(0K) committed = 26804224(26176K) max = 26804224(26176K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - CMS Old Gen Heap memory: init = 805306368(786432K) used = 0(0K) committed = 805306368(786432K) max = 805306368(786432K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - CMS Perm Gen Non-heap memory: init = 21757952(21248K) used = 16716776(16324K) committed = 21757952(21248K) max = 85983232(83968K)
INFO  [main] 2014-04-08 19:20:34,143 CassandraDaemon.java:144 - Classpath: /home/mshuler/git/cassandra/build/cobertura/classes:/tmp/dtest-x3D6Bu/test/node1/conf:/home/mshuler/git/cassandra/build/classes/main:/home/mshuler/git/cassandra/build/classes/thrift:/home/mshuler/git/cassandra/lib/airline-0.6.jar:/home/mshuler/git/cassandra/lib/antlr-3.2.jar:/home/mshuler/git/cassandra/lib/commons-cli-1.1.jar:/home/mshuler/git/cassandra/lib/commons-codec-1.2.jar:/home/mshuler/git/cassandra/lib/commons-lang3-3.1.jar:/home/mshuler/git/cassandra/lib/commons-math3-3.2.jar:/home/mshuler/git/cassandra/lib/compress-lzf-0.8.4.jar:/home/mshuler/git/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/home/mshuler/git/cassandra/lib/disruptor-3.0.1.jar:/home/mshuler/git/cassandra/lib/guava-16.0.jar:/home/mshuler/git/cassandra/lib/high-scale-lib-1.1.2.jar:/home/mshuler/git/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar:/home/mshuler/git/cassandra/lib/javax.inject.jar:/home/mshuler/git/cassandra/lib/jbcrypt-0.3m.jar:/home/mshuler/git/cassandra/lib/jline-1.0.jar:/home/mshuler/git/cassandra/lib/jna-4.0.0.jar:/home/mshuler/git/cassandra/lib/json-simple-1.1.jar:/home/mshuler/git/cassandra/lib/libthrift-0.9.1.jar:/home/mshuler/git/cassandra/lib/logback-classic-1.1.2.jar:/home/mshuler/git/cassandra/lib/logback-core-1.1.12.jar:/home/mshuler/git/cassandra/lib/lz4-1.2.0.jar:/home/mshuler/git/cassandra/lib/metrics-core-2.2.0.jar:/home/mshuler/git/cassandra/lib/netty-all-4.0.17.Final.jar:/home/mshuler/git/cassandra/lib/reporter-config-2.1.0.jar:/home/mshuler/git/cassandra/lib/slf4j-api-1.7.2.jar:/home/mshuler/git/cassandra/lib/snakeyaml-1.11.jar:/home/mshuler/git/cassandra/lib/snappy-java-1.0.5.jar:/home/mshuler/git/cassandra/lib/stream-2.5.2.jar:/home/mshuler/git/cassandra/lib/super-csv-2.1.0.jar:/home/mshuler/git/cassandra/lib/thrift-server-0.3.3.jar:/home/mshuler/.m2/repository/net/sourceforge/cobertura/cobertura/1.9.4.1/cobertura-1.9.4.1.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar
WARN  [main] 2014-04-08 19:20:34,204 CLibrary.java:130 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
INFO  [main] 2014-04-08 19:20:34,227 CacheService.java:111 - Initializing key cache with capacity of 49 MBs.
INFO  [main] 2014-04-08 19:20:34,237 CacheService.java:133 - Initializing row cache with capacity of 0 MBs
INFO  [main] 2014-04-08 19:20:34,244 CacheService.java:150 - Initializing counter cache with capacity of 24 MBs
INFO  [main] 2014-04-08 19:20:34,246 CacheService.java:161 - Scheduling counter cache save to every 7200 seconds (going to save all keys).
INFO  [main] 2014-04-08 19:20:34,360 ColumnFamilyStore.java:283 - Initializing system.schema_triggers
INFO  [main] 2014-04-08 19:20:35,608 ColumnFamilyStore.java:283 - Initializing system.compaction_history
INFO  [main] 2014-04-08 19:20:35,618 ColumnFamilyStore.java:283 - Initializing system.batchlog
INFO  [main] 2014-04-08 19:20:35,624 ColumnFamilyStore.java:283 - Initializing system.sstable_activity
INFO  [main] 2014-04-08 19:20:35,630 ColumnFamilyStore.java:283 - Initializing system.peer_events
INFO  [main] 2014-04-08 19:20:35,643 ColumnFamilyStore.java:283 - Initializing system.compactions_in_progress
INFO  [main] 2014-04-08 19:20:35,652 ColumnFamilyStore.java:283 - Initializing system.hints
INFO  [main] 2014-04-08 19:20:35,656 ColumnFamilyStore.java:283 - Initializing system.schema_keyspaces
INFO  [main] 2014-04-08 19:20:35,660 ColumnFamilyStore.java:283 - Initializing system.range_xfers
INFO  [main] 2014-04-08 19:20:35,664 ColumnFamilyStore.java:283 - Initializing system.schema_columnfamilies
INFO  [main] 2014-04-08 19:20:35,669 ColumnFamilyStore.java:283 - Initializing system.NodeIdInfo
INFO  [main] 2014-04-08 19:20:35,677 ColumnFamilyStore.java:283 - Initializing system.paxos
INFO  [main] 2014-04-08 19:20:35,682 ColumnFamilyStore.java:283 - Initializing system.schema_usertypes
INFO  [main] 2014-04-08 19:20:35,686 ColumnFamilyStore.java:283 - Initializing system.schema_columns
INFO  [main] 2014-04-08 19:20:35,691 ColumnFamilyStore.java:283 - Initializing system.IndexInfo
INFO  [main] 2014-04-08 19:20:35,695 ColumnFamilyStore.java:283 - Initializing system.peers
INFO  [main] 2014-04-08 19:20:35,700 ColumnFamilyStore.java:283 - Initializing system.local
INFO  [main] 2014-04-08 19:20:35,820 DatabaseDescriptor.java:587 - Couldn't detect any schema definitions in local storage.
INFO  [main] 2014-04-08 19:20:35,821 DatabaseDescriptor.java:592 - To create keyspaces and column families, see 'help create' in cqlsh.
INFO  [main] 2014-04-08 19:20:35,903 ColumnFamilyStore.java:853 - Enqueuing flush of local: 1109 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:35,921 Memtable.java:344 - Writing Memtable-local@1878619947(220 serialized bytes, 5 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:36,021 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-1-Data.db (171 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=403)
INFO  [main] 2014-04-08 19:20:36,038 CommitLog.java:108 - No commitlog files found; skipping replay
INFO  [main] 2014-04-08 19:20:36,535 StorageService.java:510 - Cassandra version: 2.1.0-beta1-SNAPSHOT
INFO  [main] 2014-04-08 19:20:36,535 StorageService.java:511 - Thrift API version: 19.39.0
INFO  [main] 2014-04-08 19:20:36,559 StorageService.java:512 - CQL supported versions: 2.0.0,3.1.5 (default: 3.1.5)
INFO  [main] 2014-04-08 19:20:36,595 IndexSummaryManager.java:99 - Initializing index summary manager with a memory pool size of 49 MB and a resize interval of 60 minutes
INFO  [main] 2014-04-08 19:20:36,607 StorageService.java:537 - Loading persisted ring state
INFO  [main] 2014-04-08 19:20:36,679 StorageService.java:858 - Saved tokens not found. Using configuration value: [-9223372036854775808]
INFO  [main] 2014-04-08 19:20:36,701 MigrationManager.java:206 - Create new Keyspace: KSMetaData{name=system_traces, strategyClass=SimpleStrategy, strategyOptions={replication_factor=2}, cfMetaData={sessions=org.apache.cassandra.config.CFMetaData@7892cf41[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(706172616d6574657273:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type))),comment=traced sessions,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=duration, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=parameters, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=started_at, type=org.apache.cassandra.db.marshal.TimestampType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=request, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=coordinator, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}], events=org.apache.cassandra.config.CFMetaData@3a437a40[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type),comment=,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=source, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=thread, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=activity, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=14 cap=14]=ColumnDefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=event_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]}, durableWrites=true, userTypes=org.apache.cassandra.config.UTMetaData@240f1da2}
INFO  [MigrationStage:1] 2014-04-08 19:20:36,826 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 990 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:36,834 Memtable.java:344 - Writing Memtable-schema_keyspaces@2107075397(251 serialized bytes, 7 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:36,915 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-1-Data.db (216 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:36,920 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 164066 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:36,921 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1732384250(30202 serialized bytes, 514 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:37,019 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-Data.db (6720 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:37,033 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 288881 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:37,034 Memtable.java:344 - Writing Memtable-schema_columns@985819426(46627 serialized bytes, 904 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:37,140 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-1-Data.db (10835 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:37,351 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@40e9da6[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(706172616d6574657273:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type))),comment=traced sessions,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=duration, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=parameters, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=started_at, type=org.apache.cassandra.db.marshal.TimestampType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=request, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=coordinator, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 19:20:37,357 ColumnFamilyStore.java:283 - Initializing system_traces.sessions
INFO  [MigrationStage:1] 2014-04-08 19:20:37,358 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@5b8fff5e[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type),comment=,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=source, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=thread, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=activity, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=14 cap=14]=ColumnDefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=event_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 19:20:37,363 ColumnFamilyStore.java:283 - Initializing system_traces.events
ERROR [MigrationStage:1] 2014-04-08 19:20:37,426 CassandraDaemon.java:166 - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340) ~[main/:na]
        at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [main] 2014-04-08 19:20:37,427 CassandraDaemon.java:471 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207) ~[main/:na]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543) [main/:na]
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.get(FutureTask.java:188) ~[na:1.7.0_51]
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407) ~[main/:na]
        ... 8 common frames omitted
Caused by: java.lang.AssertionError: null
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340) ~[main/:na]
        at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
ERROR [StorageServiceShutdownHook] 2014-04-08 19:20:37,437 CassandraDaemon.java:166 - Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.gms.Gossiper.stop(Gossiper.java:1270) ~[main/:na]
        at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:581) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
{noformat}",qa-resolved,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2014-04-09 00:12:18,0
12707271,auth_test system_auth_ks_is_alterable_test dtest hangs in 2.1 and 2.0,"This test hangs forever. When I hit ctl-c after running the test, then the ccm nodes actually continue running - I think ccm is looking for log lines that never occur until the test is killed(?).
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; ENABLE_VNODES=true PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 auth_test.py:TestAuth.system_auth_ks_is_alterable_test
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
system_auth_ks_is_alterable_test (auth_test.TestAuth) ... cluster ccm directory: /tmp/dtest-O3AAJr
^C
{noformat}
Search for (hanging here) below - I typed this prior to hitting ctl-c. Then the nodes start running again and I see ""Listening for thrift clients"" later on.
{noformat}
mshuler@hana:~$ tail -f /tmp/dtest-O3AAJr/test/node*/logs/system.log
==> /tmp/dtest-O3AAJr/test/node1/logs/system.log <==
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,599 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1792243696(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:2] 2014-04-08 16:45:12,603 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node1/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  14,454 bytes to 11,603 (~80% of original) in 105ms = 0.105386MB/s.  7 total partitions merged to 3.  Partition merge counts were {1:1, 2:1, 4:1, }
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,668 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504671, position=193292)
INFO  [MigrationStage:1] 2014-04-08 16:45:12,669 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6806 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,670 Memtable.java:344 - Writing Memtable-schema_columns@352928691(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:1] 2014-04-08 16:45:12,672 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node1/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  710 bytes to 233 (~32% of original) in 70ms = 0.003174MB/s.  6 total partitions merged to 3.  Partition merge counts were {1:2, 4:1, }
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,721 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504671, position=193830)
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,566 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.
INFO  [NonPeriodicTasks:1] 2014-04-08 16:45:20,570 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'
INFO  [NonPeriodicTasks:1] 2014-04-08 16:45:21,806 Auth.java:232 - Created default superuser 'cassandra'

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,214 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1004 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,215 Memtable.java:344 - Writing Memtable-schema_keyspaces@781373873(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,295 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,296 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 34190 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,297 Memtable.java:344 - Writing Memtable-schema_columnfamilies@2077216447(5746 serialized bytes, 108 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,369 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db (2088 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,370 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 37408 (0%) on-heap, 0 (0%) off-heap
INFO  [CompactionExecutor:4] 2014-04-08 16:45:12,371 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db')]
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,371 Memtable.java:344 - Writing Memtable-schema_columns@2003573271(5173 serialized bytes, 119 ops, 0%/0% of on/off-heap limit)
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,248 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,654 Memtable.java:344 - Writing Memtable-schema_keyspaces@789549279(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,788 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=193958)
INFO  [InternalResponseStage:1] 2014-04-08 16:45:11,789 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 34192 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:11,790 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1695849916(5746 serialized bytes, 108 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:11,898 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db (2087 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=194091)
INFO  [InternalResponseStage:1] 2014-04-08 16:45:11,899 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 37410 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,900 Memtable.java:344 - Writing Memtable-schema_columns@2090391222(5173 serialized bytes, 119 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,998 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db (1700 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=194091)
INFO  [main] 2014-04-08 16:45:18,654 CassandraDaemon.java:533 - No gossip backlog; proceeding
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,131 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.

(hanging here)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [NonPeriodicTasks:1] 2014-04-08 16:49:02,863 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [NonPeriodicTasks:1] 2014-04-08 16:49:02,888 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:02,919 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db (1699 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:02,922 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db')]

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [InternalResponseStage:3] 2014-04-08 16:49:02,959 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1006 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:02,960 Memtable.java:344 - Writing Memtable-schema_keyspaces@44265998(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:02,970 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 501 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:02,972 Memtable.java:344 - Writing Memtable-schema_keyspaces@603519674(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [main] 2014-04-08 16:49:03,029 Server.java:159 - Starting listening for CQL clients on /127.0.0.3:9042...

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [CompactionExecutor:4] 2014-04-08 16:49:03,064 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-13,].  13,907 bytes to 7,643 (~54% of original) in 213ms = 0.034220MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,069 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [InternalResponseStage:3] 2014-04-08 16:49:03,071 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 42850 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,071 Memtable.java:344 - Writing Memtable-schema_columnfamilies@666219518(7373 serialized bytes, 135 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,087 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=247475)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,090 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  16,411 bytes to 11,314 (~68% of original) in 165ms = 0.065393MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }
INFO  [CompactionExecutor:6] 2014-04-08 16:49:03,090 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-13-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db')]
INFO  [MigrationStage:1] 2014-04-08 16:49:03,091 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8799 (0%) on-heap, 0 (0%) off-heap

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [main] 2014-04-08 16:49:03,091 ThriftServer.java:119 - Binding thrift service to /127.0.0.3:9160

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,091 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1821762369(1609 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [Thread-8] 2014-04-08 16:49:03,098 ThriftServer.java:136 - Listening for thrift clients...
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,166 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db (2536 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [InternalResponseStage:3] 2014-04-08 16:49:03,167 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 44033 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,168 Memtable.java:344 - Writing Memtable-schema_columns@1052245443(6187 serialized bytes, 140 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:8] 2014-04-08 16:49:03,168 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9-Data.db')]

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,203 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (949 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=248454)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,204 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6833 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,205 Memtable.java:344 - Writing Memtable-schema_columns@1698728310(1041 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:6] 2014-04-08 16:49:03,218 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  750 bytes to 233 (~31% of original) in 125ms = 0.001778MB/s.  8 total partitions merged to 3.  Partition merge counts were {1:1, 3:1, 4:1, }

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,278 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db (1990 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,279 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db')]

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,310 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (431 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=248454)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [CompactionExecutor:8] 2014-04-08 16:49:03,313 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-13,].  14,821 bytes to 8,115 (~54% of original) in 140ms = 0.055279MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }
INFO  [MigrationStage:1] 2014-04-08 16:49:03,313 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 502 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,315 Memtable.java:344 - Writing Memtable-schema_keyspaces@901379409(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,332 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 501 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,333 Memtable.java:344 - Writing Memtable-schema_keyspaces@1181313624(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,392 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=217732)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,397 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8818 (0%) on-heap, 0 (0%) off-heap
INFO  [CompactionExecutor:7] 2014-04-08 16:49:03,397 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-13-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db')]
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,400 Memtable.java:344 - Writing Memtable-schema_columnfamilies@2077765501(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,405 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  16,987 bytes to 11,600 (~68% of original) in 123ms = 0.089940MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,418 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-18-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252763)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,419 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8817 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,420 Memtable.java:344 - Writing Memtable-schema_columnfamilies@495883972(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,483 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=217732)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,484 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6807 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,485 Memtable.java:344 - Writing Memtable-schema_columns@365627698(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,519 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-15-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252896)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,520 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6806 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,521 Memtable.java:344 - Writing Memtable-schema_columns@1830789468(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [CompactionExecutor:7] 2014-04-08 16:49:03,523 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  750 bytes to 233 (~31% of original) in 122ms = 0.001821MB/s.  8 total partitions merged to 3.  Partition merge counts were {1:1, 3:1, 4:1, }
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,562 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=218351)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,581 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-15-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252896)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,586 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 502 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,586 Memtable.java:344 - Writing Memtable-schema_keyspaces@543512535(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,604 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@1327e4ff[cfId=2d324e48-3275-3517-8dd5-9a2c5b0856c5,ksName=system_auth,cfName=permissions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7065726d697373696f6e73:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=7776000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UTF8Type,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=permissions, type=org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=username, type=org.apache.cassandra.db.marshal.UTF8Type, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=resource, type=org.apache.cassandra.db.marshal.UTF8Type, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=0,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 16:49:03,610 ColumnFamilyStore.java:283 - Initializing system_auth.permissions
INFO  [main] 2014-04-08 16:49:03,622 CassandraDaemon.java:501 - Waiting for gossip to settle before accepting client requests...
INFO  [InternalResponseStage:5] 2014-04-08 16:49:03,625 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1004 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,626 Memtable.java:344 - Writing Memtable-schema_keyspaces@2064761560(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,664 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-18-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=222660)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,665 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8818 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,666 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1464552123(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
<...>
{noformat}",qa-resolved,['Test/dtest/python'],CASSANDRA,Improvement,Normal,2014-04-08 21:56:40,4
12707261,bootstrap_test simple_bootstrap_test dtest fails in 2.1,"I patched ccm with https://github.com/pcmanus/ccm/pull/109 and got an error from simple_bootstrap:
{noformat}
======================================================================
FAIL: simple_bootstrap_test (bootstrap_test.TestBootstrap)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/bootstrap_test.py"", line 58, in simple_bootstrap_test
    assert_almost_equal(initial_size, 2 * size1)
  File ""/home/mshuler/git/cassandra-dtest/assertions.py"", line 26, in assert_almost_equal
    assert vmin > vmax * (1.0 - error), ""values not within %.2f%% of the max: %s"" % (error * 100, args)
AssertionError: values not within 16.00% of the max: (0, 186396)
{noformat}",qa-resolved,['Test/dtest/python'],CASSANDRA,Bug,Normal,2014-04-08 21:19:29,0
12707218,Windows launch feature parity - augment launch process using PowerShell to match capabilities of *nix launching,"The current .bat-based launching has neither the logic nor robustness of a bash or PowerShell-based solution.  In pursuit of making Windows a 1st-class citizen for C*, we need to augment the launch-process using something like PowerShell to get as close to feature-parity as possible with Linux.",Windows qa-resolved,['Packaging'],CASSANDRA,Improvement,Normal,2014-04-08 17:30:59,6
12707157,HintedHandoff - expired hints may block future hints deliveries,"For tests purposes, DC2 was shut down for 1 day. The _hints_ table was filled with millions of rows. Now, when _HintedHandOffManager_ tries to _doDeliverHintsToEndpoint_  it queries the store with QueryFilter.getSliceFilter which counts deleted (TTLed) cells and throws org.apache.cassandra.db.filter.TombstoneOverwhelmingException. 
Throwing this exception stops the manager from running compaction as it is run only after successful handoff. This leaves the HH practically disabled till administrator runs truncateAllHints. 
Wouldn't it be nicer if on org.apache.cassandra.db.filter.TombstoneOverwhelmingException run compaction? That would remove TTLed hints leaving whole HH mechanism in a healthy state.

The stacktrace is:
{quote}
org.apache.cassandra.db.filter.TombstoneOverwhelmingException
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:201)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:122)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1487)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1306)
	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:351)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:309)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92)
	at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:530)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{quote}
",HintedHandoff TTL,[],CASSANDRA,Bug,Normal,2014-04-08 11:16:31,1
12707042,Windows: remove mmap'ed I/O for index files and force standard file access,Memory-mapped I/O on Windows causes issues with hard-links; we're unable to delete hard-links to open files with memory-mapped segments even using nio.  We'll need to push for close to performance parity between mmap'ed I/O and buffered going forward as the buffered / compressed path offers other benefits.,Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Low,2014-04-07 22:26:56,6
12705654,upgradesstables does not maintain levels for existing SSTables,"Initially ran into this issue on a DSE 3.2 (C* 1.2) to DSE 4.0 (C* 2.0) upgrade, and then I was able to reproduce it when testing an upgrade from C* 2.0.5 to C* 2.1-beta so the problem still exists in the latest code.

Basically after you've upgraded to the new version and run ""nodetool upgradesstables"" on a CF/table that has been using LCS, then all of the non-L0 SSTables will be changed to L0 in the upgraded SSTables. In other words, they don't maintain their level and will have to go through the compaction again. The problem is that if you've got thousands of non-L0 SSTables before the upgrade, then all of these files showing up in L0 will push the system to do STCS and start to build some huge L0 tables. If a user doesn't budget enough free space (for example, if they used the recommended guideline and only budgeted 10% of free space because LCS is in use), then this STCS in L0 effect will have them run out of space.",lcs,[],CASSANDRA,Bug,Normal,2014-03-31 13:50:32,0
12705579,Native protocol v2 spec is missing column type definition for text,"Native protocol v2 spec is missing column type definition for text. Should be 0x000A.

https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v2.spec#L526",native_protocol,['Legacy/Documentation and Website'],CASSANDRA,Bug,Low,2014-03-31 02:57:14,2
12704100,liveRatio jumps to max when Memtable is empty,"liveRatio calculation on an empty memtable results in a value of Infinity since memtable.currentSize=0.  Infinity then gets capped at the liveRatio max of 64.

{noformat}
WARN [MemoryMeter:1] 2014-03-19 09:26:59,483 Memtable.java (line 441) setting live ratio to maximum of 64.0 instead of Infinity
INFO [MemoryMeter:1] 2014-03-19 09:26:59,485 Memtable.java (line 452) CFS(Keyspace='system', ColumnFamily='compactions_in_progress') liveRatio is 64.0 (just-counted was 64.0).  calculation took 7ms for 0 cells
{noformat}

Jumping liveRatio to the max value based on an empty Memtable leads to more frequent flushing than may be necessary.

CASSANDRA-4243 previously addressed this issue, but was resolved as fixed by CASSANDRA-3741.  It does not appear this issue has been fixed as of 2.0.5",memtables,[],CASSANDRA,Bug,Low,2014-03-27 20:49:52,1
12703505,Fix UnsupportedOperationException on CAS timeout,"This is due to CASSANDRA-6595: we call blockFor() for the paxos consistency, but we never added SERIAL/LOCAL_SERIAL in that method. Attaching trivial patch to add it.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Low,2014-03-25 16:14:18,2
12703187,Map element is not allowed in CAS condition with DELETE/UPDATE query,"{code}
CREATE TABLE test (id int, data map<text,text>, PRIMARY KEY(id));

INSERT INTO test (id, data) VALUES (1,{'a':'1'});

DELETE FROM test WHERE id=1 IF data['a']=null;
Bad Request: line 1:40 missing EOF at '='

UPDATE test SET data['b']='2' WHERE id=1 IF data['a']='1';
Bad Request: line 1:53 missing EOF at '='
{code}
These queries was successfuly executed with cassandra 2.0.5, but don't work in 2.0.6 release",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2014-03-24 12:53:47,2
12703004,ignore snapshot repair flag on Windows,"Per discussion in CASSANDRA-4050, we should ignore the snapshot repair flag on windows, and log a warning while proceeding to do non-snapshot repair.",Windows,['Legacy/Tools'],CASSANDRA,Bug,Normal,2014-03-22 01:02:41,6
12702597,Unintended update with conditional statement,"After updated to 2.0.6, I have encountered the strange behavior of conditional updates.

When I executed CQL like UPDATE test SET value = ? WHERE id = ? IF value = ? in concurrent, sometimes cassandra returns true even if value is not satisfied the condition.

I have attached the program which reproduce this issue. The program works fine in cassandra 2.0.5. But it seems that resets values while execution in 2.0.6.",LWT qa-resolved,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2014-03-20 10:14:48,2
12702474,Standardize on a single read path,"Since we actively unmap unreferenced SSTR's and also copy data out of those readers on the read path, the current memory mapped i/o is a lot of complexity for very little payoff.  Clean out the mmapp'ed i/o on the read path.",performance,['Legacy/Local Write-Read Paths'],CASSANDRA,Improvement,Normal,2014-03-19 20:43:49,6
12701462,Native protocol V3,"I think we need a V3 of the protocol for 2.1. The things that this could/should includes are:
# Adding an optional Serial CL for protocol batches (like we have for QUERY and EXECUTE). It was an oversight of V2 of not adding it, and now that we can batch conditional updates, it's definitively missing.
# Proper type codes for UDT. This is not *strictly* needed to be able to support UDT since currently a UDT will be sent as a ""custom type"" with his fully class name + arguments. But parsing that is no fun nor convenient for clients. It's also not particular space efficient (though that's probably not a huge concern since with prepared statement you can avoid sending the ResultSet metadata every time).
# Serialization format for collections. Currently the serialization format only allow for 65K elements, each of 65K bytes size at most. While collections are not meant to store large amount of data, having the limitation in the protocol serialization format is the wrong way to deal with that. Concretely, the current workaround for CASSANDRA-5428 is ugly. I'll note that the current serialization format is also an obstacle to supporting null inside collections (whether or not we want to support null there is a good question, but here again I'm not sure being limited by the serialization format is a good idea).
# CASSANDRA-6178: I continue to believe that in many case it makes somewhat more sense to have the default timestamp provided by the client (this is a necessary condition for true idempotent retries in particular). I'm absolutely fine making that optional and leaving server-side generated timestamps by default, but since client can already provide timestamp in query string anyway, I don't see a big deal in making it easier for client driver to control that without messing with the query string.
# Optional names for values in QUERY messages: it has been brought to my attention that while V2 allows to send a query string with values for a one-roundtrip bind-and-execute, a driver can't really support named bind marker with that feature properly without parsing the query. The proposition is thus to make it (optionally) possible to ship the name of the marker each value is supposed to be bound to.

I think that 1) and 2) are enough reason to make a V3 (even if there is disagreement on the rest that is).

3) is a little bit more involved tbh but I do think having the current limitations bolted in the protocol serialization format is wrong in the long run, and it turns out that due to UDT we will start storing serialized collections internally so if we want to lift said limitation in the serialization format, we should do it now and everywhere, as doing it afterwards will be a lot more painful.

4) and 5) are probably somewhat more minor, but at the same time, both are completely optional (a driver won't have to support those if he doesn't want). They are really just about making things more flexible for client drivers and they are not particularly hard to support so I don't see too many reasons not to include them.

Last but not least, I know that some may find it wrong to do a new protocol version with each major of C*, so let me state my view here: I fully agree that we shouldn't make an habit of that in the long run and that's definitively *not* my objective. However, it would be silly to expect that we could get everything right and forget nothing in the very first version. It shouldn't be surprising that we'll have to burn a few versions (and there might be a few more yet) before getting something more stable and complete and I think that delaying the addition of stuffs that are useful to create some fake notion of stability would be even more silly. On the bright side, the additions of this V3 are comparatively much more simple to implement for a client that those of V2 (in fact, for clients that want to support UDT, it will probably require less effort to add the changes for this new version than to try to support UDT without it), so I do think we make good progress on getting the protocol stabilized 
",qa-resolved,['Legacy/CQL'],CASSANDRA,New Feature,Normal,2014-03-14 13:07:27,2
12701241,Allow filtering on primary key expressions in 2i queries,"We allow

{code}
SELECT a, d FROM t.t WHERE b = 'b1' AND a = 'a14521'
{code}

and

{code}
SELECT a, d FROM t.t WHERE b = 'b1' AND token(a)  > token( 'a14521')
{code}

but not
{code}
SELECT a, d FROM t.t WHERE b = 'b1' AND a  > 'a14521'
{code}

(given an index on {{b}}, with primary key {{a}})

we allow combining other predicates with an indexed one and filtering those in a nested loop; we should allow the same for primary keys",indexes,['Feature/2i Index'],CASSANDRA,New Feature,Low,2014-03-13 14:26:41,2
12700738,Batch CAS does not support LOCAL_SERIAL,"The batch CAS feature introduced in Cassandra 2.0.6 does not support the LOCAL_SERIAL consistency level, and always uses SERIAL.

Create a cluster with 4 nodes with the following topology:

{code}
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
UN  127.0.0.3  269 KB     256     26.3%  ae92d997-6042-42d9-b447-943080569742  RAC1
UN  127.0.0.4  197.81 KB  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  RAC1
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
UN  127.0.0.1  226.92 KB  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  RAC1
UN  127.0.0.2  179.27 KB  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  RAC1
{code}

In cqlsh:
{code}
cqlsh> CREATE KEYSPACE foo WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1': 2, 'DC2': 2};
cqlsh> USE foo;
cqlsh:foo> CREATE TABLE bar (x text, y bigint, z bigint, t bigint, PRIMARY KEY(x,y));
{code}

Kill nodes 127.0.0.3 and 127.0.0.4:

{code}
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
DN  127.0.0.3  262.37 KB  256     26.3%  ae92d997-6042-42d9-b447-943080569742  RAC1
DN  127.0.0.4  208.04 KB  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  RAC1
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
UN  127.0.0.1  214.82 KB  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  RAC1
UN  127.0.0.2  178.23 KB  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  RAC1
{code}

Connect to 127.0.0.1 in DC1 and run a CAS batch at CL.LOCAL_SERIAL+LOCAL_QUORUM:

{code}
        final Cluster cluster = new Cluster.Builder()
                .addContactPoint(""127.0.0.1"")
                .withLoadBalancingPolicy(new DCAwareRoundRobinPolicy(""DC1""))
                .build();

        final Session session = cluster.connect(""foo"");

        Batch batch = QueryBuilder.batch();
        batch.add(new SimpleStatement(""INSERT INTO bar (x,y,z) VALUES ('abc', 123, 1) IF NOT EXISTS""));
        batch.add(new SimpleStatement(""UPDATE bar SET t=2 WHERE x='abc' AND y=123""));

        batch.setConsistencyLevel(ConsistencyLevel.LOCAL_QUORUM);
        batch.setSerialConsistencyLevel(ConsistencyLevel.LOCAL_SERIAL);

        session.execute(batch);
{code}

The batch fails with:

{code}
Caused by: com.datastax.driver.core.exceptions.UnavailableException: Not enough replica available for query at consistency SERIAL (3 required but only 2 alive)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:44)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:33)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:182)
	at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:66)
	... 21 more
{code}",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2014-03-11 16:04:49,2
12698552,compaction and scrub data directories race on startup," 
Hi,  

On doing a rolling restarting of a 2.0.5 cluster in several environments I'm seeing the following error:
{code}

 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,549 CompactionTask.java (line 115) Compacting [SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compactio
n_race/node1/data/system/local/system-local-jb-15-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-16-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/syst
em-local-jb-14-Data.db')]
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,557 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,560 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 WARN [main] 2014-03-03 17:11:07,608 ColumnFamilyStore.java (line 473) Removing orphans for /Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13: [CompressionInfo.db, Filter.db, Index.db, TOC.txt, Summary.db, Data.db, Statistics.
db]
ERROR [main] 2014-03-03 17:11:07,609 CassandraDaemon.java (line 479) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-local-jb-13-CompressionInfo.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:111)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:106)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:476)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,612 CompactionTask.java (line 275) Compacted 4 sstables to [/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-17,].  10,963 bytes to 5,572 (~50% of original) in 57ms = 0.093226MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }

{code}
Seems like a potential race, since compactions are occurring whilst the existing data directories are being scrubbed.
Probably an in progress compaction looks like an incomplete one and results in it being attempted to be scrubbed whilst in progress. 
On the attempt to delete in the scrubDataDirectories we discover that it no longer exists, presumably because it has now been compacted away. 
This then causes an assertion error and the node fails to start up. 

Here is a ccm script which just stops and starts a 3 node 2.0.5 cluster repeatedly. 
It seems to fairly reliably reproduce the problem, in less than ten iterations: 

{code}
#!/bin/bash

ccm create compaction_race -v 2.0.5
ccm populate -n 3
ccm start

for i in $(seq 0 1000); do 
    echo $i;
    ccm stop
    ccm start
    grep ERR ~/.ccm/compaction_race/*/logs/system.log;
done

{code}
 
Someone else should probably confirm that this is what is going wrong,  
however if it is, the solution might be as simple as to disable autocompactions slightly earlier in CassandraDaemon.setup. 
 
Or alternatively if there isn't a good reason why we are first scrubbing the system tables and then scrubbing all keyspaces (including the system keyspace), you could perhaps just scrub solely the non system keyspaces on the second scrub.

Please let me know if there is anything else I can provide.
Thanks,
Matt
",compaction concurrency starting,"['Local/Compaction', 'Local/Startup and Shutdown']",CASSANDRA,Bug,Low,2014-03-04 02:02:52,6
12698449,Forceful restart of C* during compaction on windows leads to exceptions on startup in scrubDataDirectories,"From comments on CASSANDRA-6736 (Bill Mitchell):

Trying to be polite, I started using Drain to shutdown Cassandra before rebooting the machine. In one case, this provoked numerous ThreadPoolExecutor has shutdown messages underneath the compactor:
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:23,743 StorageService.java (line 947) DRAINING: starting drain process
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:23,783 ThriftServer.java (line 141) Stop listening to thrift clients
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:24,980 Server.java (line 181) Stop listening for CQL clients
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:24,980 Gossiper.java (line 1251) Announcing shutdown
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:27,001 MessagingService.java (line 665) Waiting for messaging service to quiesce
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:27,040 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-sr@1217138300(1825983/4411193 serialized/live bytes, 29946 ops)
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:27,040 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-etol@703118381(2963818/46129889 serialized/live bytes, 68926 ops)
INFO [FlushWriter:272] 2014-02-24 08:34:27,040 Memtable.java (line 333) Writing Memtable-sr@1217138300(1825983/4411193 serialized/live bytes, 29946 ops)
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:27,054 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-events@899982591(188/1880 serialized/live bytes, 7 ops)
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:27,075 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-events_timeline@1379706298(16/160 serialized/live bytes, 1 ops)
INFO [FlushWriter:273] 2014-02-24 08:34:27,075 Memtable.java (line 333) Writing Memtable-etol@703118381(2963818/46129889 serialized/live bytes, 68926 ops)
INFO [ACCEPT-localhost/127.0.0.1] 2014-02-24 08:34:27,144 MessagingService.java (line 875) MessagingService has terminated the accept() thread
INFO [FlushWriter:272] 2014-02-24 08:34:27,411 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\testdb_1393207231382\sr\testdb_1393207231382-sr-jb-473-Data.db (428854 bytes) for commitlog position ReplayPosition(segmentId=1393178353775, position=18771262)
INFO [FlushWriter:272] 2014-02-24 08:34:27,411 Memtable.java (line 333) Writing Memtable-events@899982591(188/1880 serialized/live bytes, 7 ops)
INFO [FlushWriter:273] 2014-02-24 08:34:27,932 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\testdb_1393207231382\etol\testdb_1393207231382-etol-jb-1563-Data.db (1012805 bytes) for commitlog position ReplayPosition(segmentId=1393178353775, position=18771262)
INFO [FlushWriter:273] 2014-02-24 08:34:27,933 Memtable.java (line 333) Writing Memtable-events_timeline@1379706298(16/160 serialized/live bytes, 1 ops)
INFO [FlushWriter:272] 2014-02-24 08:34:28,366 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-32-Data.db (184 bytes) for commitlog position ReplayPosition(segmentId=1393178353775, position=18771262)
INFO [FlushWriter:273] 2014-02-24 08:34:28,456 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\OpsCenter\events_timeline\OpsCenter-events_timeline-jb-39-Data.db (47 bytes) for commitlog position ReplayPosition(segmentId=1393178353775, position=18771262)
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:28,457 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-compaction_history@814197203(1725/19675 serialized/live bytes, 45 ops)
INFO [FlushWriter:272] 2014-02-24 08:34:28,458 Memtable.java (line 333) Writing Memtable-compaction_history@814197203(1725/19675 serialized/live bytes, 45 ops)
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:28,458 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-sstable_activity@446592137(13500/207410 serialized/live bytes, 1442 ops)
INFO [FlushWriter:273] 2014-02-24 08:34:28,458 Memtable.java (line 333) Writing Memtable-sstable_activity@446592137(13500/207410 serialized/live bytes, 1442 ops)
INFO [FlushWriter:273] 2014-02-24 08:34:28,732 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\system\sstable_activity\system-sstable_activity-jb-428-Data.db (4072 bytes) for commitlog position ReplayPosition(segmentId=1393178353775, position=18771471)
INFO [FlushWriter:272] 2014-02-24 08:34:28,761 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\system\compaction_history\system-compaction_history-jb-315-Data.db (823 bytes) for commitlog position ReplayPosition(segmentId=1393178353775, position=18771471)
INFO [RMI TCP Connection(2095)-127.0.0.1] 2014-02-24 08:34:29,089 StorageService.java (line 947) DRAINED
INFO [CompactionExecutor:70] 2014-02-24 08:34:43,085 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-compactions_in_progress@1194966346(0/0 serialized/live bytes, 1 ops)
INFO [FlushWriter:273] 2014-02-24 08:34:43,086 Memtable.java (line 333) Writing Memtable-compactions_in_progress@1194966346(0/0 serialized/live bytes, 1 ops)
ERROR [CompactionExecutor:70] 2014-02-24 08:34:43,114 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:70,1,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61)
at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)
at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
at java.util.concurrent.AbstractExecutorService.submit(Unknown Source)
at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:796)
at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:861)
at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:435)
at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:202)
at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
at java.util.concurrent.FutureTask.run(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
On restart, there was an FSWriteError in deleteWithConfirm:
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,196 ColumnFamilyStore.java (line 254) Initializing testdb_1393204279041.sr
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,199 ColumnFamilyStore.java (line 254) Initializing testdb_1393204279041.etol
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,203 ColumnFamilyStore.java (line 254) Initializing testdb_1393206475253.sr
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,206 ColumnFamilyStore.java (line 254) Initializing testdb_1393206475253.etol
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,209 ColumnFamilyStore.java (line 254) Initializing testdb_1393206779625.sr
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,212 ColumnFamilyStore.java (line 254) Initializing testdb_1393206779625.etol
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,215 ColumnFamilyStore.java (line 254) Initializing OpsCenter.pdps
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,218 ColumnFamilyStore.java (line 254) Initializing OpsCenter.rollups86400
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,223 ColumnFamilyStore.java (line 254) Initializing OpsCenter.rollups7200
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,241 ColumnFamilyStore.java (line 254) Initializing OpsCenter.events
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,246 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-28 (150 bytes)
INFO [SSTableBatchOpen:2] 2014-02-24 09:27:27,247 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-26 (186 bytes)
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,295 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-31 (149 bytes)
INFO [SSTableBatchOpen:2] 2014-02-24 09:27:27,314 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-25 (2239 bytes)
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,330 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-27 (149 bytes)
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,360 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-29 (186 bytes)
INFO [SSTableBatchOpen:2] 2014-02-24 09:27:27,383 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-32 (184 bytes)
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,403 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events\OpsCenter-events-jb-30 (149 bytes)
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,427 ColumnFamilyStore.java (line 254) Initializing OpsCenter.rollups300
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,434 ColumnFamilyStore.java (line 254) Initializing OpsCenter.events_timeline
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,437 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events_timeline\OpsCenter-events_timeline-jb-39 (47 bytes)
INFO [SSTableBatchOpen:2] 2014-02-24 09:27:27,438 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\events_timeline\OpsCenter-events_timeline-jb-38 (706 bytes)
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,476 ColumnFamilyStore.java (line 254) Initializing OpsCenter.settings
INFO [SSTableBatchOpen:1] 2014-02-24 09:27:27,478 SSTableReader.java (line 223) Opening C:\Program Files\DataStax Community\data\data\OpsCenter\settings\OpsCenter-settings-jb-25 (232 bytes)
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,522 ColumnFamilyStore.java (line 254) Initializing OpsCenter.rollups60
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,526 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
INFO [CompactionExecutor:1] 2014-02-24 09:27:27,529 ColumnFamilyStore.java (line 254) Initializing system_traces.events
INFO [CompactionExecutor:1] 2014-02-24 09:27:29,930 CompactionTask.java (line 275) Compacted 32 sstables to [C:\Program Files\DataStax Community\data\data\system\compactions_in_progress\system-compactions_in_progress-jb-596,]. 2,522 bytes to 322 (~12% of original) in 21,049ms = 0.000015MB/s. 32 total partitions merged to 15. Partition merge counts were
{1:14, 2:9, }
INFO [CompactionExecutor:2] 2014-02-24 09:27:29,958 CompactionTask.java (line 275) Compacted 16 sstables to [C:\Program Files\DataStax Community\data\data\system\compactions_in_progress\system-compactions_in_progress-jb-597,]. 3,550 bytes to 1,832 (~51% of original) in 19,577ms = 0.000089MB/s. 16 total partitions merged to 14. Partition merge counts were
{1:14, 2:1, }
INFO [main] 2014-02-24 09:27:29,972 ColumnFamilyStore.java (line 784) Enqueuing flush of Memtable-local@2117211498(114/1140 serialized/live bytes, 4 ops)
INFO [FlushWriter:2] 2014-02-24 09:27:29,972 Memtable.java (line 333) Writing Memtable-local@2117211498(114/1140 serialized/live bytes, 4 ops)
INFO [FlushWriter:2] 2014-02-24 09:27:30,365 Memtable.java (line 373) Completed flushing C:\Program Files\DataStax Community\data\data\system\local\system-local-jb-631-Data.db (150 bytes) for commitlog position ReplayPosition(segmentId=1393255627523, position=1437)
INFO [CompactionExecutor:1] 2014-02-24 09:27:30,366 CompactionTask.java (line 115) Compacting [SSTableReader(path='C:\Program Files\DataStax Community\data\data\system\local\system-local-jb-628-Data.db'), SSTableReader(path='C:\Program Files\DataStax Community\data\data\system\local\system-local-jb-629-Data.db'), SSTableReader(path='C:\Program Files\DataStax Community\data\data\system\local\system-local-jb-631-Data.db'), SSTableReader(path='C:\Program Files\DataStax Community\data\data\system\local\system-local-jb-630-Data.db')]
ERROR [main] 2014-02-24 09:27:30,512 CassandraDaemon.java (line 479) Exception encountered during startup
FSWriteError in C:\Program Files\DataStax Community\data\data\system\local\system-local-tmp-jb-632-Data.db
at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:106)
at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:139)
at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:463)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
Caused by: java.nio.file.FileSystemException: C:\Program Files\DataStax Community\data\data\system\local\system-local-tmp-jb-632-Data.db: The process cannot access the file because it is being used by another process.
at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
at java.nio.file.Files.delete(Unknown Source)
at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
... 6 more
_________________
Note: I've reproduced this while working on cleaning up ccm on Windows.",Windows,"['Local/Compaction', 'Local/Startup and Shutdown']",CASSANDRA,Bug,Low,2014-03-03 15:54:45,6
12696037,A batch statements on a single partition should not create a new CF object for each update,"BatchStatement creates a new ColumnFamily object (as well as a new RowMutation object) for every update in the batch, even if all those update are actually on the same partition. This is particularly inefficient when bulkloading data into a single partition (which is not all that uncommon).",performance,[],CASSANDRA,Improvement,Normal,2014-02-19 18:23:51,2
12696033,Windows7 AccessDeniedException on commit log ,"Similar to the data file deletion of CASSANDRA-6283, under heavy load with logged batches, I am seeing a problem where the Commit log cannot be deleted:
 ERROR [COMMIT-LOG-ALLOCATOR] 2014-02-18 22:15:58,252 CassandraDaemon.java (line 192) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
 FSWriteError in C:\Program Files\DataStax Community\data\commitlog\CommitLog-3-1392761510706.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.discard(CommitLogSegment.java:150)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$4.run(CommitLogAllocator.java:217)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.AccessDeniedException: C:\Program Files\DataStax Community\data\commitlog\CommitLog-3-1392761510706.log
	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 5 more
(Attached in 2014-02-18-22-16.log is a larger excerpt from the cassandra.log.)

In this particular case, I was trying to do 100 million inserts into two tables in parallel, one with a single wide row and one with narrow rows, and the error appeared after inserting 43,151,232 rows.  So it does take a while to trip over this timing issue.  

It may be aggravated by the size of the batches. This test was writing 10,000 rows to each table in a batch.  

When I try switching the same test from using a logged batch to an unlogged batch, and no such failure appears. So the issue could be related to the use of large, logged batches, or it could be that unlogged batches just change the probability of failure.  


",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Normal,2014-02-19 18:16:01,6
12695739,redesign loadnewsstables,"CFSMBean.loadNewSSTables scans data directories for new sstables dropped there by an external agent.  This is dangerous because of possible filename conflicts with existing or newly generated sstables.

Instead, we should support leaving the new sstables in a separate directory (specified by a parameter, or configured as a new location in yaml) and take care of renaming as necessary automagically.",lhf,['Legacy/Tools'],CASSANDRA,New Feature,Low,2014-02-18 17:45:31,0
12695662,Modernize schema tables,"There is a few problems/improvements that can be done with the way we store schema:
# CASSANDRA-4988: as explained on the ticket, storing the comparator is now redundant (or almost, we'd need to store whether the table is COMPACT or not too, which we don't currently is easy and probably a good idea anyway), it can be entirely reconstructed from the infos in schema_columns (the same is true of key_validator and subcomparator, and replacing default_validator by a COMPACT_VALUE column in all case is relatively simple). And storing the comparator as an opaque string broke concurrent updates of sub-part of said comparator (concurrent collection addition or altering 2 separate clustering columns typically) so it's really worth removing it.
# CASSANDRA-4603: it's time to get rid of those ugly json maps. I'll note that schema_keyspaces is a problem due to its use of COMPACT STORAGE, but I think we should fix it once and for-all nonetheless (see below).
# For CASSANDRA-6382 and to allow indexing both map keys and values at the same time, we'd need to be able to have more than one index definition for a given column.
# There is a few mismatches in table options between the one stored in the schema and the one used when declaring/altering a table which would be nice to fix. The compaction, compression and replication maps are one already mentioned from CASSANDRA-4603, but also for some reason 'dclocal_read_repair_chance' in CQL is called just 'local_read_repair_chance' in the schema table, and 'min/max_compaction_threshold' are column families option in the schema but just compaction options for CQL (which makes more sense).

None of those issues are major, and we could probably deal with them independently but it might be simpler to just fix them all in one shot so I wanted to sum them all up here. In particular, the fact that 'schema_keyspaces' uses COMPACT STORAGE is annoying (for the replication map, but it may limit future stuff too) which suggest we should migrate it to a new, non COMPACT table. And while that's arguably a detail, it wouldn't hurt to rename schema_columnfamilies to schema_tables for the years to come since that's the prefered vernacular for CQL.

Overall, what I would suggest is to move all schema tables to a new keyspace, named 'schema' for instance (or 'system_schema' but I prefer the shorter version), and fix all the issues above at once. Since we currently don't exchange schema between nodes of different versions, all we'd need to do that is a one shot startup migration, and overall, I think it could be simpler for clients to deal with one clear migration than to have to handle minor individual changes all over the place. I also think it's somewhat cleaner conceptually to have schema tables in their own keyspace since they are replicated through a different mechanism than other system tables.

If we do that, we could, for instance, migrate to the following schema tables (details up for discussion of course):
{noformat}
CREATE TYPE user_type (
  name text,
  column_names list<text>,
  column_types list<text>
)

CREATE TABLE keyspaces (
  name text PRIMARY KEY,
  durable_writes boolean,
  replication map<string, string>,
  user_types map<string, user_type>
)

CREATE TYPE trigger_definition (
  name text,
  options map<tex, text>
)

CREATE TABLE tables (
  keyspace text,
  name text,
  id uuid,
  table_type text, // COMPACT, CQL or SUPER
  dropped_columns map<text, bigint>,
  triggers map<text, trigger_definition>,

  // options
  comment text,
  compaction map<text, text>,
  compression map<text, text>,
  read_repair_chance double,
  dclocal_read_repair_chance double,
  gc_grace_seconds int,
  caching text,
  rows_per_partition_to_cache text,
  default_time_to_live int,
  min_index_interval int,
  max_index_interval int,
  speculative_retry text,
  populate_io_cache_on_flush boolean,
  bloom_filter_fp_chance double
  memtable_flush_period_in_ms int,

  PRIMARY KEY (keyspace, name)
)

CREATE TYPE index_definition (
  name text,
  index_type text,
  options map<text, text>
)

CREATE TABLE columns (
  keyspace text,
  table text,
  name text,
  kind text, // PARTITION_KEY, CLUSTERING_COLUMN, REGULAR or COMPACT_VALUE
  component_index int;
  type text,
  indexes map<text, index_definition>,

  PRIMARY KEY (keyspace, table, name)
)
{noformat}

Nit: wouldn't hurt to create a simple enum that is reuse by both CFMetaData and CFPropDefs for table options names while we're at it once they are the same instead of repeating string constants which is fragile.",doc-impacting,[],CASSANDRA,Sub-task,Normal,2014-02-18 10:20:46,1
12695522,Fix replaying old (1.2) commitlog in Cassandra 2.0,"Our docs, and code, both explicitly say that you should drain a node before upgrading to a new major release.

If you don't do what the docs explicitly tell you to do, however, Cassandra won't scream at you. Also, we *do* currently have logic to replay 1.2 commitlog in 2.0, but it seems to be slightly broken, unfortunately.",qa-resolved,[],CASSANDRA,Bug,Normal,2014-02-17 14:08:54,1
12694767,Partition sstables by token range,"In JBOD, when someone gets a bad drive, the bad drive is replaced with a new empty one and repair is run. 
This can cause deleted data to come back in some cases. Also this is true for corrupt stables in which we delete the corrupt stable and run repair. 
Here is an example:
Say we have 3 nodes A,B and C and RF=3 and GC grace=10days. 
row=sankalp col=sankalp is written 20 days back and successfully went to all three nodes. 
Then a delete/tombstone was written successfully for the same row column 15 days back. 
Since this tombstone is more than gc grace, it got compacted in Nodes A and B since it got compacted with the actual data. So there is no trace of this row column in node A and B.
Now in node C, say the original data is in drive1 and tombstone is in drive2. Compaction has not yet reclaimed the data and tombstone.  
Drive2 becomes corrupt and was replaced with new empty drive. 
Due to the replacement, the tombstone in now gone and row=sankalp col=sankalp has come back to life. 
Now after replacing the drive we run repair. This data will be propagated to all nodes. 

Note: This is still a problem even if we run repair every gc grace. 
 ",compaction correctness dense-storage doc-impacting jbod-aware-compaction lcs performance,['Local/Compaction'],CASSANDRA,Improvement,Normal,2014-02-12 16:17:47,0
12694431,Avoid possible sstable overlaps with leveled compaction,"Two cases where we can end up with overlapping sstables in the leveled manifest;

FIrst one is when we skip levels during compaction. Here we need to make sure we are not compacting in newLevel - 1 since if, for example, we are doing a L1 -> L2 compaction and then start a new L0 compaction where we decide to skip L1, we could have overlapping sstables in L2 when the compactions are done. This case is new in 2.0 since we check if we skip levels before the compaction starts.

Second case is where we try to include as many overlapping L0 sstables as possible, here we could add sstables that are not compacting, but overlap sstables that are.",lcs,[],CASSANDRA,Bug,Normal,2014-02-11 06:31:28,0
12693358,Droppable tombstones are not being removed from LCS table despite being above 20%,"JMX is showing that one of our CQL3 LCS tables has a droppable tombstone ratio above 20% and increasing (currently at 28%).  Compactions are not falling behind and we are using the OOTB setting for this feature so I would expect not to go above 20% (will attach screen shot from JMX).   Table description:

CREATE TABLE global_user (
  user_id timeuuid,
  app_id int,
  type text,
  name text,
  extra_param map<text, text>,
  last timestamp,
  paid boolean,
  sku_time map<text, timestamp>,
  values map<timestamp, float>,
  PRIMARY KEY (user_id, app_id, type, name)
) WITH
  bloom_filter_fp_chance=0.100000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=86400 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'sstable_size_in_mb': '160', 'class': 'LeveledCompactionStrategy'} AND
  compression={'chunk_length_kb': '8', 'crc_check_chance': '0.1', 'sstable_compression': 'LZ4Compressor'}; ",qa-resolved,[],CASSANDRA,Bug,Normal,2014-02-05 14:42:04,0
12691271,Add flag to disable STCS in L0,"The initial discussion started in (closed) CASSANDRA-5371. I've rewritten my last comment here...

After streaming (e.g. during boostrap) Cassandra places all sstables at L0. At the end of the process we end up with huge number of sstables at the lowest level. 

Currently, Cassandra falls back to STCS until the number of sstables at L0 reaches the reasonable level (32 or something).

I'm not sure if falling back to STCS is the best way to handle this particular situation. I've read the comment in the code and I'm aware why it is a good thing to do if we have to many sstables at L0 as a result of too many random inserts. We have a lot of sstables, each of them covers the whole ring, there's simply no better option.

However, after the bootstrap situation looks a bit different. The loaded sstables already have very small ranges! We just have to tidy up a bit and everything should be OK. STCS ignores that completely and after a while we have a bit less sstables but each of them covers the whole ring instead of just a small part. I believe that in that case letting LCS do the job is a better option that allowing STCS mix everything up before.

Is there a way to disable STCS fallback? I'd like to test that scenario in practice during our next bootstrap...

Does Cassandra really have to put streamed sstables at L0? The only thing we have to assure is that sstables at any given level do not overlap. If we stream different regions from different nodes how can we get any overlaps?

",compaction lcs streaming,[],CASSANDRA,Improvement,Low,2014-01-26 16:23:30,0
12689250,Avoid special casing received/expected counts in CAS timeout exceptions,"For CAS, we send a few timeouts with -1 as received and expected counts, but it's kind of confusing from a user perspective (meaning that it could easily break user code that check those numbers just because they didn't expected a negative number). Suggesting to return 0 for received and whatever we block for for the CL involved for required instead.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Improvement,Low,2014-01-16 14:33:33,2
12688424,ConcurrentModificationException during nodetool netstats,"The node is leaving and I wanted to check its netstats, but it raises ConcurrentModificationException.

{code}
[ubuntu@ip-10-4-202-48 :~]# /mnt/cassandra_latest/bin/nodetool netstats
Mode: LEAVING
Exception in thread ""main"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
	at java.util.HashMap$ValueIterator.next(HashMap.java:954)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators.addAll(Iterators.java:357)
	at com.google.common.collect.Lists.newArrayList(Lists.java:146)
	at com.google.common.collect.Lists.newArrayList(Lists.java:128)
	at org.apache.cassandra.streaming.management.SessionInfoCompositeData.toArrayOfCompositeData(SessionInfoCompositeData.java:161)
	at org.apache.cassandra.streaming.management.SessionInfoCompositeData.toCompositeData(SessionInfoCompositeData.java:98)
	at org.apache.cassandra.streaming.management.StreamStateCompositeData$1.apply(StreamStateCompositeData.java:82)
	at org.apache.cassandra.streaming.management.StreamStateCompositeData$1.apply(StreamStateCompositeData.java:79)
	at com.google.common.collect.Iterators$8.transform(Iterators.java:794)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators.addAll(Iterators.java:357)
	at com.google.common.collect.Lists.newArrayList(Lists.java:146)
	at com.google.common.collect.Lists.newArrayList(Lists.java:128)
	at org.apache.cassandra.streaming.management.StreamStateCompositeData.toCompositeData(StreamStateCompositeData.java:78)
	at org.apache.cassandra.streaming.StreamManager$1.apply(StreamManager.java:87)
	at org.apache.cassandra.streaming.StreamManager$1.apply(StreamManager.java:84)
	at com.google.common.collect.Iterators$8.transform(Iterators.java:794)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators.addAll(Iterators.java:357)
	at com.google.common.collect.Sets.newHashSet(Sets.java:238)
	at com.google.common.collect.Sets.newHashSet(Sets.java:218)
	at org.apache.cassandra.streaming.StreamManager.getCurrentStreams(StreamManager.java:83)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{code}

The gossip info is as follows:
{code}
/10.215.114.239
  SCHEMA:e2a4b7b2-df50-3cc4-97f6-2ce7fb77982b
  NET_VERSION:7
  LOAD:6.42655250606E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1305722878370288637
  HOST_ID:d13374d8-e4ae-466d-ad5a-44229a2fa190
  DC:pagedb-frontend
  SEVERITY:0.0
/10.122.218.80
  SCHEMA:548308e6-bd1c-388f-ad1e-5ecf39f994fd
  NET_VERSION:7
  LOAD:2.71405175303E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1796000685025988745
  HOST_ID:7746a875-1e53-4ebe-aef6-ad59fadd6ea7
  DC:pagedb-frontend
  SEVERITY:0.0
/10.178.13.230
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.34997966591E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1646882803236172485
  HOST_ID:160d83bf-4cc0-4872-aec6-7908446eccbf
  DC:pagedb-backend
  SEVERITY:1.5584415197372437
/10.71.141.179
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.21492186385E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1277850891915823266
  HOST_ID:baa81888-6417-4e7c-8d7b-2bb79c38ca40
  DC:pagedb-frontend
  SEVERITY:0.0
/10.251.114.31
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.95280061394E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1b
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1866613386904756042
  HOST_ID:a71e7037-c4f8-4ff9-864d-e83b6d3037d7
  DC:pagedb-frontend
  SEVERITY:0.0
/10.95.128.214
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.90628204861E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1896996972976095280
  HOST_ID:3e5e6e75-56cb-4664-abf4-de8c2801bd5d
  DC:pagedb-backend
  SEVERITY:0.0
/10.228.26.240
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.46469437383E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1359255731430490566
  HOST_ID:8df816c9-7875-4106-aeda-b372b9f1fdc9
  DC:pagedb-frontend
  SEVERITY:1.5037593841552734
/10.185.67.195
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:4.6531989096E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1295072457639851651
  HOST_ID:37fe7cc7-3481-48b3-96ff-c92df17a4132
  DC:pagedb-frontend
  SEVERITY:0.0
/10.220.195.198
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:7.8265401144E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1514821029440891529
  HOST_ID:0bd7f3ab-d347-4c59-9f1a-1b7104e34a6b
  DC:pagedb-frontend
  SEVERITY:0.0
/10.62.39.130
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.7020827919E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1089815038805941976
  HOST_ID:13b12ee3-36a4-462c-9013-ccc1b83a70ff
  DC:pagedb-frontend
  SEVERITY:0.9411764144897461
/10.137.11.197
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.17329728527E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1708596056219631840
  HOST_ID:76d339f4-ac4a-4f95-bada-1ec17f67f4e3
  DC:pagedb-backend
  SEVERITY:0.5555555820465088
/10.87.145.85
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:5.78714900637E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-2223601356751123985
  HOST_ID:bdad1b28-3fb7-4788-8f34-01caace03ca9
  DC:pagedb-frontend
  SEVERITY:0.0
/10.97.135.36
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:9.3081789914E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1146411778648768253
  HOST_ID:40fb13d6-0803-45cc-9b4f-45b3ad7194fa
  DC:pagedb-frontend
  SEVERITY:0.0
/10.95.128.6
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.56500409995E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1025262993714352739
  HOST_ID:80b583a5-8f7d-4fee-91db-b948c090d055
  DC:pagedb-backend
  SEVERITY:0.7518796920776367
/10.154.136.39
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.70485777643E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:0e391fea-e4e9-4a46-b9af-87948459876c
  STATUS:LEAVING,-2690692580263318876
  DC:pagedb-backend
  SEVERITY:0.0
/10.185.9.84
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.87054930947E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1468213189211385280
  HOST_ID:080241a9-eadd-48b4-8f94-efbe35bfd6e1
  DC:pagedb-backend
  SEVERITY:0.2557544708251953
/10.251.114.15
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.86231038729E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1b
  RPC_ADDRESS:0.0.0.0
  HOST_ID:99e3e672-8410-4850-8257-edc7b814a776
  STATUS:NORMAL,-1117031336072704821
  DC:pagedb-frontend
  SEVERITY:0.0
/10.97.135.32
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.1249908096E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1176341277720804881
  HOST_ID:e9ac8ad9-c3cc-44ff-81e7-af28a4d5f576
  DC:pagedb-backend
  SEVERITY:0.0
/10.99.144.60
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:8.9296442414E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1695890443918261320
  HOST_ID:01b323c1-3f0e-40de-8ef7-3e1d33b391c5
  DC:pagedb-frontend
  SEVERITY:0.0
/10.121.13.216
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.70269351421E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1081159627867819835
  HOST_ID:05f06b6f-ddb7-42a9-9e56-8efd41cf1545
  DC:pagedb-frontend
  SEVERITY:0.0
/10.99.144.97
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.95197366549E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1188304416367949186
  HOST_ID:7a84853a-2c0a-448c-84e2-cceaa98c7523
  DC:pagedb-backend
  SEVERITY:0.26178011298179626
/10.95.132.5
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:9.8325409989E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1168449340133292844
  HOST_ID:6480a3a2-cbbe-44f4-b67b-7310f885b307
  DC:pagedb-frontend
  SEVERITY:0.0
/10.178.2.177
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.8547129498E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1337943409437095727
  HOST_ID:aa7fd551-8a47-4e59-869e-ae0afaadf27e
  DC:pagedb-frontend
  SEVERITY:0.0
/10.201.206.166
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.02276920825E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-2302424866842171501
  HOST_ID:f9651587-8f34-4dbc-af5c-64f19bc85ad6
  DC:pagedb-frontend
  SEVERITY:0.0
/10.93.6.87
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:5.55645040614E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1767037199861003446
  HOST_ID:2ca596b1-fa0a-40a8-ace1-bc7b21e82c45
  DC:pagedb-frontend
  SEVERITY:1.0230178833007812
/10.123.74.248
  SCHEMA:b2c3e6ec-3b3f-3ad9-baef-e9e311a24eb9
  NET_VERSION:7
  LOAD:3.09054824961E11
  REMOVAL_COORDINATOR:REMOVER,76d339f4-ac4a-4f95-bada-1ec17f67f4e3
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:removed,94daf57d-838d-4285-8048-e60f97629dba,1389650189822
  HOST_ID:94daf57d-838d-4285-8048-e60f97629dba
  DC:pagedb-backend
  SEVERITY:0.2493765652179718
/10.71.141.12
  SCHEMA:26cf5f29-f7f9-321d-b5c4-ac9314d90653
  NET_VERSION:7
  LOAD:1.91108857556E11
  REMOVAL_COORDINATOR:REMOVER,c7a0edfd-629e-47c3-b79c-32a9c2c98801
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:removed,4b9f63e5-8561-4e59-8011-e11fb3e8d627,1389562451886
  HOST_ID:4b9f63e5-8561-4e59-8011-e11fb3e8d627
  DC:pagedb-frontend
  SEVERITY:0.0
/10.185.0.80
  SCHEMA:039db936-c2e7-3f53-9c94-efdf2b861d53
  NET_VERSION:7
  LOAD:2.22049506986E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:removed,9885866e-af4d-4129-ad33-d465c8a7cd58,1389732020477
  HOST_ID:9885866e-af4d-4129-ad33-d465c8a7cd58
  DC:pagedb-backend
  SEVERITY:2.7707808017730713
/10.185.47.50
  SCHEMA:b13a987f-adbe-3c19-afc9-419e595d9002
  NET_VERSION:7
  LOAD:2.19790952307E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:73c27c92-1f3e-4628-9c91-a1e0a03a9e21
  STATUS:removed,73c27c92-1f3e-4628-9c91-a1e0a03a9e21,1389731586723
  DC:pagedb-backend
  SEVERITY:6.6137566566467285
/10.93.49.164
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.03359005906E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1102475930390387446
  HOST_ID:19f401cb-b418-4ce3-b0e0-ea1530ab2121
  DC:pagedb-frontend
  SEVERITY:0.0
/10.206.97.184
  SCHEMA:ede9498d-befd-3153-87ee-2ea329001466
  NET_VERSION:7
  LOAD:2.34212215287E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:8d3d9e79-5a72-4561-99f0-0817cd6ad6cc
  STATUS:LEAVING,-1397201050646007911
  DC:pagedb-backend
  SEVERITY:2.236421823501587
/10.71.141.42
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:6.0594679857E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1008058669507434673
  HOST_ID:2113b135-8566-4ed9-b74b-5e302d240c42
  DC:pagedb-frontend
  SEVERITY:0.0
/10.154.148.5
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:4.91232885735E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1261154574760184756
  HOST_ID:86642aef-f625-4561-be7b-ab8f58066294
  DC:pagedb-frontend
  SEVERITY:0.7672634720802307
/10.183.153.179
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.17986161259E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1486604450347152783
  HOST_ID:5988d85d-663c-4597-9422-7f5bc195ec5b
  DC:pagedb-frontend
  SEVERITY:0.0
/10.97.131.247
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.43618302855E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1087382214101550882
  HOST_ID:811a2966-b6f0-41fd-ba29-fc980763a69c
  DC:pagedb-backend
  SEVERITY:0.0
/10.99.144.54
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:8.9936707476E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-3327961952466683302
  HOST_ID:675e8de7-641f-4914-beeb-fad29942381f
  DC:pagedb-frontend
  SEVERITY:0.0
/10.210.91.83
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.19028851441E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1375725999837634215
  HOST_ID:b0726b39-1587-4443-853e-6e668658b1cc
  DC:pagedb-frontend
  SEVERITY:17.39130401611328
/10.71.141.158
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.11308980882E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1306761067597910068
  HOST_ID:31c7690b-30fa-455c-9f69-e98f11d4c235
  DC:pagedb-backend
  SEVERITY:0.0
/10.38.169.32
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.3296808983E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-137637605179813169
  HOST_ID:67ec4857-c867-46c8-9c62-778fd26360eb
  DC:pagedb-backend
  SEVERITY:1.671309232711792
/10.87.94.230
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.52091238152E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1519665921822534263
  HOST_ID:604b0268-187a-4a5f-a51e-b94608111825
  DC:pagedb-backend
  SEVERITY:0.27397260069847107
/10.99.144.91
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.96583444544E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-3103769032004455685
  HOST_ID:010613b8-9d0b-487d-bdd1-64e80ddc60e6
  DC:pagedb-backend
  SEVERITY:1.0204081535339355
/10.71.141.130
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.45733317443E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1099965132011727160
  HOST_ID:3c30b5fb-bf8c-4169-a60f-6ffad2c28598
  DC:pagedb-frontend
  SEVERITY:0.0
/10.99.146.244
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.6041541835E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1404826323347485057
  HOST_ID:6a96c2e0-6376-461d-bac5-0dcbe3cd1eb5
  DC:pagedb-backend
  SEVERITY:0.0
/10.95.129.124
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.61026008599E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-2498420976414553971
  HOST_ID:2a987ea1-4037-4b9e-a403-27a6d7995621
  DC:pagedb-frontend
  SEVERITY:0.0
/10.122.50.31
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.18914643786E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:b9e40521-bf2e-4833-8f6a-a8e3297df042
  STATUS:NORMAL,-1277185879546496035
  DC:pagedb-backend
  SEVERITY:0.0
/10.4.197.53
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.15526897917E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1388643925813439514
  HOST_ID:9f76061b-43a7-432b-9e6d-612a628534cf
  DC:pagedb-backend
  SEVERITY:0.7537688612937927
/10.44.183.111
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.10331330719E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-283520605333269313
  HOST_ID:1e0e3870-5026-452b-8050-dad7fcd5bd88
  DC:pagedb-backend
  SEVERITY:7.416880130767822
/10.238.138.32
  SCHEMA:b13a987f-adbe-3c19-afc9-419e595d9002
  NET_VERSION:7
  REMOVAL_COORDINATOR:REMOVER,3c30b5fb-bf8c-4169-a60f-6ffad2c28598
  LOAD:1.92772802904E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:e225c748-ffd8-4905-91bf-4e745b26fa44
  STATUS:removed,e225c748-ffd8-4905-91bf-4e745b26fa44,1389731792774
  DC:pagedb-backend
  SEVERITY:0.0
/10.95.129.103
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.50386165884E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1389777754931898083
  HOST_ID:18031026-e20c-40eb-8bb3-d771b641fb26
  DC:pagedb-frontend
  SEVERITY:0.0
/10.125.10.14
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:7.2903902818E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1822845329295703853
  HOST_ID:5764492f-31f8-4c3f-8bec-d8a2c1af4b4d
  DC:pagedb-frontend
  SEVERITY:0.0
/10.4.202.48
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.05065515518E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:76f98456-2029-4bb6-b3de-e08157220fc9
  STATUS:LEAVING,-2184837513740789816
  DC:pagedb-backend
  SEVERITY:28.76344108581543
/10.95.132.44
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.97019107429E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-243162178282959980
  HOST_ID:c7a0edfd-629e-47c3-b79c-32a9c2c98801
  DC:pagedb-backend
  SEVERITY:0.5076141953468323
/10.96.31.80
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.29408976299E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1236217677726412210
  HOST_ID:fa3ea99e-f9b6-4f83-9234-21d8059e6eb4
  DC:pagedb-frontend
  SEVERITY:0.0
/10.40.245.193
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.27656537169E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1237220198988914368
  HOST_ID:d2383010-fd55-4364-8868-5dc8a937778f
  DC:pagedb-frontend
  SEVERITY:0.0
/10.99.144.75
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:8.2445952994E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1022810197871922800
  HOST_ID:c085c62d-74ea-45e5-9ed6-6a26a72ec3bb
  DC:pagedb-frontend
  SEVERITY:0.0
{code}",decommission nodetool,['Tool/nodetool'],CASSANDRA,Bug,Low,2014-01-12 03:53:00,6
12686569,Failure to start after unclean shutdown - java.lang.IllegalArgumentException: bufferSize must be positive,"We had a severe power outage in the lab that resulted in unclean shutdown of the Cassandra servers. After the power was back I tried to start the cluster. Two out of 6 nodes cannot start because of this exception:

{code}
 INFO 20:47:11,003 Initializing system.local
 INFO [main] 2013-12-27 20:47:11,003 ColumnFamilyStore.java (line 251) Initializing system.local
 INFO 20:47:11,006 Opening /hadoop/disk1/cassandra/data/system/local/system-local-jb-2478 (5836 bytes)
 INFO [SSTableBatchOpen:1] 2013-12-27 20:47:11,006 SSTableReader.java (line 223) Opening /hadoop/disk1/cassandra/data/system/local/system-local-jb-2478 (5836 bytes)
 INFO 20:47:11,006 Opening /hadoop/disk4/cassandra/data/system/local/system-local-jb-2479 (144 bytes)
 INFO [SSTableBatchOpen:2] 2013-12-27 20:47:11,006 SSTableReader.java (line 223) Opening /hadoop/disk4/cassandra/data/system/local/system-local-jb-2479 (144 bytes)
ERROR 20:47:12,366 Exception encountered during startup
java.lang.IllegalArgumentException: bufferSize must be positive
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1363)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
        at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
        at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1526)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1645)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:137)
        at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:236)
        at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:1)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:255)
        at org.apache.cassandra.db.SystemKeyspace.getUnfinishedCompactions(SystemKeyspace.java:206)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:261)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
{code}

Collecting the logs now, will attach to the issue in a moment. ",compression,[],CASSANDRA,Bug,Normal,2013-12-27 20:53:20,7
12685496,counters++,"Continuing CASSANDRA-4775 here.

We are changing counter write path to explicitly lock-read-modify-unlock-replicate, thus getting rid of the previously used 'local' (deltas) and 'remote' shards distinction. Unfortunately, we can't simply start using 'remote' shards exclusively, since shard merge rules prioritize the 'local' shards. Which is why we are introducing the third shard type - 'global', the only shard type to be used in 2.1+.

The updated merge rules are going to look like this:

global + global = keep the shard with the highest logical clock
global + local or remote = keep the global one
local + local = sum counts (and logical clock)
local + remote = keep the local one
remote + remote = keep the shard with highest logical clock

This is required for backward compatibility with pre-2.1 counters. To make 2.0-2.1 live upgrade possible, 'global' shard merge logic will have to be back ported to 2.0. 2.0 will not produce them, but will be able to understand the global shards coming from the 2.1 nodes during the live upgrade. See CASSANDRA-6505.

Other changes introduced in this issue:

1. replicate_on_write is gone. From now on we only avoid replication at RF 1.
2. REPLICATE_ON_WRITE stage is gone
3. counter mutations are running in their own COUNTER_MUTATION stage now
4. counter mutations have a separate counter_write_request_timeout setting
5. mergeAndRemoveOldShards() code is gone, for now, until/unless a better solution is found
6. we only replicate the fresh global shard now, not the complete (potentially quite large) counter context
7. to help with concurrency and reduce lock contention, we cache node's global shards in a new counter cache ({cf id, partition key, cell name} -> {count, clock}). The cache is only used by counter writes, to help with 'hot' counters being simultaneously updated.

Improvements to be handled by separate JIRA issues:

1. Split counter context into separate cells - one shard per cell. See CASSANDRA-6506. This goes into either 2.1 or 3.0.

Potential improvements still being debated:

1. Coalesce the mutations in COUNTER_MUTATION stage if they share the same partition key, and apply them together, to improve the locking situation when updating different counter cells in one partition. See CASSANDRA-6508. Will to into 2.1 or 3.0, if deemed beneficial.
",counters,[],CASSANDRA,Improvement,Normal,2013-12-19 02:05:12,1
12685087,Endless L0 LCS compactions,"I have first described the problem here: http://stackoverflow.com/questions/20589324/cassandra-2-0-3-endless-compactions-with-no-traffic

I think I have really abused my system with the traffic (mix of reads, heavy updates and some deletes). Now after stopping the traffic I see the compactions that are going on endlessly for over 4 days.

For a specific CF I have about 4700 sstable data files right now.  The compaction estimates are logged as ""[3312, 4, 0, 0, 0, 0, 0, 0, 0]"". sstable_size_in_mb=256.  3214 files are about 256Mb (+/1 few megs), other files are smaller or much smaller than that. No sstables are larger than 256Mb. What I observe is that LCS picks 32 sstables from L0 and compacts them into 32 sstables of approximately the same size. So, what my system is doing for last 4 days (no traffic at all) is compacting groups of 32 sstables into groups of 32 sstables without any changes. Seems like a bug to me regardless of what did I do to get the system into this state...
",compaction lcs,[],CASSANDRA,Bug,Normal,2013-12-17 03:00:55,7
12684425,cassandra-shuffle not working with authentication,"When enabling authentication for a cassandra cluster the tool cassandra-shuffle is unable to connect.

The reason is, that cassandra-shuffle doesn't take any parameter for username and password for the thrift connection.

To solve that problem, parameter for username and password should be added, It should also be able to interpret cqlshrc or a separate file file with authentication data.",lhf,['Legacy/Tools'],CASSANDRA,Improvement,Low,2013-12-13 08:53:34,1
12684297,Custom secondary index options in CQL3,"The CQL3 ""create index"" statement syntax does not allow to specify the options map internally used by custom indexes. ",cql3 index,['Feature/2i Index'],CASSANDRA,Improvement,Low,2013-12-12 16:33:29,1
12683593,cleanup ClassCastException,"I enlarged the cluseter from 4 to 8 nodes. During cleaning up the ""old"" nodes with ""nodetool cleanup"" it breaks up with exception. I started cleanup from a different computer to manage them sequentially.
{panel:title=cmd.exe}
Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.ClassCastException: org.apach
e.cassandra.io.sstable.SSTableReader$EmptyCompactionScanner cannot be cast to or
g.apache.cassandra.io.sstable.SSTableScanner
        at java.util.concurrent.FutureTask.report(Unknown Source)
        at java.util.concurrent.FutureTask.get(Unknown Source)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTabl
eOperation(CompactionManager.java:227)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(C
ompactionManager.java:265)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilySt
ore.java:1054)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(Stor
ageService.java:2038)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.reflect.misc.Trampoline.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.reflect.misc.MethodUtil.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown So
urce)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown So
urce)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown
Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Sou
rce)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(Unknown Sour
ce)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run
(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(U
nknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Sou
rce)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Sour
ce)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: org.apache.cassandra.io.sstable.SSTable
Reader$EmptyCompactionScanner cannot be cast to org.apache.cassandra.io.sstable.
SSTableScanner
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompact
ion(CompactionManager.java:563)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(Compa
ctionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(Compac
tionManager.java:274)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(Compactio
nManager.java:222)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        ... 3 more
{panel}",cleanup compaction,['Legacy/Tools'],CASSANDRA,Bug,Normal,2013-12-09 12:24:02,7
12683140,nodetool getendpoints doesn't validate key arity,"I have a complex row key.

$ create table b (x int, s text, ((x,s)) primary key);

In cqlsh I cannot fill row key partially:

{noformat}
$ insert into b (x) values(4);
Bad Request: Missing mandatory PRIMARY KEY part s
{noformat}

But nodetool can find hosts by incomplete key
{noformat}
$ nodetool -h cas3 getendpoints anti_portal b 12
192.168.4.4
192.168.4.5
192.168.4.6
{noformat}

No error is reported.

I found that columns are separated by "":"".
And If I pass to many elements then the error happens.

{noformat}
$ nodetool -h cas3 getendpoints anit_portal b 12:dd:dd
Exception in thread ""main"" org.apache.cassandra.serializers.MarshalException: unable to make int from '12:dd:dd'
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:69)
    at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2495)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
    at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
    at sun.rmi.transport.Transport$1.run(Transport.java:177)
    at sun.rmi.transport.Transport$1.run(Transport.java:174)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NumberFormatException: For input string: ""12:dd:dd""
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Integer.parseInt(Integer.java:492)
    at java.lang.Integer.parseInt(Integer.java:527)
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:65)
    ... 36 more
{noformat}

I think showing huge stack trace is not proper behavior.
Error message should be printer if arity of passed key and table key are not equal.",lhf,['Tool/nodetool'],CASSANDRA,Bug,Low,2013-12-06 15:40:24,4
12679553,CAS not applied on rows containing an expired ttl column,"CREATE TABLE session (
  id text,
  usr text,
  valid int,
  PRIMARY KEY (id)
);
insert into session (id, usr) values ('abc', 'abc');
update session using ttl 1 set valid = 1 where id = 'abc';
(wait 1 sec)
And 
delete from session where id = 'DSYUCTCLSOEKVLAQWNWYLVQMEQGGXD' if usr ='demo';
Yields:
 [applied] | usr
-----------+-----
     False | abc
Rather than applying the delete.

Executing:
update session set valid = null where id = 'abc';
and again
delete from session where id = 'DSYUCTCLSOEKVLAQWNWYLVQMEQGGXD' if usr ='demo';
Positively deletes the row.",LWT,"['Feature/Lightweight Transactions', 'Legacy/Coordination']",CASSANDRA,Bug,Normal,2013-11-16 07:37:13,3
12679492,Flush memtables to separate directory,"Flush writers are a critical element for keeping a node healthy. When several compactions run on systems with low performing data directories, IO becomes a premium. Once the disk subsystem is saturated, write IO is blocked which will cause flush writer threads to backup. Since memtables are large blocks of memory in the JVM, too much blocking can cause excessive GC over time degrading performance. In the worst case causing an OOM.

Since compaction is running on the data directories. My proposal is to create a separate directory for flushing memtables. Potentially we can use the same methodology of keeping the commit log separate and minimize disk contention against the critical function of the flushwriter. ",performance,[],CASSANDRA,New Feature,Low,2013-11-15 21:24:22,7
12678906,Make gossip tolerate slow Gossip tasks,"Currently if a single gossip task bogs down the gossip Stage, Gossip will mark everyone down because it hasn't seen updates from them (since they are all queued behind the slow one).

This means that full GCs can cause gossip ""flapping"" as well as any actually problematic tasks such as recomputing pending ranges.",gossip,[],CASSANDRA,Bug,Low,2013-11-12 23:38:37,7
12678092,Refactor dtests to use python driver instead of cassandra-dbapi2,"cassandra-dbapi2 is effectively deprecated. The python driver is the future, we should refactor our dtests to use it.",qa-resolved,[],CASSANDRA,Improvement,Normal,2013-11-07 22:08:35,4
12677050,Windows 7 data files kept open / can't be deleted after compaction.,"Files cannot be deleted, patch CASSANDRA-5383 (Win7 deleting problem) doesn't help on Win-7 on Cassandra 2.0.2. Even 2.1 Snapshot is not running. The cause is: Opened file handles seem to be lost and not closed properly. Win 7 blames, that another process is still using the file (but its obviously cassandra). Only restart of the server makes the files deleted. But after heavy using (changes) of tables, there are about 24K files in the data folder (instead of 35 after every restart) and Cassandra crashes. I experiminted and I found out, that a finalizer fixes the problem. So after GC the files will be deleted (not optimal, but working fine). It runs now 2 days continously without problem. Possible fix/test:
I wrote the following finalizer at the end of class org.apache.cassandra.io.util.RandomAccessReader:

{code:title=RandomAccessReader.java|borderStyle=solid}
@Override
protected void finalize() throws Throwable {
	deallocate();
	super.finalize();
}
{code}

Can somebody test / develop / patch it? Thx.",Windows compaction,"['Legacy/Local Write-Read Paths', 'Local/Compaction']",CASSANDRA,Bug,Normal,2013-11-01 12:52:25,6
12676860,AE in PrecompactedRow.update(PrecompactedRow.java:171),"Getting this AE on destination nodes during repair:

ERROR [ValidationExecutor:78] 2013-10-31 04:35:31,243 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:78,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.update(PrecompactedRow.java:171)
        at org.apache.cassandra.repair.Validator.rowHash(Validator.java:198)
        at org.apache.cassandra.repair.Validator.add(Validator.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:799)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
",repair,[],CASSANDRA,Bug,Low,2013-10-31 14:15:58,7
12676559,SERIAL consistency in errors to v1 protocol driver,"I'm the author of the Ruby driver for CQL, and I got a bug report about strange errors when running on C* 2.0 and using lightweight transaction queries. The bug report can be found here: https://github.com/iconara/cql-rb/issues/53

The client sent {{UPDATE table SET val = 42 WHERE row_id = 5 IF val = 41}} and when C* couldn't fulfill SERIAL consistency it sent an error back saying ""Operation timed out - received only -1 responses"".

So far so good, but it also set the {{consistency}} field in the error response to 8, corresponding to {{SERIAL}} in v2 of the binary protocol, even if the communication with the client was over v1 of the protocol. Since my driver doesn't yet support v2 it doesn't think that 8 is a valid consistency, and fails to parse the frame.

Is this the intended behaviour of C*, or an oversight in how that error is formulated? I could easily add {{SERIAL}} and accept it even if the communication is over v1 of the protocol, but the bigger issue is how C* handles drivers that do not speak the latest version of the protocol. People should be able to use a driver that worked correctly with C* X with C* X+1, right?

Do drivers have to be accepting in what they receive from C* because they might get consistencies, data types, etc. that are from future versions of the protocol, or does C* guarantee that frames will conform to the protocol that the driver says it understands?",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Low,2013-10-30 06:47:10,2
12675972,CAS updates should require P.MODIFY AND P.SELECT,"With CAS it is possible to simulate a SELECT query using conditional UPDATE IF. Hence all CAS updates should require P.SELECT permission, and not just P.MODIFY.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Low,2013-10-26 23:57:36,1
12674276,NPE in system.log,"I wrote a stresstest to test C* and my code that uses CAS heavily. I see strange exception messages in logs:
{noformat}
ERROR [MutationStage:320] 2013-10-17 13:59:10,710 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:320,5,main]
java.lang.NullPointerException
ERROR [MutationStage:328] 2013-10-17 13:59:10,718 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:328,5,main]
java.lang.NullPointerException
ERROR [MutationStage:327] 2013-10-17 13:59:10,732 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:327,5,main]
java.lang.NullPointerException
ERROR [MutationStage:325] 2013-10-17 13:59:10,750 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:325,5,main]
java.lang.NullPointerException
ERROR [MutationStage:326] 2013-10-17 13:59:10,762 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:326,5,main]
java.lang.NullPointerException
ERROR [MutationStage:330] 2013-10-17 13:59:10,768 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:330,5,main]
java.lang.NullPointerException
ERROR [MutationStage:331] 2013-10-17 13:59:10,775 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:331,5,main]
java.lang.NullPointerException
ERROR [MutationStage:334] 2013-10-17 13:59:10,789 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:334,5,main]
java.lang.NullPointerException
ERROR [MutationStage:329] 2013-10-17 13:59:10,803 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:329,5,main]
java.lang.NullPointerException
ERROR [MutationStage:335] 2013-10-17 13:59:10,812 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:335,5,main]
java.lang.NullPointerException
ERROR [MutationStage:333] 2013-10-17 13:59:10,826 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:333,5,main]
java.lang.NullPointerException
ERROR [MutationStage:332] 2013-10-17 13:59:10,834 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:332,5,main]
java.lang.NullPointerException
ERROR [MutationStage:337] 2013-10-17 13:59:10,842 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:337,5,main]
java.lang.NullPointerException
ERROR [MutationStage:336] 2013-10-17 13:59:10,859 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:336,5,main]
java.lang.NullPointerException
ERROR [MutationStage:338] 2013-10-17 13:59:10,870 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:338,5,main]
java.lang.NullPointerException
ERROR [MutationStage:339] 2013-10-17 13:59:10,884 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:339,5,main]
java.lang.NullPointerException
ERROR [MutationStage:341] 2013-10-17 13:59:10,894 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:341,5,main]
java.lang.NullPointerException
ERROR [MutationStage:340] 2013-10-17 13:59:10,910 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:340,5,main]
java.lang.NullPointerException
ERROR [MutationStage:344] 2013-10-17 13:59:10,920 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:344,5,main]
java.lang.NullPointerException
{noformat}",npe nullpointerexception,[],CASSANDRA,Bug,Normal,2013-10-17 08:06:43,2
12673613,Add a warning for small sstable size setting in LCS,"Not sure ""bug"" is the right description, because I can't say for sure that the large number of SSTables is the cause of the memory issues. I'll share my research so far:

Under high read-load with a very large number of compressed SSTables (caused by the initial default 5mb sstable_size in LCS) it seems memory is exhausted, without any room for GC to fix this. It tries to GC but doesn't reclaim much.

The node first hits the ""emergency valves"" flushing all memtables, then reducing caches. And finally logs 0.99+ heap usages and hangs with GC failure or crashes with OutOfMemoryError.

I've taken a heapdump and started analysis to find out what's wrong. The memory seems to be used by the byte[] backing the HeapByteBuffer in the ""compressed"" field of org.apache.cassandra.io.compress.CompressedRandomAccessReader. The byte[] are generally 65536 byes in size, matching the block-size of the compression.

Looking further in the heap-dump I can see that these readers are part of the pool in org.apache.cassandra.io.util.CompressedPoolingSegmentedFile. Which is linked to the ""dfile"" field of org.apache.cassandra.io.sstable.SSTableReader. The dump-file lists 45248 instances of CompressedRandomAccessReader.

Is this intended to go this way? Is there a leak somewhere? Or should there be an alternative strategy and/or warning for cases where a node is trying to read far too many SSTables?

EDIT:
Searching through the code I found that PoolingSegmentedFile keeps a pool of RandomAccessReader for re-use. While the CompressedRandomAccessReader allocates a ByteBuffer in it's constructor and (to make things worse) enlarges it if it's reasing a large chunk. This (sometimes enlarged) ByteBuffer is then kept alive because it becomes part of the CompressedRandomAccessReader which is in turn kept alive as part of the pool in the PoolingSegmentedFile.",lcs,[],CASSANDRA,Improvement,Low,2013-10-13 17:39:19,7
12671623,Optimize the auth default super user/default user check,"Optimize the auth default super user/default user check by checking for the 'cassandra' user first, and only if that fails, doing the range query.

For people following our docs (don't drop 'cassandra', just strip its superuser status and change the password), this will always mean performing just one get.",auth,[],CASSANDRA,Improvement,Low,2013-10-01 17:22:26,1
12671563,Boolean constants syntax is not consistent between DDL and DML in CQL,"DDL statements allow boolean constants to be either quoted or unquoted as:
{code}
CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1} AND durable_writes = true;
CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1} AND durable_writes = 'true';
{code}

While DML statements only allow unquoted boolean constants.

While this is not a big deal, it can introduce a bit of confusion for the users. Fixing this lack of syntax consistency would break the existing scripts, so that's something we might want to consider next time we'll introduce some breaking changes in CQL...",cql,[],CASSANDRA,Bug,Low,2013-10-01 12:06:33,2
12669574,Sets not stored by INSERT with IF NOT EXISTS,An {{INSERT}} of a {{set}} column type is not stored when using {{IF NOT EXISTS}},LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-09-19 22:39:04,2
12669202,"'Internal application error' on SELECT .. WHERE col1=val AND col2 IN (1,2)","Query with error: SELECT * FROM user WHERE login='nsv' AND st IN ('1','2') ALLOW FILTERING;

Query works:
SELECT * FROM user WHERE login='nsv' AND st IN ('1') ALLOW FILTERING;
-- Single item inside IN

Table definition: 
CREATE COLUMNFAMILY user (
     KEY uuid PRIMARY KEY,
     name text,
     avatar text,
     email text,
     phone text,
     login text,
     pw text,
     st text
);

From /var/log/cassandra/output.log:
ERROR 11:58:52,454 Internal error processing execute_cql3_query
java.lang.AssertionError
	at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:749)
	at org.apache.cassandra.cql3.statements.SelectStatement.getRangeCommand(SelectStatement.java:303)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:155)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:108)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1920)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4372)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4356)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

",cql3,[],CASSANDRA,Bug,Normal,2013-09-18 08:02:30,2
12668469,CAS should distinguish promised and accepted ballots,"Currently, we only keep 1) the most recent promise we've made and 2) the last update we've accepted. But we don't keep the ballot at which that last update was accepted. And because a node always promise to newer ballot, this means an already committed update can be replayed even after another update has been committed. Re-committing a value is fine, but only as long as we've not start a new round yet.

Concretely, we can have the following case (with 3 nodes A, B and C) with the current implementation:
* A proposer P1 prepare and propose a value X at ballot t1. It is accepted by all nodes.
* A proposer P2 propose at t2 (wanting to commit a new value Y). If say A and B receive the commit of P1 before the propose of P2 but C receives those in the reverse order, we'll current have the following states:
{noformat}
A: in-progress = (t2, _), mrc = (t1, X)
B: in-progress = (t2, _), mrc = (t1, X)
C: in-progress = (t2, X), mrc = (t1, X)
{noformat}
Because C has received the t1 commit after promising t2, it won't have removed X during t1 commit (but note that the problem is not during commit, that example still stand if C never receive any commit message).
* Now, based on the promise of A and B, P2 will propose Y at t2 (C don't see this propose in particular, not before he promise on t3 below at least). A and B accepts, P2 will send a commit for Y.
* In the meantime a proposer P3 submit a prepare at t3 (for some other irrelevant value) which reaches C before it receives P2 propose&commit. That prepare reaches A and B too, but after the P2 commit. At that point the state will be:
{noformat}
A: in-progress = (t3, _), mrc = (t2, Y)
B: in-progress = (t3, _), mrc = (t2, Y)
C: in-progress = (t3, X), mrc = (t2, Y)
{noformat}
In particular, C still has X as update because each time it got a commit, it has promised to a more recent ballot and thus skipped the delete. The value is still X because it has received the P2 propose after having promised t3 and has thus refused it.
* P3 gets back the promise of say C and A. Both response has t3 as in-progress ballot (and it is more recent than any mrc) but C comes with value X. So P3 will replay X. Assuming no more contention this replay will succeed and X will be committed at t3.

At the end of that example, we've comitted X, Y and then X again, even though only P1 has ever proposed X.

I believe the correct fix is to keep the ballot of when an update is accepted (instead of using the most recent promised ballot). That way, in the example above, P3 would receive from C a promise on t3, but would know that X was accepted at t1. And so P3 would be able to ignore X since the mrc of A will tell him it's an obsolete value.
",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-09-13 11:48:31,2
12668285,CAS may return false but still commit the insert,"If a Paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. Paxos guarantees that we'll agree on ""a"" value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it.  In particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer.

Currently, if a proposer A proposes some update U but it is rejected, A will sleep a bit and retry U. But if U was accepted by at least one acceptor, some other proposer B might replay U, succeed and commit it. If A does its retry after that happens, he will prepare, check the condition, and probably find that the conditions don't apply anymore since U has been committed already. It will thus return false, even though U has been in fact committed.

Unfortunately I'm not sure there is an easy way for a proposer whose propose fails to know if the update will prevail or not eventually. Which mean the only acceptable solution I can see would be to return to the user ""I don't know"" (through some exception for instance). Which is annoying because having a proposal rejected won't be an extremely rare occurrence, even with relatively light contention, and returning ""I don't know"" often is a bit unfriendly.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-09-12 15:31:58,7
12668284,CAS does not always correctly replay inProgress rounds,"Paxos says that on receiving the result of a prepare from a quorum of acceptors, the proposer should propose the value of the higher-number proposal accepted amongst the ones returned by the acceptors, and only propose his own value if no acceptor has send us back a previously accepted value.

But in PrepareCallback we only keep the more recent inProgress commit regardless of whether is has an update. Which means we could ignore a value already accepted by some acceptors if any of the acceptor send us a more recent ballot than the other acceptor but with no values. The net effect is that we can mistakenly accept two different values for the same round.
",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-09-12 15:29:48,2
12667986,StandaloneScrubber assumes old-style json leveled manifest,"With standalone scrubber in 2.0 we can encounter both the old-style json manifest and the new way, StandaloneScrubber needs to handle this.",lcs,[],CASSANDRA,Bug,Low,2013-09-11 06:17:24,0
12667944,CQL: Support NaN and inifinities in Double literals,"My actual app uses a Map[UUID, Double], and in the upstream data source, unavailable data is encoded as Double.NaN. But when I try to insert them into C* (currently working with 2.0), I get a ""no viable alternative"" at the syntax immediately following the NaN value. 

Here's a tiny test case:
{code}
cqlsh> create table test (pk timeuuid primary key, d double);
cqlsh> insert into test (pk, d) values (now(), NaN);
Bad Request: line 1:43 no viable alternative at input ')'
{code}

The workaround suggested by 'iamaleksey' on IRC (Thanks!) allows the data to be inserted, but it's really ugly, and I'm not sure yet whether it will work OK on the read side:

{code}
// myStuff is a Map[UUID,Double]
mySuff.mapValues { 
  case x if x.isNaN => QueryBuilder.fcall(""blobAsDouble"", ByteBufferUtil.EMPTY_BYTE_BUFFER) 
  case x => x: java.lang.Double // explicit boxing, maybe not necessary 
}.asJava
{code}",lhf,[],CASSANDRA,Improvement,Low,2013-09-10 23:04:59,2
12667829,Remove leveled manifest json migration code,"We should remove the json leveled manifest migration code from 2.1

this will require users to atleast start 2.0 before upgrading to 2.1 (manifest is migrated on startup).",lcs,[],CASSANDRA,Improvement,Low,2013-09-10 15:36:59,0
12667496,Paxos replay of in progress update is incorrect,"When we replay {{inProgress}}, we need to refresh it with the newly prepared ballot, or it will be (correctly) rejected.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-09-08 23:07:45,7
12666577,Allow JVM_OPTS to be passed to sstablescrub,"Can you add a feature request to pass JVM_OPTS to the sstablescrub script -- and other places where java is being called? (Among other things, this lets us run java stuff with ""-Djava.awt.headless=true"" on OS X so that Java processes don't pop up into the foreground -- i.e. we have a script that loops over all CFs and runs sstablescrub, and without that flag being passed in the OS X machine becomes pretty much unusable as it keeps switching focus to the java processes as they start.)
 
--- a/resources/cassandra/bin/sstablescrub
+++ b/resources/cassandra/bin/sstablescrub
@@ -70,7 +70,7 @@ if [ ""x$MAX_HEAP_SIZE"" = ""x"" ]; then
     MAX_HEAP_SIZE=""256M""
 fi
 
-$JAVA -ea -cp $CLASSPATH -Xmx$MAX_HEAP_SIZE \
+$JAVA $JVM_OPTS -ea -cp $CLASSPATH -Xmx$MAX_HEAP_SIZE \
         -Dlog4j.configuration=log4j-tools.properties \
         org.apache.cassandra.tools.StandaloneScrubber ""$@""",lhf,['Legacy/Tools'],CASSANDRA,New Feature,Normal,2013-09-02 12:28:47,3
12666414,Require superuser status for adding triggers,"You can do anything from within ITrigger.augment(), bypassing any authorization checks. The only fix is to require superuser status for CREATE TRIGGER and CF updates via Thrift that involve triggers.",security triggers,[],CASSANDRA,Improvement,Low,2013-08-30 22:50:02,1
12665490,Improve the way we pick L0 compaction candidates,"We could improve the way we pick compaction candidates in level 0 in LCS.

The most common way for us to get behind on compaction is after repairs, we should exploit the fact that the streamed sstables are most often very narrow in range since the other nodes in the ring will have a similar sstable-range-distribution. We should in theory be able to do 10 concurrent compactions involving L1 - ie, partition L0 in buckets defined by the sstables in L1 to only keep one L1 SSTable busy for every compaction (be it L1 to L2 or L0 to L1).

we will need some heuristics on when to select candidates from the buckets and when to do it the old way (since L0 sstables can span several L1 sstables)",compaction lcs,[],CASSANDRA,Improvement,Normal,2013-08-26 11:06:33,0
12665231,dateOf() in 2.0 won't work with timestamp columns created in 1.2-,"dateof() return type is TimestampType now, so it won't work with previously created DateType columns
(Type error: cannot assign result of function dateof (type timestamp) to value (type 'org.apache.cassandra.db.marshal.DateType')).",cql3,[],CASSANDRA,Bug,Normal,2013-08-23 14:55:14,1
12665142,Race condition in update lightweight transaction,"I'm building some tests for a Cassandra PoC.  One scenario I need to test is consumption of 1 time tokens.  These tokens must be consumed exactly once.  The cluster involved is a 3 node cluster.  All queries are run with ConsistencyLevel.QUORUM. I'm using the following queries:

CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

CREATE TABLE IF NOT EXISTS tkns (tkn blob, consumed boolean, PRIMARY KEY (tkn));

INSERT INTO tkns (tkn, consumed) VALUES (?,FALSE) USING TTL 30;

UPDATE tkns USING TTL 1 SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

I use the '[applied]' column in the result set of the update statement to determine whether the token has been successfully consumed or if the token is being replayed.

My test involves concurrently executing many sets of 1 insert and 2 update statements (using Session#execute on BoundStatemnts) then checking to make sure that only one of the updates was applied.

When I run this test with relatively few iterations (~100) my results are  what I expect (exactly 1 update succeeds).  At ~1000 iterations, I start seeing both updates reporting success in 1-2% of cases.  While my test is running, I see corresponding error entries in the Cassandra log:

ERROR 15:34:53,583 Exception in thread Thread[MutationStage:522,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:474,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:536,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:480,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:534,5,main]
java.lang.NullPointerException


Thanks.

Update:

I'm not sure what's going on with the logging the the dev release.  I grabbed the rc2 source and built that.  The resultant log is a bit more informative:

ERROR 11:53:38,967 Exception in thread Thread[MutationStage:114,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:32)
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:26)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getUUID(UntypedResultSet.java:131)
	at org.apache.cassandra.db.SystemKeyspace.loadPaxosState(SystemKeyspace.java:785)
	at org.apache.cassandra.service.paxos.PaxosState.commit(PaxosState.java:118)
	at org.apache.cassandra.service.paxos.CommitVerbHandler.doVerb(CommitVerbHandler.java:34)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-08-22 23:17:31,2
12664958,Remove CQL2 entirely from Cassandra 3.0,"CQL2 is officially no longer worked on since 1.2. cqlsh no longer supports CQL2 as of Cassandra 2.0.

It's probably the time to deprecate CQL2 in 2.0 and to remove it entirely in 2.2 - there is nothing in CQL2 now that can't be done via CQL3 and two versions advance warning is plenty of time for those few still using CQL2 to switch to CQL3.",cql,[],CASSANDRA,Improvement,Low,2013-08-22 01:10:04,1
12664817,Add existing sstables to leveled manifest on startup,"we need to add all sstables to the leveled manifest on startup, looks like this was introduced in 6968f68cd7c",lcs,[],CASSANDRA,Bug,Normal,2013-08-21 12:14:07,0
12663200,DESCRIBE SCHEMA should omit system keyspaces by default,"cqlsh DESCRIBE with all the system keyspaces is less useful than than DESCRIBE output with just the user's keyspaces - you can dump/restore schema directly this way. This seems to be a way better default to me.

Attaching a patch that skips all the system keyspaces for regular DESCRIBE SCHEMA and adds DESCRIBE FULL SCHEMA alternative that includes system, system_auth and system_traces.",cqlsh,[],CASSANDRA,Improvement,Low,2013-08-12 12:08:36,1
12660109,CQLSH Windows: TypeError: argument of type 'NoneType' is not iterable,"I downloaded Cassandra Beta 2. 
I've tried to use CQLSH against Cassandra. 

My python version is: 
Enthought Canopy Python 2.7.3 | 32-bit | (default, Mar 25 2013, 15:38:39) [MSC v.1500 32 bit (Intel)] on win32

I get the following exception when I run the utility:

c:\Servers\apache-cassandra\bin>python cqlsh 127.0.0.1 9160
Traceback (most recent call last):
  File ""cqlsh"", line 131, in <module>
    if readline is not None and 'libedit' in readline.__doc__:
TypeError: argument of type 'NoneType' is not iterable",cqlsh windows,['Legacy/Tools'],CASSANDRA,Bug,Low,2013-07-26 18:05:41,1
12659484,DC-local CAS,"For two-datacenter deployments where the second DC is strictly for disaster failover, it would be useful to restrict CAS to a single DC to avoid cross-DC round trips.

(This would require manually truncating {{system.paxos}} when failing over.)",LWT,"['Feature/Lightweight Transactions', 'Legacy/CQL']",CASSANDRA,Bug,Low,2013-07-24 03:02:53,2
12659482,Cqlsh should return a result in the case of a CAS INSERT/UPDATE,"Right now cqlsh behave with {{INSERT/UPDATE...IF}} the same way it does for regular {{INSERT/UPDATE}} statements, that is without displaying anything if there were no error.

Ideally it should display the ResultSet returned by these CAS statements as defined in CASSANDRA-5619.",LWT,"['Feature/Lightweight Transactions', 'Legacy/Tools']",CASSANDRA,New Feature,Normal,2013-07-24 02:59:49,1
12659037,StorageProxy#cas() doesn't order columns names correctly when querying,"When querying columns for CAS, we build the SortedSet with:
{noformat}
new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames())
{noformat}
but ImmutableSortedSet.copyOf() uses the natural order of keys unless a comparator is given, which is not what we want.",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-07-22 08:32:48,2
12659025,Thrift cas() method crashes if input columns are not sorted.,"CassandraServer#cas() use UnsortedColumns for the ""updates"", which might result later to a
{noformat}
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:117)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:119)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:96)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:91)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:139)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:128)
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:175)
        at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:135)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
{noformat}",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Bug,Normal,2013-07-22 06:55:57,2
12658342,Fix system.schema_triggers-related trigger/schema issues,"Among other things, the patch does the following:
- adds missing schema_triggers to MigrationManager.resetLocalSchema()
- adds missing schema_triggers to SystemKeyspace.serializeSchema() - so that triggers would be part of schema version calculation
- adds missing schema_triggers to DefsTables.flushSchemaCFs()
- adds missing triggers to CFMetaData.toSchema(), so that CFs created via thrift with triggers from the beginning would serialize triggers
- removes triggers from CFMetaData.newIndexMetadata(), so that 2i CFs wouldn't inherit the triggers from the parent CF

There are other minor and not so minor changes, but these were the most critical ones. The patch also (unnecessarily) cleans up ColumnDefinition, but that was done to make it consistent with the new TriggerDefinition class.

The bulk of the patch is the updated thrift-gen files.

",triggers,[],CASSANDRA,Bug,Low,2013-07-17 19:34:24,1
12658129,The output of the describe command does not necessarily give you the right DDL to re-create the CF,"If compression is not set for a CF, cqlsh omits the compression attribute.  When you replay that very same DDL, you get a CF with Snappy compression.  This may occur with other parameters.  Perhaps describe should always show every parameter in full.  The absence of a setting is a setting.  (Think of the arrow in the FedEx logo).

Create a CF with cassandra-stress.  cassandra-stress defaults to NO compression.

 ~/dse/resources/cassandra/tools/bin/cassandra-stress -S 100 -c 1 --num-keys 1

describe it
CREATE TABLE ""Standard1"" (
  key blob PRIMARY KEY,
  ""C0"" blob
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'};

replay it - I changed the cf name to standard2

describe the new CF:

CREATE TABLE standard2 (
  key blob PRIMARY KEY,
  ""C0"" blob
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};


",cqlsh describe,['Legacy/Tools'],CASSANDRA,Bug,Normal,2013-07-16 22:08:10,1
12658006,add an lz4 implementation of FrameCompressor for lz4 compressed native cql protocol,"In addition to the current snappy native client compression support, it would be nice to add lz4 as an option now that lz4 has been vetted for inclusion in CASSANDRA-5038.

This would enable PYTHON-1 for the python-driver:
https://datastax-oss.atlassian.net/browse/PYTHON-1",compression lz4 native_protocol,['Legacy/CQL'],CASSANDRA,New Feature,Low,2013-07-16 13:21:55,2
12657651,cqlsh DESCRIBE should properly describe CUSTOM secondary indexes,"CASSANDRA-5484 and then CASSANDRA-5639 added CREATE CUSTOM INDEX support to CQL3, but cqlsh hasn't been updated to describe such indexes properly.",cqlsh describe,['Feature/2i Index'],CASSANDRA,Bug,Low,2013-07-14 23:19:58,1
12657639,Consistencyfy CQL3 create/alter/drop column family/table statement class names,"We've got CreateColumnFamilyStatement, AlterTableStatement, and DropColumnFamilyStatement. The patch makes them all use 'Table' consistently.",cql3 ocd,[],CASSANDRA,Improvement,Low,2013-07-14 17:17:37,1
12657484,DESC TABLE omits some column family settings,"In CQL I can create a table with settings introduced in 2.0:

{code}
cqlsh:Keyspace1> CREATE TABLE r1 ( key int PRIMARY KEY, value varchar) WITH speculative_retry='ALWAYS';
{code}

But the settings don't show up when I DESC TABLE:
{code}
cqlsh:Keyspace1> DESC TABLE r1;

CREATE TABLE r1 (
  key int PRIMARY KEY,
  value text
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

For comparison, here is the same table viewed from cassandra-cli:

{code}
[default@Keyspace1] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

WARNING: Could not connect to the JMX on 127.0.0.1:7199 - some information won't be shown.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Default time to live: 0
      Bloom Filter FP chance: default
      Index interval: default
      Speculative Retry: NONE
      Compaction Strategy: null
{code}

Ideally, all of these values that cli shows would be shown by cqlsh.
",cqlsh,[],CASSANDRA,Bug,Low,2013-07-12 16:00:25,1
12655554,CAS on 'primary key only' table,"Given a table with only a primary key, like
{noformat}
CREATE TABLE test (k int PRIMARY KEY)
{noformat}
there is currently no way to CAS a row in that table into existing because:
# INSERT doesn't currently support IF
# UPDATE has no way to update such table

So we should probably allow IF conditions on INSERT statements.

In addition (or alternatively), we could work on allowing UPDATE to update such table. One motivation for that could be to make UPDATE always be more general to INSERT. That is currently, there is a bunch of operation that INSERT cannot do (counter increments, collection appends), but that ""primary key table"" case is, afaik, the only case where you *need* to use INSERT. However, because CQL forces segregation of PK value to the WHERE clause and not to the SET one, the only syntax that I can see work would be:
{noformat}
UPDATE WHERE k=0;
{noformat}
which maybe is too ugly to allow?

 ",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Improvement,Low,2013-07-01 08:40:16,2
12654636,cqlsh should support collections in COPY FROM,"Concrete operation is as follows.
---------*---------*---------*---------*---------*---------*---------*---------*
(1)map type's export/import
<export>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace maptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use maptestks;
cqlsh:maptestks> create table maptestcf (rowkey varchar PRIMARY KEY, targetmap map<varchar,varchar>);
cqlsh:maptestks> insert into maptestcf (rowkey, targetmap) values ('rowkey',{'mapkey':'mapvalue'});
cqlsh:maptestks> select * from maptestcf;

 rowkey | targetmap
--------+--------------------
 rowkey | {mapkey: mapvalue}
cqlsh:maptestks>  copy maptestcf to 'maptestcf-20130619.txt';
1 rows exported in 0.008 seconds.
cqlsh:maptestks> exit;

[root@castor bin]# cat maptestcf-20130619.txt
rowkey,{mapkey: mapvalue}
　　　　　　　　　　　　　　　　　　　　　　　　　　　<------------------------(a)
<import>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace mapimptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use mapimptestks;
cqlsh:mapimptestks> create table mapimptestcf (rowkey varchar PRIMARY KEY, targetmap map<varchar,varchar>);

cqlsh:mapimptestks> copy mapimptestcf from ' maptestcf-20130619.txt ';
Bad Request: line 1:83 no viable alternative at input '}'
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.025 seconds.
---------*---------*---------*---------*---------*---------*---------*---------*
(2)list type's export/import
<export>
[root@castor bin]#./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace listtestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use listtestks;
cqlsh:listtestks> create table listtestcf (rowkey varchar PRIMARY KEY, value list<varchar>);
cqlsh:listtestks> insert into listtestcf (rowkey,value) values ('rowkey',['value1','value2']);
cqlsh:listtestks> select * from listtestcf;

 rowkey | value
--------+------------------
 rowkey | [value1, value2]

cqlsh:listtestks> copy listtestcf to 'listtestcf-20130619.txt';
1 rows exported in 0.014 seconds.
cqlsh:listtestks> exit;

[root@castor bin]# cat listtestcf-20130619.txt
rowkey,""[value1, value2]""
　　　　　　　　　　　　　　　　　　　　　　　　　　　<------------------------(b)
<export>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace listimptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use listimptestks;
cqlsh:listimptestks> create table listimptestcf (rowkey varchar PRIMARY KEY, value list<varchar>);
cqlsh:listimptestks> copy listimptestcf from ' listtestcf-20130619.txt ';
Bad Request: line 1:79 no viable alternative at input ']'
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.030 seconds.
---------*---------*---------*---------*---------*---------*---------*---------*
Reference: (correct, or error, in another dimension)

Manually, I have rewritten the export file.
[root@castor bin]# cat nlisttestcf-20130619.txt
rowkey,""['value1',' value2']""

....
cqlsh:listimptestks> copy listimptestcf from 'nlisttestcf-20130619.txt';
1 rows imported in 0.035 seconds.

cqlsh:listimptestks> select * from implisttestcf;
 rowkey | value
--------+------------------
 rowkey | [value1, value2]
cqlsh:implisttestks> exit;

[root@castor bin]# cat nmaptestcf-20130619.txt
rowkey,”{'mapkey': 'mapvalue'}”

[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> use  mapimptestks;
cqlsh:mapimptestks> copy mapimptestcf from 'nmaptestcf-20130619.txt';
1 rows imported in 0.023 seconds.
cqlsh:mapimptestks> select * from mapimptestcf;

 rowkey | targetmap
--------+--------------------
 rowkey | {mapkey: mapvalue}

(It appears to be as normal processing.)
---------*---------*---------*---------*---------*---------*---------*---------*
Please confirm from the operation described above.
Comparing the above (a) and (b), in the data format of export file, 
only the presence or absence of (""), 
it suggests a lack of consistency of the treatment(?).
",collections cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2013-06-25 01:12:27,1
12654635,cqlsh doesn't allow semicolons in BATCH statements,"The documentation for BATCH statements declares that semicolons are required between update operations. Currently including them results in an error 'expecting K_APPLY'. To match the design specifications, semi-colons should be allowed or optional. 

",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2013-06-25 00:59:48,1
12653906,"cqlsh shouldn't display ""null"" for empty values","For historical reason (and compatibility with thrift), all type support an empty value, even type like int for which it doesn't really make sense (see CASSANDRA-5674 too on that subject).

If you input such an empty value for a type like int, cqlsh will display it as null:
{noformat}
cqlsh:ks> CREATE TABLE test (k text PRIMARY KEY, v int);
cqlsh:ks> INSERT INTO test(k, v) VALUES ('someKey', blobAsInt(0x));
cqlsh:ks> SELECT * FROM test;

 k       | v
---------+------
 someKey | null

{noformat} 

But that's not correct, it suggests {{v}} has no value but that's not true, it has a value, it's just an empty one.

Now, one may argue support empty values for a type like int is broken, and I would agree with that. But thrift allows it so we probably need to preserve that behavior for compatibility sake. And I guess the need to use blobAsInt at least make it clear that it's kind of a hack.

That being said, cqlsh should not display null as this is confusing. Instead I'd suggest either displaying nothing (that's how an empty string is displayed after all), or to just go with some random explicit syntax like say ""[empty value]""",cqlsh,[],CASSANDRA,Bug,Low,2013-06-20 10:36:18,1
12653785,NPE in net.OutputTcpConnection when tracing is enabled,"I get multiple NullPointerException when trying to trace INSERT statements.

To reproduce:
{code}
$ ccm create -v git:trunk
$ ccm populate -n 3
$ ccm start
$ ccm node1 cqlsh < 5668_npe_ddl.cql
$ ccm node1 cqlsh < 5668_npe_insert.cql
{code}

And see many exceptions like this in the logs of node1:
{code}
ERROR [WRITE-/127.0.0.3] 2013-06-19 14:54:35,885 OutboundTcpConnection.java (line 197) error writing to /127.0.0.3
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:182)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:144)
{code}


This is similar to CASSANDRA-5658 and is the reason that npe_ddl and npe_insert are separate files.",pull-request-available qa-resolved,[],CASSANDRA,Bug,Normal,2013-06-19 19:07:51,7
12653784,Change timestamps used in CAS ballot proposals to be more resilient to clock skew,"The current time is used to generate the timeuuid used for CAS ballots proposals with the logic that if a newer proposal exists then the current one needs to complete that and re-propose. The problem is that if a machine has clock skew and drifts into the future it will propose with a large timestamp (which will get accepted) but then subsequent proposals with lower (but correct) timestamps will not be able to proceed. This will prevent CAS write operations and also reads at serializable consistency level. 

The work around is to initially propose with current time (current behavior) but if the proposal fails due to a larger existing one re-propose (after completing the existing if necessary) with the max of (currentTime, mostRecent+1, proposed+1).

Since small drift is normal between different nodes in the same datacenter this can happen even if NTP is working properly and a write hits one node and a subsequent serialized read hits another. In the case of NTP config issues (or OS bugs with time esp around DST) the unavailability window could be much larger.  

",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Improvement,Low,2013-06-19 19:03:30,7
12652810,Update CREATE CUSTOM INDEX syntax to match new CREATE TRIGGER syntax,"CASSANDRA-5484 introduced CREATE CUSTOM INDEX syntax for custom 2i and CASSANDRA-5576 will add CQL3 support for creating/dropping triggers (CREATE TRIGGER <name> ON <table> USING <classname>). For consistency' sake, CREATE CUSTOM INDEX should be updated to also use 'USING' keyword, e.g. CREATE CUSTOM INDEX ON <table>(<column>) USING <classname>.",cql3,[],CASSANDRA,Improvement,Normal,2013-06-14 05:47:43,1
12652745,Add row count to select output a la psql,"This is handy since counting rows that look mostly alike can be painful; it also makes it clear that cqlsh isn't ignoring you when you select something that doesn't exist.

Here is what psql does:

{noformat}
company=> select fname, salary from employee;

fname   |  salary
--------+--------
John    |30000.00
Franklin|40000.00
Alica   |25000.00
Jennifer|43000.00
Ramish  |38000.00
Joyce   |25000.00
Ahmad   |25000.00
James   |55000.00
(8 rows)
{noformat}",cqlsh,['Legacy/Tools'],CASSANDRA,New Feature,Low,2013-06-13 21:01:58,1
12652558,CQL support for updating multiple rows in a partition using CAS,This is currently supported via Thrift but not via CQL. ,LWT cql3,['Feature/Lightweight Transactions'],CASSANDRA,Improvement,Low,2013-06-13 04:56:27,2
12652101,Incorrect handling of blob literals when the blob column is in reverse clustering order,"Parsing goes through ReversedType.fromString() in this case, and that doesn't strip ""0x"" when calling BytesType.fromString().

The attached patch makes Constants.parsedValue() ReversedType-aware. ",cql3,[],CASSANDRA,Bug,Normal,2013-06-10 22:00:21,1
12651915,Support empty IN queries,"It would be nice to have support of empty IN queries. 
Example: ""SELECT a FROM t WHERE aKey IN ()"". 
One of the reasons is to have such support in DataStax Java Driver (see discussion here: https://datastax-oss.atlassian.net/browse/JAVA-106).",cql3,[],CASSANDRA,Improvement,Low,2013-06-09 10:05:35,1
12651400,CAS UPDATE for a lost race: save round trip by returning column values,"Looking at the new CAS CQL3 support examples [1], if one lost a race for an UPDATE, to save a round trip to get the current values to decide if you need to perform your work, could the columns that were used in the IF clause also be returned to the caller?  Maybe the columns values as part of the SET part could also be returned.

I don't know if this is generally useful though.

In the case of creating a new user account with a given username which is the partition key, if one lost the race to another person creating an account with the same username, it doesn't matter to the loser what the column values are, just that they lost.

I'm new to Cassandra, so maybe there's other use cases, such as doing incremental amount of work on a row.  In pure Java projects I've done while loops around AtomicReference.html#compareAndSet() until the work was done on the referenced object to handle multiple threads each making forward progress in updating the references object.

[1] https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py#L3044",LWT,['Feature/Lightweight Transactions'],CASSANDRA,Improvement,Normal,2013-06-06 17:53:02,2
12651102,CQL now() on prepared statements is evaluated at prepare time and not query execution time,"insert into some_table (id,time) values (?,now())

On the example above now() will always have the same value, it should probably be evaluated at ""query"" time and not at prepare time. ",cql3,['Legacy/CQL'],CASSANDRA,Bug,Normal,2013-06-05 12:10:37,2
12650803,NPE when upgrading a mixed version 1.1/1.2 cluster fully to 1.2,"See the attached upgrade_through_versions_test.py upgrade_test_mixed().

Conceptually this method does the following:

* Instantiates a 3 node 1.1.9 cluster
* Writes some data
* Shuts down node 1 and upgrades it to 1.2 (HEAD)
* Brings the node1 back up, making the cluster a mixed version 1.1/1.2
* Brings down node2 and node3 and does the same upgrade making it all the same version.
* At this point, I would run upgradesstables on each of the nodes, but there is already an error on node3 directly after it's upgrade:

{code}
INFO [FlushWriter:1] 2013-06-03 22:49:46,543 Memtable.java (line 461) Writing Memtable-peers@1023263314(237/237 serialized/live bytes, 14 op
s)
 INFO [FlushWriter:1] 2013-06-03 22:49:46,556 Memtable.java (line 495) Completed flushing /tmp/dtest-YqMtHN/test/node3/data/system/peers/syst
em-peers-ic-2-Data.db (291 bytes) for commitlog position ReplayPosition(segmentId=1370314185862, position=58616)
 INFO [GossipStage:1] 2013-06-03 22:49:46,568 StorageService.java (line 1330) Node /127.0.0.2 state jump to normal
ERROR [MigrationStage:1] 2013-06-03 22:49:46,655 CassandraDaemon.java (line 192) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:511)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:445)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{code}

This error is repeatable, but inconsistent. Interestingly, it is always node3 with the error.",qa-resolved,[],CASSANDRA,Bug,Normal,2013-06-04 02:55:25,1
12650728,ORDER BY desc breaks cqlsh COPY,"If you have a reversed text field, COPY chokes on it because the type is
""'org.apache.cassandra.db.marshal.ReversedType'<text>"" not just 'text' so the strings don't get quoted in the generated CQL.

{noformat}
    def do_import_row(self, columns, nullval, layout, row):
        rowmap = {}
        for name, value in zip(columns, row):
            if value != nullval:
                type = layout.get_column(name).cqltype.cql_parameterized_type()
                if type in ('ascii', 'text', 'timestamp', 'inet'):
                    rowmap[name] = self.cql_protect_value(value)
                else:
                    rowmap[name] = value
            else:
                rowmap[name] = 'null'
        return self.do_import_insert(layout, rowmap)
{noformat}",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2013-06-03 20:40:38,1
12649394,"CassandraAuthorizer.authorize(AuthenticatedUser, IResource) should use prepared statement","The query in the authorize method will be extensively, providing you have configured Cassandra to use CassandraAuthorizer. Given the frequency of this query against the permissions CF, a prepared statement should be used here. There is a non-trivial amount of overhead involved with parsing a statement; so, this query would greatly benefit given how much it is run.

While I was doing some testing recently, I eventually hit some timeout exceptions, but the timeouts were from the query in the authorize method. Here is a recent stack trace from one such exception,

ERROR [Native-Transport-Requests:11] 2013-05-20 16:16:16,702 ErrorMessage.java (line 210) Unexpected exception during request
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses.
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3994)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4878)
        at org.apache.cassandra.service.ClientState.authorize(ClientState.java:290)
        at org.apache.cassandra.service.ClientState.ensureHasPermission(ClientState.java:170)
        at org.apache.cassandra.service.ClientState.hasAccess(ClientState.java:163)
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:147)
        at org.apache.cassandra.cql3.statements.ModificationStatement.checkAccess(ModificationStatement.java:69)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:131)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:257)
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:121)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:287)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:793)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses.
        at org.apache.cassandra.auth.Auth.isSuperuser(Auth.java:95)
        at org.apache.cassandra.auth.AuthenticatedUser.isSuper(AuthenticatedUser.java:50)
        at org.apache.cassandra.auth.CassandraAuthorizer.authorize(CassandraAuthorizer.java:61)
        at org.apache.cassandra.service.ClientState$1.load(ClientState.java:276)
        at org.apache.cassandra.service.ClientState$1.load(ClientState.java:273)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
        ... 20 more
Caused by: org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses.
        at org.apache.cassandra.service.ReadCallback.get(ReadCallback.java:99)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:941)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:829)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:124)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:143)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:151)
        at org.apache.cassandra.auth.Auth.isSuperuser(Auth.java:90)
        ... 28 more",authorization,[],CASSANDRA,Improvement,Normal,2013-05-24 20:44:49,1
12649106,ArrayIndexOutOfBoundsException in LeveledManifest,"The following stack trace was in the system.log:

{quote}
ERROR [CompactionExecutor:2] 2013-05-22 16:19:32,402 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:2,1,main]
 java.lang.ArrayIndexOutOfBoundsException: 5
	at org.apache.cassandra.db.compaction.LeveledManifest.skipLevels(LeveledManifest.java:176)
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:215)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:155)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:410)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:223)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:991)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:230)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
{quote}",compaction,[],CASSANDRA,Bug,Low,2013-05-23 12:36:17,7
12648730,Drop CQL2/CQL3-beta support from cqlsh,"Drop CQL2/CQL3-beta support from cqlsh in 2.0. (If somebody really needs that for some reason in 2.0, they'd still be able to use cqlsh from 1.2).",cql cql3 cqlsh,['Legacy/Tools'],CASSANDRA,Task,Low,2013-05-21 20:45:22,1
12646032,ColumnFamilyInputFormat demands OrderPreservingPartitioner when specifying InputRange with tokens,"When ColumnFamilyInputFormat starts getting splits (via getSplits(...) [ColumnFamilyInputFormat.java:101]) it checks to see if a `jobKeyRange` has been set.  If it has been set it attempts to set the `jobRange`.  However the if block (ColumnFamilyInputFormat.java:124) looks to see if the `jobKeyRange` has tokens but asserts that the OrderPreservingPartitioner must be in use.

This if block should be looking for keys (not tokens).  Code further down (ColumnFamilyInputFormat.java:147) already manages the range if tokens are used but can never be reached.",hadoop,[],CASSANDRA,Bug,Normal,2013-05-03 19:53:16,7
12643966,Reduce memory consumption of IndexSummary,"I am evaluating cassandra for a use case with many tiny rows which would result in a node with 1-3TB of storage having billions of rows. Before loading that much data I am hitting GC issues and when looking at the heap dump I noticed that 70+% of the memory was used by IndexSummaries. 

The two major issues seem to be:

1) that the positions are stored as an ArrayList<Long> which results in each position taking 24 bytes (class + flags + 8 byte long). This might make sense when the file is initially written but once it has been serialized it would be a lot more memory efficient to just have an long[] (really a int[] would be fine unless 2GB sstables are allowed).

2) The DecoratedKey for a byte[16] key takes 195 bytes -- this is for the overhead of the ByteBuffer in the key and overhead in the token.

To somewhat ""work around"" the problem I have increased index_sample but will this many rows that didn't really help starts to have diminishing returns. 


NOTE: This heap dump was from linux with a 64bit oracle vm. 
",qa-resolved,[],CASSANDRA,Improvement,Normal,2013-04-22 17:27:26,7
12643123,Backport row-level bloom filter removal to 1.2,"With the possible presence of range tombstones, it is erroneous to skip checking for a given column in SSTableNamesIterator because the bloom filter says it is not there.
",qa-resolved,[],CASSANDRA,Improvement,Low,2013-04-18 04:33:55,7
12643007,Promote row-level tombstones to index file,"The idea behind promoted indexes (CASSANDRA-2319) was we could skip a seek to the row header by keeping the column index in the index file.  But, we skip writing the row-level tombstone to the index file unless it also has some column data.  So unless we read the tombstone from the data file (where it is guaranteed to exist) we can return incorrect results.",qa-resolved,[],CASSANDRA,Improvement,Normal,2013-04-17 19:11:44,7
12642817,Support custom secondary indexes in CQL,"Through thrift users can add custom secondary indexes to the column metadata.

The following syntax is used in PLSQL, and I think we could use something similar.

CREATE INDEX <NAME> ON <TABLE> (<COLUMN>) [INDEXTYPE IS (<TYPENAME>) [PARAMETERS (<PARAM>[, <PARAM>])]",cql3 index,['Feature/2i Index'],CASSANDRA,Bug,Normal,2013-04-16 21:31:15,1
12642448,isRunning flag set prematurely in org.apache.cassandra.transport.Server,"In org.apache.cassandra.transport.Server, the start() method sets the isRunning flag before calling the run() method. In the event of an initialization error like a port conflict an exception will be thrown at line 136 which is,

    Channel channel = bootstrap.bind(socket);

It seems like it might make more sense to set the isRunning flag after binding to the socket. I have a tool that deploys a node and then verifies it is ready to receive CQL requests. I do this via JMX. Unless I use a delay before making that check, the JMX call will return true even though there is a port conflict. 

",jmx server,[],CASSANDRA,Bug,Low,2013-04-14 15:07:54,2
12641685,Include fatal errors in trace events,This would help tracking down which query is causing errors.,qa-resolved,['Legacy/Tools'],CASSANDRA,Bug,Low,2013-04-09 18:50:47,7
12641116,Make Auth.SUPERUSER_SETUP_DELAY configurable,"I would like to make SUPERUSER_SETUP_DELAY configurable for automated tests. I have tests that stand up a local cluster, and sometimes if I just run a single test class, the total execution time to stand up the (usually two) nodes and run the tests takes less than 20 seconds. I am running on fast hardware, so I can probably get by with a lower value in most cases, but maybe one of my teammates runs on slower hardware and needs a larger value.

I do not see this as a problem for production use. It would just be nice to have it configurable for dev environments.

Thanks",authentication,[],CASSANDRA,Improvement,Low,2013-04-05 19:22:51,1
12640545,PasswordAuthenticator is incompatible with various Cassandra clients,"Evidently with the old authenticator it was allowed to set keyspace, and then login.  With the org.apache.cassandra.auth.PasswordAuthenticator you have to login and then setkeyspace

For backwards compatibility it would be good to allow setting before login, and perform the actual operation/validation later after the login.

",security,[],CASSANDRA,Bug,Low,2013-04-03 16:55:20,1
12640329,Push composites support in the storage engine,"CompositeType happens to be very useful and is now widely used: CQL3 heavily rely on it, and super columns are now using it too internally. Besides, CompositeType has been advised as a replacement of super columns on the thrift side for a while, so it's safe to assume that it's generally used there too.

CompositeType has initially been introduced as just another AbstractType.  Meaning that the storage engine has no nothing whatsoever of composites being, well, composite. This has the following drawbacks:
* Because internally a composite value is handled as just a ByteBuffer, we end up doing a lot of extra work. Typically, each time we compare 2 composite value, we end up ""deserializing"" the components (which, while it doesn't copy data per-se because we just slice the global ByteBuffer, still waste some cpu cycles and allocate a bunch of ByteBuffer objects). And since compare can be called *a lot*, this is likely not negligible.
* This make CQL3 code uglier than necessary. Basically, CQL3 makes extensive use of composites, and since it gets backs ByteBuffer from the internal columns, it always have to check if it's actually a compositeType or not, and then split it and pick the different parts it needs. It's only an API problem, but having things exposed as composites directly would definitively make thinks cleaner. In particular, in most cases, CQL3 don't care whether it has a composite with only one component or a non-really-composite value, but we still always distinguishes both cases.  Lastly, if we do expose composites more directly internally, it's not a lot more work to ""internalize"" better the different parts of the cell name that CQL3 uses (what's the clustering key, what's the actuall CQL3 column name, what's the collection element), making things cleaner. Last but not least, there is currently a bunch of places where methods take a ByteBuffer as argument and it's hard to know whether it expects a cell name or a CQL3 column name. This is pretty error prone.
* It makes it hard (or impossible) to do a number of performance improvements.  Consider CASSANDRA-4175, I'm not really sure how you can do it properly (in memory) if cell names are just ByteBuffer (since CQL3 column names are just one of the component in general). But we also miss oportunities of sharing prefixes. If we were able to share prefixes of composite names in memory we would 1) lower the memory footprint and 2) potentially speed-up comparison (of the prefixes) by checking reference equality first (also, doing prefix sharing on-disk, which is a separate concern btw, might be easier to do if we do prefix sharing in memory).

So I suggest pushing CompositeType support inside the storage engine. What I mean by that concretely would be change the internal {{Column.name}} from ByteBuffer to some CellName type. A CellName would API-wise just be a list of ByteBuffer. But in practice, we'd have a specific CellName implementation for not-really-composite names, and the truly composite implementation will allow some prefix sharing. From an external API however, nothing would change, we would pack the composite as usual before sending it back to the client, but at least internally, comparison won't have to deserialize the components every time, and CQL3 code will be cleaner.
",performance,[],CASSANDRA,Improvement,Normal,2013-04-02 16:19:31,2
12640301,Batch with timestamp failed,"When I create a prepared statement with the following CQL3 using Thrift protocol:
{code}
BEGIN BATCH USING TIMESTAMP <number>
<some INSERT INTO or UPDATE statements ....>
APPLY BATCH
{code}

and execute this statement in a loop, I randomly get the error:
*InvalidRequestException(why:Timestamp must be set either on BATCH or individual statements)*

All statements inside the batch have no individual USING TIMESTAMP.",qa-resolved,[],CASSANDRA,Bug,Low,2013-04-02 13:41:31,1
12640261,cqlsh returns map entries in wrong order,"The elements in the map <timeuuid,decimal> are returned in cqlsh in an order that is neither sorted by name, value or timestamp.
Below is output from cqlsh and cassandra cli. (looks a bit messy here, I have attached a text file without word wrapping)

cqlsh:iBidTest> select * from lots ;

 event_id                             | lot_id | bids_accepted | bids_details | bids_temp                                                                                                                                                                                                                                                            | minimum | number | title
--------------------------------------+--------+---------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------+-----------
 a4a70900-24e1-11df-8924-001ff3591711 |      1 |          null |         null |                                                                                                                                                                                                                                                                 null |     200 |      2 | New lot 2
 a4a70900-24e1-11df-8924-001ff3591711 |      3 |          null |         null | {8b457e90-9ae2-11e2-9bcb-a1f164a2d4a3: 1000, 9606ca50-9ae2-11e2-9bcb-a1f164a2d4a3: 650, 908fb640-9ae2-11e2-9bcb-a1f164a2d4a3: 600, 7d930650-9ae2-11e2-9bcb-a1f164a2d4a3: 500, a1ef7f10-9ae2-11e2-9bcb-a1f164a2d4a3: 1250, 9aedd360-9ae2-11e2-9bcb-a1f164a2d4a3: 950} |     100 |      1 | New lot 1


[default@ibidtest] get lots[a4a70900-24e1-11df-8924-001ff3591711];
=> (column=1:, value=, timestamp=1364829909818000)
=> (column=1:minimum, value=0000000000c8, timestamp=1364829397026000)
=> (column=1:number, value=32, timestamp=1364829909818000)
=> (column=1:title, value=4e6577206c6f742032, timestamp=1364829397026000)
=> (column=3:, value=, timestamp=1364830894466000)
=> (column=3:bids_temp:7d9306509ae211e29bcba1f164a2d4a3, value=0000000001f4, timestamp=1364830833463000)
=> (column=3:bids_temp:8b457e909ae211e29bcba1f164a2d4a3, value=0000000003e8, timestamp=1364830856441000)
=> (column=3:bids_temp:908fb6409ae211e29bcba1f164a2d4a3, value=000000000258, timestamp=1364830865317000)
=> (column=3:bids_temp:9606ca509ae211e29bcba1f164a2d4a3, value=00000000028a, timestamp=1364830874485000)
=> (column=3:bids_temp:9aedd3609ae211e29bcba1f164a2d4a3, value=0000000003b6, timestamp=1364830882711000)
=> (column=3:bids_temp:a1ef7f109ae211e29bcba1f164a2d4a3, value=0000000004e2, timestamp=1364830894466000)
=> (column=3:minimum, value=0000000064, timestamp=1364829412417000)
=> (column=3:number, value=31, timestamp=1364829852020000)
=> (column=3:title, value=4e6577206c6f742031, timestamp=1364829412417000)
Returned 14 results.
Elapsed time: 130 msec(s).
",cql3 cqlsh,"['Legacy/CQL', 'Legacy/Tools']",CASSANDRA,Improvement,Low,2013-04-02 09:47:53,1
12640117,incremental backups race,"incremental backups does not mark things referenced or compacting, so it could get compacted away before createLinks runs.  Occasionally you can see this happen during ColumnFamilyStoreTest.  (Since it runs on the background tasks stage, it does not fail the test.)

{noformat}
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ja-8-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1066)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
{noformat}",compaction,[],CASSANDRA,Bug,Low,2013-04-01 14:01:01,7
12639726,Pluggable security feature to prevent node from joining a cluster and running destructive commands,"It's possible for a node to join an existing cluster (with perhaps more stringent security restrictions i.e. not using AllowAllAuthentication) and issue destructive commands that affect the cluster at large (e.g. drop keyspace via cassandra-cli, etc).  

This can be circumvented with a pluggable security module that could be used to implement basic node vetting/identification/etc.  

",configuration security,['Local/Config'],CASSANDRA,Improvement,Low,2013-03-29 01:06:11,1
12639433,Compaction doesn't remove index entries as designed,"PerColumnIndexUpdater ignores updates where the new value is a tombstone.  It should still remove the index entry on oldColumn.

(Note that this will not affect user-visible correctness, since KeysSearcher/CompositeSearcher will issue deletes against stale index entries, but having more stale entries than we ""should"" could affect performance.)",qa-resolved,[],CASSANDRA,Bug,Low,2013-03-27 19:32:57,7
12638924,Windows 7 deleting/renaming files problem,"Two unit tests are failing on Windows 7 due to errors in renaming/deleting files:


org.apache.cassandra.db.ColumnFamilyStoreTest: 
{code}
    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 27, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 13.904 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 13:06:46,058 Unable to delete build\test\cassandra\data\Keyspace1\Indexed2\Keyspace1-Indexed2.birthdate_index-ja-1-Data.db (it will be removed on server restart; we'll also retry after GC)
    [junit] ERROR 13:06:48,508 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-7-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1057)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSliceByNamesCommandOldMetatada(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetatada(ColumnFamilyStoreTest.java:885)
    [junit] 
    [junit] 
    [junit] Testcase: testRemoveUnifinishedCompactionLeftovers(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra\build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] FSWriteError in build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:103)
    [junit] 	at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:139)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:507)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStoreTest.testRemoveUnifinishedCompactionLeftovers(ColumnFamilyStoreTest.java:1246)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra\build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ColumnFamilyStoreTest FAILED
{code}


org.apache.cassandra.db.ScrubTest:
{code}
    [junit] Testcase: testScrubFile(org.apache.cassandra.db.ScrubTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:94)
    [junit] 
    [junit] 
    [junit] Testcase: testScubOutOfOrder(org.apache.cassandra.db.ScrubTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScubOutOfOrder(ScrubTest.java:201)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ScrubTest FAILED
{code}

Reproduced in a Windows 7 VM:
java 1.6.0_43-b01
ant 1.9.0
C* trunk
run 'ant clean test'",qa-resolved,['Legacy/Testing'],CASSANDRA,Bug,Normal,2013-03-25 17:36:37,0
12638256,"Perform size-tiered compactions in L0 (""hybrid compaction"")","If LCS gets behind, read performance deteriorates as we have to check bloom filters on man sstables in L0.  For wide rows, this can mean having to seek for each one since the BF doesn't help us reject much.

Performing size-tiered compaction in L0 will mitigate this until we can catch up on merging it into higher levels.",lcs,[],CASSANDRA,Bug,Normal,2013-03-21 15:22:30,7
12637842,Transposed KS/CF arguments,"*Reproduction*
Using https://github.com/joaquincasares/java-driver's integrationtests branch, run `mvn test` from the root directory.

*Issue*
The test will fail due to https://github.com/joaquincasares/java-driver/blob/integrationtests/driver-core/src/main/java/com/datastax/driver/core/ResultSetFuture.java being swapped here:
{CODE}
case ALREADY_EXISTS:
    org.apache.cassandra.exceptions.AlreadyExistsException aee = (org.apache.cassandra.exceptions.AlreadyExistsException)te;
    return new AlreadyExistsException(aee.ksName, aee.cfName);
{CODE}

*Error*
{CODE}
repeatSchemaDefinition(com.datastax.driver.core.ExceptionsTest)  Time elapsed: 0.501 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<Table repeatschema[ks.repeatschemacf] already exists> but was:<Table repeatschema[cf.repeatschemaks] already exists>
{CODE}",datastax_qa,[],CASSANDRA,Bug,Low,2013-03-19 22:19:59,2
12637053,Add binary protocol support for bind variables to non-prepared statements,"Currently, the binary protocol allows requests as ""string"" or ""[prepared statement] id + bind vars"".  Allowing ""string + bind vars"" as well would simplify life for users with one-off statements and not have to choose between adding boilerplate for PS, and having to manually escape parameters, which is particularly painful for binary data.",cql protocol,['Legacy/CQL'],CASSANDRA,Task,Low,2013-03-14 17:12:45,0
12636841,ancestors are not cleared in SSTableMetadata after compactions are done and old SSTables are removed,"We are using LCS and have total of 38000 SSTables for one CF. During LCS, there could be over a thousand SSTable involved. All those SSTable IDs are stored in ancestors field of SSTableMetatdata for the new table. In our case, it consumes more than 1G of heap memory for those field. Put it in perspective, the ancestors consume 2 - 3 times more memory than bloomfilter (fp = 0.1 by default) in LCS. 
We should remove those ancestors from SSTableMetadata after the compaction is finished and the old SSTable is removed. It  might be a big deal for Sized Compaction since there are small number of SSTable involved. But it consumes a lot of memory for LCS. 
At least, we shouldn't load those ancestors to the memory during startup if the files are removed. 
I would love to contribute and provide patch. Please let me know how to start. ",lcs,[],CASSANDRA,Bug,Normal,2013-03-13 19:02:09,0
12636457,Don't announce migrations to pre-1.2 nodes,"I have a mixed version cluster consisting of two 1.1.9 nodes and one 1.2.2 node upgraded from 1.1.9. 

The upgrade works, and I don't see any end user problems, however I see this exception in the logs on the non-upgraded nodes:

{code}
ERROR [MigrationStage:1] 2013-03-11 18:09:09,001 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:256)
	at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:397)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:373)
	at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}


Steps to reproduce:
{code}
ccm create -v 1.1.9 1.1.9
ccm populate -n 3
ccm start
ccm node1 stress
ccm node1 stop
{code}

edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2. Edit node1's cassandra.yaml to be 1.2 compliant.

{code}
ccm node1 start
{code}

The cluster is now a mixed version, and works for user queries, but with the exception above.",qa-resolved,[],CASSANDRA,Bug,Normal,2013-03-11 22:23:21,1
12636391,Remove ASSUME from cqlsh,"Now that we have blobAsX and xAsBlob conversion functions for all the Cassandra types, ASSUME is no longer needed and can be removed.",cqlsh,['Legacy/Tools'],CASSANDRA,Task,Low,2013-03-11 16:12:42,1
12636099,Backport on-startup manifest repair to 1.2,Initially added to trunk for CASSANDRA-4872,compaction,[],CASSANDRA,Bug,Low,2013-03-08 19:00:03,7
12634867,clqsh COPY is broken after strictening validation in 1.2.2 (quotes ints),"cqlsh COPY is quoting values when it shouldn't, and that's throwing IRE in 1.2.2.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2013-03-01 17:41:13,1
12633306,rename AntiEntropyService,"Maybe ActiveRepairService or ManualRepairService.  ""AntiEntropy"" doesn't help anyone find it who knows they want to find the ""repair"" code.",repair,[],CASSANDRA,Task,Low,2013-02-20 23:43:47,7
12633090,Hinted Handoff Throttle based on cluster size,"For a 12-node EC2 m1.xlarge cluster, restarting a node causes it to get completely overloaded with the default 2-thread, 1024KB setting in 1.2.x. This seemed to be a smaller problem when it was 6-nodes, but still required us to abort handoffs. The old defaults in 1.1.x were WAY more conservative. I've dropped this way down to 128KB on our production cluster which is really conservative, but appears to have solved it. The default seems way too high on any cluster that is non-trivial in size.

After putting some thought to this, it seems that this should really be based on cluster size, making the throttle a ""target"" for how much write load a single node can swallow. As the cluster grows, the amount of hints that can be delivered by each other node in the cluster goes down, so the throttle should self-adjust to take that into account.",lhf,[],CASSANDRA,Improvement,Low,2013-02-19 22:37:56,7
12633066,Create tool to drop sstables to level 0,"after CASSANDRA-4872 we need a way to reset all sstables to level 0, previously we did this by removing the .json file from the data-directory.

",lcs tools,[],CASSANDRA,Improvement,Low,2013-02-19 20:20:47,0
12632473,"""Memory was freed"" AssertionError During Major Compaction","When initiating a major compaction with `./nodetool -h localhost compact`, an AssertionError is thrown in the CompactionExecutor from o.a.c.io.util.Memory:

ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

---

I've invoked the `nodetool compact` three times; this occurred after each. The node has been up for a couple days accepting writes and has not been restarted.

Here's the server's log since it was started a few days ago: https://gist.github.com/cscotta/4956472/raw/95e7cbc68de1aefaeca11812cbb98d5d46f534e8/cassandra.log

Here's the code being used to issue writes to the datastore: https://gist.github.com/cscotta/20cbd36c2503c71d06e9

---

Configuration: One node, one keyspace, one column family. ~60 writes/second of data with a TTL of 86400, zero reads. Stock cassandra.yaml.

Keyspace DDL:

create keyspace jetpack;
use jetpack;
create column family Metrics with key_validation_class = 'UTF8Type' and comparator = 'IntegerType';",compaction,[],CASSANDRA,Bug,Urgent,2013-02-14 21:17:53,7
12632213,Improve LeveledScanner work estimation,See https://issues.apache.org/jira/browse/CASSANDRA-5222?focusedCommentId=13577420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577420,compaction lcs,[],CASSANDRA,Improvement,Normal,2013-02-13 14:38:25,0
12632211,Avoid allocateding SSTableBoundedScanner when the range does not intersect the sstable,See https://issues.apache.org/jira/browse/CASSANDRA-5222?focusedCommentId=13577420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577420,repair,[],CASSANDRA,Improvement,Low,2013-02-13 14:30:30,7
12631388,Add cqlsh help for auth statements,Add cqlsh help for CREATE USER/DROP USER/GRANT/REVOKE etc.,cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2013-02-08 06:18:02,1
12631371,Add username autocompletion to cqlsh,Add cqlsh username autocompletion to grant/revoke/list/drop/alter queries.,cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2013-02-08 01:43:51,1
12626258,Add support for SSL sockets to use client certificate authentication.,Add an option to EncryptionOptions to require client certication authentication.,qa-resolved,[],CASSANDRA,Improvement,Low,2013-01-06 20:29:08,1
12625118,Major compaction IOException in 1.1.8,"Upgraded 1.1.6 to 1.1.8.

Now I'm trying to do a major compaction, and seeing this:

ERROR [CompactionExecutor:129] 2012-12-22 10:33:44,217 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:129,1,RMI Runtime]
java.io.IOError: java.io.IOException: Bad file descriptor
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:195)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:298)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Bad file descriptor
        at sun.nio.ch.FileDispatcher.preClose0(Native Method)
        at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.FileInputStream.close(FileInputStream.java:258)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:121)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
        at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:130)
        at org.apache.cassandra.io.sstable.SSTableScanner.close(SSTableScanner.java:89)
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:61)
        ... 9 more
",compression,[],CASSANDRA,Bug,Normal,2012-12-22 18:38:34,7
12624114,Bug in creating EnumSet in SimpleAuthorizer example,"In SimpleAuthorizer around line 47 we have,

EnumSet<Permission> authorized = EnumSet.copyOf(Permission.NONE);

This results in an IllegalArgumentException since Permission.NONE is an empty set. I think it should be changed to,

EnumSet<Permission> authorized = EnumSet.noneOf(Permission.class);
",authentication,[],CASSANDRA,Bug,Low,2012-12-15 13:24:37,1
12623764,Support CAS,"""Strong"" consistency is not enough to prevent race conditions.  The classic example is user account creation: we want to ensure usernames are unique, so we only want to signal account creation success if nobody else has created the account yet.  But naive read-then-write allows clients to race and both think they have a green light to create.
",LWT,"['Feature/Lightweight Transactions', 'Legacy/CQL']",CASSANDRA,New Feature,Normal,2012-12-13 15:24:45,7
12618265,Add a latency histogram option to stress,Averages just aren't always enough and it should be easy to wire our existing histogram utils into stress.,stress,[],CASSANDRA,Improvement,Low,2012-11-30 02:55:08,7
12616849,"""AssertionError: Wrong class type"" at CounterColumn.diff()","Thousands of the following errors are getting logged to system.log:

ERROR [ReadRepairStage:150152] 2012-11-15 12:31:02,815 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadRepairStage:150152,5,main]
java.lang.AssertionError: Wrong class type.
        at org.apache.cassandra.db.CounterColumn.diff(CounterColumn.java:110)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:248)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:342)
        at org.apache.cassandra.service.RowRepairResolver.scheduleRepairs(RowRepairResolver.java:117)
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:94)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

There are also many of the following errors intermingled with the above:

ERROR [ReadRepairStage:150158] 2012-11-15 12:30:34,148 CounterContext.java (line 381) invalid counter shard detected; (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, 916) and (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, -1590) differ only in count; will pick highest to self-heal; this indicates a bug or corruption generated a bad counter shard

I am not 100% sure whether they are related.",counters,[],CASSANDRA,Bug,Normal,2012-11-19 22:30:23,2
12615825,Add blocking force compaction (and anything else) calls to NodeProbe,"There are times when I'd like to get feedback about when compactions complete.  For example, if I'm deleting data from cassandra and want to know when it is 100% removed from cassandra (tombstones collected and all).  This is completely trivial to implement based on the existing code (the method called by the non-blocking version returns a future, so you could just wait on that, potentially with a timeout).",lhf,[],CASSANDRA,Improvement,Low,2012-11-13 00:52:09,7
12615023,Make CQL 3 data accessible via thrift.,"Following the changes from CASSANDRA-4377 data created using CQL 3 is not visible via the thrift interface. 

This goes against the spirit of many comments by the project that ""the thrift API is not going away"". These statements and ones such as ""Internally, both CQL3 and thrift use the same storage engine, so all future improvements to this engine will impact both of them equally."" (http://www.datastax.com/dev/blog/thrift-to-cql3) and the CQL3 and thrift examples given here http://www.datastax.com/dev/blog/cql3-for-cassandra-experts gave the impression CQL 3 was a layer on top of the core storage engine. It now appears to be an incompatible format change. 

It makes it impossible to explain to existing using users how CQL 3 stores it's data. 

It also creates an all or nothing approach to trying CQL 3. 

My request is to make all data written by CQL 3 readable via the thrift API. 

An example of using the current 1.2 trunk is below:

{noformat}
cqlsh:cass_college> CREATE TABLE UserTweets 
                ... (
                ...     tweet_id    bigint,
                ...     user_name   text,
                ...     body        text,
                ...     timestamp   timestamp,
                ...     PRIMARY KEY (user_name, tweet_id)
                ... );
cqlsh:cass_college> INSERT INTO 
                ...     UserTweets
                ...     (tweet_id, body, user_name, timestamp)
                ... VALUES
                ...     (1, 'The Tweet', 'fred', 1352150816917);
cqlsh:cass_college> 
cqlsh:cass_college> 
cqlsh:cass_college> select * from UserTweets;

 user_name | tweet_id | body      | timestamp
-----------+----------+-----------+--------------------------
      fred |        1 | The Tweet | 2012-11-06 10:26:56+1300
{noformat}

and in the CLI

{noformat}
[default@cass_college] show schema;
create keyspace cass_college
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 3}
  and durable_writes = true;

use cass_college;



[default@cass_college] list UserTweets;
UserTweets not found in current keyspace.
[default@cass_college] 
{noformat}


",cli cql,[],CASSANDRA,Improvement,Normal,2012-11-06 21:00:01,7
12615018,cqlsh COPY FROM command requires primary key in first column of CSV,"The cqlsh COPY FROM command requires the primary key to be in the first column of the CSV, even if the field list shows that the primary key is in a different position.

Test data available from ftp://ftp.census.gov/Econ2001_And_Earlier/CBP_CSV/cbp01us.txt

CREATE TABLE cbp01us ( 
           naics text PRIMARY KEY 
           ) WITH 
           comment='' AND 
           comparator=text AND 
           read_repair_chance=0.100000 AND 
           gc_grace_seconds=864000 AND 
           default_validation=text AND 
           min_compaction_threshold=4 AND 
           max_compaction_threshold=32 AND 
           replicate_on_write='true' AND 
           compaction_strategy_class='SizeTieredCompactionStrategy' AND 
           compression_parameters:sstable_compression='SnappyCompressor';

copy cbp01us (uscode,naics,empflag,emp,qp1,ap,est,f1_4,e1_4,q1_4,a1_4,n1_4,f5_9,e5_9,q5_9,a5_9,n5_9,f10_19,e10_19,q10_19,a10_19,n10_19,f20_49,e20_49,q20_49,a20_49,n20_49,f50_99,e50_99,q50_99,a50_99,n50_99,f100_249,e100_249,q100_249,a100_249,n100_249,f250_499,e250_499,q250_499,a250_499,n250_499,f500_999,e500_999,q500_999,a500_999,n500_999,f1000,e1000,q1000,a1000,n1000) from 'cbp01us.txt' with header=true;
Bad Request: Expected key 'NAICS' to be present in WHERE clause for 'cbp01us'
Aborting import at record #0 (line 1). Previously-inserted values still present.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2012-11-06 20:07:13,1
12614519,Authentication provider in Cassandra itself,"I've been working on an implementation for both IAuthority2 and IAuthenticator that uses Cassandra itself to store the necessary credentials. I'm planning on open sourcing this shortly.

Is there any interest in this? It tries to provide reasonable security, for example using PBKDF2 to store passwords with a configurable configuration cycle and managing all the rights available in IAuthority2. 

My main use goal isn't security / confidentiality of the data, but more that I don't want multiple consumers of the cluster to accidentally screw stuff up. Only certain users can write data, others can read it out again and further process it.

I'm planning on releasing this soon under an open source license (probably the same as Cassandra itself). Would there be interest in incorporating it as a new reference implementation instead of the properties file implementation perhaps? Or can I better maintain it separately? I would love if people from the community would want to review it, since I have been dabbling in the Cassandra source code only for a short while now.

During the development of this I've encountered a few bumps and I wonder whether they could be addressed or not.

= Moment when validateConfiguration() runs =

Is there a deliberate reason that validateConfiguration() is executed before all information about keyspaces, column families etc. is available? In the current form I therefore can't validate whether column families etc. are available for authentication since they aren't loaded yet.

I've wanted to use this to make relatively easy bootstrapping possible. My approach here would be to only enable authentication if the needed keyspace is available. This allows for configuring the cluster, then import the necessary authentication data for an admin user to bootstrap further and then restart every node in the cluster.

Basically the questions here are, can the moment when validateConfiguration() runs for an authentication provider be changed? Is this approach to bootstrapping reasonable or do people have better ideas?

= AbstractReplicationStrategy has package visible constructor =

I've added a strategy that basically says that data should be available on all nodes. The amount of data use for authentication is very limited. Replicating it to every node is there for not very problematic and allows for every node to have all data locally available for verifying requests.

I wanted to put this strategy into it's own package inside the authentication module, but since the constructor of AbstractReplicationStrategy has no visibility explicitly marked, it's only available inside the same package.

I'm not sure whether implementing a strategy to replicate data to all nodes is a sane idea and whether my implementation of this strategy is correct. What do you people think of this? Would people want to review the implementation?
",authentication authorization,[],CASSANDRA,Improvement,Normal,2012-11-02 14:56:14,1
12613989,Revert IAuthority2 interface,"CASSANDRA-4874 is about general improvements to authorization handling, this one is about IAuthority[2] in particular.

- 'LIST GRANTS OF user should' become 'LIST PERMISSIONS [on resource] [of user]'.
Currently there is no way to see all the permissions on the resource, only all the permissions of a particular user.
- IAuthority2.listPermissions() should return a generic collection of ResoucePermission or something, not CQLResult or ResultMessage.
That's a wrong level of abstraction. I know this issue has been raised here - https://issues.apache.org/jira/browse/CASSANDRA-4490?focusedCommentId=13449732&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13449732com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13449732, but I think it's possible to change this. Returning a list of {resource, user, permission, grant_option} tuples should be possible.
- We should get rid of Permission.NO_ACCESS. An empty list of permissions should mean absence of any permission, not some magical Permission.NO_ACCESS value.
It's insecure and error-prone and also ambiguous (what if a user has both FULL_ACCESS and NO_ACCESS permissions? If it's meant to be a way to strip a user
of all permissions on the resource, then it should be replaced with some form of REVOKE statement. Something like 'REVOKE ALL PERMISSIONS' sounds more logical than GRANT NO_ACCESS to me.
- Previous point will probably require adding revokeAllPermissions() method to make it explicit, special-casing IAuthority2.revoke() won't do
- IAuthorize2.grant() and IAuthorize2.revoke() accept CFName instance for a resource, which has its ks and cf fields swapped if cf is omitted. This may cause a real security issue if IAuthorize2 implementer doesn't know about the issue. We must pass the resouce as a collection of strings ([cassandra, keyspaces[, ks_name][, cf_name]]) instead, the way we pass it to IAuthorize.authorize().
- We should probably get rid of FULL_ACCESS as well, at least as a valid permission value (but maybe allow it in the CQL statement) and add an equivalent IAuthority2.grantAllPermissions(), separately. Why? Imagine the following sequence: GRANT FULL_ACCESS ON resource FOR user; REVOKE SELECT ON resource FROM user; should the user be allowed to SELECT anymore?
I say no, he shouldn't. Full access should be represented by a list of all permissions, not by a magical special value.
- P.DELETE probably should go in favour of P.UPDATE even for TRUNCATE. Presence of P.DELETE will definitely confuse users, who might think that it is somehow required to delete data, when it isn't. You can overwrite every value if you have P.UPDATE with TTL=1 and get the same result. We should also drop P.INSERT. Leave P.UPDATE (or rename it to P.MODIFY). P.MODIFY_DATA + P.READ_DATA should replace P.UPDATE, P.SELECT and P.DELETE.
- I suggest new syntax to allow setting permissions on cassandra/keyspaces resource: GRANT <permission> ON * FOR <user>.

The interface has to change because of the CFName argument to grant() and revoke(), and since it's going to be broken anyway (and has been introduced recently), I think we are in a position to make some other improvements while at it.",security,[],CASSANDRA,Improvement,Normal,2012-10-30 04:05:37,1
12613986,Possible authorizaton handling impovements,"I'll create another issue with my suggestions about fixing/improving IAuthority interfaces. This one lists possible improvements that aren't related to grant/revoke methods.

Inconsistencies:
- CREATE COLUMNFAMILY: P.CREATE on the KS in CQL2 vs. P.CREATE on the CF in CQL3 and Thrift
- BATCH: P.UPDATE or P.DELETE on CF in CQL2 vs. P.UPDATE in CQL3 and Thrift (despite remove* in Thrift asking for P.DELETE)
- DELETE: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3
- DROP INDEX: no checks in CQL2 vs. P.ALTER on the CF in CQL3

Other issues/suggestions
- CQL2 DROP INDEX should require authorization
- current permission checks are inconsistent since they are performed separately by CQL2 query processor, Thrift CassandraServer and CQL3 statement classes.
We should move it to one place. SomeClassWithABetterName.authorize(Operation, KS, CF, User), where operation would be a enum
(ALTER_KEYSPACE, ALTER_TABLE, CREATE_TABLE, CREATE, USE, UPDATE etc.), CF should be nullable.
- we don't respect the hierarchy when checking for permissions, or, to be more specific, we are doing it wrong. take  CQL3 INSERT as an example:
we require P.UPDATE on the CF or FULL_ACCESS on either KS or CF. However, having P.UPDATE on the KS won't allow you to perform the statement, only FULL_ACCESS will do.
I doubt this was intentional, and if it was, I say it's wrong. P.UPDATE on the KS should allow you to do updates on KS's cfs.
Examples in http://www.datastax.com/dev/blog/dynamic-permission-allocation-in-cassandra-1-1 point to it being a bug, since REVOKE UPDATE ON ks FROM omega is there.
- currently we lack a way to set permission on cassandra/keyspaces resource. I think we should be able to do it. See the following point on why.
- currently to create a keyspace you must have a P.CREATE permission on that keyspace THAT DOESN'T EVEN EXIST YET. So only a superuser can create a keyspace,
or a superuser must first grant you a permission to create it. Which doesn't look right to me. P.CREATE on cassandra/keyspaces should allow you to create new
keyspaces without an explicit permission for each of them.
- same goes for CREATE TABLE. you need P.CREATE on that not-yet-existing CF of FULL_ACCESS on the whole KS. P.CREATE on the KS won't do. this is wrong.
- since permissions don't map directly to statements, we should describe clearly in the documentation what permissions are required by what cql statement/thrift method.

Full list of current permission requirements: https://gist.github.com/3978182
",security,[],CASSANDRA,Improvement,Normal,2012-10-30 03:46:56,1
12613911,Move manifest into sstable metadata,"Now that we have a metadata component it would be better to keep sstable level there, than in a separate manifest.  With information per-sstable we don't need to do a full re-level if there is corruption.",lcs qa-resolved,[],CASSANDRA,Improvement,Low,2012-10-29 15:56:49,0
12613503,cqlsh can't describe system tables,"{noformat}
cqlsh> describe table system_traces.sessions;

Unconfigured column family 'sessions'
{noformat}",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Normal,2012-10-25 15:12:27,1
12612999,BulkLoader throws NPE at start up,"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml.",bulkloader,[],CASSANDRA,Bug,Normal,2012-10-22 17:10:02,7
12612328,Make consistency level configurable in cqlsh,"CASSANDRA-4734 moved consistency level to the protocol, so cqlsh needs a way to change consistency level from the default (ONE).",cqlsh,[],CASSANDRA,Improvement,Low,2012-10-17 22:22:47,1
12612325,cqlsh --cql3 unable to describe CF created with cli,"created CF with cli:

{noformat}
create column family playlists
with key_validation_class = UUIDType
 and comparator = 'CompositeType(UTF8Type, UTF8Type, UTF8Type)'
 and default_validation_class = UUIDType;
{noformat}

Then get this error with cqlsh:

{noformat}
cqlsh:music> describe table playlists;

/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:771: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected composite key CF to have column aliases, but found none
/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:794: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected [u'KEY'] length to be 3, but it's 1. comparator='org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)'
CREATE TABLE playlists (
  ""KEY"" uuid PRIMARY KEY
)
{noformat}",cql3,['Legacy/Tools'],CASSANDRA,Bug,Low,2012-10-17 21:55:57,1
12611952,"Some cqlsh help topics don't work (select, create, insert and anything else that is a cql statement)","cqlsh> help select
Improper help command.

Same will happen if you look up a help topic for any other cql statement.
38748b43d8de17375c7cc16e7a4969ca4c1a2aa1 broke it (#4198) 5 months ago.
",cqlsh,[],CASSANDRA,Bug,Low,2012-10-16 02:37:38,1
12611640,inet datatype does not work with cqlsh on windows,"{noformat}
create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
use foo;
create table one (id int primary key, c int);
TRACING ON;
insert into one (id, c) values (1, 2);

value '\x7f\x00\x00\x01' (in col 'source') can't be deserialized as inet: 'module' object has no attribute 'inet_ntop'
{noformat}",cqlsh windows,['Legacy/Tools'],CASSANDRA,Bug,Low,2012-10-12 21:37:20,1
12611627,cqlsh help is obsolete for cql3,"For example, new syntax for CREATE KEYSPACE is

create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': 1}
",cql3,['Legacy/Tools'],CASSANDRA,Bug,Low,2012-10-12 20:24:35,1
12610898,leveled compaction does less work in L0 than intended,"We have this code in the candidate loop:

{code}
.               if (SSTable.getTotalBytes(candidates) > maxSSTableSizeInBytes)
                {
                    // add sstables from L1 that overlap candidates
                    candidates.addAll(overlapping(candidates, generations[1]));
                    break;
                }
{code}

thus, as soon as we have enough to compact to make one L1 sstable's worth of data, we stop collecting candidates.",compaction lcs,[],CASSANDRA,Bug,Low,2012-10-08 22:40:28,7
12610780,Don't require quotes for true and false,"The docs at http://cassandra.apache.org/doc/cql3/CQL.html#identifiers describe using double quotes for an identifier that is a reserved word. The following works as expected,

cqlsh:test> select ""columnfamily"" from system.schema_columnfamilies;

I have a table with a boolean column. In order to insert a boolean value, I have to enclose it in single quotes. The table looks like,

CREATE TABLE bool_test (
  id int PRIMARY KEY,
  val boolean
);

Here is what happens when I try using double quotes,

cqlsh:rhq> insert into bool_test (id, val) values (4, ""false"");
Bad Request: line 1:43 no viable alternative at input 'false'


The use of single quotes here seems inconsistent with what is described in the docs, and makes things a bit confusing. It would be nice if single or double quotes could be used for identifiers that are reserved words. I also think it is a bit counter-intuitive to require quotes for true and false which are literal values.",cql3,[],CASSANDRA,Bug,Low,2012-10-08 01:20:26,2
12610765,Counters 2.0,"The existing partitioned counters remain a source of frustration for most users almost two years after being introduced.  The remaining problems are inherent in the design, not something that can be fixed given enough time/eyeballs.

Ideally a solution would give us
- similar performance
- less special cases in the code
- potential for a retry mechanism
",counters,[],CASSANDRA,New Feature,Normal,2012-10-07 19:38:57,1
12610588,(CQL3) data type not in lowercase are not handled correctly.,"Seems that we accept {{int}} but we don't accept {{INT}} (that is, the parser accepts it, but we fail later to recognize it).",cql3,[],CASSANDRA,Bug,Low,2012-10-05 15:56:15,2
12610368,StackOverflowError in CompactionExecutor thread,"Seeing the following error:


Exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.StackOverflowError
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)

",compaction,[],CASSANDRA,Bug,Normal,2012-10-04 18:17:14,7
12610366,Clean out STREAM_STAGE vestiges,Currently it appears as though bulk loading operations don't run in any stage. Seems like they should be running in STREAM_STAGE.,streaming,[],CASSANDRA,Bug,Low,2012-10-04 17:46:27,7
12610142,CQL3 Predicate logic bug when using composite columns,"Looks like a predicate logic bug that only happens when you have > 2 primary keys and use COMPACT STORAGE (meaning its using composite columns under the hood)

First I'll show it works with just 2 
{code}
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate)
       ...          ) WITH COMPACT STORAGE
       ...            AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
 key | rdate                    | num
-----+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 10.5
{code}

Now we create with 3 parts to the PRIMARY KEY
{code}
cqlsh:dev> drop TABLE testrev ;
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          rdate2 timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate,rdate2)
       ...          ) WITH COMPACT STORAGE
       ...          AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,rdate2,num) VALUES ('foo','2012-01-01','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
 key | rdate                    | rdate2                   | num
-----+--------------------------+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 2012-01-01 00:00:00-0500 | 10.5

cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
{code}

The last query should return the row...
",cql3,[],CASSANDRA,Bug,Normal,2012-10-03 21:23:54,2
12609853,cqlsh timestamp formatting is broken - displays wrong timezone info (at least on Ubuntu),"cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> create table ts (id int primary key, ts timestamp);
cqlsh:test> insert into ts (id, ts) values (1, '2012-05-14 07:53:20+0000');
cqlsh:test> select * from ts;
 id | ts
----+--------------------------
  1 | 2012-05-14 10:53:20+0000


Should've been 2012-05-14 10:53:20+0300.

cqlsh formats timestamps using '%Y-%m-%d %H:%M:%S%z' format-string and 'the %z escape that expands to the preferred hour/minute offset is not supported by all ANSI C libraries'. In this case it's just replaced with all zeroes.",cqlsh,[],CASSANDRA,Bug,Low,2012-10-02 02:09:37,1
12609728,doc/native_protocol.txt isn't up to date,CASSANDRA-4449 seems to have changed the datatype of the query id returned by a RESULT-PREPARED message from an {{int}} to a {{short}} n followed by n bytes (representing a md5sum). The specification at doc/native_protocol.txt doesn't cover this change yet.,binary_protocol,[],CASSANDRA,Bug,Low,2012-10-01 09:22:17,2
12609428,remove vestiges of Thrift unframed mode,"cassandra.yaml comments incorrectly imply that Cassandra can listen for unframed thrift connections by setting the frame size to zero.  But DatabaseDescriptor has this check since 0.8:

{code}
.           if (conf.thrift_framed_transport_size_in_mb <= 0)
                throw new ConfigurationException(""thrift_framed_transport_size_in_mb must be positive"");
{code}
",thrift,[],CASSANDRA,Improvement,Low,2012-09-27 18:04:07,7
12609349,"NPE with some load of writes, but possible snitch setting issue for a cluster","The following errors are showing under height load

ERROR [MutationStage:8294] 2012-09-25 22:01:47,628 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:8294,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.locator.PropertyFileSnitch.getDatacenter(PropertyFileSnitch.java:104)
	at com.datastax.bdp.snitch.DseDelegateSnitch.getDatacenter(DseDelegateSnitch.java:69)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:93)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1984)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1972)
	at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:262)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:248)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:505)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:56)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


ERROR [MutationStage:13164] 2012-09-25 22:19:06,486 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13164,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [Thrift:12] 2012-09-25 22:19:07,433 Cassandra.java (line 3462) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [Thrift:16] 2012-09-25 22:19:07,437 Cassandra.java (line 2999) Internal error processing get


java.lang.NullPointerException
 INFO [GossipStage:280] 2012-09-26 00:15:15,371 Gossiper.java (line 818) InetAddress /172.16.233.208 is now dead.
ERROR [GossipStage:280] 2012-09-26 00:15:15,372 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:280,5,main]
j

ERROR [MutationStage:40529] 2012-09-26 00:15:21,527 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:40529,5,main]
java.lang.NullPointerException
 INFO [GossipStage:281] 2012-09-26 00:15:23,013 Gossiper.java (line 818) InetAddress /172.16.232.159 is now dead.
ERROR [GossipStage:281] 2012-09-26 00:15:23,014 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:281,5,main]
",snitch,[],CASSANDRA,Bug,Low,2012-09-27 05:11:01,7
12609115,"binary protocol: when an invalid event type is watched via a REGISTER message, the response message does not have an associated stream id","I tried sending a REGISTER message with an eventlist including the string ""STATUS_FOO"", in order to test error handling in the python driver for that eventuality. But the response from the server (a ""Server error"" with a message of ""java.lang.IllegalArgumentException: No enum const class org.apache.cassandra.transport.Event$Type.STATUS_FOO"") had a stream_id of 0, so the driver was not able to associate it with the request.",binary_protocol,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-09-25 20:25:32,2
12609084,cqlsh fails to format values of ReversedType-wrapped classes,"See the test case for CASSANDRA-4715, but run it on trunk. The ReversedType-wrapped column (rdate) will be displayed as a floating-point integer (it gets deserialized into a native type correctly, but cqlsh's format-according-to-type machinery doesn't know how to handle the cqltypes.ReversedType subclass.",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2012-09-25 16:54:36,2
12608475,CQL Protocol should allow multiple PreparedStatements to be atomically executed,"Currently the only way to insert multiple records on the same partition key, atomically and using PreparedStatements is to use a CQL BATCH command. Unfortunately when doing so the amount of records to be inserted must be known prior to prepare the statement which is rarely the case. Thus the only workaround if one want to keep atomicity is currently to use unprepared statements which send a bulk of CQL strings and is fairly inefficient.

Therefore CQL Protocol should allow clients to send multiple PreparedStatements to be executed with similar guarantees and semantic as CQL BATCH command.",cql protocol,[],CASSANDRA,Improvement,Normal,2012-09-20 14:22:55,2
12608129,SlabAllocator spends a lot of time in Thread.yield,"When profiling high volume inserts into Cassandra running on a host with fast SSD and CPU, Thread.yield() invoked by SlabAllocator appeared as the top item in CPU samples. The fix is to return a regular byte buffer if current slab is being initialized by another thread. So instead of:


               if (oldOffset == UNINITIALIZED)
                {
                    // The region doesn't have its data allocated yet.
                    // Since we found this in currentRegion, we know that whoever
                    // CAS-ed it there is allocating it right now. So spin-loop
                    // shouldn't spin long!
                    Thread.yield();
                    continue;
                }

do:

if (oldOffset == UNINITIALIZED)
    return ByteBuffer.allocate(size);

I achieved 4x speed up in my (admittedly specialized) benchmark by using an optimized version of SlabAllocator attached. Since this code is in the critical path, even doing excessive atomic instructions or allocating unneeded extra ByteBuffer instances has a measurable effect on performance
",performance qa-resolved,[],CASSANDRA,Bug,Low,2012-09-18 20:50:29,7
12607992,cqlsh COPY TO and COPY FROM don't work with cql3,cqlsh COPY TO and COPY FROM don't work with cql3 due to previous cql3 changes.,cqlsh,[],CASSANDRA,Bug,Normal,2012-09-18 01:06:51,1
12607257,cql version race condition with rpc_server_type: sync,"If clients connect to a cassandra cluster configured with rpc_server_type: sync with heterogeneous cql versions (2 and 3), the cql version used for execution on the server changes seemingly randomly.
It's due to the fact that CustomTThreadPoolServer.java does not set the remoteSocket anytime, or does not clear the cql version in the ThreadLocal clientState object.
When CassandraServer.java calls state() it gets the ThreadLocal object clientState, which has its cqlversion already changed by a previous socket that was using the same thread.


The easiest fix is probably to do a SocketSessionManagementService.instance.set when accepting a new client and SocketSessionManagementService.instance.remove when the client is closed, but if you really want to use the ThreadLocal clientState and not alloc/destroy a ClientState everytime, then you should clear this clientState on accept of a new client.

The problem can be reproduced with cqlsh -3 on one side and a client that does not set the cql version, expecting to get version 2 by default, but actually gettingv v2/v3 depending on which thread it connects to.

The problem does not happen with other rpc_server_types, nor with clients that set their cql version at connection.",features,[],CASSANDRA,Bug,Low,2012-09-12 15:10:15,7
12607241,Truncate operation doesn't delete rows from HintsColumnFamily.,"Steps to reproduce:
1. Start writing of data to some column family, let name it 'MyCF'
2. Stop 1 node
3. Wait some time (until some data will be collected in HintsColumnFamily)
4. Start node (HintedHandoff will be started automatically for 'MyCF')
5. Run 'truncate' command for 'MyCF' column family from command from cli
6. Wait until truncate will be finished
7. You will see that 'MyCF' is not empty because HintedHandoff is copying data 

So, I suggest to clean HintsColumnFamily (for truncated column family) before we had started to discard sstables. 
I think it should be done in CompactionManager#submitTrucate() method. I can try to create patch but I need to know right way of cleaning HintsColumnFamily. Could you clarify it?
",hintedhandoff truncate,[],CASSANDRA,Bug,Low,2012-09-12 13:48:11,7
12607145,"Cassandra 1.2 should not accept CQL version ""3.0.0-beta1""","During Cassandra 1.1's whole lifecycle, the CREATE KEYSPACE syntax was pretty dramatically and incompatibly different from what is there now for 1.2. That's ok, since we explicitly said there could be breaking changes in the syntax before 3.0.0 final, but at least we should make it clear that 3.0.0 is not compatible with the 3.0.0-beta1 syntax we had out for quite a while.

If we don't want to reject connections asking for 3.0.0-beta1, we should bump the version number to 3.0.1 or something.",cql3,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-09-11 23:10:17,2
12607116,Unable to start Cassandra with simple authentication enabled,"I followed the steps for enabling simple authentication as described here, http://www.datastax.com/docs/1.1/configuration/authentication. I tried starting Cassandra with, 

cassandra -f -Dpasswd.properties=conf/passwd.properties -Daccess.properties=conf/access.properties

Start up failed with this exception in my log:

ERROR [main] 2012-09-11 15:03:04,642 CassandraDaemon.java (line 403) Exception encountered during startup
java.lang.AssertionError: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:136)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:298)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:386)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:429)
Caused by: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.service.ClientState.validateLogin(ClientState.java:254)
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:235)
        at org.apache.cassandra.cql3.statements.SelectStatement.checkAccess(SelectStatement.java:105)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:106)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:124)
        ... 4 more",security,[],CASSANDRA,Bug,Normal,2012-09-11 19:39:37,2
12606412,ORDER BY validation is not restrictive enough,"We're not able to do order by on anything that is a key range. However, we only refuse queries that have an empty where clause, but that doesn't exclude all key ranges at all.",cql3,[],CASSANDRA,Bug,Low,2012-09-06 14:51:17,2
12606102,Add AlterKeyspace statement,"Somehow we never added an ""ALTER KEYSPACE"" statement. We should.",cql3,['Legacy/CQL'],CASSANDRA,New Feature,Low,2012-09-04 15:55:30,2
12605940,use Map internally in schema_ tables where appropriate,"{replication, compression, compaction}_parameters should be stored as Map type.",cql3,['Legacy/CQL'],CASSANDRA,Improvement,Low,2012-09-03 01:48:42,1
12605495,StackOverflowError in LeveledCompactionStrategy$LeveledScanner.computeNext,"while running nodetool repair, the following was logged in system.log:


ERROR [ValidationExecutor:2] 2012-08-30 10:58:19,490 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.StackOverflowError
        at sun.nio.cs.UTF_8.updatePositions(UTF_8.java:76)
        at sun.nio.cs.UTF_8$Encoder.encodeArrayLoop(UTF_8.java:411)
        at sun.nio.cs.UTF_8$Encoder.encodeLoop(UTF_8.java:466)
        at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
        at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:258)
        at java.lang.StringCoding.encode(StringCoding.java:290)
        at java.lang.String.getBytes(String.java:954)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
        at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
.

(about 900 lines deleted)
.


        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:202)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:90)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:47)
        at org.apache.cassandra.db.compaction.CompactionIterable.iterator(CompactionIterable.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:703)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:442)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",compaction,[],CASSANDRA,Bug,Low,2012-08-30 09:20:55,7
12605133,CQL queries using LIMIT sometimes missing results,"In certain conditions, CQL queries using LIMIT clauses are not being given all of the expected results (whether unset column values or missing rows).

Here are the condition sets I've been able to identify:

First mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. Conditions:

 * Table has a multi-component primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

Second mode: result has fewer rows than it should, lower than both the LIMIT and the actual number of matching rows. Conditions:

 * Table has a single-column primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

It would make sense to me that this would have started with CASSANDRA-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing CASSANDRA-3647.

Test case for the first failure mode:

{noformat}
DROP KEYSPACE test;

CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int,
    b int,
    c int,
    d int,
    e int,
    PRIMARY KEY (a, b)
);

INSERT INTO testcf (a, b, c, d, e) VALUES (1, 11, 111, 1111, 11111);
INSERT INTO testcf (a, b, c, d, e) VALUES (2, 22, 222, 2222, 22222);
INSERT INTO testcf (a, b, c, d, e) VALUES (3, 33, 333, 3333, 33333);
INSERT INTO testcf (a, b, c, d, e) VALUES (4, 44, 444, 4444, 44444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- columns d and e in result row are null
SELECT * FROM testcf LIMIT 2; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 3; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 4; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 5; -- results are correct (4 rows returned)
{noformat}

Test case for the second failure mode:

{noformat}
CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int primary key,
    b int,
    c int,
);

INSERT INTO testcf (a, b, c) VALUES (1, 11, 111);
INSERT INTO testcf (a, b, c) VALUES (2, 22, 222);
INSERT INTO testcf (a, b, c) VALUES (3, 33, 333);
INSERT INTO testcf (a, b, c) VALUES (4, 44, 444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- gives 1 row
SELECT * FROM testcf LIMIT 2; -- gives 1 row
SELECT * FROM testcf LIMIT 3; -- gives 2 rows
SELECT * FROM testcf LIMIT 4; -- gives 2 rows
SELECT * FROM testcf LIMIT 5; -- gives 3 rows
{noformat}",cql cql3,[],CASSANDRA,Bug,Normal,2012-08-27 20:51:01,2
12603683,Mutation response(WriteResponse.java) could be smaller and not contain keyspace and key,"In the mutation response, WriteResponse.java object is send back to the co-ordinator. This object has keyspace and key in it which is not required. It is not being used at the co-ordiantor. 

This wastes IO specially in case of WAN links between DC. Also since response from each node in multi-DC deployments goes back to the co-ordinator in another DC makes it even worse. 

It also becomes worse if the the keyspace and key are of large size and the data is small. In that case, a node which is not the co-ordinator and purely receiving mutations, the outbound n/w bandwidth could be half of incoming bandwidth.  ",network,[],CASSANDRA,Improvement,Low,2012-08-16 03:16:49,7
12603419,potential backwards incompatibility in native protocol,"In the text of the native_protocol.spec document, it explains the format for a notation called {{[option]}}, which should represent ""{{a pair of <id><value>}}"".

In doing a first-draft implementation of the protocol for the python driver, though, I found that I had a misunderstanding. I read that section as saying that {{<value>}} was a {{[value]}}, and that it could have a length of 0 (i.e., the {{[int]}} on the front of the {{[value]}} could be 0). However, it turns out that {{<value>}} might not be there at all, or might be *two* {{[value]}}'s, depending on the option id and message context.

I'm not a fan of this, since

 * A protocol parsing library can't simply implement a single function to read in {{[option]}}'s, since the length of the value part is dependent on the message context
 * If we add a new native data type (a new option id which could be used inside a {{<col_spec_i>}} in a RESULT message), then older clients will not know how to read past the value part. Of course they won't know how to interpret the data or deserialize later rows of that unknown type - that's not the problem - the problem is that the older client should be able just to mark that column as unparseable and still handle the rest of the columns.

Can we make {{<value>}} be a {{[value]}}, the contents of which can be re-interpreted as a {{[string]}}, an {{[option]}}, two {{[option]}}'s, or whatever?",cql native_protocol,['Legacy/CQL'],CASSANDRA,Improvement,Low,2012-08-14 09:06:08,2
12603307,Ability for CQL3 to list partition keys,"It can be useful to know the set of in-use partition keys (storage engine row keys).  One example given to me was where application data was modeled as a few 10s of 1000s of wide rows, where the app required presenting these rows to the user sorted based on information in the partition key.  The partition count is small enough to do the sort client-side in memory, which is what the app did with the Thrift API--a range slice with an empty columns list.

This was a problem when migrating to CQL3.  {{SELECT mykey FROM mytable}} includes all the logical rows, which makes the resultset too large to make this a reasonable approach, even with paging.

One way to add support would be to allow DISTINCT in the special case of {{SELECT DISTINCT mykey FROM mytable}}.",cql3,['Legacy/CQL'],CASSANDRA,New Feature,Low,2012-08-13 18:58:24,1
12603138,NPE when trying to select a slice from a composite table,"I posted this question on StackOverflow, because i need a solution. 

Created a table with :

{noformat}
create table compositetest(m_id ascii,i_id int,l_id ascii,body ascii, PRIMARY KEY(m_id,i_id,l_id));
{noformat}

wanted to slice the results returned, so did something like below, not sure if its the right way. The first one returns data perfectly as expected, second one to get the next 3 columns closes the transport of my cqlsh

{noformat}
cqlsh:testkeyspace1> select * from compositetest where i_id<=3 limit 3;
 m_id | i_id | l_id | body
------+------+------+------
   m1 |    1 |   l1 |   b1
   m1 |    2 |   l2 |   b2
   m2 |    1 |   l1 |   b1

cqlsh:testkeyspace1> Was trying to write something for slice range.

TSocket read 0 bytes
{noformat}

Is there a way to achieve what I am doing here, it would be good if some meaning ful error is sent back, instead of cqlsh closing the transport.

On the server side I see the following error.

{noformat}
ERROR [Thrift:3] 2012-08-12 15:15:24,414 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql3.statements.SelectStatement$Restriction.setBound(SelectStatement.java:1277)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1151)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1001)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:215)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

With ThriftClient I get :

{noformat}
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql_query(Cassandra.java:1402)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_cql_query(Cassandra.java:1388)
{noformat}",Slice cql cql3,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-08-12 19:41:01,2
12601437,"update cqlsh recognized syntax (for tab completion, etc)","cqlsh syntax awareness (tab completion, etc) should be aware of several recent changes to CQL:

 * CASSANDRA-4179 (row key and column value composites; 1.2 only)
 * CASSANDRA-3647 (sets/lists/maps; 1.2 only)
 * CASSANDRA-4018 (inet type; 1.2 only)
 * CASSANDRA-4278 (hyphens in keyspace properties; 1.1 and up)
 * CASSANDRA-4217 (accessing ttl, timestamp; 1.1 and up)",cql3 cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2012-08-03 22:41:40,1
12600923,cql3: defining more than one pk should be invalid,"dtests caught this on trunk:

{noformat}
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 277, in create_invalid_test
    assert_invalid(cursor, ""CREATE TABLE test (key1 text PRIMARY KEY, key2 text PRIMARY KEY)"")
  File ""/var/lib/buildbot/cassandra-dtest/assertions.py"", line 31, in assert_invalid
    assert False, ""Expecting query to be invalid""
AssertionError: Expecting query to be invalid
{noformat}",cql3,[],CASSANDRA,Bug,Normal,2012-07-31 22:17:44,2
12600870,Using 'key' as primary key throws exception when using CQL2,"When I run following CQL on trunk, throws exception (only in CQL2). This statement used to work and I think something is broken after CASSANDRA-4179.

{code}
CREATE TABLE Standard1 (key ascii PRIMARY KEY, c0 ascii);
{code}

Exception is:

{code}
ERROR [Thrift:1] 2012-07-31 09:54:02,585 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:166)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:123)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:49)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:45)
        at org.apache.cassandra.cql3.CFDefinition.getKeyId(CFDefinition.java:167)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:81)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1382)
        at org.apache.cassandra.config.CFMetaData.keyAliases(CFMetaData.java:235)
        at org.apache.cassandra.cql.CreateColumnFamilyStatement.getCFMetaData(CreateColumnFamilyStatement.java:170)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:692)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:846)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}",cql,[],CASSANDRA,Bug,Low,2012-07-31 15:01:47,2
12599435,"CQL3: Allow preparing the consistency level, timestamp and ttl","It could be useful to allow the preparation of the consitency level, the timestamp and the ttl. I.e. to allow:
{noformat}
UPDATE foo SET .. USING CONSISTENCY ? AND TIMESTAMP ? AND TTL ? 
{noformat}

A slight concern is that when preparing a statement we return the names of the prepared variables, but none of timestamp, ttl and consistency are reserved names currently, so returning those as names could conflict with a column name. We can either:
* make these reserved identifier (I have to add that I'm not a fan because at least for ""timestamp"", I think that's a potentially useful and common column name).
* use some specific special character to indicate those are not column names, like returning ""[timestamp]"", ""[ttl]"", ""[consistency]"".",cql3,[],CASSANDRA,Improvement,Low,2012-07-19 09:50:50,2
12599426,Make prepared statement global rather than connection based,"Currently, prepared statements are connection based. A client can only use a prepared statement on the connection it prepared it on, and if you prepare the same prepared statement on multiple connections, we'll keep multiple times the same prepared statement. This is potentially inefficient but can also be fairly painful for client libraries with pool of connections (a.k.a all reasonable client library ever) as this means you need to make sure you prepare statement on every connection of the pool, including the connection that don't exist yet but might be created later.

This ticket suggests making prepared statement global (at least for CQL3), i.e. move them out of ClientState. This will likely reduce the number of stored statement on a given node quite a bit, since it's very likely that all clients to a given node will prepare the same statements (and potentially on all of their connection with the node). And given that prepared statement identifiers are the hashCode() of the string, this should be fairly trivial.

I will note that while I think using a hash of the string as identifier is a very good idea, I don't know if the default java hashCode() is good enough. If that's a concern, maybe we should use a safer (bug longer) hash like md5 or sha1. But we'd better do that now.",binary_protocol,[],CASSANDRA,Improvement,Normal,2012-07-19 09:00:04,2
12599416,CQL3: allow to define a per-cf default consistency level,"One of the goal of CQL3 is that client library should not have to parse queries to provide a good experience. In particular, that means such client (that don't want to parse queries) won't be able to allow the user to define a specific default read/write consistency level per-CF, forcing user to specific the consistency level with every query, which is not very user friendly.

This ticket suggests the addition of per-cf default read/write consitency level. Typically the syntax would be:
{noformat}
CREATE TABLE foo (...)
WITH DEFAULT_READ_CONSISTENCY = QUORUM
 AND DEFAULT_WRITE_CONSISTENCY = QUORUM
{noformat}",cql3,[],CASSANDRA,New Feature,Normal,2012-07-19 07:47:28,2
12597852,auto completion in cqlsh should work when using fully qualified name,"minor cqlsh improvement:

the auto completion in cqlsh rocks, so this is just to make a nitpick improvement: 

if i type
{panel}
cqlsh> create KEYSPACE test WITH strategy_class = 'SimpleStrategy' and 
{panel}

then tab tab after it, it will auto complete into:
{panel}
cqlsh> create KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 
{panel}

but if i use a fully qualified name:
{panel}
cqlsh> create KEYSPACE test WITH strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' AND 
                        <strategy_option_name> 
{panel}

it is not smart enough to figured out the available options.

It'd be nice to make the auto completion works for those fully qualified cases. ",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2012-07-06 23:22:15,1
12597801,Humor 32bit JVMs,"The commitlog rewrite for 1.1 uses mmap, anticipating multithreaded commitlog writes.  However, the default commitlog settings will quickly exhaust a 32bit address space.",commitlog,[],CASSANDRA,Improvement,Low,2012-07-06 16:36:51,7
12597658,Include metadata for system keyspace itself in schema_* tables,"The `system.schema_keyspaces`, `system.schema_columnfamilies`, and `system.schema_columns` virtual tables allow clients to query schema and layout information through CQL. This will be invaluable when users start to make more use of the CQL-only protocol (CASSANDRA-2478), since there will be no other way to determine certain information about available columnfamilies, keyspaces, or show metadata about them.

However, the system keyspace itself, and all the columnfamilies in it, are not represented in the schema_* tables:

{noformat}
cqlsh> select * from system.schema_keyspaces where ""keyspace"" = 'system';
cqlsh> 
cqlsh> select * from system.schema_columnfamilies where ""keyspace"" = 'system';
cqlsh> 
cqlsh> select * from system.schema_columns where ""keyspace"" = 'system';
cqlsh> 
{noformat}

It would be greatly helpful to clients which do more introspection than the minimum (say, for example, cqlsh) to be able to get information on the structure and availability of schema-definition tables.",cql cql3,[],CASSANDRA,Improvement,Low,2012-07-05 21:42:43,7
12597608,Add cursor API/auto paging to the native CQL protocol,"The goal here would be to use a query paging mechanism to the CQL native protocol. Typically the client/server with that would look something like this:
{noformat}
C sends query to S.
S sends N first rows matching the query + flag saying the response is not complete
C requests the next N rows
S sends N next rows + flag saying whether there is more
C requests the next N rows
...
S sends last rows + flag saying there is no more result
{noformat}

The clear goal is for user to not have to worry about limiting queries and doing manual paging.",cql protocol,[],CASSANDRA,New Feature,Normal,2012-07-05 16:45:58,2
12597524,Assertion with LCS compaction,"As instructed in CASSANDRA-4321 I have raised this issue as a continuation of that issue as it appears the problem still exists.

I have repeatedly run sstablescrub across all my nodes after the 1.1.2 upgrade until sstablescrub shows no errors.  The exceptions described in CASSANDRA-4321 do not occur as frequently now but the integrity check still throws exceptions on a number of nodes.  Once those exceptions occur compactionstats shows a large number of pending tasks with no progression afterwards.

{code}
ERROR [CompactionExecutor:150] 2012-07-05 04:26:15,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:150,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
",lcs,[],CASSANDRA,Bug,Normal,2012-07-05 04:52:18,2
12596836,cleanup uses global partitioner to estimate ranges in index sstables,"Introduced in CASSANDRA-1404, CleanupTest is showing this on trunk (on stderr, so test doesn't actually fail):

{noformat}
    [junit] java.lang.ClassCastException: org.apache.cassandra.dht.Token$KeyBound cannot be cast to org.apache.cassandra.dht.Token
    [junit]     at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:24)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:386)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:383)
    [junit]     at java.util.Arrays.mergeSort(Arrays.java:1270)
    [junit]     at java.util.Arrays.sort(Arrays.java:1210)
    [junit]     at java.util.Collections.sort(Collections.java:159)
    [junit]     at org.apache.cassandra.dht.Range.normalize(Range.java:382)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:570)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:549)
    [junit]     at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:111)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:136)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't happen on the 1.1 branch (less robust test?) but the problem is still there.",compaction,[],CASSANDRA,Bug,Low,2012-07-02 22:19:20,7
12596424,Subcolumns not removed when compacting tombstoned super column,"When we compact a tombstone for a super column with the old data for that super column, we end up writing the deleted super column and all the subcolumn data that is now worthless to the new sstable. This is especially inefficient when reads need to scan tombstones during a slice.

Here is the output of a simple test I ran to confirm:

insert supercolumn, then flush
{noformat}
Nicks-MacBook-Pro:12:20:52 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-1-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": -9223372036854775808, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
{noformat}

delete supercolumn, flush again

{noformat}
[Nicks-MacBook-Pro:12:20:59 cassandra-1.0] cassandra$ bin/nodetool -h localhost flush
[Nicks-MacBook-Pro:12:22:41 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-2-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": []}}
}
{noformat}

compact and check resulting sstable

{noformat}
[Nicks-MacBook-Pro:12:22:55 cassandra-1.0] cassandra$ bin/nodetool -h localhost compact 
[Nicks-MacBook-Pro:12:23:09 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-3-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
[Nicks-MacBook-Pro:12:23:20 cassandra-1.0] cassandra$ 
{noformat}",compaction,[],CASSANDRA,Bug,Low,2012-06-29 17:34:59,7
12595941,cleanup optimization can delete data but not corresponding index entries,introduced by CASSANDRA-4079,compaction,[],CASSANDRA,Bug,Normal,2012-06-26 17:18:15,7
12595805,CQL3 Range Query contains unwanted results with composite columns,"Here is a CQL3 range query sample where I get wrong results (tested using cqlsh --cql3) from my perspective:

CREATE KEYSPACE testing WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;

USE testing;

CREATE TABLE bug_test (a int, b int, c int, d int, e int, f text, PRIMARY KEY (a, b, c, d, e) );

INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 2, '2');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 2, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 3, '3');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 5, '5');

----------

Normal select everything query:

SELECT * FROM bug_test;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 1 | 1
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Everything fine so far.

----------

Select with greater equal comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e >= 2;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

----------

Select with greater comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e > 2;

Results:
 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

The same issue is also present with between ranges (e >= 1 AND e <= 2)...",bug composite compositeColumns cql3 query range,['Legacy/CQL'],CASSANDRA,Bug,Normal,2012-06-25 16:33:23,2
12595422,Compaction invalidates row cache,"Compactions invalidate row cache after CASSANDRA-3862

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionIterable.java#L87",compaction,[],CASSANDRA,Bug,Low,2012-06-21 12:29:51,7
12595306,cqlsh can't display reversed type values properly,"Here is table and data:

CREATE TABLE t1 (
  a int,
  b bigint,
  c varchar,
  d varchar,
  PRIMARY KEY (a,b,c)
) WITH CLUSTERING ORDER BY (b DESC, c DESC);

INSERT INTO db.t1 (a,b,c,d)  VALUES (1,10,'u1','s1');
INSERT INTO db.t1 (a,b,c,d)  VALUES (1,15,'u1','d1');
INSERT INTO db.t1 (a,b,c,d)  VALUES (1,21,'u3','ghfgh f1g');
INSERT INTO db.t1 (a,b,c,d)  VALUES (1,31,'u2','1gh');
INSERT INTO db.t1 (a,b,c,d)  VALUES (1,41,'u3','fgh1');

And here's the query

cqlsh:db> SELECT * FROM t1;
 a | b                                | c  | d
---+----------------------------------+----+-----------
 1 |    \x00\x00\x00\x00\x00\x00\x00) | u3 |      fgh1
 1 | \x00\x00\x00\x00\x00\x00\x00\x1f | u2 |       1gh
 1 | \x00\x00\x00\x00\x00\x00\x00\x15 | u3 | ghfgh f1g
 1 | \x00\x00\x00\x00\x00\x00\x00\x0f | u1 |        d1
 1 |   \x00\x00\x00\x00\x00\x00\x00\n | u1 |        s1


As you can see, cqlsh can't display reversed type values properly ...",cqlsh,['Legacy/Tools'],CASSANDRA,Bug,Low,2012-06-20 17:38:55,1
12595294,CQL3: allow definition with only a PK,"Currently, in CQL3 and contrarily to SQL, one cannot define a table having only a PK but no other columns. Related to that, a CQL always needs at least one column outside of the PK to be inserted to exist. All that may force people to add 'fake' value that they don't really need.

The goal of this ticket is to lift that limitation and allow table definition to have only a PK, and to have CQL rows exist even if only the PK has been inserted (in other words, to have CQL rows behave like SQL rows).

Following CASSANDRA-4329, one way to do that with the sparse-composite encoding CQL3 uses would be to insert as marker of the CQL row presence a CQL column with an empty name (the underlying column name won't be empty though since it's a composite). The drawback though is that we will need to insert that marker with every insert to the CQL row (in other word, we'll add a slight overhead to the size of each write). The pros is that if we have such marker for the CQL row presence, we will be able to reoptimize back queries that select only a few columns (since following CASSANDRA-3982 we query all columns of a CQL row every time).
",cql3,['Legacy/CQL'],CASSANDRA,Improvement,Normal,2012-06-20 16:50:41,2
12560663,Small SSTable Segments Can Hurt Leveling Process,"This concerns:

static int MAX_COMPACTING_L0 = 32;

Repair can create very small SSTable segments. We should consider moving to a threshold that takes into account the size of the files brought into compaction rather than the number of files for this and similar situations. Bringing the small files from L0 to L1 magnifies the issue.

If there are too many very small files in L0 perhaps even an intermediate compaction would even reduce the magnifying effect of a L0 to L1 compaction.

",compaction lcs,[],CASSANDRA,Improvement,Low,2012-06-14 16:39:04,7
12560470,Data insertion fails because of commitlog rename failure,"h3. Configuration
Cassandra server configuration:
{noformat}heap size: 4 GB
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: ""xxx.xxx.xxx.10,xxx.xxx.xxx.11""
listen_address: xxx.xxx.xxx.10
rpc_address: 0.0.0.0
rpc_port: 9160
rpc_timeout_in_ms: 20000
endpoint_snitch: PropertyFileSnitch{noformat}

cassandra-topology.properties
{noformat}xxx.xxx.xxx.10=datacenter1:rack1
xxx.xxx.xxx.11=datacenter1:rack1
default=datacenter1:rack1{noformat}

Ring configuration:
{noformat}Address         DC          Rack        Status State   Load            Effective-Ownership Token
                                                                                           85070591730234615865843651857942052864
xxx.xxx.xxx.10  datacenter1 rack1       Up     Normal  23,11 kB        100,00%             0
xxx.xxx.xxx.11  datacenter1 rack1       Up     Normal  23,25 kB        100,00%             85070591730234615865843651857942052864{noformat}

h3.Problem
I have ctreated keyspace and column family using CLI commands:
{noformat}create keyspace testks with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options = {datacenter1:2};
use testks;
create column family testcf;{noformat}

Then I started my Java application, which inserts 50 000 000 rows to created column family using Hector client. Client is connected to node 1.
After about 30 seconds (160 000 rows were inserted) Cassandra server on node 1 throws an exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-13 10:26:38,393 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}
	
Then, few seconds later Cassandra server on node 2 throws the same exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-14 10:26:44,005 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}

After that, my application cannot insert any more data. Hector gets TimedOutException from Thrift:
{noformat}Thread-4 HConnectionManager.java 306 2012-06-14 10:26:56,034 HConnectionManager  operateWithFailover 	 WARN  	 %Could not fullfill request on this host CassandraClient<xxx.xxx.xxx.10:9160-10> 
Thread-4 HConnectionManager.java 307 2012-06-14 10:26:56,034 HConnectionManager operateWithFailover 	 WARN  	 %Exception:  
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:35)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)
	at patrycjusz.nosqltest.db.cassandra.CassandraHectorDbAdapter.commitTransaction(CassandraDbAdapter.java:63)
	at patrycjusz.nosqltest.DbTest.insertData(DbTest.java:459)
	at patrycjusz.nosqltest.gui.InsertPanel.executeTask(NePanel.java:154)
	at patrycjusz.nosqltest.gui.InsertPanel$1.run(NePanel.java:141)
	at java.lang.Thread.run(Unknown Source)
Caused by: TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20269)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:922)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:908)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:103)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:258)
	... 8 more{noformat}",commitlog,[],CASSANDRA,Bug,Normal,2012-06-13 09:34:53,7
12560158,CQL3: Always use composite types by default,"Currently, when defining a table with a single (non-composite) PRIMARY KEY, we don't use a CompositeType in the underlying comparator. This is however a problem for CASSANDRA-3647 as this means those tables cannot use collections.  So this ticket suggests to change that default behavior, and to always use (by default at least, see below) a composite comparator underneath. I'll note that doing so will mean an overhead of 3 bytes per column for non-composite columns, but I believe getting collection is well worth it.

Of course the suggestion above apply to the default behavior and this ticket would also add an option to table creation to get back to the current behavior of not using a composite comparator (if ony for backward compatibility sake).  And I believe that we can actually reuse 'COMPACT STORAGE' for that.
",cql3,[],CASSANDRA,Improvement,Normal,2012-06-11 12:22:27,2
12559783,stackoverflow building interval tree & possible sstable corruptions,"After upgrading to 1.1.1 (from 1.1.0) I have started experiencing StackOverflowError's resulting in compaction backlog and failure to restart. 

The ring currently consists of 6 DC's and 22 nodes using LCS & compression.  This issue was first noted on 2 nodes in one DC and then appears to have spread to various other nodes in the other DC's.  

When the first occurrence of this was found I restarted the instance but it failed to start so I cleared its data and treated it as a replacement node for the token it was previously responsible for.  This node successfully streamed all the relevant data back but failed again a number of hours later with the same StackOverflowError and again was unable to restart. 

The initial stack overflow error on a running instance looks like this:

ERROR [CompactionExecutor:314] 2012-06-07 09:59:43,017 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:314,1,main]
java.lang.StackOverflowError
        at java.util.Arrays.mergeSort(Arrays.java:1157)
        at java.util.Arrays.sort(Arrays.java:1092)
        at java.util.Collections.sort(Collections.java:134)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow.  Compactions stop from this point onwards]


I restarted this failing instance with DEBUG logging enabled and it throws the following exception part way through startup:

ERROR 11:37:51,046 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.db.Table$2.apply(Table.java:574)
        at org.apache.cassandra.db.Table$2.apply(Table.java:571)
        at com.google.common.collect.Iterators$8.next(Iterators.java:751)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG 11:37:51,052 Initializing ksU.cfS


And then finally fails with the following:

DEBUG 11:49:03,752 Creating IntervalNode from [Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b))]
java.lang.reflect.InvocationTargetException
DEBUG 11:49:03,753 Configured datacenter replicas are dc1:2, dc2:2, dc3:2, dc4:2, dc5:0, dc6:2, dc7:0, dc8:0, dc9:2
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3

Running with assertions enabled allows me to start the instance but when doing so I get errors such as:

ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)

and:

ERROR 01:27:58,946 Exception in thread Thread[CompactionExecutor:9,1,main]
java.lang.AssertionError: Last written key DecoratedKey(81958437188197992567937826278457419048, 4fa1aebad23f81e4321d344d) >= current key DecoratedKey(64546479828744423263742604083767363606, 4fcafc0f19f6a8092d4d4f94) writing into /var/lib/XX/data/cassandra/ks1/cf1/ks1-cf1-tmp-hd-657317-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Just like the initial errors compactions appear to stop occurring after this point.  

Given the above this looks like sstables are getting corrupted.  By restarting nodes I am able to identify several hundred sstables exhibiting the same problem and this appears to be growing.

I have tried scrubbing those affected nodes but the problem continues to occur.  If this is due to sstable corruptions is there another way of validating sstables for correctness?  Given that it has spread to various servers in other DC's it looks like this is directly related to the 1.1.1 upgrade recently performed on the ring.",lcs,[],CASSANDRA,Bug,Normal,2012-06-08 02:00:49,2
12559766,Nodetool compactionstats fails with NullPointerException,"Test uses Column family C defined as follows:

create column family C with caching = 'keys_only' and key_validation_class = 'LongType' and compression_options = { sstable_compression: SnappyCompressor, chunk_length_kb: 64 } and max_compaction_threshold=0; 

max_compaction_threshold is set to 0 to disable auto compaction.

SSTables are streamed via sstableloader, after which a major compaction is triggered using ""nodetool compact MyKeyspace C"".

Thereafter, attempts to request compaction stats via ""nodetool compactionstats"" fail with the following exception:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.compaction.CompactionInfo.asMap(CompactionInfo.java:103)
        at org.apache.cassandra.db.compaction.CompactionManager.getCompactions(CompactionManager.java:1115)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662) ",caching,[],CASSANDRA,Bug,Low,2012-06-07 22:25:03,7
12559668,Compaction Throttle too bursty with large rows,"In org.apache.cassandra.db.compaction.CompactionIterable the check for compaction throttling occurs once every 1000 rows. In our workload this is much too large as we have many large rows (16 - 100 MB).

With a 100 MB row, about 100 GB is read (and possibly written) before the compaction throttle sleeps. This causes bursts of essentially unthrottled compaction IO followed by a long sleep which yields inconsistence performance and high error rates during the bursts.

We applied a workaround to check throttle every row which solved our performance and error issues:

line 116 in org.apache.cassandra.db.compaction.CompactionIterable:
                if ((row++ % 1000) == 0)
replaced with
                if ((row++ % 1) == 0)

I think the better solution is to calculate how often throttle should be checked based on the throttle rate to apply sleeps more consistently. E.g. if 16MB/sec is the limit then check for sleep after every 16MB is read so sleeps are spaced out about every second.

",qa-resolved,[],CASSANDRA,Improvement,Normal,2012-06-07 10:57:25,7
12559479,clean up messagingservice protocol limitations,"Weaknesses of the existing protocol:

- information asymmetry: node A can know what version node B expects, but not vice versa (see CASSANDRA-4101)
- delayed information: node A will often not know what version node B expects, until after first contacting node B -- forcing it to throw that first message away and retry for the next one
- protocol cannot handle both cross-dc forwarding and broadcast_address != socket address (see bottom of CASSANDRA-4099)
- version is partly global, partly per-connection, and partly per-message, resulting in some interesting hacks (CASSANDRA-3166) and difficulty layering more sophisticated OutputStreams on the socket (CASSANDRA-3127, CASSANDRA-4139)",jmx,[],CASSANDRA,Bug,Normal,2012-06-06 02:18:57,7
12558177,SizeTieredCompactionStrategy.getBuckets is quadradic in the number of sstables,"getBuckets first sorts the sstables by size (N log N) then adds each sstable to a bucket (N**2 in the worst case of all sstables the same size, because we use the bucket's contents as a hash key).",compaction,[],CASSANDRA,Bug,Low,2012-05-25 19:47:30,7
12558112,CQL3: dates are not handled correctly in slices ,"Our timestamp type allows to input timestamp as dates like '2012-06-06'. However, those don't work as expected in slice queries, as for instance:
{noformat}
SELECT * FROM timeline
  WHERE k = ...
  AND time > '2012-06-06'
  AND time <= '2012-06-09'
{noformat}
will return timestamps from '2012-06-06' and not those from '2012-06-09'. The reason being of course that we always translate a date the same way, using 0 for whichever part is not precised.

A reasonably simple fix could be to add a new fromString(String s, boolean gt) method to AbstractType that is used when the the string should be interpreted in an inequality (the boolean gt would then say which kind of inequality).
",cql3,['Legacy/CQL'],CASSANDRA,Bug,Normal,2012-05-25 14:35:28,2
12557067,kick off background compaction when min/max changed,"When the threshold changes, we may be eligible for a compaction immediately (without waiting for a flush to trigger the eligibility check).",compaction,[],CASSANDRA,Bug,Low,2012-05-23 20:51:04,7
12557052,Can't specify certain keyspace properties in CQL,"A user using EC2MultiRegionSnitch, where the datacenter name has to match the AWS region names, will not be able to specify a keyspace's replica counts for those datacenters using CQL. AWS region names contain hyphens, which are not valid identifiers in CQL, and CQL keyspace/columnfamily properties must be identifiers or identifiers separated by colons.

Example:

{noformat}
CREATE KEYSPACE Foo
  WITH strategy_class = 'NetworkTopologyStrategy'
      AND strategy_options:""us-east""=1
      AND strategy_options:""us-west""=1;
{noformat}

(see http://mail-archives.apache.org/mod_mbox/cassandra-user/201205.mbox/browser for context)

..will not currently work, with or without the double quotes.

CQL should either allow hyphens in COMPIDENT, or allow quoted parts of a COMPIDENT token.",cql cql3,[],CASSANDRA,Bug,Low,2012-05-23 19:24:13,2
12556274,CQL3 range query with secondary index fails,"This query fails:
select * from indextest where setid = 0 and row < 1;
when there's a secondary index on 'setid'; row isn't the primary key.

{code:title=CQL3}
bin$ ./cqlsh --cql3
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> create table indextest (id int primary key, row int, setid int);
cqlsh:warehouse1> create index indextest_setid_idx on indextest (setid);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (0, 0, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (1, 1, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (2, 2, 0);
cqlsh:warehouse1> select * from indextest where setid = 0;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
  2 |   2 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row = 1;
 id | row | setid
----+-----+-------
  1 |   1 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
TSocket read 0 bytes
{code}

{code:title=Error message}
ERROR 13:36:23,544 Error occurred during processing of message.
java.lang.NullPointerException
  at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:546)
  at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:253)
  at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
  at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
  at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
  at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
  at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
{code}

Works fine in CQL2:
{code:title=CQL2}
bin$ ./cqlsh_uuid --cql2
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
 id | row | setid
----+-----+-------
  0 |   0 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 2;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
{code}",cql3 index,"['Feature/2i Index', 'Legacy/CQL']",CASSANDRA,Bug,Low,2012-05-17 19:06:45,2
12556268,concurrent modif ex when repair is run on LCS,"came across this, will try to figure a way to systematically reprod this. But the problem is the sstable list in the manifest is changing as the repair is triggered:

{panel}
Exception in thread ""main"" java.util.ConcurrentModificationException 
 at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
 at java.util.AbstractList$Itr.next(Unknown Source)
 at org.apache.cassandra.io.sstable.SSTable.getTotalBytes(SSTable.java:250)
 at org.apache.cassandra.db.compaction.LeveledManifest.getEstimatedTasks(LeveledManifest.java:435)
 at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getEstimatedRemainingTasks(LeveledCompactionStrategy.java:128)
 at org.apache.cassandra.db.compaction.CompactionManager.getPendingTasks(CompactionManager.java:1063)
 at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(Unknown Source)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(Unknown Source)
 at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
 at sun.rmi.transport.Transport$1.run(Unknown Source)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
 at java.lang.Thread.run(Unknown Source)
{panel}

maybe we could change the list to a copyOnArrayList? just a suggestion, haven't investigated much yet:

{code:title=LeveledManifest.java}
generations[i] = new ArrayList<SSTableReader>();
{code}",compaction lcs,[],CASSANDRA,Bug,Low,2012-05-17 18:29:34,7
12555858,cql3 ORDER BY not ordering,"Creating the simplest composite-key cql3 table I can think of, populating it with a few rows of data, then trying to do a query with an ORDER BY does not yield ordered results.

Here's a cql script:

{noformat}
create keyspace test with strategy_class = 'SimpleStrategy'
   and strategy_options:replication_factor = 1;
use test;
create table moo (a int, b int, c int, primary key (a, b));

insert into moo (a, b, c) values (123, 12, 3400);
insert into moo (a, b, c) values (122, 13, 3500);
insert into moo (a, b, c) values (124, 10, 3600);
insert into moo (a, b, c) values (121, 11, 3700);

select * from moo;
select * from moo order by b;
{noformat}

Here is the output of those two queries:

{noformat}
 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400

 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400
{noformat}

I also tried these using the bare thrift interface, to make sure it wasn't python-cql or cqlsh doing something stupid. Same results. Am I totally missing something important here about how this is supposed to work?",cql3,[],CASSANDRA,Bug,Normal,2012-05-15 19:05:27,2
12554549,overlapping sstables in leveled compaction strategy,"CASSANDRA-4142 introduces test failures, that are caused by overlapping tables within a level, which Shouldn't Happen.",lcs,[],CASSANDRA,Bug,Normal,2012-05-09 19:53:39,2
12553794,Easy access to column timestamps (and maybe ttl) during queries,"It would be interesting to allow accessing the timestamp/ttl of a column though some syntax like
{noformat}
SELECT key, value, timestamp(value) FROM foo;
{noformat}
and the same for ttl.

I'll note that currently timestamp and ttl are returned in the resultset because it includes thrift Column object, but adding such syntax would make our future protocol potentially simpler as we wouldn't then have to care about timestamps explicitely (and more compact in general as we would only return timestamps when asked)",cql3,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2012-05-03 16:20:12,2
12553679,COMPACT STORAGE should not require a value to be aliased,"It's legitimate to only need the column name in a schema, e.g., system.NodeIdInfo.",cql3,[],CASSANDRA,Bug,Normal,2012-05-02 22:58:14,2
12553427,"CQL 3.0 prepare_cql_query fails on ""BEGIN BATCH""","Preparing the following (contrived) statement with the C++ Thrift bindings 
throws a TTransportException (""No more data to read."" from TTransport.h:41)

q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";
client.prepare_cql_query(pr, q, Compression::NONE);

{code:title=crashtest.cpp}
#include <protocol/TBinaryProtocol.h>
#include <thrift/transport/TSocket.h>
#include <thrift/transport/TTransportUtils.h>
#include ""Cassandra.h""

using namespace std;
using namespace apache::thrift;
using namespace apache::thrift::protocol;
using namespace apache::thrift::transport;
using namespace org::apache::cassandra;
using namespace boost;

int main(int argc, char **argv) {
    shared_ptr<TTransport> socket(new TSocket(""127.0.0.1"", 9160));
    shared_ptr<TTransport> transport(new TFramedTransport(socket));
    shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));

    CassandraClient client(protocol);

    try {
        transport->open();
        client.set_keyspace(""test1"");
        client.set_cql_version(""3.0.0"");

        CqlResult cr;
        CqlPreparedResult pr;

        // In cqlsh: create table crashtest (id int primary key, val text);
        const char *q;
        // q = ""insert into crashtest (id, val) values (?, ?)""; // This works fine
        q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";

        client.prepare_cql_query(pr,  q, Compression::NONE);

        vector<string> vtypes = pr.variable_types;
        vector<string>::iterator it;

        for (it = vtypes.begin(); it != vtypes.end(); it++) {
            cout << *it << endl;
        }
    } catch (TException &tx) {
        cerr << ""TException ERROR: "" << tx.what() << endl;
    }
}
{code}

{code:title=backtrace}
#0  0x00007fff901800e9 in __cxa_throw ()
#1  0x0000000100009ab9 in apache::thrift::transport::readAll<apache::thrift::transport::TBufferBase> (trans=@0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:41
#2  0x0000000100009c1d in apache::thrift::transport::TBufferBase::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:82
#3  0x0000000100009c5b in apache::thrift::transport::TFramedTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:390
#4  0x0000000100004b45 in apache::thrift::transport::TVirtualTransport<apache::thrift::transport::TFramedTransport, apache::thrift::transport::TBufferBase>::readAll_virt (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TVirtualTransport.h:99
#5  0x00000001000034c1 in apache::thrift::transport::TTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:126
#6  0x0000000100009f4c in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readI32 (this=0x100401370, i32=@0x7fff5fbff020) at TBinaryProtocol.h:372
#7  0x000000010000b5bf in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TBinaryProtocol.h:203
#8  0x0000000100006b07 in apache::thrift::protocol::TVirtualProtocol<apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>, apache::thrift::protocol::TProtocolDefaults>::readMessageBegin_virt (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TVirtualProtocol.h:432
#9  0x00000001000abe78 in apache::thrift::protocol::TProtocol::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TProtocol.h:518
#10 0x0000000100069a98 in org::apache::cassandra::CassandraClient::recv_prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0) at Cassandra.cpp:10231
#11 0x000000010003bf3f in org::apache::cassandra::CassandraClient::prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0, query=@0x7fff5fbff6b0, compression=org::apache::cassandra::Compression::NONE) at Cassandra.cpp:10206
#12 0x00000001000020ea in main (argc=1, argv=0x7fff5fbff8c8) at crashtest.cpp:36
{code}

{code:title=server error message}

ERROR 17:13:55,089 Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.cassandra.cql3.statements.UpdateStatement.prepare(UpdateStatement.java:278)
	at org.apache.cassandra.cql3.statements.BatchStatement.prepare(BatchStatement.java:157)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:207)
	at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:158)
	at org.apache.cassandra.thrift.CassandraServer.prepare_cql_query(CassandraServer.java:1260)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3484)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3472)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{code}",c++ cql3 thrift,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-04-30 22:03:16,2
12553420,Preserve commitlog size cap when recycling segments at startup,"1. Create a single node cluster, use default configuration, use cassandra.bat to start the server:

2. run the following commands in cli:
{code}
create keyspace toto;
use toto;
create column family titi;
truncate titi;
{code}

3. the node dies with this error:
{code}
ERROR 23:23:02,118 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Map failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:202)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:159)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(Unknown Source)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:119)
        ... 5 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        ... 7 more
 INFO 23:23:02,122 Stop listening to thrift clients
 INFO 23:23:02,123 Waiting for messaging service to quiesce
 INFO 23:23:02,125 MessagingService shutting down server thread.
{code}",commitlog,[],CASSANDRA,Bug,Low,2012-04-30 21:28:26,7
12553104,CQL3: improve experience with time uuid,"This ticket proposes to add a timeuuid type to CQL3. I know that the uuid type does support version 1 UUID (which is fine), but my rational is that time series is a very common use case for Cassandra. But when modeling time series, it seems to me that you'd almost always want to use time uuids rather than timestamps to avoid having to care about collision. In those case, using a timeuuid type would imo have the following advantages over simply uuid:
# the type convey the idea that this is really a date (but need to avoid collision). In other words, the 'time' in timeuuid has a documentation purpose.
# it validates that you do only insert a UUID v1. Inserting non-time based UUID when you really care about the time ordering is a important mistake, it's nice to validate this doesn't happen (it's one of the goal of the type after all)
# it'll allow to parse date values (which TimeUUIDType already does). Since timeuuid is really a date, it's useful and convenient to allow '2012-04-27 11:32:02' as a value.

I'll note that imho there really is no reason not to at least allow 3) and even if there is strong opposition to adding a new timeuuid type (though I don't see why that would be a big deal) we could add the parsing of date to uuid. But I do think personally that 1) and 2) are equally important and warrant the addition of timeuuid (and it'll feel less random to parse date as timeuuid than to do it for uuid).
",cql3,['Legacy/CQL'],CASSANDRA,Improvement,Low,2012-04-27 07:54:12,2
12552933,cql delete does not delete,"tested in 1.1 and trunk branch on a single node:
{panel}
cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;
cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps

cqlsh:test> delete from testcf_old where username = 'abc' and id =2;
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps
{panel}

same also when not using compact:
{panel}
cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps

cqlsh:test> delete from testcf where username = 'abc' and id =2;
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps
{panel}",cql3,[],CASSANDRA,Bug,Normal,2012-04-26 19:11:06,2
12552832,CQL3: fix index dropping and assign default name if none provided at index creation,"This ticket proposes to fix two problems of CQL3 index handling:
# DROP INDEX is broken (because the code forgot to clone the metadata before doing modification which break the schema update path)
# If an index is created with a name (which CREATE INDEX allow), there is no way to drop the index (note that we will internally assign a name to the index ColumnFamilyStore, but we don't assign a name in the ColumnDefinition object, which is the only one checked by DROP INDEX).",cql3,['Legacy/CQL'],CASSANDRA,Bug,Normal,2012-04-26 10:35:43,2
12552706,Apparent data loss using super columns and row cache via ConcurrentLinkedHashCacheProvider,"Tested on a vanilla single-node cassandra 1.0.9 installation.

When using super columns along with row caching via ConcurrentLinkedHashCacheProvider (default if no JNA available, or explicitly configured even if JNA available), there's what appears as transient data loss.

Given this script executed in cassandra-cli:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and rows_cached=75000 and row_cache_provider='ConcurrentLinkedHashCacheProvider';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The output from the 3 gets above is as follows:

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

It's clear that the second and third set commands (country, region) are missing in the returned results.

If the row cache is explicitly invalidated (in a second terminal, via `nodetool -h localhost invalidaterowcache Test Users`), the missing data springs to life on next 'get':
{quote}
[default@Test] get Users['mina'];
=> (super_column=attrs,
     (column=country, value=Canada, timestamp=1335377839592000)
     (column=name, value=Mina, timestamp=1335377788441000)
     (column=region, value=Quebec, timestamp=1335377871353000))
Returned 1 results.
{quote}

From cursory checks, this does not to appear to happen with regular columns, nor with JNA enabled + SerializingCacheProvider.

",ConcurrentLinkedHashCacheProvider cache supercolumns,[],CASSANDRA,Bug,Normal,2012-04-25 19:01:16,2
12552691,CQL3: move {max/min}_compaction_thresholds to compaction options,"It makes way more sense to have min_compaction_threshold and max_compaction_threshold be parts of the compaction_strategy_options. They are not in thrift (and CQL2) only for historical reasons, but there is no reason not to fix it. Especially given that they don't make sense for all compaction strategy (Leveled compaction ignores them).",cql3,['Legacy/CQL'],CASSANDRA,Improvement,Low,2012-04-25 17:30:20,2
12552685,CQL3: make some keywords unreserved,"CQL has quite a few keywords. Currently all of them are reserved, but this is not always necessary. PostreSQL for instance distinguish between reserved keywords and non-reserved ones, and allow things like {{key}}, {{timestamp}} or {{type}} as identifiers. I suggest we do the same as convenience for the user.",cql3,['Legacy/CQL'],CASSANDRA,Improvement,Low,2012-04-25 16:01:48,2
12552645,Minor CQL3 fixes,"The goal of this ticket is to be the home for a number of minor fixes/improvements in CQL3 that I didn't felt warranted a ticket each. It includes 4 patches:
* The first one fixes the grammar for float constants, so as to not recognize 3.-3, but to actually allow 3. (i.e, with radix point but with the fractional part left blank)
* The second one correctly detect the (invalid) case where a table is created with COMPACT STORAGE but without any 'clustering keys'.
* The third one fixes COUNT, first by making sure both COUNT(*) and COUNT(1) are correctly recognized and also by ""processing"" the internal row before counting, are there isn't a 1-to-1 correspondence between internal rows and CQL rows in CQL3. The grammar change in this patch actually rely on CASSANDRA-4184
* The fourth and last patch disallows the counter type for keys (i.e. any column part of the PRIMARY KEY) as it is completely non-sensical and will only led to confusion.
",cql3,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-04-25 13:08:14,2
12552639,Make identifier and value grammar for CQL3 stricter,"The current grammar for CQL3 allows:
# uuid and integer constants as identifiers
# identifier as value (aka term in the grammar)

I think both of those should be removed.

For 1, mostly because this feels useless and slightly complicates the grammar which is annoying for the documentation of CQL3 for instance (note that this doesn't mean forbidding integer or uuid as identifier, but means they have to be double-quoted when used as such).
For 2, I think that allowing identifier as value is actually misleading, typically if you write things like {{SELECT foo WHERE foo=foo}}. It suggests we support JOIN when we do not.

Also, if both are done, then one will always be able to distinguish between identifier and value even without any context, which is a nice property.",cql3,['Legacy/CQL'],CASSANDRA,Improvement,Normal,2012-04-25 12:46:27,2
12551799,Single-pass compaction for LCR,"LazilyCompactedRow reads all data twice to compact a row which is obviously inefficient. The main reason we do that is to compute the row header. However, CASSANDRA-2319 have removed the main part of that row header. What remains is the size in bytes and the number of columns, but it should be relatively simple to remove those, which would then remove the need for the two-phase compaction.",compaction,[],CASSANDRA,Improvement,Normal,2012-04-20 15:00:36,7
12551750,"Add more general support for composites (to row key, column value)","Currently CQL3 have a nice syntax for using composites in the column name (it's more than that in fact, it creates a whole new abstraction but let's say I'm talking implementation here). There is however 2 other place where composites could be used (again implementation wise): the row key and the column value. This ticket proposes to explore which of those make sense for CQL3 and how.

For the row key, I really think that CQL support makes sense. It's very common (and useful) to want to stuff composite information in a row key. Sharding a time serie (CASSANDRA-4176) is probably the best example but there is other.

For the column value it is less clear. CQL3 makes it very transparent and convenient to store multiple related values into multiple columns so maybe composites in a column value is much less needed. I do still see two cases for which it could be handy:
# to save some disk/memory space, if you do know it makes no sense to insert/read two value separatly.
# if you want to enforce that two values should not be inserted separatly. I.e. to enforce a form of ""constraint"" to avoid programatic error.

Those are not widely useful things, but my reasoning is that if whatever syntax we come up for ""grouping"" row key in a composite trivially extends to column values, why not support it.


As for syntax I have 3 suggestions (that are just that, suggestions):
# If we only care about allowing grouping for row keys:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value text,
    PRIMARY KEY ((name, month), ts)
)
{noformat}
# A syntax that could work for both grouping in row key and colum value:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value1 text,
    value2 text,
    GROUP (name, month) as key,
    GROUP (value1, value2),
    PRIMARY KEY (key, ts)
)
{noformat}
# An alternative to the preceding one:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value1 text,
    value2 text,
    GROUP (name, month) as key,
    GROUP (value1, value2),
    PRIMARY KEY (key, ts)
) WITH GROUP (name, month) AS key
   AND GROUP (value1, value2)
{noformat}",cql3,['Legacy/CQL'],CASSANDRA,Sub-task,Low,2012-04-20 09:02:08,2
12551531,cql3 ALTER TABLE foo WITH default_validation=int has no effect,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY) WITH default_validation=timestamp;
ALTER TABLE test WITH default_validation=int;
{noformat}

does not actually change the default validation type of the CF. It does under cql2.

No error is thrown. Some properties *can* be successfully changed using ALTER WITH, such as comment and gc_grace_seconds, but I haven't tested all of them. It seems probable that default_validation is the only problematic one, since it's the only (changeable) property which accepts CQL typenames.",cql3,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-04-18 21:01:39,2
12551529,cql3 ALTER TABLE ALTER TYPE has no effect,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY, bar int);
ALTER TABLE test ALTER bar TYPE float;
{noformat}

does not actually change the column type of bar. It does under cql2.

Note that on the current cassandra-1.1.0 HEAD, this causes an NPE, fixed by CASSANDRA-4163. But even with that applied, the ALTER shown here has no effect.",cql3,['Legacy/CQL'],CASSANDRA,Bug,Normal,2012-04-18 20:57:07,2
12551356,Generate Digest file for compressed SSTables,"We use the generated *Digest.sha1-files to verify backups, would be nice if they were generated for compressed sstables as well.",performance,[],CASSANDRA,Improvement,Low,2012-04-18 06:18:04,7
12551206,ORDER BY ... DESC reverses comparrison predicates in WHERE,"When issuing a cql select statement with an ORDER BY ... DESC clause the comparison predicates in the WHERE clause gets reversed. 

Example: (see also attached)

SELECT number FROM test WHERE number < 3 ORDER BY number DESC

returns the results expected of WHERE number > 3",cql3,['Legacy/CQL'],CASSANDRA,Bug,Normal,2012-04-17 12:06:49,2
12550633,OOM Exception during repair session with LeveledCompactionStrategy,"We encountered an OOM Exception on 2 nodes during repair session.
Our CF are set up to use LeveledCompactionStrategy and SnappyCompressor.
These two options used together maybe the key to the problem.

Despite of setting XX:+HeapDumpOnOutOfMemoryError, no dump have been generated.
Nonetheless a memory analysis on a live node doing a repair reveals an hotspot: an ArrayList of SSTableBoundedScanner which appears to contain as many objects as there are SSTables on disk. 
This ArrayList consumes 786 MB of the heap space for 5757 objects. Therefore each object is about 140 KB.

Eclipse Memory Analyzer's denominator tree shows that 99% of a SSTableBoundedScanner object's memory is consumed by a CompressedRandomAccessReader which contains two big byte arrays.

Cluster information:
9 nodes
Each node handles 35 GB (RandomPartitioner)

This JIRA was created following this discussion:
http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Why-so-many-SSTables-td7453033.html

",lcs,[],CASSANDRA,Improvement,Normal,2012-04-12 07:26:11,2
12549262,nodetool cleanup giving exception,"We just recently started using version 1.0.9, previously we were using tiered compaction because of a bug in 1.0.8 (not letting us use leveled compaction) and now since moving to 1.0.9 we have started using leveled compaction.

Trying to do a cleanup we are getting the following exception:

root@test:~# nodetool -h localhost cleanup 
Error occured during cleanup
java.util.concurrent.ExecutionException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:204)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:240)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:988)
        at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1639)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.NoSuchElementException
        at java.util.ArrayList$Itr.next(ArrayList.java:757)
        at org.apache.cassandra.db.compaction.LeveledManifest.replace(LeveledManifest.java:196)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:147)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:495)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1010)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:802)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:244)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:183)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)

cheers,
Shoaib",compaction,[],CASSANDRA,Bug,Normal,2012-04-03 00:18:48,7
12548428,mlockall() returned code is ignored w/o assertions,"We log that mlockall() was successful only based on the lack of an assertion failure, so for anyone running w/o {{-ea}} we are lying about mlockall() succeeding.",jna,[],CASSANDRA,Bug,Low,2012-03-28 03:13:43,7
12547849,Cut down on the comparisons needed during shouldPurge and needDeserialize,shouldPurge in particular is still a performance sore point with LCS.,compaction,[],CASSANDRA,Improvement,Low,2012-03-23 18:15:29,7
12547839,Check SSTable range before running cleanup,"Before running a cleanup compaction on an SSTable we should check the range to see if the SSTable falls into the range we want to remove. If it doesn't we can just mark the SSTable as compacted and be done with it, if it does, we can no-op.

Will not help with STCS, but for LCS, and perhaps some others we may see a benefit here after topology changes.",compaction,[],CASSANDRA,Improvement,Low,2012-03-23 17:31:33,7
12546471,Rewrite RandomAccessReader to use FileChannel / nio to address Windows file access violations,"On Windows w/older java I/O libraries the files are not opened with FILE_SHARE_DELETE.  This causes problems as hard-links cannot be deleted while the original file is opened - our snapshots are a big problem in particular.  The nio library and FileChannels open with FILE_SHARE_DELETE which should help remedy this problem.

Original text:
I'm using Cassandra 1.0.8, on Windows 7.  When I take a snapshot of the database, I find that I am unable to delete the snapshot directory (i.e., dir named ""{datadir}\{keyspacename}\snapshots\{snapshottag}"") while Cassandra is running:  ""The action can't be completed because the folder or a file in it is open in another program.  Close the folder or file and try again"" [in Windows Explorer].  If I terminate Cassandra, then I can delete the directory with no problem.

I expect to be able to move or delete the snapshotted files while Cassandra is running, as this should not affect the runtime operation of Cassandra.",Windows,['Legacy/Local Write-Read Paths'],CASSANDRA,Bug,Low,2012-03-14 20:22:30,6
12545902,"cqlsh: Alphabetize the ""Miscellaneous Help Topics"" section","The first section in the help is in alphabetical order, but not the second part:

{code}
Miscellaneous help topics:
==========================
DROP_INDEX                 CREATE                       DELETE_WHERE       
ALTER_DROP                 DROP_KEYSPACE                UPDATE_USING       
SELECT_EXPR                ALTER_ALTER                  UPDATE_WHERE       
UUID_INPUT                 TYPES                        TIMESTAMP_OUTPUT   
DELETE_COLUMNS             SELECT_COLUMNFAMILY          CONSISTENCYLEVEL   
ALTER_ADD                  CREATE_COLUMNFAMILY_OPTIONS  CREATE_INDEX       
ALTER_WITH                 BEGIN                        CREATE_KEYSPACE    
APPLY                      UPDATE_SET                   ASCII_OUTPUT       
DELETE_USING               UPDATE_COUNTERS              DROP               
CREATE_COLUMNFAMILY_TYPES  TRUNCATE                     TIMESTAMP_INPUT    
DROP_COLUMNFAMILY          INSERT                       ALTER              
BLOB_INPUT                 TEXT_OUTPUT                  CREATE_COLUMNFAMILY
SELECT_WHERE               UPDATE                       DELETE             
BOOLEAN_INPUT              SELECT_LIMIT
{code}",cql cqlsh,[],CASSANDRA,Improvement,Low,2012-03-09 20:33:47,1
12544881,nodetool cleanup/scrub/upgradesstables promotes all sstables to next level (LeveledCompaction),"1.0.7 + LeveledCompactionStrategy
If you run nodetool cleanup, scrub, or upgradesstables, Cassandra execute compaction for each sstable. During the compaction, it put the new sstable to next level of the original sstable. If you run cleanup many times, sstables will reached to the highest level, and CASSANDRA-3608 will happens at next cleanup.

Reproduce procedure:
# create column family CF1 with compaction_strategy=LeveledCompactionStrategy and compaction_strategy_options={sstable_size_in_mb: 5};
# Insert some data into CF1.
# nodetool flush
# Verify the sstable is created at L1 in CF1.json
# nodetool cleanup
# Verify sstable in L1 is removed and new sstable is created at L2 in CF1.json
# repeat nodetool cleanup some times",lcs,[],CASSANDRA,Bug,Low,2012-03-02 03:21:57,2
12544647,Explore not returning range ghosts,"This ticket proposes to remove range ghosts in CQL3.
The basic argument is that range ghosts confuses users a lot and don't add any value since range ghost don't allow to distinguish between the two following case:
* the row is deleted
* the row is not deleted but don't have data for the provided filter",cql3,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2012-02-29 17:27:37,2
12544643,Don't include original exception class name in CQL message,"In CreateColumnFamilyStatement, we do
{noformat}
catch (ConfigurationException e)
{
    throw new InvalidRequestException(e.toString());
}
{noformat}

This result in having the exception message looking like:
{noformat}
java.sql.SQLSyntaxErrorException: org.apache.cassandra.config.ConfigurationException: Cf1 already exists in keyspace Keyspace1
{noformat}",cql,[],CASSANDRA,Bug,Low,2012-02-29 16:33:11,2
12544623,Consider providing error code with exceptions (and documenting them),"It could be a good idea to assign documented error code for the different exception raised. Currently, one may have to parse the exception string (say if one wants to know if its 'create keyspace' failed because the keyspace already exists versus other kind of exception), but it means we cannot improve the error message at the risk of breaking client code. Adding documented error codes with the message would avoid this.",cql3,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2012-02-29 14:15:51,2
12544447,Hints Should Be Dropped When Missing CFid Implies Deleted ColumnFamily,"If hints have accumulated for a CF that has been deleted, Hinted Handoff repeatedly fails until manual intervention removes those hints. For 1.0.7, UnserializableColumnFamilyException is thrown only when a CFid is unknown on the sending node. As discussed on #cassandra-dev, if the schema is in agreement, the affected hint(s) should be deleted to avoid indefinite repeat failures.",datastax_qa,[],CASSANDRA,Bug,Normal,2012-02-28 16:51:15,7
12544067,Remove random HH delay,"{code}
.       // sleep a random amount to stagger handoff delivery from different replicas.
        // (if we had to wait, then gossiper randomness took care of that for us already.)
        if (waited == 0)
        {
            // use a 'rounded' sleep interval because of a strange bug with windows: CASSANDRA-3375
            int sleep = FBUtilities.threadLocalRandom().nextInt(2000) * 30;
            logger_.debug(""Sleeping {}ms to stagger hint delivery"", sleep);
            Thread.sleep(sleep);
        }
{code}

This is obsolete now that we have the per-hint configurable delay.  And large hint loads (which are the ones that matter most) are going to overlap anyway even with the maximum 60s difference.",hintedhandoff,[],CASSANDRA,Improvement,Low,2012-02-24 22:39:21,7
12544061,Supercolumn serialization assertion failure,"As reported at http://mail-archives.apache.org/mod_mbox/cassandra-user/201202.mbox/%3CCADJL=w5kH5TEQXOwhTn5Jm3cmR4Rj=NfjcqLryXV7pLyASi95A@mail.gmail.com%3E,

{noformat}
ERROR 10:51:44,282 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 4690 to accomodate data size
of 2347 (predicted 2344) for RowMutation(keyspace='Player',
key='36336138643338652d366162302d343334392d383466302d356166643863353133356465',
modifications=[ColumnFamily(PlayerCity [SuperColumn(owneditem_1019
[]),SuperColumn(owneditem_1024 []),SuperColumn(owneditem_1026
[]),SuperColumn(owneditem_1074 []),SuperColumn(owneditem_1077
[]),SuperColumn(owneditem_1084 []),SuperColumn(owneditem_1094
[]),SuperColumn(owneditem_1130 []),SuperColumn(owneditem_1136
[]),SuperColumn(owneditem_1141 []),SuperColumn(owneditem_1142
[]),SuperColumn(owneditem_1145 []),SuperColumn(owneditem_1218
[636f6e6e6563746564:false:5@1329648704269002
,63757272656e744865616c7468:false:3@1329648704269006
,656e64436f6e737472756374696f6e54696d65:false:13@1329648704269007
,6964:false:4@1329648704269000,6974656d4964:false:15@1329648704269001
,6c61737444657374726f79656454696d65:false:1@1329648704269008
,6c61737454696d65436f6c6c6563746564:false:13@1329648704269005
,736b696e4964:false:7@1329648704269009,78:false:4@1329648704269003
,79:false:3@1329648704269004,]),SuperColumn(owneditem_133
[]),SuperColumn(owneditem_134 []),SuperColumn(owneditem_135
[]),SuperColumn(owneditem_141 []),SuperColumn(owneditem_147
[]),SuperColumn(owneditem_154 []),SuperColumn(owneditem_159
[]),SuperColumn(owneditem_171 []),SuperColumn(owneditem_253
[]),SuperColumn(owneditem_422 []),SuperColumn(owneditem_438
[]),SuperColumn(owneditem_515 []),SuperColumn(owneditem_521
[]),SuperColumn(owneditem_523 []),SuperColumn(owneditem_525
[]),SuperColumn(owneditem_562 []),SuperColumn(owneditem_61
[]),SuperColumn(owneditem_634 []),SuperColumn(owneditem_636
[]),SuperColumn(owneditem_71 []),SuperColumn(owneditem_712
[]),SuperColumn(owneditem_720 []),SuperColumn(owneditem_728
[]),SuperColumn(owneditem_787 []),SuperColumn(owneditem_797
[]),SuperColumn(owneditem_798 []),SuperColumn(owneditem_838
[]),SuperColumn(owneditem_842 []),SuperColumn(owneditem_847
[]),SuperColumn(owneditem_849 []),SuperColumn(owneditem_851
[]),SuperColumn(owneditem_852 []),SuperColumn(owneditem_853
[]),SuperColumn(owneditem_854 []),SuperColumn(owneditem_857
[]),SuperColumn(owneditem_858 []),SuperColumn(owneditem_874
[]),SuperColumn(owneditem_884 []),SuperColumn(owneditem_886
[]),SuperColumn(owneditem_908 []),SuperColumn(owneditem_91
[]),SuperColumn(owneditem_911 []),SuperColumn(owneditem_930
[]),SuperColumn(owneditem_934 []),SuperColumn(owneditem_937
[]),SuperColumn(owneditem_944 []),SuperColumn(owneditem_945
[]),SuperColumn(owneditem_962 []),SuperColumn(owneditem_963
[]),SuperColumn(owneditem_964 []),])])
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
",datastax_qa,[],CASSANDRA,Bug,Normal,2012-02-24 21:51:28,2
12543576,mergeShardsChance deprecated; remove from thrift?,Or at least it should be marked deprecated somehow.,thrift_protocol,['Legacy/CQL'],CASSANDRA,Bug,Low,2012-02-21 22:00:15,2
12542719,Dropping a column should do more than just remove the definition,"Dropping a column should:

- immediately make it unavailable for {{SELECT}}, including {{SELECT *}}
- eventually (i.e., post-compaction) reclaim the space formerly used by that column
",compaction cql,[],CASSANDRA,Sub-task,Normal,2012-02-15 17:28:53,1
12540203,snapshot-before-compaction snapshots entire keyspace,Should only snapshot the CF being compacted,compaction,[],CASSANDRA,Bug,Low,2012-01-27 22:04:26,7
12540197,cqlsh: ASSUME should also change how values are sent to cassandra,"cqlsh's ASSUME command currently only changes how query *return* values are deserialized, and never transforms user CQL text before sending to Cassandra.

Apparently cassandra-cli also changes how values are interpreted and marshaled for Cassandra, so user expectation is that cqlsh should also do this.",cqlsh,['Legacy/Tools'],CASSANDRA,New Feature,Low,2012-01-27 21:30:57,1
12539978,remove deprecated KsDef.replication_factor field,"KsDef.replication_factor is superceded by KsDef.strategy_options, but we've been keeping special-case code around to populate the old r_f field for SimpleStrategy so that pre-0.8 clients can still create and introspect the schema.  Time to clean that up.",thrift,['Legacy/CQL'],CASSANDRA,Improvement,Low,2012-01-26 23:47:29,7
12539918,Support query by names for compact CF,"Current code don't allow doing a query by names on wide rows (compact CF). I.e. with:
{noformat}
CREATE TABLE test1 (
    k int,
    c int,
    v int,
    PRIMARY KEY (k, c)
) WITH COMPACT STORAGE;
{noformat}
you cannot do:
{noformat}
SELECT v FROM test1 WHERE k = 0 AND c IN (5, 2, 8)
{noformat}
even though this is a simple name query.

This ticket proposes to allow it.",cql3,['Legacy/CQL'],CASSANDRA,Sub-task,Low,2012-01-26 15:57:01,2
12539808,Support slice with exclusive start and stop,"Currently, slices are always start and end inclusive. However, for CQL 3.0, we already differenciate between inclusivity/exclusivity for the row key and for the component of composite columns. It would be nice to always support that distinction.",cql3,[],CASSANDRA,Sub-task,Normal,2012-01-25 18:54:21,2
12539699,CQL support for changing row key type in ALTER TABLE,There is currently no way to alter the key_validation_class from CQL. jbellis suggested that this could be done by being able to ALTER the type of the KEY alias.,cql,[],CASSANDRA,Improvement,Normal,2012-01-25 00:17:56,2
12539666,need forked language document,The language doc ({{doc/cql/CQL.textile}}) needs to be forked for CQLv3 and updated accordingly.,cql,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2012-01-24 20:47:47,2
12539665,KEY IN (...) queries do not work,{{...KEY IN (...)}} queries fail due to faulty validation.  A pull request for cassandra-dtest was opened that demonstrates this: https://github.com/riptano/cassandra-dtest/pull/2,cql,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2012-01-24 20:45:27,2
12539478,Allow paging through non-ordered partitioner results in CQL3,"CQL < 3 silently turns a ""key >= X"" into ""token(key) >= token(X)"".  This is not what users will expect, since many of the rows returned will not in fact satisfy the requested key inequality.  We should add syntax that makes the difference between keys and tokens explicit, possibly with a token() ""function"" as imagined here.",cql,['Legacy/CQL'],CASSANDRA,Sub-task,Low,2012-01-23 14:33:42,2
12539274,hadoop word count example is unable to output to cassandra with default settings,"{noformat}
12/01/21 06:03:16 WARN mapred.LocalJobRunner: job_local_0001
java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:407)
        at org.apache.cassandra.hadoop.ConfigHelper.getOutputPartitioner(ConfigHelper.java:384)
        at org.apache.cassandra.client.RingCache.<init>(RingCache.java:58)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:99)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:93)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:62)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:553)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
{noformat}

(Output to filesystem still works.)",hadoop,[],CASSANDRA,Bug,Low,2012-01-21 06:05:16,7
12539160,CQL 3.0,"This ticket is a reformulation/generalization of CASSANDRA-2474. The core change of CQL 3.0 is to introduce the new syntaxes that were discussed in CASSANDRA-2474 that allow to:
# Provide a better/more native support for wide rows, using the idea of transposed vie.
# The generalization to composite columns.

The attached text file create_cf_syntaxes.txt recall the new syntaxes introduced.

The changes proposed above allow (and strongly suggest in some cases) a number of other changes to the language that this ticket proposes to explore/implement (more details coming in the comments).",cql,['Legacy/CQL'],CASSANDRA,New Feature,Normal,2012-01-20 16:14:28,2
12539040,cqlsh: allow configuration of value display formats,"With CASSANDRA-3726, cqlsh now formats some types of query result data to be more human-readable, such as timestamps and hex data. The format of timestamps and the precision of floating point values should be configurable by cqlshrc and/or command line.

see the {{Shell.display_time_format}} and {{Shell.display_float_precision}} attributes.",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2012-01-19 21:15:06,1
12538863,Update CqlPreparedResult to provide type information,"As discussed on CASSANDRA-3634, adding type information to a prepared statement would allow more client-side error checking.",cql,['Legacy/CQL'],CASSANDRA,Improvement,Urgent,2012-01-18 19:02:19,2
12538739,Possible livelock during commit log playback,"In CommitLog.recover, there seems to be the possibility of concurrent inserts to tablesRecovered (a HashSet) in the Runnables instantiated a bit below (line 323 in 1.0.7). This apparently happened during a commit log playback during startup of a node that had not shut down cleanly (the cluster was under heavy load previously and there were several gigabytes of commit logs), resulting in two threads running in perpetuity (2 cores were at 100% from running these threads), preventing the node from coming up. The relevant portion of the stack trace is:

{noformat}
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:25"" prio=10 tid=0x00002aaad01e0800 nid=0x6f62 runnable [0x0000000044d54000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:21"" prio=10 tid=0x00002aaad00a2800 nid=0x6f5e runnable [0x0000000044950000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)

{noformat}

The most recently modified file in the commit log directory was this entry:
{noformat}
-rw-r----- 1 <redacted> <redacted>    0 Jan 16 16:03 CommitLog-1326758622599.log
{noformat}
though I'm not sure if this was related or not. ",commitlog,[],CASSANDRA,Bug,Normal,2012-01-18 05:19:30,7
12537925,Clean up isMarkedForDelete / getLocalDeletionTime,"As explained in CASSANDRA-3579, isMarkedForDelete() depends on the current system clock so it can change during a two-pass compaction.  Suggested fix is to replace iMFD + gLDT with a getExpirationTime method, so comparison with the compaction's gcBefore will remain constant.",compaction,[],CASSANDRA,Improvement,Normal,2012-01-10 17:36:49,2
12537700,Unsustainable Thread Accumulation in ParallelCompactionIterable.Reducer ThreadPoolExecutor,"With multithreaded compaction enabled, it looks like Reducer creates a new thread pool for every compaction.  These pools seem to just sit around - i.e. ""executor.shutdown()"" never gets called and the Threads live forever waiting for tasks that will never come.  For instance...


Name: CompactionReducer:1
State: TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@72938aea
Total blocked: 0  Total waited: 1

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1043)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1103)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
java.lang.Thread.run(Thread.java:722)
",compaction memory_leak threading threads,[],CASSANDRA,Bug,Low,2012-01-08 23:46:40,7
12537433,log Compaction Active tasks in StatusLogger instead of n/a,"currently StatusLogger log:
{noformat}
logger.info(String.format(""%-25s%10s%10s"",
                                  ""CompactionManager"", ""n/a"", CompactionManager.instance.getPendingTasks()));
{noformat}

It'd be great if it could actually log the number of active tasks being processed. Without looking into the code, I thought there was no compaction running when reading the log.


{code: title=CompactionManager.java}
    public int getActiveCompactions()
    {
        return CompactionExecutor.compactions.size();
    }
{code}",compaction,[],CASSANDRA,Improvement,Low,2012-01-05 22:43:06,7
12537183,strange values of pending tasks with compactionstats (below 0),"during scrub:

Every 2.0s: for i in 1 2 3; do nodetool -h 192.168.2.$i compactionstats; done                                                                                                                                     Wed Jan  4 13:48:13 2012

pending tasks: 2147483646
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     28034971475     72393139120    38.73%
pending tasks: -2147483647
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     24575687282     72385305067    33.95%
pending tasks: 0

",compaction,['Legacy/Tools'],CASSANDRA,Bug,Low,2012-01-04 12:53:28,7
12537079,LeveledCompactionStrategy is broken because of generation pre-allocation in LeveledManifest.,"LeveledManifest constructor has the following code:

{code}
for (int i = 0; i < generations.length; i++)
{
    generations[i] = new ArrayList<SSTableReader>();
    lastCompactedKeys[i] = new DecoratedKey(cfs.partitioner.getMinimumToken(), null);
}
{code}

But in the DecoratedKey constructor we have:

{code}
assert token != null && key != null && key.remaining() > 0;
{code}

so when you tried to create a CF with LeveledCompressionStrategy that will result in 

{noformat}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:865)
	at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:953)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace.process(Cassandra.java:4103)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3078)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:857)
	... 7 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:770)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:209)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:300)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:281)
	at org.apache.cassandra.db.Table.initCf(Table.java:339)
	at org.apache.cassandra.db.Table.<init>(Table.java:288)
	at org.apache.cassandra.db.Table.open(Table.java:117)
	at org.apache.cassandra.db.migration.AddKeyspace.applyModels(AddKeyspace.java:72)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
	at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:850)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:752)
	... 14 more
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
	at org.apache.cassandra.db.compaction.LeveledManifest.<init>(LeveledManifest.java:79)
	at org.apache.cassandra.db.compaction.LeveledManifest.create(LeveledManifest.java:85)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.<init>(LeveledCompactionStrategy.java:74)
	... 19 more
ERROR 19:52:44,029 Fatal exception in thread Thread[MigrationStage:1,5,main]
{noformat}",lcs,[],CASSANDRA,Bug,Normal,2012-01-03 17:01:44,2
12536553,Multiple threads can attempt hint handoff to the same target,"HintedHandOffManager attempts to prevent multiple threads sending hints to the same target with the queuedDeliveries set, but the code is buggy.  If two handoffs *do* occur concurrently, the second thread can use an arbitrarily large amount of memory skipping tombstones when it starts paging from the beginning of the hint row, looking for the first live hint.  (This is not a problem with a single thread, since it always pages starting with the last-seen hint column name, effectively skipping the tombstones.  Then it compacts when it's done.)

Technically this bug is present in all older Cassandra releases, but it only causes problems in 1.0.x since the hint rows tend to be much larger (since there is one hint per write containing the entire mutation, instead of just one per row consisting of just the key).",hintedhandoff,[],CASSANDRA,Bug,Low,2011-12-29 03:03:36,7
12536532,Add Support for Composite Secondary Indexes,"CASSANDRA-2474 and CASSANDRA-3647 add the ability to transpose wide rows differently, for efficiency and functionality secondary index api needs to be altered to allow composite indexes.  

I think this will require the IndexManager api to have a maybeIndex(ByteBuffer column) method that SS can call and implement a PerRowSecondaryIndex per column, break the composite into parts and index specific bits, also including the base rowkey.

Then a search against a TRANSPOSED row or DOCUMENT will be possible.

 ",cql3 secondary_index,['Feature/2i Index'],CASSANDRA,Sub-task,Normal,2011-12-28 21:20:53,2
12536306,Parallel streaming for sstableloader,"One of my colleague had reported the bug regarding the degraded performance of the sstable generator and sstable loader.
ISSUE :- https://issues.apache.org/jira/browse/CASSANDRA-3589 
As stated in above issue generator performance is rectified but performance of the sstableloader is still an issue.

3589 is marked as duplicate of 3618.Both issues shows resolved status.But the problem with sstableloader still exists.

So opening other issue so that sstbleloader problem should not go unnoticed.

FYI : We have tested the generator part with the patch given in 3589.Its Working fine.

Please let us know if you guys require further inputs from our side.",streaming,['Legacy/Tools'],CASSANDRA,Improvement,Low,2011-12-23 19:16:39,6
12536283,Changing compaction strategy from Leveled to SizeTiered logs millions of messages about nothing to compact,"When column family compaction strategy is changed from Leveled to SizeTiered and there're Leveled compaction tasks pending, Cassandra starting to flood in logs with thousands per sec messages:

Nothing to compact in ColumnFamily1.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)

As a result, log disk is full and system is down.",compaction,[],CASSANDRA,Bug,Normal,2011-12-23 13:09:42,7
12536150,Fix smallish problems find by FindBugs,"I've just run (the newly released) FindBugs 2 out of curiosity. Attaching a number of patches related to issue raised by it. There is nothing major at all so all patches are against trunk.

I've tried keep each issue to it's own patch with a self describing title. It far from covers all FindBugs alerts, but it's a picky tool so I've tried to address only what felt at least vaguely useful. Those are still mostly nits (only patch 2 is probably an actual bug).",fingbugs,[],CASSANDRA,Bug,Low,2011-12-22 11:28:52,2
12535918,NPE when running upgradesstables,"Running a test upgrade from 0.7(version f sstables) to 1.0.
upgradesstables runs for about 40 minutes and then NPE's when trying to retrieve a key.

No files have been succesfully upgraded. Likely related is that scrub (without having run upgrade) consumes all RAM and OOMs.

Possible theory is that a lot of paths call IPartitioner's decorateKey, and, at least in the randompartitioner's implementation, if any of those callers pass a null ByteBuffer, they key will be null in the stack trace below.


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:970)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1540)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:65)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:92)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:137)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
",compaction,[],CASSANDRA,Bug,Normal,2011-12-20 22:33:10,7
12535723,"Support collection (list, set, and map) value types in CQL",Composite columns introduce the ability to have arbitrarily nested data in a Cassandra row.  We should expose this through CQL.,cql,['Legacy/CQL'],CASSANDRA,New Feature,Normal,2011-12-19 17:29:28,2
12535015,Hinted Handoff - related OOM,"One of our nodes had collected alot of hints for another node, so when the dead node came back and the row mutations were read back from disk, the node died with an OOM-exception (and kept dying after restart, even with increased heap (from 8G to 12G)). The heap dump contained alot of SuperColumns and our application does not use those (but HH does). 

I'm guessing that each mutation is big so that PAGE_SIZE*<mutation_size> does not fit in memory (will check this tomorrow)

A simple fix (if my assumption above is correct) would be to reduce the PAGE_SIZE in HintedHandOffManager.java to something like 10 (or even 1?) to reduce the memory pressure. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every *mutation* anyway (not every page), thoughts?

If anyone runs in to the same problem, I got the node started again by simply removing the HintsColumnFamily* files.",hintedhandoff,[],CASSANDRA,Bug,Normal,2011-12-13 20:34:48,7
12534935,"Proposal for distributed deletes - fully automatic ""Reaper Model"" rather than GCSeconds and manual repairs","Proposal for an improved system for handling distributed deletes, which removes the requirement to regularly run repair processes to maintain performance and data integrity. 

h2. The Problem

There are various issues with repair:

* Repair is expensive to run
* Repair jobs are often made more expensive than they should be by other issues (nodes dropping requests, hinted handoff not working, downtime etc)
* Repair processes can often fail and need restarting, for example in cloud environments where network issues make a node disappear from the ring for a brief moment
* When you fail to run repair within GCSeconds, either by error or because of issues with Cassandra, data written to a node that did not see a later delete can reappear (and a node might miss a delete for several reasons including being down or simply dropping requests during load shedding)
* If you cannot run repair and have to increase GCSeconds to prevent deleted data reappearing, in some cases the growing tombstone overhead can significantly degrade performance

Because of the foregoing, in high throughput environments it can be very difficult to make repair a cron job. It can be preferable to keep a terminal open and run repair jobs one by one, making sure they succeed and keeping and eye on overall load to reduce system impact. This isn't desirable, and problems are exacerbated when there are lots of column families in a database or it is necessary to run a column family with a low GCSeconds to reduce tombstone load (because there are many write/deletes to that column family). The database owner must run repair within the GCSeconds window, or increase GCSeconds, to avoid potentially losing delete operations. 

It would be much better if there was no ongoing requirement to run repair to ensure deletes aren't lost, and no GCSeconds window. Ideally repair would be an optional maintenance utility used in special cases, or to ensure ONE reads get consistent data. 

h2. ""Reaper Model"" Proposal

# Tombstones do not expire, and there is no GCSeconds
# Tombstones have associated ACK lists, which record the replicas that have acknowledged them
# Tombstones are deleted (or marked for compaction) when they have been acknowledged by all replicas
# When a tombstone is deleted, it is added to a ""relic"" index. The relic index makes it possible for a reaper to acknowledge a tombstone after it is deleted
# The ACK lists and relic index are held in memory for speed
# Background ""reaper"" threads constantly stream ACK requests to other nodes, and stream back ACK responses back to requests they have received (throttling their usage of CPU and bandwidth so as not to affect performance)
# If a reaper receives a request to ACK a tombstone that does not exist, it creates the tombstone and adds an ACK for the requestor, and replies with an ACK. This is the worst that can happen, and does not cause data corruption. 

ADDENDUM

The proposal to hold the ACK and relic lists in memory was added after the first posting. Please see comments for full reasons. Furthermore, a proposal for enhancements to repair was posted to comments, which would cause tombstones to be scavenged when repair completes (the author had assumed this was the case anyway, but it seems at time of writing they are only scavenged during compaction on GCSeconds timeout). The proposals are not exclusive and this proposal is extended to include the possible enhancements to repair described.

NOTES

* If a node goes down for a prolonged period, the worst that can happen is that some tombstones are recreated across the cluster when it restarts, which does not corrupt data (and this will only occur with a very small number of tombstones)
* The system is simple to implement and predictable 
* With the reaper model, repair would become an optional process for optimizing the database to increase the consistency seen by ConsistencyLevel.ONE reads, and for fixing up nodes, for example after an sstable was lost

h3. Planned Benefits

* Reaper threads can utilize ""spare"" cycles to constantly scavenge tombstones in the background thereby greatly reducing tombstone load, improving query performance, reducing the system resources needed by processes such as compaction, and making performance generally more predictable 
* The reaper model means that GCSeconds is no longer necessary, which removes the threat of data corruption if repair can't be run successfully within that period (for example if repair can't be run because of a new adopter's lack of Cassandra expertise, a cron script failing, or Cassandra bugs or other technical issues)
* Reaper threads are fully automatic, work in the background and perform finely grained operations where interruption has little effect. This is much better for database administrators than having to manually run and manage repair, whether for the purposes of preventing data corruption or for optimizing performance, which in addition to wasting operator time also often creates load spikes and has to be restarted after failure.  ","GCSeconds, deletes, distributed_deletes, merkle_trees repair,",[],CASSANDRA,Improvement,Normal,2011-12-13 11:51:01,1
12534402,"cqlsh: use libedit when readline isn't available, if possible","Cqlsh provides context-sensitive tab-completion functionality, but it's only available when the readline library is available, and this is not the case where readline's GPL license proves problematic. [libedit|http://www.thrysoee.dk/editline/] is a common replacement, which would be available to Mac OS X-bundled Python users, and the Python readline module makes libedit almost a drop-in replacement.

If possible, fallback to libedit functionality when providing tab completion.",cqlsh,['Legacy/Tools'],CASSANDRA,Improvement,Low,2011-12-08 21:14:57,1
12533701,Cache saving broken on windows,CASSANDRA-1740 broke cache saving on Windows.,windows,[],CASSANDRA,Bug,Low,2011-12-02 23:26:07,2
12533699,CQL CF creation skips most of the validation code,"Most validation is done by ThriftValidation.validateCfDef, which we call from QP when creating an index but not on CF creation.",cql,['Legacy/CQL'],CASSANDRA,Bug,Low,2011-12-02 23:08:30,7
12533605,CFMetaData conversions to Thrift/Native schema should be inverse one of the other,"In other word, it would probably be a good idea to have:
{noformat}
  cfm == CFMetadata.fromThrift(cfm.toThrift())
  cfm == CFMetadata.fromSchema(cfm.toSchema())
{noformat}
In particular, we could have unit tests to check that, which would avoid things like CASSANDRA-3558.

It is not the case today for thrift because of the keyAlias. For some reason, if the keyAlias is not set, we return with toThrift() the default alias. I don't think this serves any purpose though.",avro thrift,[],CASSANDRA,Improvement,Normal,2011-12-02 12:14:13,2
12533603,Compression chunk_length_kb is not correctly returned for thrift/avro,"CASSANDRA-3492 fixed the interpretation of chunk_length_kb as a size in bytes but infortunately forgot to convert it back to kb when returning it for thrift/avro. In particular, this means that a {{describe cf}} would return things like {{chunk_length_kb: 65535}}.

I'm afraid that because migration uses Avro this is kind of a problem. One may have to issue an 'update column family' with the right chunk_length_kb to be sure to be in a safe place.",compression,[],CASSANDRA,Bug,Normal,2011-12-02 11:59:41,2
12533568,Hints are not replayed unless node was marked down,"If B drops a write from A because it is overwhelmed (but not dead), A will hint the write.  But it will never get notified that B is back up (since it was never down), so it will never attempt hint delivery.",hintedhandoff jmx,[],CASSANDRA,Bug,Normal,2011-12-02 05:46:15,7
12532941,Assertion error during bootstraping cassandra," I have a 3 node cassandra cluster. I have RF set to 3 and do reads
and writes using QUORUM.

Here is my initial ring configuration

[root@CAP4-CNode1 ~]# /root/cassandra/bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load
Owns    Token

       113427455640312821154458202477256070484
10.19.104.11    datacenter1 rack1       Up     Normal  1.66 GB
33.33%  0
10.19.104.12    datacenter1 rack1       Up     Normal  1.06 GB
33.33%  56713727820156410577229101238628035242
10.19.104.13    datacenter1 rack1       Up     Normal  1.61 GB
33.33%  113427455640312821154458202477256070484

I want to add 10.19.104.14 to the cluster.

I edited the 10.19.104.14 cassandra.yaml file and set the token to
127605887595351923798765477786913079296 and set auto_bootstrap to
true.

When I started cassandra I am getting Assertion Error.  

thanks
Ramesh




[root@CAP4-CNode4 cassandra]#  INFO 10:29:46,093 Logging initialized
 INFO 10:29:46,099 JVM vendor/version: Java HotSpot(TM) 64-Bit Server
VM/1.6.0_25
 INFO 10:29:46,100 Heap size: 8304721920/8304721920
 INFO 10:29:46,100 Classpath:
bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/apache-cassandra-1.0.2.jar:bin/../lib/apache-cassandra-clientutil-1.0.2.jar:bin/../lib/apache-cassandra-thrift-1.0.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/mx4j-examples.jar:bin/../lib/mx4j-impl.jar:bin/../lib/mx4j.jar:bin/../lib/mx4j-jmx.jar:bin/../lib/mx4j-remote.jar:bin/../lib/mx4j-rimpl.jar:bin/../lib/mx4j-rjmx.jar:bin/../lib/mx4j-tools.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar
 INFO 10:29:48,713 JNA mlockall successful
 INFO 10:29:48,726 Loading settings from
file:/root/apache-cassandra-1.0.2/conf/cassandra.yaml
 INFO 10:29:48,883 DiskAccessMode 'auto' determined to be mmap,
indexAccessMode is mmap
 INFO 10:29:48,898 Global memtable threshold is enabled at 2640MB
 INFO 10:29:49,203 Couldn't detect any schema definitions in local storage.
 INFO 10:29:49,204 Found table data in data directories. Consider
using the CLI to define your schema.
 INFO 10:29:49,220 Creating new commitlog segment
/var/lib/cassandra/commitlog/CommitLog-1321979389220.log
 INFO 10:29:49,227 No commitlog files found; skipping replay
 INFO 10:29:49,230 Cassandra version: 1.0.2
 INFO 10:29:49,230 Thrift API version: 19.18.0
 INFO 10:29:49,230 Loading persisted ring state
 INFO 10:29:49,235 Starting up server gossip
 INFO 10:29:49,259 Enqueuing flush of
Memtable-LocationInfo@122130810(192/240 serialized/live bytes, 4 ops)
 INFO 10:29:49,260 Writing Memtable-LocationInfo@122130810(192/240
serialized/live bytes, 4 ops)
 INFO 10:29:49,317 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db (300 bytes)
 INFO 10:29:49,340 Starting Messaging Service on port 7000
 INFO 10:29:49,349 JOINING: waiting for ring and schema information
 INFO 10:29:50,759 Applying migration
4b0e20f0-1511-11e1-0000-c11bc95834d7 Add keyspace: MSA, rep
strategy:SimpleStrategy{}, durable_writes: true
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Migrations@1507565381(6744/8430 serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Writing Memtable-Migrations@1507565381(6744/8430
serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Schema@1498835564(2889/3611 serialized/live bytes, 3 ops)
 INFO 10:29:50,776 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-1-Data.db (6808 bytes)
 INFO 10:29:50,777 Writing Memtable-Schema@1498835564(2889/3611
serialized/live bytes, 3 ops)
 INFO 10:29:50,797 Completed flushing
/var/lib/cassandra/data/system/Schema-h-1-Data.db (3039 bytes)
 INFO 10:29:50,814 Applying migration
4b6f2cb0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1639d811[cfId=1000,ksName=MSA,cfName=modseq,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2f984f7d,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,815 Enqueuing flush of
Memtable-Migrations@948613108(7482/9352 serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Writing Memtable-Migrations@948613108(7482/9352
serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Enqueuing flush of
Memtable-Schema@421910828(3294/4117 serialized/live bytes, 3 ops)
 INFO 10:29:50,831 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-2-Data.db (7546 bytes)
 INFO 10:29:50,832 Writing Memtable-Schema@421910828(3294/4117
serialized/live bytes, 3 ops)
 INFO 10:29:50,846 Completed flushing
/var/lib/cassandra/data/system/Schema-h-2-Data.db (3444 bytes)
 INFO 10:29:50,854 Applying migration
4b8c9fc0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1bd97d0d[cfId=1001,ksName=MSA,cfName=msgid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@63a0eec3,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,855 Enqueuing flush of
Memtable-Migrations@1520138062(7750/9687 serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Writing Memtable-Migrations@1520138062(7750/9687
serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Enqueuing flush of
Memtable-Schema@347459675(3630/4537 serialized/live bytes, 3 ops)
 INFO 10:29:50,878 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-3-Data.db (7814 bytes)
 INFO 10:29:50,879 Writing Memtable-Schema@347459675(3630/4537
serialized/live bytes, 3 ops)
 INFO 10:29:50,894 Completed flushing
/var/lib/cassandra/data/system/Schema-h-3-Data.db (3780 bytes)
 INFO 10:29:50,900 Applying migration
4ba1ae60-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@6a095b8a[cfId=1002,ksName=MSA,cfName=participants,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@c58f769,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,900 Enqueuing flush of
Memtable-Migrations@618337492(8194/10242 serialized/live bytes, 1 ops)
 INFO 10:29:50,901 Writing Memtable-Migrations@618337492(8194/10242
serialized/live bytes, 1 ops)
 INFO 10:29:50,902 Enqueuing flush of
Memtable-Schema@724860211(4020/5025 serialized/live bytes, 3 ops)
 INFO 10:29:50,917 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-4-Data.db (8258 bytes)
 INFO 10:29:50,918 Writing Memtable-Schema@724860211(4020/5025
serialized/live bytes, 3 ops)
 INFO 10:29:50,925 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-3-Data.db')]
 INFO 10:29:50,934 Completed flushing
/var/lib/cassandra/data/system/Schema-h-4-Data.db (4170 bytes)
 INFO 10:29:50,935 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-3-Data.db')]
 INFO 10:29:50,940 Applying migration
4bb4e840-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@318c69a9[cfId=1003,ksName=MSA,cfName=subinfo,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=5000.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=14400,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@796cefa8,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Migrations@1682081063(8618/10772 serialized/live bytes, 1
ops)
 INFO 10:29:50,941 Writing Memtable-Migrations@1682081063(8618/10772
serialized/live bytes, 1 ops)
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Schema@1083461053(4427/5533 serialized/live bytes, 3 ops)
 INFO 10:29:50,977 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-5-Data.db (8682 bytes)
 INFO 10:29:50,978 Writing Memtable-Schema@1083461053(4427/5533
serialized/live bytes, 3 ops)
 INFO 10:29:50,991 Compacted to
[/var/lib/cassandra/data/system/Schema-h-5-Data.db,].  14,433 to
14,106 (~97% of original) bytes for 5 keys at 0.269051MB/s.  Time:
50ms.
 INFO 10:29:50,995 Completed flushing
/var/lib/cassandra/data/system/Schema-h-7-Data.db (4577 bytes)
 INFO 10:29:51,000 Applying migration
4bc6e9a0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@20b00ec2[cfId=1004,ksName=MSA,cfName=transactions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@698f352,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,001 Enqueuing flush of
Memtable-Migrations@596545504(9027/11283 serialized/live bytes, 1 ops)
 INFO 10:29:51,002 Writing Memtable-Migrations@596545504(9027/11283
serialized/live bytes, 1 ops)
 INFO 10:29:51,003 Enqueuing flush of
Memtable-Schema@1686621532(4835/6043 serialized/live bytes, 3 ops)
 INFO 10:29:51,029 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-7-Data.db (9091 bytes)
 INFO 10:29:51,029 Writing Memtable-Schema@1686621532(4835/6043
serialized/live bytes, 3 ops)
 INFO 10:29:51,031 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-6-Data.db,].  30,426 to
30,234 (~99% of original) bytes for 1 keys at 0.272013MB/s.  Time:
106ms.
 INFO 10:29:51,044 Completed flushing
/var/lib/cassandra/data/system/Schema-h-8-Data.db (4985 bytes)
 INFO 10:29:51,049 Applying migration
4bd76460-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@4ab4faeb[cfId=1005,ksName=MSA,cfName=uid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1500000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2fc5809e,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,050 Enqueuing flush of
Memtable-Migrations@1333730706(9421/11776 serialized/live bytes, 1
ops)
 INFO 10:29:51,050 Writing Memtable-Migrations@1333730706(9421/11776
serialized/live bytes, 1 ops)
 INFO 10:29:51,051 Enqueuing flush of
Memtable-Schema@577668356(5236/6545 serialized/live bytes, 3 ops)
 INFO 10:29:51,065 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-9-Data.db (9485 bytes)
 INFO 10:29:51,066 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-6-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-7-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-5-Data.db')]
 INFO 10:29:51,066 Writing Memtable-Schema@577668356(5236/6545
serialized/live bytes, 3 ops)
 INFO 10:29:51,081 Completed flushing
/var/lib/cassandra/data/system/Schema-h-9-Data.db (5386 bytes)
 INFO 10:29:51,083 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-5-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-8-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-7-Data.db')]
 INFO 10:29:51,114 Compacted to
[/var/lib/cassandra/data/system/Schema-h-10-Data.db,].  29,054 to
28,727 (~98% of original) bytes for 8 keys at 0.913207MB/s.  Time:
30ms.
 INFO 10:29:51,144 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-10-Data.db,].  57,492 to
57,300 (~99% of original) bytes for 1 keys at 0.700584MB/s.  Time:
78ms.
 INFO 10:29:51,410 Node /10.19.104.13 is now part of the cluster
 INFO 10:29:51,412 InetAddress /10.19.104.13 is now UP
 INFO 10:29:51,414 Enqueuing flush of
Memtable-LocationInfo@709342045(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,415 Writing Memtable-LocationInfo@709342045(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,428 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db (89 bytes)
 INFO 10:29:51,439 Node /10.19.104.12 is now part of the cluster
 INFO 10:29:51,439 InetAddress /10.19.104.12 is now UP
 INFO 10:29:51,441 Enqueuing flush of
Memtable-LocationInfo@1292444743(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,441 Writing Memtable-LocationInfo@1292444743(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,455 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db (89 bytes)
 INFO 10:29:51,456 Node /10.19.104.11 is now part of the cluster
 INFO 10:29:51,457 InetAddress /10.19.104.11 is now UP
 INFO 10:29:51,459 Enqueuing flush of
Memtable-LocationInfo@1891328597(20/25 serialized/live bytes, 1 ops)
 INFO 10:29:51,459 Writing Memtable-LocationInfo@1891328597(20/25
serialized/live bytes, 1 ops)
 INFO 10:29:51,471 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db (74 bytes)
 INFO 10:29:51,473 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db')]
 INFO 10:29:51,497 Compacted to
[/var/lib/cassandra/data/system/LocationInfo-h-5-Data.db,].  552 to
444 (~80% of original) bytes for 3 keys at 0.018410MB/s.  Time: 23ms.
 INFO 10:30:19,349 JOINING: getting bootstrap token
 INFO 10:30:19,352 Enqueuing flush of
Memtable-LocationInfo@225265367(36/45 serialized/live bytes, 1 ops)
 INFO 10:30:19,353 Writing Memtable-LocationInfo@225265367(36/45
serialized/live bytes, 1 ops)
 INFO 10:30:19,364 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-7-Data.db (87 bytes)
 INFO 10:30:19,374 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:30:49,375 JOINING: Starting to bootstrap...
ERROR 10:31:13,444 Fatal exception in thread Thread[Thread-49,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
       at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
       at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
       at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:922)
       at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
       at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:102)
       at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
       at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)",compaction,[],CASSANDRA,Bug,Normal,2011-11-28 17:44:23,7
12532195,CounterColumnFamily Compaction error (ArrayIndexOutOfBoundsException),"On a single node, I'm seeing the following error when trying to compact a CounterColumnFamily. This appears to have started with version 1.0.3.

nodetool -h localhost compact TRProd MetricsAllTime
Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:250)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1471)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1523)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.cassandra.utils.ByteBufferUtil.arrayCopy(ByteBufferUtil.java:292)
	at org.apache.cassandra.db.context.CounterContext$ContextState.copyTo(CounterContext.java:792)
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:709)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:260)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:306)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:271)
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:86)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:102)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:133)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:116)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:277)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more",compaction,[],CASSANDRA,Bug,Normal,2011-11-21 19:13:35,2
12530929,"During repair, ""incorrect data size"" & ""Connection reset"" errors. Repair unable to complete.","This has been happening since 1.0.2. I wasn't on 1.0 for very long but I'm fairly certain repair was working ok. Repair worked decently for me in 0.8 (data bloat sucked). All my SSTables are version h.

On one node:

java.lang.AssertionError: incorrect row data size 596045 written to /mnt/cassandra/data/TRProd/Metrics1m-tmp-h-25036-Data.db; correct is 586675
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:253)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:146)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:87)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)

On the other node:

4999 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-24953-Data.db sections=1707 progress=0/1513497639 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25000-Data.db sections=635 progress=0/53400713 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25002-Data.db sections=570 progress=0/709993 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25003-Data.db sections=550 progress=0/449498 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25005-Data.db sections=516 progress=0/316301 - 0%], 6 sstables.
 INFO [StreamStage:1] 2011-11-09 19:45:22,795 StreamOutSession.java (line 203) Streaming to /10.38.69.192
ERROR [Streaming:1] 2011-11-09 19:47:47,964 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR [Streaming:1] 2011-11-09 19:47:47,970 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
",connection repair,[],CASSANDRA,Bug,Normal,2011-11-09 21:31:56,2
12530617,Hinted handoff not working after rolling upgrade from 0.8.7 to 1.0.2,"While testing rolling upgrades from 0.8.7 to 1.0.2 on a test cluster I've noticed that hinted hand-off didn't always work properly. Hints generated on an upgraded node does not seem to be delivered to other newly upgraded nodes once they rejoin the ring. They only way I've found to get a node to deliver its hints is to restart it.

Here's some steps to reproduce this issue:

1. Install cassandra 0.8.7 on node1 and node2 using default settings.
2. Create keyspace foo with {replication_factor: 2}. Create column family bar
3. Shutdown node2 
4. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
5. Start node2 and verify that hinted handoff is performed and HintsColumnFamily becomes empty again.

6. Upgrade and restart node1
7. Shutdown node2 
8. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
9. Upgrade and start node2
10. Notice that hinted handoff is *not* performed when ""node2"" comes back. (Only if node1 is restarted)
",hintedhandoff,[],CASSANDRA,Bug,Normal,2011-11-07 19:16:31,7
12529885,Problem SliceByNamesReadCommand on super column family after flush operation,"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: .....",supercolumns,[],CASSANDRA,Bug,Urgent,2011-11-02 16:15:25,7
12529794,local writes timing out cause attempt to hint to self,"As reported by Ramash Natarajan on the mailing list:

{noformat}
We have a 8 node cassandra cluster running 1.0.1. After running a load
test for a day we are seeing this exception in system.log file.

ERROR [EXPIRING-MAP-TIMER-1] 2011-11-01 13:20:45,350
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /10.19.102.12
       at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:339)
       at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:201)
       at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:64)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:175)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:152)
       at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
       at java.util.TimerThread.mainLoop(Timer.java:512)
       at java.util.TimerThread.run(Timer.java:462)
{noformat}",hintedhandoff,[],CASSANDRA,Bug,Normal,2011-11-02 02:11:02,7
12529618,invalidate / unregisterSSTables is confused,"invalidate doesn't call unregisterSSTables, and vice versa, making it easy to get yourself into a situation that ""shouldn't happen.""  This is causing test failures post-CASSANDRA-3116.",indexing,[],CASSANDRA,Bug,Normal,2011-11-01 04:07:39,7
12529565,CQL Metadata has inconsistent schema nomenclature,"The dumped object below shows that the default_name_type and the default_value_type are referenced inconsistently .. default_name_type should probably use the shortened version like everything else.

--- !ruby/object:CassandraCQL::Thrift::CqlResult 
rows: 
- !ruby/object:CassandraCQL::Thrift::CqlRow 
  columns: 
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: id
    timestamp: -1
    value: test string
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: test_column
    timestamp: 1320097088551000
    value: test
  key: test string
schema: !ruby/object:CassandraCQL::Thrift::CqlMetadata 
  default_name_type: org.apache.cassandra.db.marshal.UTF8Type
  default_value_type: UTF8Type
  name_types: 
    id: AsciiType
  value_types: 
    id: AsciiType
    test_column: UTF8Type
type: 1",cql,[],CASSANDRA,Bug,Low,2011-10-31 21:43:17,7
12529516,Avoid large array allocation for compressed chunk offsets,"For each compressed file we keep the chunk offsets in memory (a long[]). The size of this array is directly proportional to the sstable file and the chunk_length_kb used, but say for a 64GB sstable, we're talking ~8MB in memory by default.

Without being absolutely huge, this probably makes the life of the GC harder than necessary for the same reasons than CASSANDRA-2466, and this ticket proposes the same solution, i.e. to break down those big array into smaller ones to ease fragmentation.

Note that this is only a concern for size tiered compaction. But until leveled compaction is battle tested, the default and we know nobody uses size tiered anymore, it's probably worth making the optimization.",compression,[],CASSANDRA,Improvement,Low,2011-10-31 17:28:13,2
12529483,"CompressionMetadata is not shared across threads, we create a new one for each read","The CompressionMetada holds the compressed block offsets in memory. Without being absolutely huge, this is still of non-negligible size as soon as you have a bit of data in the DB. Reallocating this for each read is a very bad idea.

Note that this only affect range queries, since ""normal"" queries uses CompressedSegmentedFile that does reuse a unique CompressionMetadata instance.

( Background: http://thread.gmane.org/gmane.comp.db.cassandra.user/21362 )",compression,[],CASSANDRA,Bug,Normal,2011-10-31 13:57:36,2
12529403,Selecting just the row_key returns nil instead of just the row_key,"CREATE KEYSPACE CassandraCQLTestKeyspace WITH strategy_class='org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor=1
USE CassandraCQLTestKeyspace
CREATE COLUMNFAMILY row_key_validation_cf_ascii (id ascii PRIMARY KEY, test_column text)
INSERT INTO row_key_validation_cf_ascii (id, test_column) VALUES ('test string', 'test')

# Works as expected
SELECT * FROM row_key_validation_cf_ascii WHERE id = 'test string'

# Returns an empty result, unexpected
SELECT id FROM row_key_validation_cf_ascii WHERE id = 'test string'
",cql,[],CASSANDRA,Bug,Low,2011-10-30 12:22:41,7
12527792,NPE in hinted handoff,"I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more


this could possibly be related to #3291
",hintedhandoff,[],CASSANDRA,Bug,Low,2011-10-19 17:44:53,7
12527620,correct dropped messages logging,"CASSANDRA-3004 switched MessagingService back to logging only ""recent"" dropped messages instead of server lifetime totals, but the log message was not updated.",logging,[],CASSANDRA,Bug,Low,2011-10-18 17:02:52,7
12527375,Deflate Compression corrupts SSTables,"Hi,

it seems that the Deflate Compressor corrupts the SSTables. 3 out of 3 Installations were corrupt. Snappy works fine.

Here is what I did:

1. Start a single cassandra node (I was using ByteOrderedPartitioner)
2. Write data into cf that uses deflate compression - I think it has to be enough data so that the data folder contains some files.
3. When I now try to read (I did a range scan) from my application, it fails and the logs show corruptions:

Caused by: org.apache.cassandra.io.compress.CorruptedBlockException: (/home/cspriegel/Development/cassandra1/data/Test/Response-h-2-Data.db): corruption detected, chunk at 0 of length 65536.

regards,
Christian",compression,[],CASSANDRA,Bug,Normal,2011-10-16 23:04:01,2
12527269,Allow one leveled compaction task to kick off another,"Leveled compaction wants to prevent multiple tasks from running at once, but this check also defeats the ""kick off another compaction if there is more work to do"" code in CompactionTask.  So currently LCS relies completely on the every-five-minutes compaction check, which is not enough to keep up with heavy insert load.",compaction lcs,[],CASSANDRA,Improvement,Low,2011-10-14 22:55:35,7
12526654,Repair still streams unnecessary sstables,"Through rebases, CASSANDRA-2610 unfortunately got committed with the use of the wrong streaming method, the one that stream all the sstables of the keyspace.",repair,[],CASSANDRA,Bug,Low,2011-10-11 13:07:45,2
12526414,Uncompressed sizes are used to estimate space for compaction of compressed sstables,We are using the uncompressed data size when estimating if we have enough to compact sstables. This means we can easily refuse compaction when there is clearly enough room to compact.,compression,[],CASSANDRA,Bug,Normal,2011-10-10 08:30:43,2
12526275,dropping index causes some inflight mutations to fail,"dropping index causes some inflight mutations to fail. hector on client side didnt throw any exception

 INFO [MigrationStage:1] 2011-10-07 23:11:53,742 Migration.java (line 119) Applying migration fb1a8540-f128-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@786669[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@8bb33c,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=461 lim=464 cap=466],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=null, index_name='null'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:11:53,805 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:11:53,820 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:53,820 Memtable.java (line 237) Writing Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-14-Data.db (7924 bytes)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 237) Writing Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [CompactionExecutor:4] 2011-10-07 23:11:55,008 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-11-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-12-Data.db')]
 INFO [FlushWriter:3] 2011-10-07 23:11:56,430 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-14-Data.db (3470 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:11:56,446 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-12-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-11-Data.db')]
ERROR [MutationStage:56] 2011-10-07 23:11:56,508 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:56,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:51] 2011-10-07 23:11:56,539 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:51,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:38] 2011-10-07 23:11:56,633 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:38,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:57] 2011-10-07 23:11:56,664 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:57,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",indexing,[],CASSANDRA,Bug,Low,2011-10-07 21:20:13,7
12526261,remove more copies from read/write network path,RowMutation.serializedBuffer and ReadVerbHandler both do an extra copy of the serialized data. We can avoid this be pre-computing the serialized size and allocating an appropriate buffer.,network,[],CASSANDRA,Improvement,Low,2011-10-07 19:16:20,7
12526215,Improve CompactionTask extensibility,"CompactionTask is still fairly coupled to SizeTieredCompaction, including some ugly casting.",compaction,[],CASSANDRA,Improvement,Low,2011-10-07 14:34:30,7
12525770,Fail to delete -Index files if index is currently building,"If there is index building in progress, following errors are thrown if cassandra is trying to delete *-Index.db files. There is no problem with deleting -Data or -Filter.. files. CF is using leveled compaction but it is probably not related.

ERROR [NonPeriodicTasks:1] 2011-10-05 09:13:03,702 AbstractCassandraDaemon.java
(line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cas
sandra\data\test\sipdb-h-772-Index.db
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:3
4)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)",compaction indexing,[],CASSANDRA,Bug,Low,2011-10-05 07:31:53,7
12525766,Cancelling index build throws assert error,"Canceling index build throws this, but checking log there was no compaction running in background.

INFO 08:46:41,343 Writing Memtable-IndexInfo@9480253(34/42 serialized/live byte
s, 1 ops)
ERROR 08:46:41,343 Fatal exception in thread Thread[CompactionExecutor:3,5,main]

java.lang.AssertionError
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates
(SecondaryIndexManager.java:397)
        at org.apache.cassandra.db.Table.indexRow(Table.java:534)
        at org.apache.cassandra.db.index.SecondaryIndexBuilder.build(SecondaryIn
dexBuilder.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$7.run(Compaction
Manager.java:856)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 08:46:41,531 Completed flushing \var\lib\cassandra\data\system\IndexInfo-h
-1-Data.db (88 bytes)",indexing,[],CASSANDRA,Bug,Low,2011-10-05 06:58:15,7
12525723,Add compaction_thread_priority back,"In CASSANDRA-3104, this was removed with the following reasoning:

bq. compaction_throughput_mb_per_sec is a more effective throttle on compaction.

This turns out to be false in the majority of deployments.  In many (if not most) situations, compaction is actually CPU bound, not IO bound, so multithreaded compaction is generally helpful, but the priority needs to be lowered in order to prevent it from stealing CPU used for reads/writes.

Compaction is always CPU bound on both real hardware (sw raid0 with two SATA disks) and on a rackspace cloud server (though my understanding is they are back by a raid10 array underneath) however I suspect even a single drive is fast enough to handle the ~20MB/s that compaction is currently performing when unthrottled.",compaction,[],CASSANDRA,Improvement,Low,2011-10-04 19:44:44,7
12525557,truncate can still result in data being replayed after a restart,Our first stab at fixing this was CASSANDRA-2950.,commitlog,[],CASSANDRA,Bug,Normal,2011-10-03 18:02:05,7
12525396,creating column family sets durable_writes to true,"[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *false*
    Options: [datacenter1:1]
  Column Families:
[default@rapidshare] create column family t1;
1ba19300-ebfa-11e0-0000-34912694d0bf
Waiting for schema agreement...
... schemas agree across the cluster
[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *true*
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: t1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.028124999999999997/1440/6 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []",schema,[],CASSANDRA,Bug,Low,2011-10-01 07:00:16,7
12525291,Bootstrap is broken in 1.0.0-rc1,"The commit of #3219 introduced two bugs: the condition to bootstrap is that there *are* non-system tables instead, a _not_ is missing, and the setToken() was wrongly push up into the ""I'm not bootstrapping"" block so a boostrapping node was left in the joining state.",bootstrap,[],CASSANDRA,Bug,Urgent,2011-09-30 10:53:06,2
12525107,Error during multi-threaded compaction in 0.8,"I'm running 0.8.6 plus the multi-threaded compaction patch in issue 2901.  I'm getting an error compacting last night:


Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:278)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1856)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1447)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getCompactedRow(ParallelCompactionIterable.java:211)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:185)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:146)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:105)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:92)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:573)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:507)
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:320)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",compaction,[],CASSANDRA,Bug,Normal,2011-09-28 21:40:20,7
12524866,Add wide row paging for ColumnFamilyInputFormat and ColumnFamilyOutputFormat,"Hadoop input/output formats currently can OOM on wide rows.

We can add a new option to the ConfigHelper like columnPagingSize with a default of Integer.MAX_VALUE.
The input format would page the row internally rather than pull it over at once.
The output format could also use this to avoid sending huge rows over at once.",hadoop,[],CASSANDRA,Improvement,Normal,2011-09-27 13:53:07,7
12524504,AssertionError when repairing a node,"When repairing a node, the following exception was thrown two times:

{code}
ERROR [AntiEntropyStage:2] 2011-09-23 23:00:24,016 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropyStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:170)
        at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:90)
        at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.doVerb(AntiEntropyService.java:518)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

No other errors occurred on the node. From peeking at the code, this assertion appears to simply check if an existing repair session could be found. Interestingly, the repair did continue to run after this as evidenced by several other AntiEntropyService entires in the log.

8 node ring with an RF of 3, if that matters at all. No other nodes in the ring threw exceptions.",repair,[],CASSANDRA,Bug,Low,2011-09-24 06:10:36,2
12524448,inherent deadlock situation in commitLog flush?,"after my system ran for a while, it consitently goes into frozen state where all the mutations stage threads are waiting
on the switchlock,

the reason is that the switchlock is held by commit log, as shown by the following thread dump:



""COMMIT-LOG-WRITER"" prio=10 tid=0x00000000010df000 nid=0x32d3 waiting on condition [0x00007f2d81557000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3579eec060> (a java.util.concurrent.FutureTask$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:248)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.getContext(CommitLog.java:386)
        at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:650)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.commitlog.CommitLog.createNewSegment(CommitLog.java:573)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:81)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:596)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)


we can clearly see that the COMMIT-LOG-WRITER thread is running the regular appender , but the appender itself calls getContext(), which again submits a new Callable to be executed, and waits on the Callable. but the new Callable is never going to be executed since the executor has only *one* thread.


I believe this is a deterministic bug.



",commitlog,[],CASSANDRA,Bug,Urgent,2011-09-23 17:53:14,7
12524396,sstableloader ignores option doesn't work correctly,The --ignores option is supposed to take an argument but it doesn't.,bulkloader,[],CASSANDRA,Bug,Low,2011-09-23 09:07:56,2
12523921,refactor super column implmentation to use composite column names instead,"super columns are annoying.  composite columns offer a better API and performance.  people should use composites over super columns.  some people are already using super columns.  C* should implement the super column API in terms of composites to reduce code, complexity and testing as well as increase performance.",ponies,[],CASSANDRA,Improvement,Low,2011-09-21 21:09:59,2
12523796,LeveledCompaction has several performance problems,"Two main problems:

- BF size calculation doesn't take into account LCS breaking the output apart into ""bite sized"" sstables, so memory use is much higher than predicted
- ManyToMany merging is slow.  At least part of this is from running the full reducer machinery against single input sources, which can be optimized away.",lcs,[],CASSANDRA,Bug,Normal,2011-09-20 23:47:56,7
12523482,"cassandra-cli use micro second timestamp, but CQL use milli second","cassandra-cli set micro second timestamp by FBUtilities.timestampMicros. But CQL insert or update operation set milli second timestamp by AbstractModification.getTimestamp.

If you register data by cassandra-cli, you can't update data by CQL. Because CQL timestamp is judged as past time.",cql,['Legacy/CQL'],CASSANDRA,Bug,Normal,2011-09-18 15:55:13,7
12523442,LeveledCompactionStrategy is too complacent,"As the title says, it barely does anything.  I inserted 50G worth of data with 1G heap and 99% overwrite ratio, and it only compacted twice:

{noformat}
 INFO [CompactionExecutor:1] 2011-09-16 22:29:54,572 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-1-Data.db')]
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,606 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db,].  12,595,811 to 12,595,811 (~100% of original) bytes for 40,501 keys at 3.058122MBPS.  Time: 3,928ms.
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,607 CompactionTask.java (line 222) CF Total Bytes Compacted: 12,595,811
 INFO [CompactionExecutor:3] 2011-09-16 22:29:58,889 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-3-Data.db')]
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,900 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-7-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-9-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-11-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-12-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-14-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-15-Data.db,].  28,374,396 to 28,374,396 (~100% of original) bytes for 91,236 keys at 3.380379MBPS.  Time: 8,005ms.
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,901 CompactionTask.java (line 222) CF Total Bytes Compacted: 40,970,207
{noformat}

Resulting in the following levels:

{noformat}
L0: 4965
L1: 6
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

This is obviously going to result in extremely poor read performance.",compaction,[],CASSANDRA,Bug,Normal,2011-09-17 18:10:17,7
12523364,Nodes started at the same time end up with the same token,"Since autoboostrap is defaulted to on when you start a cluster at once (http://screenr.com/5G6) you can end up with nodes being assigned the same token.

{code}
INFO 17:34:55,688 Node /67.23.43.14 is now part of the cluster
 INFO 17:34:55,698 InetAddress /67.23.43.14 is now UP
 INFO 17:34:55,698 Nodes /67.23.43.14 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /67.23.43.14
 INFO 17:34:55,698 Node /98.129.220.182 is now part of the cluster
 INFO 17:34:55,698 InetAddress /98.129.220.182 is now UP
 INFO 17:34:55,698 Nodes /98.129.220.182 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /98.129.220.182
{code}",bootstrap,[],CASSANDRA,Bug,Normal,2011-09-16 17:39:55,2
12523332,A streamOutSession keeps sstables references forever if the remote end dies,"A streamOutSession acquire a reference on the sstable it will stream and release them as soon as each sstable has been fully streamed. However, since a stream session has currently no means to know when it failed, we'll keep references indefinitely (meaning until next restart) if their is a failure. One way a stream session could very easily fail is if the remote end dies. We must make sure we correctly release sstable references when that happens.

Note that it won't be bulletproof, there is probably other means by which a streaming could fail: a bug in the code throwing an exception, no space left on the receiving end, etc... But those are unlikely enough that I propose to care only for the case of a node dying for now and leave the bullet-proofing to CASSANDRA-3112. ",streaming,[],CASSANDRA,Bug,Low,2011-09-16 13:36:46,2
12523081,Log message at INFO when a global or keyspace level repair operation completes,"If JMX times out it's difficult to tell when repair completes.Right now we log at DEBUG for each column family but we need a way to tell when the repair operation completes as a whole.
",logging repair,[],CASSANDRA,Bug,Low,2011-09-14 18:05:18,2
12522905,Repair: compare all trees together (for a given range/cf) instead of by pair in isolation,"Currently, repair compare merkle trees by pair, in isolation of any other tree. What that means concretely is that if I have three node A, B and C (RF=3) with A and B in sync, but C having some range r inconsitent with both A and B (since those are consistent), we will do the following transfer of r: A -> C, C -> A, B -> C, C -> B.

The fact that we do both A -> C and C -> A is fine, because we cannot know which one is more to date from A or C. However, the transfer B -> C is useless provided we do A -> C if A and B are in sync. Not doing that transfer will be a 25% improvement in that case. With RF=5 and only one node inconsistent with all the others, that almost a 40% improvement, etc...

Given that this situation of one node not in sync while the others are is probably fairly common (one node died so it is behind), this could be a fair improvement over what is transferred. In the case where we use repair to rebuild completely a node, this will be a dramatic improvement, because it will avoid the rebuilded node to get RF times the data it should get.",repair,[],CASSANDRA,Improvement,Low,2011-09-13 14:58:03,0
12522820,Fix backwards compatibilty for cql memtable properties,Removed memtable_flush_after_mins in CASSANDRA-2183 instead of making it a no-op.,cql,[],CASSANDRA,Improvement,Low,2011-09-12 22:34:56,7
12522760,Remove special-cased maximum on sstables-to-compact for leveled strategy,With CASSANDRA-3171 fixed we don't need to be scared of large numbers of compaction candidates anymore.,compaction lcs,[],CASSANDRA,Bug,Low,2011-09-12 16:01:51,7
12522759,Compaction fails to occur,"Compaction just stops running at some point.  To repro, insert like 20M rows with a 1G heap and you'll get around 1k sstables.  Restarting doesn't help, you have to invoke a major to get anything to happen.",compaction,[],CASSANDRA,Bug,Normal,2011-09-12 15:51:23,7
12522749,Counter shard merging is not thread safe,"The first part of the counter shard merging process is done during counter replication. This was done there because it requires that all replica are made aware of the merging (we could only rely on nodetool repair for that but that seems much too fragile, it's better as just a safety net). However this part isn't thread safe as multiple threads can do the merging for the same shard at the same time (which shouldn't really ""corrupt"" the counter value per se, but result in an incorrect context).

Synchronizing that part of the code would be very costly in term of performance, so instance I propose to move the part of the shard merging done during replication to compaction. It's a better place anyway. The only downside is that it means compaction will sometime send mutations to other node as a side effect, which doesn't feel very clean but is probably not a big deal either.",counters,[],CASSANDRA,Bug,Normal,2011-09-12 14:58:17,2
12522664,Disabling hinted handoff counterintuitively continues to log handoff messages,"In order to test a theory, we tried to disable hinted handoff on our cluster.  We updated all of the yaml files and then restarted all the nodes in our cluster.  However we continue to get messages such as this in the logs after restarting:
{quote}
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:45,025 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:41:45,026 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.3.4.5
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.3.4.5
{quote}

Also looking at the System.HintsColumnFamily in jmx there is activity there such as pending tasks that come and go.",HintedHandoff,[],CASSANDRA,Bug,Low,2011-09-10 23:20:47,7
12522420,remove drivers/ from new branches,"Per previous discussions, the drivers should be removed from the newly created 1.x branches.  The attached patch (to follow shortly) removes build and test from build.xml.  The remaining bit would be to perform the actual svn rm, and the merge or merges, complete with whatever magic arguments are necessary to ensure that doesn't propagate upward to trunk (yet).",cql,['Legacy/CQL'],CASSANDRA,Task,Low,2011-09-09 13:10:54,2
12522322,GCInspector still not avoiding divide by zero,"This is because Long objects need to be compared with .equals, not ==.

CASSANDRA-3076 is the original issue but we should use a new ticket for this since 0.7.9 and 0.8.5 are both released already.",thistimeforsure,[],CASSANDRA,Bug,Low,2011-09-08 20:59:38,7
12521621,Update CQL type names to match expected (SQL) behavor,"As discussed in CASSANDRA-3031, we should make the following changes:

- rename bytea to blob
- rename date to timestamp
- remove int, pending addition of CASSANDRA-3031 (bigint and varint will be unchanged)",cql,[],CASSANDRA,Improvement,Low,2011-09-07 16:49:12,7
12521381,"Expose server, api versions to CQL",Need to expose the CQL api version; might as well include the server version while we're at it.,cql,[],CASSANDRA,New Feature,Low,2011-09-05 19:29:53,7
12521007,Compactions can (seriously) delay schema migrations,A compaction lock is acquired when dropping keyspaces or column families which will cause the schema migration to block if a compaction is in progress.,compaction,[],CASSANDRA,Bug,Normal,2011-09-01 02:42:09,7
12520595,Secondary index still does minor compacting after deleting index,We deleted all of our secondary indexes.  A couple of days later I was watching compactionstats on one of the nodes and it was in the process of minor compacting one of the deleted secondary indexes.  I double checked the keyspace definitions on the CLI and there were no secondary indexes defined.,compaction,['Feature/2i Index'],CASSANDRA,Bug,Low,2011-08-29 15:41:32,7
12520318,Leveled compaction allows multiple simultaneous compaction Tasks,"CASSANDRA-1608 attempts to restrict itself to one compaction task per CF (see discussion there for why this is necessary) by synchronizing LCS.getBackgroundTasks but this is not sufficient.  Consider this sequence of events:

1. getBackgroundTasks returns a Task for compacting some L0 sstables.  this Task is scheduled.
2. Another SSTable for this CF is flushed, so CompactionManager.submitBackground is called.  getBT is not currently in-progress so the synchronization does not stop another Task from being returned and scheduled.",lcs,[],CASSANDRA,Bug,Normal,2011-08-26 22:54:08,7
12519731,Fix count(),count() has been broken since it was introduced in CASSANDRA-1704.,cql,['Legacy/CQL'],CASSANDRA,Sub-task,Normal,2011-08-22 19:52:52,7
12519038,Implement authentication in Pig loadFunc,"Using already existing options for authentication in ConfigHelper, and adding a call to client#login just before client#set_keyspace to in CassandraStorage#initSchema",authentication pig,[],CASSANDRA,Improvement,Low,2011-08-16 19:33:33,1
12518911,AssertionError on nodetool cleanup,"While doing a cleanup I got the following AssertionError, I have tried a scrub and a major compaction before the cleanup which has not helped.

ST:

 INFO 18:49:58,540 Scrubbing SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db')
 INFO 18:49:58,834 Scrub of SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db') complete: 4 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:58,913 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db')
 INFO 18:49:59,218 Scrub of SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db') complete: 1 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,256 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db')
 INFO 18:49:59,323 Scrub of SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db') complete: 34 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,416 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db')
 INFO 18:50:50,137 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db') complete: 91735 rows in new sstable and 32 empty (tombstoned) rows dropped
 INFO 18:50:50,137 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db')
 INFO 18:50:53,075 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db') complete: 27940 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:50:53,089 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db')

 INFO 18:51:10,302 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db') complete: 70815 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:53:05,420 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5078-Data.db')
 INFO 18:53:13,266 Cleaned up to /vol/cassandra/data/SpiderServices/Content2-tmp-g-5079-Data.db.  198,705,176 to 198,705,176 (~100% of original) bytes for 27,940 keys.  Time: 7,846ms.
 INFO 18:53:13,267 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5077-Data.db')
ERROR 18:53:33,913 Fatal exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:132)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:866)
	at org.apache.cassandra.db.compaction.CompactionManager.access$500(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:204)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",exception nodetool,[],CASSANDRA,Bug,Normal,2011-08-15 19:18:47,7
12518303,NullPointerException in MessagingService.java:420,"I'm getting large quantity of exceptions during streaming. It is always in MessagingService.java:420. The streaming appears to be blocked.

 INFO 10:11:14,734 Streaming to /10.235.77.27
ERROR 10:11:14,734 Fatal exception in thread Thread[StreamStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.MessagingService.stream(MessagingService.java:420)
        at org.apache.cassandra.streaming.StreamOutSession.begin(StreamOutSession.java:176)
        at org.apache.cassandra.streaming.StreamOut.transferRangesForRequest(StreamOut.java:148)
        at org.apache.cassandra.streaming.StreamRequestVerbHandler.doVerb(StreamRequestVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",nullpointerexception streaming,[],CASSANDRA,Bug,Low,2011-08-09 10:22:23,7
12517821,We should refuse query for counters at CL.ANY,"We currently do not reject writes for counters at CL.ANY, even though this is not supported (and rightly so).",counters,[],CASSANDRA,Bug,Low,2011-08-03 18:10:50,2
12515424,Batch mutation of counters in multiple supercolumns throws an exception during replication.,"Steps to reproduce:
* Perform a batch mutation of more than one counter in more than one super-column in the same column-family.
* The following exception is thrown during replication:

DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 CounterMutationVerbHandler.java (line 52) Applying forwarded CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:8@1311696312648,]),SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:8@1311696312648,]),])]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 StorageProxy.java (line 432) insert writing local & replicate CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[cf1]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,654 Table.java (line 398) applying mutation of row 4ae71336e44bf9bf
ERROR [ReplicateOnWriteStage:125] 2011-07-26 17:05:12,655 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ReplicateOnWriteStage:125,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:123)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:120)
        at org.apache.cassandra.service.StorageProxy$5$1.runMayThrow(StorageProxy.java:455)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
",batch_mutate counters supercolumns,[],CASSANDRA,Bug,Urgent,2011-07-26 16:19:51,2
12514422,Simplified classes to write SSTables (for bulk loading usage),"sstableloader only stream existing sstables. If you need to load data existing in another form (json, csv, whatnot), you need to first write the sstable(s) to load. The recommended way to do this is either to use json2sstable or to modify it if your input is not json. Modifying json2sstable is however more involved than it needs to be, you'll need at least some basic understanding of a bunch of internal classes (DecoratedKey, ColumnFamily, SuperColumn, ...). Even for json input, you can use json2sstable only if your json actually conform to what is expected and even then, good luck to someone that want to add counters.

This ticket proposes to add a simple interface to write sstables. ",bulkloader,[],CASSANDRA,New Feature,Low,2011-07-18 11:32:32,2
12514405,Have the bulkloader rebuild the index file if necessary,"The bulk loader needs both the data file and the index file. This ticket proposes to make it rebuild the index part if it is not present (it would still be faster to have the index file around, but it is annoying to not be able to bulk load a sstable just because you happen to have deleted the index file).

",bulkloader,[],CASSANDRA,Improvement,Low,2011-07-18 07:33:16,2
12514404,Fix bulkload JMX call,"The bulkload JMX call is supposed to simplify bulkloading when done from a Cassandra node (so you don't have to configure the bulkloading client to not conflict with the node itself), but that call doesn't work (it forgets to add the ranges to stream).",bulkloader,[],CASSANDRA,Bug,Low,2011-07-18 07:23:01,2
12513859,"Don't ""replicate_on_write"" with RF=1","For counters with RF=1, we still do a read to replicate, even though there is nothing to replicate it too.",counters,[],CASSANDRA,Improvement,Low,2011-07-12 22:04:36,2
12513838,Randomize (to some extend) the choice of the first replica for counter increment,"Right now, we choose the first replica for a counter increments based solely on what the snitch returns. If the clients requests are well balanced over the cluster and the snitch not ill configured, this should not be a problem, but this is probably too strong an assumption to make.

The goal of this ticket is to change this to choose a random replica in the current data center instead.",counters,[],CASSANDRA,Improvement,Low,2011-07-12 19:28:38,2
12513837,Avoids having replicate on write tasks stacking up at CL.ONE,"The counter design involves a read on the first replica during a write. At CL.ONE, this read is not involved in the latency of the operation (the write is acknowledged before). This means it is fairly easy to insert too quickly at CL.ONE and have the replicate on write tasks falling behind. The goal of this ticket is to protect against that.

An option could be to bound the replicate on write task queue so that write start to block once we have too much of those in the queue. Another option could be to drop the oldest tasks when they are too old, but it's probably a more unsafe option.",counters,[],CASSANDRA,Improvement,Normal,2011-07-12 19:14:31,2
12513477,Allow map/reduce to use server-side query filters,"Currently, when running a MapReduce job against data in a Cassandra data store, it reads through all the data for a particular ColumnFamily.  This could be optimized to only read through those rows that have to do with the query.
",hadoop,[],CASSANDRA,New Feature,Urgent,2011-07-10 21:39:00,7
12513118,Starting 0.8.1 after upgrade from 0.7.6-2 fails,"After upgrading the binaries to 0.8.1 I get an exception when starting cassandra:

{noformat}
[root@bserv2 local]#  INFO 12:51:04,512 Logging initialized
 INFO 12:51:04,523 Heap size: 8329887744/8329887744
 INFO 12:51:04,524 JNA not found. Native methods will be disabled.
 INFO 12:51:04,531 Loading settings from file:/usr/local/apache-cassandra-0.8.1/conf/cassandra.yaml
 INFO 12:51:04,621 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 12:51:04,707 Global memtable threshold is enabled at 2648MB
 INFO 12:51:04,708 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,713 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,714 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,716 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,717 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,719 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,770 reading saved cache /vm1/cassandraDB/saved_caches/system-IndexInfo-KeyCache
 INFO 12:51:04,776 Opening /vm1/cassandraDB/data/system/IndexInfo-f-9
 INFO 12:51:04,792 reading saved cache /vm1/cassandraDB/saved_caches/system-Schema-KeyCache
 INFO 12:51:04,794 Opening /vm1/cassandraDB/data/system/Schema-f-194
 INFO 12:51:04,797 Opening /vm1/cassandraDB/data/system/Schema-f-195
 INFO 12:51:04,802 Opening /vm1/cassandraDB/data/system/Schema-f-193
 INFO 12:51:04,811 Opening /vm1/cassandraDB/data/system/Migrations-f-193
 INFO 12:51:04,814 reading saved cache /vm1/cassandraDB/saved_caches/system-LocationInfo-KeyCache
 INFO 12:51:04,815 Opening /vm1/cassandraDB/data/system/LocationInfo-f-292
 INFO 12:51:04,843 Loading schema version 586e70fd-a332-11e0-828e-34b74a661156
ERROR 12:51:04,996 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
{noformat}

It seems this has something to do with indexes, and I do have a CF with an index on it, but it is not used.
I can try and remove the index with 0.7.x binaries, but I will wait a bit to see if anyone needs it to reproduce the bug.",exception index starting,[],CASSANDRA,Bug,Normal,2011-07-07 10:20:03,7
12512248,Always use even distribution for merkle tree with RandomPartitionner,"When creating the initial merkle tree, repair tries to be (too) smart and use the key samples to ""guide"" the tree splitting. While this is a good idea for OPP where there is a good change the data distribution is uneven, you can't beat an even distribution for the RandomPartitionner. And a quick experiment even shows that the method used is significantly less efficient than an even distribution for the ranges of the merkle tree (that is, an even distribution gives a much better of distribution of the number of keys by range of the tree).

Thus let's switch to an even distribution for RandomPartitionner. That 3 lines change alone amounts for a significant improvement of repair's precision.",repair,[],CASSANDRA,Improvement,Low,2011-06-29 19:03:27,2
12511983,CFMetadata don't set the default for Replicate_on_write correctly,"Replicate_on_write *must* default to true (defaulting to false is very dangerous and imho, the option of setting it to false shouldn't exist in the first place) and CFMetadata.DEFAULT_REPLICATE_ON_WRITE is correctly true. But it doesn't get set correctly. Instead, the code force the value of the cf_def even if it wasn't defined, resulting in setting it to false since that is the default value for a boolean in JAVA.",counters,[],CASSANDRA,Bug,Normal,2011-06-28 12:47:19,2
12511479,NullPointerException after upgrade to 0.8.0,"I'm getting NullPointerException on a node upgraded from 0.7 to 0.8.0 (Debian package). The exception is thrown quickly several times after start. Then the Cassandra node is unresponsive. The Stack trace is:

ERROR 14:36:49,712 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,758 Fatal exception in thread Thread[WRITE-/10.227.101.171,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,797 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:50,756 Fatal exception in thread Thread[WRITE-/10.226.194.239,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)",nullpointerexception,[],CASSANDRA,Bug,Low,2011-06-24 14:46:04,7
12511343,Repair doesn't synchronize merkle tree creation properly,"Being a little slow, I just realized after having opened CASSANDRA-2811 and CASSANDRA-2815 that there is a more general problem with repair.

When a repair is started, it will send a number of merkle tree to its neighbor as well as himself and assume for correction that the building of those trees will be started on every node roughly at the same time (if not, we end up comparing data snapshot at different time and will thus mistakenly repair a lot of useless data). This is bogus for many reasons:
* Because validation compaction runs on the same executor that other compaction, the start of the validation on the different node is subject to other compactions. 0.8 mitigates this in a way by being multi-threaded (and thus there is less change to be blocked a long time by a long running compaction), but the compaction executor being bounded, its still a problem)
* if you run a nodetool repair without arguments, it will repair every CFs. As a consequence it will generate lots of merkle tree requests and all of those requests will be issued at the same time. Because even in 0.8 the compaction executor is bounded, some of those validations will end up being queued behind the first ones. Even assuming that the different validation are submitted in the same order on each node (which isn't guaranteed either), there is no guarantee that on all nodes, the first validation will take the same time, hence desynchronizing the queued ones.

Overall, it is important for the precision of repair that for a given CF and range (which is the unit at which trees are computed), we make sure that all node will start the validation at the same time (or, since we can't do magic, as close as possible).

One (reasonably simple) proposition to fix this would be to have repair schedule validation compactions across nodes one by one (i.e, one CF/range at a time), waiting for all nodes to return their tree before submitting the next request. Then on each node, we should make sure that the node will start the validation compaction as soon as requested. For that, we probably want to have a specific executor for validation compaction and:
* either we fail the whole repair whenever one node is not able to execute the validation compaction right away (because no thread are available right away).
* we simply tell the user that if he start too many repairs in parallel, he may start seeing some of those repairing more data than it should.
",repair,[],CASSANDRA,Bug,Normal,2011-06-23 11:35:30,2
12511327,Bad timing in repair can transfer data it is not suppose to ,"The core of the problem is that the sstables used to construct a merkle tree are not necessarily the same than the ones for which streaming is initiated. This is usually not a big deal: newly compacted sstables don't matter since data hasn't change. Newly flushed data (between the start of the validation compaction and the start of the streaming) are a little bit more problematic, even though one could argue that on average this won't be not too much data.

But there can be more problematic scenario: suppose a 3 nodes cluster with RF=3, all the data on node3 is removed and then repair is started on node3.  Also suppose the cluster havs two CFs, cf1 and cf2, sharing evenly the data.
Node3 will request the usual merkle trees and let's pretend validation compaction is mono-threaded to simplify.
What can happen is the following:
  # node3 computes its merkle trees for all requests very quickly, having no data. It will thus wait on the other nodes trees (his own trees saying ""I have no data"").
  # node1 starts computing its tree for say cf1 (queuing the computation for cf2). In the meantime, node2 may start computing the tree for cf2 (queuing the one for cf1).
  # when node1 completes his first tree, it sends it to node3. Node3 receives it, compare to his own tree and initiate transfer of all the data for cf1 from node1 to him.
  # not too long after that, node2 completes it's first tree, the one for cf2 and send it to node3. Based on it, transfer of all the data for cf2 from node2 to node3 starts.
  # An arbitrary long time after that (compaction validation can take time), node2 will finish his second tree (for cf1 that is and send it back to node3. Node3 will compare to his own (empty) tree and decide that all the ranges should be repaired with node2 for cf1. The problem is that when that happens, the transfer of cf1 from node1 may have been done already, or at least partly done. For that reason, node3 will start streaming all this data to node2.
  # For the same reasons, node3 may end up transfering all or part of cf2 to node1.

So the problem is, even though at the beginning node1 and node2 may be perfectly consistent, we will end up streaming potentially huge amount of data to them.

I think this affects both 0.7 and 0.8, though differently because compaction multi-threading and the fact that 0.8 differentiates the ranges make the timing different.

One solution (in a way, the theoretically right solution) would be to grab references to the sstables we use for the validation compaction, and only initiate streaming on these sstables. However, in 0.8 where compaction is multi-threaded, this would mean retaining compacted sstables longer that necessary. This is also a bit complicated and in particular would require a network protocol change (because a streaming request message would have to contain some information allowing to decide what set of sstables to use).

A maybe simpler solution could be to have the node coordinating the repair wait for the tree of all the remotes (in this obviously only for a given column family and range) before starting streaming.
",repair,[],CASSANDRA,Bug,Normal,2011-06-23 07:53:05,2
12510951,Repair hangs if a neighbor has nothing to send ,"This is actually a streaming problem. If a StreamOutSession has nothing to transfer (i.e, no sstables have the requested ranges), it will not even initiate the transfer and simply close the session right away. The problem is that if the session was initiated by a remote end (through a StreamRequestMessage), the remote end will never be notified and never run his callback.
",repair streaming,[],CASSANDRA,Bug,Low,2011-06-20 12:33:54,2
12510664,Add startup option renew the NodeId (for counters),"If an sstable of a counter column family is corrupted, the only safe solution a user have right now is to:
# Remove the NodeId System table to force the node to regenerate a new NodeId (and thus stop incrementing on it's previous, corrupted, subcount)
# Remove all the sstables for that column family on that node (this is important because otherwise the node will never get ""repaired"" for it's previous subcount)

This is far from being ideal, but I think this is the price we pay for avoiding the read-before-write. In any case, the first step (remove the NodeId system table) happens to remove the list of the old NodeId this node has, which could prevent us for merging the other potential previous nodeId. This is ok but sub-optimal. This ticket proposes to add a new startup flag to make the node renew it's NodeId, thus replacing this first.",counters,[],CASSANDRA,Improvement,Low,2011-06-17 14:39:01,2
12510655,"After a minor compaction, deleted key-slices are visible again","After a minor compaction, deleted key-slices are visible again.

Steps to reproduce:

1) Insert a row named ""test"".
2) Insert 500000 rows. During this step, row ""test"" is included in a major compaction:
   file-1, file-2, file-3 and file-4 compacted to file-5 (includes ""test"").
3) Delete row named ""test"".
4) Insert 500000 rows. During this step, row ""test"" is included in a minor compaction:
   file-6, file-7, file-8 and file-9 compacted to file-10 (should include tombstoned ""test"").
After step 4, row ""test"" is live again.

Test environment:

Single node with empty database.

Standard configured super-column-family (I see this behavior with several gc_grace settings (big and small values):
create column family Customers with column_type = 'Super' and comparator = 'BytesType;

In Cassandra 0.7.6 I observe the expected behavior, i.e. after step 4, the row is still deleted.

I've included a .NET program to reproduce the problem. I will add a Java version later on.",compaction,[],CASSANDRA,Bug,Normal,2011-06-17 12:42:37,2
12509845,Scrub could lose increments and replicate that loss,"If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup).",counters,[],CASSANDRA,Bug,Normal,2011-06-10 16:20:55,2
12509041,CQL protocol does not include schema information,"*The following statement fails when used with a Statement or PreparedStatement*
{code}
res = stmt.executeQuery(""SELECT bar FROM users"");  
res.next();
{code}

*Error Message*
{code}
    [junit] Testcase: simpleSelect(com.datastax.cql.reproBugTest):	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.cassandra.cql.jdbc.ColumnDecoder.makeKeyColumn(ColumnDecoder.java:136)
    [junit] 	at org.apache.cassandra.cql.jdbc.CResultSet.next(CResultSet.java:388)
    [junit] 	at com.datastax.cql.reproBugTest.simpleSelect(reproBugTest.java:57)
    [junit] 
    [junit] 
    [junit] Test com.datastax.cql.reproBugTest FAILED
{code}


*Here is a quick repro.  Showing that res.next() works with other statements but not select.*
_Also notice that ResultSet.getMetaData().getColumnCount() always returns zero._  
_I noticed in the existing driver tests similar test cases, so not sure the issue._

*Steps to run script*
* you will need to drop this in your test directory
* change the package declaration
* ant test -Dtest.name=reproBugTest

{code}
package com.datastax.cql;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

import org.junit.Test;

public class reproBugTest {
    
    @Test
    public void simpleSelect() throws Exception {   
        Connection connection = null;
        ResultSet res;
        Statement stmt;
        int colCount = 0;
        
        try {
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            
            // Check create keyspace
            connection = DriverManager.getConnection(""jdbc:cassandra:root/root@127.0.0.1:9160/default"");     
            stmt = connection.createStatement();

            try {
              System.out.println(""Running DROP KS Statement"");  
              res = stmt.executeQuery(""DROP KEYSPACE ks1"");  
              res.next();
              
              System.out.println(""Running CREATE KS Statement"");
              res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
              res.next();

            } catch (SQLException e) {
                if (e.getMessage().startsWith(""Keyspace does not exist"")) 
                {
                    res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
                } 
            }   
            connection.close();    
            
            // Run Test
            connection = DriverManager.getConnection(""jdbc:cassandra:root/root@127.0.0.1:9160/ks1"");     
            stmt = connection.createStatement();

            System.out.print(""Running CREATE CF Statement"");
            res = stmt.executeQuery(""CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)"");    
            colCount = res.getMetaData().getColumnCount();
            System.out.println("" -- Column Count: "" + colCount); 
            res.next();
            
            System.out.print(""Running INSERT Statement"");
            res = stmt.executeQuery(""INSERT INTO users (KEY, password) VALUES ('user1', 'ch@nge')"");  
            colCount = res.getMetaData().getColumnCount();
            System.out.println("" -- Column Count: "" + colCount); 
            res.next();
            
            System.out.print(""Running SELECT Statement"");
            res = stmt.executeQuery(""SELECT bar FROM users"");  
            colCount = res.getMetaData().getColumnCount();
            System.out.println("" -- Column Count: "" + colCount); 
            res.getRow();
            res.next();
                
            connection.close();               

        } catch (SQLException e) {
            e.printStackTrace();
        }
    }
       
}
{code}
",cql,['Legacy/CQL'],CASSANDRA,Sub-task,Low,2011-06-02 00:12:16,7
12507136,Cli should be able to specify a limit for get_slice,"The cli doesn't allow
{noformat}
get cf['k'] limit 5;
{noformat}
but should. Actually it should probably allow
{noformat}
get cf['k']['c':'g'] limit 10;
{noformat}",cli,['Legacy/Tools'],CASSANDRA,Improvement,Low,2011-05-13 14:40:30,2
12506191,Expose through JMX the ability to repair only the primary range,"CASSANDRA-2324 introduces the ability to do a repair only on a given range. This ticket proposes to add a nodetool repairPrimaryRange to trigger the repair of only the primary range of a node. This allows to repair a full cluster without any work duplication (or at least make it much simpler). This also introdude a global_repair command to clustertool to trigger the primary range repair on each node of the cluster one after another (we could do all in parallel, but that's probably not nice on the cluster).",repair,[],CASSANDRA,Improvement,Low,2011-05-04 16:51:52,2
12504969,Improve the precision of the repair merkle trees,"Repair uses the sstable sampled keys to split the merkle tree. This means the 'precision' of the tree will be index_interval (so 128 by default). This is probably fine when you have lots of skinny rows. But when you have less fat rows, this is probably unnecessary imprecise.

Added to that the fact that each node will not have the same set of samples, you may not always end up using the more precise range in the trees when computing differences, which could make the imprecision worst (to be fair, it is quite possible this happens very rarely).

Anyway, this ticket proposes to add an additional 'split_factor' (can be fixed, can be configurable (by the user or based on metrics on how fat the rows are)) that makes use re-split 'split_factor' times each ranges after the initial sample-based split of the tree.",repair,[],CASSANDRA,Improvement,Low,2011-04-22 11:37:02,2
12504840,CQL: create keyspace does not the replication factor argument and allows invalid sql to pass thru,"I ran the following SQL in cqlsh and immediately received a socket closed error.  After that point, I couldn't run nodetool, and then got an exception starting up the cluster.

Please Note:  The following syntax is valid in 0.74 but invalid in 0.8.
The 0.8 cassandra-cli errors on the following statement, so the resolution of the bug is to have cqlsh block this incorrect syntax.

{code}
create keyspace testcli with replication_factor=1
and placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy';
{code}

{code}
CREATE KEYSPACE testcql with replication_factor = 1 and strategy_class = 'org.apache.cassandra.locator.SimpleStrategy';	
{code}


{code}
ERROR 01:29:26,989 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
{code}",cql,[],CASSANDRA,Bug,Urgent,2011-04-21 01:35:08,7
12504694,missing imports in CQL Python driver,"Try:

bq. cd drivers/py && python -c 'from cql import DateFromTicks; DateFromTicks(1)'

Also:
{{cql.connection}} is missing an import of {{AuthenticationRequest}} from {{ttypes}}, and the exceptions {{NotSupportedError}}, and {{InternalError}}.

Also:
{{marshal.unmarshal_long}} has a NameError waiting to happen in the form of ""unpack""",cql,['Legacy/Tools'],CASSANDRA,Bug,Normal,2011-04-19 17:44:05,7
12504618,"Eagerly re-write data at read time (""superseding / defragmenting"")","Oncdsed. This basic approach would improve read performance considerably, but would cause a lot of duplicate data to be written, and would make compaction's work more necessary.

Augmenting the basic idea, if when we superseded data in a file we marked it as superseded somehow, the next compaction that touched that file could remove the data. Since our file format is immutable, the values that a particular sstable superseded could be recorded in a component of that sstable. If we always supersede at the ""block"" level (as defined by CASSANDRA-674 or CASSANDRA-47), then the list of superseded blocks could be represented using a generation number and a bitmap of block numbers. Since 2498 would already allow for sstables to be eliminated due to timestamps, this information would probably only be used at compaction time (by loading all superseding information in the system for the sstables that are being compacted).

Initially described on [1608|https://issues.apache.org/jira/secure/EditComment!default.jspa?id=12477095&commentId=12920353].",compaction performance,[],CASSANDRA,New Feature,Normal,2011-04-19 02:16:01,7
12504612,Error running cqlsh from .tar file -- global name 'SchemaDisagreementException' is not defined,"*Error when running cqlsh*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 126, in execute
    except SchemaDisagreementException, sde:
NameError: global name 'SchemaDisagreementException' is not defined
{code}


*Build*
* Install the cassandra binary from the nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz

* Install cql from .tar file on nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/cql-1.0.0.tar.gz

*CQL Install Output*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ sudo python2.6 ./setup.py install
[sudo] password for cassandra: 
running install
running build
running build_py
running build_scripts
creating build/scripts-2.6
copying and adjusting cqlsh -> build/scripts-2.6
changing mode of build/scripts-2.6/cqlsh from 644 to 755
running install_lib
creating /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/results.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/marshal.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/cursor.py -> /usr/lib/python2.6/site-packages/cql
creating /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/__init__.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/Cassandra.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/constants.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/ttypes.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/decoders.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/__init__.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/errors.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection_pool.py -> /usr/lib/python2.6/site-packages/cql
byte-compiling /usr/lib/python2.6/site-packages/cql/results.py to results.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/marshal.py to marshal.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection.py to connection.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cursor.py to cursor.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/Cassandra.py to Cassandra.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/constants.py to constants.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/ttypes.py to ttypes.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/decoders.py to decoders.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/errors.py to errors.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection_pool.py to connection_pool.pyc
running install_scripts
copying build/scripts-2.6/cqlsh -> /usr/bin
changing mode of /usr/bin/cqlsh to 755
running install_egg_info
Writing /usr/lib/python2.6/site-packages/cql-1.0.0-py2.6.egg-info

{code}
",cql,[],CASSANDRA,Bug,Normal,2011-04-18 22:51:39,7
12504590,Improve read performance in update-intensive workload,"Read performance in an update-heavy environment relies heavily on compaction to maintain good throughput. (This is not the case for workloads where rows are only inserted once, because the bloom filter keeps us from having to check sstables unnecessarily.)

Very early versions of Cassandra attempted to mitigate this by checking sstables in descending generation order (mostly equivalent to descending mtime): once all the requested columns were found, it would not check any older sstables.

This was incorrect, because data timestamp will not correspond to sstable timestamp, both because compaction has the side effect of ""refreshing"" data to a newer sstable, and because hintead handoff may send us data older than what we already have.

Instead, we could create a per-sstable piece of metadata containing the most recent (client-specified) timestamp for any column in the sstable.  We could then sort sstables by this timestamp instead, and perform a similar optimization (if the remaining sstable client-timestamps are older than the oldest column found in the desired result set so far, we don't need to look further). Since under almost every workload, client timestamps of data in a given sstable will tend to be similar, we expect this to cut the number of sstables down proportionally to how frequently each column in the row is updated. (If each column is updated with each write, we only have to check a single sstable.)

This may also be useful information when deciding which SSTables to compact.

(Note that this optimization is only appropriate for named-column queries, not slice queries, since we don't know what non-overlapping columns may exist in older sstables.)",ponies,[],CASSANDRA,Improvement,Low,2011-04-18 20:00:04,7
12504294,Custom CQL protocol/transport,"A custom wire protocol would give us the flexibility to optimize for our specific use-cases, and eliminate a troublesome dependency (I'm referring to Thrift, but none of the others would be significantly better).  Additionally, RPC is bad fit here, and we'd do better to move in the direction of something that natively supports streaming.

I don't think this is as daunting as it might seem initially.  Utilizing an existing server framework like Netty, combined with some copy-and-paste of bits from other FLOSS projects would probably get us 80% of the way there.",cql,['Legacy/CQL'],CASSANDRA,New Feature,Low,2011-04-14 16:56:40,2
12503646,Failed Streams Break Repair,"Running repair in cases where a stream fails we are seeing multiple problems.

1. Although retry is initiated and completes, the old stream doesn't seem to clean itself up and repair hangs.
2. The temp files are left behind and multiple failures can end up filling up the data partition.

These issues together are making repair very difficult for nearly everyone running repair on a non-trivial sized data set.

This issue is also being worked on w.r.t CASSANDRA-2088, however that was moved to 0.8 for a few reasons. This ticket is to fix the immediate issues that we are seeing in 0.7.",repair,[],CASSANDRA,Bug,Normal,2011-04-07 15:40:10,2
12503450,Risk of counter over-count when recovering commit log,"When a memtable was flush, there is a small delay before the commit log replay position gets updated. If the node fails during this delay, all the updates of this memtable will be replay during commit log recovery and will end-up being over-counts.",counters,[],CASSANDRA,Bug,Normal,2011-04-05 20:49:22,2
12502977,Compaction thread should try to empty a bucket before moving on,"As suggested by Aaron Morton [(1)|https://issues.apache.org/jira/browse/CASSANDRA-2191?focusedCommentId=13010077&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13010077], a compaction thread should attempt to empty a bucket before moving on to a larger bucket. This would change the submitMinorIfNeeded {{for}} loop into a while loop that regenerated the buckets and started from the bottom after each successful compaction.",compaction,[],CASSANDRA,Improvement,Low,2011-03-31 05:29:09,7
12501263,Promote row index,"The row index contains entries for configurably sized blocks of a wide row. For a row of appreciable size, the row index ends up directing the third seek (1. index, 2. row index, 3. content) to nearby the first column of a scan.

Since the row index is always used for wide rows, and since it contains information that tells us whether or not the 3rd seek is necessary (the column range or name we are trying to slice may not exist in a given sstable), promoting the row index into the sstable index would allow us to drop the maximum number of seeks for wide rows back to 2, and, more importantly, would allow sstables to be eliminated using only the index.

An example usecase that benefits greatly from this change is time series data in wide rows, where data is appended to the beginning or end of the row. Our existing compaction strategy gets lucky and clusters the oldest data in the oldest sstables: for queries to recently appended data, we would be able to eliminate wide rows using only the sstable index, rather than needing to seek into the data file to determine that it isn't interesting. For narrow rows, this change would have no effect, as they will not reach the threshold for indexing anyway.

A first cut design for this change would look very similar to the file format design proposed on #674: http://wiki.apache.org/cassandra/FileFormatDesignDoc: row keys clustered, column names clustered, and offsets clustered and delta encoded.",index timeseries,[],CASSANDRA,Improvement,Normal,2011-03-12 19:44:43,2
12501231,NoSuchElement exception on node which is streaming a repair,"Running latest SVN snapshot of 0.7.

When I ran a repair on a node, that node's neighbor threw the following exception. Let me know what other info could be helpful.

{code}
 INFO 23:43:44,358 Streaming to /10.251.166.15
ERROR 23:50:21,321 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.util.NoSuchElementException
        at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:146)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:366)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:825)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}",repair,[],CASSANDRA,Bug,Normal,2011-03-12 07:12:22,7
12499332,Compaction can echo data which breaks upon sstable format changes,"While compaction, if for a row we have only 1 sstable holding data, we echo this data. This breaks when we change the data format, creating mixed (corrupted) sstable.

(I suspect this is the cause of CASSANDRA-2195, but opening a new ticket until we can confirm that hunch)",compaction,[],CASSANDRA,Bug,Urgent,2011-02-22 12:17:19,2
12498670,EOFException during name query,"As reported by Jonas Borgstrom on the mailing list:

{quote}
While testing the new 0.7.1 release I got the following exception:

ERROR [ReadStage:11] 2011-02-15 16:39:18,105
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.io.IOError: java.io.EOFException
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:75)
       at
org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
       at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
       at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1274)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1166)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
       at org.apache.cassandra.db.Table.getRow(Table.java:384)
       at
org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
       at
org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
       at java.io.DataInputStream.readInt(DataInputStream.java:392)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:48)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
       at
org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
       ... 12 more

{quote}",EOF,[],CASSANDRA,Bug,Normal,2011-02-15 18:11:20,7
12498027,"Add ""reduce memory usage because I tuned things poorly"" feature","Users frequently create too many columnfamilies, set the memtable thresholds too high (or adjust throughput while ignoring operations), and/or set caching thresholds too high.  Then their server OOMs and they tell their friends Cassandra sucks.",ponies,[],CASSANDRA,Improvement,Low,2011-02-08 20:35:01,7
12497624,Warn or fail when read repair is disabled with the dynamic snitch,"When read repair is completely disabled, the dynamic snitch will never adjust.

Options:
# Explicitly disallow running with RR==0 + des
# Implicitly enable RR at some low fraction when des is in use
# Log warnings",des,[],CASSANDRA,Improvement,Low,2011-02-03 22:00:39,7
12497550,Fix the read race condition in CFStore for counters ,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
",counters,[],CASSANDRA,Bug,Normal,2011-02-03 10:51:07,2
12497480,Add description to nodetool commands,"The help of nodetool is not very pretty, in particular there is no description of proposed commands",nodetool,[],CASSANDRA,Improvement,Low,2011-02-02 17:15:35,2
12497179,Eliminate excess comparator creation,"Despite the singleton status of each AbstractType, we end up creating at least one new comparator per query. By making more of the ""wrapper"" comparators that exist in the codebase members of AbstractType, we could cut down on the ""new Comparator"" spam.",abstract_types gc,[],CASSANDRA,Improvement,Low,2011-01-31 07:40:22,7
12495682,"CFS.maybeSwitchMemtable() calls CommitLog.instance.getContext(), which may block, under flusher lock write lock","While investigate CASSANDRA-1955 I realized I was seeing very poor latencies for reasons that had nothing to do with flush_writers, even when using periodic commit log mode (and flush writers set ridiculously high, 500).

It turns out writes blocked were slow because Table.apply() was spending lots of time (I can easily trigger seconds on moderate work-load) trying to acquire a flusher lock read lock (""flush lock millis"" log printout in the logging patch I'll attach).

That in turns is caused by CFS.maybeSwitchMemtable() which acquires the flusher lock write lock.

Bisecting further revealed that the offending line of code that blocked was:

                    final CommitLogSegment.CommitLogContext ctx = writeCommitLog ? CommitLog.instance.getContext() : null;

Indeed, CommitLog.getContext() simply returns currentSegment().getContext(), but does so by submitting a callable on the service executor. So independently of flush writers, this can block all (global, for all cf:s) writes very easily, and does.

I'll attach a file that is an independent Python script that triggers it on my macos laptop (with an intel SSD, which is why I was particularly surprised) (it assumes CPython, out-of-the-box-or-almost Cassandra on localhost that isn't in a cluster, and it will drop/recreate a keyspace called '1955').

I'm also attaching, just FYI, the patch with log entries that I used while tracking it down.

Finally, I'll attach a patch with a suggested solution of keeping track of the latest commit log with an AtomicReference (as an alternative to synchronizing all access to segments). With that patch applied, latencies are not affected by my trigger case like they were before. There are some sub-optimal > 100 ms cases on my test machine, but for other reasons. I'm no longer able to trigger the extremes.

",commitlog,[],CASSANDRA,Improvement,Normal,2011-01-15 18:29:43,7
12463494,ColumnFamilyRecordReader returns duplicate rows,"There's a bug in ColumnFamilyRecordReader that appears when processing a single split (which happens in most tests that have small number of rows), and potentially in other cases.  When the start and end tokens of the split are equal, duplicate rows can be returned.

Example with 5 rows:
token (start and end) = 53193025635115934196771903670925341736

Tokens returned by first get_range_slices iteration (all 5 rows):
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190
 99079589977253916124855502156832923443
 144992942750327304334463589818972416113
 166860289390734216023086131251507064403

Tokens returned by next iteration (first token is last token from
previous, end token is unchanged)
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190

Tokens returned by final iteration  (first token is last token from
previous, end token is unchanged)
 [] (empty)

In this example, the mapper has processed 7 rows in total, 2 of which
were duplicates.

",hadoop mapreduce,[],CASSANDRA,Bug,Normal,2010-05-01 16:18:02,7
