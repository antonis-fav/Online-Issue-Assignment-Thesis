id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13391377,Improvements to security related Delegators in org.apache.jackrabbit.oak.jcr.delegate,"the security related Delegator classes in _org.apache.jackrabbit.oak.jcr.delegate_ deserve a bit of housekeeping.
while doing so i spotted that the param validation in the {{UserManagerDelegator}} constructor throws {{IllegalStateException}} instead of {{IllegalArgumentException}}.",technical_debt,['jcr'],OAK,Improvement,Minor,2021-07-22 15:05:19,0
13391165,Enable minimum line and branch test coverage for oak-jcr,"today _jacoco-maven-plugin_ is skipped in _oak-jcr_ and thus doesn't record line/branch coverage. i would suggest to enable it by default. as of now line coverage is 72%, branch coverage is 60%.

[~mreutegg], unless you have any concerns i would go ahead and add the 2 properties to the pom.xml:
{code}
    <skip.coverage>false</skip.coverage>
    <minimum.coverage>0.72</minimum.coverage>
    <minimum.branch.coverage>0.60</minimum.branch.coverage>
{code}",technical_debt,['jcr'],OAK,Improvement,Minor,2021-07-21 16:46:32,0
13391157,Duplicate code blocks in authorization modules,there area  few duplicate code blocks across oak authorization modules.,technical_debt,"['authorization-principalbased', 'core', 'security-spi']",OAK,Improvement,Minor,2021-07-21 15:41:56,0
13391102,link to PrincipalProvider points to wrong resource,on https://jackrabbit.apache.org/oak/docs/security/principal/differences.html the link to the {{PrincipalProvider}} points to the {{PrincipalManager}}.,technical_debt,['doc'],OAK,Documentation,Major,2021-07-21 09:36:01,0
13391100,Oak Security Documentation : links to Jackrabbit-API point to svn,with OAK-8339 the Jackrabbit API has been moved to the Oak source but the links to security related interfaces still point to svn.apache.org. instead they are now generated with the oak javadoc and links should be adjusted such that they capture the latest state of the API.,technical_debt,"['doc', 'security']",OAK,Documentation,Major,2021-07-21 09:32:54,0
13389668,UserInitializer: info property of 'principalName' index not accurate,"the description of the 'principalName' index definition stored in the 'info' property is IMHO misleading. 
[~thomasm], do you recall what your intention was when stating _if it was constructed manually_? that sounds a bit odd as the rep:principalName property is always defined when a new user/group is created through the API. on JCR level users/groups cannot be created manually using regular write operations due to the protected nature some properties.",technical_debt,['core'],OAK,Improvement,Trivial,2021-07-14 16:33:03,0
13389392,Address vulnerabilities found by dependency checker plugin,"{noformat}
One or more dependencies were identified with known vulnerabilities in Jackrabbit Oak:aggs-matrix-stats-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/aggs-matrix-stats-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
bcprov-jdk15on-1.65.jar (pkg:maven/org.bouncycastle/bcprov-jdk15on@1.65, cpe:2.3:a:bouncycastle:legion-of-the-bouncy-castle-java-crytography-api:1.65:*:*:*:*:*:*:*) : CVE-2020-28052
commons-io-2.6.jar (pkg:maven/commons-io/commons-io@2.6, cpe:2.3:a:apache:commons_io:2.6:*:*:*:*:*:*:*) : CVE-2021-29425
cxf-core-3.3.6.jar (pkg:maven/org.apache.cxf/cxf-core@3.3.6, cpe:2.3:a:apache:cxf:3.3.6:*:*:*:*:*:*:*) : CVE-2020-13954, CVE-2021-22696, CVE-2021-30468
elasticsearch-core-7.1.1.jar (pkg:maven/org.elasticsearch/elasticsearch-core@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
fluent-hc-4.5.12.jar (pkg:maven/org.apache.httpcomponents/fluent-hc@4.5.12, cpe:2.3:a:apache:httpclient:4.5.12:*:*:*:*:*:*:*) : CVE-2020-13956
groovy-2.5.2.jar (pkg:maven/org.codehaus.groovy/groovy@2.5.2, cpe:2.3:a:apache:groovy:2.5.2:*:*:*:*:*:*:*) : CVE-2020-17521
groovy-all-2.4.17.jar (pkg:maven/org.codehaus.groovy/groovy-all@2.4.17, cpe:2.3:a:apache:groovy:2.4.17:*:*:*:*:*:*:*) : CVE-2020-17521
guava-15.0.jar (pkg:maven/com.google.guava/guava@15.0, cpe:2.3:a:google:guava:15.0:*:*:*:*:*:*:*) : CVE-2018-10237, CVE-2020-8908
guava-18.0.jar (pkg:maven/com.google.guava/guava@18.0, cpe:2.3:a:google:guava:18.0:*:*:*:*:*:*:*) : CVE-2018-10237, CVE-2020-8908
hibernate-validator-5.3.6.Final.jar (pkg:maven/org.hibernate/hibernate-validator@5.3.6.Final, cpe:2.3:a:hibernate:hibernate-validator:5.3.6:*:*:*:*:*:*:*, cpe:2.3:a:redhat:hibernate_validator:5.3.6:*:*:*:*:*:*:*) : CVE-2020-10693
http2-client-9.4.27.v20200227.jar (pkg:maven/org.eclipse.jetty.http2/http2-client@9.4.27.v20200227, cpe:2.3:a:eclipse:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.27:20200227:*:*:*:*:*:*) : CVE-2019-17638, CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
httpclient-4.5.12.jar (pkg:maven/org.apache.httpcomponents/httpclient@4.5.12, cpe:2.3:a:apache:httpclient:4.5.12:*:*:*:*:*:*:*) : CVE-2020-13956
httpclient-osgi-4.5.12.jar/META-INF/maven/org.apache.httpcomponents/httpclient-cache/pom.xml (pkg:maven/org.apache.httpcomponents/httpclient-cache@4.5.12, cpe:2.3:a:apache:httpclient:4.5.12:*:*:*:*:*:*:*) : CVE-2020-13956
jackson-databind-2.10.3.jar (pkg:maven/com.fasterxml.jackson.core/jackson-databind@2.10.3, cpe:2.3:a:fasterxml:jackson-databind:2.10.3:*:*:*:*:*:*:*) : CVE-2020-25649
java-xmlbuilder-1.1.jar (pkg:maven/com.jamesmurty.utils/java-xmlbuilder@1.1) : CWE-611: Improper Restriction of XML External Entity Reference ('XXE')
javax-websocket-server-impl-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty.websocket/javax-websocket-server-impl@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:java-websocket_project:java-websocket:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
javax.servlet-3.0.0.v201112011016.jar (pkg:maven/org.eclipse.jetty.orbit/javax.servlet@3.0.0.v201112011016, cpe:2.3:a:eclipse:jetty:3.0.0:201112011016:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:3.0.0:201112011016:*:*:*:*:*:*) : CVE-2009-5045, CVE-2009-5046, CVE-2017-7656, CVE-2017-7657, CVE-2017-7658, CVE-2020-27216, CVE-2021-28169, CVE-2021-34428
javax.websocket-api-1.0.jar (pkg:maven/javax.websocket/javax.websocket-api@1.0, cpe:2.3:a:java-websocket_project:java-websocket:1.0:*:*:*:*:*:*:*) : CVE-2020-11050
jdom2-2.0.6.jar (pkg:maven/org.jdom/jdom2@2.0.6, cpe:2.3:a:jdom:jdom:2.0.6:*:*:*:*:*:*:*) : CVE-2021-33813
jetty-http-9.4.27.v20200227.jar (pkg:maven/org.eclipse.jetty/jetty-http@9.4.27.v20200227, cpe:2.3:a:eclipse:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.27:20200227:*:*:*:*:*:*) : CVE-2019-17638, CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
jetty-io-8.2.0.v20160908.jar (pkg:maven/org.eclipse.jetty/jetty-io@8.2.0.v20160908, cpe:2.3:a:mortbay_jetty:jetty:8.2.0:20160908:*:*:*:*:*:*) : CVE-2021-28165
jetty-io-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty/jetty-io@9.4.18.v20190429, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2021-28165
jetty-io-9.4.27.v20200227.jar (pkg:maven/org.eclipse.jetty/jetty-io@9.4.27.v20200227, cpe:2.3:a:mortbay_jetty:jetty:9.4.27:20200227:*:*:*:*:*:*) : CVE-2021-28165
jetty-server-8.2.0.v20160908.jar (pkg:maven/org.eclipse.jetty/jetty-server@8.2.0.v20160908, cpe:2.3:a:eclipse:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:8.2.0:20160908:*:*:*:*:*:*) : CVE-2017-7656, CVE-2017-7657, CVE-2017-7658, CVE-2017-9735, CVE-2019-10241, CVE-2019-10247, CVE-2020-27216, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
jetty-server-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty/jetty-server@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
jetty-util-8.2.0.v20160908.jar (pkg:maven/org.eclipse.jetty/jetty-util@8.2.0.v20160908, cpe:2.3:a:eclipse:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:8.2.0:20160908:*:*:*:*:*:*) : CVE-2017-7656, CVE-2017-7657, CVE-2017-7658, CVE-2019-10247, CVE-2020-27216, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
junit-4.12.jar (pkg:maven/junit/junit@4.12) : CVE-2020-15250
lang-mustache-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/lang-mustache-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
log4j-1.2.16.jar (pkg:maven/log4j/log4j@1.2.16, cpe:2.3:a:apache:log4j:1.2.16:*:*:*:*:*:*:*) : CVE-2019-17571, CVE-2020-9488
log4j-1.2.17.jar (pkg:maven/log4j/log4j@1.2.17, cpe:2.3:a:apache:log4j:1.2.17:*:*:*:*:*:*:*) : CVE-2019-17571, CVE-2020-9488
log4j-api-2.11.1.jar (pkg:maven/org.apache.logging.log4j/log4j-api@2.11.1, cpe:2.3:a:apache:log4j:2.11.1:*:*:*:*:*:*:*) : CVE-2020-9488
log4j-over-slf4j-1.7.30.jar (pkg:maven/org.slf4j/log4j-over-slf4j@1.7.30, cpe:2.3:a:apache:log4j:1.7.30:*:*:*:*:*:*:*) : CVE-2020-9488
mongo-java-driver-3.12.7.jar (pkg:maven/org.mongodb/mongo-java-driver@3.12.7, cpe:2.3:a:mongodb:java_driver:3.12.7:*:*:*:*:*:*:*) : CVE-2021-20328
netty-3.7.0.Final.jar (pkg:maven/io.netty/netty@3.7.0.Final, cpe:2.3:a:netty:netty:3.7.0:*:*:*:*:*:*:*) : CVE-2014-0193, CVE-2014-3488, CVE-2015-2156, CVE-2019-16869, CVE-2019-20444, CVE-2019-20445, CVE-2021-21290, CVE-2021-21295, CVE-2021-21409, POODLE vulnerability in SSLv3.0 support
netty-transport-4.1.47.Final.jar (pkg:maven/io.netty/netty-transport@4.1.47.Final, cpe:2.3:a:netty:netty:4.1.47:*:*:*:*:*:*:*) : CVE-2021-21290, CVE-2021-21295, CVE-2021-21409
netty-transport-4.1.52.Final.jar (pkg:maven/io.netty/netty-transport@4.1.52.Final, cpe:2.3:a:netty:netty:4.1.52:*:*:*:*:*:*:*) : CVE-2021-21290, CVE-2021-21295, CVE-2021-21409
oak-jackrabbit-api-1.34.0.jar (pkg:maven/org.apache.jackrabbit/oak-jackrabbit-api@1.34.0, cpe:2.3:a:apache:jackrabbit:1.34.0:*:*:*:*:*:*:*, cpe:2.3:a:apache:jackrabbit_oak:1.34.0:*:*:*:*:*:*:*) : CVE-2015-1833
oak-segment-1.6.0.jar (pkg:maven/org.apache.jackrabbit/oak-segment@1.6.0, cpe:2.3:a:apache:jackrabbit:1.6.0:*:*:*:*:*:*:*, cpe:2.3:a:apache:jackrabbit_oak:1.6.0:*:*:*:*:*:*:*) : CVE-2015-1833, CVE-2020-1940
org.apache.felix.webconsole-4.2.10-all.jar: jquery-1.8.3.js (pkg:javascript/jquery@1.8.3) : CVE-2012-6708, CVE-2015-9251, CVE-2019-11358, CVE-2020-11022, CVE-2020-11023
org.apache.felix.webconsole-4.2.10-all.jar: jquery-ui-1.9.2.js (pkg:javascript/jquery-ui-dialog@1.9.2, pkg:javascript/jquery-ui-tooltip@1.9.2) : CVE-2010-5312, CVE-2012-6662, CVE-2016-7103
pom.xml (pkg:maven/org.apache.jackrabbit/oak-jackrabbit-api@1.22.8-SNAPSHOT, cpe:2.3:a:apache:jackrabbit:1.22.8:snapshot:*:*:*:*:*:*, cpe:2.3:a:apache:jackrabbit_oak:1.22.8:snapshot:*:*:*:*:*:*) : CVE-2015-1833
pom.xml (pkg:maven/org.apache.jackrabbit/oak-solr-core@1.22.8-SNAPSHOT, cpe:2.3:a:apache:jackrabbit_oak:1.22.8:snapshot:*:*:*:*:*:*, cpe:2.3:a:apache:solr:1.22.8:snapshot:*:*:*:*:*:*) : CVE-2012-6612, CVE-2013-6397, CVE-2013-6407, CVE-2013-6408, CVE-2015-8795, CVE-2015-8796, CVE-2015-8797, CVE-2017-3163, CVE-2017-3164, CVE-2018-11802, CVE-2018-1308, CVE-2019-0193, CVE-2020-13941, CVE-2021-27905, CVE-2021-29262, CVE-2021-29943
org.apache.servicemix.bundles.dom4j-2.1.1_1.jar (pkg:maven/org.apache.servicemix.bundles/org.apache.servicemix.bundles.dom4j@2.1.1_1, cpe:2.3:a:dom4j_project:dom4j:2.1.1.1:*:*:*:*:*:*:*) : CVE-2020-10683
org.apache.sling.commons.logservice-1.0.4.jar (pkg:maven/org.apache.sling/org.apache.sling.commons.logservice@1.0.4, cpe:2.3:a:apache:sling:1.0.4:*:*:*:*:*:*:*) : CVE-2016-5394, CVE-2016-6798
parent-join-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/parent-join-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
pdfbox-2.0.19.jar (pkg:maven/org.apache.pdfbox/pdfbox@2.0.19, cpe:2.3:a:apache:pdfbox:2.0.19:*:*:*:*:*:*:*) : CVE-2021-27807, CVE-2021-27906, CVE-2021-31811, CVE-2021-31812
preflight-2.0.19.jar (pkg:maven/org.apache.pdfbox/preflight@2.0.19, cpe:2.3:a:apache:pdfbox:2.0.19:*:*:*:*:*:*:*) : CVE-2021-27807, CVE-2021-27906, CVE-2021-31811, CVE-2021-31812
rank-eval-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/rank-eval-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
sentiment-analysis-parser-0.1.jar (pkg:maven/edu.usc.ir/sentiment-analysis-parser@0.1, cpe:2.3:a:data_tools_project:data_tools:0.1:*:*:*:*:*:*:*) : CVE-2018-18749
sis-netcdf-1.0.jar (pkg:maven/org.apache.sis.storage/sis-netcdf@1.0, cpe:2.3:a:storage_project:storage:1.0:*:*:*:*:*:*:*) : CVE-2021-20291
snakeyaml-1.17.jar (pkg:maven/org.yaml/snakeyaml@1.17, cpe:2.3:a:snakeyaml_project:snakeyaml:1.17:*:*:*:*:*:*:*) : CVE-2017-18640
solr-solrj-8.6.3.jar (pkg:maven/org.apache.solr/solr-solrj@8.6.3, cpe:2.3:a:apache:solr:8.6.3:*:*:*:*:*:*:*) : CVE-2021-27905, CVE-2021-29262, CVE-2021-29943
spring-core-4.3.24.RELEASE.jar (pkg:maven/org.springframework/spring-core@4.3.24.RELEASE, cpe:2.3:a:pivotal_software:spring_framework:4.3.24:release:*:*:*:*:*:*, cpe:2.3:a:springsource:spring_framework:4.3.24:release:*:*:*:*:*:*, cpe:2.3:a:vmware:spring_framework:4.3.24:release:*:*:*:*:*:*, cpe:2.3:a:vmware:springsource_spring_framework:4.3.24:release:*:*:*:*:*:*) : CVE-2020-5421
tagsoup-1.2.1.jar (pkg:maven/org.ccil.cowan.tagsoup/tagsoup@1.2.1, cpe:2.3:a:tag_project:tag:1.2.1:*:*:*:*:*:*:*) : CVE-2020-29242, CVE-2020-29243, CVE-2020-29244, CVE-2020-29245
tika-core-1.24.1.jar (pkg:maven/org.apache.tika/tika-core@1.24.1, cpe:2.3:a:apache:tika:1.24.1:*:*:*:*:*:*:*) : CVE-2021-28657
vorbis-java-tika-0.8.jar (pkg:maven/org.gagravarr/vorbis-java-tika@0.8, cpe:2.3:a:flac_project:flac:0.8:*:*:*:*:*:*:*) : CVE-2017-6888
websocket-common-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty.websocket/websocket-common@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:java-websocket_project:java-websocket:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:websocket-extensions_project:websocket-extensions:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
websocket-server-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty.websocket/websocket-server@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:java-websocket_project:java-websocket:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
xmpbox-2.0.19.jar (pkg:maven/org.apache.pdfbox/xmpbox@2.0.19, cpe:2.3:a:apache:pdfbox:2.0.19:*:*:*:*:*:*:*) : CVE-2021-27807, CVE-2021-27906, CVE-2021-31811, CVE-2021-31812
zookeeper-3.4.6.jar (pkg:maven/org.apache.zookeeper/zookeeper@3.4.6, cpe:2.3:a:apache:zookeeper:3.4.6:*:*:*:*:*:*:*) : CVE-2016-5017, CVE-2017-5637, CVE-2018-8012, CVE-2019-0201, CVE-2021-21409
zookeeper-3.5.7.jar (pkg:maven/org.apache.zookeeper/zookeeper@3.5.7, cpe:2.3:a:apache:zookeeper:3.5.7:*:*:*:*:*:*:*) : CVE-2021-21409
-1,548 {noformat}",candidate_oak_1_22,[],OAK,Task,Major,2021-07-13 13:15:55,1
13385676,Use Filter for SyncConfigTracker to limit respected references ,while reviewing the proposed changes for OAK-9462 [~kpauls] noticed that {{SyncConfigTracker#hasDynamicMembership}} could be simplified by introducing a filter to the service tracking. consequently only synchandler-references that have dynamic-membership enabled would be tracked.,technical_debt,['auth-external'],OAK,Improvement,Major,2021-06-24 16:10:07,0
13381686,Cold Standby SSL certificates should be configurable,"The cold standby is able to do SSL connections to the primary, but currently only using on-the-fly generated certificates. This means that data is transferred over an encrypted connection but there is no protection against a man in the middle yet.

With this issue we want to:
* make server and client certificates configurable
* optionally validate the client certificate
* optionally only allow matching subjects in client and server certificates ",cold-standby,['segment-tar'],OAK,Improvement,Major,2021-06-02 12:03:13,1
13380389,Test failure: LeaseUpdateSocketTimeoutIT.leaseUpdateFailureOnSocketTimeout,"The test fails consistently on Jenkins but succeeds on Travis and when running locally.
{noformat}org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: Configured cluster node id 1 already in use: leaseEnd 1621945215196 > 1621945095197 - 119999ms in the future
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.createInstance(ClusterNodeInfo.java:628)
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.getInstance(ClusterNodeInfo.java:471)
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.getInstance(ClusterNodeInfo.java:440)
	at org.apache.jackrabbit.oak.plugins.document.mongo.LeaseUpdateSocketTimeoutIT.newClusterNodeInfo(LeaseUpdateSocketTimeoutIT.java:153)
	at org.apache.jackrabbit.oak.plugins.document.mongo.LeaseUpdateSocketTimeoutIT.leaseUpdateFailureOnSocketTimeout(LeaseUpdateSocketTimeoutIT.java:107)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",continuous_integration,['mongomk'],OAK,Bug,Minor,2021-05-25 16:50:29,2
13375867,slightly misleading debug message in JournalEntry,"In <https://github.com/apache/jackrabbit-oak/blob/cde907088a5c460178fd714cb4edc141cd45e23d/oak-store-document/src/main/java/org/apache/jackrabbit/oak/plugins/document/JournalEntry.java#L387-L395>:

- Exception should be created with message ""call stack"" instead of null
- replace ""would"" with ""will""",candidate_oak_1_22,['documentmk'],OAK,Improvement,Minor,2021-04-29 15:16:25,3
13375040,Improve oak-run compact to better support Azure compaction,"Currently {{oak-run compact}} for Azure Segment Store always compacts the source container in place. It should be better to allow compacting the source container to a different container, allowing thus to skip source container cleanup. Moreover, in order to speed up compaction, Azure compaction should always employ a persistent disk cache, whose path for storing the segments and size should be configurable.",tooling,"['segment-azure', 'segment-tar']",OAK,Improvement,Major,2021-04-26 10:45:42,1
13372660,Minor improvements to oak-auth-external,"there are a couple of minor improvements for oak-auth-external code base 
- private fields could be final
- unused imports
- unused log fields
- trivial code simplifications",technical_debt,['auth-external'],OAK,Improvement,Minor,2021-04-15 14:31:01,0
13372233,Breaking recovery lock issue,"There won't be a leaseEndTime when the recovering clusterId, the one referred to in {{recoveryBy}}, is not active anymore. The implementation of {{RecoveryLock.tryBreakRecoveryLock()}} should only call {{getLeaseEndTime()}} when {{recovering.isActive()}} is true.",candidate_oak_1_22 candidate_oak_1_6,['documentmk'],OAK,Bug,Major,2021-04-14 08:06:02,2
13358367,"DocumentNodeStore: in dispose(), improve lease update diagnostics","Log state of lease update thread.

Optionally log stack trace of thread (log level depending on whether lease has expired).",candidate_oak_1_22 candidate_oak_1_8,['documentmk'],OAK,Improvement,Minor,2021-02-12 15:06:46,3
13357932,Index update: release the correct checkpoint,"The async index update uses checkpoints to ensure consistency: it
* 1. creates checkpoint2
* 2. indexes everything between checkpoint1 and checkpoint2
* 3. update the indexing lane to checkpoint2
If there is s a problem before step 3, the checkpoint2 is removed. That's fine.

However, if there is a problem after step 3 is complete, in some cases it also removes checkpoint2. This is incorrect, as checkpoint2 is needed.

We have seen this is the case if the node store is closed, but dispose works. See OAK-9300. So fixing that should resolve most issues. However, the current code is still not correct.",index,['indexing'],OAK,Improvement,Major,2021-02-10 13:01:58,4
13345098,Update Tika dependency to 1.24.1 (backport),From 1.24.0,candidate_oak_1_8,['parent'],OAK,Task,Minor,2020-12-09 14:49:38,4
13329558,oak-run explore should support Azure Segment Store,"{{oak-run explore}} should accept Azure URIs for the segment store in order to be able to browse azure segments. 

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2020-09-26 21:14:22,1
13328580,Fix OSGi wiring after netty update to 4.1.52.Final,"After netty update in OAK-9210,  {{OSGiIT}} fails with the following exception:

{code}
ERROR: Bundle org.apache.jackrabbit.oak-segment-tar [41] Error starting file:/var/folders/jh/rvxkcm515dl3bksp1zlzdws80000gn/T/1600699057212-0/bundles/org.apache.jackrabbit.oak-segment-tar_1.35.0.SNAPSHOT.jar (org.osgi.framework.BundleException: Unable to resolve org.apache.jackrabbit.oak-segment-tar [41](R 41.0): missing requirement [org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate) Unresolved requirements: [[org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate)])
org.osgi.framework.BundleException: Unable to resolve org.apache.jackrabbit.oak-segment-tar [41](R 41.0): missing requirement [org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate) Unresolved requirements: [[org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate)]
	at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:4368)
	at org.apache.felix.framework.Felix.startBundle(Felix.java:2281)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1539)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)
	at java.base/java.lang.Thread.run(Thread.java:835)
{code}",cold-standby,['segment-tar'],OAK,Bug,Major,2020-09-21 14:41:38,1
13327862,Bump netty dependency from 4.1.17.Final to 4.1.52.Final,"The current version presents several vulnerability issues: 

BDSA-2018-4022,BDSA-2018-4482,BDSA-2019-2642,BDSA-2019-2643.",cold-standby,['segment-tar'],OAK,Task,Major,2020-09-16 12:56:31,1
13326424,PersistentRedisCache: failure to write segment is not an error,"Failure to write a segment to the redis cache results in an error level log message with a stack trace. However, this is expected behaviour: socket timeouts prevent the cache from effectively slowing down a request. OTOH too many socket timeouts make the cache ineffective, so it's good to have a way to log such errors when debugging. My suggestion is therefore to change the log level to ""debug"" and avoid the stack trace.",patch,['segment-azure'],OAK,Bug,Minor,2020-09-08 09:23:23,1
13324427,AbstractAccessControlManager: improve refresh strategy of PermissionProvider,"since {{AbstractAccessControlManager}} does not have access to the {{RefreshStrategy}} present in _oak-jcr_, it currently eagerly refresh the {{PermissionProvider}} used internally. [~asanso] reported that this may lead to performance issues, when item read operations are combined with frequent calls to {{AccessControlManager.hasPrivileges}} or {{AccessControlManager.getPrivileges}}.",performance,"['core', 'jcr', 'security']",OAK,Improvement,Major,2020-08-25 09:40:23,0
13318203,OakAnalyzer applies LowerCaseFilter and WordDelimiterFilter in wrong order,"I believe OakAnalyzer applies LowerCaseFilter and WordDelimiterFilter in the wrong order.  WordDelimiterFilter is invoked with the GENERATE_WORD_PARTS flag, which splits camelCase/PascalCase into multiple terms, but since the LowerCaseFilter is applied first, the mixed-case is lost and the terms can't be split.

Searching for savings, the damAssetLucene index (which uses the default OakAnalyzer) does not find an asset named savingsAccount.svg.

Upon configuring the index's analyzers (/oak:index/damAssetLucene/analyzers) to apply WordDelimiterFilter before LowerCaseFilter, the correct behaviour was seen.

{noformat}
{
  ""jcr:primaryType"": ""nt:unstructured"",
  ""default"": {
    ""jcr:primaryType"": ""nt:unstructured"",
    ""tokenizer"": {
      ""jcr:primaryType"": ""nt:unstructured"",
      ""name"": ""Standard""
    },
    ""filters"": {
      ""jcr:primaryType"": ""nt:unstructured"",
      ""WordDelimiter"": {""jcr:primaryType"": ""nt:unstructured""},
      ""LowerCase"": {""jcr:primaryType"": ""nt:unstructured""}
    }
  }
}
{noformat}
",easyfix pull-request-available,"['indexing', 'jcr', 'lucene']",OAK,Bug,Minor,2020-07-20 09:00:44,5
13315609,Allow elasticsearch port to be read from secrets,"There is a support for reading osgi configs from file using felix interpolation plugin - 
[https://github.com/apache/felix-dev/tree/9bd66812f21944925268bfd5551218c436886d30/configadmin-plugins/interpolation]

We already support reading ES host using this technique. We need to be able to read port number as well.",amrit,['indexing'],OAK,Improvement,Major,2020-07-08 11:04:39,4
13315105,Remove mix:referenceable from nt:frozenNode definition,"One of the changes between JCR 1.0 and JCR 2.0 is the definition of nt:frozenNode. In JCR 1.0 the node type extends from mix:referenceable, while in JCR 2.0 it does [not anymore|https://docs.adobe.com/docs/en/spec/jcr/2.0/3_Repository_Model.html#3.13.4.1%20nt:frozenNode].

Oak currently uses a nt:frozenNode definition that extends from mix:referenceable. This adds quite a bit of overhead because each node written under a JCR version gets a jcr:uuid, which is indexed by default.

The proposal is to remove the supertype ""mix:referenceable"" from nt:frozenNode.

Removing this supertype, the frozenNodes wouldn't have a ""jcr:uuid"" field, which at the end is not used, and allows to reduce the size of the index.",patch,"['core', 'jcr']",OAK,Improvement,Minor,2020-07-06 08:38:08,6
13310571,Change default timeout to mark indexes corrupt,"Currently, async indexes (Lucene) are marked corrupt if they couldn't be updated for some time. I think the default delay of 30 minutes is far too low. In the past, we have seen the following cases where async indexing is delayed:

* Topology problems (no leader)
* High load prevents updating them

For cases where index update is run, but failing, include:

* Restart of Oak with inconsistent data in the /repository/index directory
* Out-of-disk-space

For these cases, it's better to stop Oak, clean the index directory, and restart. This might anyway be happening regularly (e.g. daily). Marking the index corrupt, so that reindexing is needed, doesn't seem to be needed or helping.

This setting is already configurable in org.apache.jackrabbit.oak.plugins.index.AsyncIndexerService: failingIndexTimeoutSeconds. But instead of changing the configuration everywhere, it's probably better to change the default value for failingIndexTimeoutSeconds in Oak, to 604800L = 60L * 60 * 24 * 7 (one week).",index,[],OAK,Improvement,Major,2020-06-10 07:13:47,4
13308651,RDBDocumentStore: Update error code for MSSQL 2019,"Getting below exception on RDBMK setup with MSSQL 2019 using jdbc version 8.2.2

 

Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakOak0001: Update for path failedCaused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakOak0001: Update for path failed at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.mergeFailed(DocumentNodeStoreBranch.java:334) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$600(DocumentNodeStoreBranch.java:55) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:539) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:194) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:119) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:170) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1869) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:250) [org.apache.jackrabbit.oak-core:1.10.8] at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:346) [org.apache.jackrabbit.oak-jcr:1.10.8] at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:493) [org.apache.jackrabbit.oak-jcr:1.10.8] ... 14 common frames omittedCaused by: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: Update for path failed at org.apache.jackrabbit.oak.plugins.document.rdb.RDBJDBCTools.asDocumentStoreException(RDBJDBCTools.java:434) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.handleException(RDBDocumentStore.java:2377) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.handleException(RDBDocumentStore.java:2382) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.updateDocument(RDBDocumentStore.java:2108) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.internalUpdate(RDBDocumentStore.java:1670) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.internalCreateOrUpdate(RDBDocumentStore.java:1637) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.findAndUpdate(RDBDocumentStore.java:603) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.util.LeaseCheckDocumentStoreWrapper.findAndUpdate(LeaseCheckDocumentStoreWrapper.java:141) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.updateCommitRoot(DocumentNodeStore.java:1540) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.Commit.conditionalCommit(Commit.java:440) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:329) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:252) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:220) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:208) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:310) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:275) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$500(DocumentNodeStoreBranch.java:55) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:531) [org.apache.jackrabbit.oak-store-document:1.10.8] ... 21 common frames omittedCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: String or binary data would be truncated in table 'NODES', column 'DATA'. Truncated value: '\{""_deleted"":{""r171a9e9bdfa-0-1"":""false""},""jcr:created"":{""r171a9e9bdfa-0-1"":""\""dat:2020-04-24T02:00:0'. at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1632) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:600) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:522) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7225) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3053) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:247) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:222) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeUpdate(SQLServerPreparedStatement.java:471) [com.microsoft.sqlserver.mssql-jdbc:8.2.2] at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.tomcat.jdbc.pool.interceptor.AbstractQueryReport$StatementProxy.invoke(AbstractQueryReport.java:212) [org.apache.sling.datasource:1.0.4] at com.sun.proxy.$Proxy19.executeUpdate(Unknown Source) at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.tomcat.jdbc.pool.interceptor.StatementDecoratorInterceptor$StatementProxy.invoke(StatementDecoratorInterceptor.java:237) [org.apache.sling.datasource:1.0.4] at com.sun.proxy.$Proxy19.executeUpdate(Unknown Source) at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.tomcat.jdbc.pool.StatementFacade$StatementProxy.invoke(StatementFacade.java:114) [org.apache.sling.datasource:1.0.4] at com.sun.proxy.$Proxy19.executeUpdate(Unknown Source) at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStoreJDBC.appendingUpdate(RDBDocumentStoreJDBC.java:132) [org.apache.jackrabbit.oak-store-document:1.10.8] at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.updateDocument(RDBDocumentStore.java:2074) [org.apache.jackrabbit.oak-store-document:1.10.8] ... 35 common frames omitted",candidate_oak_1_8,['rdbmk'],OAK,Bug,Minor,2020-06-01 07:25:57,7
13301237,Improve azure archive recovery during startup,"During repository startup if archive directory is not closed properly, recovery will be performed. During that procedure, segents are copied to the backup directory and deleted from the source direcory, one by one.

It can create problems and negativelly impact other ongoing actiivties, which are accessing the same archive. This activity, for example, can be repository cloning in order to create new environment. 

Proposed patch, after creating backup is not deleting all segments from archive, but only segments which could not be recovered. 

[^proposal.patch]

 

+API change+

Proposed patch is changing major version of exported SPI package, org.apache.jackrabbit.oak.segment.spi.persistence.split.

 ",Patch,"['segment-azure', 'segment-tar']",OAK,Improvement,Major,2020-04-27 11:21:56,1
13300281,oak-search-mt imports org.slf4j.impl,"...this seems to be caused by the fact that the project's pom lists slf4j-api as ""provided"" (what for?), and joshua-incubating pulls in a reference to slf4j-log4j12.

Removing the provided dependency to slf4j-api seems to fix this.",candidate_oak_1_22,['search-mt'],OAK,Bug,Minor,2020-04-22 14:13:59,3
13299938,upgrade to mockito-core 3.3.3,Needed für certain mocking features to work with Java 14.,candidate_oak_1_22,['parent'],OAK,Task,Minor,2020-04-21 09:29:23,3
13298551,Standardize handling of system properties,"I'd like to somewhat standardize getting system properties, with respect to:

- default handling
- parsing
- logging
",candidate_oak_1_22,['documentmk'],OAK,Improvement,Major,2020-04-15 12:26:15,3
13293445,OR  query with ORDER BY don't work as expected,"A query with Or and having order by along with limit Don't reproduce results as per order mentioned. 

e.g. 

Let content be:

""/UnionQueryTest1/node0"",
""/UnionQueryTest/node0"",
""/UnionQueryTest1/node0/node1"",
""/UnionQueryTest/node0/node1"",
""/UnionQueryTest1/node0/node1/node2"",
""/UnionQueryTest/node0/node1/node2"",
""/UnionQueryTest1/node0/node1/node2/node3"",
""/UnionQueryTest/node0/node1/node2/node3""

each node having x = number in node name.

SELECT idn1.* FROM [nt:base] as idn1 WHERE ISDESCENDANTNODE([/UnionQueryTest]) OR  ISDESCENDANTNODE([/UnionQueryTest1]) ORDER BY idn1.[x] ASC

 

result should be  same as above mentioned where as current result come out to be

/UnionQueryTest1/node0
/UnionQueryTest1/node0/node1
/UnionQueryTest1/node0/node1/node2
/UnionQueryTest1/node0/node1/node2/node3
/UnionQueryTest1/node0/node1/node2/node3/node4
/UnionQueryTest/node0
/UnionQueryTest/node0/node1
/UnionQueryTest/node0/node1/node2

 ",candidate_oak_1_8,['search'],OAK,Bug,Major,2020-03-24 05:27:17,8
13291031,ObservationManager.addEventListener() throws NPE with invalid paths in filter,"While registering a resource change listener, we encountered the following exception : 

 
{code:java}
05.03.2020 23:39:00.728 *ERROR* [FelixDispatchQueue] org.apache.sling.resourceresolver FrameworkEvent ERROR (java.lang.NullPointerException)
java.lang.NullPointerException: null
at org.apache.jackrabbit.oak.commons.PathUtils.unifyInExcludes(PathUtils.java:501) [org.apache.jackrabbit.oak-commons:1.8.17]
at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.addEventListener(ObservationManagerImpl.java:240) [org.apache.jackrabbit.oak-jcr:1.8.17]
at org.apache.sling.jcr.resource.internal.JcrListenerBaseConfig.register(JcrListenerBaseConfig.java:136) [org.apache.sling.jcr.resource:3.0.16.1]
{code}
 

On further debugging, we found that issues lies in this snippet : 
{code:java}
if (exclude.equals(include) || isAncestor(exclude, include)) {
 includesRemoved.add(include);{code}
'exclude' can be null if the getOakPath() method returns a null. This NPE causes listeners(ResourceChangeListener in our case) to fail at registration.",Observation,['jcr'],OAK,Bug,Minor,2020-03-11 09:31:47,2
13289743,Oak run recovery fails when running on mongo replicaSet with auth enabled,"When running oak run jar in recovery mode on a mongo replica set with auth enabled. it fails to pass the auth data for a findOne command called in *GetRootRevisionsCallable*

 
{code:java}
DBObject root = collection.findOne(new BasicDBObject(Document.ID, ""0:/""));{code}
Stack Trace as below
 07:07:27.790 [MongoDocumentStore replica set info provider] ERROR o.a.j.o.p.d.m.replica.ReplicaSetInfo - Can't connect to the Mongo instance07:07:27.790 [MongoDocumentStore replica set info provider] ERROR o.a.j.o.p.d.m.replica.ReplicaSetInfo - Can't connect to the Mongo instancejava.util.concurrent.ExecutionException: com.mongodb.MongoQueryException: Query failed with error code 13 and error message 'not authorized on dampro64tmp to execute command { find: ""nodes"", filter:

{ _id: ""0:/"" }

, limit: 1, singleBatch: true }' on server at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.getRootRevisions(ReplicaSetInfo.java:346) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.updateRevisions(ReplicaSetInfo.java:269) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.updateReplicaStatus(ReplicaSetInfo.java:181) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.updateLoop(ReplicaSetInfo.java:144) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.run(ReplicaSetInfo.java:133) at java.lang.Thread.run(Thread.java:745)Caused by: com.mongodb.MongoQueryException: Query failed with error code 13 and error message 'not authorized on DB to execute command { find: ""nodes"", filter:

{ _id: ""0:/"" }

, limit: 1, singleBatch: true }' on server at com.mongodb.operation.FindOperation$1.call(FindOperation.java:722) at com.mongodb.operation.FindOperation$1.call(FindOperation.java:711) at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:471) at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:415) at com.mongodb.operation.FindOperation.execute(FindOperation.java:711) at com.mongodb.operation.FindOperation.execute(FindOperation.java:83) at com.mongodb.Mongo$3.execute(Mongo.java:826) at com.mongodb.Mongo$3.execute(Mongo.java:813) at com.mongodb.DBCursor.initializeCursor(DBCursor.java:877) at com.mongodb.DBCursor.hasNext(DBCursor.java:144) at com.mongodb.DBCursor.one(DBCursor.java:683) at com.mongodb.DBCollection.findOne(DBCollection.java:829) at com.mongodb.DBCollection.findOne(DBCollection.java:792) at com.mongodb.DBCollection.findOne(DBCollection.java:739) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.GetRootRevisionsCallable.call(GetRootRevisionsCallable.java:58) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.GetRootRevisionsCallable.call(GetRootRevisionsCallable.java:34) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.getRootRevisions(ReplicaSetInfo.java:340) ... 5 common frames omitted",candidate_oak_1_6,['run'],OAK,Task,Minor,2020-03-05 09:03:15,2
13289303,Improve ClusterNodeInfo MAC address detection ,"ClusterNodeInfo is taking the lowest MAC address found in the machine. When the machine has ""docker0"" bridge, it takes that MAC address (which may change) and uses it. Resulting in errors like this one:
Configured cluster node id 123 already in use: needs recovery and machineId/instanceId do not match: mac:02421b0c73d3//home/ec2-user != mac:0242a5c0c5e5//home/ec2-user   ",candidate_oak_1_6,['documentmk'],OAK,Improvement,Minor,2020-03-03 17:37:37,3
13289151,Indexing: filter entries with a regular expression,"We should provide a way to filter the index using a regular expression. For example, only index nodes that contain a reference to another node. (Not a JCR reference, but a reference within the value itself). For example, index a node if one of the properties contains:

* /content/abc
* <html...> <a href=""/content/abc"">
* and so on

This will allow to run a query to find if /content/abc is referenced. The index and the query will probably need to use a tag, and the cost of the index needs to be high. Otherwise the query engine can't know when this index should be used.

",amrit,['indexing'],OAK,Improvement,Major,2020-03-03 09:03:11,4
13288550,RDBDocumentStore: allow RDBVersionGC support fallback to simpler algorithm,"The current (new) algorithm (introduced in OAK-5855) may cause SELECT and DELETE statements to run concurrently, which might cause problems on certain DBs, such as SQL Server.

Added a fallback (triggered by a system property) to the older (but slower) algorithm. Select it by setting system property like that:
{noformat}
-Dorg.apache.jackrabbit.oak.plugins.document.rdb.RDBVersionGCSupport.MODE=1 {noformat}",candidate_oak_1_6,['rdbmk'],OAK,Technical task,Major,2020-03-01 14:01:25,3
13285694,Improve OAK Lucene Index Documentation,"Improve [http://jackrabbit.apache.org/oak/docs/query/lucene.html] with the following:
 * Extend the *analyzers* section including a reference on how to support *stemming* ([http://jackrabbit.apache.org/oak/docs/query/lucene.html])
 * *supersedes* - does not seem to be documented**
 * *functionName (string)* & *useIfExists (string)* are not listed in the canonical *Index Definition* structure.
 * *function (string)* is not listed in the canonical *Property Definitions* structure
 * *weight* - in the canonical structure the default value is -1, but the actual default is 5",amrit,[],OAK,Task,Minor,2020-02-17 07:44:33,4
13282907,Add javadoc to package-info files,"Add javadoc to package-info files in all packages of {{oak-lucene}} , {{oak-query-spi}} and {{oak-search}} .",amrit,[],OAK,Task,Minor,2020-02-03 06:14:29,4
13281598,deprecate use of Guava Predicate class in oak-core API,...offering JDK 8 Predicate API in addition.,candidate_oak_1_22,['core'],OAK,Improvement,Major,2020-01-26 16:32:49,3
13281162,Access azure segments metadata in a case-insensitive way,"We use azcopy to copy segments from one azure blob container to another for testing. There is a bug in the current version of azcopy (10.3.3), which makes all metadata keys start with a capital letter - ""type"" becomes ""Type"". As a consequence, the current implementation can not find the segments in the azure blob storage. 
 
The azcopy issue was already reported [1] in 2018. I have little hope that azcopy will be fixed soon.
 
Therefore I suggest a patch to oak-segment-azure, that would be backward compatible and ignore the case of the keys when reading metadata. We should be strict in what we write and tolerant in what we read.  ",azureblob,['segment-azure'],OAK,Improvement,Major,2020-01-23 10:52:52,1
13274106,oak-store-document: update org.quartz-scheduler dependency to 2.3.2,We are probably not affected by the CVE though...,candidate_oak_1_6,['documentmk'],OAK,Task,Minor,2019-12-12 10:54:40,3
13273865,oak-solr-osgi: decouple jackson dependency from project dependency,"oak-solr-osgi embeds jackson-core and jackson-dataformat-smile, currently at 2.9.10.

Once we update the project's jackson dependency to 2.10.1, oak-solr-osgi will break because the in jackson, dependencies and packaging changed.

To make oak-solr-osgi work with 2.9.10, we'll have to also include -databind and -annotations, increasing the bundle size by ~1.4M.

For now and instead, I'll hardwire the 2.9.10 dependency and let the indexing team (cc [~thomasm], [~teofili]) decide how to proceed with the general question about what to embed in the bundle.",candidate_oak_1_6,['solr'],OAK,Task,Minor,2019-12-11 12:21:12,3
13272238,oak-solr-osgi: adjust Import-Package declaration for upgrade of maven-bundle-plugin to 4.2.1,See <https://issues.apache.org/jira/browse/OAK-8798?focusedCommentId=16987186&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16987186>,candidate_oak_1_6,['solr'],OAK,Task,Minor,2019-12-04 13:46:52,3
13271259,oak-lucene: adjust Import-Package declaration for upgrade of maven-bundle-plugin to 4.2.1,See https://issues.apache.org/jira/browse/OAK-8798?focusedCommentId=16984471&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16984471,candidate_oak_1_6,['lucene'],OAK,Task,Minor,2019-11-28 16:01:30,3
13270725,oak-http: broken Export-Package statement,"The bare ""*"" causes run time exceptions in newer versions of the bundle plugin, see OAK-8798.",candidate_oak_1_6,['oak-http'],OAK,Bug,Minor,2019-11-26 13:43:32,3
13267750,ClusterViewDocument uses static instance of SimpleDateFormat,...which is not thread-safe.,candidate_oak_1_8,['documentmk'],OAK,Bug,Minor,2019-11-12 15:44:47,3
13265315,dead locking related code in NodeDelegate.updateMixins,"{noformat}
            // 3. deal with locked nodes
            boolean wasLockable = isNodeType(MIX_LOCKABLE);
            boolean isLockable = isNodeType(MIX_LOCKABLE);
            if (wasLockable && !isLockable && holdsLock(false)) {
                // TODO: This should probably be done in a commit hook
                unlock();
                sessionDelegate.refresh(true);
            }
{noformat}

AFAICT, this code block will never be executed. Clean up, [~angela]?",candidate_oak_1_6,['jcr'],OAK,Bug,Minor,2019-10-30 14:29:17,3
13264800,Oak run check command must return the status of repository consistency check,"Currently the consistency check reports only if the command runs successfully (return code 0) or fails (return code 1).
Into this logic will also add the status of repository consistency:
- checking only the last revision: will return 0 if the revision is consistent and the command runs successfully OR will return 1 if the revision is inconsistent or job did not run successfully (some errors/exception were encountered during the run)
- checking multiple revisions: will return 0 if at least one revision is consistent and the job runs successfully OR will return 1 if none of the revisions are consistent or the command did not run successfully (some errors/exception were encounter during the run) ",oak-run segment-tar,[],OAK,Improvement,Major,2019-10-28 11:32:37,1
13263811,Queries with facets should not use traversal,"Consider a scenario where a query is there with facets and the traversal cost is less than the index cost that serves the facet query . This would be problematic.

 

In this case we should maybe set the traversal cost to infinity so that traversal is not an option for queries with facets.

 

In case there is no index available to serve this faceted query we can probably throw an exception with a meaningful message .",amrit,[],OAK,Bug,Major,2019-10-22 13:56:56,9
13262887,DocumentMK documentation: fix INTERMEDIATE SplitDocType value,"The INTERMEDIATE value (30) in the documentmk docs does not align with the one in SplitDocType enum (40).

[https://github.com/apache/jackrabbit-oak/blob/jackrabbit-oak-1.12.0/oak-store-document/src/main/java/org/apache/jackrabbit/oak/plugins/document/NodeDocument.java#L299]

cc [~mreutegg] [~reschke]",fabriziofortino,['doc'],OAK,Documentation,Trivial,2019-10-17 15:12:17,3
13262099,NoClassDefFound running tests in oak-search-mt,"The test runs fine through {{mvn clean install}} or when {{mvn clean test}} is run inside {{oak-search-mt}} folder.

However, running {{mvn clean test}} from trunk throws the following exception:
{code:java}
[INFO] Running org.apache.jackrabbit.oak.plugins.index.mt.MTFulltextQueryTermsProviderTest
 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.57 s <<< FAILURE! - in org.apache.jackrabbit.oak.plugins.index.mt.MTFulltextQueryTermsProviderTest
 [ERROR] testGetQueryTermWithPhraseTranslation(org.apache.jackrabbit.oak.plugins.index.mt.MTFulltextQueryTermsProviderTest)  Time elapsed: 0.527 s  <<< ERROR!
 java.lang.NoClassDefFoundError: org/apache/lucene/analysis/standard/StandardTokenizer
    at org.apache.jackrabbit.oak.plugins.index.mt.MTFulltextQueryTermsProviderTest.testGetQueryTermWithPhraseTranslation(MTFulltextQueryTermsProviderTest.java:57)
 Caused by: java.lang.ClassNotFoundException: org.apache.lucene.analysis.standard.StandardTokenizer
    at org.apache.jackrabbit.oak.plugins.index.mt.MTFulltextQueryTermsProviderTest.testGetQueryTermWithPhraseTranslation(MTFulltextQueryTermsProviderTest.java:57){code}",fabriziofortino,"['search-mt', 'test']",OAK,Bug,Major,2019-10-14 08:06:17,10
13261807,state time of start of LeaseFailure in exception/log entry,...this would make it easier to find the relevant part in the system log.,candidate_oak_1_8,['documentmk'],OAK,Improvement,Minor,2019-10-11 13:39:56,3
13260246,Merge may fail when commit root is a bundled node,"Changing and merging descendant nodes of a bundled node fails when the commit root of the changes is located on a bundled node. The merge tries to apply the final commit changes on a document that does not exist (because the bundled node is located on an ancestor document).

The exception is misleading but looks like this:
{noformat}
Caused by: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: Conflicting concurrent change. Update operation failed: key: 3:/foo/bar/baz update {_revisions.r16d8bf18282-0-1=SET_MAP_ENTRY c, _modified=MAX 1570010920}
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:398) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStoreWithTiming(Commit.java:278) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:262) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:230) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:218) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:320) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:282) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$500(DocumentNodeStoreBranch.java:56) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:548) [org.apache.jackrabbit.oak-store-document:1.18.0]
{noformat}",candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Major,2019-10-03 07:16:14,2
13259835,Deprecate support for lucene custom scorer,"The custom scorer exposes the Lucene API directly. This needs to be removed in order to support remote index services (solr, elasticsearch). Since this feature does not seem to be used, we can safely remove it.

 

[https://github.com/apache/jackrabbit-oak/tree/trunk/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/score]",fabriziofortino,"['lucene', 'oak-search']",OAK,Task,Major,2019-10-01 08:07:51,9
13257936,Composite node store tests with document store,"CompositeNodeStore tests using document store (h2, document memory) are currently disabled because the index creation does not work. 

[https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-lucene/src/test/java/org/apache/jackrabbit/oak/composite/CompositeNodeStoreQueryTestBase.java] 

The below assertion fails because the lucene index is not found. This does not happen with segment and memory stores.

 
{noformat}
java.lang.AssertionError: java.lang.AssertionError: Expected: a string containing ""/* traverse \""//*\"" where ([a].[foo] = 'bar'""     but: was ""plan: [nt:base] as [a] /* lucene:luceneTest(/oak:index/luceneTest) foo:bar where ([a].[foo] = 'bar') and (isdescendantnode([a], [/])) */ ""Expected :a string containing ""/* traverse \""//*\"" where ([a].[foo] = 'bar'""Actual   :""plan: [nt:base] as [a] /* lucene:luceneTest(/oak:index/luceneTest) foo:bar where ([a].[foo] = 'bar') and (isdescendantnode([a], [/])) */ ""<Click to see difference>
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) 
at org.junit.Assert.assertThat(Assert.java:956) 
at org.junit.Assert.assertThat(Assert.java:923) 
at org.apache.jackrabbit.oak.composite.CompositeNodeStoreLuceneIndexTest.removeLuceneIndex(CompositeNodeStoreLuceneIndexTest.java:169) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:498) 
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) 
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 
at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 
at org.junit.runners.Suite.runChild(Suite.java:128) 
at org.junit.runners.Suite.runChild(Suite.java:27) 
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) 
at org.junit.rules.RunRules.evaluate(RunRules.java:20) 
at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 
at org.junit.runner.JUnitCore.run(JUnitCore.java:137) 
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) 
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) 
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) 
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{noformat}",fabriziofortino indexingPatch,"['composite', 'indexing', 'test']",OAK,Improvement,Major,2019-09-20 12:24:51,4
13257125,Node bundling exposes hidden properties,The DocumentNodeStore node bundling feature may expose a hidden internal property when a bundled node structure is deleted and re-created with a non-bundling nodetype.,candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Minor,2019-09-17 13:53:52,2
13255224,Composite Node Store + Counter Index: allow indexing from scratch / reindex,"When using the composite node store with a read-only portion of the repository, the counter index does not allow to index from scratch / reindex.

Index from scratch is needed in case the async checkpoint is lost. Reindex is started by setting the ""reindex"" flag to true.

Currently the failure is:

{noformat}
05.09.2019 09:29:21.892 *WARN* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate [async] The index update is still failing
java.lang.UnsupportedOperationException: This builder is read-only.
	at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.unsupported(ReadOnlyBuilder.java:44) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child(ReadOnlyBuilder.java:189) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child(ReadOnlyBuilder.java:34) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.getBuilder(NodeCounterEditor.java:184) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.leaveNew(NodeCounterEditor.java:162) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.index.counter.NodeCounterEditor.leave(NodeCounterEditor.java:114) [org.apache.jackrabbit.oak-core:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.spi.commit.CompositeEditor.leave(CompositeEditor.java:73) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:59) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeAdded(EditorDiff.java:129) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) [org.apache.jackrabbit.oak-store-spi:1.16.0.R1866113]
	at org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:504) [org.apache.jackrabbit.oak-segment-tar:1.16.0.R1866113]

{noformat}",fabriziofortino,"['composite', 'indexing']",OAK,Improvement,Major,2019-09-06 10:15:11,11
13251784,Backport OAK-8066 to 1.10 and 1.8,Backport OAK-8066 to 1.10 and 1.8.,TarMK,['segment-tar'],OAK,Improvement,Blocker,2019-08-20 08:50:59,12
13242886,Memory Node store implementation of PropertyState throws Exception that is not in line with the API documentation,"As per the api doc here [https://github.com/apache/jackrabbit-oak/blob/trunk/oak-api/src/main/java/org/apache/jackrabbit/oak/api/PropertyState.java#L67#L69] for PropertyState the getValue method should throw an IllegalStateException if the actual Property stored is multivalued and the code calling getValue(Type type) expects it to be single valued i.e type.isArray() == false .

 

In SegmentStore implementation , this is followed and the getValue method throws an IllegalStateException in this scenario .

 

However in case of MemoryNodeStore , the code here throws an IllegalArgumentException [https://github.com/apache/jackrabbit-oak/blob/trunk/oak-store-spi/src/main/java/org/apache/jackrabbit/oak/plugins/memory/MultiPropertyState.java#L151#L154] instead of IllegalStateException .

Since MemoryNodeStore is primarily used in testing , this difference in implementation can lead to false or incomplete testing .

 

For example , the fix for https://issues.apache.org/jira/browse/OAK-8328 required to catch IllegalStateException in a use case where the property was configured to be multi valued in the repository but the code expected it to be single valued . Now the fix works fine on SegmentStore , but due to the difference in implementation in MemoryNodeStore , a test written with memorynode store fails .

 ",indexingPatch,['store-spi'],OAK,Bug,Major,2019-07-03 03:35:55,4
13240824,oak-run check should expose repository statistics for the last good revision,"{{oak-run check}} should expose the head node and property counts for the last good revision. Currently these are only logged at the end of the check operation as
{noformat}
Checked X nodes and Y properties.{noformat}",tooling,"['oak-run', 'segment-tar']",OAK,Improvement,Minor,2019-06-21 09:53:54,1
13236196,Add remote store monitoring  for Azure,"Add remote store monitoring
Implement the remote store monitoring for Azure Store. This should include:
- request_count : number of request to azure store
- error_count : number of failed requests to azure store
- duration : duration of a request to azure store in nanoseconds ",TarMK,['segment-azure'],OAK,New Feature,Major,2019-05-29 09:11:13,12
13235965,oak-run check should have an option for specifying memory mapping,"{{oak-run check}} currently uses memory mapping by default when building the {{FileStore}}. This setting should be configurable, to allow switching memory mapping off.",tooling,"['run', 'segment-tar']",OAK,New Feature,Minor,2019-05-28 10:16:13,1
13235311,Expose local index directory size as a metric,"We need to expose the local index directory size a metric that can later be consumed for monitor and alerting purposes .

 ",indexingPatch,"['indexing', 'lucene']",OAK,Task,Major,2019-05-24 07:50:43,4
13233909,Bug in index definition can block indexing / cause indexing in a loop,"If we  set ""/oak:index/indexName/entryCount"" to a Long multi-valued property. That will cause the system to reindex in a loop... You only see the root cause if debug level logging is enabled. There are likely other such problems. Oak should log a proper meaningful exception for config errors, and if possible not get into this loop. It also blocks other indexes to be updated I think.

 

 
{code:java}
29.03.2019 11:58:55.688 *INFO* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.IndexUpdate Reindexing will be performed for following indexes: [/oak:index/unifiedCreatedLucene]
29.03.2019 11:59:00.691 *INFO* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.IndexUpdate Reindexing will be performed for following indexes: [/oak:index/unifiedCreatedLucene]
29.03.2019 11:59:05.685 *INFO* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.IndexUpdate Reindexing will be performed for following indexes: [/oak:index/unifiedCreatedLucene]
29.03.2019 11:59:10.687 *INFO* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.IndexUpdate Reindexing will be performed for following indexes: [/oak:index/unifiedCreatedLucene]
29.03.2019 11:59:15.685 *INFO* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.IndexUpdate Reindexing will be performed for following indexes: [/oak:index/unifiedCreatedLucene]
29.03.2019 11:59:20.688 *INFO* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.IndexUpdate Reindexing will be performed for following indexes: [/oak:index/unifiedCreatedLucene]

29.03.2019 12:13:50.692 *DEBUG* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate [async] The index update is still failing
java.lang.IllegalStateException: null
	at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
	at org.apache.jackrabbit.oak.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:145)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition.<init>(IndexDefinition.java:358)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition.<init>(IndexDefinition.java:95)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition$Builder.build(IndexDefinition.java:314)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.enableReindexMode(LuceneIndexEditorContext.java:184)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.enter(LuceneIndexEditor.java:118)
	at org.apache.jackrabbit.oak.spi.commit.ProgressNotificationEditor.enter(ProgressNotificationEditor.java:71)
{code}",indexingPatch,"['core', 'lucene']",OAK,Bug,Major,2019-05-17 07:28:24,4
13232448, SLOW_QUERY_COUNT don't get updated for each slow query.,The metrics added should show slow query count but the count is not getting updated for each query.,indexingPatch,['indexing'],OAK,Bug,Major,2019-05-09 11:27:07,4
13230882,Update org.apache.felix.framework for jdk13,"{noformat}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.felix.framework.URLHandlers (file:/C:/Users/jre/.m2/repository/org/apache/felix/org.apache.felix.framework/5.6.10/org.apache.felix.framework-5.6.10.jar) to constructor sun.net.www.protocol.file.Handler()
WARNING: Please consider reporting this to the maintainers of org.apache.felix.framework.URLHandlers
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Creating bundle watcher with scanner [org.ops4j.pax.swissbox.extender.BundleManifestScanner@2834f898]...
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.apache.felix.framework]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.exam]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.exam.inject]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.exam.extender.service]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [osgi.cmpn]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.base]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.swissbox.core]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.swissbox.extender]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.swissbox.framework]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.swissbox.lifecycle]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.swissbox.tracker]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.apache.geronimo.specs.geronimo-atinject_1.0_spec]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.tipi.junit]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.tipi.hamcrest.core]
[org.ops4j.pax.swissbox.extender.BundleWatcher] : Scanning bundle [org.ops4j.pax.exam.invoker.junit]
ERROR: org.apache.felix.scr (15): Exception starting during restart
java.lang.IllegalStateException: Stream handler unavailable.
        at org.apache.felix.framework.URLHandlersStreamHandlerProxy.getDefaultPort(URLHandlersStreamHandlerProxy.java:180)
        at java.base/java.net.URL.getDefaultPort(URL.java:885)
        at java.base/sun.net.util.URLUtil.urlNoFragString(URLUtil.java:68)
        at java.base/java.security.CodeSource.<init>(CodeSource.java:120)
        at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:779)
        at java.base/jdk.internal.loader.BuiltinClassLoader.findClassInModuleOrNull(BuiltinClassLoader.java:703)
        at java.base/jdk.internal.loader.BuiltinClassLoader.findClass(BuiltinClassLoader.java:584)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:633)
        at java.base/java.lang.Class.forName(Class.java:492)
        at java.base/java.util.ServiceLoader.loadProvider(ServiceLoader.java:853)
        at java.base/java.util.ServiceLoader$ModuleServicesLookupIterator.hasNext(ServiceLoader.java:1077)
        at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)
        at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)
        at java.base/sun.util.cldr.CLDRLocaleProviderAdapter$1.run(CLDRLocaleProviderAdapter.java:89)
        at java.base/sun.util.cldr.CLDRLocaleProviderAdapter$1.run(CLDRLocaleProviderAdapter.java:86)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:553)
        at java.base/sun.util.cldr.CLDRLocaleProviderAdapter.<init>(CLDRLocaleProviderAdapter.java:86)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.ReflectAccess.newInstance(ReflectAccess.java:166)
        at java.base/jdk.internal.reflect.ReflectionFactory.newInstance(ReflectionFactory.java:404)
        at java.base/java.lang.Class.newInstance(Class.java:591)
        at java.base/sun.util.locale.provider.LocaleProviderAdapter.forType(LocaleProviderAdapter.java:176)
        at java.base/sun.util.locale.provider.LocaleProviderAdapter.findAdapter(LocaleProviderAdapter.java:279)
        at java.base/sun.util.locale.provider.LocaleProviderAdapter.getAdapter(LocaleProviderAdapter.java:250)
        at java.base/java.text.NumberFormat.getInstance(NumberFormat.java:951)
        at java.base/java.text.NumberFormat.getInstance(NumberFormat.java:491)
        at java.base/java.text.MessageFormat.subformat(MessageFormat.java:1293)
        at java.base/java.text.MessageFormat.format(MessageFormat.java:885)
        at java.base/java.text.Format.format(Format.java:158)
        at java.base/java.text.MessageFormat.format(MessageFormat.java:860)
        at org.apache.felix.scr.impl.Activator.warn(Activator.java:460)
        at org.apache.felix.utils.extender.AbstractExtender.createExtension(AbstractExtender.java:268)
        at org.apache.felix.utils.extender.AbstractExtender.modifiedBundle(AbstractExtender.java:227)
        at org.apache.felix.utils.extender.AbstractExtender.addingBundle(AbstractExtender.java:187)
        at org.osgi.util.tracker.BundleTracker$Tracked.customizerAdding(BundleTracker.java:469)
        at org.osgi.util.tracker.BundleTracker$Tracked.customizerAdding(BundleTracker.java:415)
        at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256)
        at org.osgi.util.tracker.AbstractTracked.trackInitial(AbstractTracked.java:183)
        at org.osgi.util.tracker.BundleTracker.open(BundleTracker.java:156)
        at org.apache.felix.utils.extender.AbstractExtender.startTracking(AbstractExtender.java:150)
        at org.apache.felix.utils.extender.AbstractExtender.doStart(AbstractExtender.java:142)
        at org.apache.felix.scr.impl.Activator.doStart(Activator.java:172)
        at org.apache.felix.utils.extender.AbstractExtender.start(AbstractExtender.java:114)
        at org.apache.felix.scr.impl.Activator.restart(Activator.java:142)
        at org.apache.felix.scr.impl.config.ScrConfigurationImpl.configure(ScrConfigurationImpl.java:196)
        at org.apache.felix.scr.impl.config.ScrConfigurationImpl.start(ScrConfigurationImpl.java:117)
        at org.apache.felix.scr.impl.Activator.start(Activator.java:110)
        at org.apache.felix.framework.util.SecureAction.startActivator(SecureAction.java:697)
        at org.apache.felix.framework.Felix.activateBundle(Felix.java:2240)
        at org.apache.felix.framework.Felix.startBundle(Felix.java:2146)
        at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1373)
        at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)
        at java.base/java.lang.Thread.run(Thread.java:835)
{noformat}",jdk13,['it'],OAK,Task,Minor,2019-04-30 11:10:39,3
13229853,Orphaned branch commit entries after restart,"The DocumentNodeStore does not clean up orphaned branch commit entries ({{_bc}}) after a restart. Cleanup for those entries happens while the DocumentNodeStore is running as well, but only when the branch is not referenceable anymore. In some cases it may happen that a branch is referenced right until the DocumentNodeStore is disposed and the cleanup must happen on startup.",candidate_oak_1_8,['documentmk'],OAK,Bug,Minor,2019-04-24 12:06:17,2
13229537,Indexing lane failing but the index is not marked corrupt,"Steps to reproduce issue:

Delete blob filesystem(in case of tarmk) from repository/datastore and empty repository/index folder.

Now asyncIndexUpdate will run periodically and fail but index won't get marked as corrupt. ",indexingPatch,['indexing'],OAK,Bug,Minor,2019-04-23 08:22:02,4
13228157,Build failure: ThreadLeakError,"No description is provided

The build Jackrabbit Oak #2091 has failed.
First failed run: [Jackrabbit Oak #2091|https://builds.apache.org/job/Jackrabbit%20Oak/2091/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/2091/console]

{noformat}
[ERROR] Tests run: 13, Failures: 0, Errors: 2, Skipped: 2, Time elapsed: 3.402 s <<< FAILURE! - in org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest
[ERROR] org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest  Time elapsed: 3.08 s  <<< ERROR!
com.carrotsearch.randomizedtesting.ThreadLeakError: 
1 thread leaked from SUITE scope at org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest: 
   1) Thread[id=143, name=oak-scheduled-executor-61, state=TIMED_WAITING, group=main]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:1129)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
	at __randomizedtesting.SeedInfo.seed([7416B4EE1CED31C6]:0)
{noformat}",candidate_oak_1_8,"['continuous integration', 'solr']",OAK,Bug,Major,2019-04-15 16:32:34,10
13227519,Node#setPrimaryType(String) does not create child nodes defined as autoCreated,"In contrast to Node#addNode(String, String), Node#setPrimaryType(String) does not create child nodes that are defined as autoCreated. See attached failing test case.",candidate_oak_1_6,['jcr'],OAK,Bug,Minor,2019-04-11 18:12:20,13
13225702,Getting warning message in error logs while trying to search using filters,"Log level warn is being used  at [1]. In case an index can't be used because of some filter not supported by that index, a warning is being locked. Though is a normal scenario as that query may have been created keeping another index in mind.

e.g. 
 select [jcr:path], [jcr:score], [rep:facet(jcr:content/metadata/x)], [rep:facet(jcr:content/metadata/y)] from [z] as
 a where isdescendantnode(a, '/content/p')
 Lets assume we have facets configured on index definition z on properties which are not indexed in nt:base leading to warning log. 

[1]: [https://github.com/apache/jackrabbit-oak/blob/5deeea0055deb776fffbf268b66c4a97d096d124/oak-search/src/main/java/org/apache/jackrabbit/oak/plugins/index/search/spi/query/FulltextIndexPlanner.java#L224]",indexingPatch,['indexing'],OAK,Task,Major,2019-04-03 05:41:32,4
13224619,Add logging around getReferenceCheckpoint method ,"04.03.2019 06:42:46.956 *WARN* [sling-oak-3-org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate-async] org.apache.jackrabbit.oak.composite.CompositeNodeStore Checkpoint f7e6524d-62c4-4589-8137-ca873cec934b doesn't exist


The above warning was observed - but there are not enough logs around this to identify what's causing this .

 

We need to add stack trace when this is logged and probably changed the message to make it unique.

 

Also , Since this was logged from a flow in AsyncIndexUpdate - It would make sense to add some more info/ warn level logging there around the code that calls getReferenceCheckpoint()",indexingPatch,"['composite', 'lucene']",OAK,Improvement,Major,2019-03-28 15:04:46,4
13224315,ExternalGroupPrincipalProvider support for full text search,The {{ExternalGroupPrincipalProvider}} related changes for OAK-8131.,principal-management-extensions,"['auth-external', 'security']",OAK,Improvement,Major,2019-03-27 13:56:18,14
13223920,With uneven distribution of ACL restriction across facet labels statistical facet count become too inaccurate,"With the statistical mode, facet count is updated proportionally to the percentage of accessible samples, which works for secured contents scattered across different facets. For edge case where the whole facet (results) is not accessible, the count still shows a number after the sampling percent is applied. Even if the number is small, user experience is misleading/inaccurate as nothing would return when the facet is clicked (applied as a query condition).

For example, a ACLs/CUGs guarded ""private"" folder, in which all the assets are tagged with the same facet value. Non authorized user may still see this facet with a count but gets nothing when clicking on the facet.",vulnerability,"['lucene', 'query']",OAK,Bug,Major,2019-03-26 01:44:55,4
13223813,Index definition with orderable property definitions with and without functions breaks index,"If an index definition contains the same orderable property with and without functions, it will fail to index any node which contains that property. The failure will be logged as [1].

Steps to reproduce:
* Configure index with the two property definitions shown at [2].
* Refresh the index definition
* Modify a node that falls under the definition - it will fail with the exception shown at [1]
* Modify the 'non-function' index definition to not be orderable (orderable=false)
* Refresh the index definition
* Modify the same node - note there is no exception.

Thanks to [~catholicon] for assistance identifying root cause.


[1]
{code}
25.03.2019 15:39:04.135 *WARN* [async-index-update-async] org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor Failed to index the node [/content/dam/Unknown-2.png]
java.lang.IllegalArgumentException: DocValuesField "":dvjcr:content/metadata/dc:title"" appears more than once in this document (only one value is allowed per field)
	at org.apache.lucene.index.SortedDocValuesWriter.addValue(SortedDocValuesWriter.java:62) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.DocValuesProcessor.addSortedField(DocValuesProcessor.java:125) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.DocValuesProcessor.addField(DocValuesProcessor.java:59) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.TwoStoredFieldsConsumers.addField(TwoStoredFieldsConsumers.java:36) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:236) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:253) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:455) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1507) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.jackrabbit.oak.plugins.index.lucene.writer.DefaultIndexWriter.updateDocument(DefaultIndexWriter.java:86) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.addOrUpdate(LuceneIndexEditor.java:258) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:140) [org.apache.jackrabbit.oak-lucene:1.8.9]
	at org.apache.jackrabbit.oak.spi.commit.CompositeEditor.leave(CompositeEditor.java:74) [org.apache.jackrabbit.oak-store-spi:1.8.9]
{code}


[2] 
{code}
""dcTitle"": {
    ""jcr:primaryType"": ""nt:unstructured"",
    ""nodeScopeIndex"": ""true"",
    ""useInSuggest"": ""true"",
    ""ordered"": ""true"",
    ""propertyIndex"": ""true"",
    ""useInSpellcheck"": ""true"",
    ""name"": ""jcr:content/metadata/dc:title"",
    ""boost"": ""2.0""
    },
  ""dcTitleLowercase"": {
    ""jcr:primaryType"": ""nt:unstructured"",
    ""ordered"": ""true"",
    ""propertyIndex"": ""true"",
    ""function"": ""fn:lower-case(jcr:content/metadata/@dc:title)""
    }
{code}",indexingPatch nitin,['indexing'],OAK,Bug,Major,2019-03-25 16:02:02,11
13223359,CompositePrincipalProvider.findPrincipals lacks NotNull and Overrides annotation,"[~stillalex], the new {{findPrincipals}} method added recent lacks NotNull and Overrides annotation in {{CompositePrincipalProvider}}. Hope you don't mind if I add it.",principal-management-extensions,['security-spi'],OAK,Improvement,Minor,2019-03-22 14:23:44,0
13222812, [Indexing] Implement sling metric to calculate number of slow queries(relative to all queries),"# We need a metric which calculates the number of slow queries compared to all run queries. The value should be a percentile for slow queries relative to all queries to prove 99% absence of slow product queries that traverse more than 100'000 nodes)
 # Add another metric showing total number of slow queries executed.

See: [https://sling.apache.org/documentation/bundles/metrics.html]
 ",indexingPatch,['indexing'],OAK,Task,Major,2019-03-20 11:17:01,11
13222600,CompositePrincipalProvider support for full text search,The {{CompositePrincipalProvider}} related changes for OAK-8131.,principal-management-extensions,"['security', 'security-spi']",OAK,Improvement,Major,2019-03-19 14:38:22,14
13222312,UserPrincipalProvider support for full text search,The {{UserPrincipalProvider}} related changes for OAK-8131.,principal-management-extensions,"['core', 'security']",OAK,Improvement,Major,2019-03-18 13:13:37,14
13222303,DocumentDiscoveryLiteService hasBacklog silencing must support maven version format,"OAK-3492 silences log warns when it encounters an 1.0 or 1.2 oak version (in the case where there is an inactive cluster node that doesn't have lastWrittenRootRev set).

The silencing uses osgi Version to do the version comparison, however the actual version is stored in maven format. This breaks for eg the case where version is set to something like 1.0.10-SNAPSHOT where it expects 1.0.10.SNAPSHOT and the following exception would occur:
{{org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteService hasBacklog: couldn't parse version 1.0.10-SNAPSHOT : java.lang.IllegalArgumentException: invalid version ""1.0.10-SNAPSHOT"": non-numeric ""10-SNAPSHOT""}}

The silencing should be fixed.",candidate_oak_1_6,['documentmk'],OAK,Bug,Minor,2019-03-18 12:49:07,3
13222202,facets: use secure mode when search hit count low,"In cases where search hit count is less than sample size defined for statistical mode for facets, secure mode should be used so that the counts are accurate for non-admin users.

 

 ",candidate_oak_1_6 candidate_oak_1_8 indexingPatch,"['query', 'search']",OAK,Improvement,Major,2019-03-18 03:14:28,11
13222184,Oak run tooling to run a RepositoryInitializer on top a list index definitions,"Product index definitions are delivered using {{RepositoryInitializer}} (e.g. AEM and oak itself).
Oak run has tooling to dump index definitions as a json and also ability to create lucene indexes as defined by an index definition json.

That leaves a gap in tooling where we can dump current set of index definitions and prepare an index definition description to prepare of upgrades (thus saving time ""during"" upgrade). This step is currently done manually and hence is error prone. It'd be nice to have some automated tooling to assist in such a flow.",doc-impacting,['run'],OAK,Improvement,Major,2019-03-17 23:50:13,11
13221854,Index Copier Stats MBean shows stale info," In cases an index is re-indexed, but indexing is stopped in between, the index copier mbean shows the partially created new index stats even though old indexes are still in use.

1. Reindex by setting reindex= true

2. stop reindexing using from jmx console (indexstats) as follows:
 * <host>:<port>/system/console/jmx
 * click on index's ""index stats"" whose reindexing is triggered.
 * call abortAndPause operation. 

 ",indexingPatch,['indexing'],OAK,Task,Minor,2019-03-15 09:29:47,4
13221696,Principal Management APIs full text search support,"Followup of OAK-7994, there's another improvement I would like to add: the option to run a full text search instead the current {{jcr:like}} on the {{rep:principalName}} property.
As far as the apis go, this will be reflected in a new flag to be passed to the method: {{fullText}}, it would be a best-effort attempts, as for some of the existing impls this doesn't make sense (like the {{ExternalGroupPrincipalProvider}}).",principal-management-extensions,"['security', 'security-spi']",OAK,Improvement,Major,2019-03-14 15:11:13,14
13221146,PrincipalProviderImpl support for range search,The {{PrincipalProviderImpl}} related changes for OAK-7994.,principal-management-extensions,"['core', 'security']",OAK,Improvement,Major,2019-03-12 14:42:43,14
13220126,Expose text extraction metrics as sling metrics,We want to expose Text extraction stats as sling metrics (time metrics).,indexingPatch,"['indexing', 'oak-search']",OAK,Task,Major,2019-03-07 08:38:50,4
13220021,IndexDefinitionBuilder should be smarter when to reindex while updating a definition,"{{IndexDefinitionBuilder}} currently sets reindex flag while building an index definition if there is a difference in existing def v/s what it builds.

The only place it acts smarter is while setting up nrt or sync flag. There are quite a few properties which only affect querying or cost-estimation and don't imply a change in indexed data. For such cases, simply setting {{refresh=true}} should suffice.",indexingPatch nitin,"['lucene', 'search']",OAK,Improvement,Major,2019-03-06 20:29:39,11
13218575,The cold standby server cannot handle blob requests for long blob IDs,"If the standby client issues a request for a binary ID larger than 8192 bytes, it will fail on the server side due to the current frame limitation, set to 8192 bytes:
{noformat}
28.02.2019 00:01:36.034 *WARN* [primary-32] org.apache.jackrabbit.oak.segment.standby.server.ExceptionHandler Exception caught on the server
io.netty.handler.codec.TooLongFrameException: frame length (35029) exceeds the allowed maximum (8192)
        at io.netty.handler.codec.LineBasedFrameDecoder.fail(LineBasedFrameDecoder.java:146) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.fail(LineBasedFrameDecoder.java:142) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.decode(LineBasedFrameDecoder.java:131) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.decode(LineBasedFrameDecoder.java:75) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2019-02-28 09:13:13,1
13218375,DocumentNodeStore dispose can fail when duration of final background ops exceeds lease time,"The problem is that {{dispose()}} let's the {{BackgroundLeaseUpdateThread}} run once.

If the duration of the remaining operations then exceeds the lease update interval, these operations will fail with a {{DocumentStoreException}}.
",candidate_oak_1_6,['documentmk'],OAK,Bug,Minor,2019-02-27 15:18:35,3
13218371,Add refresh head revision time to background update stats,The background update stats does not have timing information for refreshing the head revision.,candidate_oak_1_6,['documentmk'],OAK,Improvement,Minor,2019-02-27 15:12:39,2
13218345,Upgrade spotbugs to 3.1.11,"...due to a Java 11 incompatibility:

{noformat}
     [java] The following errors occurred during analysis:
     [java]   Error scanning java/lang/Throwable for referenced classes
     [java]     java.lang.UnsupportedOperationException
     [java]       At org.objectweb.asm.ClassVisitor.visitNestMemberExperimental(ClassVisitor.java:248)
     [java]       At org.objectweb.asm.ClassReader.accept(ClassReader.java:651)
     [java]       At edu.umd.cs.findbugs.asm.FBClassReader.accept(FBClassReader.java:44)
     [java]       At org.objectweb.asm.ClassReader.accept(ClassReader.java:391)
     [java]       At edu.umd.cs.findbugs.classfile.engine.ClassParserUsingASM.parse(ClassParserUsingASM.java:519)
     [java]       At edu.umd.cs.findbugs.classfile.engine.ClassParserUsingASM.parse(ClassParserUsingASM.java:703)
     [java]       At edu.umd.cs.findbugs.classfile.engine.ClassInfoAnalysisEngine.analyze(ClassInfoAnalysisEngine.java:79)
     [java]       At edu.umd.cs.findbugs.classfile.engine.ClassInfoAnalysisEngine.analyze(ClassInfoAnalysisEngine.java:38)
     [java]       At edu.umd.cs.findbugs.classfile.impl.AnalysisCache.getClassAnalysis(AnalysisCache.java:262)
     [java]       At edu.umd.cs.findbugs.FindBugs2.buildReferencedClassSet(FindBugs2.java:774)
     [java]       At edu.umd.cs.findbugs.FindBugs2.execute(FindBugs2.java:220)
     [java]       At edu.umd.cs.findbugs.FindBugs.runMain(FindBugs.java:401)
     [java]       At edu.umd.cs.findbugs.FindBugs2.main(FindBugs2.java:1185)

{noformat}
",jdk11,['parent'],OAK,Task,Minor,2019-02-27 13:40:42,3
13217859,UserPrincipalProvider support for range search,The {{UserPrincipalProvider}} related changes for OAK-7994.,principal-management-extensions,"['core', 'security']",OAK,Improvement,Major,2019-02-25 15:27:36,14
13217162,Log warning for too many transient modifications of direct child nodes,"In a first step towards resolving OAK-8066, I want to add some logging regarding the number of transiently modified direct child nodes in {{DefaultSegmentWriter}} ",TarMK,['segment-tar'],OAK,Technical task,Blocker,2019-02-21 10:56:42,15
13217128,Nodes with many direct children can lead to OOME when saving,{{DefaultSegmentWriter}} keeps a map of [child nodes|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/DefaultSegmentWriter.java#L805] of a node being written. This can lead to high memory consumption in the case where many child nodes are added at the same time. The latter could happen in the case where a node needs to be rewritten because of an increase in the GC generation from a concurrently completed revision garbage collection.,TarMK,['segment-tar'],OAK,Improvement,Major,2019-02-21 08:35:49,15
13216958,The cold standby client doesn't correctly handle backward references,"The logic from {{StandbyClientSyncExecution#copySegmentHierarchyFromPrimary}} has a flaw when it comes to ""backward references"". Suppose we have the following data segment graph to be transferred from primary: S1, which references \{S2, S3} and S3 which references S2. Then, the correct transfer order should be S2, S3 and S1.

Going through the current logic employed by the method, here's what happens:
{noformat}
Step 0: batch={S1}

Step 1: visited={S1}, data={S1}, batch={S2, S3}, queued={S2, S3}

Step 2: visited={S1, S2}, data={S2, S1}, batch={S3}, queued={S2, S3}

Step 3: visited={S1, S2, S3}, data={S3, S2, S1}, batch={}, queued={S2, S3}.{noformat}
Therefore, at the end of the loop, the order of the segments to be transferred will be S3, S2, S1, which might trigger a {{SegmentNotFoundException}} when S3 is further processed, because S2 is missing on standby (see OAK-8006).

/cc [~frm]",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2019-02-20 15:20:23,1
13215267,AccessControlManagerImpl.getEffectivePolicies returns empty ACLs,"while writing tests for OAK-8000 i noticed that {{AccessControlManagerImpl.getEffectivePolicies}} may return empty access control lists. this doesn't seem valuable to me as they have no real effect. since i am indecided about OAK-8000 i would suggest to fix that separately.

[~stillalex], fyi",candidate_oak_1_8,"['core', 'security']",OAK,Bug,Minor,2019-02-12 15:00:49,0
13214937,Intermittent test failure of CompactionAndCleanupIT.testMixedSegments,"{{CompactionAndCleanupIT.testMixedSegments}} fails every 50-th build or so:
{code:java}
[ERROR] testMixedSegments(org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT)  Time elapsed: 1.064 s  <<< FAILURE!
java.lang.AssertionError: Mixed segments found: 7395f14c-15dd-4224-ad53-ec981f46f5cd
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.testMixedSegments(CompactionAndCleanupIT.java:701)
{code}
 

This might have the same root cause as OAK-8033. But no causality found yet.",TarMK,[],OAK,Bug,Critical,2019-02-11 08:11:35,15
13214560,Debug logging when two or more indices have same or very close cost amounts doesn't work in case both indices belong to the same type of Query Index,"Steps to reproduce -
 # You would need two index definitions of same query index type , let's say fullText index , having similar cost evaluations.
 # Now while the index plan is evaluated you will notice that there should have been a debug log saying that selected index has similar cost - this serves as an important warning to either change the index definitions or the query ([https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/query/QueryImpl.java#L1069#L1072)]

 

However If we have a look at this code block here - [https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/query/QueryImpl.java#L1017#L1041] , there is no comparison for near best costs for index plans having same query index type .

 

cc : [~teofili]",indexingPatch,['query'],OAK,Bug,Major,2019-02-08 09:17:28,3
13214414,Node states sometimes refer to more than a single generation of segments after a full compaction,"Due to a regression introduced with OAK-7867 a full compaction can sometimes cause nodes that are written concurrently to reference segments from more than a single gc generation.

This happens when the {{borrowWriter}} method needs to [create a new writer|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentBufferWriterPool.java#L197-L201]. In this case the new writer will be of the generation of the current head state instead of the generation associated with the current write operation in progress.

 

cc [~frm], [~ahanikel]

 ",TarMK,['segment-tar'],OAK,Bug,Major,2019-02-07 16:09:53,15
13213866,AccessControlManagerImpl can not handle repository level when editing policies by principal,"[~stillalex], it seems that editing access control by principal in the default implementation doesn't allow for applying entries to the 'null' path.

initially i thought that we can use an empty string value instead for the {{rep:nodePath}}, but that doesn't work as it gets converted to ""."" for some reason. ",candidate_oak_1_8,"['core', 'security']",OAK,Bug,Minor,2019-02-05 10:41:17,0
13212224,SegmentBlob#readLongBlobId might cause SegmentNotFoundException on standby,"When persisting a segment transferred from master, among others, the cold standby needs to read the binary references from the segment. While this usually doesn't involve any additional reads from any other segments, there is a special case concerning binary IDs larger than 4092 bytes. These can live in other segments (which got transferred prior to the current segment and are already on the standby), but it might also be the case that the binary ID is stored in the same segment. If this happens, the call to {{blobId.getSegment()}}[0], triggers a new read of the current, un-persisted segment . Thus, a {{SegmentNotFoundException}} is thrown:
{noformat}
22.01.2019 09:35:59.345 *ERROR* [standby-run-1] org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSync Failed synchronizing state.
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment d40a9da6-06a2-4dc0-ab91-5554a33c02b0 not found
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readSegmentUncached(AbstractFileStore.java:284) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.lambda$readSegment$10(FileStore.java:498) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentCache$NonEmptyCache.lambda$getSegment$0(SegmentCache.java:163) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3932) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at org.apache.jackrabbit.oak.segment.SegmentCache$NonEmptyCache.getSegment(SegmentCache.java:160) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:498) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:153) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:98) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(SegmentBlob.java:206) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentBlob.readBlobId(SegmentBlob.java:163) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore$3.consume(AbstractFileStore.java:262) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.Segment.forEachRecord(Segment.java:601) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readBinaryReferences(AbstractFileStore.java:257) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:533) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.copySegmentFromPrimary(StandbyClientSyncExecution.java:225) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.copySegmentHierarchyFromPrimary(StandbyClientSyncExecution.java:194) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.compareAgainstBaseState(StandbyClientSyncExecution.java:101) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.execute(StandbyClientSyncExecution.java:76) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSync.run(StandbyClientSync.java:165) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:347) [org.apache.sling.commons.scheduler:2.7.2]
        at org.quartz.core.JobRunShell.run(JobRunShell.java:202) [org.apache.sling.commons.scheduler:2.7.2]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834){noformat}
 

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentBlob.java#L205",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2019-01-28 12:24:15,1
13210334,Principal Management APIs don't allow for search pagination,"Current apis are limited to passing a search token, I'd like to also add range (offset, limit).",principal-management-extensions,"['security', 'security-spi']",OAK,Improvement,Major,2019-01-18 09:02:45,14
13208546,Add multi-threaded segment transfer to oak-run segment-copy,"The transfer of segments between different persistence types when using {{oak-run segment-copy}} can be sped up by employing multiple threads in the transfer. The idea is to try to load {{n}} segments from the source, which are then consumed by the writer on the target, keeping the ordering of the segments in the process. ",tooling,"['oak-run', 'segment-azure']",OAK,Improvement,Minor,2019-01-09 13:36:03,1
13200222,Composite node store: technical debt for create index / reindex,"OAK-7910 resolves the immediate problem, now it should be possible to creating a new Lucene index and reindex.

However, there is no test case, and the general approach to resolve the issue is questionable.",indexingPatch tech-debt tech-debt-test,"['composite', 'core', 'lucene']",OAK,Improvement,Major,2018-11-23 10:34:07,4
13193775,S3 Bucket iterator stops too early,"Fixed a major bug in the S3 bucket iterator.

When the returned queue of records is empty due to the fact that we get a full page of records starting with the META/ key, the iterator stops while there is still data available in the bucket.

This causes problems with datastore GC, and datastore consistency checks (both online and offline), and possibly even more.

A little explainer. But based on a batch size of 2 instead of 1000.

Suppose your list of S3 keys looks as follows:
 * 1
 * 2
 * 3
 * 4
 * META/1
 * META/2
 * 5
 * 6

loadBatch would first load [1, 2], filter out no META/ keys and pass [1, 2] to the caller.
Next time, loadBatch would load [3, 4], filter out no META/ keys and pass [3, 4] to the caller.
Than, loadBatch would load [META/1, META/2], filter out the META/ keys and pass [] to the caller.

When that happens, traversing the bucket would stop, because the returned list is empty, even if there are many more batches to load.

The fix checks if the returned list is empty and there are more batches available, it would load (a) new batch(es) until there is data in the batch or there is no more batch available.

We are currently running Oak 1.6.6 on AEM 6.3.1.2, but as the bug is still in trunk, all previous versions of Oak are affected as well.

I provided 2 pull requests: one for trunk ([https://github.com/apache/jackrabbit-oak/pull/103)] and one for the 1.6 branch ([https://github.com/apache/jackrabbit-oak/pull/104).]

CI failed on [https://github.com/apache/jackrabbit-oak/pull/103,] but I don't think it's related to my changes.

For the record, the patch works as I was able to successfully test this on our production repository using oak-run --id. With version 1.6.6 it reported 800k items, with my patched version, it reported 1.8m items. (As our META/ nodes are listed somewhere half-way through.)

 ",newbie pull-request-available,['blob-cloud'],OAK,Bug,Critical,2018-10-24 07:36:20,16
13193763,S3#getAllIdentifiers may trim listing when filtering out metadata objects,S3#getAllIdentifiers can trim the listing once the whole batch contains only the metadata entries.,candidate_oak_1_4,['blob-cloud'],OAK,Bug,Major,2018-10-24 05:52:40,16
13190670,[DirectBinaryAccess] AzureDataStore not chaining exceptions on upload completion,"In {{AzureBlobStoreBackend#completeHttpUpload()}}, the code catches {{StorageException}} and {{URISyntaxException}} as thrown by the Azure SDK, but this exception is not chained in the {{DataStoreException}} that is thrown if those exceptions are caught, which makes it more difficult to diagnose the real issue behind the {{DataStoreException}}.

This was just a simple oversight in the original implementation.  We should also examine the other code related to direct binary access, both in the Azure and S3 implementations, to see if there are any other similar bugs.",easyfix,['blob-cloud-azure'],OAK,Bug,Minor,2018-10-10 15:44:36,17
13186010,Update tika dependency to 1.19,"Note: requires Java 8, so can't be done for branches before Oak 1.8.

CVEs: CVE-2018-8017, CVE-2018-11761, CVE-2018-11762",requires-java8,['parent'],OAK,Task,Minor,2018-09-19 09:43:39,3
13185131,"deadlock TarMK flush, lucene","We are getting the following deadlock. Please help! (Production environment)

I have already annotated possible locks and synchronized blocks in between:

{noformat}
""TarMK flush [/opt/condat/epet9/sling/repository/segmentstore]"":
  waiting to lock Monitor@0x00007fedfc00cc28 (Object@0x00000004795519a8, a org/apache/jackrabbit/oak/segment/SegmentId),
  which is held by ""oak-lucene-14""
""oak-lucene-14"":
 waiting for ownable synchronizer 0x00000003c13818c0, (a java/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync),
 which is held by ""TarMK flush [/opt/condat/epet9/sling/repository/segmentstore]""

Thread 28883: (state = BLOCKED)
 - org.apache.jackrabbit.oak.segment.SegmentId.getSegment() @bci=12, line=121 (Compiled frame)
        synchronized (this) 
 - org.apache.jackrabbit.oak.segment.Record.getSegment() @bci=4, line=70 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.BlockRecord.read(int, byte[], int, int) @bci=49, line=57 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.read(byte[], int, int) @bci=314, line=189 (Compiled frame)
 - com.google.common.io.ByteStreams.read(java.io.InputStream, byte[], int, int) @bci=43, line=828 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[], int, int) @bci=4, line=695 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[]) @bci=5, line=676 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.getString() @bci=93, line=103 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.readString(int) @bci=189, line=524 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=15, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=37, line=167 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.AbstractFileStore$4.consume(int, org.apache.jackrabbit.oak.segment.RecordType, int) @bci=24, line=354 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.forEachRecord(org.apache.jackrabbit.oak.segment.Segment$RecordConsumer) @bci=48, line=716 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.AbstractFileStore.populateTarBinaryReferences(org.apache.jackrabbit.oak.segment.Segment, org.apache.jackrabbit.oak.segment.file.TarWriter) @bci=25, line=349 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(org.apache.jackrabbit.oak.segment.SegmentId, byte[], int, int) @bci=136, line=657 (Compiled frame)
        fileStoreLock.writeLock().lock(); Zeile 639
        bis: populateTarBinaryReferences
 - org.apache.jackrabbit.oak.segment.SegmentBufferWriter.flush() @bci=383, line=383 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.flush() @bci=165, line=148 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentWriter.flush() @bci=4, line=143 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$7.call() @bci=7, line=373 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$7.call() @bci=1, line=370 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.TarRevisions.doFlush(java.util.concurrent.Callable) @bci=25, line=224 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.TarRevisions.flush(java.util.concurrent.Callable) @bci=42, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.flush() @bci=20, line=370 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$2.run() @bci=15, line=233 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.SafeRunnable.run() @bci=21, line=67 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)

Locked ownable synchronizers:
    - <0x00000003c1361228>, (a java/util/concurrent/locks/ReentrantLock$NonfairSync)
    - <0x00000003c13818c0>, (a java/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync)
    - <0x00000003c1c3a0d8>, (a java/util/concurrent/ThreadPoolExecutor$Worker)

Thread 31035: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(int) @bci=83, line=967 (Interpreted frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(int) @bci=10, line=1283 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock() @bci=5, line=727 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$8.call() @bci=158, line=567 (Compiled frame)
    fileStoreLock.readLock().lock();
 - org.apache.jackrabbit.oak.segment.file.FileStore$8.call() @bci=1, line=542 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(org.apache.jackrabbit.oak.segment.SegmentId, java.util.concurrent.Callable) @bci=1, line=95 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(org.apache.jackrabbit.oak.segment.SegmentId) @bci=14, line=542 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentId.getSegment() @bci=38, line=125 (Compiled frame)
    synchronized (this) 
 - org.apache.jackrabbit.oak.segment.Record.getSegment() @bci=4, line=70 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.BlockRecord.read(int, byte[], int, int) @bci=49, line=57 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.read(byte[], int, int) @bci=314, line=189 (Compiled frame)
 - com.google.common.io.ByteStreams.read(java.io.InputStream, byte[], int, int) @bci=64, line=833 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[], int, int) @bci=4, line=695 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[]) @bci=5, line=676 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.getString() @bci=93, line=103 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.readString(int) @bci=189, line=524 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=15, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.length() @bci=124, line=115 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.<init>(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeBuilder, java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$BlobFactory) @bci=204, line=409 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.<init>(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeBuilder, java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$BlobFactory) @bci=25, line=589 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.fileLength(java.lang.String) @bci=64, line=176 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.copyFilesToLocal(org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory$CORFileReference, boolean, boolean) @bci=195, line=214 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.prefetchIndexFiles() @bci=96, line=170 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.<init>(org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier, org.apache.lucene.store.Directory, org.apache.lucene.store.Directory, boolean, java.lang.String, java.util.concurrent.Executor) @bci=85, line=81 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.wrapForRead(java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.lucene.store.Directory, java.lang.String) @bci=35, line=122 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.reader.DefaultIndexReaderFactory.createReader(org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.jackrabbit.oak.spi.state.NodeState, java.lang.String, java.lang.String, java.lang.String) @bci=61, line=102 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.reader.DefaultIndexReaderFactory.createReaders(org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.jackrabbit.oak.spi.state.NodeState, java.lang.String) @bci=20, line=61 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.plugins.index.lucene.reader.LuceneIndexReaderFactory, org.apache.jackrabbit.oak.plugins.index.lucene.hybrid.NRTIndexFactory) @bci=17, line=68 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker$1.leave(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=30, line=132 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=66, line=153 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.MapRecord.compare(org.apache.jackrabbit.oak.segment.MapRecord, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=197, line=415 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=909, line=608 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=43, line=148 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.MapRecord.compare(org.apache.jackrabbit.oak.segment.MapRecord, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=400, line=457 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=909, line=608 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(org.apache.jackrabbit.oak.spi.commit.Editor, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=34, line=52 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.diffAndUpdate(org.apache.jackrabbit.oak.spi.state.NodeState) @bci=140, line=142 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.update(org.apache.jackrabbit.oak.spi.state.NodeState) @bci=36, line=113 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call() @bci=79, line=135 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call() @bci=1, line=128 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Compiled frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)

Locked ownable synchronizers:
    - <0x0000000476194f30>, (a java/util/concurrent/ThreadPoolExecutor$Worker)   

{noformat}

 ",deadlock,['segment-tar'],OAK,Bug,Blocker,2018-09-14 12:25:29,12
13184064,Clarify update semantics on deleted nodes,"It's not entirely clear to me what behavior we expect if:
 
 * node 1 creates a document
 * node 2 deletes it
 * node 1 tries to update it

(and, related to that, whether the behavior really matters in practice)",candidate_oak_1_8,['documentmk'],OAK,Task,Major,2018-09-10 14:12:44,2
13182799,Order by jcr:score descending is not always ignored,"XPath union queries that contain ""order by @jcr:score descending"" actually do order by jcr:score, which is unexpected.

This is a bit related to OAK-7131.",indexingPatch mohit,['query'],OAK,Improvement,Minor,2018-09-04 12:13:05,4
13181865,CheckCommand should consistently use an alternative journal if specified,"Callers of the {{check}} command can specify an alternative journal with the {{\-\-journal}} option. This option instructs the {{ConsistencyChecker}} to check the revisions stored in that file instead of the ones stored in the default {{journal.log}}.

I spotted at least two problems while using {{\-\-journal}} on a repository with a corrupted {{journal.log}} that didn't contain any valid revision.

First, the path to the {{FileStore}} is validated by {{FileStoreHelper#isValidFileStoreOrFail}}, which checks for the existence of a {{journal.log}} in the specified folder. But if a {{journal.log}} doesn't exist and the user specified a different journal on the command line this check should be ignored.

Second, when opening the {{FileStore}} the default {{journal.log}} is scanned to determine the initial revision of the head state. If a user specifies an alternative journal on the command line, that journal should be used instead of the default {{journal.log}}. It might be that the default journal contains no valid revision, which would force the system to crash when opening a new instance of {{FileStore}}.",technical_debt,"['run', 'segment-tar']",OAK,Bug,Major,2018-08-29 10:12:04,12
13181405,Documentation for direct binary access is unclear,"In the opening paragraph of the documentation on Direct Binary Access, reference is made to S3DataStore and AzureDataStore as blob stores that support this feature, but is is not clear whether they are mentioned as examples or as an exhaustive list of supporting blob stores.  It is only on further examination of the documentation that you are able to determine that the list was specific and not as examples.
We should change the documentation so it is more clear up front which blob stores support the feature.",doc,['doc'],OAK,Bug,Minor,2018-08-27 16:38:04,2
13180468,Test failure: SecurityProviderRegistrationTest.testRequiredUserAuthenticationFactoryNotAvailable(),Some SecurityProviderRegistrationTest.testRequiredUserAuthenticationFactoryNotAvailable() fails every now and then. The last occurrence on travis-ci was here: https://travis-ci.org/apache/jackrabbit-oak/jobs/414016611 but I've also seen the same test fail on other infrastructure.,continuous_integration,['pojosr'],OAK,Bug,Minor,2018-08-22 13:09:41,2
13179095,Backport OAK-6648 to Oak 1.6,"We have seen instances were offline compaction did not clean up all reclaimable tar files on the first run but would reclaim them on subsequent runs. Apparently this is caused by OAK-6648, which fixes an issue where the file reaper is invoked without a prior call to {{System.gc}} reclaiming potential references.",TarMK,['segment-tar'],OAK,Task,Major,2018-08-15 14:05:52,15
13177340,Replace usage of static ValueFactoryImpl methods,The ValueFactoryImpl has a few static methods that are used to create a {{Value}} from a {{PropertyState}} or {{PropertyValue}}. Those methods should be refactored to make it easier to  add a {{BlobAccessProvider}} for OAK-7569.,technical_debt,"['core', 'jcr', 'security-spi', 'store-spi']",OAK,Improvement,Minor,2018-08-07 09:08:37,2
13177087,oak-commons: upgrade to project default mockito version,"Unfortunately, mocking varargs seems to work different in current mockito versions. With the 2:* default from the parent pom, I'm getting the test failure below:
{noformat}
[ERROR] Tests run: 12, Failures: 8, Errors: 0, Skipped: 0, Time elapsed: 0.828 s <<< FAILURE! - in org.apache.jackrabbit.oak.commons.PerfLoggerTest
[ERROR] logAtDebugMessageStartWithInfoLog(org.apache.jackrabbit.oak.commons.PerfLoggerTest)  Time elapsed: 0.045 s  <<< FAILURE!
org.mockito.exceptions.verification.junit.ArgumentsAreDifferent:

Argument(s) are different! Wanted:
logger.debug(
    <any string>,
    <any java.lang.Object[]>
);
-> at org.apache.jackrabbit.oak.commons.PerfLoggerTest.verifyDebugInteractions(PerfLoggerTest.java:227)
Actual invocation has different arguments:
logger.debug(
    ""message [took 0ms]"",
    ""argument""
);
-> at org.apache.jackrabbit.oak.commons.PerfLogger.end(PerfLogger.java:223)

        at org.apache.jackrabbit.oak.commons.PerfLoggerTest.verifyDebugInteractions(PerfLoggerTest.java:227)
        at org.apache.jackrabbit.oak.commons.PerfLoggerTest.logAtDebugMessageStartWithInfoLog(PerfLoggerTest.java:144)


{noformat}",candidate_oak_1_6,['commons'],OAK,Task,Minor,2018-08-06 12:01:27,2
13176090,Introduce oak-run segment-copy for moving around segments in different storages,"Often there's the need to transform a type of {{SegmentStore}} (e.g. local TarMK) into *the exact same* counter-part, using another persistence type (e.g. Azure Segment Store). While {{oak-upgrade}} partially solves this through sidegrades (see OAK-7623), there's a gap in the final content because of the level at which {{oak-upgrade}} operates (node store level). Therefore, the resulting sidegraded repository doesn't contain all the (possibly stale, unreferenced) data from the original repository, but only the latest head state. A side effect of this is that the resulting repository is always compacted.

Introducing a new command in {{oak-run}}, namely {{segment-copy}}, would allow us to operate at a lower level (i.e. segment persistence), dealing only with constructs from {{org.apache.jackrabbit.oak.segment.spi.persistence}}: journal file, gc journal file, archives and archive entries. This way the only focus of this process would be to ""translate"" a segment between two persistence formats, without caring about the node logic stored inside (referenced/unreferenced node/property).",tooling,"['oak-run', 'segment-tar']",OAK,Improvement,Major,2018-08-01 11:34:04,1
13174984,Refactor AzureCompact and Compact,"{{AzureCompact}} in {{oak-segment-azure}} follows closely the structure and logic of {{Compact}} in {{oak-segment-tar}}. Since the only thing which differs is the underlying persistence used (remote in Azure vs. local in TAR files), the common logic should be extracted in a super-class, extended by both. ",tech-debt technical_debt tooling,['segment-tar'],OAK,Improvement,Major,2018-07-26 20:31:35,1
13173120,Remove strategy to optimize secondary reads,OAK-3865 introduced a strategy to optimize reads from secondaries. This has been superseded by OAK-6087. This task is about removing the old strategy.,technical_debt,['mongomk'],OAK,Task,Major,2018-07-19 07:26:21,2
13172835,Revert changes done by OAK-6770,"With the changes from OAK-6770 applied, it seems that {{repository.home}} attribute is not correctly set, causing the repository to be written one level up in the directory hierarchy from where it was supposed to. ",osgi,['segment-tar'],OAK,Technical task,Major,2018-07-18 06:05:59,12
13172638,various internal APIs missing in package export filter,"- org.apache.jackrabbit.oak.spi.cluster
- org.apache.jackrabbit.oak.namepath.impl
- org.apache.jackrabbit.oak.plugins.index.property.strategy
- org.apache.jackrabbit.oak.plugins.migration.report",candidate_oak_1_6,['parent'],OAK,Task,Minor,2018-07-17 12:32:27,3
13172627,Prevent commits in the past,"This is similar to OAK-3883, but must prevent commits with revisions that are older than already present in the repository. At runtime, this is already taken care of with static fields in the Revision class, but on startup the clock may have jumped into the past since Oak was stopped.",resilience,['documentmk'],OAK,Improvement,Minor,2018-07-17 11:49:32,2
13172315,oak-run check should support Azure Segment Store,"{{oak-run check}} should accept Azure URIs for the segment store in order to be able to check for data integrity. This will come handy in the light of remote compacted segment stores and/or sidegraded remote segment stores (see OAK-7623, OAK-7459).

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2018-07-16 06:43:34,1
13171335,SegmentNodeStore - sidegrade support between TarPersistence and AzurePersistence,"Azure support for segment-tar (OAK-6922) allowed us to plug another storage option for the segment store. Since sometimes there's the need to compare how local vs remote storage behaves, a sidegrade from local tar storage to remote azure storage must be implemented.

This would allow us to replicate the exact repository content, changing only the underlying storage mechanism. Analogous to OAK-7459, the Azure Segment Store connection details will be supplied in the following format:
 * an URI with the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider
 * a secret key supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",azure migration,"['segment-tar', 'upgrade']",OAK,New Feature,Major,2018-07-11 08:23:12,1
13168004,Add stats for DataStore GC performance,"To ascertain effectiveness of DataStore GC following metrics are quite useful:
* Number of candidates identified
* Number of blobs deleted
* Total estimated size of the blobs deleted",maintenance,['blob-plugins'],OAK,Improvement,Major,2018-06-25 09:57:51,16
13167314,Rename metrics for DataStore garbage collection,Minor renaming of the metrics to mimic what was done for OAK-7555,maintenance,['blob-plugins'],OAK,Bug,Major,2018-06-21 05:57:35,16
13167181,Commit fails when forced journal push throws exception,"OAK-3976 introduced a force push of of a journal entry during the commit when the accumulated changes reach 100'000 elements.

Creating the journal entry however may fail with a DocumentStoreException and fail the commit even though all changes, including the one on the commit root, made it to the DocumentStore.",candidate_oak_1_6,['documentmk'],OAK,Bug,Minor,2018-06-20 14:43:15,2
13167093,CacheActionDispatcher not memory bound,The persistent cache has an async write mode enabled by default and the write queue is maintained by the CacheActionDispatcher. The queue has a fixed size of 16'384 and is not memory bound. It may happen that the queue retains a lot of memory (multiple GB of heap) when the pending write actions reference big cache values and cause OOME.,candidate_oak_1_4 candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Major,2018-06-20 07:36:41,2
13166027,MissingLastRevSeekerTest fails on MongoDB with secondary preferred,"MissingLastRevSeekerTest actually does not run on MongoDB right now, but changing the test accordingly revealed an issue with the MongoDB specific implementation of MissingLastRevSeeker when running on a replica-set and secondary preferred read preference.

The class MongoMissingLastRevSeeker uses the configured read preference when checking for cluster node info entries that require recovery. This is inconsistent because the candidate nodes are read with primary read preference.",candidate_oak_1_6,"['documentmk', 'mongomk']",OAK,Bug,Minor,2018-06-14 07:31:01,2
13163397,Enable collection of simple operation stats for DataStore garbage collection,"Enable collection of simple collection stats related to the DSGC:
* Start counter
* Finish success counter
* Finish failure counter
* Duration timer
This can later be enhanced to capture other performance metrics as well.",maintenance,['blob-plugins'],OAK,New Feature,Major,2018-06-01 10:33:19,16
13162018,Run repository initializers with hooks,"Currently the repository initializers (RepositoryInitializer and WorkspaceInitializer) run when the repo boots without any hooks [0] which means that current RepositoryInitializers need to setup custom Roots (roots with hardcoded editor providers like NamespaceEditorProvider and TypeEditorProvider) on top of the provided builders to be able to setup properly. I'm looking at the InitialContent [1] and the CugConfiguration [2] in the context of installing node types.

I would like to look into removing the hardcoded providers and trying to run all existing editors over the content produced by the initializers.

As an added benefit this will allow decoupling of hard dependencies between components (see for example OAK-7499)



[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/Oak.java#L687 
[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/InitialContent.java#L134
[2] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-authorization-cug/src/main/java/org/apache/jackrabbit/oak/spi/security/authorization/cug/impl/CugConfiguration.java#L162
",modularization,['core'],OAK,Improvement,Major,2018-05-25 09:10:36,14
13159633,Deprecate org.apache.jackrabbit.oak.plugins.lock.LockConstants in favor of corresponding SPI,"for consistency with other JCR specific constants that have been copied to _oak-core-spi_, we should do the same for {{LockConstants}}. This will help to further reduce the dependency to _oak-core_ across the different modules.",m12n,"['core', 'core-spi']",OAK,Improvement,Minor,2018-05-16 07:30:12,0
13158259,VersionablePathHook should be located with authorization code,"in order to cleanup troublesome dependencies within oak core, the {{VersionablePathHook}} associated with the default authorization model should be co-located with the latter instead of being placed inside _o.a.j.oak.plugins.version_.

[~stillalex], fyi",m12n,"['core', 'security']",OAK,Improvement,Major,2018-05-09 17:52:56,0
13156818,Remove Usage of ImmutableTree and AbstractTree in Security Code,"With a minor extension to {{TreeProvider}} we would be able to get rid of the direct casting to implementation details like {{ImmutableTree}} and {{AbstractTree}} altogether.

[~stillalex], patch for review will follow right away.",m12n,"['authorization-cug', 'core', 'security-spi']",OAK,Improvement,Major,2018-05-03 09:26:59,0
13155694,oak-run compact should support Azure Segment Store,"{{oak-run compact}} should accept Azure URIs for the segment store in order to enable OffRC for Azure Segment Store.

-Proposed options to add:-
 * -{{azure-connection}}: connection URL to to connect to the Azure Storage-
 * -{{azure-container}}: name of the container to use-
 * -{{azure-root-path}}: segment store directory-

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where *az* identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2018-04-27 12:22:01,1
13154803,Allow collection of IOTraces during normal operation,"The IO tracing facility introduced with OAK-5655 should also be available during normal operation. The idea is to log IO reads intercepted via an {{IOMonitor}} instance to a logger. If {{DEBUG}} logging is not enabled for that logger at the time when the {{FileStore}} is instantiated, then no tracing would take place.",monitoring tooling,['segment-tar'],OAK,New Feature,Major,2018-04-24 13:29:05,15
13154787,Remove dependency to commons-codec,The module oak-store-document currently only uses a single utility method from commons-codec. It shouldn't be too difficult to remove this dependency.,technical_debt,['documentmk'],OAK,Improvement,Minor,2018-04-24 12:26:14,2
13153315,Introduce SegmentNodeStoreMonitorService for exposing writerGroups as an OSGi config property,"It would be useful to expose {{writerGroups}} in {{SegmentNodeStoreStats}} through an OSGi config property. Since this is a low level configuration related to monitoring, it shouldn't be added to {{SegmentNodeStoreService}}, but to a newly created service, {{SegmentNodeStoreMonitorService}} that would handle exposing monitoring related stuff. ",tooling,['segment-tar'],OAK,Improvement,Minor,2018-04-18 10:46:59,1
13152910,Contribute a 'proc' subtree for the Segment Node Store,"With guidance from [~mduerig], I recently developed a way to expose Segment Node Store's internal information through the NodeState API. 

The concept is similar in spirit to the proc file system in Linux: the proc subtree exposes internal information in a straightforward manner, enabling consumers to rely on a well-understood API to access the data. This proc subtree shelters tooling from variations of the internal APIs of the Segment Store. As long as the data exported through the proc subtree is stable, the same tools are going to work across different versions of the Segment Store with minimal to no modifications.

The proc subtree has been developed in [this branch on GitHub|https://github.com/francescomari/jackrabbit-oak/tree/proc]. I created this issue in order to review the work done so far, and to track the contribution of the proc subtree in Oak.",tooling,['segment-tar'],OAK,Improvement,Major,2018-04-17 06:53:58,12
13151579,Expose UI for collecting IO traces,"[http://svn.apache.org/viewvc?view=revision&revision=1827841] introduced utility classes to collect IO traces. See e.g. https://issues.apache.org/jira/browse/OAK-5655?focusedCommentId=16415730&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16415730. Currently the only way to run such traces is via code or JUnit (see {{IOTracerRunner}}).

Going forward we should wire this functionality to {{oak-run}} to make it more generally useful. 

 

[~frm], FYI.",tooling,"['oak-run', 'segment-tar']",OAK,Improvement,Major,2018-04-11 08:05:29,15
13151561,Changes kept in memory when update limit is hit in commit hook,"In some cases no persisted branch is created by the DocumentNodeStore when the number of changes hit the update limit. This happens when the current branch state is in-memory and the commit hook contributes changes that reach the update limit. The implementation keeps those changes in memory, which may lead to a commit way bigger than specified by the update limit.",candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Major,2018-04-11 06:30:29,2
13149438,SegmentNodeStoreStats should expose stats for previous minute per thread group,"The current ""CommitsCountPerWriter"" stats exposed by {{SegmentNodeStoreStats}} are hard to follow since there can be too many writers at a time. To improve this, a more coarse-grained version of this metric should be added, in which commits are recorded for groups of threads. The groups should be configurable and represent regexes to be matched by individual thread names. An additional group (i.e. ""other"") will group all threads not matching any of the defined group regexes. 

The current behaviour will be split in two:
* ""CommitsCountOtherThreads"" will expose a snapshot of threads currently in ""other"" group
* ""CommitsCountPerGroup"" will expose an aggregate of commits count per thread group for the previous minute.

Both metrics will be reset each minute.",tooling,['segment-tar'],OAK,Improvement,Minor,2018-04-02 08:55:03,1
13148170,"Lucene Index: per-column selectivity, assume 5 unique entries","Currently, if a query has a property restriction of the form ""property = x"", and the property is indexed in a Lucene property index, the estimated cost is the index is the number of documents indexed for that property. This is a very conservative estimate, it means all documents have the same value. So the cost is relatively high for that index.

In almost all cases, there are many distinct values for a property. Rarely there are few values, or a skewed distribution where one value contains most documents. But in almost all cases there are more than 5 distinct values.

I think it makes sense to use 5 as the default value. It is still conservative (cost of the index is high), but much better than now.",candidate_oak_1_8,"['lucene', 'query']",OAK,Improvement,Major,2018-03-27 09:12:42,4
13146525,Migrate to the MongoDB Java driver API 3.0,The {{MongoDocumentStore}} currently uses the old API in {{com.mongodb}}. Starting with MongoDB Java driver 3.0 a new client API was introduced in {{com.mongodb.client}}. New features like client sessions are only available in the new API and MongoDB may remove some deprecated methods/classes/interfaces in the future. The implementation should be migrated to the new client API.,technical_debt,['mongomk'],OAK,Technical task,Major,2018-03-20 12:09:46,2
13145333,Improvements to PermissionEntryProviderImpl,Container issue to track potential improvements to {{PermissionEntryProviderImpl}} based on discussions with [~stillalex].,performance,"['core', 'security']",OAK,Improvement,Major,2018-03-15 10:39:06,0
13145009,Fix all sidegrades breaking with UnsupportedOperationException on MissingBlobStore by introducing LoopbackBlobStore,"h4. Problem

In some edge cases when the binary under the same path (/content/asset1) is modified by 2 independent checkpoints: A & B the sidegrade without providing DataStore might fail with the following error:
{noformat:title=An exception thrown by oak-upgrade tool}
Caused by: java.lang.UnsupportedOperationException: null
    at org.apache.jackrabbit.oak.upgrade.cli.blob.MissingBlobStore.getInputStream(MissingBlobStore.java:62)
    at org.apache.jackrabbit.oak.plugins.blob.BlobStoreBlob.getNewStream(BlobStoreBlob.java:47)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.getNewStream(SegmentBlob.java:276)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.getNewStream(SegmentBlob.java:86)
    at org.apache.jackrabbit.oak.plugins.memory.AbstractBlob$1.openStream(AbstractBlob.java:44)
    at com.google.common.io.ByteSource.contentEquals(ByteSource.java:344)
    at org.apache.jackrabbit.oak.plugins.memory.AbstractBlob.equal(AbstractBlob.java:67)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.equals(SegmentBlob.java:227)
    at com.google.common.base.Objects.equal(Objects.java:60)
    at org.apache.jackrabbit.oak.plugins.memory.AbstractPropertyState.equal(AbstractPropertyState.java:59)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.equals(SegmentPropertyState.java:242)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareProperties(SegmentNodeState.java:617)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:511)
(the same nested methods)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:604)
    at org.apache.jackrabbit.oak.upgrade.PersistingDiff.diff(PersistingDiff.java:139)
    at org.apache.jackrabbit.oak.upgrade.PersistingDiff.childNodeChanged(PersistingDiff.java:191)
    at org.apache.jackrabbit.oak.plugins.segment.MapRecord$3.childNodeChanged(MapRecord.java:440)
    at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:483)
    at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:432)
    at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:604)
    at org.apache.jackrabbit.oak.upgrade.PersistingDiff.diff(PersistingDiff.java:139)
    at org.apache.jackrabbit.oak.upgrade.PersistingDiff.applyDiffOnNodeState(PersistingDiff.java:106)
    at org.apache.jackrabbit.oak.upgrade.RepositorySidegrade.copyDiffToTarget(RepositorySidegrade.java:403)
    at org.apache.jackrabbit.oak.upgrade.RepositorySidegrade.migrateWithCheckpoints(RepositorySidegrade.java:347)
{noformat}
 
h4. Abstract of proposed solution

The idea for migration is simple: instead of failing on:
{code:java}
public InputStream getInputStream(String blobId) throws IOException;
{code}
or
{code:java}
public int readBlob(String blobId, long pos, byte[] buff, int off, int length) throws IOException;
{code}
lets introduce a BlobStore implementation that acts similarly as a *localhost* interface that what you sent it will resend back to this interface.
h4. How it works

It works as a *localhost* interface, the same way: when *{{blobId}}* is requested... then *{{blobId}}* is served as a binary content instead *of throwing*: {{UnsupportedOperationException}}.

This allows to act quickly on migrations that requires to compare binaries in order to satisfy requirements for checkpoints to be rewritten, copied from scratch.
h4. Pros
 * simplifies simple sidegrade migration use cases: you do not need anymore to include your DataStore (which slows that migration not necessary) on the command line when the migration is failing
 * speeds up the migration as it doesn't require to reference BlobStore implementation in cases where binary references are only copied together with NodeStore inlined binaries
 * you're always copying checkpoints which means (no need anymore for {{--skip-checkpoints}} option) that no full re-indexes are happening anymore after migration on migrated repository

h4. Cons and risks
 * *Low risk:* not visible effect to user that for a specific migration a DataStore is needed (whether you are running into this specific edge case)
 * *Medium risk:* NodeStore storage overhead for checkpoints if compared binaries across checkpoints have different blob IDs (in example different algorithms SHA256 vs SHA512). *This we'll lead in comparison to not equal evaluation and the node will be rewritten for the checkpoint.*
 * *Low risk:* Currently we're accepting in {{readBlob}} requests to copy smaller binaries whilst the caller expects higher length of binary (it might be the case if {{getBlobLength}} is not used for some reasons apriori to the {{readBlob}} and the original length of the binary (that is really placed in real DataStore) is kept somewhere in cache. The API here informs anyway how many bytes were read and it recommends to caller to check for that value.

 ",candidate_oak_1_4,['upgrade'],OAK,Bug,Major,2018-03-14 12:26:46,18
13144981,CommitsTracker data is always empty when exposed via JMX,"Due to duplicate registration of {{SegmentNodeStoreStats}} in both {{SegmentNodeStore}}  and {{LockBasedScheduler}}, we end up with two instances of this MBean. The former gets exposed via JMX and always returns empty tables for CommitsCountPerWriter and QueuedWriters, while the latter correctly tracks these data, but is not exposed. To address this, we should stick to only one instance of {{SegmentNodeStoreStats}}, used in both {{SegmentNodeStore}} and {{LockBasedScheduler}}.

While at this, two additional points to be addressed:
# {{CommitsTracker}} needs to be unit tested
# commits count map size needs to be configurable via {{SegmentNodeStoreStats}}",tooling,['segment-tar'],OAK,Bug,Major,2018-03-14 10:02:29,1
13144371,Transform CacheWeightEstimator into a unit test,We should transform {{CacheWeightEstimator}} from a stand-alone utility into a unit test such that we can regularly run in on a CI.,technical_debt,['segment-tar'],OAK,Improvement,Major,2018-03-12 16:51:18,15
13143850,RDBDocumentStore: Refactor exception handling,Refactor exception handling in preparation for OAK-7286.,candidate_oak_1_8,['rdbmk'],OAK,Task,Minor,2018-03-09 09:49:13,3
13143234,SegmentParser#parseBlob does not long ids of external blobs,Support for long ids of external blobs where introduces with OAK-3107. {{SegmentParser.parseBlob()}} is still oblivious about them.,technical_debt tooling,['segment-tar'],OAK,Bug,Major,2018-03-07 14:40:04,15
13141749,Remove debug logging to the console during tests,OAK-4707 introduced logging at debug logging to the system console for sorting out test failures on Jenkins. Since we haveen't seen these failures for a while and that issue is fixed I would like to remove the extra logging again to avoid cluttering the console unnecessarily.,technical_debt,['segment-tar'],OAK,Improvement,Major,2018-03-01 10:00:43,15
13138487,DocumentStore: add test coverage for various types of IDs,Including invalid ones.,candidate_oak_1_8,['documentmk'],OAK,Technical task,Major,2018-02-14 13:39:57,3
13137875,Improve SegmentNodeStoreStats to include number of commits per thread and threads currently waiting on the semaphore,"When investigating the performance of  {{segment-tar}}, the source of the writes (commits) is a very useful indicator of the cause.

To better understand which threads are currently writing in the repository and which are blocked on the semaphore, we need to improve {{SegmentNodeStoreStats}} to:
 * expose the number of commits executed per thread
 * expose threads currently waiting on the semaphore",tooling,['segment-tar'],OAK,Improvement,Major,2018-02-12 14:22:03,1
13137149,"Indexes with excludedPaths, or includedPaths should not be picked for queries without path","Queries that don't have a clear path restriction should not use indexes that have excludedPaths or includedPaths set, except in some exceptional cases (to be defined).

For example, if a query doesn't have a path restriction, say:

{noformat}
/jcr:root//element(*, nt:base)[@status='RUNNING']
{noformat}

Then an index that has excludedPaths set (for example to /etc) shouldn't be used, at least not if a different index is available. Currently it is used currently, actually in _favor_ of another index, if the property ""status"" is commonly used in /etc. Because of that, the index that doesn't have excludedPath has a higher cost (as it indexes the property ""status"" in /etc, and so has more entries for ""status"", than the index that doesn't index /etc).

The same for includedPaths, in case queryPaths isn't set to the same value(s).",indexingPatch mohit,"['lucene', 'query']",OAK,Improvement,Critical,2018-02-08 13:20:24,4
13136571,Remove deprecated deep option from check command,"With OAK-5595 we have enabled deep traversals by default when using the check command. At the same time we have deprecated the --{{deep}} option.

Since all these happened for {{1.8}}, the next logical step to do for {{1.10}} is to remove this option altogether.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2018-02-06 14:14:57,1
13135379,Reduce calls to DocumentStore,Analyze and further reduce calls to the DocumentStore when content is written to the repository. ,performance,['documentmk'],OAK,Improvement,Major,2018-02-01 10:52:00,2
13135038,oak-run check should have an option to check the segments checksums,"{{oak-run check}} does currently *not* check the checksums of the segments. As a consequence, there is no quick way of determining the state of the repository (corrupt/valid), after corrupting some random node record, as we currently do in {{CheckRepositoryTestBase#corruptRecord}}. To determine that, there needs to be an attempt to read the corrupt record as part of a traversal.

An easier way would be to have a new dedicated option for this (i.e., {{--segments}}) which checks by default the content of segments against the checksums from all the tar files in the specified location. Additionally, it could accept as an argument a list of tar files, the segments of which to be checked.",tooling,"['run', 'segment-tar']",OAK,New Feature,Minor,2018-01-31 10:29:01,1
13134479,Remove support for binaries and documents in persistent cache,"The persistent cache currently stores binaries up to one MB by default. However most of the BlobStore implementations already provide some form of caching. E.g. for S3 a cache on the local filesystem is maintained and when using a Jackrabbit FileDataStore the persistent cache is actually unnecessary.

Support for documents in the persistent cache should also be removed. In contrast to other cache entries, documents are mutable and may cause consistency issues when enabled with the persistent cache. ",technical_debt,['documentmk'],OAK,Task,Minor,2018-01-29 15:46:48,2
13134438,Add configurable repository size cap to SegmentOverflowExceptionIT,"{{SegmentOverflowExceptionIT}} potentially consumes a lot of disk space. Running it for 10 minutes on a AWS m4.4xlarge instance with 900 / 3000 IOPS resulted in 80GB being taken up. 
Currently the test can be time boxed but not size boxed. I suggest to add another option to cap the repository size in addition to the existing {{-Dtimeout}}.",test,['segment-tar'],OAK,Improvement,Major,2018-01-29 12:39:03,15
13134388,Avoid call for child node when bundle contains all children,"When nodes are bundled in a document, the DocumentNodeStore keeps track of whether all children are included in a document. The presence of the hidden {{:doc-has-child-non-bundled}} property indicates there are non bundled child nodes. For the case when a document contains all children in the bundle, the DocumentNodeStore still does a find call on the DocumentStore when asked for an unknown child node.",bundling candidate_oak_1_6,['documentmk'],OAK,Improvement,Minor,2018-01-29 08:48:43,2
13133748,Various disallowed control characters are accepted in item names,"Our node name check currently allow control characters other than CR, LF and TAB. This is a bug according to JCR, names being restricted to XML characters.",candidate_oak_1_8,['jcr'],OAK,Bug,Major,2018-01-25 16:46:07,3
13133362,Node.getMixinNodeTypes() may check for child node named jcr:mixinTypes,In some cases a call to {{Node.getMixinNodeTypes()}} may result in a check whether there is a child node named {{jcr:mixinTypes}}. ,performance,['jcr'],OAK,Improvement,Minor,2018-01-24 13:37:15,2
13132712,guava: ListenableFuture.transform() changes to transformAsync in version 20,"See https://google.github.io/guava/releases/19.0/api/docs/com/google/common/util/concurrent/Futures.html#transform(com.google.common.util.concurrent.ListenableFuture,%20com.google.common.util.concurrent.AsyncFunction)",technical_debt,['segment-tar'],OAK,Technical task,Major,2018-01-22 13:40:37,15
13131603,Update documentation for oak-run check,We should review and update the documentation of [{{oak-run check}}|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#check]. E.g. to include the new options from OAK-6373.,documentation,"['doc', 'oak-run', 'segment-tar']",OAK,Task,Major,2018-01-17 15:05:40,1
13131600,Document TarMK specific MBeans,Currently the [TarMK documentation|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#monitoring-via-jmx] only mentions {{SegmentRevisionGarbageCollection}}. We should review that paragraph and also include documentation for all other relevant JMX endpoints.,documentation,"['doc', 'segment-tar']",OAK,Task,Major,2018-01-17 15:01:58,15
13131220,Race condition on revisions head between compaction and scheduler could result in skipped commit,"There is a race condition on {{TarRevisions#head}} between a running compaction trying to set the new head [0] and the scheduler doing the same after executing a specific commit [1]. If the compaction thread is first, then the head assignment in the scheduler will fail and not be re-attempted. 

IMO, the simple if statement should be changed to a while loop in which the head is refreshed and the commit is re-applied against the new head, before attempting again to set a new head in {{TarRevisions}}. This is somehow similar to what we previously had [2], but without the unneeded optimistic/pessimistic strategies involving tokens.

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/FileStore.java#L764
[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/scheduler/LockBasedScheduler.java#L253
[2] https://github.com/apache/jackrabbit-oak/blob/1.6/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentNodeStore.java#L686",scalability,['segment-tar'],OAK,Bug,Blocker,2018-01-16 12:17:50,1
13129793,SNFE after full compaction,"In some cases we observed a {{SNFE}} right after a the cleanup following a full compaction:

{noformat}
31.12.2017 04:25:19.816 *ERROR* [pool-17-thread-22] org.apache.jackrabbit.oak.segment.SegmentNotFoundExceptionListener Segment not found: a82a99a3-f1e9-49b7-a1e0-55e7fec80c41. SegmentId age=609487478ms,segment-generation=GCGeneration{generation=4,fullGeneration=2,isCompacted=true}
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment a82a99a3-f1e9-49b7-a1e0-55e7fec80c41 not found
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readSegmentUncached(AbstractFileStore.java:276)
        at org.apache.jackrabbit.oak.segment.file.FileStore.lambda$readSegment$5(FileStore.java:478)
        at org.apache.jackrabbit.oak.segment.SegmentCache.lambda$getSegment$0(SegmentCache.java:116)
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)
        at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:113)
        at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:478)
        at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:154)
        at org.apache.jackrabbit.oak.segment.CachingSegmentReader$1.apply(CachingSegmentReader.java:94)
        at org.apache.jackrabbit.oak.segment.CachingSegmentReader$1.apply(CachingSegmentReader.java:90)
        at org.apache.jackrabbit.oak.segment.ReaderCache.get(ReaderCache.java:118)
        at org.apache.jackrabbit.oak.segment.CachingSegmentReader.readString(CachingSegmentReader.java:90)
        at org.apache.jackrabbit.oak.segment.MapRecord.getEntry(MapRecord.java:220)
        at org.apache.jackrabbit.oak.segment.MapRecord.getEntry(MapRecord.java:173)
        at org.apache.jackrabbit.oak.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:423)
        at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.<init>(MemoryNodeBuilder.java:143)
        at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.<init>(SegmentNodeBuilder.java:93)
        at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.createChildBuilder(SegmentNodeBuilder.java:148)
        at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNode(MemoryNodeBuilder.java:331)
        at org.apache.jackrabbit.oak.core.SecureNodeBuilder.<init>(SecureNodeBuilder.java:112)
        at org.apache.jackrabbit.oak.core.SecureNodeBuilder.getChildNode(SecureNodeBuilder.java:329)
        at org.apache.jackrabbit.oak.core.MutableTree.getTree(MutableTree.java:290)
        at org.apache.jackrabbit.oak.core.MutableRoot.getTree(MutableRoot.java:220)
        at org.apache.jackrabbit.oak.core.MutableRoot.getTree(MutableRoot.java:69)
        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.getItem(SessionDelegate.java:442)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.getItemInternal(SessionImpl.java:167)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.access$400(SessionImpl.java:82)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl$3.performNullable(SessionImpl.java:229)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl$3.performNullable(SessionImpl.java:226)
        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.performNullable(SessionDelegate.java:243)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.getItemOrNull(SessionImpl.java:226)
{noformat}

",compaction,['segment-tar'],OAK,Bug,Blocker,2018-01-10 09:44:03,15
13126821,Update documentation for cold standby,"Improve monitoring section of cold standby in {{oak-doc}} to include missing MBean screenshots.

-[~mduerig], [~frm]: How about adding a *Benchmarking* section to the cold standby page covering a bit ways to use the new {{Oak-Segment-Tar-Cold}} fixture and also running {{ScalabilityStandbySuite}} on top of it?-",documentation,"['doc', 'segment-tar', 'tarmk-standby']",OAK,Documentation,Major,2017-12-22 14:17:46,1
13126227,Compaction should log generation info,When compaction starts it should also log the current gc generation and the new gc generation it is going to create. ,compaction gc,['segment-tar'],OAK,Improvement,Minor,2017-12-20 09:24:53,15
13125964,ArrayIndexOutOfBoundsException when upgrading from Oak 1.6,"When starting on an Oak 1.6 repository and the {{gc.log}} file is present the TarMK fails with:
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 5
        at org.apache.jackrabbit.oak.segment.file.GCJournal$GCJournalEntry.parseString(GCJournal.java:217)
        at org.apache.jackrabbit.oak.segment.file.GCJournal$GCJournalEntry.fromString(GCJournal.java:204)
        at org.apache.jackrabbit.oak.segment.file.GCJournal.read(GCJournal.java:115)
        at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.compact(FileStore.java:750)
        at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.compactFull(FileStore.java:731)
        at org.apache.jackrabbit.oak.segment.file.FileStore.compactFull(FileStore.java:385)
        at org.apache.jackrabbit.oak.segment.tool.Compact.run(Compact.java:273)
        at org.apache.jackrabbit.oak.run.CompactCommand.execute(CompactCommand.java:72)
        at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
{noformat}",migration,['segment-tar'],OAK,Bug,Critical,2017-12-19 16:41:02,15
13125830,Segment-Tar-Cold fixture should have options for secure communication and one shot runs,"The newly introduced {{Segment-Tar-Cold}} fixture should support secure communication between primary and standby via a {{--secure}} option. Moreover, the current implementation allows only for continuous sync between primary and standby. It should be possible to allow a ""one-shot run"" of the sync to easily measure and compare specific metrics ({{--oneShotRun}} option).",cold-standby,"['benchmarks', 'segment-tar', 'tarmk-standby']",OAK,Improvement,Minor,2017-12-19 09:07:54,1
13125733,NullPointerException in FilteredSortedSetDocValuesFacetCounts during query evaluation,"Running the following query {{select \[rep:facet(simple/tags)] from \[nt:base] where contains(\[text], 'ipsum')}} with the following content 

{code}
/content/foo
 - text = ""lorem lorem""
 + simple/
   - tags = [""tag1"", ""tag2""]
/content/bar
 - text = ""lorem ipsum""
{code}

runs in the following NPE

{code}
java.lang.NullPointerException
	at org.apache.jackrabbit.oak.plugins.index.lucene.util.FilteredSortedSetDocValuesFacetCounts.getTopChildren(FilteredSortedSetDocValuesFacetCounts.java:63)
	at org.apache.lucene.facet.MultiFacets.getTopChildren(MultiFacets.java:52)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex$LucenePathCursor$2.getValue(LucenePropertyIndex.java:1646)
	... 38 more
{code}

This is because the result set for the query only contains {{/content/bar}} and with that the count of the dimension {{simple/tag}} is 0. For that case [SortedSetDocValuesFacetCounts#getDim()|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java#L108] returns {{null}} and so does {{getTopChildren}}.

This expected behaviour is properly handled in [LucenePropertyIndex.java#L1647|https://github.com/apache/jackrabbit-oak/blob/jackrabbit-oak-1.6.7/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java#L1647] but not in [FilteredSortedSetDocValuesFacetCounts.java#L63|https://github.com/apache/jackrabbit-oak/blob/jackrabbit-oak-1.6.7/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/util/FilteredSortedSetDocValuesFacetCounts.java#L63] where {{topChildren}} is dereferenced without null check.

To workaround that secure facets can be set to false, though the default value is true.
 ",facet,['lucene'],OAK,Bug,Major,2017-12-18 22:38:14,11
13125593,Document oak-run compact arguments and system properties,"Ensure {{oak-doc}} is up to date with the current version of {{oak-run compact}}, its current command line arguments and system properties. ",documentation,"['doc', 'segment-tar']",OAK,Task,Major,2017-12-18 10:47:03,15
13125455,rep:excerpt selector broken as regression of OAK-6750,"The change made here:
https://github.com/apache/jackrabbit-oak/commit/00c94b71293abcae6d76bb162c3f55c7d09b702e#diff-d4bdf443c61f24b634f33aab607e2114

breaks the logic in line 676:

{{else if (oakPropertyName.equals(QueryConstants.REP_EXCERPT + ""(""))}}

This statement doesn't make much sense considering a query like {{select \[rep:excerpt] from \[test:Page] as page where contains(\*, 'term\*')}} or even {{select \[rep:excerpt(text)] from \[test:Page] as page where contains(page.\[text], 'term\*')}}",excerpt,['lucene'],OAK,Bug,Major,2017-12-17 09:41:15,11
13124683,oak-run compact reports success even when it was cancelled,When {{oak-run compact}} gets cancelled because running out of disk space it will send a corresponding warning to the logs and bail out. However on the console it will still report success. ,production tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-12-13 14:09:31,15
13124679,Segment.toString: Record table should include an index into the hexdump,Currently the Segment dump created in {{Segment.toString}} includes a list of records with their offsets. However these offsets do no match the ones in the subsequent raw byte dump of the segment. We should add a raw offsets to the list of records so finding the actual data that belongs to a record doesn't involve manually fiddling with logical / physical offset translation. ,tooling,['segment-tar'],OAK,Improvement,Minor,2017-12-13 14:05:42,15
13124359,Remove dangling reference to compress-interval system property from oak-run documentation,"{{oak-doc/src/site/markdown/command_line.md}} refers to the {{compress-interval}} system property, which does not exist any more. ",documentation,['doc'],OAK,Improvement,Major,2017-12-12 13:02:53,15
13124338,Offline compaction corrupts repository,Offline compaction can corrupt the repository in some cases: when offline compaction is cancelled by the {{CancelCompactionSupplier}} the corresponding return value is not correctly passed up the call chain resulting in a incomplete compacted head state being set as the compacted head state (instead of being discarded). ,corruption data-corruption,['segment-tar'],OAK,Bug,Blocker,2017-12-12 11:00:34,15
13124118,DocumentStore API: clarify key length,"""For keys, the maximum length is 512 bytes in the UTF-8 representation (in the latest Unicode version).""

That's a bit misleading, as the UTF-8 representation is independent of the Unicode version.",candidate_oak_1_4,['documentmk'],OAK,Task,Minor,2017-12-11 12:26:32,3
13122808,Test failure: ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout,"Seen on an internal Windows Jenkins node:

h3. Regression

org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout

h3. Error Message

{noformat}
Values should be different. Actual: { root = { ... } }
{noformat}

h3. Stacktrace

{noformat}
java.lang.AssertionError: Values should be different. Actual: { root = { ... } }
	at org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout(ExternalPrivateStoreIT.java:87)
{noformat}

h3. Standard Output

{noformat}
22:41:13.646 INFO  [main] FileStoreBuilder.java:340         Creating file store FileStoreBuilder{version=1.8-SNAPSHOT, directory=target\junit2834122541179880349\junit3041268421527563090, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore], maxFileSize=1, segmentCacheSize=0, stringCacheSize=0, templateCacheSize=0, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1, memoryMapping=false, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}}
22:41:13.646 INFO  [main] FileStore.java:241                TarMK opened at target\junit2834122541179880349\junit3041268421527563090, mmap=false, size=0 B (0 bytes)
22:41:13.646 DEBUG [main] FileStore.java:247                TAR files: TarFiles{readers=[],writer=target\junit2834122541179880349\junit3041268421527563090\data00000a.tar}
22:41:13.646 DEBUG [main] TarWriter.java:185                Writing segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to target\junit2834122541179880349\junit3041268421527563090\data00000a.tar
22:41:13.646 INFO  [main] FileStoreBuilder.java:340         Creating file store FileStoreBuilder{version=1.8-SNAPSHOT, directory=target\junit2834122541179880349\junit4470899745425503556, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore], maxFileSize=1, segmentCacheSize=0, stringCacheSize=0, templateCacheSize=0, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1, memoryMapping=false, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}}
22:41:13.646 INFO  [main] FileStore.java:241                TarMK opened at target\junit2834122541179880349\junit4470899745425503556, mmap=false, size=0 B (0 bytes)
22:41:13.646 DEBUG [main] FileStore.java:247                TAR files: TarFiles{readers=[],writer=target\junit2834122541179880349\junit4470899745425503556\data00000a.tar}
22:41:13.646 DEBUG [main] TarWriter.java:185                Writing segment 8d19c7dc-8b48-4e10-a58d-31c15c93f2fe to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.646 INFO  [main] DataStoreTestBase.java:127        Test begin: testSyncFailingDueToTooShortTimeout
22:41:13.646 INFO  [main] SegmentNodeStore.java:120         Creating segment node store SegmentNodeStoreBuilder{blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore]}
22:41:13.646 INFO  [main] LockBasedScheduler.java:155       Initializing SegmentNodeStore with the commitFairLock option enabled.
22:41:13.708 DEBUG [main] StandbyServer.java:248            Binding was successful
22:41:13.708 DEBUG [main] TarWriter.java:185                Writing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to target\junit2834122541179880349\junit3041268421527563090\data00000a.tar
22:41:13.739 DEBUG [main] TarRevisions.java:240             TarMK journal update null -> 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c
22:41:13.755 DEBUG [standby-1] GetHeadRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for current head
22:41:13.755 DEBUG [primary-1] ClientFilterHandler.java:53  Client /127.0.0.1:65480 is allowed
22:41:13.755 DEBUG [primary-1] RequestDecoder.java:42       Parsed 'get head' message
22:41:13.755 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get head' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [primary-1] GetHeadRequestHandler.java:43 Reading head for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [primary-1] GetHeadResponseEncoder.java:36 Sending head 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [standby-1] ResponseDecoder.java:82      Decoding 'get head' response
22:41:13.755 DEBUG [standby-run-23] StandbyClientSyncExecution.java:103 Found missing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [standby-run-23] StandbyClientSyncExecution.java:124 Inspecting segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [standby-1] GetReferencesRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [primary-1] RequestDecoder.java:48       Parsed 'get references' message
22:41:13.771 DEBUG [primary-1] GetReferencesRequestHandler.java:39 Reading references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetReferencesResponseEncoder.java:34 Sending references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:94      Decoding 'get references' response
22:41:13.771 DEBUG [standby-run-23] StandbyClientSyncExecution.java:184 Found reference from 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [standby-run-23] StandbyClientSyncExecution.java:124 Inspecting segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [standby-1] GetReferencesRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:48       Parsed 'get references' message
22:41:13.771 DEBUG [primary-1] GetReferencesRequestHandler.java:39 Reading references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetReferencesResponseEncoder.java:34 Sending references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:94      Decoding 'get references' response
22:41:13.771 INFO  [standby-run-23] StandbyClientSyncExecution.java:196 Copying data segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a from primary
22:41:13.771 DEBUG [standby-1] GetSegmentRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:45       Parsed 'get segment' message
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get segment' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentRequestHandler.java:39 Reading segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:125 Segment with size 192 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentResponseEncoder.java:43 Sending segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:86      Decoding 'get segment' response
22:41:13.771 DEBUG [standby-run-23] TarWriter.java:185      Writing segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.771 INFO  [standby-run-23] StandbyClientSyncExecution.java:196 Copying data segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 from primary
22:41:13.771 DEBUG [standby-1] GetSegmentRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:45       Parsed 'get segment' message
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get segment' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentRequestHandler.java:39 Reading segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:125 Segment with size 448 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentResponseEncoder.java:43 Sending segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:86      Decoding 'get segment' response
22:41:13.771 DEBUG [standby-run-23] TarWriter.java:185      Writing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.771 DEBUG [standby-1] GetBlobRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:39       Parsed 'get blob' request
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get blob id' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetBlobRequestHandler.java:41 Reading blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:130 Binary with size 5242880 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetBlobResponseEncoder.java:41 Sending blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.786 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 1/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 2/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 1/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.802 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 3/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 2/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.818 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 4/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 3/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.818 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 5/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 4/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 5/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:167     Received entire blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.880 DEBUG [standby-run-23] ResponseDecoder.java:66 Processing input stream finished! Deleting file C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp
22:41:13.896 DEBUG [standby-run-23] TarRevisions.java:240   TarMK journal update null -> 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c
22:41:13.911 WARN  [standby-1] ExceptionHandler.java:37     Exception caught on client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
io.netty.handler.timeout.ReadTimeoutException: null
22:41:13.911 INFO  [standby-run-23] StandbyClientSyncExecution.java:82 updated head state successfully: true in 156ms.
22:41:13.911 DEBUG [standby-run-23] StandbyClient.java:157  Channel closed
22:41:16.137 DEBUG [main] StandbyClientSync.java:277        Group shut down
22:41:16.137 DEBUG [main] StandbyServer.java:219            Channel disconnected
22:41:16.137 DEBUG [main] StandbyServer.java:219            Channel disconnected
22:41:16.137 DEBUG [main] StandbyServer.java:230            Boss group shut down
22:41:16.137 DEBUG [main] StandbyServer.java:236            Worker group shut down
22:41:16.137 INFO  [main] DataStoreTestBase.java:132        Test end: testSyncFailingDueToTooShortTimeout
22:41:16.137 DEBUG [main] Scheduler.java:134                The scheduler FileStore background tasks was successfully shut down
22:41:16.137 DEBUG [main] TarRevisions.java:236             Head state did not change, skipping flush
22:41:16.184 INFO  [main] FileStore.java:480                TarMK closed: target\junit2834122541179880349\junit4470899745425503556
22:41:16.184 DEBUG [main] Scheduler.java:134                The scheduler FileStore background tasks was successfully shut down
22:41:16.184 DEBUG [main] TarRevisions.java:236             Head state did not change, skipping flush
22:41:16.199 INFO  [main] FileStore.java:480                TarMK closed: target\junit2834122541179880349\junit3041268421527563090
{noformat}
",test-failure,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-12-05 08:52:10,12
13121939,Estimation for FULL can be off sometimes,"Since OAK-6883, FULL estimation compares segmentstore size with the previous FULL. There can be cases where the current segmentstore is smaller than the previous FULL (i.e. due to TAIL cleaning up more). This leads to FULL being skipped for much more than anticipated.

A case to illustrate this scenario:

    Start Oak with a 10 GB repo
    GC #1: run FULL results in segmenstore of 20GB
    GC #2: run TAIL results in segmentstore of 11GB
    GC #3: run FULL (saturday) - skipped because the reference is 20GB from the previous FULL

FULL be executed again only when the segmentstore grows back above 20GB, which might be too late.

Estimation should take this situation into account this and take a better decision.",compaction gc,['segment-tar'],OAK,Bug,Major,2017-11-30 15:23:28,15
13120892,High read IO in compaction retry cycles,"We have seen unusual high read IO in compaction retry cycles in our longevity tests at Adobe.

!cpu.png|width=1000!

Above picture shows the CPU utilisation during an online compaction run, which starts at 03:00. At about 03:22 CPU user time drops and CPU waiting for IO time increases. This point in time coincides with the start of the first retry cycle of compaction. 
",performance,['segment-tar'],OAK,Bug,Major,2017-11-27 11:00:15,15
13120406,test failure seen in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT,"{noformat}
INFO] Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file C:\projects\apache\oak\trunk\oak-segment-tar\target\failsafe-reports\2017-11-23T08-48-55_999-jvmRun1.dumpstream
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 17.984 s <<< FAILURE! - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
[ERROR] offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 5.024 s  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:143)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:108)

{noformat}
",test-failure windows,['segment-tar'],OAK,Bug,Critical,2017-11-23 08:10:31,12
13119685,Document tail compaction,"We need to add documentation of tail compaction:
* What is it, how does it work?
* How is it configured and scheduled?
* How can it be monitored, what are the related log entries?
* What are its limitations?
* What if it fails?",documentation,"['doc', 'segment-tar']",OAK,Documentation,Major,2017-11-20 15:17:59,15
13117925,Enable the -Dcache of offline compaction,The {{-Dcache}} option currently has no effect when used in conjunction with the {{compact}} run mode of {{oak-run}}. However we should enable users to configure the segment cache size through this option if necessary. ,compaction gc tooling,['segment-tar'],OAK,Improvement,Major,2017-11-13 12:49:01,15
13116641,Cold standby performance regression due to segment caching,"The changes to the segment cache introduced in r1793527 [0] introduced a performance regression on the primary for the case in which a standby is attached to it. Below a benchmark duration comparison between primary w/o and w/ standby for r1793527 (after the segment cache changes) and r1793526 (before the changes) :

|Oak 1.6 r1793527 (20170502)|{noformat}
# BasicWriteTest                   C     min     10%     50%     90%     max       N
Oak-Segment-Tar                    1      19      21      22      26     160    2491
Oak-Segment-Tar-DS                 1      56      59      63      70     181     919
Oak-Segment-Tar-Cold(Shared DS)    1      58      66     159     177     372     302
{noformat}|
|Oak 1.6 r1793526 (20170502)|{noformat}
# BasicWriteTest                   C     min     10%     50%     90%     max       N
Oak-Segment-Tar                    1      19      21      22      25      52    2584
Oak-Segment-Tar-DS                 1      56      60      63      69     158     925
Oak-Segment-Tar-Cold(Shared DS)    1      57      60      64      70     122     915
{noformat}|

[0] https://github.com/apache/jackrabbit-oak/commit/efafa4e1710621b7f3b8e92d0b2681669185fcd4",cold-standby performance scalability,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-11-07 11:47:37,1
13116579,Provide a way to tune inline size while storing binaries,"SegmentNodeStore currently inlines binaries of size less that 16KB (Segment.MEDIUM_LIMIT) even if external BlobStore is configured. 

Due to this behaviour quite a bit of segment tar storage consist of blob data. In one setup out of 370 GB segmentstore size 290GB is due to inlined binary. If most of this binary content is moved to BlobStore then it would allow same repository to work better in lesser RAM

So it would be useful if some way is provided to disable this default behaviour and let BlobStore take control of inline size i.e. in presence of BlobStore no inlining is attempted by SegmentWriter.",performance scalability,['segment-tar'],OAK,Improvement,Major,2017-11-07 04:55:40,1
13116445,Offline compaction should not use mmap on Windows,The offline compaction tool should do an effort to detect whether it is being run on windows and disable memory mapping if so. Rational: with memory mapping enabled it might fail to remove the old tar files (see OAK-4274 and [JDK-4724038|http://bugs.java.com/view_bug.do?bug_id=4724038]).,compaction gc tooling,['segment-tar'],OAK,Improvement,Major,2017-11-06 16:38:38,12
13116443,FileStore.compact does not persist compacted head to journal,"When {{FileStore.compact()}} returns the {{journal.log}} does not necessarily contain the head created by the compactor. This can lead to problems downstream like e.g. in OAK-6894 where the compactor tool wrote the wrong (i.e. uncompacted) head to the {{journal.log}}. 

Proposed fix is to call on of the {{FileStore.flush()}} methods after compaction and add a test case that verifies the {{journal.log}} contains the correct head state. ",compaction gc,['segment-tar'],OAK,Bug,Major,2017-11-06 16:32:51,15
13115677,Unknown channel option 'TCP_NODELAY' for channel warning in cold standby,"After the netty upgrade in OAK-6564, there's a recurring warning appearing in the server thread:
{noformat}
18:54:44.691 [main] WARN  io.netty.bootstrap.ServerBootstrap - Unknown channel option 'TCP_NODELAY' for channel '[id: 0xa64bc5c4]'
{noformat}

We need to see what's causing it (i.e. was that option removed in the latest version? if yes, is there a substitute/change needed?).

/cc [~frm]",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Minor,2017-11-02 16:58:12,12
13115540,Log SegmentStore size at startup,"It would be useful if we can log the segmentstore size at time of startup. FileStore already computes the size to initialize the FileStoreStats so we just need to log it

This size often help when customer report issues and provide log files",production,['segment-tar'],OAK,Improvement,Minor,2017-11-02 04:32:10,12
13115364,org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments failing,"{noformat}
[ERROR] offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 7.446 s  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:141)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:107)
{noformat}",compaction gc,['segment-tar'],OAK,Bug,Blocker,2017-11-01 14:04:08,3
13113327,Executions of background threads might pile up,"The background threads used in {{FileStore}} are implemented by wrapping {{Runnable}} instances in {{SafeRunnable}}, and by handing the {{SafeRunnable}} instances over to a {{ScheduledExecutorService}}. 

The documentation of {{ScheduledExecutorService#scheduleAtFixedRate}} states that ""if any execution of a task takes longer than its period, then subsequent executions may start late, but will not concurrently execute"". This means that if an execution is delayed, the piled up executions might fire in rapid succession.

This way of running the periodic background threads might not be ideal. For example, it doesn't make much sense to flush the File Store five times in a row. On the other hand, if the background tasks are coded with this caveat in mind, this issue might not be a problem at all. For example, flushing the File Store five times in a row might not be a problem if many of those executions don't do much and return quickly.

Tasks piling up might be a problem when it comes to release the resource associated with the {{FileStore}} in a responsive way. Since the {{ScheduledExecutorService}} is gracefully shut down, it might take some time before all the scheduled background tasks are processed and the {{ScheduledExecutorService}} is ready to be terminated.",production,['segment-tar'],OAK,Bug,Major,2017-10-31 15:28:15,15
13113323,Background threads might not be automatically restarted,"The background threads used in {{FileStore}} are implemented by wrapping {{Runnable}} instances in {{SafeRunnable}}, and by handing the {{SafeRunnable}} instances over to a {{ScheduledExecutorService}}. 

The documentation of {{ScheduledExecutorService#scheduleAtFixedRate}} states that ""if any execution of the task encounters an exception, subsequent executions are suppressed"". But a {{SafeRunnable}} always re-throws any {{Throwable}} that it catches, effectively preventing itself from executing again in the future.

There is more than one solution to this problem. One of these is to never re-throw any exception. Even if it doesn't always make sense, e.g. in case of an {{OutOfMemoryError}}, never re-throwing an exception would better fulfil the assumption that background threads should always be up and running even in case of error.",resilience,['segment-tar'],OAK,Bug,Critical,2017-10-31 15:13:59,12
13113277,OffRC always logs 0 for the number of compacted nodes in gc.log,"After an offline compaction the {{gc.log}} always contains 0 for the number of compacted nodes. This is caused by {{org.apache.jackrabbit.oak.segment.tool.Compact.compact()}} instantiating a new {{FileStore}} to run cleanup. That file store has new {{GCMonitor}} instance, which did no see any of the nodes written by the compaction that was run on the previous {{FileStore}} instance. 

",compaction gc tooling,['segment-tar'],OAK,Bug,Major,2017-10-31 10:57:49,12
13113047,TarMK disk space check is not synchronized with FileStore opened state,"It seems that the disk space check is not properly synchronized with {{FileStore}} as I revealed a race condition while using oak-upgrade during migration to {{segment-tar}}.

The {{FileStore}} instance is closed while TarMK disk check tries to execute and it seems it is dependent on the state of segment ({{org.apache.jackrabbit.oak.segment.file.FileStore.checkDiskSpace(FileStore.java:541)}} that needs to be opened. 

{noformat}
30.10.2017 11:26:05.834 WARN   o.a.j.o.s.f.Scheduler: The scheduler FileStore background tasks takes too long to shut down
30.10.2017 11:26:11.674 INFO   o.a.j.o.s.f.FileStore: TarMK closed: /data/cq/crx-quickstart/repository-segment-tar-20171030-112401/segmentstore
30.10.2017 11:26:11.676 ERROR  o.a.j.o.s.f.SafeRunnable: Uncaught exception in TarMK disk space check [/data/cq/crx-quickstart/repository-segment-tar-20171030-112401/segmentstore]
java.lang.IllegalStateException: already shut down
    at org.apache.jackrabbit.oak.segment.file.ShutDown.keepAlive(ShutDown.java:42)
    at org.apache.jackrabbit.oak.segment.file.FileStore.size(FileStore.java:302)
    at org.apache.jackrabbit.oak.segment.file.FileStore.checkDiskSpace(FileStore.java:541)
    at org.apache.jackrabbit.oak.segment.file.FileStore.access$300(FileStore.java:102)
    at org.apache.jackrabbit.oak.segment.file.FileStore$3.run(FileStore.java:237)
    at org.apache.jackrabbit.oak.segment.file.SafeRunnable.run(SafeRunnable.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{noformat}",concurrency production,"['segment-tar', 'upgrade']",OAK,Bug,Minor,2017-10-30 14:50:45,15
13113043,The compaction estimator should take the compaction type (tail vs. full) into consideration,"Currently the compaction estimator unconditionally looks at the growth of the repository since the last compaction run. This turn out to be not optimal when interleaving tail and full compaction. It would be better to have the estimator look at the growth of the repository since last full compaction when running full compaction. 

cc [~frm]",compaction gc,['segment-tar'],OAK,Improvement,Critical,2017-10-30 14:46:56,12
13112267,Segment-Tar-Cold fixture doesn't correctly set up standby blob store,"When {{--shareDataStore}} option is used for {{Segment-Tar-Cold}}, the standby instance ends up without a blob store configured.",cold-standby,"['benchmarks', 'segment-tar', 'tarmk-standby']",OAK,Bug,Minor,2017-10-26 12:27:31,1
13110251,Log correct path while initializing the DataStore,DataStore initialization logs the {{repository.home}} path ignoring the one specified with a {{path}} property which takes precedence which leads to confusion.,candidate_oak_1_4 candidate_oak_1_6,['blob-plugins'],OAK,Bug,Minor,2017-10-18 09:53:51,16
13110217,Refactor FileStore.close,{{FileStore.close}} should take better advantage of the {{Closer}} instance to close its resources (including the file store lock). Also the order of the close calls should be aligned with their dependencies. ,refactoring technical_debt,['segment-tar'],OAK,Improvement,Major,2017-10-18 08:10:43,15
13108896,Implement support for disabling indexes which are replaced with newer index,"For upgrade case in many applications older index type is set to {{disabled}} when new index is provisioned. If the new index is async then it would take some time for reindex and till then any query which used to make use of old index would end up traversing the repository

To avoid such a scenario we should only mark older index as ""disabled"" only if the newer index is reindex. ",docs-impacting,['indexing'],OAK,New Feature,Major,2017-10-12 11:59:08,19
13107951,Provide a way to for persistent cache to determine which all nodes can be cached,"Currently persistent cache if enabled for nodes caches all nodes accessed on the system. It would be better if it can be configured to only cache those nodes which are not volatile so that caching can be effective

Purpose of this issue is to
* Provide an extension point in PersistentCache logic to check if a node is to be cached
* Provide an impl which relies on some static OSGi config to determine that

Later we can make this impl dynamic i.e. rely on access pattern to cache imp stuff",doc-impacting,['documentmk'],OAK,Improvement,Minor,2017-10-09 11:18:10,19
13107238,Exceptions are inhibited in oak-run compact,"Exceptions thrown by {{oak-run compact}} are inhibited so the exit code of the command is not correct in case of error. 

Example: 
{code}
$ java -jar oak-run-1.7.8-R1809845.jar compact test-oak-run/
Apache Jackrabbit Oak 1.7.8-R1809845
Compacting test-oak-run-6.3.0
With default access mode
    before
        Thu Oct 05 15:14:22 CEST 2017, journal.log
        Thu Oct 05 15:14:23 CEST 2017, data00000a.tar
        Thu Oct 05 15:14:23 CEST 2017, manifest
        Thu Oct 05 15:14:23 CEST 2017, repo.lock
    size 119.1 MB (119133142 bytes)
    -> compacting
org.apache.jackrabbit.oak.segment.file.InvalidFileStoreVersionException: Using a too recent version of oak-segment-tar
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkStoreVersion(ManifestChecker.java:81)
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkManifest(ManifestChecker.java:70)
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkAndUpdateManifest(ManifestChecker.java:51)
	at org.apache.jackrabbit.oak.segment.file.FileStore.<init>(FileStore.java:191)
	at org.apache.jackrabbit.oak.segment.file.FileStoreBuilder.build(FileStoreBuilder.java:343)
	at org.apache.jackrabbit.oak.segment.tool.Compact.newFileStore(Compact.java:165)
	at org.apache.jackrabbit.oak.segment.tool.Compact.compact(Compact.java:135)
	at org.apache.jackrabbit.oak.segment.tool.Compact.run(Compact.java:128)
	at org.apache.jackrabbit.oak.run.SegmentTarUtils.compact(SegmentTarUtils.java:183)
	at org.apache.jackrabbit.oak.run.CompactCommand.execute(CompactCommand.java:93)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
    after
        Thu Oct 05 15:14:22 CEST 2017, journal.log
        Thu Oct 05 15:14:23 CEST 2017, data00000a.tar
        Thu Oct 05 15:14:23 CEST 2017, manifest
        Thu Oct 05 15:14:23 CEST 2017, repo.lock
    size 119.1 MB (119133142 bytes)
    removed files []
    added files []
Compaction succeeded in 211.9 ms (0s).
{code}

A quick fix would be to wrap the exception into a {{RuntimeException}}:
{code}
--- a/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/tool/Compact.java
+++ b/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/tool/Compact.java
@@ -127,7 +127,7 @@ public class Compact implements Runnable {
         try {
             compact();
         } catch (Exception e) {
-            e.printStackTrace();
+            throw new RuntimeException(""Failed to run compact"", e);
         }
     }
{code}",compaction gc tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-10-05 14:16:27,12
13105674,Standby server should send timely responses to all client requests,"Currently all the {{GetXXXRequestHandler}} (where XXX stands for Blob, Head, References and Segment), on the server discard client requests which cannot be satisfied (i.e. the requested object does not exist (yet) on the server). A more transparent approach would be to timely respond to all client requests, clearly stating that the object was not found. This would improve a lot debugging for example, because all requests and their responses could be easily followed from the client log, without needing to know what actually happened on the server.

Below, a possible implementation for {{GetHeadRequestHandler}}, suggested by [~frm] in a comment on OAK-6678:

{noformat}
String id = reader.readHeadRecordId();

if (id == null) {
    ctx.writeAndFlush(new NotFoundGetHeadResponse(msg.getClientId(), id));
    return;
}

ctx.writeAndFlush(new GetHeadResponse(msg.getClientId(), id));
{noformat}
",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Minor,2017-09-28 06:55:40,1
13105438,Lucene Index: improved cost estimation by using document count per field,"The cost estimation of the Lucene index is somewhat inaccurate because (by default) it just used the number of documents in the index (as of Oak 1.7.4 by default, due to OAK-6333).

Instead, it should use the number of documents for the given fields (the minimum, if there are multiple fields with restrictions). 

Plus divided by the number of restrictions (as we do now already).",doc-impacting,"['lucene', 'query']",OAK,Improvement,Major,2017-09-27 13:40:19,11
13104491,CommitMitigated merge policy should not reduce performance significantly,"While running performance tests (internally) using latest unstable releases having {{CommitMitigatedTieredMergePolicy}} we observed significant drop in performance (OAK-6704).
The policy, although bad for performance, showed dramatic drop in churn created in data store size. So, clearly, OAK-5192 did what it intended to do.

Opening this issue to factor in drop in performance and then set the default back to {{CommitMitigatedTieredMergePolicy}}.",performance scalability,['lucene'],OAK,Improvement,Major,2017-09-23 09:21:16,10
13104303,TarWriter.close() must not throw an exception on subsequent invocations,"Invoking TarWriter.close() on an already closed writer throws an {{ISE}}. According to the general contract this is not allowed:

{code}
* Closes this stream and releases any system resources associated
* with it. If the stream is already closed then invoking this
* method has no effect.
{code}

We should adjust the behvaviour of that method accordingly. 

Failing to comply with that general contract causes {{TarWriter}} instances to fail in try-resource statements when multiple wrapped streams are involved.

Consider 

{code}
try (
    StringWriter string = new StringWriter();
    PrintWriter writer = new PrintWriter(string);
    WriterOutputStream out = new WriterOutputStream(writer, Charsets.UTF_8))
{
    dumpHeader(out);
    writer.println(""----------------------------------------"");
    dumpHex(out);
    writer.println(""----------------------------------------"");
    return string.toString();
}
{code}

This code would cause exceptions to be thrown if e.g. the {{PrintWriter.close}} method would not be idempotent. ",technical_debt,['segment-tar'],OAK,Bug,Minor,2017-09-22 14:34:25,15
13104227,Improve cold standby resiliency to incoherent configs,"In order to correctly configure cold standby there are two OSGi configurations that need to be provided. Among other settings, {{org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.config}} needs {{standby=B""true""}} and {{org.apache.jackrabbit.oak.segment.standby.store.StandbyStoreService.config}} needs {{mode=""standby""}}. The problem is that sometimes we have {{mode=""standby""}} in {{StandbyStoreService}} and {{standby=B""false""}} in {{SegmentNodeStoreService}} which leads to starting a problematic standby instance (with primary behaviour enabled, e.g. indexing, etc.). This problem stems from the fact that there are two components whose configuration should be coordinated. Proposals to mitigate this:

# Keep the {{mode=""standby""}}, but merge the configuration of {{StandbyStoreService}} in the one for {{SegmentNodeStoreService}} and eliminate {{StandbyStoreService}} altogether
# {{StandbyStoreService}} should derive {{mode=""standby""}} from {{""standby=B""true""}} in {{SegmentNodeStoreService}}
# {{SegmentNodeStoreService}} should derive {{""standby=B""true""}} from {{mode=""standby""}} in {{StandbyStoreService}} even if this is backwards when compared to how the synchronization currently happens, with {{StandbyStoreService}} waiting for for a proper initialisation of {{SegmentNodeStoreService}}
# Make {{StandbyStoreService}} configuration mandatory, but require a {{mode=""off""}} setting. This way the removal of {{standby=B""true""}} from {{SegmentNodeStoreService}} would be guaranteed and any synchronization between the two components would be avoided.

/cc  [~frm], [~volteanu], [~mduerig]
",cold-standby resilience,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Major,2017-09-22 09:11:35,1
13103614,ReadOnly connection to fresh SegmentNodeStore setup failing,"Started a server with Oak version 1.7.7 and tried to connect oak-run-1.7.7 to same setup. This resulted in following exception

{noformat}
2017-09-20 19:47:22,213 INFO  [main] o.a.j.o.segment.file.FileStore - Creating file store FileStoreBuilder{version=1.7.7, directory=/path/to/repository/segmentstore, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.oak.plugins.blob.datastore.OakFileDataStore], maxFileSize=256, segmentCacheSize=256, stringCacheSize=256, templateCacheSize=64, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1048576, memoryMapping=true, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}} 
2017-09-20 19:47:22,243 WARN  [main] o.a.j.o.s.file.tar.TarReader - Unable to load index of file data00000a.tar: Unrecognized magic number 
2017-09-20 19:47:22,243 INFO  [main] o.a.j.o.s.file.tar.TarReader - No index found in tar file data00000a.tar, skipping... 
2017-09-20 19:47:22,243 WARN  [main] o.a.j.o.s.file.tar.TarReader - Could not find a valid tar index in /path/to/repository/segmentstore/data00000a.tar, recovering read-only 
2017-09-20 19:47:22,243 INFO  [main] o.a.j.o.s.file.tar.TarReader - Recovering segments from tar file /path/to/repository/segmentstore/data00000a.tar 
2017-09-20 19:47:22,315 INFO  [main] o.a.j.o.s.file.tar.TarReader - Regenerating tar file/path/to/repository/segmentstore/data00000a.tar.ro.bak 
2017-09-20 19:47:22,460 ERROR [main] o.a.j.oak.index.IndexCommand - Error occurred while performing index tasks 
java.lang.IllegalArgumentException: invalid segment buffer
	at org.apache.jackrabbit.oak.segment.data.SegmentDataLoader.newSegmentData(SegmentDataLoader.java:37) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.data.SegmentData.newSegmentData(SegmentData.java:66) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.writeSegment(AbstractFileStore.java:212) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.access$000(AbstractFileStore.java:66) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore$1.recoverEntry(AbstractFileStore.java:125) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarReader.generateTarFile(TarReader.java:213) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarReader.openRO(TarReader.java:162) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles.<init>(TarFiles.java:298) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles.<init>(TarFiles.java:58) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles$Builder.build(TarFiles.java:167) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.ReadOnlyFileStore.<init>(ReadOnlyFileStore.java:74) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.FileStoreBuilder.buildReadOnly(FileStoreBuilder.java:383) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.SegmentTarFixtureProvider.configureSegment(SegmentTarFixtureProvider.java:63) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.NodeStoreFixtureProvider.create(NodeStoreFixtureProvider.java:71) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.NodeStoreFixtureProvider.create(NodeStoreFixtureProvider.java:47) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.index.IndexCommand.execute(IndexCommand.java:98) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49) [oak-run-1.7.7.jar:1.7.7]

{noformat}

Post restart of server oak-run was able to connect fine. So looks like issue with very fresh setup only

Command used for oak-run
{noformat}
java -jar oak-run-1.7.7.jar index --fds-path=/path/to/repository/datastore --checkpoint head --reindex --index-paths=/oak:index/lucene /path/to/repository/segmentstore --metrics
{noformat}",tooling,['segment-tar'],OAK,Bug,Minor,2017-09-20 14:20:00,15
13102961,Test failure: DocumentNodeStoreTest.disabledBranchesWithBackgroundWrite,"{{DocumentNodeStoreTest.disabledBranchesWithBackgroundWrite}} fails when I try a clean build on Windows, as per r1808698.

{noformat}
[ERROR] Failures:
[ERROR]   DocumentNodeStoreTest.disabledBranchesWithBackgroundWrite:3199 expected:<1> but was:<0>
{noformat}",test-failure,['documentmk'],OAK,Bug,Major,2017-09-18 14:00:09,2
13102805,Syncing big blobs fails since StandbyServer sends persisted head,"With changes for OAK-6653 in place, {{ExternalPrivateStoreIT#testSyncBigBlog}} and sometimes {{ExternalSharedStoreIT#testSyncBigBlob}} are failing on CI:

{noformat}
org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT)  Time elapsed: 96.82 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
...
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT)  Time elapsed: 95.254 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

Partial stacktrace:
{noformat}
14:09:08.355 DEBUG [main] StandbyServer.java:242            Binding was successful
14:09:08.358 DEBUG [standby-1] GetHeadRequestEncoder.java:33 Sending request from client Bar for current head
14:09:08.359 DEBUG [primary-1] ClientFilterHandler.java:53  Client /127.0.0.1:52988 is allowed
14:09:08.360 DEBUG [primary-1] RequestDecoder.java:42       Parsed 'get head' message
14:09:08.360 DEBUG [primary-1] CommunicationObserver.java:79 Message 'get head' received from client Bar
14:09:08.362 DEBUG [primary-1] GetHeadRequestHandler.java:43 Reading head for client Bar
14:09:08.363 WARN  [primary-1] ExceptionHandler.java:31     Exception caught on the server
java.lang.NullPointerException: null
	at org.apache.jackrabbit.oak.segment.standby.server.DefaultStandbyHeadReader.readHeadRecordId(DefaultStandbyHeadReader.java:32) ~[oak-segment-tar-1.8-SNAPSHOT.jar:1.8-SNAPSHOT]
	at org.apache.jackrabbit.oak.segment.standby.server.GetHeadRequestHandler.channelRead0(GetHeadRequestHandler.java:45) ~[oak-segment-tar-1.8-SNAPSHOT.jar:1.8-SNAPSHOT]
{noformat}",cold-standby resilience,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-09-17 05:53:00,1
13102497,Create a more complex IT for cold standby,"At the moment all integration tests for cold standby are using the same scenario in their tests: some content is created on the server (including binaries), a standby sync cycle is started and then the content is checked on the client. The only twist here is using/not using a data store for storing binaries.

Although good, this model could be extended to cover many more cases. For example, {{StandbyDiff}} covers the following 6 cases node/property added/changed/deleted. From these, with the scenario described, the removal part is never tested (and the change part is covered in only one test). 

It would be nice to have an IT which would add content on the server, do a sync, remove some of the content, do a sync and then call OnRC. This way all cases will be covered, including if cleanup works as expected on the client.

/cc [~frm]",cold-standby technical_debt test,"['segment-tar', 'tarmk-standby']",OAK,Task,Major,2017-09-15 08:10:43,1
13102494,Improve cold standby logging,"With the new feature which allows chunking in blob transfer between server and client, there are various places in which more meaningful log messages could be used. For example, on the server, there's a tally of the no. of chunks sent/total no. of chunks, but this part is missing on the client, i.e. no. of chunks received/total no. of chunks. 

Another case which would benefit from improved logging is when a big blob can't be sent fully from the server to the client in {{readTimeoutMs}}. The current exception message is a bit scarce in details (e.g. {{""Unable to load remote blob "" + blobId + "" at "" + path + ""#"" + pName}}). This could also mean that the remote blob doesn't exist on the server in the first place. A better option would be to advise about increasing {{readTimeoutMs}}.

Finally, the same log level should be used everywhere, since currently {{DEBUG}} and {{INFO}} are  interchangeably mixed.

/cc [~frm]",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Task,Minor,2017-09-15 08:00:45,1
13102284,Refactor StandbyDiff for better clarity and understandability,"{{StandbyDiff}} still makes use of the {{logOnly}} property for deciding when to act upon node/property changes. The official documentation of {{logOnly}} states that it helps for

{quote}
/**
     * read-only traversal of the diff that has 2 properties: one is to log all
     * the content changes, second is to drill down to properly level, so that
     * missing binaries can be sync'ed if needed
     */
{quote}

but it's use is a bit misleading. The first call to {{StandbyDiff}} is always with {{logOnly==false}}, while subsequent calls are done with {{logOnly==true}}. Implementing {{StandbyDiff}} without this mechanism would result in better clarity and maintainability.

Another minor improvement is to rename {{#binaryCheck}} methods and {{#readBinary}} to {{#fetchBinary}} and {{#fetchAndStoreBlob}} which is more appropriate to their purpose.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Minor,2017-09-14 15:14:12,1
13102249,Move DocumentNodeStore into its own bundle,"Move the DocumentNodeStore implementation and the two backends (MongoDB, RDB) into its own bundle. This will make it possible to release oak-core and the NodeStore implementation independently.",modularization technical_debt,"['core', 'documentmk']",OAK,Task,Major,2017-09-14 13:06:16,2
13101964,ResponseDecoder should check that the length of the received blob matches the length of the sent blob,"As already explained in OAK-6659, there can be cases in which deleting the previous spool file fails (Windows) and new (duplicate) content is added under the hood to the old file. This way the persisted blob doesn't match in content and id with the original sent by the server.

A first improvement here is to not allow the decoding to continue if the old spool file cannot be deleted. For this, the call to {{File#delete}} needs to be replaced with {{java.nio.file.Files#delete}} which would throw an exception if something wrong happens.

By ensuring that the spool file has the same size as the original blob we solve this problem. This check is sufficient, since all the chunks received are individually checked by hash, before appending them to the spool file. Moreover, the single threaded nature of the client ensures that races in which a new thread starts appending new content, after the length check has just passed can never happen.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Major,2017-09-13 13:40:25,1
13101940,Cold standby should fail loudly when a big blob can't be timely transferred,"Due to changes done in OAK-4969, currently there are two 'sync blob' cycles triggered by {{StandbyDiff#childNodeChanged}}. The test scenario is the same as the one in {{DataStoreTestBase#testSyncBigBlob}}: on the primary file store, a new big blob (1GB) is added and then a standby sync is triggered to sync this content to the secondary file store. 

The first 'sync blob' cycle happens as a result of {{#process}} being called in {{StandbyDiff#childNodeChanged}}. Therefore, a new 'get blob' request is created on the client and the server starts sending chunks from the big blob. Now, if the time needed for transferring the entire blob from server to client exceeds {{readTimeoutMs}} an {{IllegalStateException}} will be correctly thrown by {{StandbyDiff#readBlob}}, but will be swallowed by the {{StandbyDiff#childNodeChanged}} in its catch clause. A second 'sync blob' cycle will be triggered and, -this might succeed with the same {{readTimeoutMs}} for which it was failing before-, if {{readTimeoutMs * 2}} is enough, the blob will be synced on the standby. This happens because the server will continue sending the remaining chunks after {{IllegalStateException}} was thrown (first 'sync blob' cycle).

The consequence of these two 'sync blob' cycles is that sometimes, deleting the temporary file to which chunks are spooled to on the client fails (see Windows for example and OAK-6641 specifically). This way, instead of deleting the previous incomplete transfer, new chunks from the second 'sync blob' cycle are added. The blob persisted in the blob store on the client won't have the same size and id as the initial blob sent by the server.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Critical,2017-09-13 12:12:40,1
13101632,Standby server must always send the persisted head to clients,"Currently the standby server sends an un-persisted head record to clients. Under normal circumstances, the TarMK flush thread is able to persist it and its corresponding segment at a 5 seconds interval.
However, there are cases (uploading a very large blob > 10 GB) in which the flush thread writes the segment too late, and the 20s allowed by {{FileStoreUtil#readSegmentWithRetry}} are not enough. Therefore the server can't read the segment containing the head record and a timeout occurs on the client.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-09-12 12:46:53,1
13101295,test failure seen in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT,"{noformat}
Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@6bb75258
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 18.543 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 5.578 sec  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:143)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:109)


Results :

Failed tests:
  UpgradeIT.offRCUpgradesSegments:109->checkSegmentVersion:143 Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
{noformat}
",test-failure windows,['segment-tar'],OAK,Bug,Blocker,2017-09-11 15:03:38,12
13101189,Backport OAK-6110 to 1.6 (Offline compaction uses too much memory),Backport the fix from OAK-6110 to the 1.6 branch.,compaction gc memory perfomance,['segment-tar'],OAK,Task,Major,2017-09-11 08:00:52,15
13100831,test failure in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT,"{noformat}
Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 99.858 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT)  Time elapsed: 71.122 sec  <<< ERROR!
java.lang.RuntimeException: Error occurred while obtaining InputStream for blobId [8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4#1073741824]
Caused by: java.io.IOException: org.apache.jackrabbit.core.data.DataStoreException: Record 8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4 does not exist
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Record 8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4 does not exist

{noformat}

(might be specific to Windows)",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Blocker,2017-09-08 14:38:13,1
13100451,Confusing log entries when memory requirements are not met at start of OnRC,"When the memory requirements for running OnRC are not met before the estimation phase the estimator will run nevertheless. The process will only be cancelled at the beginning of the compaction phase. The entries in the log file reflect this:

{code}
TarMK GC #1: canceling compaction because available memory level 306.4 MB (306395472 bytes) is too low...
TarMK GC #1: estimation started
TarMK GC #1: estimation completed in 343.5 ms (343 ms). Estimation skipped because of missing gc journal data (expected on first run)
TarMK GC #1: running full compaction
TarMK GC #1: compaction started ...
TarMK GC #1: unable to estimate number of nodes for compaction, missing gc history.
TarMK GC #1: compaction cancelled: Not enough memory.
TarMK GC #1: cleaning up after failed compaction
{code}

However they can easily be (mis-)read as compaction being re-triggered after having been cancelled and then being cancelled again. ",compaction gc,['segment-tar'],OAK,Improvement,Major,2017-09-07 12:57:33,15
13100448,[upgrade] oak-upgrade should support azure blobstorage,oak-upgrade should support azuredatastore in addition to s3,azureblob,['upgrade'],OAK,Improvement,Major,2017-09-07 12:44:47,18
13100402,Remove unused datastore code relying on JR2 data store caching,Parent task for removing older code relying on JR2 caching.,technical_debt,"['blob', 'blob-cloud', 'blob-plugins']",OAK,Task,Major,2017-09-07 08:45:39,16
13100179,The backup command should not silently upgrade the FileStore,"The backup command in oak-run should not accidentally perform a transparent upgrade of the FileStore. Instead, it should use a strict version check to fail fast if the code is run on an outdated version of the FileStore.",production resilience tooling,['run'],OAK,Technical task,Major,2017-09-06 15:20:58,12
13100178,Replace standby blob chunk size configuration with feature flag,We should remove the {{StandbyStoreService#BLOB_CHUNK_SIZE}} OSGi configuration and replace it with a feature flag. Rational: we expect customer to rarely change this thus not justifying the additional configuration complexity and testing overhead. ,cold-standby configuration,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Major,2017-09-06 15:16:36,1
13100175,Avoid oak-run compact inadvertently upgrading the segment format ,"The transparent upgrade feature for segments from version 12 to 13 should not cause ""accidental upgrades"". That is, running Oak 1.8 oak-run compact on a store with segment version 12 should bail out unless a special option was specified on the command line. 

",gc production resilience,"['run', 'segment-tar']",OAK,Improvement,Major,2017-09-06 15:09:40,15
13099813,Add new segment-tar fixture for attaching a cold-standby to benchmarked primary,"If this fixture is chosen, a cold standby instance will be started, syncing with the primary every {{n}} seconds. All the benchmarks specified via {{[testcases]}} argument will be run on primary instance, and all statistics and reports will be linked to primary.

This could work similarly to {{Oak-Segment-Tar-DS}} and have dedicated options like {{--no-data-store}}, {{--private-data-store}} or {{--shared-data-store}}. ",cold-standby,"['benchmarks', 'segment-tar']",OAK,Improvement,Minor,2017-09-05 11:47:22,1
13099592,Provide list of all bundled nodes within a given DocumentNodeState,"For OAK-6353 we need to know all bundled nodestate in a given parent. For this purpose we should provide following method in DocumentNodeState

{code}
public Iterable<DocumentNodeState> getBundledNodesStates() {
{code}",bundling,['documentmk'],OAK,Improvement,Minor,2017-09-04 11:50:59,19
13098847,Move BulkTransferBenchmark to oak-benchmarks module,{{BulkTransferBenchmark}} should be moved from {{oak-segment-tar}} to {{oak-benchmarks}} to allow standard run of this cold standby related benchmark.,cold-standby,['segment-tar'],OAK,Task,Minor,2017-08-31 11:45:38,1
13098793,Improve resource management in BulkTransferBenchmark,"BulkTransferBenchmark might improperly dispose of test resources if error conditions occur. This is mostly due to improper resource tracking and finalization in BenchmarkBase, but similar mistakes have been made in BulkTransferBenchmark too.",cold-standby,['segment-tar'],OAK,Bug,Major,2017-08-31 07:52:15,12
13098752,SegmentWriteOperation.isOldGeneration() too eager,"The {{SegmentWriteOperation.isOldGeneration()}} predicate includes some segments that are not ""old"". This leads to more deferred compaction operations than strictly necessary. The affected segments are those generated by tail compaction. Tail compaction created segments should only be included in the predicate once they are from another full compaction operation. Otherwise referencing such segments is fine as they will not be reclaimed in a cleanup following a tail compaction.",compaction gc,['segment-tar'],OAK,Bug,Major,2017-08-31 07:00:26,15
13098388,rep:excerpt not working for content indexed by aggregation in lucene,"I mentioned that properties that got indexed due to an aggregation are not considered for excerpts (highlighting) as they are not indexed as stored fields.

See the attached patch that implements a test for excerpts in {{LuceneIndexAggregationTest2}}.

It creates the following structure:

{code}
/content/foo [test:Page]
 + bar (String)
 - jcr:content [test:PageContent]
  + bar (String)
{code}

where both strings (the _bar_ property at _foo_ and the _bar_ property at _jcr:content_) contain different text. 

Afterwards it queries for 2 terms (""tinc*"" and ""aliq*"") that either exist in _/content/foo/bar_ or _/content/foo/jcr:content/bar_ but not in both. For the former one the excerpt is properly provided for the later one it isn't.",excerpt,['lucene'],OAK,Bug,Major,2017-08-29 20:58:32,19
13097912,UpgradeIT produces unwanted output,"When {{UpgradeIT}} is executed, the following output is produced.

{noformat}
Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@75e01201
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@75e01201
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@75e01201
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.17 sec - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
{noformat}

The test should not produce any output, especially such a useless one.",test,['segment-tar'],OAK,Improvement,Minor,2017-08-28 08:48:54,15
13097268,Add tooling API,"h3. Current situation
Current segment store related tools are implemented ad-hoc by potentially relying on internal implementation details of Oak Segment Tar. This makes those tools less useful, portable, stable and potentially applicable than they should be.

h3. Goal
Provide a common and sufficiently stable Oak Tooling API for implementing segment store related tools. The API should be independent of Oak and not available for normal production use of Oak. Specifically it should not be possible to it to implement production features and production features must not rely on it. It must be possible to implement the Oak Tooling API in Oak 1.8 and it should be possible for Oak 1.6.

h3. Typical use cases
* Query the number of nodes / properties / values in a given path satisfying some criteria
* Aggregate a certain value on queries like the above
* Calculate size of the content / size on disk
* Analyse changes. E.g. how many binaries bigger than a certain threshold were added / removed between two given revisions. What is the sum of their sizes?
* Analyse locality: measure of locality of node states. Incident plots (See https://issues.apache.org/jira/browse/OAK-5655?focusedCommentId=15865973&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15865973).
* Analyse level of deduplication (e.g. of checkpoint) 

h3. Validation
Reimplement [Script Oak|https://github.com/mduerig/script-oak] on top of the tooling API. 

h3. API draft
* Whiteboard shot of the [API entities|https://wiki.apache.org/jackrabbit/Oakathon%20August%202017?action=AttachFile&do=view&target=IMG_20170822_163256.jpg] identified initially.
* Further [drafting of the API|https://github.com/mduerig/oak-tooling-api] takes place on Github for now. We'll move to the Apache SVN as soon as considered mature enough and have a consensus of where to best move it. ",tooling,['segment-tar'],OAK,New Feature,Major,2017-08-24 08:49:36,15
13096992,Lucene index: include/exclude key pattern list,"Similar to OAK-4637 but for lucene indexes

In some cases, property indexes contain many nodes, and updating them can be slow. Right now we have filters for node and mixin types, path (include and exclude). 

An include and exclude list of values (patterns) would be useful. For example the property ""status"", if we only ever run queries with the condition ""status = 'ACTIVE'"", then nodes with status INACTIVE and DONE don't need to be indexed.",docs-impacting,['lucene'],OAK,Improvement,Major,2017-08-23 11:04:18,19
13096362,Fix OSGi wiring after netty update to 4.1.x,"After netty update in OAK-6564, {{OSGiIT}} fails with the following exception:

{code}
Running org.apache.jackrabbit.oak.osgi.OSGiIT
ERROR: Bundle org.apache.jackrabbit.oak-segment-tar [36] Error starting file:/oak-it-osgi/target/test-bundles/oak-segment-tar.jar (org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.jackrabbit.oak-segment-tar [36]: Unable to resolve 36.0: missing requirement [36.0] osgi.wiring.package; (osgi.wiring.package=com.ning.compress))
org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.jackrabbit.oak-segment-tar [36]: Unable to resolve 36.0: missing requirement [36.0] osgi.wiring.package; (osgi.wiring.package=com.ning.compress)
	at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:3974)
	at org.apache.felix.framework.Felix.startBundle(Felix.java:2037)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)
	at java.lang.Thread.run(Thread.java:745)
{code}",cold-standby,['segment-tar'],OAK,Bug,Major,2017-08-21 08:07:14,12
13095635,GetBlobResponseEncoder should not write all chunks at once,"{{GetBlobResponseEncoder}} writes too fast all the chunks, leaving the channel in a not-writable state, after the first write. The problem is not visible at a first glance, especially when using small blobs for testing. Increasing the blobs size, as done for OAK-6538, revealed the problem. Not only this triggers hidden {{OutOfMemory}} errors on either server or client, but sometimes incomplete blobs are sent along, which are interpreted by the client as valid.

A more elegant solution, which also solves the memory consumption problem, would be to use {{ChunkedWriteHandler}} which employs complex logic on how and when to write the chunks. {{ChunkedWriteHandler}} must be used in conjunction with a custom {{ChunkedInput<ByteBuf>}} implementation to generate {{header}} + {{payload}} chunks from an {{InputStream}}, as done currently. This way the server will send more chunks only when the previous one was consumed by the client.

/cc [~frm]",cold-standby,['segment-tar'],OAK,Improvement,Major,2017-08-18 13:31:33,1
13095632,Update netty dependency to 4.1.x,"Taking into account the improvements listed at [0], and also the individual issues solved since our current netty version (4.0.41.Final) was released, I propose to bump up netty version to latest 4.1.14.Final.

/cc [~frm]

[0] http://netty.io/wiki/new-and-noteworthy-in-4.1.html ",cold-standby,['segment-tar'],OAK,Improvement,Minor,2017-08-18 13:16:06,1
13094761,gc.log should contain recordId of compacted root after offline compaction,Running offline gc currently adds an entry to the {{gc.log}} with a {{NULL}} record id. We should improve this and record the record id of the compacted root node state. ,compaction gc,['segment-tar'],OAK,Improvement,Major,2017-08-15 13:41:37,15
13094728,Implement ITs for rolling upgrade,We need basic IT coverage for the rolling upgrade scenario from Oak 1.6. to Oak 1.8. See OAK-6531.,IT migration test upgrade,['segment-tar'],OAK,Task,Major,2017-08-15 10:23:24,15
13094458,Progress indicator for compaction ,"We should implement a basic progress indicator for compaction displaying percent completed. This could be done by estimating the number of nodes from the entries in the {{gc.log}} and the timestamp of the last entry in the {{journal.log}}. Such a feature would explicitly *not* give an ETA as too many factors have an impact here (concurrent activity, IO bandwidth, quota, etc.).",compaction gc operation,['segment-tar'],OAK,New Feature,Major,2017-08-14 12:53:07,15
13094046,Move gcType to SegmentGCOptions,The {{gcType}} property should move from the {{FileStore}} class to the {{SegmentGCOptions}} along with all other GC related properties. ,gc,['segment-tar'],OAK,Improvement,Major,2017-08-11 08:28:48,15
13093446,Investigate cold standby memory consumption ,"In an investigation from some time ago, 4GB of heap were needed for transferring 1GB blob and 6GB for 2GB blob. This was in part due to using {{addTestContent}} [0] in the investigation, which allocates a huge {{byte[]}} on the heap. 

OAK-5902 introduced chunking for transferring blobs between primary and standby. This way, the memory needed for syncing a big blob should be around the chunk size used. Solving the way test data is created, it should be possible to transfer a big blob (e.g. 2.5 GB) with less memory.

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/test/java/org/apache/jackrabbit/oak/segment/standby/DataStoreTestBase.java#L96",cold-standby,['segment-tar'],OAK,Task,Minor,2017-08-09 12:03:56,1
13093176,Implement rolling upgrade from Oak 1.6,"The segment format changes introduced for tail compaction (OAK-3349) must not require an explicit migration step. Instead there should be a rolling migration during normal operation. 

Things to consider:
* Segments from Oak 1.6
* Changes in tar index formats induced by the segment format changes
* Changes in gc.log induced by the segment format changes
* Required changes in the repository manifest and its interpretation
",migration upgrade,['segment-tar'],OAK,New Feature,Major,2017-08-08 14:18:38,12
13092516,Slow queries: ability to track them via JMX,"Slow queries (that traverse many nodes, load many nodes in memory, and those that don't use an index) are currently logged. It would be better if there is a way to retrieve at least the _number_ of slow queries (for example from the last seconds / minutes / hours / days), if there were any. Plus maybe the queries themselves, and the type of problem / how severe the problem was.",documentation-update,['query'],OAK,Improvement,Critical,2017-08-04 16:23:08,4
13092410,Implement unit tests for OnlineCompactor,{{OnlineCompactor}} needs more unit test coverage going forward. ,compaction gc,['segment-tar'],OAK,Task,Major,2017-08-04 08:35:16,15
13092407,Improve tail compactions resilience when base state cannot be determined,"This is a follow up to OAK-3349: in tail compaction the {{FileStore.GarbageCollector.getBase()}} might fail to determine the base state to rebase onto. In this case we should fall back to full compaction and report (log, JMX) the problem and its exact cause. 

Failing to determine the base state might be caused by a missing or invalid {{gc.log}} file or a invalid or missing record id for the base state being recorded in {{gc.log}}. None of these cases should impact system stability. 

",compaction gc,['segment-tar'],OAK,Improvement,Major,2017-08-04 08:31:38,15
13092402,Properly handle tail compactions in deduplication caches,"This is a follow up to OAK-3349, which introduced tail compactions:

The deduplication caches currently only take the full generations into account and ignore the tail generations. Cache generations need to be a monotonically increasing, ordered sequence consisting of the full and tail part of the gc generation. See {{FileStoreBuilder.EvictingWriteCacheManager.evictOldGeneration}}. Optimally we find a way to decouple the segment generation from the cache generations as these are really separate concerns. ",compaction gc,['segment-tar'],OAK,Task,Major,2017-08-04 08:20:22,15
13087518,Script to import oak-run generated indexing to older Oak setup,"The indexing tooling implemented in OAK-6081 can be used to perform reindex and import the indexes for any Oak 1.7+ setups. For older setups we would be using 2 phase approach

# Perform [out-of-band indexing|https://jackrabbit.apache.org/oak/docs/query/oak-run-indexing.html#out-of-band-indexing]. This can be done via oak-run from 1.7.x against any older version of Oak
# Import index - For this step we cannot use oak-run from trunk for older branches as write operations would not be compatible with older version of Oak. 

For import then we have 2 options either 
# backport all the work in OAK-6271 to older branch 
# OR implement a script which can be used with oak-run or Felix Script Console to just import the lucene index with any other manual step

Purpose of this task is to implement such a script

Note - The proposed script is meant to be run from within running Oak server using Felix Script Console [1]. This is required as older version of oak-run do not support all types of BlobStores

[1] http://felix.apache.org/documentation/subprojects/apache-felix-script-console-plugin.html
",docs-impacting,"['indexing', 'run']",OAK,Improvement,Major,2017-07-17 09:16:24,11
13085141,Add flag for controlling percentile of commit time used in scheduler,"In OAK-4732 the 50th percentile of the last 1000 commits is used as wait time before returning the current root. In order to parametrise the value of the percentile, it would be nice to have a new feature flag, e.g. {{oak.scheduler.head.lockWaitPercentile}}. Setting it to {{0}} would basically disable this. Setting it to a different value might be interesting in future experiments.",Performance scalability,['segment-tar'],OAK,Improvement,Minor,2017-07-06 09:17:41,1
13083453, Refactor oak.spi.query into a separate module/bundle ,"now that OAK-6304 and OAK-6355 have been resolved, i would like to suggest that we move the _o.a.j.oak.spi.query_ code base into a separate module/bundle in order to prevent the introduction of bogus cycles and odd package exports in the future.

[~tmueller], patch will follow asap.",modularization,"['core', 'indexing', 'query']",OAK,Improvement,Major,2017-06-29 12:57:07,0
13083418,Cleanup constants in Segment class,"Some of the constants in the {{Segment}} class still refer to the old 255 segment references limit. We should fix the comments, the constants and their usage to reflect the current situation where that limit has been lifted. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2017-06-29 10:24:16,15
13082516,Refactor monitoring of deduplication caches,Currently monitoring is of the deduplication caches is hard wired into the cache manager. It would be cleaner (and is in fact a pre-requisite for OAK-5790) to decouple the monitoring from the caches. ,technical_debt,['segment-tar'],OAK,Improvement,Major,2017-06-26 14:20:44,15
13081072,oak-run check should also check checkpoints ,"{{oak-run check}} does currently *not* traverse and check the items in the checkpoint. I think we should change this and add an option to traverse all, some or none of the checkpoints. When doing this we need to keep in mind the interaction of this new feature with the {{filter}} option: the paths passed through this option need then be prefixed with {{/root}}. ",tooling,"['run', 'segment-tar']",OAK,Improvement,Blocker,2017-06-20 10:18:03,1
13079958,Improve log reporting if multiple indexes compete for same query,"Currently if multiple indexes compete for same query i.e. they report cost > 0 and less than infinity then the log output is confusing. For e.g. for a query like

{noformat}
QueryEngineImpl Parsing xpath statement: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')]
QueryEngineImpl XPath > SQL2: select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */
QueryImpl cost using filter Filter(query=select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */ fullText=""foo.png"", path=/content/dam//*)
QueryImpl cost for aggregate lucene is 2.2424751E7
QueryImpl cost for lucene-property[/oak:index/index-a][/oak:index/index-b] is 146914.0
QueryImpl cost for reference is Infinity
QueryImpl cost for ordered is Infinity
QueryImpl cost for nodeType is Infinity
QueryImpl cost for property is Infinity
QueryImpl cost for traverse is Infinity
QueryImpl query execute select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */
QueryImpl query plan [dam:Asset] as [a] /* lucene:index-b(/oak:index/index-b) +(((...) +:ancestors:/content/dam ft:(""foo.png"") where (contains([a].[*], 'foo.png')) and (isdescendantnode([a], [/content/dam])) */
QueryImpl query execute select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */
QueryImpl query plan [dam:Asset] as [a] /* lucene:index-b(/oak:index/index-b) +(((...)) +:ancestors:/content/dam ft:(""foo.png"") where (contains([a].[*], 'foo.png')) and (isdescendantnode([a], [/content/dam])) */
{noformat}

Here both index-a and index-b satisfy the query. However from logs it appears that both have same cost. While actually the reported cost is the lowest one which in this case would be for index-b

{noformat}
QueryImpl cost for lucene-property[/oak:index/index-a][/oak:index/index-b] is 146914.0
{noformat}

So as a fix
* Report cost for multiple plans returned by same index type separately
* If cost is less than infinity and that plan is eventually not selected then also log its plan. This would help to see why specific plan lost
",candidate_oak_1_4 candidate_oak_1_6,['query'],OAK,Improvement,Minor,2017-06-15 04:47:45,19
13079441,MapRecord#getKeys should should initialize child iterables lazily,"Recently we saw OutOfMemory using [oakRepoStats|https://github.com/chetanmeh/oak-console-scripts/tree/master/src/main/groovy/repostats] script with a SegmentNodeStore setup where uuid index has 16M+ entries and thus creating a very flat hierarchy. This happened while computing Tree#getChildren iterator which internally invokes MapRecord#getKeys to obtain an iterable for child node names.

This happened because code in getKeys computes the key list eagerly by calling bucket.getKeys() which recursivly calls same for each child bucket and thus resulting in eager evaluation.
{code}
        if (isBranch(size, level)) {
            List<MapRecord> buckets = getBucketList(segment);
            List<Iterable<String>> keys =
                    newArrayListWithCapacity(buckets.size());
            for (MapRecord bucket : buckets) {
                keys.add(bucket.getKeys());
            }
            return concat(keys);
        }
{code}

Instead here we should use same approach as used in MapRecord#getEntries i.e. evalate the iterable for child buckets lazily
{code}
        if (isBranch(size, level)) {
            List<MapRecord> buckets = getBucketList(segment);
            List<Iterable<MapEntry>> entries =
                    newArrayListWithCapacity(buckets.size());
            for (final MapRecord bucket : buckets) {
                entries.add(new Iterable<MapEntry>() {
                    @Override
                    public Iterator<MapEntry> iterator() {
                        return bucket.getEntries(diffKey, diffValue).iterator();
                    }
                });
            }
            return concat(entries);
        }
{code}",candidate_oak_1_6,['segment-tar'],OAK,Improvement,Minor,2017-06-13 10:19:56,15
13078419,Add missing @Nonnull annotations,There is a few places where this annotation is missing and should be added.,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:07:22,15
13078418,Closeable.close in StandbyStoreService declares exception that is never thrown,"Remove the throws clause for the {{IOException}}, which never thrown.
",cold-standby technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:05:32,15
13078417,LockBasedScheduler.execute declares exception that is never thrown,"Remove the throws clause for the {{InterruptedException}}, which never thrown.",technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:03:08,15
13078414,Declare immutable field of FileStore.CompactionResult final,Immutable fields should be final,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:00:12,15
13078413,Declare StandbyStoreService.closer final,The field is immutable and should thus be declared final.,cold-standby technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 19:57:31,15
13078412,Convert FileStore.maxFileSize fiels into local variable,That field is only used in the constructor an can thus be converted to a local variable. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 19:55:13,15
13078411,Remove unused field SegmentNodeStore.reader,That field is not used any more since the explicit commit queue was introduced. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 19:52:30,15
13076144,Unreferenced argument reference in method SegmentBufferWriter.writeRecordId,This is a leftover from when we switched from record ids to record numbers as at that point it became unnecessary to track such references. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-05-31 09:34:10,15
13076114,Test failure: UploadStagingCacheTest.testUpgrade,"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

Failed run: [Jackrabbit Oak #357|https://builds.apache.org/job/Jackrabbit%20Oak/357/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/357/console]

Initially reported by @Hudson with OAK-6275. See https://issues.apache.org/jira/browse/OAK-6275?focusedCommentId=16029669&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16029669

{noformat}
Regression

org.apache.jackrabbit.oak.plugins.blob.UploadStagingCacheTest.testUpgrade
Failing for the past 1 build (Since Failed#357 )
Took 5.3 sec.
add description
Error Message

expected null, but was:<target/junit454362693043657302/junit3108871978195343785/upload/12/34/50/123450>

Stacktrace

java.lang.AssertionError: expected null, but was:<target/junit454362693043657302/junit3108871978195343785/upload/12/34/50/123450>
	at org.apache.jackrabbit.oak.plugins.blob.UploadStagingCacheTest.testUpgrade(UploadStagingCacheTest.java:608)

{noformat}",CI jenkins test-failure,['blob-plugins'],OAK,Bug,Major,2017-05-31 07:24:22,16
13076111,Test failure: JdbcToSegmentTest.validateMigration,"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #356 has failed.
First failed run: [Jackrabbit Oak #356|https://builds.apache.org/job/Jackrabbit%20Oak/356/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/356/console]

(Initially reported by @Hudson with OAK-6275)

{noformat}
Regression

org.apache.jackrabbit.oak.upgrade.cli.JdbcToSegmentTest.validateMigration
Failing for the past 1 build (Since Failed#356 )
Took 1 min 1 sec.
add description
Error Message

Failed to copy content

Stacktrace

javax.jcr.RepositoryException: Failed to copy content
Caused by: java.lang.IllegalStateException: Branch with failed reset
Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakOak0100: Branch reset failed
Caused by: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: Empty branch cannot be reset

{noformat}",CI jenkins test-failure,['upgrade'],OAK,Bug,Major,2017-05-31 07:21:12,18
13074194,Test failure: VersionGCTest.gcMonitorInfoMessages,"Regression

org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorInfoMessages
Failing for the past 1 build (Since Failed#330 )
Took 28 ms.
Error Message

expected:<3> but was:<7>

Stacktrace

java.lang.AssertionError: expected:<3> but was:<7>
	at org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorInfoMessages(VersionGCTest.java:224)

Failed run: [Jackrabbit Oak #330|https://builds.apache.org/job/Jackrabbit%20Oak/330/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/330/console]

",CI Jenkins test-failure,"['continuous integration', 'documentmk']",OAK,Bug,Major,2017-05-23 13:10:44,2
13074117,Minor performance impact by collecting data for SegmentCache statistics ,"OAK-5956 improved the statistics collected for the segment cache. As I expected this had an impact on performance as measured by our micro benchmarks. An impact is visible for {{ConcurrentReadTest}}, {{ConcurrentReadWriteTest}} and {{ConcurrentWriteTest}}. Impact on full stack operation is yet to be determined (I assume it is neglectable though). 

{noformat}
# ConcurrentReadTest               C     min     10%     50%     90%     max       N 
Oak-Segment-Tar (base)             1      43     101     112     129     219     525
Oak-Segment-Tar (OAK-5956)         1      45     104     118     138     264     496
{noformat}

The impact seems to be mostly caused by the {{SegmentId.onAccess}} callback. 

Possible solutions:
* Replace the {{SegmentId.onAccess}} callback with a direct reference to the underlying counter. 
* Allow disabling of the cache statistics. 
* Do nothing and accept the performance impact. 

The first approach is least attractive as it breaks encapsulation. Depending on the impact of this on full stack operations I'd either go with the 2nd or 3rd option.  ",perfomance,['segment-tar'],OAK,Improvement,Minor,2017-05-23 08:50:07,15
13073321,Sidegrade support for DocumentNodeStore to Secondary NodeStore,"Seondary NodeStore feature (OAK-4180) currently creates the secondary NodeStore upon initialization. This may take long time for existing repo. To simplify that we should add support in sidegrade to migrate an existing DocumentNodeStore setup to Secondary NodeStore

This would also allow us to use the sidegraded repository as a clone of existing repository with the advantage that it would support incremental updates. Currently if a repository is cloned it does not support incremental update as NodeState comparison is done based on default implementation (actual comparison) as it cannot make use of recordId or revision to optimize the diff to determine trees which are not changed between source and target",secondary-nodestore,"['documentmk', 'upgrade']",OAK,New Feature,Major,2017-05-19 04:09:00,18
13073003,Minor cleanup for S3 tests,"- Rename config to be used for S3 to s3.config mirroring the azure.config used for Azure
- Closing InputStream",technical_debt,['blob'],OAK,Bug,Minor,2017-05-18 03:57:56,16
13072474,There should be a way to retrieve oldest timestamp to keep from nodestores,"For implementing OAK-2808 (eager/unsafe blob garbage collection approach), we need a way for nodestores to expost last safe timestamp such that blobs deleted before that timestamp can be eagerly collected (uniqueness of blob and that it won't be resurrected elsewhere is assumed to be guaranteed elsewhere e.g. OakDirectory's blobs have randomly generated bytes as content).

What we want to ensure in this task is that the garbage collection shouldn't collect stuff that could still be retrieved back - for example checkpoints.

[~chetanm] suggested that it might be an overkill to have this API in NodeStore - but maybe, it's ok to expose it in NodeStore mbean (where the impl specific mbeans known implementation detail of the nodestore to expose such data).
The mbean just needs to expose the safe oldest timestamp (UTC epoch!?).

Another thing that is potentially done in repositories (albeit not really supported afaik) is rolling back repository head state by say offline journal edit.",datastore performance,['core'],OAK,Sub-task,Major,2017-05-16 14:43:19,11
13070838,oak-run compact should have an option to disable/enable memory mapping,"Before 1.6 {{oak-run compact}} had a way to explicitly enable/disable memory mapping of the tar files. Somehow this got lost in {{oak-segment-tar}}. 

We need to add this back as e.g. on Windows memory mapping does not work well and we need to be able to explicitly disable it. ",gc,['segment-tar'],OAK,Bug,Major,2017-05-10 14:52:02,15
13070389,IllegalStateException when closing the FileStore during garbage collection,"When the file store is shut down during gc compaction is properly aborted. Afterwards it will trigger a cleanup cycle though, which runs concurrently to the proceeding shutdown potentially causing an {{ISE}}:

{noformat}
at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
at org.apache.jackrabbit.oak.segment.file.TarWriter.close(TarWriter.java:333)
at org.apache.jackrabbit.oak.segment.file.TarWriter.createNextGeneration(TarWriter.java:376)
at org.apache.jackrabbit.oak.segment.file.FileStore.newWriter(FileStore.java:682)
at org.apache.jackrabbit.oak.segment.file.FileStore.access$1700(FileStore.java:100)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.cleanup(FileStore.java:1069)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.cleanupGeneration(FileStore.java:1195)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.run(FileStore.java:803)
at org.apache.jackrabbit.oak.segment.file.FileStore.gc(FileStore.java:387)
{noformat}
",gc,['segment-tar'],OAK,Bug,Major,2017-05-09 11:52:33,12
13069938,MongoMissingLastRevSeeker may return incomplete candidate set,The set returned by {{MongoMissingLastRevSeeker.getCandidates()}} may be incomplete. See also discussion in OAK-4535 and on [oak-dev|https://lists.apache.org/thread.html/36ade745b7f6a0417aab578c21ca9fb072a7d6e5c43c724b85a153bf@%3Coak-dev.jackrabbit.apache.org%3E].,candidate_oak_1_6,['mongomk'],OAK,Bug,Minor,2017-05-08 10:17:29,2
13069223,Add unit test coverage for IOUtils.humanReadableByteCount,"There is no unit test coverage for {{IOUtils.humanReadableByteCount}} in {{oak-commons}}.

I will add a patch shortly.",unit-test-missing,['commons'],OAK,Test,Minor,2017-05-04 17:44:42,15
13069198,Test failure: VersionGCTest.gcMonitorStatusUpdates,"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #253 has failed.
First failed run: [Jackrabbit Oak #253|https://builds.apache.org/job/Jackrabbit%20Oak/253/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/253/console]

{code}
java.lang.AssertionError: expected:<[INITIALIZING, COLLECTING, UPDATING, SPLITS_CLEANUP, IDLE]> but was:<[INITIALIZING, COLLECTING, CHECKING, COLLECTING, DELETING, SORTING, DELETING, UPDATING, SPLITS_CLEANUP, IDLE]>
	at org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorStatusUpdates(VersionGCTest.java:207)
{code}
",CI jenkins test-failure,"['continuous integration', 'documentmk']",OAK,Bug,Major,2017-05-04 16:30:46,2
13069197,Add unit test coverage for IOUtils.copy,"There is no unit test coverage for {{IOUtils.copy}} in {{oak-commons}}.

I will add a patch shortly.",unit-test-missing,['commons'],OAK,Test,Minor,2017-05-04 16:30:26,15
13069132,Refactor MongoBlobReferenceIterator,...to extend from {{BlobReferenceIterator}} once OAK-6162 is ready.,candidate_oak_1_4,"['documentmk', 'mongomk']",OAK,Task,Major,2017-05-04 13:02:17,3
13068885,Add unit test coverage for IOUtils.writeInt/writeLong and IOUtils.readInt/readLong,"There is no unit test coverage for IOUtils.writeInt(), IOUtils.writeLong(), IOUtils.readInt(), and IOUtils.readLong() in oak-commons.

I am working on a patch and will have one to submit shortly.",easyfix patch test,['commons'],OAK,Test,Minor,2017-05-03 19:04:05,15
13068793,BlobReferenceIterator refactoring,"- generic iterator should use {{Utils.getSelectedDocuments()}} to obtain iterator

- {{MongoBlobReferenceIterator}} should just extend {{BlobReferenceIterator}}, using its own iterator (EDIT: will move this into a separate ticket)

- {{Utils.getSelectedDocuments()}} should allow specifying the batch size",candidate_oak_1_4,['documentmk'],OAK,Task,Major,2017-05-03 14:40:52,3
13067389,Create RDB-specific BlobReferenceIterator,"Mirroring {{MongoBlobReferenceIterator}}, using the capabilities added in OAK-5855.

(might lead to refactoring of {{MongoBlobReferenceIterator}})",candidate_oak_1_4,['rdbmk'],OAK,Technical task,Major,2017-04-27 14:05:16,3
13065492,Offline compaction uses too much memory ,"Using offline compaction on a repository with nodes having many direct child node I observed a steady increase of heap usage. This is cause by using a {{MemoryNodeBuilder}} in {{CompactDiff.childNodeAdded()}}, which causes all those child nodes to be cached in memory. 

Changing the line

{code}
child = EMPTY_NODE.builder();
{code}

to 

{code}
child = writer.writeNode(EMPTY_NODE).builder();
{code}

fixes the problem as the latter returns a {{SegmentNodeBuilder}} where the former returns a {{MemoryNodeBuilder}}.",memory performance,['segment-tar'],OAK,Bug,Major,2017-04-20 11:36:17,15
13065446,Excessive memory usage by the cached segment references,"The references of a segment to other segments are cached within {{Segment.readReferencedSegments()}}. However caching the {{SegmentId}} instances themselves leads to excessive heap usage as each id also keeps a reference to its underlying segment. 

I suggest to cache those references as msb, lsb pairs instead and create the {{SegmentId}} instance on the fly when required. ",perfomance,['segment-tar'],OAK,Bug,Major,2017-04-20 08:59:43,15
13065049,Test failure: CompositeDataStoreCacheTest.concurrentGetCached(),"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #182 has failed.
First failed run: [Jackrabbit Oak #182|https://builds.apache.org/job/Jackrabbit%20Oak/182/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/182/console]

{noformat}
concurrentGetCached(org.apache.jackrabbit.oak.plugins.blob.CompositeDataStoreCacheTest)  Time elapsed: 0.013 sec  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<1>
	at org.apache.jackrabbit.oak.plugins.blob.CompositeDataStoreCacheTest.concurrentGetCached(CompositeDataStoreCacheTest.java:468)
{noformat}",CI Jenkins,"['blob', 'continuous integration']",OAK,Bug,Major,2017-04-19 10:32:42,16
13064749,Move exercise code to separate packages to avoid build warnings,"building the {{oak-exercise}} module generates warnings due to occupation of same package space:

{quote}
[WARNING] Bundle org.apache.jackrabbit:oak-exercise:bundle:1.8-SNAPSHOT : Split package, multiple jars provide the same package:org/apache/jackrabbit/oak/security/authentication
Use Import/Export Package directive -split-package:=(merge-first|merge-last|error|first) to get rid of this warning
Package found in   [Jar:., Jar:oak-core, Jar:oak-core]
Class path         [Jar:., Jar:jcr, Jar:oak-jcr, Jar:oak-core-spi, Jar:oak-store-spi, Jar:oak-api, Jar:oak-core, Jar:oak-blob, Jar:jackrabbit-data, Jar:jcl-over-slf4j, Jar:oak-blob-plugins, Jar:commons-codec, Jar:commons-io, Jar:oak-commons, Jar:oak-auth-external, Jar:jackrabbit-api, Jar:jackrabbit-jcr-commons, Jar:jsr305, Jar:org.apache.felix.scr.annotations, Jar:org.apache.felix.jaas, Jar:org.osgi.core, Jar:org.osgi.compendium, Jar:guava, Jar:slf4j-api, Jar:junit, Jar:hamcrest-core, Jar:mongo-java-driver, Jar:logback-classic, Jar:logback-core, Jar:jul-to-slf4j, Jar:h2, Jar:jackrabbit-jcr-tests, Jar:concurrent, Jar:oak-commons, Jar:oak-jcr, Jar:oak-core, Jar:geronimo-jta_1.0.1B_spec]
[WARNING] Bundle org.apache.jackrabbit:oak-exercise:bundle:1.8-SNAPSHOT : Split package, multiple jars provide the same package:org/apache/jackrabbit/oak/security/authorization/restriction
Use Import/Export Package directive -split-package:=(merge-first|merge-last|error|first) to get rid of this warning
Package found in   [Jar:., Jar:oak-core, Jar:oak-core]
Class path         [Jar:., Jar:jcr, Jar:oak-jcr, Jar:oak-core-spi, Jar:oak-store-spi, Jar:oak-api, Jar:oak-core, Jar:oak-blob, Jar:jackrabbit-data, Jar:jcl-over-slf4j, Jar:oak-blob-plugins, Jar:commons-codec, Jar:commons-io, Jar:oak-commons, Jar:oak-auth-external, Jar:jackrabbit-api, Jar:jackrabbit-jcr-commons, Jar:jsr305, Jar:org.apache.felix.scr.annotations, Jar:org.apache.felix.jaas, Jar:org.osgi.core, Jar:org.osgi.compendium, Jar:guava, Jar:slf4j-api, Jar:junit, Jar:hamcrest-core, Jar:mongo-java-driver, Jar:logback-classic, Jar:logback-core, Jar:jul-to-slf4j, Jar:h2, Jar:jackrabbit-jcr-tests, Jar:concurrent, Jar:oak-commons, Jar:oak-jcr, Jar:oak-core, Jar:geronimo-jta_1.0.1B_spec]
[WARNING] Bundle org.apache.jackrabbit:oak-exercise:bundle:1.8-SNAPSHOT : Split package, multiple jars provide the same package:org/apache/jackrabbit/oak/security/principal
Use Import/Export Package directive -split-package:=(merge-first|merge-last|error|first) to get rid of this warning
Package found in   [Jar:., Jar:oak-core, Jar:oak-core]
Class path         [Jar:., Jar:jcr, Jar:oak-jcr, Jar:oak-core-spi, Jar:oak-store-spi, Jar:oak-api, Jar:oak-core, Jar:oak-blob, Jar:jackrabbit-data, Jar:jcl-over-slf4j, Jar:oak-blob-plugins, Jar:commons-codec, Jar:commons-io, Jar:oak-commons, Jar:oak-auth-external, Jar:jackrabbit-api, Jar:jackrabbit-jcr-commons, Jar:jsr305, Jar:org.apache.felix.scr.annotations, Jar:org.apache.felix.jaas, Jar:org.osgi.core, Jar:org.osgi.compendium, Jar:guava, Jar:slf4j-api, Jar:junit, Jar:hamcrest-core, Jar:mongo-java-driver, Jar:logback-classic, Jar:logback-core, Jar:jul-to-slf4j, Jar:h2, Jar:jackrabbit-jcr-tests, Jar:concurrent, Jar:oak-commons, Jar:oak-jcr, Jar:oak-core, Jar:geronimo-jta_1.0.1B_spec]
{quote}

Moving the exercises and the sample code to an _execise_ package space would also make it really obvious that this is not part of the productive code base.",technical_debt,['exercise'],OAK,Bug,Minor,2017-04-18 15:23:43,0
13064720,org.apache.jackrabbit.oak.management.ManagementOperation should use TimeDurationFormatter,See OAK-6020.,technical_debt,['core'],OAK,Task,Minor,2017-04-18 14:17:43,15
13064699,Avoid reads from MongoDB primary,"With OAK-2106 Oak now attempts to read from a MongoDB secondary when it detects the requested data is available on the secondary.

When multiple Oak cluster nodes are deployed on a MongoDB replica set, many reads are still directed to the primary. One of the reasons why this is seen in practice, are observers and JCR event listeners that are triggered rather soon after a change happens and therefore read recently modified documents. This makes it difficult for Oak to direct calls to a nearby secondary, because changes may not yet be available there.

A rather simple solution for the observers may be to delay processing of changes until they are available on the near secondary.

A more sophisticated solution discussed offline could hide the replica set entirely and always read from the nearest secondary. Writes would obviously still go to the primary, but only return when the write is available also on the nearest secondary. This guarantees that any subsequent read is able to see the preceding write.",scalability,['mongomk'],OAK,Improvement,Major,2017-04-18 13:32:49,2
13064434,Missing Images,"The page at [0] (documentation page for tarMK Cold Standby) has several images missing.  Example : http://jackrabbit.apache.org/oak/docs/coldstandby/client_mbean_server_working.png

[0] http://jackrabbit.apache.org/oak/docs/coldstandby/coldstandby.html
",documentation,"['doc', 'tarmk-standby']",OAK,Documentation,Trivial,2017-04-17 17:36:07,13
13063204,Assign meaningful names to cold standby threads,"For easier understanding of the cold standby behaviour, threads used by the cold standby implementations should be assigned more human readable names.",cold-standby,['segment-tar'],OAK,Improvement,Major,2017-04-11 14:06:13,12
13063168,Migration of binaries relies on implementation details of the TarMK,"This was uncovered with the work on OAK-6051.

On {{NodeStore.merge()}} the TarMK uses {{SegmentNodeState#fastEquals}} to efficiently determine whether there is something to commit and whether the base state changed since the builder about to commit was acquired. For efficiency reasons {{fastEquals}} can return ""false negatives"". AFAIU migration of binaries depends on this implementation detail because without this optimisation migration fails. ({{SegmentToExternalMigrationTest}} and {{ExternalToExternalMigrationTest}} fail). ",migration tooling,['upgrade'],OAK,Bug,Major,2017-04-11 11:30:48,18
13062916,Test failure: StandbyTestIT.testSyncLoop,"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #144 has failed.
First failed run: [Jackrabbit Oak #144|https://builds.apache.org/job/Jackrabbit%20Oak/144/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/144/console]

Error Message

{code}
expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
{code}

Stacktrace

{code}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
    at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:126)
{code}

Standard Output

[^stdout.log]

Also failed at https://builds.apache.org/job/Jackrabbit%20Oak/394/",CI flaky-test jenkins,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-04-10 14:41:34,12
13062146,Move OakInitializer from org.apache.jackrabbit.oak.spi.lifecycle to o.a.j.oak,see http://markmail.org/message/2wgjv6obefbfpd7u?q=list:org%2Eapache%2Ejackrabbit%2Eoak-dev&page=2 for discussion.,modularization,['core'],OAK,Improvement,Major,2017-04-06 15:43:13,0
13062144,Cleanup blocks writers,"The refactoring from OAK-6002 moved the cleanup of the tar readers into the read lock, which blocks concurrent writers from progressing. This was a problem with {{oak-segment}} before and fixed with OAK-3329.
As cleanup can take up to a couple of minutes on busy system we should re-establish the former behaviour. ",cleanup gc,['segment-tar'],OAK,Improvement,Major,2017-04-06 15:38:54,12
13062077,org.apache.jackrabbit.oak.plugins.tika.TextExtractorMain must have private constructor,see OAK-6041,technical_debt,['run'],OAK,Bug,Minor,2017-04-06 11:58:22,0
13062043,org.apache.jackrabbit.oak.util.OakVersion must have private constructor,see OAK-6041,technical_debt,['core'],OAK,Bug,Minor,2017-04-06 09:02:46,0
13062035,org.apache.jackrabbit.oak.commons.jmx.JmxUtil must have a private constructor,see also OAK-6041,technical_debt,['commons'],OAK,Bug,Major,2017-04-06 08:17:05,0
13062021,o.a.j.oak.plugins.identifier.ClusterRepositoryInfo should have private constructor,the utility class {{o.a.j.oak.plugins.identifier.ClusterRepositoryInfo}} should have a private constructor to avoid instantiation... I consider it a utility because all it's fields and methods are declared static.,technical_debt,['core'],OAK,Bug,Minor,2017-04-06 07:14:37,0
13061714,Drop dependency of spi.security.* tests from AbstractSecurityTest,"the {{AbstractSecurityTest}} provides the setup of an Oak content repository that relies on the default implementation of the various security modules. In the light of modularization efforts I would like to refactor those security related tests in oak.spi.security.* that depend on {{AbstractSecurityTest}} as they should not require a particular implementation.

This currently affects the following classes:

- -org.apache.jackrabbit.oak.spi.security.authorization.accesscontrol.AbstractAccessControlTest.java-
- -org.apache.jackrabbit.oak.spi.security.authorization.restriction.AbstractRestrictionProviderTest.java-
- -org.apache.jackrabbit.oak.spi.security.authorization.restriction.CompositeRestrictionProviderTest.java-
- -org.apache.jackrabbit.oak.spi.security.privilege.PrivilegeBitsProviderTest.java-
- -org.apache.jackrabbit.oak.spi.security.privilege.PrivilegeBitsTest.java-
- -org.apache.jackrabbit.oak.spi.security.user.action.AccessControlActionTest.java-
- -org.apache.jackrabbit.oak.spi.security.user.action.ClearMembershipActionTest.java-
                    ",modularization,['core'],OAK,Improvement,Major,2017-04-05 12:14:18,0
13061659,Remove WhiteboardUtils#registerObserver method,"Seems the added value of this method is minimal, worse it adds an unneeded dependency to the {{Observer}} class. [0]

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/spi/whiteboard/WhiteboardUtils.java#L101",modularization,['core'],OAK,Improvement,Major,2017-04-05 08:08:30,14
13061657,Mark AbstractLoginModule and Authentication as provider types,"With OAK-5903 the {{AbstractLoginModule}} and {{Authentication}} interfaces received new methods and the exported package version was bumped from 1.1.0 to 2..0.0. This is unfortunately a breaking change for anyone importing the {{org.apache.jackrabbit.oak.spi.security.authentication}}, including Sling's oak-server module.

To me these classes do not look as they are usually implemented by clients, but part of an SPI implemented mostly by oak-core, e.g. they are 'provider types' in OSGi vocabulary.

I've marked them with the {{@ProviderType}} annotation which allows the exported package version to only be increased to 1.2.0 and allows consumer to reference those packages without changing import ranges across all Oak version.",modularization,['security'],OAK,Improvement,Major,2017-04-05 07:50:25,14
13061639,Move security related service trackers from spi.whiteboard to oak.security package space ,"while investigating possibilities to improve modularization we spotted user management related classes being located in the oak.spi.whiteboard package. IMHO it would make sense to move those to oak.security.user.whiteboard as they don't belong to the spi package space.

[~alexparvulescu], wdyt?",modularization,['core'],OAK,Improvement,Major,2017-04-05 07:01:56,0
13061466,Test failure: CompactionAndCleanupIT.concurrentCleanup,"That test fails for me on every 2nd run or so. This seems to be a regression introduced with OAK-6002. 

{code}
org.junit.ComparisonFailure: Expected nothing to be cleaned but generation 'b' for file data00002b.tar indicates otherwise. 
Expected :a
Actual   :b


at org.junit.Assert.assertEquals(Assert.java:115)
org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.concurrentCleanup(CompactionAndCleanupIT.java:1252)
{code}

[~frm], could you have a look?

",gc test-failure,['segment-tar'],OAK,Bug,Major,2017-04-04 16:05:05,12
13061396,Add TarFiles to the architecture diagram,The newly created {{TarFiles}} should be added to the architecture diagram for oak-segment-tar.,documentation,"['doc', 'segment-tar']",OAK,Improvement,Major,2017-04-04 09:56:35,12
13061085,Remove segment graph functionality from oak-run,"We could probably remove the segment graph functionality from oak-run. This has been implemented mainly (and solely?) for the purpose of analysing the problems around OAK-3348 and I assume it would quickly start falling behind as we move forward. Also for this kind of analysis I have switched to [oak-script|https://github.com/mduerig/script-oak], which is far more flexible. 

Let's decide closer to cutting 1.8 how to go forward here.",technical_debt tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2017-04-03 10:31:54,12
13061054,DocumentNodeStore.compare() fails with IllegalStateException in read-only mode,"Comparing node states in read-only mode may fail with an IllegalStateException when the journal is used to perform a diff.

{noformat}
java.lang.IllegalStateException: Root document does not have a lastRev entry for local clusterId 0
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199)
    at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
    at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)
    at org.apache.jackrabbit.oak.plugins.document.MemoryDiffCache.getChanges(MemoryDiffCache.java:83)
    at org.apache.jackrabbit.oak.plugins.document.TieredDiffCache.getChanges(TieredDiffCache.java:50)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.compare(DocumentNodeStore.java:1632)
[...]
Caused by: java.lang.IllegalStateException: Root document does not have a lastRev entry for local clusterId 0
    at org.apache.jackrabbit.oak.plugins.document.JournalDiffLoader.call(JournalDiffLoader.java:82)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffImpl(DocumentNodeStore.java:2428)
{noformat}

See also OAK-6011.",candidate_oak_1_6,['documentmk'],OAK,Bug,Minor,2017-04-03 08:02:55,2
13060317,Test failure: JdbcToSegmentTest:validateMigration ,"{code}
java.lang.RuntimeException: javax.jcr.RepositoryException: Failed to copy content
    at com.google.common.io.Closer.rethrow(Closer.java:149)
    at org.apache.jackrabbit.oak.upgrade.cli.OakUpgrade.migrate(OakUpgrade.java:81)
    at org.apache.jackrabbit.oak.upgrade.cli.OakUpgrade.migrate(OakUpgrade.java:67)
    at org.apache.jackrabbit.oak.upgrade.cli.OakUpgrade.main(OakUpgrade.java:48)
    at org.apache.jackrabbit.oak.upgrade.cli.AbstractOak2OakTest.prepare(AbstractOak2OakTest.java:112)
Caused by: javax.jcr.RepositoryException: Failed to copy content
    at org.apache.jackrabbit.oak.upgrade.RepositorySidegrade.copy(RepositorySidegrade.java:264)
    at org.apache.jackrabbit.oak.upgrade.cli.OakUpgrade.sidegrade(OakUpgrade.java:92)
    at org.apache.jackrabbit.oak.upgrade.cli.OakUpgrade.migrate(OakUpgrade.java:78)
    ... 34 more
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Root document does not have a lastRev entry for local clusterId 0
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199)
    at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
    at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)
    at org.apache.jackrabbit.oak.plugins.document.MemoryDiffCache.getChanges(MemoryDiffCache.java:83)
    at org.apache.jackrabbit.oak.plugins.document.TieredDiffCache.getChanges(TieredDiffCache.java:50)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.compare(DocumentNodeStore.java:1632)
    at org.apache.jackrabbit.oak.plugins.document.AbstractDocumentNodeState.compareAgainstBaseState(AbstractDocumentNodeState.java:114)
    at org.apache.jackrabbit.oak.upgrade.RepositorySidegrade.migrateWithCheckpoints(RepositorySidegrade.java:369)
    at org.apache.jackrabbit.oak.upgrade.RepositorySidegrade.copyState(RepositorySidegrade.java:294)
    at org.apache.jackrabbit.oak.upgrade.RepositorySidegrade.copy(RepositorySidegrade.java:257)
    ... 36 more
Caused by: java.lang.IllegalStateException: Root document does not have a lastRev entry for local clusterId 0
    at org.apache.jackrabbit.oak.plugins.document.JournalDiffLoader.call(JournalDiffLoader.java:82)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffImpl(DocumentNodeStore.java:2428)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.access$1100(DocumentNodeStore.java:136)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$8.call(DocumentNodeStore.java:1637)
    at org.apache.jackrabbit.oak.plugins.document.MemoryDiffCache$1.call(MemoryDiffCache.java:89)
    at org.apache.jackrabbit.oak.plugins.document.MemoryDiffCache$1.call(MemoryDiffCache.java:83)
    at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)
    at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
    at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
    at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
    ... 45 more
{code}",CI test-failure,['upgrade'],OAK,Bug,Major,2017-03-30 09:57:20,18
13060292,Simplify cancellation of compaction by timeout ,Cancellation of (force) compaction by timeout is currently implemented on top of the {{CancelCompactionSupplier}}. It should better be implemented within though to simplify the implementation of OAK-3349. ,compaction gc refactoring technical_debt,['segment-tar'],OAK,Improvement,Major,2017-03-30 08:08:45,15
13060282,Introduce a FailingDocumentStore,Introduce a DocumentStore wrapper that can be instructed to fail after some number of operations or with some probability.,candidate_oak_1_4,['documentmk'],OAK,Test,Minor,2017-03-30 07:50:05,2
13060280,MultiplexingNodeStore sometimes fails to release checkpoint,"If the DocumentNodeStore is used as a part of the MultiplexingNodeStore, we should check whether the checkpoint name refers to a valid checkpoint in release() and retrieve() methods. Otherwise we can get an IllegalArgumentException.",multiplexing,['core'],OAK,Bug,Major,2017-03-30 07:43:28,18
13060276,Add record id of the compacted root to the GC journal,We should also add the record id of the root node resulting from compaction to the gc log. This would have been helpful a couple of times already in the past for testing and post mortems. It will likely also be a requirement to implement the tail compaction approach form OAK-3349. ,compaction gc,['segment-tar'],OAK,Improvement,Major,2017-03-30 07:22:31,15
13059956,Revisions.setHead(Function) should return the new head or null instead of boolean,"Currently {{Revisions.setHead(Function, Option)}} returns a {{boolean}} to indicate success or failure. The caller has no access to the head resulting from this call. I would thus like to change this into the record id of the new head in case of success and {{null}} otherwise. ",refactoring,['segment-tar'],OAK,Improvement,Major,2017-03-29 07:53:41,15
13058911,add CloseableIterator similar to CloseableIterable,Needed by OAK-5855 but tracked separately so it can be independently back-ported.,candidate_oak_1_4,['commons'],OAK,Improvement,Major,2017-03-24 14:40:12,3
13058163,Support path exclusion in secondary nodestore,"Secondary NodeStore feature (OAK-4180) for now currently supports path inclusion. It would be useful to have support for path exclusion also.

Using this a user can can include all content  under / but exclude /oak:index/uuid/:index entries.",secondary-nodestore,['documentmk'],OAK,Improvement,Major,2017-03-22 09:34:03,19
13058120,Disable caching for S3 integration tests,Caching should be disabled in the tests by default as these tests require synchronous uploads.,candidate_oak_1_6,['blob'],OAK,Test,Minor,2017-03-22 06:21:30,16
13057878,Document Metrics related classes and interfaces,"The Metrics related classes and interfaces in {{org.apache.jackrabbit.oak.stats}} and {{org.apache.jackrabbit.oak.plugins.metric}} are largely undocumented. Specifically it is not immediately how they should be used, how a new {{Stats}} instance should be added, what the effect this would have and how it would (or would) not be exposed (e.g. via JMX). 

",documentation technical_debt,['core'],OAK,Improvement,Major,2017-03-21 12:40:44,19
13057868,Improve cache statistics of the segment cache,"The statistics provided by the segment cache are off due to the fact it serves as 2nd level cache: as it doesn't see all the hits in the 1st level cache ({{SegmentId.getSegment()}}), it reports a hit/miss rate that is to low. 

We should look into how we could expose better statistics wrt. caching of segments. Possible consolidated over 1st and 2nd level caches. ",monitoring,['segment-tar'],OAK,Improvement,Major,2017-03-21 12:19:04,15
13057842,Unify and simplify the deduplication caches ,"As a preparation to enable better monitoring and add more precise monitoring probes to the deduplication caches I would like to unify their interfaces and simplify their setup. 
* Don't expose the cache statistics via the {{FileStore}} and leverage the the {{FileStoreBuilder}} instead for this.
* All deduplication caches should implement a unified {{Cache}} interface to simplify wrapping them (e.g. for additional access statistics collection). 
* Replace the ad-hoc collection of cache statistics in the {{NodeWriteStats}} inner class of the {{SegmentWriter}} and replace it with a more structured approach. 
* Expose additional cache access statistics via Metrics. 
* The additional statistics should discriminate caches access occurring as regular writes from such occurring during compaction. ",monitoring refactoring technical_debt,['segment-tar'],OAK,Improvement,Major,2017-03-21 10:12:38,15
13057836,PriorityCache statistics should support load exception count,Currently {{CacheStats.loadExceptionCount()}} always returns 0 on a cache statistics retrieved from the {{PriorityCache}}. I would like to implement this statistics by returning the number of times a {{put()}} failed on that cache because it did not find an empty slot. ,monitoring,['segment-tar'],OAK,Improvement,Major,2017-03-21 09:46:03,15
13056628,Improve the checkpoint release & retrieve for multiplexing node store,"Right now, the checkpoint retrieval & release uses the checkpoint reference to get the checkpoint metadata from the global store. This metadata contains checkpoint references on the other stores.

We should also support a case in which the checkpoint id exists on all the configured stores. This happens if we clone the repository and configure it under different mounts.",multiplexing,['core'],OAK,Improvement,Major,2017-03-16 11:33:59,18
13056362,Remove CachedNodeDocument,The CachedNodeDocument interface was introduced with OAK-891 but then the feature was later removed with OAK-2937. The interface is not used anywhere and should be removed.,candidate_oak_1_4 technical_debt,"['core', 'documentmk']",OAK,Improvement,Minor,2017-03-15 17:22:25,2
13056257,Remove unused depth parameter SegmentWriteOperation#writeNode and related methods,That depth parameter is a leftover from when the node de-duplication cache used the depth of a node in the tree for its eviction strategy. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-03-15 11:06:36,15
13049988,Make import org.apache.log4j optional,"{{oak-segment-tar}} imports {{org.apache.log4j}}. I didn't investigate why {{oak-segment-tar}} is importing {{org.apache.log4j}} at all (no direct use), but that import could be optional (did some tests in Sling).",OSGi,['segment-tar'],OAK,Improvement,Major,2017-03-10 09:47:30,14
13049537,OOM in SegmentReferenceLimitTestIT,Running that IT (with 4g heap) currently results in an {{OOME}}. We need to check whether the expectations are still valid for Segment Tar and either adapt the test or look into the memory consumption. ,test-failure,['segment-tar'],OAK,Bug,Major,2017-03-09 09:07:43,15
13049223,Reduce copying of data when reading mmapped records,"The idea is to reduce the amount of extra byte buffers created when reading mmapped records, if possible pushing the ByteBuffer all the way to the consumer.
For example reading a String from a Segment right now means first reading the bytes of of the record into a byte array, then creating a string with an encoding (which behind the scenes will copy the byte array again and run it through the decoder). An alternative is to call {{decode}} on the Charset and pass in the ByteBuffer, skipping the intermediate operations.

There are a few cases of this I included in the patch, but there may be others (like the {{SegmentStream}} which needs a full rewrite).

Interested in what others think of this!",memory performance,['segment-tar'],OAK,Improvement,Major,2017-03-08 10:08:21,14
13048908,Authentication: add extension to retrieve user principal,In the current default setup the implementations of the {{Authentication}} interface resolve a user for the given login credentials but don't provide means to retrieve the associated principal. Consequently upon {{LoginModule.commit}} the user needs to resolved a second time to compute the set of all principals. This could be simplified by using {{PrincipalProvider.getGroupMembership(Principal)}} if the users principals was available upon successful call to {{Authentication.authenticate}}.,performance,['core'],OAK,New Feature,Major,2017-03-07 15:04:57,0
13048893,Cold standby should allow syncing of blobs bigger than 2.2 GB,"Currently there is a limitation for the maximum binary size (in bytes) to be synced between primary and standby instances. This matches {{Integer.MAX_VALUE}} (2,147,483,647) bytes and no binaries bigger than this limit can be synced between the instances.

Per comment at [1], the current protocol needs to be changed to allow sending of binaries in chunks, to surpass this limitation.

[1] https://github.com/apache/jackrabbit-oak/blob/1.6/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/standby/client/StandbyClient.java#L125",cold-standby,['segment-tar'],OAK,Improvement,Minor,2017-03-07 13:54:09,1
13048486,ResultRowToAuthorizable: create user/group from tree,"while measuring performance of possible fixes for OAK-4920, i noticed a possible improvement within the ResultRowToAuthorizable used to within the user query code base: instead of retrieving authorizables by ID the code can be simplified by using the tree that has already been obtained.",perfomance,['core'],OAK,Improvement,Major,2017-03-06 11:14:16,0
13048032,Stricter validation on primary type change,"This issue tracks the {{oak-core}} changes on the {{TypeEditor}} to have better protection against corruption when changing a node's primary type.

Basically first 2 items on [~mreutegg]'s summarization of OAK-5229 [0].


[0] https://issues.apache.org/jira/browse/OAK-5229?focusedCommentId=15863871&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15863871",candidate_oak_1_6,"['core', 'jcr']",OAK,Bug,Major,2017-03-03 12:52:49,14
13047970,segment-tar should have a tarmkrecovery command,"{{oak-segment}} had a {{tarmkrecovery}} command responsible with listing candidates for head journal entries. We should re-enable this also for {{oak-segment-tar}}.

/cc [~mduerig] [~frm]",technical_debt tooling,"['run', 'segment-tar']",OAK,New Feature,Minor,2017-03-03 10:07:49,15
13047961,Evaluate utility of RepositoryGrowthTest benchmark,"{{RepositoryGrowthTest}} is a benchmark which makes use of the deprecated {{SegmentFixture}}. Since OAK-5834 removes the old {{oak-segment}} module and the code associated with it, {{RepositoryGrowthTest}} was also removed. If there's value in it, we can adapt it to work with the new {{SegmentTarFixture}}.

/cc [~chetanm]",benchmark,"['run', 'segment-tar']",OAK,Task,Minor,2017-03-03 09:51:37,1
13047677,Oak upgrade usage note refers to oak-run,"Running {{java -jar oak-upgrade*.jar}} prints 

{noformat}
Usage: java -jar oak-run-*-jr2.jar upgrade [options] jcr2_source [destination]
       (to upgrade a JCR 2 repository)

       java -jar oak-run-*-jr2.jar upgrade [options] source destination
       (to migrate an Oak repository)
{noformat}

Which incorrectly refers to {{oak-run upgrade}}. The latter will send me back to {{oak-run}}: ""This command was moved to the oak-upgrade module"". ",production tooling usability,['upgrade'],OAK,Bug,Minor,2017-03-02 13:34:03,2
13047606,Duplicate uploads might happen with AbstractSharedCachingDataStore,"If a file is staged for async upload in UploadStagingCache and then another call to AbstractSharedCachingDataStore.addRecord is made for a file with same SHA1, the new call goes directly to the backed to write the file, because the cache is not taking into account pending uploads. This makes 2 uploads to happen for the same blob: one async (from UploadStagingCache) and one sync (from AbstractSharedCachingDataStore.addRecord  

(cc [~amitjain])",candidate_oak_1_6 performance,['blob'],OAK,Bug,Minor,2017-03-02 08:36:00,16
13046916,Compressed segments,"It would be interesting to see the effect of compressing the segments within the tar files with a sufficiently effective and performant compression algorithm:

* Can we increase overall throughput by trading CPU for IO?
* Can we scale to bigger repositories (in number of nodes) by squeezing in more segments per MB and thus pushing out onset of thrashing?
* What would be a good compression algorithm/library?
* Can/should we make this optional? 
* Migration and compatibility issues?
",scalability,['segment-tar'],OAK,New Feature,Major,2017-02-28 09:47:04,1
13046660,Potential expensive call to NodeState.getChildNodeCount() in constructor of Template,"On of the {{Template}} constructors (the one used when writing templates) performs a call to {{NodeState.getChildNodeCount()}} to determine the value of {{Template.childName}}. I have seen this call comping up in performance traces on various occasions, which leads me to believe there is room for improvement here. ",performance,['segment-tar'],OAK,Improvement,Minor,2017-02-27 15:04:13,15
13046007,Remove the deprecated oak-segment module,"The {{oak-segment}} module has been deprecated for 1.6 with OAK-4247. We should remove it entirely now:

* Remove the module
* Remove fixtures and ITs pertaining to it
* Remove references from documentation where not needed any more

An open question is how we should deal with the tooling for {{oak-segment}}. Should we still maintain this in trunk and keep the required classes (which very much might be all) or should we maintain the tooling on the branches? What about new features in tooling? 
",deprecation technical_debt,['segmentmk'],OAK,Task,Minor,2017-02-24 12:46:27,1
13045908,Don't use SHA-1 for new DataStore binaries,"A [collision for SHA-1|https://www.schneier.com/blog/archives/2017/02/sha-1_collision.html] has been published. We still use SHA-1 for the FileDataStore, and I believe the S3 DataStore right now. Given there is a collision, we should switch to a stronger algorithm, for example SHA-256, for new binaries.",candidate_oak_1_4,[],OAK,Improvement,Major,2017-02-24 07:06:14,16
13045682,TarMK: Implement tooling to repair broken nodes,"With {{oak-run check}} we can determine the last good revision of a repository and use it to manually roll back a corrupted segment store. 

Complementary to this we should implement a tool to roll forward a broken revision to a fixed new revision. Such a tool needs to detect which items are affected by a corruption and replace these items with markers. With this the repository could brought back online and the markers could be used to identify the locations in the tree where further manual action might be needed. ",production technical_debt tooling,"['run', 'segment-tar']",OAK,New Feature,Major,2017-02-23 17:07:20,1
13045662,Chronologically rebase checkpoints on top of each other during compaction,"Currently the compactor does just a rewrite of the super root node without any special handling of the checkpoints. It just relies on the node de-duplication cache to avoid fully exploding the checkpoints. 
I think this can be improved by subsequently rebasing checkpoints on top of each other during compaction. (Very much like checkpoints are handled in migration). ",compaction gc performance,['segment-tar'],OAK,Improvement,Major,2017-02-23 16:00:16,15
13045626,Perform update of single node in one remote call if possible,"If a single node is modified in a commit then currently it performs 2 remote calls

# The actual update
# Update of commit root

as for single node update commitRoot == node being updated we can optimize this case to see if both operations can be done in same call",performance,['documentmk'],OAK,Improvement,Major,2017-02-23 13:56:27,2
13045620,BlobStore should be AutoCloseable,"{{DocumentNodeStore}} currently calls {{close()}} if the blob store instance implements {{Closeable}}.

This has led to problems where wrapper implementations did not implement it, and thus the actual blob store instance wasn't properly shut down.

Proposal: make {{BlobStore}} extend {{Closeable}} and get rid of all {{instanceof}} checks.

[~thomasm] [~amitjain] - feedback appreciated.",candidate_oak_1_8,['blob'],OAK,Improvement,Minor,2017-02-23 13:34:20,3
13045547,hashCode of RestrictionImpl doesn't include value,"The hashCode generation of {{RestrictionImpl}} currently looks as follows:

{code}
    public int hashCode() {
        return Objects.hashCode(definition, property);
    }
{code}

However, the hashCode of our {{PropertyState}} implementation doesn't include the value. See {{AbstractPropertyState}}:

{code}
public static int hashCode(PropertyState property) {
        return property.getName().hashCode();
    }
{code}

Consequently the hashCode of the {{AccessControlEntry}} implementation, the validation of ACEs in {{AccessControlValidator}} and the {{AcEntry}} created in the {{PermissionHook}} generates the same hashCode for entries that only differ by the value of a restriction.",performance,['core'],OAK,Improvement,Major,2017-02-23 08:44:59,0
13045516,Test failure: persistentCache.BroadcastTest.broadcastTCP ,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1447 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1447|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1447/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1447/console]",test-failure ubuntu,"['cache', 'continuous integration', 'core']",OAK,Bug,Major,2017-02-23 06:08:55,4
13045336,Test failure: segment.standby.MBeanIT.testClientAndServerEmptyConfig,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #472 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #472|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/472/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/472/console]",test-failure windows,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-02-22 18:54:25,1
13045164,Consistency check incorrectly fails for broken partial paths ,"To better explain the bug I'll describe the content of the revisions:
# Valid Revision
Adds child nodes {{a}}, {{b}}, {{c}}, {{d}}, {{e}}, {{f}} with various properties (blobs included)
# Invalid Revision
Adds child node {{z}} with some blob properties and then corrupts the {{NODE}} record holding {{z}}.
Now when the consistency check is run, it correctly detects that the second revision is broken, *marks the path {{/z}} as corrupt* and then continues checking the first valid revision. Because of a check introduced for OAK-5556 [1], which tries to validate the user provided absolute paths before checking them, the checker tries to check {{/z}} in the first revision, where of course it can't find it. Therefore the check incorrectly fails for this revision, although it shouldn't have to.

/cc [~mduerig], [~frm]",tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-02-22 09:40:34,1
13045162,Remove duplicate code for background operation timing log,There are multiple places in DocumentNodeStore where background operations log timing. This should be consolidated.,technical_debt,"['core', 'documentmk']",OAK,Improvement,Minor,2017-02-22 09:36:32,2
13045070,Test failure: PojoSR run.osgi.SecurityProviderRegistrationTest,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1441 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1441|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1441/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1441/console]",test-failure ubuntu,"['continuous integration', 'pojosr']",OAK,Bug,Major,2017-02-22 04:24:36,20
13044813,deliver overflow change even without new commit,"As [reported|http://markmail.org/message/2qxle24f6zu2vpms] by [~catholicon] on oak-dev the observation queue only delivers the so-called _overflow entry/change_ only when new commits are 'coming in'. We might want to consider fixing this, even though arguably this is a very rare case (since typically the observation queue is configured to be very large)",candidate_oak_1_4 candidate_oak_1_6,['jcr'],OAK,Bug,Minor,2017-02-21 11:21:51,21
13044658,Test failure: org.apache.jackrabbit.oak.run.osgi.SegmentNodeStoreConfigTest.testDeadlock,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=unittesting #465 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=unittesting #465|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=unittesting/465/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=unittesting/465/console]",test-failure windows,"['continuous integration', 'pojosr']",OAK,Bug,Major,2017-02-20 21:59:47,15
13043539,Tool to detect permission store inconsistencies,"I think we should prepare for cases where the permission store (managed as a tree mirrored to the content tree) goes out of sync with the content tree for whatever reason.

Ideally, that would be an online tool (maybe exposed via JMX) that goes back the MVCC revisions to find the offending commit (so that have a chance to reduce the number of such occurences) and fixes the inconsistency on head.",production resilience tools,['core'],OAK,Sub-task,Minor,2017-02-16 08:13:33,0
13043378,Test failure: observation.ObservationQueueFullWarnTest.warnOnRepeatedQueueFull,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_RDB,profile=unittesting #452 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_RDB,profile=unittesting #452|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_RDB,profile=unittesting/452/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_RDB,profile=unittesting/452/console]",test-failure ubuntu windows,"['continuous integration', 'jcr']",OAK,Bug,Major,2017-02-15 20:40:46,11
13042975,InitialContent depends on document.bundlor.BundlingConfigInitializer,"[~chetanm], in the light of OAK-4975 a dependency to the document nodestore code got introduced in {{org.apache.jackrabbit.oak.plugins.nodetype.write.InitialContent}} by adding the following line:
{code}
        BundlingConfigInitializer.INSTANCE.initialize(builder);
{code}

the {{BundlingConfigInitializer}} is defined in the {{org.apache.jackrabbit.oak.plugins.document.bundlor}}.

To me that looks quite troublesome and I don't think the generic JCR-InitialContent should have any dependency on the document nodestore code base.

Why not defining a dedicated {{RepositoryInitializer}} for that kind of init an making sure it is listed in the (default) setup scenarios (or at least in those that actually have a document store and thus require this)?
",modularization tech-debt,['core'],OAK,Bug,Minor,2017-02-14 15:29:45,2
13042569,Revisit FileStoreStats mbean stats format,"This is a bigger refactoring item to revisit the format of the exposed data, moving towards having it in a more machine consumable friendly format.",monitoring,['segment-tar'],OAK,Improvement,Major,2017-02-13 13:06:28,15
13042568,Expose IOMonitor stats via JMX,"Followup of OAK-5632 and OAK-5631, to expose the collected data via JMX for external use.",monitoring,['segment-tar'],OAK,New Feature,Minor,2017-02-13 13:04:48,15
13041869,Test failure: org.apache.jackrabbit.oak.cache.ConcurrentTest.testLoaderBlock,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_NS,profile=unittesting #445 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_NS,profile=unittesting #445|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_NS,profile=unittesting/445/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_NS,profile=unittesting/445/console]",test-failure windows,"['continuous integration', 'core']",OAK,Bug,Major,2017-02-09 22:40:58,4
13041683,Simplify consistency check,"The current implementation of the consistency check ({{ConsistencyChecker}},{{CheckCommand}})
is cluttered with unnecessary checks regarding deprecated arguments of the {{check}} command. 
With OAK-5595, deep traversals are enabled by default, therefore the code needs to be revised to take this into account. The same applies to the argument taken by {{--bin}} option, which was removed in OAK-5604.

Moreover, {{ConsistencyChecker}} could be refactored in order to better distinguish when:
* a full path at the given revision is checked
* a node and its properties are checked
* a node and its descendants are checked",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2017-02-09 12:26:23,1
13041595,Test failure: segment.standby.ExternalSharedStoreIT.testProxyFlippedIntermediateByteChange2,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=integrationTesting #443 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=integrationTesting #443|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=integrationTesting/443/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=integrationTesting/443/console]",test-failure windows,"['continuous integration', 'segmentmk']",OAK,Bug,Major,2017-02-09 05:54:16,12
13041591,Test failure: org.apache.jackrabbit.oak.run.osgi.DocumentNodeStoreConfigTest.testRDBDocumentStoreRestart,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1412 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1412|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1412/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1412/console]",test-failure ubuntu windows,"['continuous integration', 'pojosr']",OAK,Bug,Major,2017-02-09 05:14:55,3
13041582,Test failure: org.apache.jackrabbit.mk.util.CommitGateIT.test,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #443 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #443|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/443/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/443/console]",test-failure windows,['continuous integration'],OAK,Bug,Major,2017-02-09 04:04:42,2
13041547,Test failure: security.authentication.ldap.LdapProviderTest.testGetUnknownUserByRef,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=unittesting #441 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=unittesting #441|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=unittesting/441/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=unittesting/441/console]",test-failure windows,"['auth-ldap', 'continuous integration']",OAK,Bug,Major,2017-02-09 00:40:53,13
13041411,"The check command should accept a non-argument ""bin"" option for checking binaries","Currently the {{--bin}} option expects a {{Long}} argument, as the {{LENGTH}} up to which to scan the content of binary properties. The {{--bin}} option should be simplified so that it doesn't take any arguments. Running {{check}} without the {{--bin}} flag won't scan any binary properties, while including {{--bin}} option will scan all binaries, no matter their size.

If an argument is given with {{--bin}}, there will be a failure and a warning will be displayed.

The message displayed at the end of the consistency check will be changed to take into account whether binary properties were traversed or not.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2017-02-08 15:25:25,1
13041376,Test failure: oak.upgrade.cli.blob.CopyBinariesTest,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_NS,profile=unittesting #438 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_NS,profile=unittesting #438|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_NS,profile=unittesting/438/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_NS,profile=unittesting/438/console]",test-failure windows,"['continuous integration', 'upgrade']",OAK,Bug,Major,2017-02-08 13:05:44,18
13041330,Test coverage for CheckCommand,"We should add tests for {{o.a.j.o.r.CheckCommand}} in order to validate recent changes introduced by adding/removing options and their arguments (see OAK-5275, OAK-5276, OAK-5277, OAK-5595). There is also a new feature introduced by OAK-5556 (filter paths) and a refactoring in OAK-5620 which must be thoroughly tested in order to avoid regressions.",tooling,"['run', 'segment-tar']",OAK,Task,Minor,2017-02-08 09:39:12,1
13041209,Slim oak-run,oak-run is rather big (50MB). Let's see what we can do to slim it down,tooling,['run'],OAK,Epic,Major,2017-02-07 23:32:47,22
13040675,The check command should do deep traversals by default,"Only checking accessibility of the root nodes doesn't make much sense. Even more so because the file store automatically rolls back on startup if a root revision is not accessible. In terms of not doing full traversals, it is more interesting to restrict by path (aka OAK-5556).

The {{--deep}} option will still be accepted, but there will be a failure when it is specified. An explanation that full traversal is now done regardless of that option will be printed.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2017-02-06 16:08:57,1
13040625,"The check command doesn't do any check when ""deep"" option is not provided","When the {{check}} command is used without {{--deep}} option, there is no check/traversal being done against the repository.

First relevant line in code is [1], where a check is supposed to happen, but due to a mismatch between argument expected/argument provided, {{null}} is always returned without checking anything. The method which should do the actual check [2] expects a set of paths to be traversed, but this set is always empty. Therefore, relevant code for running the check is never executed [3].

[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L120
[2] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L183
[3] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L194",tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-02-06 12:51:21,1
13040030,org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1399 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1399|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1399/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1399/console]",cold-standby test-failure ubuntu windows,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-02-03 04:10:23,12
13039435,Reduce reads with overlapping previous documents,"Reading a node state in a past revision can become expensive when the change history in the previous documents have overlapping changes. In this case, the changes in the previous documents must be merge sorted to find the correct value for the properties on the node. The more overlapping ranges there are, the more sorting is needed.

There is a prominent node in the repository that seems to create quite many of those previous documents. The {{/:async}} nodes gets frequent updates and is therefore split on a regular basis. Because the properties on this node are not all updated at the same time, it is quite likely that previous document ranges overlap.",candidate_oak_1_4 observation performance,"['core', 'documentmk']",OAK,Improvement,Major,2017-02-01 11:19:24,2
13039208,Allow filter paths for Check command,It would be good if the {{check}} command would allow for filtering on content path. This would help in quickly identifying what is the good revision of a specific broken node in cases of very large repos.,tooling,"['segment-tar', 'segmentmk']",OAK,New Feature,Major,2017-01-31 15:01:09,1
13038925,Document TarMK design,"We should improve our documentation of the internal design of the the TarMK. There is currently a [single section|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#design]. 

* Add a high level class diagram and description of the overall structure of the TarMK. 
* Decide what to do with {{segmentmk.md}}. My preference would be to incorporate everything from it we didn't cover so far into {{segment/overview.md}}, {{segment/records.md}} and {{segment/tar.md}}. 
* Rewrite, clarify the design section in {{segment/overview.md}}. 
",documentation,['segment-tar'],OAK,Technical task,Major,2017-01-30 16:13:41,12
13038747,Improve indexing resilience,grouping the improvements for indexer resilience in this issue for easier tracking,resilience,['lucene'],OAK,Epic,Critical,2017-01-29 12:22:52,19
13038623,Test failure: security.authentication.ldap.LdapProviderTest (Address already in use),"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting #1390 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting #1390|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting/1390/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting/1390/console]",test-failure ubuntu windows,"['auth-ldap', 'continuous integration']",OAK,Bug,Major,2017-01-28 06:53:48,15
13038418,Test failure: standalone.RepositoryBootIT.repositoryLogin,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1386 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1386|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1386/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1386/console]",test-failure,['continuous integration'],OAK,Bug,Major,2017-01-27 11:19:23,19
13038374,Test failure: segment.standby.ExternalSharedStoreIT/BrokenNetworkTest.test...,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1384 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1384|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1384/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1384/console]",test-failure ubuntu,"['continuous integration', 'segmentmk']",OAK,Bug,Major,2017-01-27 06:57:47,12
13037891,Skip problematic binaries instead of blocking indexing,"If a text extraction is blocked (weird PDF) or a blob cannot be found in the datastore or any other error upon indexing one item from the repository that is outside the scope of the indexer, it currently halts the indexing (lane). Thus one item (that maybe isn't important to the users at all) can block the indexing of other, new content (that might be important to users), and it always requires manual intervention  (which is also not easy and requires oak experts).

Instead, the item could be remembered in a known issue list, proper warnings given, and indexing continue. Maintenance operations should be available to come back to reindex these, or the indexer could automatically retry after some time. This would allow normal user activity to go on without manual intervention, and solving the problem (if it's isolated to some binaries) can be deferred.

I think the line should probably be drawn for binary properties. Not sure if other JCR property types could trigger a similar issue, and if a failure in them might actually warrant a halt, as it could lead to an ""incorrect"" index, if these properties are important. But maybe the line is simply a try & catch around ""full text extraction"".",resilience,['indexing'],OAK,New Feature,Major,2017-01-25 23:45:43,4
13037770,SNFE when running compaction after a cancelled gc,When a gc run is cancelled the subsequent run (compaction phase to be precise) can result in a {{SNFE}}. The reason for this is the node deduplication cache not being purged in the cancellation case. This causes the subsequent compaction to reference node states from that cache that have been cleaned. ,compaction gc,['segment-tar'],OAK,Bug,Blocker,2017-01-25 17:16:16,15
13037624,Standby Automatic Cleanup should be on by default,"The OSGi setting controlling standby automatic cleanup, {{standby.autoclean}}, should be set to {{true}} by default. When the automatic cleanup is on, the {{cleanup()}} method will be called on standby, provided the size of the store increases over 25% on a sync cycle.",cold-standby,['segment-tar'],OAK,Improvement,Minor,2017-01-25 09:06:49,12
13037570,Reduce lookup for oak:index node under each changed node,"Currently {{IndexUpdate}} does a lookup for {{oak:index}} node under each changed node. This is done to pickup index definitions and create IndexEditor based on those so as to index content under that subtree. 

This lookup results in extra remote calls on DocumentNodeStore based setup as for non leaf nodes NodeStore has to check from remote storage to determine if {{oak:index}} node is present or not.

This lookup can be avoided by
# Having an {{Editor}} which adds a hidden property {{:oak-index-present}} in parent node upon addition of {{oak:index}} as a child node
# IndexUpdate would then does a lookup for {{oak:index}} node only if such a hidden property is found

For upgrade we would have some logic which would make use of Nodetype index to identify all such nodes and mark them

Discussion [thread|https://lists.apache.org/thread.html/70d5ffff0f950d7fc25bc1bbb41527f5672825f8cf2b238f54df2966@%3Coak-dev.jackrabbit.apache.org%3E] on oak-dev",performance,['indexing'],OAK,Improvement,Major,2017-01-25 04:10:07,19
13036530,Allow to migrate only paths matching given path fragment,"Let's introduce a new option, {{\-\-name\-fragments}}. It'll traverse the paths specified in the {{\-\-include\-paths}}, looking for nodes which contain specified strings in their names. Only such names (and their subtrees) will be migrated.

It can be used to migrate the contents that belongs to the given mount, eg. by using:

{noformat}
--include-paths=/oak:index --name-fragments=:oak:mount-libs
{noformat}",multiplexing,['upgrade'],OAK,Improvement,Major,2017-01-20 11:41:31,18
13036529,Allow to disable mounting path fragments in the MultiplexingNodeStore,"Let's assume we have a working instance with multiplexing in place, the /libs is mounted in the read-only mode. Now, when an index is created or modified, the whole repository (including /libs) will be reindexed. Then, the IndexUpdate will try to store part of the generated data to {{/oak:index/lucene/:oak:mount-libs-index-data}}. The MultiplexingNodeStore won't allow this (as the libs store is a read-only one) - so the whole reindexing operation will fail and be started again.

A solution proposal: let's allow to disable mounting the path fragments. Indexing data for the libs mount will be stored on the main repository, but under a separate path.",multiplexing,['core'],OAK,Improvement,Major,2017-01-20 11:40:45,18
13036145,Test failure: LdapDefaultLoginModuleTest address already in use,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1375 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1375|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1375/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1375/console]",test-failure ubuntu windows,"['auth-ldap', 'continuous integration']",OAK,Bug,Major,2017-01-19 09:12:56,15
13036089,Test failure: LdapProviderTest - Address already in use,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1374 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1374|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1374/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1374/console]",test-failure,"['auth-ldap', 'continuous integration']",OAK,Bug,Major,2017-01-19 04:09:22,22
13035890,Overdue document split with many cluster nodes,"There are cases where a document split is overdue and it continues to grow until it hits the MongoDB limit of 16MB.

It gets more likely, the more cluster nodes are running and when they all update the same property with a somewhat larger value. E.g. the :childOrder property of a node with many children.

The current split logic has multiple triggers that will result in creating a previous document.

- There are 100 old changes for a cluster node that can be moved
- A node was recreated with a binary bigger than 4k
- The main document is bigger than 256k and 30% of its size can be moved to a previous document

The last condition may cause the uncontrolled growth of the document when there are many cluster nodes. If all cluster nodes continuously change a property on a node, then none of the cluster nodes will be able to move 30% of the document.",candidate_oak_1_4,"['core', 'documentmk']",OAK,Bug,Minor,2017-01-18 14:36:33,2
13035770,Too many files with unapproved license,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=rat #1372 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=rat #1372|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=rat/1372/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=rat/1372/console]",licensing,['continuous integration'],OAK,Bug,Major,2017-01-18 04:53:45,16
13035466,Improve the transaction rate of the TarMK,"The TarMK's write throughput is limited by the way concurrent commits are processed: rebasing and running the commit hooks happen within a lock without any explicit scheduling. This epic covers improving the overall transaction rate. The proposed approach would roughly be to first make scheduling of transactions explicit, then add monitoring on transaction to gather a better understanding and then experiment and implement explicit scheduling strategies to optimise particular aspects. 

h2. Summary of ideas mentioned in an offline sessions

h3. Advantages of explicit scheduling:
* Control over (order) of commits
* Sophisticated monitoring (commit statistics, e.g. commit rate, time in queue, etc.) 
* Favour certain commits (e.g. checkpoints)
* Reorder commits to simplify rebasing
* Suspend the compactor on concurrent commits and have it resume where it left off afterwards
* Parallelise certain commits (e.g. by piggy backing)
* Implement a concurrent commit editor. we'd need to take care of proper access to the shared state; [~frm] maybe introduce the idea of a common context to enforce concurrent access semantics.

h3. Scheduler Implementation
* Expedite
* Prioritise
* Defer
* Collapse
* Coalesce
* Parallelise
* Piggy back: can we piggy back commits on top of each other? The idea would be while processing the changes of one commit to also check them for conflicts with the changes of other commits waiting to commit. If a conflict is detected there, that other commit can immediately be failed (given the current commit doesn't fail).
* Merging non conflicting commits. Given multiple transactions ready to commit at the same time. Can we process them as one (given they don't conflict) instead of one after each other, which requires rebasing the later transaction to be rebase on the former.
* Shield the file store from {{InterruptedException}} because of thread boundaries introduced
* Implement tests, benchmarks and fixtures for verification
",scalability,['segment-tar'],OAK,Epic,Major,2017-01-17 10:45:57,15
13035392,Test failure: RepositoryBootIT.repositoryLogin,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1369 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1369|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1369/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1369/console]",test-failure ubuntu,"['continuous integration', 'examples']",OAK,Bug,Major,2017-01-17 04:01:49,19
13033612,Test failure: BasicServerTest.testServerOk() Address already in use,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1363 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1363|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1363/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1363/console]",test-failure,"['continuous integration', 'run']",OAK,Bug,Major,2017-01-11 06:43:51,19
13033423,Add a persistence-dependent namespace when running CLI commands,"Commands in oak-run currently live in a flat namespace. If a command is specific to only one implementation, it will leave along other implementation-specific commands without any means of distinguishing what belongs where.

I would like to add a layer of indirection to the oak-run command line interface, so to parse commands in the following fashion:

{noformat}
oak-run segment debug /path/to/folder
oak-run mongo debug mongodb://host:12345
oak-run rdb debug jdbc:oracle:oci8:scott/tiger@myhost
{noformat}

In this scenario, oak-run would become a simple entry point that would delegate to implementation-specific command line utilities based on the first argument. In the previous example, {{segment}}, {{mongo}} and {{rdb}} would delegate to three different implementation specific CLI utilities. Each of these CLI utilities will understand the {{debug}} command and will collect command-line parameters as it sees fit.

If the code for a command is so generic that can be reused from different commands, it can be parameterised and reused from different implementation-specific commands.

The benefit of this approach is that we can start moving commands closer to the implementations. This approach would benefit oak-run as well, which is overloaded with many commands from many different implementations.",tooling,['run'],OAK,Improvement,Major,2017-01-10 15:45:25,12
13033355,Skip processing of queued changes if async index update is detected in ExternalIndexObserver,"ExternalIndexObserver is currently backed by a queue (its wrapped in BackgroundObserver). Currently it processed the changes one by one as received from the queue. If this processing takes long time then its possible that it would lag behind the async indexing cycle.

So ExternalIndexObserver may be busy indexing changes from [r1-r2] but async indexing is already done indexing changes upto r3 (r3 > r2) and IndexTracker would move to newer index version. In such case work done by ExternalIndexObserver is wasted. 

This can be optimized by ensuring that ExternalIndexObserver can see the lastIndexTo of :async as per latest entry in queue. If that is newer than one its processing then it can skip processing the queue entry and thus free up space in queue",performance,['lucene'],OAK,Improvement,Major,2017-01-10 12:00:48,19
13033276,Test Failure: org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.compactionNoBinaryClone,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1360 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1360|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1360/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1360/console]",test-failure,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-01-10 05:38:07,15
13032769,Test failure: TomcatIT.testTomcat(),"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1357 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1357|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1357/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1357/console]",test-failure ubuntu,"['continuous integration', 'examples']",OAK,Test,Major,2017-01-07 04:04:59,19
13032554,Async reindex of a sync property does't release created checkpoint,"Async reindex of a property index creates a checkpoint to use as a reference, but it fails to clean it up when done. In the usual reindexing scenario the async indexer needs to keep the created checkpoint as a reference for subsequent runs, but this is a 'one off' case, so cleanup of this reference checkpoint must also happen at the end of the cycle.",candidate_oak_1_4,['core'],OAK,Bug,Minor,2017-01-06 14:09:54,14
13032466,Test failure: org.apache.jackrabbit.oak.run.osgi.DocumentNodeStoreConfigTest,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1356 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1356|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1356/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1356/console]",test-failure ubuntu,"['continuous integration', 'pojosr']",OAK,Bug,Major,2017-01-06 04:38:31,3
13032183,Test failure: segment.standby.BrokenNetworkTest,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1355 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1355|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1355/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1355/console]

Initially reported test failure: testProxyFlippedStartByte()
Additional test failure reported via OAK-5476: testProxySSLSkippedBytes()
Additional test failure reported via OAK-5477: testProxyFlippedStartByteSSL()
Additional test failures reported via OAK-5478: all tests failed",test-failure,"['continuous integration', 'tarmk-standby']",OAK,Bug,Major,2017-01-05 07:42:26,12
13031946,Test failure:  SolrIndexQueryTestIT.testNativeMLTQuery and testNativeMLTQueryWithStream,"{noformat}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.testNativeMLTQueryWithStream(SolrIndexQueryTestIT.java:286)

java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.testNativeMLTQuery(SolrIndexQueryTestIT.java:264)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
{noformat}",test-failure ubuntu,"['continuous integration', 'solr']",OAK,Bug,Major,2017-01-04 11:52:14,10
13030476,Test failure: ...stats.ClockTest.testClockDrift,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1351 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1351|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1351/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1351/console]",test-failure,['continuous integration'],OAK,Bug,Major,2016-12-24 07:30:44,15
13030290,Test failure: persistentCache.BroadcastTest.broadcastTCP,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1347 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1347|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1347/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1347/console]

{noformat}
broadcastTCP(org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest)  Time elapsed: 0.047 sec  <<< FAILURE!
java.lang.AssertionError: expected null, but was:<Hello World 1>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotNull(Assert.java:755)
	at org.junit.Assert.assertNull(Assert.java:737)
	at org.junit.Assert.assertNull(Assert.java:747)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest.broadcastTry(BroadcastTest.java:220)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest.broadcast(BroadcastTest.java:195)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest.broadcastTCP(BroadcastTest.java:146)
{noformat}
",test-failure ubuntu,"['continuous integration', 'core']",OAK,Bug,Major,2016-12-23 03:43:46,4
13030176,Test failure: ConcurrentQueryAndUpdateIT.cacheUpdate[RDBFixture: RDB-H2(file)],"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1345 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1345|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1345/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1345/console]

{noformat}
cacheUpdate[RDBFixture: RDB-H2(file)](org.apache.jackrabbit.oak.plugins.document.ConcurrentQueryAndUpdateIT)  Time elapsed: 54.353 sec  <<< FAILURE!
java.lang.AssertionError: Unexpected revision timestamp for 1:/node-39 expected:<867> but was:<866>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.jackrabbit.oak.plugins.document.ConcurrentQueryAndUpdateIT.cacheUpdate(ConcurrentQueryAndUpdateIT.java:80)

Failed tests:   cacheUpdate[RDBFixture: RDB-H2(file)](org.apache.jackrabbit.oak.plugins.document.ConcurrentQueryAndUpdateIT): Unexpected revision timestamp for 1:/node-39 expected:<867> but was:<866>
{noformat}

Also seen on Windows: https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=DOCUMENT_NS,profile=integrationTesting/448/",Windows test-failure,"['continuous integration', 'rdbmk']",OAK,Bug,Major,2016-12-22 16:38:22,3
13029783,Cancelled garbage collection not reported to GCMonitor,When revision garbage collection is cancelled because one of the conditions in {{CancelCompactionSupplier}} then this should be reported to {{GCMonitor.skipped}}. Currently it is not. ,gc monitoring production technical_debt,['segment-tar'],OAK,Bug,Major,2016-12-21 15:19:49,15
13029507,Test failure: CompactionAndCleanupIT,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1338 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1338|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1338/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1338/console]",test-failure,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2016-12-20 16:49:36,15
13029502,Too eager refreshing of tree permissions in SecureNodeBuilder,"{{SecureNodeBuilder.baseChanged()}} calls {{SecureNodeBuilder.getTreePermission()}} even though the tree permission would be calculated lazily as needed anyway. Re-calculating the tree permissions at this point bears the risk of accessing stale data from the underlying not yet fully refreshed root (when being called e.g. from {{MutableRoot.refresh()}}. 

I would thus argue for removing the call to {{SecureNodeBuilder.getTreePermission()}} from {{SecureNodeBuilder.baseChanged()}}. 

See also OAK-5296 for an in-depth analysis. ",technical_debt,['core'],OAK,Bug,Major,2016-12-20 16:34:42,0
13029416,Enable RevisionGC task for non primary SegmentNodeStore,As explained in OAK-5351 the RevisionGCMBean is not being registered for secondary SegmentNodeStore. This task is meant to enable that once OAK-5309 is resolved ,gc monitoring operations secondary-nodestore,['segment-tar'],OAK,Task,Major,2016-12-20 10:49:01,18
13029400,Improve code coverage of oak-segment-tar,Improve code coverage of oak-segment-tar.,technical_debt,['segment-tar'],OAK,Improvement,Major,2016-12-20 09:57:22,12
13029134,Clarify the various directories and their usages in SegmentNodeStoreService,"In {{SegmentNodeStoreService}} there is {{repository.home}}, {{DIRECTORY}}, {{getRootDirectory()}}, {{getDirectory()}} and {{getBaseDirectory()}} mostly without documentation about their intention. I think we should clarify, document and consolidate them. ",technical_debt,['segment-tar'],OAK,Task,Major,2016-12-19 13:27:31,1
13028399,Remove ReversedLinesFileReaderTestParamBlockSize,"{{org.apache.jackrabbit.oak.segment.file.ReversedLinesFileReaderTestParamBlockSize}} should actually have been removed with OAK-4467, where we replaced our {{ReversedLinesFileReader}} implementation with the one from {{commons-io}}. ",technical_debt,['segment-tar'],OAK,Task,Minor,2016-12-15 14:46:13,15
13028126,Remove legacy upgrade code from AbstractFileStore.collectFiles,{{AbstractFileStore.collectFiles()}} contains legacy upgrade code dating back to special handling of binaries in older version of {{oak-segment}} (bulkFiles). We should remove this code. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2016-12-14 15:34:03,15
13028123,Possible null dereference in MapRecord,"{{MapEntry.compareTo()}} passes possibly {{null}} {{MapEntry.value}} to {{ComparisonChain.compare(Comparable, Comparable)}}, which does not accept {{null}} values. ",technical_debt,['segment-tar'],OAK,Bug,Major,2016-12-14 15:22:44,15
13027751,Static code analysis and code cleanup,"We should run some static analysis (i.e. sonar, find bugs, etc.) on our code base and fix the most sever issues. ",technical_debt,['segment-tar'],OAK,Task,Major,2016-12-13 10:28:28,15
13027713,The tarmkdiff command does too many things,"The {{tarmkdiff}} command is actually the combination of two commands. 

The first command, activated when the {{\-\-list}} flag is specified, list available revisions in the Segment Store. For this command, only the {{\-\-output}} option is relevant. If other options are specified, they are ignored.

The second command is the proper logic of {{tarmkdiff}}. This logic is activated only if the {{\-\-list}} flag is not specified. For this command, every option on the command line is relevant.

The logic listing available revisions in the Segment Store should be encapsulated in its own command, without cluttering the CLI of {{tarmkdiff}}.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-13 08:01:58,12
13027472,"The check command defines a useless default value for the ""bin"" option","The {{check}} command enables the traversal of binary properties via the {{--bin}} option. The user could provide a value for this option to specify the amount of bytes that should be traversed for every binary value. The default value for the {{--bin}} option is zero, effectively disabling the traversal of binary properties. Instead, if a value for this property is not specified, the tools should traverse the binary properties in their entirety. A value should be specified only to restrict the amount of bytes to traverse.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-12 14:59:27,1
13027471,"The check command overloads the meaning of the ""deep"" option","The {{--deep}} option accepted by the {{check}} command is semantically overloaded. It is used both as a flag to enable deep content traversal and as a way to specify the frequency of debug messages printed by the tool. 

This option should be split in two. In particular, {{--deep}} should retain its behaviour of on/off flag for deep traversal, and a new command line option should be introduced to specify the interval of debug messages.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-12 14:52:58,1
13027470,The check command should accept the path to the store as a positional argument,The {{check}} tool requires the path to the store to be specified. The path is passed to the tool via a required option {{--path}}. This way of specifying the path to the store is verbose for no good reason. It would be nicer if the path to the Segment Store would be specified via a positional argument instead.,tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-12 14:48:33,1
13027169,Test failure: SolrIndexHookIT.testPropertyAddition,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting #1323 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting #1323|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting/1323/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting/1323/console]",test-failure ubuntu,"['continuous integration', 'solr']",OAK,Bug,Major,2016-12-10 04:52:47,10
13027002,Better default for size delta estimation ,The default value for the size delta estimation used during garbage collection should be changed to 1GB.,gc,['segment-tar'],OAK,Bug,Major,2016-12-09 13:47:59,12
13026547,Test failure: ExternalPrivateStoreIT. testSyncUpdatedBinaryProperty(),"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1320 has failed.
First failed run: [Apache Jackrabbit Oak matrix/jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1320|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1320/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1320/console]",test-failure,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2016-12-08 03:50:37,12
13025018,Avoid updating the index nodestate if no change is done in index,"As noted in OAK-5211 directory listing was getting modified (due to reorder) even if no change happens in index. 

Another place where we update state post index close is at "":status"" node where we store {{lastUpdated}} and {{indexedNodes}} post index close. In normal cases LuceneIndexEditor avoids initializing the IndexWriter if there is no change. However it can happen that when any node gets deleted the editor performs a delete operation. It can happen that tree being deleted is not indexed but still editor would do this as it cannot determine that easily. And in doing that IndexWriter would be initialized.

Currently IndexWriter being initialized is considered same as index updated. Due to this index status nodes gets unnecessarily updated even if there is no change in index which causes the IndexTracker to reopen the index even when it has not changed. 

We should make this more explicit and find a way to determine if index has been updated or not",performance,['lucene'],OAK,Improvement,Minor,2016-12-02 08:59:46,19
13025009,OakDirectory should not save dir listing if no change is done,"[~alex.parvulescu] noted that OakDirectory saves the directory listing even if no actual change happened in the directory. Only change that happens is the order of entries in set. 

In normal cases LuceneIndexEditor avoids initializing the IndexWriter if there is no change. However it can happen that when any node gets deleted the editor performs a delete operation. It can happen that tree being deleted is not indexed but still editor would do this as it cannot determine that easily. This would lead to OakDirectory being closed without any change and thus can lead save of dir listing with just change in order of entries",performance,['lucene'],OAK,Improvement,Minor,2016-12-02 08:09:35,19
13024402,Reduce Lucene related growth of repository size,"I observed Lucene indexing contributing to up to 99% of repository growth. While the size of the index itself is well inside reasonable bounds, the overall turnover of data being written and removed again can be as much as 99%. 

In the case of the TarMK this negatively impacts overall system performance due to fast growing number of tar files / segments, bad locality of reference, cache misses/thrashing when looking up segments and vastly prolonged garbage collection cycles.",perfomance scalability,"['lucene', 'segment-tar']",OAK,Improvement,Major,2016-11-30 11:34:20,10
13024389,Deprecate stubs and fixtures related to oak-segment,I would like to deprecate the various fixtures and stubs for {{oak-segment}} and replace them where possible with its corresponding variants of {{oak-segment-tar}},deprecation technical_debt,"['commons', 'it', 'jcr', 'lucene', 'run', 'segmentmk', 'solr', 'tarmk-standby']",OAK,Improvement,Major,2016-11-30 10:46:41,15
13023967,Non default MissingIndexProviderStrategy is not being passed to child editor,"{{IndexUpdate}} allows passing in a custom {{MissingIndexProviderStrategy}}. However the custom provider is only stored as instance variable in {{IndexUpdate}} and does not get passed to child IndexUpadate instance.

As a fix it should be stored in IndexUpdateRootState and accessed from that",candidate_oak_1_4,['core'],OAK,Bug,Minor,2016-11-29 05:30:04,19
13023742,Get rid of test dependency to json.org JSON parser,"See <http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201611.mbox/%3C0CE2E8C9-D9B7-404D-93EF-A1F8B07189BF%40apache.org%3E>

",legal,['remoting'],OAK,Task,Blocker,2016-11-28 14:16:12,12
13023222,Increase default size of the observation queue from 1000 to 10000,"To mitigate problems when hitting the observation queue limit (OAK-2683) we should bump the default size from 1000 to 10000.

cc [~stefanegli]",resilience,"['core', 'jcr']",OAK,Improvement,Blocker,2016-11-24 15:49:21,15
13022465,Collect stats around number of nodes traversed by AsyncIndexer,"At times we see very long time in async index update cycle

{noformat}
06.11.2016 18:16:18.703 *INFO* [aysnc-index-update-async] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate [async] AsyncIndex update run completed in 25.58 min. Indexed 7498 nodes
06.11.2016 18:41:43.088 *INFO* [aysnc-index-update-async] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate [async] AsyncIndex update run completed in 24.79 min. Indexed 28335 nodes
{noformat}

It would be good to also include the number of nodes traversed in that diff. For the record such high times were seen on a setup which did not had persistent cache enabled which probably caused slow diff",candidate_oak_1_4,['query'],OAK,Improvement,Minor,2016-11-22 10:32:23,19
13020824,Change default size of the node deduplication cache,The current default of the node deduplication cache is 8M. We should consider changing this to a smaller value still resulting in effective compactions. ,compaction gc perfomance,['segment-tar'],OAK,Improvement,Major,2016-11-15 15:16:07,15
13020468,Backup is not incremental i.e. previous tar files are duplicated,"Performing two backups via {{RepositoryManagementMBean.startBackup}}, directory size increases not only with the delta, but also again with the size of existing tar files. This lead me to the conclusion that backup is not incremental.",operations production tooling,['segment-tar'],OAK,Improvement,Minor,2016-11-14 10:26:15,1
13020123,improve DocumentNodeStoreService robustness for RDB configs,"It appears that very strange things can happen when the system isn't configured properly, for instance when multiple DataSources with the same name are configured.

TODO:

- log.info unbindDataSource() calls
- prevent registrations of multiple node stores when bindDataSource() is called multiple times
- in unbindDataSource(), check that the datasource is actually the one that was bound before
",patch-available resilience,"['documentmk', 'rdbmk']",OAK,Technical task,Minor,2016-11-11 14:47:36,3
13018193,Journal diff not working for changes in bundled node,For changes in bundled nodes diff is not reporting change in properties of bundled node,bundling,['documentmk'],OAK,Bug,Major,2016-11-04 18:23:37,19
13017939,Test failure in DocumentNodeStoreConfigTest.testRDBDocumentStore_CustomBlobDataSource,"Saw a test failure [here|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/1262/jdk=JDK%201.7%20(latest),label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=unittesting/testReport/junit/org.apache.jackrabbit.oak.run.osgi/DocumentNodeStoreConfigTest/testRDBDocumentStore_CustomBlobDataSource/]

In the logs following can be seen
{noformat}
Caused by: org.h2.jdbc.JdbcSQLException: Syntax error in SQL statement ""create alias if not exists unix_timestamp as [*]$$ long unix_timestamp() { return System.currentTimeMillis()/1000L; }""; SQL statement:
create alias if not exists unix_timestamp as $$ long unix_timestamp() { return System.currentTimeMillis()/1000L; } $$; [42000-193]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.message.DbException.get(DbException.java:179) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.message.DbException.get(DbException.java:155) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.message.DbException.getSyntaxError(DbException.java:191) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.Parser.getSyntaxError(Parser.java:530) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.Parser.checkRunOver(Parser.java:3694) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.Parser.initialize(Parser.java:3559) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.Parser.parse(Parser.java:304) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.Parser.parse(Parser.java:293) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.CommandContainer.recompileIfRequired(CommandContainer.java:73) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.CommandContainer.update(CommandContainer.java:93) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.command.Command.executeUpdate(Command.java:258) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.jdbc.JdbcStatement.executeInternal(JdbcStatement.java:184) ~[h2-1.4.193.jar:1.4.193]
	at org.h2.jdbc.JdbcStatement.execute(JdbcStatement.java:158) ~[h2-1.4.193.jar:1.4.193]
	at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.initialize(RDBDocumentStore.java:802) ~[oak-core-1.6-SNAPSHOT.jar:1.6-SNAPSHOT]
	at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.<init>(RDBDocumentStore.java:209) ~[oak-core-1.6-SNAPSHOT.jar:1.6-SNAPSHOT]
	... 26 common frames omitted
03.11.2016 14:52:10.662 *ERROR* [CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreService)] org.apache.jackrabbit.oak-core [org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreService(23)] Failed creating the component instance; see log for reason
{noformat}",test,['documentmk'],OAK,Test,Minor,2016-11-04 04:35:11,4
13017636,Improve GC scalability on TarMK,"This issue is about making TarMK gc more scalable: 
* how to deal with huge repositories.
* how to deal with massive concurrent writes.
* how can we improve monitoring to determine gc health. 
** Monitor deduplication caches (e.g. deduplication of checkpoints)

Possible avenues to explore:
* Can we partition gc? (e.g. along sub-trees, along volatile vs. static content)
* Can we pause and resume gc? (e.g. to give precedence to concurrent writes) 
* Can we make gc a real background process not contending with foreground operations? 

This issue is a follow up to OAK-2849, which was about efficacy of gc.",gc scalability,['segment-tar'],OAK,Epic,Major,2016-11-03 13:40:55,15
13017275,Optimise ImmutableRecordNumbers,{{ImmutableRecordNumbers}} is based on a map. This turns to be expensive as the items in the map are accessed very frequently. I would like to look into way of optimising this using a linear storage model instead. ,perfomance,['segment-tar'],OAK,Improvement,Major,2016-11-02 15:14:31,15
13017273,Add @Description annotations to methods in RepositoryManagementMBean,The methods in {{RepositoryManagementMBean}} do not provide any description to the end user and might seem unclear for someone trying to trigger them via JMX.,osgi-config,['segment-tar'],OAK,Task,Trivial,2016-11-02 15:13:14,1
13017185,Upgrade to Tika 1.15 version,"Oak Lucene index is currently using Tika 1.5 version while current latest release of Apache Tika is 1.14, I think there are lots of ""interesting"" bugs fixed, and possibly improvements (performance, more accurate text extraction, etc.) we could get at almost 0 cost by just bumping the version number.

Release notes https://tika.apache.org/1.15/index.html",candidate_oak_1_4,['lucene'],OAK,Improvement,Major,2016-11-02 11:50:17,19
13017154,Remove the old estimation OSGi setting (compaction.gainThreshold),"Currently, there are two implementations for finding out the gain in repository size after running compaction: the old one, {{CompactionGainEstimate}} and the new one, {{SizeDeltaGcEstimation}}. Similarly, there are also two configurations for customising them, in {{SegmentNodeStoreService}}, {{compaction.gainThreshold}} and {{compaction.sizeDeltaEstimation}}.

At the moment both of them are exposed as OSGi configurations, but only the new one should be exposed (e.g. {{compaction.sizeDeltaEstimation}}). 

It must be evaluated whether it makes sense to keep the logic associated with the old implementation.",osgi-config,['segment-tar'],OAK,Improvement,Minor,2016-11-02 10:19:33,1
13017105,Support bundling of nodes present in version store,"Bundling logic would not work for node structures which are present in versionstore i.e. nodes stored under /jcr:system/jcr:versionStorage as the nodes there always have type {{nt:frozenNode}}. So any node structure which gets version would not get benefit of bundling

Currently bundling logic looks for {{jcr:primaryType}} and {{jcr:mixinTypes}} for determining type information. To support bundling for nodes stored in version store we should also look for 

* jcr:frozenPrimaryType
* jcr:frozenMixinTypes

These properties contains the type information of original node stored which got versioned",bundling,['documentmk'],OAK,Improvement,Major,2016-11-02 06:47:31,19
13016843,Improve caching of segments,"Various aspects of how Segment Tar caches segments could possibly improved. The current cache is a result of replacing the former ad-hoc cache with a proper one in OAK-3055. While the former was prone to contention under concurrent load the current cache is too oblivious about reads: read accesses are always served through {{SegmentId.segment}} and never actually hit the cache. This results in frequently accessed segments not to be seen as such by the cache and potentially being prematurely evicted. 

Possibly approaches to address this problem include: 
* Reinstantiating the cache we had pre OAK-3055 but making in fully concurrent. 
* Convey the information about read accesses to the current cache. 
* In either of the above cases avoid bulk segments from being placed into the cache. ",performance scalability,['segment-tar'],OAK,Improvement,Major,2016-11-01 09:35:34,15
13015883,Copying the versions store is slow and increase the repository size,"This is a follow up to OAK-4970: when defining an workspace name that doesn't match the name of the workspace of the source repository (via the {{WORKSPACE_NAME_PROP}} system property), upgrading is still very slow and causes the repository size to grow way too much. 

",perfomance,['upgrade'],OAK,Improvement,Major,2016-10-27 16:14:37,18
13015847,Remove DocumentStore.update(),OAK-3018 removed the single production usage of DocumentStore.update(). I propose we remove the method to reduce maintenance.,technical_debt,"['documentmk', 'mongomk', 'rdbmk']",OAK,Task,Major,2016-10-27 14:41:54,2
13015787,Speed up ACE node name generation,"Currently, {{o.a.j.oak.security.authorization.accesscontrol.Util#generateAceName}} is traversing all the existing ACE of a certain node in order to generate continuous numbering (allow0, allow1, allow2).
While that certainly helps to produce human readable names, it represents quite a performance bottleneck when the number of existing ACE starts to grow.

Since the naming is a pure implementation detail, my proposal is to keep the continuous numbering for the first hundreds of nodes and then use a random number to generate unique names in a faster fashion.",performance,['core'],OAK,Improvement,Minor,2016-10-27 12:02:47,0
13015762,Improve the usage of the SegmentWriter for compaction,"I would like to improve how the {{SegmentWriter}} is used for compaction. In particular I dislike how the {{SegmentBufferWriter}} needs to be looped into {{SegmentWriter.writeNode()}}.
Furthermore creating a {{SegmentWriter}} for offline compaction with its own cache (instead of using the caches we have) is a bit wasteful wrt. to memory. ",refactoring,['segment-tar'],OAK,Improvement,Major,2016-10-27 10:13:52,15
13015707,Standby test failures,"I've recently seen a couple of the standby tests fail. E.g. on Jenkins: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/1245/

{noformat}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
	at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:122)
{noformat}

{noformat}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
	at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:122)
{noformat}

{{org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT.testProxySkippedBytes}}:
{noformat}
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-27 07:27:57,12
13015704,OOM in SegmentDataStoreBlobGCIT,"I've seen that test going OOM on our Jenkins: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/1245/#showFailuresLink

{noformat}
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.jackrabbit.oak.segment.SegmentDataStoreBlobGCIT.setUp(SegmentDataStoreBlobGCIT.java:220)
	at org.apache.jackrabbit.oak.segment.SegmentDataStoreBlobGCIT.setUp(SegmentDataStoreBlobGCIT.java:141)
	at org.apache.jackrabbit.oak.segment.SegmentDataStoreBlobGCIT.consistencyCheckInit(SegmentDataStoreBlobGCIT.java:317)
{noformat}",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-27 07:22:12,16
13015399,ExternalToExternalMigrationTest failures on Windows,"{noformat}
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 13.484 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.migration.ExternalToExternalMigrationTest
blobsCanBeReadAfterSwitchingBlobStore(org.apache.jackrabbit.oak.segment.migration.ExternalToExternalMigrationTest)  Time elapsed: 0.463 sec  <<< ERROR!
java.io.IOException: Unable to delete file: C:\tmp\1477483643533-0\segmentstore\data00001a.tar

blobsExistsOnTheNewBlobStore(org.apache.jackrabbit.oak.segment.migration.ExternalToExternalMigrationTest)  Time elapsed: 13.021 sec  <<< ERROR!
java.io.IOException: Unable to delete file: C:\tmp\1477483643996-0\segmentstore\data00000a.tar

Running org.apache.jackrabbit.oak.segment.migration.SegmentToExternalMigrationTest
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 12.719 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.migration.SegmentToExternalMigrationTest
blobsCanBeReadAfterSwitchingBlobStore(org.apache.jackrabbit.oak.segment.migration.SegmentToExternalMigrationTest)  Time elapsed: 0.157 sec  <<< ERROR!
java.io.IOException: Unable to delete file: C:\tmp\1477483657018-0\segmentstore\data00001a.tar

blobsExistsOnTheNewBlobStore(org.apache.jackrabbit.oak.segment.migration.SegmentToExternalMigrationTest)  Time elapsed: 12.561 sec  <<< ERROR!
java.io.IOException: Unable to delete file: C:\tmp\1477483657193-0\segmentstore\data00000a.tar
{noformat}",test-failure,"['core', 'segment-tar']",OAK,Bug,Major,2016-10-26 12:24:43,18
13015240,Offline compaction explodes checkpoints ,"Running offline compaction on a repository with checkpoints will explode those into full copies. Observed e.g. with OAK-5001. 

I think we should consider improving this by compacting checkpoints on top of each other in the proper order ({{oak-upgrade}} does this successfully). 

[~alex.parvulescu], WDYT? What was our take on this in the previous Oak versions? ",compaction gc tooling,['segment-tar'],OAK,Improvement,Major,2016-10-25 21:30:07,15
13014980,Persistent cache should not cache those paths which are covered by DocumentNodeStateCache,With OAK-4180 its possible to use a SegmentNodeStore as secondary store and thus like a cache for certain set of path. In such kind of setup persistent cache should not cache those NodeStates which are covered by DocumentNodeStateCache,secondary-nodestore,['documentmk'],OAK,Improvement,Major,2016-10-25 04:50:53,19
13014773,SegmentBufferWriter should not depend on SegmentTracker,"The former depends on the latter only for generation sequence numbers of segments, which are subsequently used to generate the segment meta information. I suggest to replace that dependency with a generalised one. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-10-24 16:00:36,15
13014772,Simplify GCListener,We should simplify {{GCListener}} to minimise the boilerplate necessary in {{FileStoreBuilder}}. ,technical_debt,['segment-tar'],OAK,Improvement,Major,2016-10-24 15:56:02,15
13014670,Server time unavailable with authenticated connection to MongoDB,"The MongoDocumentStore gets the current server time with the {{serverStatus}} command. When MongoDB is configured with authentication, the command may fail because it requires the [clusterMonitor|https://docs.mongodb.com/manual/reference/built-in-roles/#clusterMonitor] role.

The method will then simply log a WARN message and assume no time difference. Maybe there is a different command we can use to get the time on the server?",resilience,"['core', 'mongomk']",OAK,Bug,Minor,2016-10-24 09:59:25,2
13014649,Config option to disable specific bundling config,"With OAK-4975 Oak would be shipping some default bundling config. An application might want to disable such bundling and for those cases we need to support some config option to disable bundling for specific nodetypes.

*Proposal*

Have a boolean property {{disabled}} on bundling config for specific nodetype to indication that this bundling config is not to be used
",bundling docs-impacting,['documentmk'],OAK,Improvement,Minor,2016-10-24 07:53:54,19
13014175,Enable configuring QueryEngineSettings via OSGi config,"Oak QueryEngine exposes few settings options via {{QueryEngineSettings}}. Currently they can be configured via

# System properties
# JMX - The settings are not persistent 

We should have a way to configure them via OSGi also. A simple option can be to have a OSGi component which obtains a reference to {{QueryEngineSettingsMBean}} and then modifies the config upon activation",docs-impacting,['query'],OAK,Improvement,Minor,2016-10-21 09:26:39,19
13013781,Test failure: SegmentDataStoreBlobGCIT,"SegmentDataStoreBlobGCIT seems to crash the JVM on Java 7. Following is the relevant part of the build output.

{noformat}
[INFO] --- maven-failsafe-plugin:2.19.1:integration-test (default) @ oak-segment-tar ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.oak.segment.file.FileStoreIT
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.301 sec - in org.apache.jackrabbit.oak.segment.file.FileStoreIT
Running org.apache.jackrabbit.oak.segment.file.SegmentReferenceLimitTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec - in org.apache.jackrabbit.oak.segment.file.SegmentReferenceLimitTestIT
Running org.apache.jackrabbit.oak.segment.file.LargeNumberOfPropertiesTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.001 sec - in org.apache.jackrabbit.oak.segment.file.LargeNumberOfPropertiesTestIT
Running org.apache.jackrabbit.oak.segment.SegmentOverflowExceptionIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec - in org.apache.jackrabbit.oak.segment.SegmentOverflowExceptionIT
Running org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 45.78 sec - in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverSslTestIT
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.202 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverSslTestIT
Running org.apache.jackrabbit.oak.segment.standby.BrokenNetworkIT
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 63.024 sec - in org.apache.jackrabbit.oak.segment.standby.BrokenNetworkIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverMultipleClientsTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.052 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverMultipleClientsTestIT
Running org.apache.jackrabbit.oak.segment.standby.MBeanIT
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.287 sec - in org.apache.jackrabbit.oak.segment.standby.MBeanIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverIPRangeIT
Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.691 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverIPRangeIT
Running org.apache.jackrabbit.oak.segment.standby.StandbyTestIT
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.303 sec - in org.apache.jackrabbit.oak.segment.standby.StandbyTestIT
Running org.apache.jackrabbit.oak.segment.standby.RecoverTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.415 sec - in org.apache.jackrabbit.oak.segment.standby.RecoverTestIT
Running org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.002 sec - in org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
Running org.apache.jackrabbit.oak.segment.SegmentDataStoreBlobGCIT

Results :

Tests run: 65, Failures: 0, Errors: 0, Skipped: 3

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10:17 min
[INFO] Finished at: 2016-10-19T20:45:40+00:00
[INFO] Final Memory: 63M/553M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:2.19.1:integration-test (default) on project oak-segment-tar: Execution default of goal org.apache.maven.plugins:maven-failsafe-plugin:2.19.1:integration-test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /apps/jenkins/workspace/oak-segment-tar && /opt/jdk-7/jre/bin/java -Xmx512m -XX:MaxPermSize=64m -XX:+HeapDumpOnOutOfMemoryError -Dupdate.limit=100 -Djava.awt.headless=true -jar /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefirebooter4283069132546797078.jar /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefire8963659563100379656tmp /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefire_03767892930481742588tmp
{noformat}",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-20 08:02:49,12
13013555,Standalone tooling for segment tar,"Currently there is no easy way to run the tools provided by {{oak-run}} against a snapshot version of Segment Tar. In order to have better CI coverage (e.g. benchmarks) of Segment Tar, we need to introduce a way for running such tools independently of {{oak-run}}. Eventually {{oak-run}} should even be using that tooling front-end instead of directly depending on {{oak-segment-tar}}. ",tooling,['segment-tar'],OAK,New Feature,Major,2016-10-19 14:52:47,15
13013458,Review the support for wildcards in bundling pattern,"Bundling pattern currently supports wild card pattern. This makes it powerful but at same time can cause issue if it misconfigured. 

We should review this aspect before 1.6 release to determine if this feature needs to be exposed or not. ",bundling,['documentmk'],OAK,Task,Major,2016-10-19 10:01:25,19
13013457,Review the security aspect of bundling configuration,"The config for node bundling feature in DocumentNodeStore is currently stored under {{jcr:system/rep:documentStore/bundlor}}. This task is meant to 

* Review the access control aspect - This config should be only updatetable by system admin
* Config under here should be writeable via JCR api",bundling,['documentmk'],OAK,Task,Major,2016-10-19 09:59:05,19
13013452,Test failure: BrokenNetworkTest,"On some machines the {{BrokenNetworkTest}} fails:

{noformat}
Failed tests:
  BrokenNetworkTest.testProxyFlippedEndByteSSL:103->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedIntermediateByte:88->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedIntermediateByteSSL:93->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedStartByte:78->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySSLSkippedBytes:63->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySSLSkippedBytesIntermediateChange:73->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySkippedBytesIntermediateChange:68->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

Stack traces are all similar to 
{noformat}
testProxySkippedBytesIntermediateChange(org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest)  Time elapsed: 5.577 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.useProxy(BrokenNetworkTest.java:146)
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.useProxy(BrokenNetworkTest.java:113)
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.testProxySkippedBytesIntermediateChange(BrokenNetworkTest.java:68)
{noformat}",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-19 09:40:37,12
13013446,SegmentRevisionGC MBean should report more detailed gc status information  ,"Regarding this, the current ""Status"" is showing the last log info. This is useful, but it would also be interesting to expose the real-time status. For monitoring it would be useful to know exactly in which phase we are, e.g. a field showing on of the following:
- idle
- estimation
- compaction
- compaction-retry-1
- compaction-retry-2
- compaction-forcecompact
- cleanup


",gc monitoring,['segment-tar'],OAK,Improvement,Major,2016-10-19 09:09:55,12
13013225,SetPropertyTest benchmark fails on Segment Tar,"The {{SetPropertyTest}} fails on Oak Segment Tar:

{noformat}
javax.jcr.InvalidItemStateException: This item [/testfb3e8f1a/ca1ef350-f650-4466-b9e3-7f77d83e6303] does not exist anymore
	at org.apache.jackrabbit.oak.jcr.delegate.ItemDelegate.checkAlive(ItemDelegate.java:86)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl$ItemWriteOperation.checkPreconditions(ItemImpl.java:96)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$35.checkPreconditions(NodeImpl.java:1366)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.prePerform(SessionDelegate.java:615)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:205)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:112)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.internalSetProperty(NodeImpl.java:1363)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.setProperty(NodeImpl.java:506)
	at org.apache.jackrabbit.oak.benchmark.SetPropertyTest.runTest(SetPropertyTest.java:65)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.execute(AbstractTest.java:372)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.runTest(AbstractTest.java:221)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.run(AbstractTest.java:197)
	at org.apache.jackrabbit.oak.benchmark.BenchmarkRunner.main(BenchmarkRunner.java:456)
	at org.apache.jackrabbit.oak.run.BenchmarkCommand.execute(BenchmarkCommand.java:26)
	at org.apache.jackrabbit.oak.run.Mode.execute(Mode.java:63)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
{noformat}
",test-failure,['segment-tar'],OAK,Bug,Minor,2016-10-18 15:29:59,1
13013213,Optimise MutableRecordNumbers,{{MutableRecordNumbers}} is based on a map. This turns to be expensive as the items in the map are accessed very frequently. I would like to look into way of optimising this using a linear storage model instead. ,performance,['segment-tar'],OAK,Improvement,Major,2016-10-18 14:37:26,15
13013126,SegmentWriter buffers child node list changes,"The {{SegmentWriter}} currently buffers the list of child nodes changed on a nodestate update [0] (new node or updated node). This can be problematic in a scenario where there are a large number of children added to a node (ie. unique index size seen to spike above {{10MM}} in one case).

To have a reference for the impact of this, at the {{SegmentWriter}} level, for a list of map entries of almost {{3MM}} items, I saw it take up around {{245MB}} heap.

This issue serves to track a possible improvement here in how we handle this update scenario.




[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentWriter.java#L516",performance,['segment-tar'],OAK,Improvement,Major,2016-10-18 10:27:28,15
13012799,Provide status for gc process ,This is the {{oak-segment-tar}} side of OAK-4919,gc monitoring,['segment-tar'],OAK,Technical task,Major,2016-10-17 09:29:18,15
13012756,Isolate corrupted index and make async indexer more resilient,"Currently if any one of the async index gets corrupted it brings down the whole async indexer and no other index gets updated untill system administrator reindexes the problamatic async index. 

Instead of fail all we should isolate such corrupted index and mark them as corrupted. And still let async indexer progress for other working indexes. 

This would ensure that one corrupted index does not affect the whole system and allow the application to work partially. 

Feature branch - https://github.com/chetanmeh/jackrabbit-oak/compare/trunk...chetanmeh:OAK-4939?expand=1",docs-impacting,"['lucene', 'query']",OAK,Improvement,Major,2016-10-17 04:31:46,19
13012324,clarify contract for UpdateOp with missing operation on _id,See OAK-4937,technical_debt,['documentmk'],OAK,Task,Minor,2016-10-14 11:00:17,3
13012046,Too many segment cache misses,"Running the {{ConcurrentWriteTest}} benchmark and monitoring the hits and misses of the segment cache (LIRS), I noticed that some segments are loaded over and over again (up to 3000 times). ",performance,['segment-tar'],OAK,Bug,Major,2016-10-13 16:18:45,15
13011274,SegmentS3DataStoreStatsTest failing,"The tests are currently failing with 

{noformat}
ava.lang.RuntimeException: Unable to invoke method 'activate' for class org.apache.jackrabbit.oak.segment.SegmentNodeStoreService

	at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.invokeMethod(OsgiServiceUtil.java:262)
	at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.activateDeactivate(OsgiServiceUtil.java:86)
	at org.apache.sling.testing.mock.osgi.MockOsgi.activate(MockOsgi.java:162)
	at org.apache.sling.testing.mock.osgi.MockOsgi.activate(MockOsgi.java:173)
	at org.apache.sling.testing.mock.osgi.context.OsgiContextImpl.registerInjectActivateService(OsgiContextImpl.java:142)
	at org.apache.jackrabbit.oak.segment.SegmentS3DataStoreStatsTest.registerSegmentNodeStoreService(SegmentS3DataStoreStatsTest.java:113)
	at org.apache.jackrabbit.oak.segment.SegmentS3DataStoreStatsTest.testUseS3BlobStore(SegmentS3DataStoreStatsTest.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:117)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:43)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:239)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.NoSuchMethodError: org.apache.jackrabbit.oak.spi.state.RevisionGC.<init>(Ljava/lang/Runnable;Ljava/util/concurrent/Executor;)V
	at org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.registerSegmentStore(SegmentNodeStoreService.java:471)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.registerNodeStore(SegmentNodeStoreService.java:339)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.activate(SegmentNodeStoreService.java:304)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.invokeMethod(OsgiServiceUtil.java:253)
	... 38 more
{noformat}

Most likely our changes in OAK-4835 caused this regression. ",regression test-failure,['it'],OAK,Bug,Major,2016-10-11 07:22:28,15
13011261,"DefaultSyncHandler.listIdentities() search too broad, triggers traversal warning","DefaultSyncHandler.listIdentities() collects users by [searching for all nodes under /home|https://github.com/apache/jackrabbit-oak/blob/b3e62e3467bf6433b5a419c2f371331f33e57820/oak-auth-external/src/main/java/org/apache/jackrabbit/oak/spi/security/authentication/external/impl/DefaultSyncHandler.java#L143] – the xpath query executed is

{noformat}
/jcr:root/home//element(*)[@jcr:primaryType]
{noformat}

With a few hundred users this easily gives an oak index traversal warning:
{noformat}
org.apache.jackrabbit.oak.spi.query.Cursors$TraversingCursor Traversed 1000 nodes with filter Filter(query=select [jcr:path], [jcr:score], * from [nt:base] as a where [jcr:primaryType] is not null and isdescendantnode(a, '/home') /* xpath: /jcr:root/home//element(*)[@jcr:primaryType] */, path=/home//*, property=[jcr:primaryType=[is not null]]); consider creating an index or changing the query
{noformat}

A few lines later [it actually reduces|https://github.com/apache/jackrabbit-oak/blob/b3e62e3467bf6433b5a419c2f371331f33e57820/oak-auth-external/src/main/java/org/apache/jackrabbit/oak/spi/security/authentication/external/impl/DefaultSyncHandler.java#L151] the result to authorizables which have a {{rep:externalId}}. Since OAK-4301 there is an oak index for {{rep:externalId}}, so the query can be optimized by searching for anything with {{rep:externalId}} instead:
{code:java}
userManager.findAuthorizables(""rep:externalId"", null);
{code}
",performance,['auth-external'],OAK,Improvement,Major,2016-10-11 05:57:16,0
13011087,Better feedback from method invocations on RevisionGCMBean,"The methods to invoke and cancel revision gc return void. This is by design as those calls are asynchronous. The idea is that {{RevisionGC.getRevisionGCStatus()}} would return the current status of an ongoing gc operation. However, currently that method only returns the status of the asynchronous task that was fired off. It should instead be able to convey back the real status of the underlying operation. ",monitoring,['core'],OAK,Improvement,Major,2016-10-10 15:59:23,15
13011031,Interrupt online revision cleanup on documentmk,Sub task of OAK-4835 for the {{document}} specific changes,management,"['core', 'documentmk']",OAK,Technical task,Major,2016-10-10 13:20:11,2
13011027,Interrupt online revision cleanup on segment-tar,Sub task of OAK-4835 for the {{segment-tar}} specific changes,management,['segment-tar'],OAK,Technical task,Major,2016-10-10 13:13:38,15
13010921,Best-effort prefiltering in ChangeProcessor based on ChangeSet,"This is a subtask as a result of [discussions|https://issues.apache.org/jira/browse/OAK-4796?focusedCommentId=15550962&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15550962] around design in OAK-4796:

Based on the ChangeSet provided with OAK-4907 in the CommitContext, the ChangeProcessor should do a best-effort prefiltering of commits before they get added to the (BackgroundObserver's) queue.

This consists of the following parts:
* -the support for optionally excluding commits from being added to the queue in the BackgroundObserver- EDIT: factored that out into OAK-4916
* -the BackgroundObserver signaling downstream Observers that a change should be excluded via a {{NOOP_CHANGE}} CommitInfo- EDIT: factored that out into OAK-4916
* the ChangeProcessor using OAK-4907's ChangeSet of the CommitContext for best-effort prefiltering - and handling the {{NOOP_CHANGED}} CommitInfo introduced in OAK-4916
",review,"['core', 'jcr']",OAK,Technical task,Major,2016-10-10 07:31:34,21
13010407,For unique indexes avoid consulting indexes other than property index,"Currently for queries involving unique index like 

{noformat}
SELECT * FROM [nt:base] WHERE [jcr:uuid] = $id
{noformat}

QueryEngine would start consulting all types of index in order of minimum cost. Where minimum cost are
* PropertyIndex - 2.0
* LucenePropertyIndex - 2.1

For such queries in case of match property index returns a cost of (base) 2 + (count) 1 = 3 due to which QE has to consult Lucene index also. Given such queries a very frequent it would be better to avoid consulting Lucene index as that adds unnecessary overhead",candidate_oak_1_4 performance,['query'],OAK,Improvement,Major,2016-10-07 09:49:01,4
13010103,Enable persistent caches by default,The diff persistent cache is important for efficient processing of external changes and should be enabled by default.,observation,"['core', 'documentmk']",OAK,Improvement,Minor,2016-10-06 08:48:58,2
13010089,Include option to stop GC in RevisionGCMBean and RepositoryManagementMBean,"With OAK-4765 Oak Segment Tar acquired the capability for stopping a running revision gc task. This is currently exposed via {{SegmentRevisionGCMBean.stopCompaction}}. I think it would make to expose this functionality through {{RevisionGCMBean}} and {{RepositoryManagementMBean}} also/instead. 

[~mreutegg], [~alex.parvulescu] WDYT? Could the document node store also implement this or would we just not support it there? ",management production resilience,['core'],OAK,Improvement,Major,2016-10-06 07:57:14,15
13009902,FileStore cleanup should not leak out file handles,"{{FileStore.cleanup()}} currently returns a list of {{File}} instances relying on the caller to remove those files. This breaks encapsulation as the file store is the sole owner of these files and only the file store should be removing them.

I suggest to replace the current cleanup method with one that returns {{void}}. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-10-05 15:42:41,15
13009826,Document conflict handling,We should add documentation how Oak deals with conflicts. This was once documented in the Javadocs of {{MicroKernel.rebase()}} but got lost along with that class. Note that OAK-1553 refines conflict handling but this refinement has not been implemented in all backends yet. ,documentation,['doc'],OAK,Task,Major,2016-10-05 09:56:31,15
13009693,Warn or fail queries above a configurable cost value,"*Problem:* It's easy to run into performance problems with queries that are not backed by an index or miss the right one. Developers writing these queries typically do not have the real large production data, and thus don't see that a query would scale badly, and would not see any traversal warnings, as these only happen with a large number of results.

*Proposal:* Oak's query engine already calculates a cost estimate to make a decision which index to use, or even if there is any at all. This cost estimate could be used to find out if a query is not supported by an index or with one that is not suitable enough (e.g. ordering by property that is not indexed)

If a query is above a certain cost value, a big warning could be put out or even the query could be made to fail (maybe a per query option, that you might want to have to ""fail"" by default to ensure people are not overlooking the problem). The cost limit should be configurable, as it might depend on the hardware power.",docs-impacting,['query'],OAK,Improvement,Major,2016-10-04 20:56:48,4
13009491,Missing BlobGarbageCollection MBean on standby instance,"The {{BlobGarbageCollection}} MBean is no longer available on a standby instance, this affects non-shared datastore setups (on a shared datastore you'd only need to run blob gc on the primary).
This change was introduced by OAK-4089 (and backported to 1.4 branch with OAK-4093).",candidate_oak_1_4 gc,"['segment-tar', 'segmentmk']",OAK,Bug,Major,2016-10-04 10:24:00,12
13009466,Make merge semaphore in SegmentNodeStore fair by default,"Currently the merge semaphore in SegmentNodeStore is by default non fair. OAK-3588 provided a config option to make it fair.

We should change the default to fair so as to ensure writer threads never get starved. 

Eventually this change would need to be backported to branches. Further going forward OAK-4122 would replace the lock",candidate_oak_1_4,"['segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-10-04 08:02:33,19
13009230,Remove logging of cleaned segment id on cleanup,"With OAK-2404 we started logging all segment ids that a cleanup cycle removes. While this is useful for post mortems in the case of a {{SNFE}}, it also increases log files by many megabytes. 

Since OAK-2405 added some additional gc information, which is logged along with the missing segment in the case of a {{SNFE}}, I think the logging of the cleaned segment ids is not superfluous and we should remove it. ",gc logging monitoring,['segment-tar'],OAK,Improvement,Major,2016-10-03 13:24:28,15
13009176,Avoid running GC too frequently,"As {{RevisionGCMBean.startRevisionGC()}} can be used to manually invoke a gc cycle, there is the danger of running into a {{SNFE}} when gc is run multiple times in quick succession (due to the retention time being based on number of generations). We should come up with a mechanism to prevent this scenario. ",compaction gc,['segment-tar'],OAK,Improvement,Blocker,2016-10-03 07:26:29,15
13008877,Multiplexing NodeStore,"For a variety of use cases it is useful to multiplex multiple node stores behind a single interface.

Together with [~tomek.rekawek] I have been working on a limited-purpose MultiplexingNodeStore which aims to multiplex multiple stores with the limitation that only one store may be written to, while others are read-only.

The implementation has been validated to be functional, but not yet tested for behaviour under heavy load, performance, etc.

I believe that we can still incorporate this early to make maintenance simpler and ensure that the high-level design is correct.

Link to pull request -  https://github.com/apache/jackrabbit-oak/pull/55",multiplexing,['core'],OAK,New Feature,Major,2016-09-30 13:40:47,18
13008676,Avoid queries for first level previous documents during GC,Revision GC in a first phase finds all documents for deleted nodes and their previous documents. Reading the previous documents can be avoided in some cases. The ids of first level previous documents can be derived directly from the previous map in the main document.,candidate_oak_1_4 performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-29 19:06:27,2
13008560,Test failure: HeavyWriteIT,"Said test sometimes fails with the following stack trace:

{noformat}
09:46:40.900 ERROR [main] SegmentId.java:127                Segment not found: 8399230c-9338-47e3-acf5-b92d326cf171. SegmentId age=7473ms,gc-count=32,gc-status=success,store-generation=29,reclaim-predicate=(generation<=27),segment-generation27
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 8399230c-9338-47e3-acf5-b92d326cf171 not found
at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1345) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1285) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1013) ~[oak-core-1.5.8.jar:1.5.8]
at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:974) ~[oak-core-1.5.8.jar:1.5.8]
at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:285) ~[oak-core-1.5.8.jar:1.5.8]
at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:92) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1285) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:123) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.Record.getSegment(Record.java:70) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeState.getStableIdBytes(SegmentNodeState.java:139) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeState.getStableId(SegmentNodeState.java:122) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeState.fastEquals(SegmentNodeState.java:633) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:604) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:265) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:85) [test-classes/:na]
{noformat}

This is a problem with the test, not a regression:

{noformat}
Segment not found: 8399230c-9338-47e3-acf5-b92d326cf171. SegmentId age=7473ms,gc-count=32,gc-status=success,store-generation=29,reclaim-predicate=(generation<=27),segment-generation27
{noformat}

This means the missing segment was successfully gc'ed at GC #32. Its generation was 27 while the store just got bumped to generation 29. This causes a cleanup of all generations <= 27. 

The test itself calls {{FileStore.gc()}} in quick succession while at the same time writing to the store. This is likely to at some point cause a write to be based on an already collected segment. I suggest to fix this by increasing the number of retained generations to a sufficiently high value (for this test). 

On a side node, this issue (and being able to to a root cause analysis) validates the additional logging that we added with OAK-2405! 
",test,['segment-tar'],OAK,Bug,Minor,2016-09-29 14:57:21,15
13008550,Reduce query batch size for deleted documents,"MongoVersionGCSupport uses the default batchSize when it queries for possibly deleted documents. The default will initially read 100 documents and then as many as fit into a 4MB response. Depending on the document size a couple of thousand will fit in there and take time to process. It may happen that the MongoDB cursor then times out and the VersionGC fails.

An easy and safe solution is to reduce the batch size to a given number of documents.",resilience,['mongomk'],OAK,Improvement,Minor,2016-09-29 14:30:59,2
13008092,Support space chars common in CJK inside item names,"Oak (like Jackrabbit) does not allow spaces commonly used in CJK like {{u3000}} (ideographic space) or {{u00A0}} (no-break space) _inside_ a node name, while allowing some of them (the non breaking spaces) at the _beginning or end_.

They should be supported for better globalization readiness, and filesystems allow them, making common filesystem to JCR mappings unnecessarily hard. Escaping would be an option for applications, but there is currently no utility method for it ([Text.escapeIllegalJcrChars|https://jackrabbit.apache.org/api/2.8/org/apache/jackrabbit/util/Text.html#escapeIllegalJcrChars(java.lang.String)] will not escape these spaces), nor is it documented for applications how to do so.",candidate_oak_1_8,['core'],OAK,Improvement,Major,2016-09-28 00:16:45,3
13007081,Improve oak-blob-cloud tests,Most S3 DataStore tests inherit from Jackrabbit 2's TestCaseBase. To make extend them we should copy the class into Oak and let S3DataStore inherit from that. In doing do we can also use the DataStore initialization part already available.,technical_debt,['blob'],OAK,Technical task,Blocker,2016-09-23 08:06:54,16
13006824,Analyse effects of simplified record ids,"OAK-4631 introduced a simplified serialisation for record ids. This causes their footprint on disk to increase from 3 bytes to 18 bytes. OAK-4631 has some initial analysis on the effect this is having on repositories as a whole. 

I'm opening this issue as a dedicated task to further look into mitigation strategies (if necessary). ",performance,['segment-tar'],OAK,Improvement,Major,2016-09-22 11:51:36,15
13006821,Missing export for org.apache.jackrabbit.oak.backup package,"Oak segment tar does not export {{org.apache.jackrabbit.oak.backup}}, which makes backup restore functionality unavailable from OSGi containers. 

Instead of just exporting this package I think we should:
* Separate API from implementation
* Add semantic versioning to the exported API

/cc [~frm]",API OSGi,['segment-tar'],OAK,Bug,Major,2016-09-22 11:44:29,12
13006739,Move S3 classes to oak-blob-cloud module,"Some S3 related classes are present in oak-core module. These should be moved to oak-blob-cloud.
This would also flip the module dependencies to oak-core -> oak-blob-cloud",technical_debt,['blob'],OAK,Technical task,Blocker,2016-09-22 03:57:34,16
13006737,Improved caching for DataStore,"The current CachingDataStore implementation used with S3DataStore has certain problems:
* Lack of stats to show hit rate/miss rates for files being requested for downloads
* Lack of stats for async uploads
* CachingDataStore starts proactively downloading files in the background when a call to {{getRecord}} is made.
* Async upload functionality leaks into Backend implementations, LocalCache classes.
* The call to {{DataStore#getRecord()}} which makes multiple calls to backends which is problematic for S3 (i.e. when not being served bu cache)
* There is some functionality which is not used with Oak like length cache, sync/async touch etc. which can be removed and code simplified.",performance,['blob'],OAK,Improvement,Major,2016-09-22 03:55:27,16
13006493,Provide generic option to interrupt online revision cleanup,JMX binding for stopping a running compaction process,compaction gc management monitoring,['core'],OAK,Improvement,Major,2016-09-21 13:18:24,15
13006454,Document storage format changes,"This issue serves as collection of all changes to the storage format introduced with  Oak Segment Tar and their impact. Once sufficiently stabilised this information should serve as basis for the documentation in {{oak-doc}}. 

|| Change || Rational || Impact || Migration || Since || Issues ||
|Generation in segment header |Required to unequivocally determine the generation of a segment during cleanup. Segment retention time is given in number of generations (2 by default). |No performance, space impact expected |offline |0.0.2 |OAK-3348 | 
|Stable id for node states |Required to efficiently determine equality of node states. This can be seen as an intermediate step to decoupling the address of records from their identity. The next step is to introduce logical record ids (OAK-4659). |Node states increase by the size of one record id (3 bytes / 20 bytes after OAK-4631). On top of that there is an additional block record à 18 bytes per node state. |offline |0.0.2 |OAK-3348
|Binary index in tar files |Avoid traversing the repository to collect the gc roots for DSGC. Fetch them from an index instead. |Additional index entry per tar file. Adds a couple of bytes per external binary to each tar file. Exact size to be determined. [~frm] could you help with this? OAK-4740 is a regression wrt. to resiliency caused by this change (and the fact that the blob store might return blob ids longer than 2k chars).  |offline |0.0.4 |OAK-4101
|Simplified record ids |Preparation and precondition for logical record ids (OAK-4659). At the same time the simplest possible fix for OAK-2896. The latter leads to degeneration of segment sizes, which in turn has adverse effects on overall performance, resource utilisation and memory requirements. Without this fix OAK-2498 would need to be fixed in a different way that would require other changes in the storage format. I started to regard this issue as removing a premature optimisation (which caused OAK-2498). OTOH with OAK-4844 we should also start looking into mitigations and what those would mean to size vs. simplicity vs. performance.  |Record ids grow from 3 bytes to 18 bytes when serialised into records. Impact on repositories to be assessed but can be anywhere between almost none to x6. OAK-4812 is a performance regression caused by this chance. Its overall impact is yet to be assessed. |offline |0.0.10 |OAK-4631, OAK-4844
|Storage format versioning |In order to be able to further evolve the storage format with minimal impact on existing deployments we need to carefully versions the various storage entities (segments, tar files, etc.) |No performance, space impact expected |offline |0.0.2/ 0.0.10 |OAK-4232, OAK-4683, OAK-4295
|Logical record ids |We need to separate addresses of records from their identity to be able to further scale the TarMK. OAK-3348 (the online compaction misery) can be seen as a symptom of failing to understand this earlier. The stable ids introduced with OAK-3348 are a first step into this direction. However this is not sufficient to implement features like e.g. background compaction (OAK-4756), partial compaction (OAK-3349) or incremental compaction (OAK-3350).  |A small size overhead per segment for the logical id table. Further impact to be evaluated ([~frm], please add your assessment here). |offline |0.0.14 (planned) |OAK-4659
|External index for segments |Avoid recreating tar files if indexes are corrupt/missing. Just recreate the indexes. |Faster startup after a crash. Overall less disk space usage as no unnecessary backup files are created. |online |not yet planned |OAK-4649
|In-place journal |Reduce complexity by in-lining the journal log. Less files, less chances to break something. Also the granularity of the log would increase as flushing of the persisted head would not be required any more. Resilience would improve as the roll-back functionality could operate at a finer granularity. |No more journal.log. Better resiliency. Significant risk for regression of OAK-4291 if not implemented properly. Most likely a significant refactoring of some parts of the code is required before we can proceed with this issue.  |online |not yet planned |OAK-4103
|Root record types |With the information currently available from the segment headers we cannot collect statistics about segment usage on repositories of non trivial sizes. This fix would allow us to build more scalable tools to that respect.  |None expected wrt. to performance and size under normal operation. |offline |0.0.14 (planned) (waiting for OAK-4659 as implementation depends on how we progress there) |OAK-2498

Misc ideas currently on the back burner:
* SegmentMK: Arch segments (OAK-1905)
* Extension headers for segments (no issue yet)
* More memory efficient serialisation of values (e.g. boolean) (no issue yet)
* Protocol Buffer for serialising records (no issue yet)

",documentation,"['doc', 'segment-tar']",OAK,Technical task,Major,2016-09-21 10:26:36,12
13006181,Use optional resolution for optional dependencies,OAK-4775 upgraded Netty to 4.0.41.Final. This version seem to bring a couple of optional dependencies that we should also specify as optional in the Import-Package clause. ,OSGi,['segment-tar'],OAK,Improvement,Major,2016-09-20 13:18:23,15
13005960,Deadlock in TarWriter,"There is the potential for a deadlock between concurrent calls to {{TarWriter#createNextGeneration()}} and {{TarWriter#flush()}}: both methods try to acquire a lock on this and another lock on {{TarWriter.file}} but in different order. I observed the deadlock when running {{CompactionAndCleanupIT.randomAccessFileConcurrentReadAndLength()}}.

This is a regression introduced with OAK-4746: the method {{TarWriter.createNextGeneration()}} seems over eagerly synchronized. I would argue that we could drop synchronization for that method entirely as the part after the call to {{close()}} will in any case only ever be executed once by a single thread. All other threads will fail with an {{IllegalStateException}}).

[~alexparvulescu], WDYT?",deadlock resilience threading,['segment-tar'],OAK,Bug,Critical,2016-09-19 16:12:25,14
13005873,Improve revision GC resilience,Revision GC may currently fail when a document with a malformed id is read from the DocumentStore. E.g. a document stored accidentally in the nodes collection or malformed for some other reason.,resilience,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-19 10:57:25,2
13005064,Remove usage of Tree in LuceneIndexEditor,"{{LuceneIndexEditor}} currently creates 2 tree instances for determining IndexRule. [~ianeboston] highlighted this on list [1] and this is something which we should avoid and remove usage of Tree api

This was earlier done so as to simplify future support for conditional rules (OAK-2281) which might need access to ancestor which is not possible with NodeState api.  As that is not going to be done so we can get rid of Tree construction in the editor.

[1] https://lists.apache.org/thread.html/7d51b45296f5801c3b510a30a4847ce297707fb4e0d4c2cefe19be62@%3Coak-dev.jackrabbit.apache.org%3E
",performance,['lucene'],OAK,Improvement,Minor,2016-09-15 04:19:03,19
13004849,Basic cache consistency test on exception,"OAK-4774 and OAK-4793 aim to check if the cache behaviour of a DocumentStore implementation when the underlying backend throws an exception even though the operation succeeded. E.g. the response cannot be sent back because of a network issue.

This issue will provide the DocumentStore independent part of those tests.",resilience,"['core', 'documentmk']",OAK,Test,Minor,2016-09-14 11:47:13,2
13004315,Optimise stable ids ,"Currently {{SegmentNodeState#getStableId()}} returns a string with all its associated overhead:
* high memory requirements (42 characters plus the overhead of a {{String}} instance. The raw requirements are a mere 20 bytes (long msb, long lsb, int offset). The memory overhead is problematic as the stable id is used as key in the node deduplication cache (See OAK-4635).
* high serialisation cost. I have seen {{getStableId()}} occurring in stack traces. This is to be expected as that method is called quite often when comparing node states. 

This issue is to explore options for reducing both CPU and memory overhead of stable ids. ",memory performance,['segment-tar'],OAK,Improvement,Major,2016-09-12 14:51:00,15
13004309,filter events before adding to ChangeProcessor's queue,"Currently the [ChangeProcessor.contentChanged|https://github.com/apache/jackrabbit-oak/blob/f4f4e01dd8f708801883260481d37fdcd5868deb/oak-jcr/src/main/java/org/apache/jackrabbit/oak/jcr/observation/ChangeProcessor.java#L335] is in charge of doing the event diffing and filtering and does so in a pooled Thread, ie asynchronously, at a later stage independent from the commit. This has the advantage that the commit is fast, but has the following potentially negative effects:
# events (in the form of ContentChange Objects) occupy a slot of the queue even if the listener is not interested in it - any commit lands on any listener's queue. This reduces the capacity of the queue for 'actual' events to be delivered. It therefore increases the risk that the queue fills - and when full has various consequences such as loosing the CommitInfo etc.
# each event==ContentChange later on must be evaluated, and for that a diff must be calculated. Depending on runtime behavior that diff might be expensive if no longer in the cache (documentMk specifically).

As an improvement, this diffing+filtering could be done at an earlier stage already, nearer to the commit, and in case the filter would ignore the event, it would not have to be put into the queue at all, thus avoiding occupying a slot and later potentially slower diffing.

The suggestion is to implement this via the following algorithm:

* During the commit, in a {{Validator}} the listener's filters are evaluated - in an as-efficient-as-possible manner (Reason for doing it in a Validator is that this doesn't add overhead as oak already goes through all changes for other Validators). As a result a _list of potentially affected observers_ is added to the {{CommitInfo}} (false positives are fine).
** Note that the above adds cost to the commit and must therefore be carefully done and measured
** One potential measure could be to only do filtering when listener's queues are larger than a certain threshold (eg 10)
* The ChangeProcessor in {{contentChanged}} (in the one created in [createObserver|https://github.com/apache/jackrabbit-oak/blob/f4f4e01dd8f708801883260481d37fdcd5868deb/oak-jcr/src/main/java/org/apache/jackrabbit/oak/jcr/observation/ChangeProcessor.java#L224]) then checks the new commitInfo's _potentially affected observers_ list and if it's not in the list, adds a {{NOOP}} token at the end of the queue. If there's already a NOOP there, the two are collapsed (this way when a filter is not affected it would have a NOOP at the end of the queue). If later on a no-NOOP item is added, the NOOP's {{root}} is used as the {{previousRoot}} for the newly added {{ContentChange}} obj.
** To achieve that, the ContentChange obj is extended to not only have the ""to"" {{root}} pointer, but also the ""from"" {{previousRoot}} pointer which currently is implicitly maintained.",observation,['jcr'],OAK,Improvement,Major,2016-09-12 14:12:30,21
13004306,Node writer statistics is skewed,The statistics about writing nodes collected by the {{SegmentWriter}} instances is a bit off. This was caused by the changes introduced with OAK-4570. Starting with these changes also base node states of a node being written are de-duplicated. Collecting the node writer stats does not differentiate however e.g. the cache hits/misses between deduplication for the base state or the actual state being written. ,monitoring,['segment-tar'],OAK,Bug,Major,2016-09-12 14:00:13,15
13004290,Check usage of DocumentStoreException in RDBDocumentStore,"With OAK-4771 the usage of DocumentStoreException was clarified in the DocumentStore interface. The purpose of this task is to check usage of the DocumentStoreException in RDBDocumentStore and make sure JDBC driver specific exceptions are handled consistently and wrapped in a DocumentStoreException. At the same time, cache consistency needs to be checked as well in case of a driver exception. E.g. invalidate if necessary.",resilience,"['core', 'rdbmk']",OAK,Technical task,Minor,2016-09-12 13:05:27,3
13003584,ClusterNodeInfo may renew lease while recovery is running,ClusterNodeInfo.renewLease() does not detect when it is being recovered by another cluster node.,resilience,"['core', 'documentmk']",OAK,Bug,Major,2016-09-08 13:51:31,2
13003477,Check usage of DocumentStoreException in MongoDocumentStore,"With OAK-4771 the usage of DocumentStoreException was clarified in the DocumentStore interface. The purpose of this task is to check usage of the DocumentStoreException in MongoDocumentStore and make sure MongoDB Java driver specific exceptions are handled consistently and wrapped in a DocumentStoreException. At the same time, cache consistency needs to be checked as well in case of a driver exception. E.g. invalidate if necessary.",resilience,"['core', 'mongomk']",OAK,Task,Minor,2016-09-08 07:30:14,2
13003229,Clarify exceptions in DocumentStore,"The current DocumentStore contract is rather vague about exceptions. The class JavaDoc mentions implementation specific runtime exceptions, but does not talk about the DocumentStoreException used by all the DocumentStore implementations. We should make this explicit in all relevant methods.",resilience,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-07 13:12:10,2
13003226,Missing exception handling in ClusterNodeInfo.renewLease(),ClusterNodeInfo.renewLease() does not handle a potential DocumentStoreException on {{findAndModify()}}. This may leave {{previousLeaseEndTime}} in an inconsistent state and a subsequent {{renewLease()}} call then considers the lease timed out. ,resilience,"['core', 'documentmk']",OAK,Bug,Major,2016-09-07 13:04:05,2
13002928,Provide option to interrupt online revision cleanup,JMX binding for stopping a running compaction process,compaction gc,['segment-tar'],OAK,Improvement,Major,2016-09-06 13:55:32,14
13002866,Adjust default timeout values for RDBDocumentStore,"Some default values timeouts of the RDBDocumentStore driver do not work well with the lease time we use in Oak.

See also OAK-4739.",resilience,"['core', 'rdbmk']",OAK,Improvement,Minor,2016-09-06 09:17:15,3
13002851,Adjust default timeout values for MongoDocumentStore,"Some default values timeouts of the MongoDB Java driver do not work well with the lease time we use in Oak.

Per default there is no socket timeout set and the driver waits for a new connection up to 120 seconds, which is too log for lease update operations. 

See also OAK-4739.",resilience,"['core', 'mongomk']",OAK,Improvement,Minor,2016-09-06 08:09:13,2
13002746,A parallel approach to garbage collection,"Assuming that:

# Logic record IDs are implemented.
# TAR files are ordered in reverse chronological order.
# When reading segments, TAR files are consulted in order.
# Segments in recent TAR files shadow segments in older TAR files with the same segment ID.

A new algorithm for garbage collection can be implemented:

# Define the input for the garbage collection process. The input consists of the current set of TAR files and a set of record IDs representing the GC roots.
# Traverse the GC roots and mark the records that are still in use. The mark phase traverses the record graph and produces a list of record IDs. These record IDs are referenced directly or indirectly by the given set of GC roots and need to be kept. The list of record IDs is ordered by segment ID first and record number next. This way, it is possible to process this list in one pass and figure out which segment and which record should be saved at the end of the garbage collection.
# Remove unused records from segments and rewrite them in a new set of TAR files. The list is produced in the previous step is traversed. For each segment encountered, a new segment is created containing only the records that were marked in the previous phase. This segment is then saved in a new set of TAR files. The set of new TAR files is the result of the garbage collection process. 
# Add the new TAR files to the system. The system will append the new TAR files to the segment store. The segments in these TAR files will shadow the ones in older TAR files.
# Remove TAR files from the old generation. It is safe to do so because the new set of TAR files are currently shadowing the initial set of TAR files.

While the garbage collection process is running, the system can work as usual by starting a fresh TAR file. The result of the garbage collection is made visible atomically only at the end, when the new TAR files are integrated into the running system.",gc scalability,['segment-tar'],OAK,New Feature,Major,2016-09-05 15:08:09,12
13002722,Leaderboard in ConsolidatedListenerMBean,"ConsolidatedListenerMBean contains various stats about JCR event listeners. However, it is rather difficult to get an overview of how expensive listeners are.

The MBean should expose a simple leaderboard that orders the listener according to the processing time (producer & consumer time).",observation,['jcr'],OAK,Improvement,Minor,2016-09-05 12:30:56,2
13002681,Include initial cost in stats for observation processing,"The jackrabbit-jcr-commons {{ListenerTracker}} collects timing for JCR event listeners. It tracks producer (oak internal) and consumer (JCR EventListener) time. The initial producer cost is currently not reflected in these stats, because {{ChangeProcessor}} in oak-jcr does an initial {{hasNext()}} on the {{EventIterator}} outside of the {{ListenerTracker}}. For some listeners this initial producer time may even account for the entire cost when the event filter rejects all changes.",observation,['jcr'],OAK,Improvement,Minor,2016-09-05 07:58:40,2
13002069,Improve FileStoreStatsMBean,"We should add further data to that MBean (if feasible):

* Number of commits
* Number of commits queuing (blocked on the commit semaphore)
* Percentiles of commit times (exclude queueing time)
* Percentiles of commit queueing times 
* Last gc run / size before gc and after gc / time gc took broken down into the various phases

",monitoring,['segment-tar'],OAK,Improvement,Major,2016-09-01 14:27:20,12
13001917,Reduce DocumentStore reads for local changes (2),"There is another case where the local cache is incorrectly updated, which leads to unnecessary reads from the DocumentStore. See also OAK-4715.",performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-01 07:06:43,2
13001743,(Slightly) prioritise reads over writes ,"When fetching the current root from the {{SegmentNodeStore}} an older revision will be returned when a commit is being processed concurrently. I think it would make sense to wait for a short time in this case increasing the chance of returning an up to date state. The idea is that this would lower the rebasing work that need to be done later on should the returned root be used for further modifications. 

An interesting value for the wait time is to use  the median (or more general a percentile) of the commit time of the last say 1000 commits. This would mean that (for the median) we have a 50% chance of getting up to date date. For a 90% percentile we would have longer wait times but then a 90% chance of getting up to date date. ",Performance scalability,['segment-tar'],OAK,Improvement,Minor,2016-08-31 16:23:34,1
13001737,Refine forced compaction,"Forced compaction currently acquires an exclusive write lock on the repository blocking all concurrent commits during the complete time it needs to finish compaction. I think we should refine this:

* Add a time out so we could limit the time during which the repository does not accept writes while still giving compaction another chance to finish.

* Boost the compaction threads priority. This could actually already be done during the regular compaction cycles to increase the changes to finish in time. 

",compaction gc,['segment-tar'],OAK,Improvement,Major,2016-08-31 16:16:04,15
13001705,OSGi config for the size based estimation,This follows OAK-4293 which needs to be exposed via OSGi configs as well.,gc,['segment-tar'],OAK,Improvement,Minor,2016-08-31 15:09:22,14
13001638,Oak Segment Tar tests should not check node store fixture,Checking the node store fixture {{commons.FixturesHelper#getFixtures()}} is a left over from when the segment node store was part of {{oak-core}} and we wanted to avoid running the tests multiple times. As we are now in a separate module this check is not necessary any more. It is currently even harmful as certain tests are skipped. The default value for the fixtures is still {{SEGMENT_MK}} because {{oak-segment-tar}} cannot yet depend on Oak 1.5.9 (not released yet) where the default was switched to {{SEGMENT_TAR}} (see OAK-4706).,tests,['segment-tar'],OAK,Improvement,Major,2016-08-31 12:39:27,15
13001354,Prefetch external changes,"In a cluster with listeners that are registered to receive external changes, pulling in external changes can become a bottleneck. While processing those external changes, further local changes are put into the observation queue leading to a system where the queue eventually fills up.

Instead of processing external changes one after another, the implementation could prefetch them as they come in and if needed pull them in parallel.",observation,"['core', 'documentmk']",OAK,Improvement,Major,2016-08-30 15:12:41,2
13001317,Optimize PathRev as/from String,PathRev instances are used as keys for various cache entries and asString() / fromString() methods are called frequently when the persistent cache is enabled.,performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-08-30 13:59:18,2
13000913, Reduce DocumentStore reads for local changes,"The observation test added for OAK-4528 shows significant time spent in reading documents from the store when local changes are processed. Since those changes were done on the local cluster node, they should be served from cache and not reach out to the underlying store.",observation performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-08-29 10:06:52,2
13000012,Run tests against SEGMENT_TAR fixture,Oak's integration tests still run against the {{SEGMENT_MK}} fixture. I suggest we switch to the {{SEGMENT_TAR}} fixture. ,testing,"['parent', 'segment-tar']",OAK,Task,Major,2016-08-25 13:29:13,15
12999637,Optimize read of old node state,"Reading a node state with an old revision from a document can be expensive when many changes happened on a property in the meantime.

A typical stack trace looks like this:

{noformat}
	at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getPreviousDocument(NodeDocument.java:1337)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$1.apply(PropertyHistory.java:70)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$1.apply(PropertyHistory.java:63)
	at com.google.common.collect.Iterators$8.transform(Iterators.java:794)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:646)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at com.google.common.collect.Iterators$PeekingImpl.hasNext(Iterators.java:1139)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$2.refillQueue(PropertyHistory.java:121)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$2.computeNext(PropertyHistory.java:96)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$2.computeNext(PropertyHistory.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.jackrabbit.oak.plugins.document.ValueMap$1$3.nextIterator(ValueMap.java:105)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:98)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.next(MergeSortedIterators.java:85)
	at com.google.common.collect.Iterators$PeekingImpl.peek(Iterators.java:1162)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.adjustFirst(MergeSortedIterators.java:117)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.next(MergeSortedIterators.java:78)
	at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getLatestValue(NodeDocument.java:1972)
	at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:990)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:1079)
{noformat}

The read operation goes through the property history until it finds the most recent change. The old the read revision, the more changes are scanned.",observation performance,"['core', 'documentmk']",OAK,Improvement,Major,2016-08-24 11:43:48,2
12998462,Avoid concurrent calls to FileStore.cleanup() and FileStore.compact(),At the moment it is possible to have concurrent calls to {{FileStore.cleanup}} and to {{FileStore.compact()}}. The former is called from the latter and also from {{FileStore.flush()}} (this is tracked in OAK-4138). We should change this status quo and also make the calls to {{compact()}} and {{cleanup()}} mutually exclusive.,cleanup gc,"['segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-08-19 11:21:10,15
12997573,SNFE thrown while testing FileStore.cleanup() running concurrently with writes,"{{SegmentNotFoundException}} is thrown from time to time in the following scenario: plenty of concurrent writes (each creating a {{625 bytes}} blob) interrupted by a cleanup. 

Stack trace (including some debugging statements added by me):
{code:java}
Pre cleanup readers: []
Before cleanup readers: [/Users/dulceanu/work/test-repo/data00000a.tar]
Initial size: 357.4 kB
After cleanup readers: [/Users/dulceanu/work/test-repo/data00000a.tar]
After cleanup size: 357.4 kB
Final size: 361.0 kB
Exception in thread ""pool-5-thread-74"" org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Cannot copy record from a generation that has been gc'ed already
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1207)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:1096)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:1013)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:1013)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:987)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.access$700(SegmentWriter.java:379)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$8.execute(SegmentWriter.java:337)
	at org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.execute(SegmentBufferWriterPool.java:105)
	at org.apache.jackrabbit.oak.segment.SegmentWriter.writeNode(SegmentWriter.java:334)
	at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:111)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:550)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:571)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:627)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:287)
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT$1.run(CompactionAndCleanupIT.java:961)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 4fb637cc-5013-4925-ab13-0629c4406481 not found
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1341)
	at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:123)
	at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:94)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1199)
	... 18 more
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Invalid segment format. Dumping segment 4fb637cc-5013-4925-ab13-0629c4406481
00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000040 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000090 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000C0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000D0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000120 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000130 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000140 39 37 39 31 31 36 30 38 2D 63 31 63 65 2D 34 62 97911608-c1ce-4b
00000150 35 63 2D 61 36 33 37 2D 39 36 61 65 39 34 38 38 5c-a637-96ae9488
00000160 61 37 65 38 2E 30 61 62 34 30 36 38 36 00 00 00 a7e8.0ab40686...
00000170 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000180 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000190 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001A0 00 00 00 00 30 30 30 30 34 30 30 00 30 30 30 30 ....0000400.0000
000001B0 30 30 30 00 30 30 30 30 30 30 30 00 30 30 30 30 000.0000000.0000
000001C0 30 30 30 31 33 30 30 00 31 32 37 35 34 36 30 33 0001300.12754603
000001D0 37 32 32 00 30 31 32 33 30 37 00 20 30 00 00 00 722.012307. 0...
000001E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000210 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000230 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000240 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000250 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000260 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000270 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000280 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000290 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................

	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1015)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:972)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:283)
	at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:92)
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1275)
	... 21 more
Caused by: java.lang.IllegalStateException: Invalid segment format. Dumping segment 4fb637cc-5013-4925-ab13-0629c4406481
00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000040 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000090 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000C0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000D0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000120 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000130 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000140 39 37 39 31 31 36 30 38 2D 63 31 63 65 2D 34 62 97911608-c1ce-4b
00000150 35 63 2D 61 36 33 37 2D 39 36 61 65 39 34 38 38 5c-a637-96ae9488
00000160 61 37 65 38 2E 30 61 62 34 30 36 38 36 00 00 00 a7e8.0ab40686...
00000170 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000180 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000190 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001A0 00 00 00 00 30 30 30 30 34 30 30 00 30 30 30 30 ....0000400.0000
000001B0 30 30 30 00 30 30 30 30 30 30 30 00 30 30 30 30 000.0000000.0000
000001C0 30 30 30 31 33 30 30 00 31 32 37 35 34 36 30 33 0001300.12754603
000001D0 37 32 32 00 30 31 32 33 30 37 00 20 30 00 00 00 722.012307. 0...
000001E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000210 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000230 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000240 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000250 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000260 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000270 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000280 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000290 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................

	at com.google.common.base.Preconditions.checkState(Preconditions.java:150)
	at org.apache.jackrabbit.oak.segment.Segment.<init>(Segment.java:185)
	at org.apache.jackrabbit.oak.segment.file.FileStore$15.call(FileStore.java:1292)
	at org.apache.jackrabbit.oak.segment.file.FileStore$15.call(FileStore.java:1)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1011)
	... 25 more
0
{code}
",cleanup gc,"['segment-tar', 'segmentmk']",OAK,Bug,Major,2016-08-16 12:55:29,1
12996938,Cleanup creates new generation of tar file without removing any segments ,"On some deployments I have seen tar files with a quite hight generation post-fix (e.g. 'v'). From the log files I could deduce that this particular tar file was rewritten multiple times without actually any segment being removed.
I assume this is caused by the 25% gain threshold not taking the sizes contributed by the index and the graph entries into account.

The attached test case can be used to verify the above hypothesis.",cleanup gc,['segment-tar'],OAK,Bug,Minor,2016-08-12 13:05:48,1
12996529,Disabling lease check via DocumentMK builder does not work,"While writing a cluster test where multiple DocumentNodeStore instance are created the test case log exception like 

{noformat}
17:20:34.852 TRACE [DocumentNodeStore lease update thread (1)] ClusterNodeInfo.java:761 renewLease - leaseEndTime: 1470829945825, leaseTime: 120000, leaseUpdateInterval: 10000
17:20:35.006 TRACE [DocumentNodeStore lease update thread (2)] ClusterNodeInfo.java:761 renewLease - leaseEndTime: 1470829945987, leaseTime: 120000, leaseUpdateInterval: 10000
17:20:35.855 TRACE [DocumentNodeStore lease update thread (1)] ClusterNodeInfo.java:761 renewLease - leaseEndTime: 1470829945825, leaseTime: 120000, leaseUpdateInterval: 10000
17:20:35.856 ERROR [DocumentNodeStore lease update thread (1)] ClusterNodeInfo.java:779 This oak instance failed to update the lease in time and can therefore no longer access this DocumentNodeStore.
17:20:35.857 WARN  [DocumentNodeStore lease update thread (1)] DocumentNodeStore.java:2590 Background operation failed: java.lang.AssertionError: This oak instance failed to update the lease in time and can therefore no longer access this DocumentNodeStore.
java.lang.AssertionError: This oak instance failed to update the lease in time and can therefore no longer access this DocumentNodeStore.
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.renewLease(ClusterNodeInfo.java:780) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.renewClusterIdLease(DocumentNodeStore.java:1772) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$BackgroundLeaseUpdate.execute(DocumentNodeStore.java:2643) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$NodeStoreTask.run(DocumentNodeStore.java:2588) ~[classes/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]
{noformat}

This happens because {{instanceId}} is same (same JVM process used for both cluster node). To enable such test we need to disable the leasecheck but doing that via DocumentMK does not work",candidate_oak_1_4,['documentmk'],OAK,Bug,Minor,2016-08-11 05:24:02,21
12995640,Allow to mount the secondary node store as a read-only subtree,"For the document node store it should be possible to mount ""another"" node store under some path. Assumptions for the OSGi setup:

* the mounted node store provider has to be registered with {{(role=secondary)}} in OSGi,
* the MountInfoProvider contains a Mount registered as {{private}},
* all reads of the paths configured in MountInfoProvider are redirected to the mounted node store,
* mounted subtrees are read-only,
* the properties characteristic to the document node store (lastRev, rootRev) are set to a constant value.",secondary-nodestore,['documentmk'],OAK,New Feature,Major,2016-08-08 12:24:35,18
12994907,Improve documentation about structure of TAR files,"Improve the page at [1] to include a picture of the contents of a TAR file, as done for segments in [2], to cover missing parts (e.g. binary references files) and to better align it with latest oak-segment-tar improvements.

[1] http://jackrabbit.apache.org/oak/docs/nodestore/segment/tar.html
[2] http://jackrabbit.apache.org/oak/docs/nodestore/segment/records.html",documentation,"['doc', 'segment-tar']",OAK,Sub-task,Minor,2016-08-04 13:58:46,1
12994527,Improve cache eviction policy of the node deduplication cache,"{{NodeCache}} uses one stripe per depth (of the nodes in the tree). Once its overall capacity (default 1000000 nodes) is exceeded, it clears all nodes from the stripe with the greatest depth. This can be problematic when the stripe with the greatest depth contains most of the nodes as clearing it would result in an almost empty cache. 

",perfomance,['segment-tar'],OAK,Improvement,Major,2016-08-03 09:10:58,15
12993523,External invocation of background operations,"The background operations (flush, compact, cleanup, etc.) are historically part of the implementation of the {{FileStore}}. They should better be scheduled and invoked by an external agent. The code deploying the {{FileStore}} might have better insights on when and how these background operations should be invoked. See also OAK-3468.
",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-07-29 12:08:41,15
12993518,Unify RecordCacheStats and CacheStats,There is {{org.apache.jackrabbit.oak.cache.CacheStats}} in {{oak-core}} and {{org.apache.jackrabbit.oak.segment.RecordCacheStats}} in {{oak-segment-tar}}. Both exposing quite similar functionality. We should try to unify them as much as possible. ,refactoring technical_debt,"['core', 'segment-tar']",OAK,Improvement,Minor,2016-07-29 11:58:50,15
12993517,Align GCMonitorMBean MBean with new generation based GC,"The {{GCMonitorMBean}} MBean still dates back to the old {{oak-segment}}. We need to review its endpoints and only keep those that make sense for {{oak-segment-tar}}, adapt the others as necessary any add further functionality as required. 

Specifically I think we should get rid of the time series for {{getRepositorySize()}} and {{getReclaimedSize()}}.

Also the name {{getRepositorySize()}} is confusing and we should change it. It leads callers to think it would return current size of the repository opposed to the size it had after the last cleanup. (There is {{FileStoreStatsMBean.getRepositorySize()}} for the latter.)",production,['segment-tar'],OAK,Task,Major,2016-07-29 11:55:50,1
12993514,Align SegmentRevisionGC MBean with new generation based GC,"The {{SegmentRevisionGC}} MBean still dates back to the old {{oak-segment}}. We need to review its endpoints and only keep those that make sense for {{oak-segment-tar}}, adapt the others as necessary any add further functionality as required. ",production,['segment-tar'],OAK,Task,Major,2016-07-29 11:51:45,15
12992910,Report age of oldest queue entry in EventListenerMBean and ConsolidatedListenerMBean,This is related to JCR-4000 and the remaining work in Oak that hooks into the ListenerTracker and exposes the info also in the consolidated listener MBean.,observation,"['core', 'jcr']",OAK,Improvement,Minor,2016-07-27 12:28:35,2
12992628,Separate persistent cache for diff and local_diff,"The DocumentNodeStore currently uses a single persistent cache for all types (node, nodeChildren, diff, etc.). With this setup it is not possible to assign a specific amount of disk space for some cache type(s). If there are many inserts for one cache type, entries of another type may become unavailable. In practice this can be a problem for the local_diff cache entries that are important for efficient node state comparison.

Separating the diff and local_diff cache entries would also allow for different configuration options like compression and different behaviour when the async write back queue is full.",observation,"['core', 'documentmk']",OAK,Improvement,Major,2016-07-26 16:00:27,2
12992483,Collection of references retrieves less when large number of blobs added,"When large number of external blobs are added to the DataStore (50000) and a cycle of compaction executed then the reference collection logic only returns lesser number of blob references. It reports correct number of blob references when number of blobs added are less indicatingsome sort of overflow.
Another related issue observed when testing with lesser number of blobs is that the references returned are double the amount expected, so maybe there should be some sort of de-duplication which should be added.

Without compaction the blob references are returned correctly atleast till 100000 (ExternalBlobId#testNullBlobId)",datastore gc,['segment-tar'],OAK,Bug,Major,2016-07-26 05:08:58,12
12991809,Clarify implementation and documentation of TarReader#mark,"There is a few peculiarities with that method:

* The Javadoc ""A bulk segment is reclaimable if it is in bulkRefs"" is wrong. It should be ""A bulk segment is reclaimable if it is *not* in bulkRefs"".
* (Why) is it necessary to iterate in reverse over the entries in tar file?
* Why the extra check for bulk references in the else branch?
* The condition {{!reclaim.remove(id)}} is always true as {{id}} can only be in {{reclaim}} it it had been added in the same iteration (as ids are unique). But this would have been in the if branch, contradicting us being in the else branch. 
",cleanup gc,['segment-tar'],OAK,Improvement,Major,2016-07-22 13:09:18,15
12991403,Don't ignore the cached NULLs in bulk createOrUpdate for Mongo,"The bulk createOrUpdate operation needs to have old versions of all updated documents. If some documents are not in the cache, they are requested from the Mongo. However, if the document key is cached with the NULL value (which means it doesn't exists), the createOrUpdate ignores such information and tries to load the document from Mongo anyway.

We shouldn't ignore these NULLs but assume that keys cached with the NULL value indicate an non-existing document.

If the document exists (because the cache became outdated), the sendBulkUpdate() method will detect it.",performance,['mongomk'],OAK,Improvement,Major,2016-07-21 10:06:30,18
12991357,Move DocumentMK specific methods from DocumentNodeStore,"There are some DocumentMK specific methods in DocumentNodeStore, which should be moved to the DocumentMK.",technical_debt,"['core', 'documentmk']",OAK,Technical task,Minor,2016-07-21 07:11:08,2
12991135,Improve FileStore.size calculation,"A new approach for calculating {{FileStore::size}} is needed because this method is prone to lock contention and should not be called too often.

The steps to implement the approach are:
# reduce the lock surface of the size() method. This should be simple enough by creating a copy of the readers / writer inside the lock and do the actual size calculation on that snapshot but outside of the lock.
# lower size() visibility to package to avoid misuse (from monitoring tools)
# remove {{approximateSize}} and associated logic and replace it with {{size()}}.
",resilience,['segment-tar'],OAK,Task,Minor,2016-07-20 14:24:43,1
12991058,Clarify weight related methods/parameters/arguments of the LIRS cache,"{{CacheLIRS}} has various means for specifying the weight of an element:

* via {{setAverageMemory()}}
* via {{CacheLIRS.Builder.averageWeight()}}
* via {{CacheLIRS.Builder.weigher()}}
* via the {{memory}} argument of {{put()}}

It is not clear how this various ways interact which each other when specifying one but not the other and which would take precedence if multiple are specified. 

Moreover there is the related {{CacheStats}} class, which also require a {{Weigher}}. How does that one related to the arguments of the respective cache instance? 

[~tmueller], could you please help clarifying these points? E.g. by expanding on the Javadoc.",documentation,['core'],OAK,Improvement,Major,2016-07-20 09:28:31,4
12990990,BlobGC performance improvements,Parent task for blob gc performance improvements,performance,['blob'],OAK,Improvement,Major,2016-07-20 04:03:14,16
12990383,Overflow to disk threshold too high,"The overflow to disk threshold for {{StringSort}} used by JournalEntry is too high. JournalEntry assumes the threshold is in bytes, whereas the threshold is actually the number of Strings.",observation,"['core', 'documentmk']",OAK,Bug,Minor,2016-07-18 13:07:35,2
12990286,JournalEntry.applyTo() creates complete change tree in memory,"While applying the changes from {{StringSort}} to the diff cache, the method recreates the entire change tree in memory. Depending on the revision range, the number of changes can be very high and cause an OOME.",observation,"['core', 'documentmk']",OAK,Bug,Major,2016-07-18 07:56:39,2
12990274,Define oak:Resource nodetype as non referenceable alternative to nt:resource,"In most cases where code uses JcrUtils.putFile [1] it leads to
creation of below content structure

{noformat}
+ foo.jpg (nt:file)
   + jcr:content (nt:resource)
       - jcr:data
{noformat}

Due to usage of nt:resource each nt:file node creates a entry in uuid
index as nt:resource is referenceable. So if a system has 1M
nt:file nodes then we would have 1M entries in /oak:index/uuid as in
most cases the files are created via [1] and hence all such files are
referenceable

The nodetype defn for nt:file does not mandate that the
requirement for jcr:content being nt:resource. To support such non referenceable files we would define a new nodeType similar to nt:resource but which is non referenceable.

See [2] for related discussion

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-jcr-commons/src/main/java/org/apache/jackrabbit/commons/JcrUtils.java#L1062
[2] http://jackrabbit-oak.markmail.org/thread/qicpzm5ltnzfsd42",docs-impacting,['core'],OAK,Improvement,Major,2016-07-18 07:04:38,19
12989746,S3Backend fails to upload large metadata records,"If a large enough metadata record is added to a S3 DS (like the list of blob references collected during the mark phase of the MarkSweepGC) the upload will fail (i.e. never start). This is caused by {{S3Backend.addMetadataRecord()}} providing an InputStream to the S3 TransferManager without specifying the size in the Metadata. 
A warning to this effect is logged by the AWS SDK each time you add a metadata record: 
{noformat}
[s3-transfer-manager-worker-1] AmazonS3Client.java:1364 No content length specified for stream data.  Stream contents will be buffered in memory and could result in out of memory errors.
{noformat}

Normally this shouldn't be too big of a problem but in a repository with over 36 million blob references the list of marked refs produced by the GC is over 5GB. In this case the S3 transfer worker thread will be stuck in a seemingly endless loop where it tries to allocate the memory reading the file into memory and never finishes (although the JVM has 80GB of heap), eating away resources in the process:

{noformat}
   java.lang.Thread.State: RUNNABLE
	at org.apache.http.util.ByteArrayBuffer.append(ByteArrayBuffer.java:90)
	at org.apache.http.util.EntityUtils.toByteArray(EntityUtils.java:137)
	at org.apache.http.entity.BufferedHttpEntity.<init>(BufferedHttpEntity.java:63)
	at com.amazonaws.http.HttpRequestFactory.newBufferedHttpEntity(HttpRequestFactory.java:247)
	at com.amazonaws.http.HttpRequestFactory.createHttpRequest(HttpRequestFactory.java:126)
	at com.amazonaws.http.AmazonHttpClient$ExecOneRequestParams.newApacheRequest(AmazonHttpClient.java:650)
	at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:730)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:505)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:317)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3595)
	at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1382)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:139)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:47)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat} 

The last log message by the GC thread will be like this:
{noformat}
*INFO* [sling-oak-observation-1273] org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector Number of valid blob references marked under mark phase of Blob garbage collection [36147734]
{noformat} 

followed by the above AWS warning, then it will stall waiting for the transfer to finish.",gc s3,['blob'],OAK,Bug,Major,2016-07-15 10:45:09,16
12989714,Specify thread pool name which should be used by Async Indexing task,"While running Oak in Sling we rely on Sling Scheduler to ensure that async indexing task are run on leader (OAK-1246) with specified frequency. 

Be default Sling Scheduler uses a default pool for managing all tasks. It can happen that number of task can be quite hight which can then lead to default thread pool getting exhausted and that causes async indexing to get delayed.

To ensure that async indexing is not affected by such scenarios we should make use of a dedicated thread pool. This is now supported by SLING-5831",docs-impacting,['core'],OAK,Improvement,Minor,2016-07-15 08:36:47,19
12989393,SegmentNodeState.fastEquals() can trigger two I/O operations,"The implementation of {{SegmentNodeState.fastEquals()}} compares the stable IDs of two instances of {{SegmentNodeState}}. In some cases, reading the stable ID would trigger a read of an additional record, the block record containing the serialized version of the segment ID.

This issue is about evaluating the performance implications of this strategy and, in particular, if it would be better to store the serialized stable ID in the node record itself.",performance,['segment-tar'],OAK,Improvement,Minor,2016-07-14 09:08:48,12
12987729,SegmentDataStoreBlobGCIT failures,"Possibly followup of OAK-3793, the {{SegmentDataStoreBlobGCIT}} fails sporadically, there's probably a timing issue with collecting the {{SegmentId}} instances as UUIDs so they can be GC'ed, then later loading them, which in turn could fail after a GC.",datastore gc,['segmentmk'],OAK,Bug,Major,2016-07-08 08:51:48,14
12987139,Add info about event generation and consumption by observer,"I'm not sure if it's possible in the current scheme of things (implementation), but it'd useful to be able to easily differentiate between slow diff calculation or slow observer as a reason to see why observation queue might fill up.",candidate_oak_1_4 monitoring observation performance,"['core', 'jcr']",OAK,Improvement,Minor,2016-07-06 13:16:54,2
12987138,PerfLogger should also allow a threshold to log at INFO,"Currently, {{PerfLogger}} logs at DEBUG if time spent in operation is more that threshold ms.

We should also be able to have a second level threshold of time, beyond which the log should happen at INFO. It helps to catch cases for which the timing gets too poor at the onset of some performance issue and by the time of investigation (opportunity to add DEBUG logger) is too late already.",monitoring performance,[],OAK,Improvement,Minor,2016-07-06 13:13:43,11
12987119,Cache update blocks new commits,"When caches are updated after a commit (within CommitQueue.Callback.headOfQueue()), other threads are blocked when they try to acquire new revisions from the queue.",concurrency,"['core', 'documentmk']",OAK,Improvement,Minor,2016-07-06 11:01:10,2
12986196,diff calculation in DocumentNodeStore should try to re-use journal info on diff cache miss,"Currently, diff information is filled into caches actively (local commits pushed in local_diff, externally read changes pushed into memory_diff). At the time of event processing though, the entries could have already been evicted.
In that case, we fall back to computing diff by comparing 2 node-states which becomes more and more expensive (and eventually fairly non-recoverable leading to OAK-2683).

To improve the situation somewhat, we can probably try to consult journal entries to read a smaller-superset of changed paths before falling down to comparison.

/cc [~mreutegg], [~chetanm], [~egli]",observation resilience,"['core', 'documentmk']",OAK,Improvement,Minor,2016-07-01 14:12:30,2
12984997,Improve CommitRateLimiter to optionally block some commits,"The CommitRateLimiter of OAK-1659 can delay commits, but doesn't currently block them, and delays even those commits that are part of handling events. Because of that, the queue can still get full, and possibly delaying commits while handling events can make the situation even worse.

In Jackrabbit 2.x, we had a similar feature: JCR-2402. Also related is JCR-2746.",observation,['jcr'],OAK,New Feature,Major,2016-06-29 13:25:49,4
12982613,Offline compaction clearer output values,"I'd like to have offline compaction return a failure code in case of any error happening during the process (like an OOME). this would help greatly with scripting efforts for automated maintenance operations.

Other ideas worth pursuing:
* a one line status at the end of the process: ""Compaction succeeded in XXX min"" or ""Compaction failed in XXX mins"".
* a better directory listing pre and post compaction (sorted list, and possibly including the last modified date).
* bonus points for delta output (tar files removed, tar files added).
",compaction gc,"['run', 'segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-06-24 08:44:34,14
12982552,LucenePropertyIndex doesn't use filter's path for ACL checks of suggest queries,"When querying for suggestions, the {{LucenePropertyIndex}} performs the ACL checks for the suggested terms incorrectly if the {{oak:index}} definition is not located under the root.

In my example, I have an {{oak:index}} definition under {{/content/wcgcom/demo/example/oak:index/lucene-suggest}} looking like this:
{code}
          <lucene-suggest
              jcr:primaryType=""oak:QueryIndexDefinition""
              async=""async""
              compatVersion=""{Long}2""
              reindex=""{Boolean}false""
              reindexCount=""{Long}5""
              type=""lucene"">
              <indexRules jcr:primaryType=""nt:unstructured"">
                  <nt:base jcr:primaryType=""nt:unstructured"">
                      <properties jcr:primaryType=""nt:unstructured"">
                          <props
                              jcr:primaryType=""nt:unstructured""
                              analyzed=""{Boolean}true""
                              isRegexp=""{Boolean}true""
                              name=""jcr:(title|description)|title|subtitle|boldTitle""
                              propertyIndex=""{Boolean}true""
                              useInSuggest=""{Boolean}true""/>
                      </properties>
                  </nt:base>
              </indexRules>
              <suggestion
                  jcr:primaryType=""nt:unstructured""
                  suggestAnalyzed=""{Boolean}true""
                  suggestUpdateFrequencyMinutes=""{Long}20""/>
          </lucene-suggest>
{code}

And most relevant content under this path: {{/content/wcgcom/demo/example/home}}

When inspecting the ACL checks happening in the suggestion part of {{LucenePropertyIndex#loadDocs}} it seems the Document's path as returned by {{retrievedDoc.get(FieldNames.PATH)}} starts from the root path of the index. So in this case an example of a document path from the index above could be {{/home/about-us/news/jcr:content/headerParagraph/shortheader}} (notice that it's missing the full path to the root of the JCR workspace (specifically missing {{/content/wcgcom/demo/example}} in this case)

I believe this could be solved by simply prefixing the document path with {{filter.getPath()}}. And looking through the code, it looks like the same problem is present for the spellcheck type queries.

Here's a patch that could potentially fix this (untested): 

{noformat}
diff --git a/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java b/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java
index 7e5291f..a262f3e 100644
--- a/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java
+++ b/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java
@@ -464,7 +464,7 @@ public class LucenePropertyIndex implements AdvancedQueryIndex, QueryIndex, Nati
                             if (topDocs.totalHits > 0) {
                                 for (ScoreDoc doc : topDocs.scoreDocs) {
                                     Document retrievedDoc = searcher.doc(doc.doc);
-                                    if (filter.isAccessible(retrievedDoc.get(FieldNames.PATH))) {
+                                    if (filter.isAccessible(filter.getPath() + retrievedDoc.get(FieldNames.PATH))) {
                                         queue.add(new LuceneResultRow(suggestion.string));
                                         break;
                                     }
@@ -492,7 +492,7 @@ public class LucenePropertyIndex implements AdvancedQueryIndex, QueryIndex, Nati
                             if (topDocs.totalHits > 0) {
                                 for (ScoreDoc doc : topDocs.scoreDocs) {
                                     Document retrievedDoc = searcher.doc(doc.doc);
-                                    if (filter.isAccessible(retrievedDoc.get(FieldNames.PATH))) {
+                                    if (filter.isAccessible(filter.getPath() + retrievedDoc.get(FieldNames.PATH))) {
                                         queue.add(new LuceneResultRow(suggestion.key.toString(), suggestion.value));
                                         break;
                                     }
{noformat}",patch,['lucene'],OAK,Bug,Major,2016-06-24 02:59:53,10
12981819,Offline compaction persisted mode,"I'm investigating a case where offline compaction is unable to finish, and crashes with OOMEs because of the content structure, namely large child node lists. The biggest issue is with the UUID index which has 55M nodes.

In the current implementation, the compactor will use an inmemory nodestate to collect all the data, and persist at the very end, once compaction is done [0]. 
This is prone to OOME once the size of the data parts (no binaries involved) grows beyond a certain size (in this case I have 350Gb but there's a fair amount of garbage due to compaction not running).

My proposal is to add a special flag {{oak.compaction.eagerFlush=true}} that should be enabled only in case the size of the repo will not allow running offline compaction with the available heap size. This will turn the inmemory compaction transaction into one based on a persisted SegmentNodeState, meaning we're trading disk space (and IO) for memory.


[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/Compactor.java#L248

",compaction gc,"['segment-tar', 'segmentmk']",OAK,Bug,Major,2016-06-22 09:08:37,14
12979807,Enable writer cache for offline compaction,"Followup of OAK-4279, offline compaction should use the default writer cache.",compaction gc,['segment-tar'],OAK,Bug,Major,2016-06-16 12:41:56,14
12979401,CI failing on branches due to unknown fixture SEGMENT_TAR,"These failures are caused by adding the SEGMENT_TAR fixture to the matrix. That one doesn't exit in the branches thus the {{IllegalArgumentException}} ""No enum constant"".

See discussion http://markmail.org/message/oaptnvco5y2a4rjk",CI build jenkins,[],OAK,Bug,Major,2016-06-15 15:53:49,19
12979389,Finalise SegmentCache,"{{SegmentCache}} needs documentation, management instrumentation and monitoring tests and logging. ",cache monitoring production,['segment-tar'],OAK,Task,Major,2016-06-15 15:07:22,15
12979326,Decouple SegmentReader from Revisions,"The {{SegmentReader.readHeadState()}} introduces a de-facto dependency to {{Revisions}} as access to the latter is required for obtaining the record id of the head. 

To decouple SegmentReader from Revisions I propose to replace {{SegmentReader.readHeadState()}} with {{SegmentReader.readHeadState(Revisions revisions)}}. As this results in a lot of boilerplate for callers (i.e. {{fileStore.getReader().getHeadState(fileStore.getRevisions())}}), we should also introduce a convenience method {{FileStore.getHead()}} clients could use to that matter.
",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-06-15 10:31:10,15
12979249,More compact storage format for Documents,"Aim of this task is to evaluate storage cost of current approach for various Documents in DocumentNodeStore. And then evaluate possible alternative to see if we can get a significant reduction in storage size.

Possible areas of improvement
# NodeDocument
## Use binary encoding for property values - Currently property values are stored in JSON encoding i.e. arrays and single values are encoded in json along with there type
## Use binary encoding for Revision values - In a given document Revision instances are a major part of storage size. A binary encoding might provide more compact storage
# Journal - The journal entries can be stored in compressed form

Any new approach should support working with existing setups i.e. provide gradual change in storage format. 

*Possible Benefits*
More compact storage would help in following ways
# Low memory footprint of Document in Mongo and RDB
# Low memory footprint for in memory NodeDocument instances - For e.g. property values when stored in binary format would consume less memory
# Reduction in IO over wire - That should reduce the latency in say distributed deployments where Oak has to talk to remote primary

Note that before doing any such change we must analyze the gains. Any change in encoding would make interpreting stored data harder and also represents significant change in stored data where we need to be careful to not introduce any bug!",performance,['documentmk'],OAK,Improvement,Major,2016-06-15 05:23:12,19
12978771,Upgrade commons-io to 2.5 and remove ReversedLinesFileReader,"For OAK-2605 we copied the source of {{ReversedLinesFileReader}} to Oak to get the fix for IO-471 in. As this is now fixed in {{commons-io}} 2.5, I suggest we upgrade our dependency and remove that duplicated class.",technical_debt,"['auth-external', 'blob', 'commons', 'core', 'examples', 'parent', 'pojosr', 'run', 'segment-tar', 'segmentmk', 'webapp']",OAK,Improvement,Major,2016-06-14 12:34:33,15
12977615,Remove SegmentNodeStore.getSuperRoot(),I like to remove {{SegmentNodeStore.getSuperRoot}} That method leaks implementations details (e.g. checkpoints). Access to the super root is still possible through lower level APIs (e.g. {{SegmentReader#readHeadState}}. ,technical_debt,['segment-tar'],OAK,Technical task,Major,2016-06-10 13:56:07,15
12977548,Improve logging during compaction cycles,When compaction needs to go into cycles because of concurrent commits the number of total cycles should be logged alongside with the number of attempted cycles. ,compaction gc,['segment-tar'],OAK,Improvement,Major,2016-06-10 09:39:20,15
12977379,Setup Windows builds ,As [discussed | http://markmail.org/message/2dk6i3yxjfkknrzp] we should also have CI coverage on Windows.,CI build infrastructure jenkins,[],OAK,Technical task,Blocker,2016-06-09 20:50:55,15
12977102,Cleanup blob gc related tests,The various blob gc related tests have a fair bit of duplication. These should be de duplicated and cleaned up.,technical_debt,['blob'],OAK,Task,Minor,2016-06-09 05:49:22,16
12976918,Consistently use the term segment-tar,"We should make an effort to consistently use the term ""segment-tar"" instead of ""SegmentMK"", ""TarMK"", etc. in logging, exceptions, labels, descriptions, documentation etc.",documentation production,"['doc', 'segment-tar']",OAK,Task,Minor,2016-06-08 16:13:59,15
12976915,Implement a proper template cache,"The template cache is currently just a map per segment. This is problematic in various ways: 
* A segment needs to be in memory and probably loaded first only to read something from the cache. 
* No monitoring, instrumentation of the cache
* No control over memory consumption 

We should there for come up with a proper template cache implementation in the same way we have done for strings ({{StringCache}}) in OAK-3007. Analogously that cache should be owned by the {{CachingSegmentReader}}. ",cache monitoring production,['segment-tar'],OAK,Improvement,Major,2016-06-08 16:08:39,15
12976911,Properly split the FileStore into read-only and r/w variants ,"The {{ReadOnlyFileStore}} class currently simply overrides the {{FileStore}} class replacing all mutator methods with a trivial implementation. This approach however leaks into its ancestor as the read only store needs to pass a flag to the constructor of its super class so some fields can be instantiated properly for the read only case. 

We should clean this up to properly separate the read only and the r/w store. Most likely we should factor the commonalities into a common, abstract base class.",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-06-08 15:57:37,14
12976909,SegmentNodeStore and SegmentStore builders should log their parameters on build(),{{SegmentNodeStoreBuilder}} and {{FileStoreBuilder}} should log the arguments used to build new instances of the respective classes when one of its {{build()}} methods is called. This facilitates post mortem analysis of log files.,logging production,['segment-tar'],OAK,Improvement,Minor,2016-06-08 15:51:47,15
12976870,Release oak-segment-tar,"Tweak our setup in order to be able to cut an initial release of {{oak-segment-tar}} and perform the release. 

",release,['segment-tar'],OAK,Task,Major,2016-06-08 13:38:34,12
12976859,Collect write statistics ,"We should come up with a good set of write statistics to collect like number of records/nodes/properties/bytes. Additionally those statistics should be collected for normal operation vs. compaction related operation. This would allow us to more precisely analyse the effect of compaction on the overall system. 
",compaction gc monitoring,['segment-tar'],OAK,Improvement,Major,2016-06-08 13:12:24,15
12976857,Reduce number of calls to NodeBuilder.getNodeState from MergingNodeStateDiff,The result of the static {{MergingNodeStateDiff.merge}} method is only used in the base case of a recursive diff. In all other cases while traversing the child diffs that result is simply discarded. As calculating the result involves an extra call to {{NodeBuilder.getNodeState}} it inflicts a performance penalty in the case of segment-tar: in that case this call causes the changes in the builder to be written ahead into the store. I figure it is simple enough to specialise the merge method in a way so that call is only done when its result is actually used. ,perfomance,['core'],OAK,Improvement,Major,2016-06-08 13:06:54,15
12976475,Fix the errors reported by the Javadoc tool in JDK8,Some Javadoc is not strict enough according to the Javadoc tool shipped in JDK8.,candidate_oak_1_4,[],OAK,Bug,Major,2016-06-07 14:44:49,12
12976451,Segments created by an unsuccessful compaction run should get cleaned,Cleaning of segment created by an unsuccessful compaction run currently only works if forced compaction is enabled. Otherwise those segments will only get cleaned in a much later cleanup cycle. ,compaction gc,['segment-tar'],OAK,Bug,Major,2016-06-07 13:58:44,15
12976346,HeavyWriteIT sporadically fails,"I've seen {{HeavyWriteIT}} fail sporadically on my local checkout.

{noformat}
3d13e2927fc0d75454a692ef5c8703880dc2ea0d
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 31b75992-aaf7-4f2b-a5de-b5a268c1fdb3 not found

	at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1377)
	at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1317)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1011)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:972)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:283)
	at org.apache.jackrabbit.oak.segment.SegmentCache.geSegment(SegmentCache.java:80)
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1317)
	at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:111)
	at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:94)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1010)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:906)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:885)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.access$700(SegmentWriter.java:319)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$8.execute(SegmentWriter.java:277)
	at org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.execute(SegmentBufferWriterPool.java:110)
	at org.apache.jackrabbit.oak.segment.SegmentWriter.writeNode(SegmentWriter.java:274)
	at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:111)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:516)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:284)
	at org.apache.jackrabbit.oak.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:91)
{noformat}

I suspect this is a problem with {{isOldGeneration}} itself not being prepared for the old segment actually being gone. ",gc,['segment-tar'],OAK,Bug,Critical,2016-06-07 08:19:55,15
12976344,checkpointDeduplicationTest sometimes fails on Jenkins,"{{CompactionAndCleanupIT.checkpointDeduplication}} irregularly [fails|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/938/jdk=latest1.7,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/console] on Jenkins. 

This might point to an issue with the de-duplication caches, which are crucial in getting the checkpoints de-duplicated. 

{code}
checkpointDeduplicationTest(org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT)  Time elapsed: 0.15 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<[7211975a-04ce-45ff-aff5-16795ec2cc72]:261932> but was:<[11083c4b-9b2e-4d17-a8c0-8f6b1f2a3173]:261932>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.checkpointDeduplicationTest(CompactionAndCleanupIT.java:899)
{code}
",compaction gc test,['segment-tar'],OAK,Bug,Critical,2016-06-07 08:12:27,15
12976334,Remove segment version argument from segment writer and and related classes,"The {{SegmentWriter}} and its related classes accept a {{SegmentVersion}} argument. This is confusing since that version is only stored in the segment's segment version field. The writer cannot and does not actually write segments at older version than the latest (12). 

I suggest we remove the explicit segment version from all classes where it can be specified and hard code the segment version to 12 for now. This is the only segment version {{segment-tar}} currently supports anyway. Should  the need to support other segment version arise in the future, we need to decide at that point how to parametrise {{segment-tar}} on the segment version. ",refactoring,['segment-tar'],OAK,Improvement,Major,2016-06-07 07:40:43,15
12975788,Optimize RevisionVector methods,"{{RevisionVector}} is used in very critical paths and we should look into optimzing some of its critical method

",performance,['documentmk'],OAK,Improvement,Minor,2016-06-05 06:30:35,19
12975375,Optimize PathUtils.concat by using a properly sized StringBuilder,{{PathUtils.concat}} does not specify the default size of StringBuilder. Default string constructor uses a string.length() + 16 as the buffer size. Given size of appended path is known we can properly size the string builder buffer to avoid any expansion during actual append,performance,['commons'],OAK,Improvement,Minor,2016-06-03 04:25:04,19
12975125,Optimize Revison fromString and toString implementation,"Current implementation of Revision {{fromString}} and {{toString}} make use of std JDK API to perform string manipulation. While running some performance test it was seen that these 2 methods are called quite frequently and that adds up to some decent times. Further they also generate quite a bit of short lived objects.

!hot-methods.png!

It would be worthwhile to perform a micro benchmark of these method and optimize them further such that they perform better and also generate less garbage. The micro optimized code would be bit more complex but if performance numbers are better we can look into changing the current implementation",performance,['documentmk'],OAK,Improvement,Major,2016-06-02 11:04:58,19
12974757,Move temp files to target directory,"(Some of?) the changes done in the other subtasks cause the temporary files to be created in the systems temporary folder instead of the target folder as before. This causes issues on system where the temporary folder resides on a small partition. 

See my [comment |https://issues.apache.org/jira/browse/OAK-4208?focusedCommentId=15296019&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15296019] on OAK-4208 where we have been seeing this on the Apache Jenkins instance. See also  INFRA-11837.",test,[],OAK,Technical task,Minor,2016-06-01 10:21:52,12
12972439,Correlate index with the index definition used to build it,"Currently, if the definition of an index is changed without reindexing, it will get in an ""inconsistent"" state. 

Of course, the reindexing is usually necessary, but it would be useful to know with which definition the index was built. This could increase the visibility of the indexing state and help debugging issues related to it.

Some questions this improvement should respond to:
# What is the definition of the index when the (re)indexing was triggered?
# Are there any changes in the definition since the trigger? Which?

I can imagine a solution built by ""versioning"" the definition nodes (oak:QueryIndexDefinition). When the reindex is triggered, a new version of the node is created and the indexer stores a reference to it.
This would also allow the indexer to keep using the same definition until a new reindex, even if changes are made meanwhile (i.e. use a fixed version instead of the latest definition).",docs-impacting,"['lucene', 'query']",OAK,Improvement,Major,2016-05-24 16:36:42,19
12972329,DefaultSyncContext.syncMembership may sync group of a foreign IDP,"With the following scenario the {{DefaultSyncContext.syncMembership}} may end up synchronizing (i.e. updating) a group defined by an foreign IDP and even add the user to be synchronized as a new member:

- configuration with different IDPs
- foreign IDP synchronizes a given external group 'groupA' => rep:externalID points to foreign-IDP (e.g. rep:externalId = 'groupA;foreignIDP')
- my-IDP contains a group with the same ID (but obviously with a different rep:externalID) and user that has declared group membership pointing to 'groupA' from my IDP

if synchronizing my user first the groupA will be created with a rep:externalId = 'groupA;myIDP'.
however, if the group has been synced before by the foreignIDP the code fails to verify that an existing group 'groupA' really belongs to the same IDP and thus may end up synchronizing the group and updating it's members.

IMHO that's a critical issue as it violates the IDP boundaries.
the fix is pretty trivial as it only requires testing for the IDP of the existing group as we do it in other places (even in the same method).",security,['auth-external'],OAK,Bug,Critical,2016-05-24 09:18:51,0
12972025,Run SegmentParserTest off memory store instead of file store,Running of the memory store would improve test speed without impacting test coverage.,test,['segment-tar'],OAK,Improvement,Major,2016-05-23 10:30:23,15
12972022,Remove deprecated string cache,OAK-3007 replaced the now deprecated strings cache with a proper cache. We should now remove the former. ,technical_debt,['segment-tar'],OAK,Technical task,Major,2016-05-23 10:21:29,15
12972018,Decouple FileStoreStatsTest,"That test is currently unnecessarily strongly tied to the file store, which makes it prone to failing if implementation details in the store change. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-05-23 10:09:02,15
12971136,Batch mode for SyncMBeanImpl,"the {{SyncMBeanImpl}} currently calls {{Session.save()}} for every single sync, which IMO make the synchronization methods extra expensive.

IMHO we should consider introducing a batch mode that reduces the number of save calls. the drawback of this was that the complete set of sync-calls withing a given batch would succeed or fail. in case of failure the 'original' sync-result would need to be replaced by one with operation status 'ERR'.

now that we have the basis for running benchmarks for the {{SyncMBeanImpl}}, we should be able to verify if this proposal actually has a positive impact (though benchmark results from OAK-4119 and OAK-4120 seem to indicate that this is the case).",performance,['auth-external'],OAK,Improvement,Major,2016-05-19 12:43:25,0
12970406,Refactor SegmentTracker,"The {{SegmentTracker}} class has become the dumping ground for everything that wouldn't fit else where. In a personal discussion with [~frm], we figured that this class might be a good starting point refactoring {{segment-tar}} towards better encapsulation. 
The aim would be to return {{SegmentTracker}} to its initial purpose (i.e. tracking segments) and move all unrelated concerns elsewhere.",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-05-17 15:36:45,15
12970308,Overly zealous warning about checkpoints on compaction ,"{{FileStore.compact}} logs a warning {{TarMK GC #{}: compaction found {} checkpoints, you might need to run checkpoint cleanup}} if there is more than a single checkpoints. 

AFIK this is now the norm as async indexing has uses 2 checkpoints ([~chetanm], [~edivad] please clarify). 

In any case should we improve this and not hard code any number of expected checkpoints. Maybe make the threshold configurable?",compaction gc logging,"['segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-05-17 09:23:47,15
12962944,BlobReferenceRetriever#collectReferences should allow exceptions,"{{BlobReferenceRetriever#collectReferences}} currently does not allow implementations to throw an exception. In case anything goes wrong during reference collection, implementations should be able to indicate this through an exception so the DSGC can safely abort. ",datastore gc resilience,"['core', 'segment-tar']",OAK,Improvement,Major,2016-04-27 09:34:09,15
12962623,Fix test failures in SegmentDataStoreBlobGCIT,"{{SegmentDataStoreBlobGCIT#gcWithInlined}}, {{gc}}, {{gcLongRunningBlobCollection}} and {{consistencyCheckWithGc}} fail since the removal of the old cleanup strategy in OAK-4276. 

The test setup needs to be adapted to the brutal strategy: i.e. {{setup()}} needs to simulate so many compaction cycles until a subsequent cleanup actually remove the segments in question. 

This is not sufficient though as then {{SegmentTracker#collectBlobReferences}} causes a SNFE for those segment ids actually removed but still in the segment id tables. ",cleanup gc,['segment-tar'],OAK,Task,Major,2016-04-26 15:08:29,15
12962573,Align property labels and descriptions in SegmentNodeStoreService,"We need to align / improve the labels and descriptions in {{SegmentNodeStoreService}} to match their actual purpose. At the same time I would opt for changing ""compaction"" to ""revision gc"" in all places where it is used synonymously for the latter. ",production,['segment-tar'],OAK,Task,Major,2016-04-26 12:06:47,15
12962192,Use the oak-segment-next in the oak-upgrade tests,"Since the oak-segment will be deprecated, let's switch to oak-segment-next in all oak-upgrade tests (apart from the specific test covering the ""classic"" oak-segment support).",migration upgrade,"['segment-tar', 'upgrade']",OAK,Task,Major,2016-04-25 09:23:57,18
12962161,Missing protection for system-maintained rep:externalId ,"while working on OAK-4101 i noticed that the current implementation doesn't provide any protection for the system maintained property {{rep:externalId}}, which is intended to be an identifier for a given synchronized user/group within an external IDP.

in other words:
- the system doesn't assert the uniqueness of a given external-id
- the external-id properties can be changed using regular JCR API 

up to now i didn't manage to exploit the missing protection with the current default implementation but i found that minor (legitimate) changes have the potential to turn this into a critical vulnerability.

therefore I would strongly recommend to change the default implementation such that the rep:externalId really becomes system-maintained and prevent any unintentional or malicious modification outside of the scope of the sync-operations. furthermore uniqueness of this property should be asserted.",security,['auth-external'],OAK,Bug,Critical,2016-04-25 07:37:09,0
12962140,Cost per entry for Lucene index of type v1 should be higher than that of v2,"Currently default cost per entry for Lucene index of type
# v1 - which uses query time aggregation
# v2 - which uses index time aggregation

Are same. However given that query time aggregation would require more effort it should result in a higher cost per entry.

This fact impacts the result in cases like OAK-2081 (see last few comments) where with usage of limits both index are currently considered equals",candidate_oak_1_4,['lucene'],OAK,Bug,Minor,2016-04-25 04:19:11,19
12962133,oak-run console should connect to repository in read-only mode by default,"With OAK-4182 and OAK-4298, oak-run->console supports connecting to a repository in read only mode. But, as [~edivad] suggested [here|https://issues.apache.org/jira/browse/OAK-4182?focusedCommentId=15245661&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15245661], it might be more useful to connect to repositories in read-only mode by default and require an explicit read-write flag for write operations.

This, of course, doesn't align with 'always read-write mode' that's operational currently.

Btw, since, 1.5.2 hasn't been release yet, so, we can do this switch pretty cleanly currently. Post 1.5.2 (which would then have {{--read-only}}, we might have to retain that flag)",production tools,['run'],OAK,Improvement,Minor,2016-04-25 03:24:58,11
12962103,oak-run->console should have a read-only mode to connect to segment store,"Similar to OAK-4182, we should expose a parameter ({{\-\-read-only}}) to oak-run->console while stating oak-run to connect to a segment storage. Note, read-only mode can be a global boolean for lifetime of oak-run (i.e. we don't really need dynamic switching between read-only and read-write modes)",production tools,['run'],OAK,Improvement,Minor,2016-04-24 20:05:18,11
12961897,Proper versioning of storage format,"OAK-3348 introduced changes to the segment format (which has been bumped to 12 with OAK-4232). However it also changes the format of the tar files (the gc generation of the segments is written to the index file) which would also require proper versioning.

In a offline discussion [~frm] brought up the idea of adding a manifest file to the store that would specify the format versions of the individual components. ",resilience technical_debt,['segment-tar'],OAK,Task,Major,2016-04-22 20:05:14,12
12961896,Consider making FileStore.writer volatile,That filed is not volatile although access by different threads. We should consider changing it to volatile.,technical_debt,['segment-tar'],OAK,Improvement,Minor,2016-04-22 20:01:12,15
12961892,Refactor / rework compaction gain estimation ,"I think we have to take another look at {{CompactionGainEstimate}} and see whether we can up with a more efficient way to estimate the compaction gain. The current implementation is expensive wrt. IO, CPU and cache coherence. If we want to keep an estimation step we need IMO come up with a cheap way (at least 2 orders of magnitude cheaper than compaction). Otherwise I would actually propose to remove the current estimation approach entirely ",gc,['segment-tar'],OAK,Task,Major,2016-04-22 19:49:25,14
12961891,Document oak-segment-tar,"Document Oak Segment Tar. Specifically:
* New and changed configuration and monitoring options
* Changes in gc (OAK-3348 et. all)
* Changes in segment / tar format (OAK-3348)
",documentation gc,"['doc', 'segment-tar']",OAK,Task,Major,2016-04-22 19:43:02,12
12961884,FileStore.flush prone to races leading to corruption,"There is a small window in {{FileStore.flush}} that could lead to data corruption: if we crash right after setting the persisted head but before any delay-flushed {{SegmentBufferWriter}} instance flushes (see {{SegmentBufferWriterPool.returnWriter()}}) then that data is lost although it might already be referenced from the persisted head.

We need to come up with a test case for this. 

A possible fix would be to return a future from {{SegmentWriter.flush}} and rely on a completion callback. Such a change would most likely also be useful for OAK-3690. 
",resilience,['segment-tar'],OAK,Bug,Critical,2016-04-22 19:36:57,15
12961872,Update segment parser to work with the new segment format,{{SegmentParser}} does not correctly handle the record id added to the segment node states: for those segment node state containing an actual record id the segment parser should process it and call the respective call backs. ,gc tooling,"['run', 'segment-tar']",OAK,Task,Minor,2016-04-22 19:23:42,15
12961864,Remove the gc generation from the segment meta data,The segment meta info (OAK-3550) still contains the segment's gc generation. As with OAK-3348 the gc generation gets written to the segment header directly we should remove it from the segment meta info and update {{oak-run graph}} accordingly. ,compaction gc tooling,"['run', 'segment-tar']",OAK,Task,Minor,2016-04-22 19:14:59,15
12961852,TarReader.calculateForwardReferences only used by oak-run graph tool,{{TarReader.calculateForwardReferences}} is not used for production but only for tooling so it would be good if we could remove that method from the production code an put it into a tooling specific module. ,cleanup gc tooling,['segment-tar'],OAK,Task,Minor,2016-04-22 19:09:49,15
12961845,Disable / remove SegmentBufferWriter#checkGCGen,"{{SegmentBufferWriter#checkGCGen}} is an after the fact check for back references (see OAK-3348), logging a warning if detects any. As this check loads the segment it checks the reference for, it is somewhat expensive. We should either come up with a cheaper way for this check or remove it (at least disable it by default). ",assertion compaction gc,['segment-tar'],OAK,Task,Major,2016-04-22 19:03:44,15
12961706,Rework failing tests in CompactionAndCleanupIT,The fix for OAK-3348 caused some of the tests in {{CompactionAndCleanupIT}} to fail and I put the to ignored for the time being. We need to check whether the test expectations still hold and rework them as required. ,cleanup compaction gc tests,['segment-tar'],OAK,Task,Major,2016-04-22 15:58:09,15
12961705,Align cleanup of segment id tables with the new cleanup strategy ,"We need to align cleanup of the segment id tables with the new ""brutal"" strategy introduced with OAK-3348. That is, we need to remove those segment id's from the segment id tables whose segment have actually been gc'ed. ",cleanup gc,['segment-tar'],OAK,Task,Major,2016-04-22 15:53:12,14
12961701,Garbage left behind when compaction does not succeed,"As a result of the new cleanup approach introduced with OAK-3348 (brutal) a compaction cycle that is not successful (either because of cancellation of because of giving up waiting for the lock) leaves garbage behind, which is only cleaned up 2 generations later. 

We should look into ways to remove such garbage more pro-actively. 
",cleanup compaction gc,['segment-tar'],OAK,Improvement,Major,2016-04-22 15:48:55,15
12961693,Align GCMonitor API with implementation ,The argument taken by {{GCMonitor.compacted}} related to parameters of the compaction map. The latter has gone with OAK-3348. We need to come up with a way to adjust this API accordingly. Also it might make sense to broaden the scope of {{GCMonitor}} from its initial intent (logging) to a more general one as this is how it is already used e.g. by the {{RefreshOnGC}} implementation and for OAK-4096. ,api-change compaction gc,['segment-tar'],OAK,Task,Major,2016-04-22 15:41:32,14
12961684,Make the number of retained gc generation configurable,"The number of retained gc generations (""brutal"" cleanup strategy) is currently hard coded to 2. I think we need to make this configurable. ",cleanup gc,['segment-tar'],OAK,Task,Major,2016-04-22 15:34:56,15
12961679,Rework memory estimation for compaction,As a result of OAK-3348 we need to partially rework the memory estimation step done for deciding whether compaction can run or not. In {{oak-segment}} there was a {{delta}} value derived from the compaction map. As the latter is gone in {{oak-segment-next}} we need to decide whether there is another way to derive this delta or whether we want to drop it entirely. ,compaction gc,['segment-tar'],OAK,Task,Major,2016-04-22 15:29:31,15
12961673,Compaction cannot be cancelled ,"As a result of the de-duplication cache based online compaction approach from OAK-3348 compaction cannot be cancelled any more (in the sense of OAK-3290). 

As I assume we still need this feature we should look into ways to re-implement it on top of the current approach. 

Also I figure implementing a [partial compaction | https://issues.apache.org/jira/browse/OAK-4122?focusedCommentId=15223924&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15223924] approach on top of a commit scheduler (OAK-4122) would need a feature of this sort. ",compaction gc,['segment-tar'],OAK,Improvement,Major,2016-04-22 15:20:26,15
12961664,Rework offline compaction,"The fix for OAK-3348 broke some of the previous functionality of offline compaction:
* No more progress logging
* Compaction is not interruptible any more (in the sense of OAK-3290)
* Offline compaction could remove the ids of the segment node states to squeeze out some extra space. Those are only needed for later generations generated via online compaction. 

We should probably implement offline compaction again through a dedicated {{Compactor}} class as it was done in {{oak-segment}} instead of relying on the de-duplication cache (aka online compaction). 

",compaction gc,['segment-tar'],OAK,Task,Blocker,2016-04-22 14:57:07,14
12961662,Fix backup and restore,"{{FileStoreBackup}} and {{FileStoreRestore}} are currently broken as a side effect of the fix from OAK-3348. ({{FileStoreBackupTest}} is currently being skipped). 

In {{oak-segment}} backup and restore functionality relied on the {{Compactor}} class. The latter is gone in {{oak-segment-next}} as it is not needed for online compaction any more. 
Instead of sharing functionality from compaction directly, I think backup and restore should come up with its own implementation that could be individually tweaked for its task. If there is commonalities with offline compaction those can still be shared thorough a common base class. See OAK-4279",backup restore technical_debt,['segment-tar'],OAK,Task,Critical,2016-04-22 14:45:38,14
12961656,Finalise de-duplication caches,"OAK-3348 ""promoted"" the record cache to a de-duplication cache, which is heavily relied upon during compaction. Now also node states go through this cache, which can seen as one concern of the former compaction map (the other being equality). 
The current implementation of these caches is quite simple and served its purpose for a POC for getting rid of the ""back references"" (OAK-3348). Before we are ready for a release we need to finalise a couple of things though:

* Implement cache monitoring and management
* Make cache parameters now hard coded configurable
* Implement proper UTs 
* Add proper Javadoc
* Fine tune eviction logic and move it into the caches themselves (instead of relying on the client to evict items pro-actively)
* Fine tune caching strategies: For the node state cache the cost of the item is determined just by its position in the tree. We might want to take further things into account (e.g. number of child nodes). Also we might want to implement pinning so e.g. checkpoints would never be evicted. 
* Finally we need to decide who should own this cache. It currently lives with the {{SegmentWriter}}. However this is IMO not the correct location as during compaction there is dedicated segment writer whose cache need to be shared with the primary's segment writer upon successful completion. ",caching compaction gc monitoring,['segment-tar'],OAK,Task,Major,2016-04-22 14:29:25,15
12961650,Refactor / rework compaction strategies ,"After the changes from OAK-3348 many if not all of the options in {{CompactionStrategy}} do not apply any more. Specifically the new ""brutal"" strategy is hard coded to always be in effect. We need to:

* Decide which cleanup methods we want to keep supporting,
* decide which options to expose through CompactionStrategy. E.g. {{cloneBinaries}} was so far always set to {{false}} and I would opt to remove the option as implementing might be tricky with the de-duplication cache based compaction we now have,
* optimally refactor {{CompactionStrategy}} into a proper abstract data type. ",compaction gc technical_debt,['segment-tar'],OAK,Task,Major,2016-04-22 14:05:01,15
12960635,Provide a way to abort an async indexing run,"In some cases where a user is tweaking the indexing config it can happen that he saves the config mid way which triggers a long indexing run. Currently there is no easy way to abort such a run and only way to avoid wasting time in the long indexing cycle is to shut down the system.

For such cases it would be good to provide an ""abort"" operation as part of {{IndexStatsMBean}} which user can invoke to abort any run safely and cleanly",docs-impacting,['query'],OAK,Improvement,Major,2016-04-21 07:39:17,19
12960374,Define and implement migration from oak-segment to oak-segment-tar,"We need to come up with a plan, implementation and documentation for how we deal with migrating from {{oak-segment}} to {{oak-segment-next}}. ",migration,"['segment-tar', 'segmentmk', 'upgrade']",OAK,Task,Major,2016-04-20 15:01:17,18
12960367,Implement fixtures for running again oak-segment and/or oak-segment-next,"We need fixtures to run UTs / ITs against either or both segment implementations {{oak-segment}} and {{oak-segment-next}}. 

Ideally we can enable them individually through e.g. environment variables. A standard build would run against {{oak-segment}} so not to affect others. {{oak-segment-next}} could be enabled on request locally or for the CI. 
Once we deprecate {{oak-segment}} we would switch the default fixture to {{oak-segment-next}}. ",testing,['segment-tar'],OAK,Task,Blocker,2016-04-20 14:35:39,12
12960329,CLONE - Cross gc sessions might introduce references to pre-compacted segments,"I suspect that certain write operations during compaction can cause references from compacted segments to pre-compacted ones. This would effectively prevent the pre-compacted segments from getting evicted in subsequent cleanup phases. 

The scenario is as follows:
* A session is opened and a lot of content is written to it such that the update limit is exceeded. This causes the changes to be written to disk. 
* Revision gc runs causing a new, compacted root node state to be written to disk.
* The session saves its changes. This causes rebasing of its changes onto the current root (the compacted one). At this point any node that has been added will be added again in the sub-tree rooted at the current root. Such nodes however might have been written to disk *before* revision gc ran and might thus be contained in pre-compacted segments. As I suspect the node-add operation in the rebasing process *not* to create a deep copy of such nodes but to rather create a *reference* to them, a reference to a pre-compacted segment is introduced here. 

Going forward we need to validate above hypothesis, assess its impact if necessary come up with a solution.
",cleanup compaction gc,['segmentmk'],OAK,Improvement,Critical,2016-04-20 12:47:20,15
12960318,CLONE - FileStore.containsSegment returns alway true (almost),"{{FileStore.containsSegment()}} looks [funky|https://github.com/mduerig/jackrabbit-oak/blob/36cb3bf6e5078e3afa75581fb789eeca7b5df2e2/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/FileStore.java#L1197-L1197]. This ""optimisation"" causes it to always return {{true}}. 

{{containsSegment}} is used for deduplication and revision gc. The current implementation causes {{SNFE}} exceptions once gc is effective (as I experienced while working on OAK-3348). ",compaction gc stability,['segment-tar'],OAK,Bug,Critical,2016-04-20 12:29:20,15
12960315,CLONE - BackgroundThread should log and re-throw instances of Error,If the run method of a {{BackgroundThread}} instance hits an {{Error}} it dies silently. Instead it should log an re-throw the error. ,resilience,['segment-tar'],OAK,Improvement,Major,2016-04-20 12:24:48,15
12960313,CLONE - TarReader#loadGraph wrongly detects segment graph as corrupt ,{{org.apache.jackrabbit.oak.plugins.segment.file.TarReader#loadGraph}} sometimes detects a segment graph as corrupt although it isn't. This results in cleanup rewriting the tar file (all over again). ,cleanup gc,['segment-tar'],OAK,Bug,Major,2016-04-20 12:19:56,15
12960275,Deprecate oak-segment,"Before the next major release we need to deprecate {{oak-segment}} and make {{oak-segment-tar}} the new default implementation:

* Deprecate all classes in {{oak-segment}}
* Update documentation to reflect this change
* Update tooling to target {{oak-segment-tar}} (See OAK-4246). 
* Update dependencies of upstream modules / projects from {{oak-segment}} to {{oak-segment-tar}}. 
* Ensure {{oak-segment-tar}} gets properly released (See OAK-4258). 
* Tests run against the {{SEGMENT_TAR}} fixture.",technical_debt,"['segment-tar', 'segmentmk']",OAK,Task,Critical,2016-04-20 09:20:15,12
12960273,Update segment tooling to choose target store,"We need to add command line options segment specific tooling so users could chose between {{oak-segment}} and {{oak-segment-next}}. {{oak-segment}} should be the default until deprecated, where {{oak-segment-next}} should be made the default. ",tooling,['segment-tar'],OAK,Task,Blocker,2016-04-20 09:16:20,12
12959937,Bump segment version to 12,"We need to bump {{SegmentVersion}} to 12 to properly reflect the change in persistence format. At the same time we need to remove our dependencies to older segment versions as this is a non backward compatible change. 

All segment stores written by code prior to this change will not work any more with code once this change has been applied and vice versa. 

",compatibility version,['segment-tar'],OAK,Technical task,Major,2016-04-19 08:47:03,15
12958457,oak-run console should have command to export relevant documents (same as oak-mongo.js' printMongoExportCommand),"Oak-run console should export a command to export relevant documents from connected repository.
Something like:
{noformat}
/> export-docs /oak:index/my_index/:index/test/content/a export.json
{noformat}",production tools,['run'],OAK,Improvement,Minor,2016-04-13 12:37:53,11
12958434,Remove deprecated constructors from SegmentNodeStore,Switch API clients to {{SegmentNodeStoreBuilder}} instead. ,technical_debt,['segmentmk'],OAK,Technical task,Major,2016-04-13 10:27:52,12
12958390,Refactor DocumentNodeStore to have a ReadOnlyDocumentNodeStore which is gives read-only behavior,With OAK-4148 we can now have a read only document node store. That required us to put if(readOnly){} at places. It'd be cleaner to refactor out a ReadOnlyDocumentNodeStore to take care of the read-only parts.,technical_debt,['documentmk'],OAK,Improvement,Minor,2016-04-13 07:39:11,11
12958024,"oak-run->console should have a read-only mode to connect to document stores (mongo, rdb, etc)","oak-run, when connecting to a persistence storage, works like a live/independent oak instance. For clustered setup (read doc stores such backed by mongo or rdb), this means that another connected instance would treat oak-run's connection as a new one. While, theoretically, there is no issue with it and the world would work fine BUT an investigative tool such as oak-run->console should minimize touching the system as much as possible (especially while collecting forensics).

So, we should expose a parameter (say {{\-\-read-only}} to oak-run->console while stating oak-run to connect to a document store storage. Note, read-only mode can be a global boolean for lifetime of oak-run (i.e. we don't really need dynamic switching between read-only and read-write modes)",docs-impacting production tools,['run'],OAK,Improvement,Minor,2016-04-12 08:26:46,11
12958002,Use another NodeStore as a local cache for a remote Document store,"DocumentNodeStore makes use of persistent cache to speed up its processing and save on making remote calls for data already present in cache

In addition to that we can look into make use of Segment NodeStore as kind of ""local copy"" for certain paths in repository and route calls to it if possible. As part of this task I would like to prototype such an approach. At high level it would work as below

# At start bootstrap the setup and shutdown it down
# Use a modified ""sidegrade"" and copy over the NodeStats from Document store to Segment store. In such a copy we also store some Document specific properties like {{readRevision}} and {{lastRevision}} as hidden property in Segment NodeStates
# In DocumentNodeStore we refactor the current code to extract a 
## {{AbstractDocumentNodeState}} - Abase class which has some logic move out from {{DocumentNodeState}}
## {{SegmentDocumentNodeState}} extends above and delegate calls to a wrapped {{SegmentNodeState}}
## {{DocumentNodeState}} would also extend {{AbstractDocumentNodeState}} and hence delegate to some calls to parent. In this when a call comes for {{getChildNode}} it can check if that can be served by a local copy of {{SegmentNodeStore}} for given {{rootRevision}} then it delegates to that
# For update plan is to make use of {{Observer}} which listens to changes and updates the local copy for certain configured paths. 
## Key aspect to address here is handle the restart case where in a cluster a specific node restarts after some time then how it refreshes itself there

*Usage*
Following 2 OSGi configs would need to be seed

* org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.config
{noformat}
secondary=B""true""
{noformat}
* org.apache.jackrabbit.oak.plugins.document.secondary.SecondaryStoreCacheService.config
{noformat}
includedPaths=[ \
  ""/"",
  ]
{noformat}

With these settings if DocumentNodeStoreService gets started it would pickup the cache and use it. Change {{includedPaths}} depending on paths in repository which you want to include in secondary store.

*Feature Docs*
http://jackrabbit.apache.org/oak/docs/nodestore/document/secondary-store.html",secondary-nodestore,['documentmk'],OAK,New Feature,Major,2016-04-12 07:08:01,19
12955262,Too verbose logging during revision gc,"{{FileStore.cleanup}} logs the segment id of any forward reference found when including those in the reference graph. The logged information can amount to several MBs impacting normal operation. Furthermore the actually reclaimed segments are logged, which also makes the log files explode. Finally the processing of the references and individual tar files might be too wordy. 

",cleanup gc logging,['segment-tar'],OAK,Improvement,Blocker,2016-04-01 10:34:58,12
12952883,Improve tarmkrecovery docs,"Add some helper steps on output and what you can actually do with it:

{quote}
1. Run tarmkrecovery command
{code:none}
nohup java -Xmx2048m -jar oak-run-*.jar tarmkrecovery repository/segmentstore &> tarmkrecovery.log &
{code}

2. Take the output of the tarmkrecovery, take the top 10 items output (excluding ""Current head revision line"") then reverse the order of those and format them to journal.log file format (revision:offset root) and put those values in a fresh journal.log in that format
For example:
{code:none}
6ee64a26-491e-4630-ac2e-bdad1f27e73a:257016 root
5ee64a26-491e-4630-ac2e-bdad1f27e73b:257111 root
{code}

3. After setting up the new journal.log then run this command on the segmentstore
{code:none}
nohup java -Xmx2048m -jar oak-run-*.jar check -p repository/segmentstore -d &> check.log &
{code}

4. That command will give you output of which of those 10 items in the journal.log are good. Now remove all lines from the journal that come after the last known good revision.
{quote}",documentation,"['run', 'segment-tar', 'segmentmk']",OAK,Documentation,Minor,2016-03-23 16:06:58,14
12952459,allow skip UT in pedantic profile,"Back in the days we introduced the maven variable
{{-Dsurefire.skip.ut=true}} which we use on jenkins in order to skip
the unit testing when running the integrationTesting profile.

This is because we have a unittesting profile as well.

The default value for such variable is {{false}} anyhow.

Noticed that the {{pedantic}} profile does not use such variable and
therefore we have on jenkins the unit testing running twice which
consume resources and time.

Attaching a [patch|^OAK-4142-1.patch] which allows the pendatic to skip running the unit
testing.
",jenkins,[],OAK,Improvement,Major,2016-03-22 14:46:04,22
12952080,Decouple revision cleanup from the flush thread,"I suggest we decouple revision cleanup from the flush thread. With large repositories where cleanup can take several minutes to complete it blocks the flush thread from updating the journal and the persisted head thus resulting in larger then necessary data loss in case of a crash. 

/cc [~alex.parvulescu]",resilience,['segment-tar'],OAK,Improvement,Major,2016-03-21 16:15:40,15
12951446,JaasConfigSpiTest fails intermittently with missing LoginModule exception,"Following failure is seen on some CI

{noformat}
Running org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.98 sec <<< FAILURE!
defaultConfigSpiAuth(org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest)  Time elapsed: 0.97 sec  <<< ERROR!
java.lang.reflect.UndeclaredThrowableException
	at com.sun.proxy.$Proxy16.login(Unknown Source)
	at javax.jcr.Repository$login.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest.defaultConfigSpiAuth(JaasConfigSpiTest.groovy:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.jackrabbit.oak.run.osgi.OakOSGiRepositoryFactory$RepositoryProxy.invoke(OakOSGiRepositoryFactory.java:485)
	... 39 more
Caused by: javax.jcr.LoginException: No LoginModules configured for jackrabbit.oak
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:288)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:244)
	... 44 more
Caused by: javax.security.auth.login.LoginException: No LoginModules configured for jackrabbit.oak
	at javax.security.auth.login.LoginContext.init(LoginContext.java:272)
	at javax.security.auth.login.LoginContext.<init>(LoginContext.java:520)
	at org.apache.jackrabbit.oak.spi.security.authentication.JaasLoginContext.<init>(JaasLoginContext.java:49)
	at org.apache.jackrabbit.oak.security.authentication.LoginContextProviderImpl.getLoginContext(LoginContextProviderImpl.java:85)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:164)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:280)
	... 45 more
{noformat}",CI test,['pojosr'],OAK,Task,Minor,2016-03-18 06:14:36,19
12951120,Simplify IdentifierManager.getReferences,"with the improvements suggested in OAK-4119 we can simplify {{IdentifierManager.getReferences}} it is will only be used by {{Node.get(Weak)References}}, which doesn't need the whole bunch of flexibility provided by the method as it is today.

this might also be beneficial from a performance point of view as with the proposed changes the tree associated with a given result row is no longer resolved in case a property-name has been specified. also the nt-verification can be completely omitted, which afaik isn't perfectly cheap either.",technical_debt,['core'],OAK,Improvement,Major,2016-03-17 08:30:01,0
12949107,Replace the commit semaphore in the segment node store with a scheduler,"{{SegmentNodeStore}} currently uses a semaphore to coordinate concurrent commits thus relying on the scheduling algorithm of that implementation and ultimately of the JVM for in what order commits are processed. 

I think it would be beneficial to replace that semaphore with an explicit queue of pending commit. This would allow us to implement a proper scheduler optimising for e.g. minimal system load, maximal throughput or minimal latency etc. A scheduler could e.g. give precedence to big commits and order commits along the order of its base revisions, which would decrease the amount of work to be done in rebasing. 


",operations performance scalability throughput,['segment-tar'],OAK,New Feature,Major,2016-03-11 14:45:34,1
12949098,CompactionMap#get not transitive across compaction map generations,"{{CompactionMap#get(RecordId before)}} searches through the compaction maps until it finds one containing {{before}} returning its value. However that one might already have been compacted again an be present as key in a later compaction map generation. 

A correct implementation of {{CompactionMap#get(RecordId before)}} should consider the transitive closure over all maps starting at {{before}}. Note however that in this case we would also need to stop removing keys from the compaction map after cleanup as this would break transitivity again. (See http://svn.apache.org/viewvc?view=revision&revision=1673791)",compaction gc,['segmentmk'],OAK,Bug,Major,2016-03-11 14:21:13,15
12948663,Cached lucene index gets corrupted in case of unclean shutdown and journal rollback in SegmentNodeStore,"Currently Oak Lucene support would copy index files to local file system as part of CopyOnRead feature. In one of the setup it has been observed that index logic was failing with following error

{noformat}
04.02.2016 17:47:52.391 *WARN* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier [/oak:index/lucene] Found local copy for _2ala.cfs in MMapDirectory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 9320 differs from remote 3714150. Content would be read from remote file only
04.02.2016 17:47:52.399 *WARN* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier [/oak:index/lucene] Found local copy for segments_28je in MMapDirectory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 1214 differs from remote 1175. Content would be read from remote file only
04.02.2016 17:47:52.491 *ERROR* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker Failed to open Lucene index at /oak:index/lucene
org.apache.lucene.index.CorruptIndexException: codec header mismatch: actual header=1953790076 vs expected header=1071082519 (resource: SlicedIndexInput(SlicedIndexInput(_2ala.fnm in _2ala.cfs) in _2ala.cfs slice=8810:9320))
	at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:128)
	at org.apache.lucene.codecs.lucene46.Lucene46FieldInfosReader.read(Lucene46FieldInfosReader.java:56)
	at org.apache.lucene.index.SegmentReader.readFieldInfos(SegmentReader.java:215)
{noformat}

Here size of __2ala.cfs_ differed from remote copy and possible other index file may have same size but different content. Comparing the modified time of the files with those in Oak it can be seen that one of file system was older than one in Oak

{noformat}

_2alr.cfs={name=_2alr.cfs, size=1152402, sizeStr=1.2 MB, modified=Thu Feb 04 17:52:31 GMT 2016, osModified=Feb 4 17:52, osSize=1152402, mismatch=false}
_2ala.cfe={name=_2ala.cfe, size=224, sizeStr=224 B, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=224, mismatch=false}
_2ala.si={name=_2ala.si, size=252, sizeStr=252 B, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=252, mismatch=false}
_2ala.cfs={name=_2ala.cfs, size=3714150, sizeStr=3.7 MB, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=9320, mismatch=true}
_14u3_29.del={name=_14u3_29.del, size=1244036, sizeStr=1.2 MB, modified=Thu Feb 04 16:37:35 GMT 2016, osModified=Feb 4 16:37, osSize=1244036, mismatch=false}
_2akw.si={name=_2akw.si, size=252, sizeStr=252 B, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=252, mismatch=false}
_2akw.cfe={name=_2akw.cfe, size=224, sizeStr=224 B, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=224, mismatch=false}
_2akw.cfs={name=_2akw.cfs, size=4952761, sizeStr=5.0 MB, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=4952761, mismatch=false}
{noformat}

And on same setup the system did saw a rollback in segment node store 
{noformat}

-rw-rw-r--. 1 crx crx  25961984 Feb  4 16:47 data01357a.tar
-rw-rw-r--. 1 crx crx  24385536 Feb  4 16:41 data01357a.tar.bak
-rw-rw-r--. 1 crx crx    359936 Feb  4 17:18 data01358a.tar
-rw-rw-r--. 1 crx crx    345088 Feb  4 17:17 data01358a.tar.bak
-rw-rw-r--. 1 crx crx  70582272 Feb  4 18:35 data01359a.tar
-rw-rw-r--. 1 crx crx  66359296 Feb  4 18:33 data01359a.tar.bak
-rw-rw-r--. 1 crx crx    282112 Feb  4 18:46 data01360a.tar
-rw-rw-r--. 1 crx crx    236544 Feb  4 18:45 data01360a.tar.bak
-rw-rw-r--. 1 crx crx    138240 Feb  4 18:56 data01361a.tar
{noformat}

So one possible cause is that 
# At some time earlier to 17:17 lucene index got updated and __2ala.cfs_ got created. 
# Post update the head revision in Segment store was updated but the revision yet to made it to journal log
# Lucene CopyOnRead logic got event for the change and copied the file
# System crashed and hence journal did not got updated
# System restarted and per last entry in journal system suffered with some ""data loss"" and hence index checkpoint also moved back
# As checkpoint got reverted index started at earlier state and hence created a file with same name __2ala.cfs_ 
# CopyOnRead detected file length change and logged a warning routing call to remote
# However other files like _2ala.si, _2ala.cfe which were created in same commit had same size but likely different content which later cause lucene query to start failing

In such a case a restart after cleaning the existing index content would have brought back the system to normal state.

So as a fix we would need to come up with some sanity check at time of system startup",resilience,['lucene'],OAK,Bug,Critical,2016-03-10 05:31:47,19
12948373,Replace the query exclusive lock with a cache tracker,"The {{MongoDocumentStore#query()}} method uses an expensive {{TreeLock#acquireExclusive}} method, introduced in OAK-1897 to avoid caching outdated documents.

It should be possible to avoid acquiring the exclusive lock, by tracking the cache changes that occurs during the Mongo find() operation. When the find() is done, we can update the cache with the received documents if they haven't been invalidated in the meantime.

",performance,"['documentmk', 'mongomk']",OAK,Improvement,Major,2016-03-09 11:06:57,18
12948139,Reclaimed size reported by FileStore.cleanup is off,"The current implementation simply reports the difference between the repository size before cleanup to the size after cleanup. As cleanup runs concurrently to other commits, the size increase contributed by those is not accounted for. In the extreme case where cleanup cannot reclaim anything this can even result in negative values being reported. 

We should either change the wording of the respective log message and speak of before and after sizes or adjust our calculation of reclaimed size (preferred). ",cleanup gc,['segment-tar'],OAK,Bug,Minor,2016-03-08 19:49:35,1
12948135,Implement FileStore.size through FileStore.approximateSize,"{{FileStore.size()}} is prone to lock contention and should not be called too often. As OAK-2879 already introduced an approach for tracking the current size of the file store without having to lock, we might as well promote his to be ""the official"" implementation. 

[~frm] WDYT?",resilience,['segment-tar'],OAK,Technical task,Minor,2016-03-08 19:43:06,1
12948120,Replace journal.log with an in place journal,Instead of writing the current head revision to the {{journal.log}} file we could make it an integral part of the node states: as OAK-3804 demonstrates we already have very good heuristics to reconstruct a lost journal. If we add the right annotations to the root node states this could replace the current approach. The latter is problematic as it relies on the flush thread properly and timely updating {{journal.log}}. See e.g. OAK-3303. ,resilience,['segment-tar'],OAK,New Feature,Minor,2016-03-08 19:14:51,1
12948112,Break cyclic dependency of FileStore and SegmentTracker,"{{SegmentTracker}} and {{FileStore}} are mutually dependent on each other. This is problematic and makes initialising instances of these classes difficult: the {{FileStore}} constructor e.g. passes a not fully initialised instance to the {{SegmentTracker}}, which in turn writes an initial node state to the store. Notably using the not fully initialised {{FileStore}} instance!",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-03-08 19:03:23,12
12947985,Lucene index appear to be corrupted with compaction enabled,"While running on SegmentNodStore and online compaction enabled it can happen that access to Lucene index start failing with SegmentNotFoundException

{noformat}
Caused by: org.apache.jackrabbit.oak.plugins.segment.SegmentNotFoundException: Segment a949519a-8903-44f9-a17e-b6d83fb32186 not found
       at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:870)
       at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:136)
       at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:108)
       at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
       at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.getNewStream(SegmentBlob.java:64)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.loadBlob(OakDirectory.java:259)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.readBytes(OakDirectory.java:307)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.readBytes(OakDirectory.java:404)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.readByte(OakDirectory.java:411)
       at org.apache.lucene.store.DataInput.readVInt(DataInput.java:108)
       at org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2397)
       at org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekCeil(BlockTreeTermsReader.java:1973)
       at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:225)
       at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:78)
       at org.apache.lucene.search.ConstantScoreAutoRewrite.rewrite(ConstantScoreAutoRewrite.java:95)
       at org.apache.lucene.search.MultiTermQuery$ConstantScoreAutoRewrite.rewrite(MultiTermQuery.java:220)
       at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:418)
       at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:636)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:683)
       at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:378)
{noformat}

The above segmentId was mentioned in the compaction log

{noformat}
06.03.2016 02:03:30.706 *INFO* [TarMK flush thread [/app/repository/segmentstore], active since Sun Mar 06 02:03:29 GMT 2016, previous max duration 8218ms] org.apache.jackrabbit.oak.plugins.segment.file.TarReader-GC Cleaned segments from data00233a.tar:
       37ec786e-a9f7-46eb-a3b5-ce5d4777ea01, f36051fe-d8c4-46d1-ac1d-081946389eb6, fae91ff2-8ca6-4ac1-a8d8-d4bd09b7f6a6, 16d87f09-721b-4155-a9c8-b8ecf471bfc3,
       e641f1a3-b323-44e6-aad0-7b894a1efb69, edc9d141-6c05-42c9-a2a2-d7130fd9c826, b602372c-b17a-448a-a8e9-8bdccc64fb82, acc2f032-07ba-46ed-a9c7-d3a05ab53d7a,
       a7323ed2-b2de-4006-ae51-e4f84165a0e4, cb320c70-5ca9-4ed1-a972-e87a6bba9f9b, f45afd7e-5417-42dd-a2f7-4624f74b6c6e, c66f66ef-cdd0-4327-abc6-bf910cb5768d,
       7f925a07-ff56-4613-ac8f-272a0e481926, 4ad044ec-3b2d-4c3e-aeb0-d5f5a04bc23e, 82f1c3aa-2e0c-421c-a033-e4ffcb6002c7, 1387655b-f633-4011-a55c-d9580e40929b,
       c50c94fc-2e8b-4904-a37f-0a33cc001312, 7915e9ce-bb9d-4628-ad6f-e7f2844b2399, e7cd013b-a147-426a-af29-fa025058a08a, f16d43b0-2113-4808-aea6-5910102e5c7d,
...
*31edad2e-e14b-463d-a6af-540bac6009f1*,
...,
*a949519a-8903-44f9-a17e-b6d83fb32186*,
...
{noformat}

*Note that system recovered after a restart so the corruption was transient*
",resilience,['lucene'],OAK,Bug,Blocker,2016-03-08 12:09:13,19
12947896,Include timestamp in journal log entries,"Currently the journal log has entries like below. At times while debugging crash or some issue we need to determine the probable root state at some point in the past. 

{noformat}
3dea11bb-bd43-4319-a37d-59df778a7271:260988 root
a7a509ac-a9d4-4e2c-a0d8-df71ebe123a0:259736 root
1d889da9-b41c-4889-a0cd-a9aa9dcc1737:259992 root
b78e4aa6-ec68-4e70-a364-f04ccbf4c3b3:259964 root
{noformat}

Currently there is no way to determine from above log what is the root state wrt time. So we need to workaround that by reading each root state and look for some path which has some time related property. To simplify such case it would be helpful to also include timestamp while adding a journal entry

{noformat}
1d889da9-b41c-4889-a0cd-a9aa9dcc1737:259992 root 1457408708772
b78e4aa6-ec68-4e70-a364-f04ccbf4c3b3:259964 root 1457408708899
{noformat}

*Key points*
# Timestamp comes at end
# Such a feature can be enabled without affecting backward compatibility - Just that new entries would have timestamp included
# {{JournalReader}} - Just reads the first column so would work as is",candidate_oak_1_4,['segmentmk'],OAK,Improvement,Minor,2016-03-08 03:45:39,19
12946622,Replace Sync of configured AutoMembership by Dynamic Principal Generation,"the {{DefaultSyncConfig}} comes with a configuration option {{PARAM_USER_AUTO_MEMBERSHIP}} indicating the set of groups a given external user must always become member of upon sync into the repository.

this results in groups containing almost all users in the system (at least those synchronized form the external IDP). while this behavior is straight forward (and corresponds to the behavior in the previous crx version), it wouldn't be necessary from a repository point of view as a given {{Subject}} can be populated from different principal sources and dealing with this kind of dynamic-auto-membership was a typical use-case.

what does that mean:
instead of performing the automembership on the user management, the external authentication setup could come with an auto-membership {{PrincipalProvider}} implementation that would expose the desired group membership for all external principals (assuming that they were identified as such).

[~tripod], do you remember if that was ever an option while building the {{oak-auth-external}} module? if not, could that be worth a second thought also in the light of OAK-3933?
",performance,['auth-external'],OAK,Improvement,Major,2016-03-03 17:37:00,0
12944830,Use read concern majority when connected to a replica set,"Mongo 3.2 introduces new query option: {{readConcern}}. It allows to read only these changes that have been already committed to the majority of secondary instances.

It prevents stale reads - a situation in which a change has been committed on the primary (and read from it), but due to the network partition a new primary is elected and the change is rolled back.

We should use this new option (together with {{w:majority}} implemented in OAK-3554) when running Oak on MongoDB replica set.

References:
* [Jepsen: MongoDB stale reads|https://aphyr.com/posts/322-jepsen-mongodb-stale-reads]
* [MongoDB documentation: Read Concern in|https://docs.mongodb.org/manual/reference/read-concern/]",resilience,['mongomk'],OAK,Improvement,Major,2016-02-26 09:17:05,18
12944433,Allow use of pre extrcated text cache for incremental indexing,"Pre Extraction support was implemented with an assumption that such big indexing would happen as part of reindex so it was used in reindex phase only. Reason to avoid using it in incremental indexing (non reindex case) were
# Incremental index would does not have text for newly added files. So checking with pre extracted cache would not be useful
# PreExtraction logic keeps in memory state (blobs_empty.txt,blobs_error.txt) which would then unnecessary hog memory.

However in some cases people make use of new incremental migration feature in upgrade. Which would lead to one big incremental indexing step once next migration is done and that would then not able to make use of pre extraction support.

So as a fix we should provide a policy option to ignore the reindex clause per admin setting",docs-impacting,['lucene'],OAK,Improvement,Minor,2016-02-25 09:26:21,19
12943081,FileStore.containsSegment returns alway true (almost),"{{FileStore.containsSegment()}} looks [funky|https://github.com/mduerig/jackrabbit-oak/blob/36cb3bf6e5078e3afa75581fb789eeca7b5df2e2/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/FileStore.java#L1197-L1197]. This ""optimisation"" causes it to always return {{true}}. 

{{containsSegment}} is used for deduplication and revision gc. The current implementation causes {{SNFE}} exceptions once gc is effective (as I experienced while working on OAK-3348). ",compaction gc stability,['segmentmk'],OAK,Bug,Critical,2016-02-24 20:52:14,15
12941554,NPE in oak-run graph when repository contains bulk segments,"{code}
Exception in thread ""main"" java.lang.NumberFormatException: null
	at java.lang.Long.parseLong(Long.java:404)
	at java.lang.Long.valueOf(Long.java:540)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentGraph.asLong(SegmentGraph.java:494)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentGraph.writeNode(SegmentGraph.java:464)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentGraph.writeSegmentGraph(SegmentGraph.java:173)
	at org.apache.jackrabbit.oak.run.GraphCommand.execute(GraphCommand.java:92)
	at org.apache.jackrabbit.oak.run.Mode.execute(Mode.java:63)
{code}

The cause for this is not properly checking the info map before deciding whether to print a bulk or a data node. ",tooling,['run'],OAK,Bug,Major,2016-02-23 21:53:59,15
12941553,NPE when running oak-run from within the IDE,"Running oak-run from within the IDE causes a {{NPE}}:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.jackrabbit.oak.run.Main.getProductVersion(Main.java:76)
	at org.apache.jackrabbit.oak.run.Main.getProductVersion(Main.java:66)
	at org.apache.jackrabbit.oak.run.Main.getProductInfo(Main.java:53)
	at org.apache.jackrabbit.oak.run.Main.printProductInfo(Main.java:86)
{code}

This is caused by not checking the return value of {{getResourceAsStream}} for {{null}} when trying to load {{/META-INF/maven/org.apache.jackrabbit/oak-run/pom.properties}}. That file is not on the class path when running from within the IDE. ",tooling,['run'],OAK,Bug,Major,2016-02-23 21:48:57,15
12941339,Some classes from o.a.j.o.plugins.segment.compaction should be exported,"Classes {{org.apache.jackrabbit.oak.plugins.segment.compaction.CompactionStrategy}} and {{org.apache.jackrabbit.oak.plugins.segment.compaction.CompactionStrategyMBean}} should be exported. The former is used in the public API of multiple classes from {{org.apache.jackrabbit.oak.plugins.segment.file}} and {{org.apache.jackrabbit.oak.plugins.segment}}, while the latter is used as interface type for a service registered in the whiteboard.",technical_debt,['segment-tar'],OAK,Bug,Major,2016-02-23 09:31:16,12
12940564,DocumentNodeStore: required server time accuracy,"The DocumentNodeStore currently requires that the local time and the persistence time differ at most 2 seconds.

I recently tried to run a cluster with two Windows machines, and despite them being configured to use the same NTP service, they were still 4..5 s off.

https://blogs.technet.microsoft.com/askds/2007/10/23/high-accuracy-w32time-requirements/ seems to confirm that by default, Windows can't provide the required accuracy.

One workaround seems to be to install custom ntp clients; but do we really want to require this?",documentation,"['doc', 'documentmk']",OAK,Documentation,Minor,2016-02-19 17:13:43,2
12938830,Expedite commits from the compactor,"Concurrent commits during compaction cause those to be re-compacted. Currently it seems that the compaction thread can end up waiting for some time to acquire the commit lock [1], which in turn causes more commits to pile up to be re-compacted. I think this could be improved by tweaking the lock such that the compactor could jump ahead of the queue. I.e. use a lock which can be acquired in expedited mode. 

[1] SegmentNodeStore#commitSemaphore",compaction gc perfomance,['segment-tar'],OAK,Improvement,Major,2016-02-12 15:42:17,15
12938753,Set online compaction default to paused,As online compaction is still not at the point where we would like it to have we will need to disable it by default for the upcoming major release. ,compaction gc,['segmentmk'],OAK,Bug,Blocker,2016-02-12 09:28:21,15
12936960,Add S3 datastore support for Text Pre Extraction,Text pre extraction feature introduced in OAK-2892 only supports FileDataStore. For files present in S3 we should add support for S3DataStore,docs-impacting,['run'],OAK,Improvement,Minor,2016-02-05 06:36:28,19
12936605,SharedDataStore - Set the unique repository ID using sling if configured,"As per discussion on the issue OAK-3935 [1] & [2] we could use sling to get a repository ID injected obviating the need for a Osgi config param override in case of a cloned instance.

[1] - https://issues.apache.org/jira/browse/OAK-3935?focusedCommentId=15122957&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15122957
[2] - https://issues.apache.org/jira/browse/OAK-3935?focusedCommentId=15126096&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15126096",resilience,['blob'],OAK,Improvement,Major,2016-02-04 04:36:14,16
12936387,Add segment size to segment graph,The segment graph produced by {{oak-run graph}} should also contain the sizes of the segments. ,gc tooling,['run'],OAK,Improvement,Major,2016-02-03 16:45:09,15
12936278,Forward edges missing in SegmentGraph ,"The graph produced by {{oak-run graph}} does not include forward edges (i.e. references from older segments to newer segments). Such references where introduced with  OAK-1828. See also OAK-3864, where this has been fixed for the file store cleanup.

",cleanup gc technical_debt tooling,['run'],OAK,Bug,Major,2016-02-03 08:28:56,15
12935394,Boosted field don't work if parent nodes are covered in aggregate definition,"With index def of the form:
{noformat}
+/indexName/indexRules/nodeType1/properties/prop0
     -name=subChild/indexedProp
     -nodeScopeIndex=true
     -boost=2.0
+indexName/aggregates/nodeType1/include0
     -path=subChild
     -relativeNode=true
{noformat}

A query like {{//element(*, nodeType1)\[jcr:contains(subChild, 'bar')]}} should rank nodes with {{subChild/\@indexedProp=bar}} above other nodes with any prop {{=bar}} under subChild.",lucene,['lucene'],OAK,Bug,Major,2016-01-30 23:24:40,11
12935039,[oak-blob-cloud] Update oak-blob-cloud with jackrabbit-aws-ext updates,"The code in oak-blob-cloud should sync with the following minor updates to jackrabbit-aws-ext.
* JCR-3864
* JCR-3867
* JCR-3886
* JCR-3889
* JCR-3914",technical_debt,['blob'],OAK,Technical task,Major,2016-01-29 07:47:22,16
12934692,[oak-run] Option to dump blob references ,Add an option in oak-run to dump blob references currently used. This should help in analyzing GC issues.,resilience tooling,"['blob', 'run']",OAK,Improvement,Major,2016-01-28 05:39:38,16
12934691,SharedDataStore - Allow unique repository ID to be specified by config ,"For GC in a shared DataStore, a unique repository Id is currently saved in a hidden path in the node store on startup and registered in the DataStore.
This will cause problems where the publish environments are cloned and share a datastore.

There should be an option to specify a unique id in the config when setting up the node store.",resilience,"['blob', 'segmentmk']",OAK,Improvement,Major,2016-01-28 05:33:15,16
12934509,Log ids of segments being released for gc because of their age. ,When {{CompactionStrategy.CleanupType#CLEAN_OLD}} releases a segment for gc because of its age it should log a message. This helps to determine the root cause of a {{SNFE}}. ,cleanup gc,['segmentmk'],OAK,Technical task,Major,2016-01-27 16:28:26,15
12934109,oak-run primary/standby should check segment version,The primary and standby run modes should exit with an error if run on a store with non matching segment version.,resilience tooling,['run'],OAK,Technical task,Critical,2016-01-26 13:36:03,12
12934107,oak-run checkpoint should check segment version,The checkpoint runmode should exit with an error if run on a store with non matching segment version.,resilience tooling,['run'],OAK,Technical task,Major,2016-01-26 13:33:00,15
12934106,oak-run backup/recover should check segment version,Backup/restore should exit with an error if run on a store with non matching segment version. ,resilience tooling,['run'],OAK,Technical task,Major,2016-01-26 13:30:58,15
12933790,DataStoreBlobStore - Limit resolveChunks only to non inlined blobs,"Currently, DataStoreBlobStore#resolveChunks resolves all blob ids including in-lined blobs. This is different from the AbstractBlobStore#resolveChunks which removes in-lined blobs.
Due to this the InMemoryDataRecord had to be made public for OAK-3184 which it should not have. A proper solution would be to have the resolveChunks only return blobs stored in blob/data store. ",technical_debt,[],OAK,Improvement,Minor,2016-01-25 12:10:33,16
12933027,Compaction Map predicate should use cached state for evaluation,"In the case of offline compaction, the Compactor predicate would try to evaluate if a specific node is candidate for the map of not based on a set of conditions.
To evaluate said conditions, the predicate currently uses the compacted state, the one that was just written by the SegmentWriter [0], but this offers very poor performance as this NodeState will be accessed from the TarWriter directly, a very IO intensive call (no memory mapping, no caching of the segment) [1].
A much better thing is to use the cached nodestate, in my local test (on a SSD) this accounts for 10% of perf loss, I would imagine the gains are more significant on a non-SSD disk.




[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/Compactor.java#L252
[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/TarWriter.java#L190",compaction gc,['segmentmk'],OAK,Improvement,Major,2016-01-21 09:25:47,14
12933019,Commit fails even though change made it to the DocumentStore,"In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore. The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails. On the Oak side the call will be considered as failed, but the change was actually applied.

The issue can be reproduced with the test attached to OAK-1641 and a replica-set with 3 nodes. Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself.",resilience,"['core', 'documentmk']",OAK,Bug,Major,2016-01-21 08:54:42,2
12932451,Add filter capabilities to the segment graph run mode,I like to add a filter capability to {{oak-run graph}} to specify the inclusion criteria of segments via a regular expression.,tooling,['run'],OAK,Improvement,Major,2016-01-19 15:35:37,15
12932156,SegmentWriter records cache could use thinner keys,"The SegmentWriter keeps a records deduplication cache ('records' map) that maintains 2 types of mappings:
* template -> recordid
* strings -> recordid

For the first one (template-> recordid) we can come up with a thinner representation of a template (a hash function that is fast and not very collision prone) so we don't have to keep a reference to each template object.

Same applies for second one, similar to what is happening in the StringsCache now, we could keep the string value up to a certain size and beyond that, hash it and use that for the deduplication map.




",performance,['segment-tar'],OAK,Improvement,Minor,2016-01-18 13:41:32,14
12932118,Robuster test expectations for FileStoreIT,{{FileStoreIT.testRecovery}} currently hard codes expected segment offsets. I would like to refactor this to make it more robust against changes in how exactly records are stored / de-duplicated. ,technical_debt,['segmentmk'],OAK,Improvement,Major,2016-01-18 10:45:13,15
12932101,Test failure: BroadcastTest,"The test failed rather frequently on Apache Jenkins as well as travis-ci.

The failure is:

{noformat}
broadcastTCP(org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest): min: 90 got: 80
{noformat}",CI Jenkins,"['core', 'documentmk']",OAK,Bug,Minor,2016-01-18 09:09:11,4
12930181,Avoid commit from too far in the future (due to clock skews) to go through,"Following up [discussion|http://markmail.org/message/m5jk5nbby77nlqs5] \[0] to avoid bad commits due to misbehaving clocks. Points from the discussion:
* We can start self-destruct mode while updating lease
* Revision creation should check that newly created revision isn't beyond leaseEnd time
* Implementation done for OAK-2682 might be useful

[0]: http://markmail.org/message/m5jk5nbby77nlqs5",resilience,"['core', 'documentmk']",OAK,Improvement,Major,2016-01-14 16:16:46,2
12930165,Collision may mark the wrong commit,"In some rare cases it may happen that a collision marks the wrong commit. OAK-3344 introduced a conditional update of the commit root with a collision marker. However, this may fail when the commit revision of the condition is moved to a split document at the same time.",resilience,"['core', 'documentmk']",OAK,Bug,Minor,2016-01-14 15:14:50,2
12930078,Lucene index / compatVersion 2: search for 'abc!' does not work,"When using a Lucene fulltext index with compatVersion 2, then the following query does not return any results. When using compatVersion 1, the correct result is returned.

{noformat}
SELECT * FROM [nt:unstructured] AS c 
WHERE CONTAINS(c.[jcr:description], 'abc!') 
AND ISDESCENDANTNODE(c, '/content')
{noformat}

With compatVersion 1 and 2, searching for just 'abc' works. Also, searching with '=' instead of 'contains' works.",docs-impacting,['lucene'],OAK,Bug,Major,2016-01-14 08:43:32,19
12929747,Don't pass the compaction map to FileStore.cleanup,That argument is unused and I'll remove it thus. ,technical_debt,['segmentmk'],OAK,Technical task,Minor,2016-01-13 09:07:53,15
12929505,Refactor RecordWriter.write to always return a RecordId,"I think it would be cleaner if {{RecordId.write}} would always return a {{RecordId}} instead of depending on its type parametrisation and would like to refactor it to that respect.. 

This is also a pre-requisite for my work on OAK-3348 and might also be for OAK-3864. ",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-01-12 14:54:19,15
12929502,Move createSegmentWriter() from FileStore to SegmentTracker,"I think it makes sense to move said method. This simplifies the code in various places as it somewhat decouples the concern ""writing segments"" from an implementation ({{FileStore}}). 

Also this is somewhat a prerequisite for my current work on OAK-3348.",technical_debt,['segmentmk'],OAK,Technical task,Major,2016-01-12 14:41:06,15
12929498,New strategy to optimize secondary reads,"*Introduction*

In the current trunk we'll only read document _D_ from the secondary instance if:
(1) we have the parent _P_ of document _D_ cached and
(2) the parent hasn't been modified in 6 hours.

The OAK-2106 tried to optimise (2) by estimating lag using MongoDB replica stats. It was unreliable, so the second approach was to read the last revisions directly from each Mongo instance. If the modification date of _P_ is before last revisions on all secondary Mongos, then secondary can be used.

The main problem with this approach is that we still need to have the _P_ to be in cache. I think we need another way to optimise the secondary reading, as right now only about 3% of requests connects to the secondary, which is bad especially for the global-clustering case (Mongo and Oak instances across the globe). The optimisation provided in OAK-2106 doesn't make the things much better and may introduce some consistency issues.

*Proposal - tldr version*

Oak will remember the last revision it has ever seen. In the same time, it'll query each secondary Mongo instance, asking what's the available stored root revision. If all secondary instances have a root revision >= last revision seen by a given Oak instance, it's safe to use the secondary read preference.

*Proposal*

I had following constraints in mind preparing this:
1. Let's assume we have a sequence of commits with revisions _R1_, _R2_ and _R3_ modifying nodes _N1_, _N2_ and _N3_. If we already read the _N1_ from revision _R2_ then reading from a secondary shouldn't result in getting older revision (eg. _R1_).
2. If an Oak instance modifies a document, then reading from a secondary shouldn't result in getting the old version (before modification).

So, let's have two maps:
* _M1_ the most recent document revision read from the Mongo for each cluster id,
* _M2_ the oldest last rev value for root document for each cluster id read from all the secondary instances.

Maintaining _M1_:
For every read from the Mongo we'll check if the lastRev for some cluster id is newer than _M1_ entry. If so, we'll update _M1_. For all writes we'll add the saved revision id with the current cluster id in _M1_.

Maintaining _M2_:
It should be periodically updated. Such mechanism is already prepared in the OAK-2106 patch.

The method deciding whether we can read from the secondary instance should compare two maps. If all entries in _M2_ are newer than _M1_ it means that the secondary instances contains at least as new repository state as we already accessed and therefore it's safe to read from secondary.

Regarding the documents modified by the local Oak instance, we should remember all the locally-modified paths and their revisions and use primary Mongo to access them as long as the changes are not replicated to all the secondaries. When the secondaries are up to date with the modification, we can remove it from the local-changes collections.

Attached image diagram.png presents the idea.",performance,['mongomk'],OAK,Improvement,Major,2016-01-12 14:07:24,18
12929459,Filestore cleanup removes referenced segments,"In some situations {{FileStore.cleanup()}} may remove segments that are still referenced, subsequently causing a {{SNFE}}. 

This is a regression introduced with OAK-1828. 

{{FileStore.cleanup()}} relies on the ordering of the segments in the tar files: later segments only reference earlier segments. As we have seen in other places this assumption does not hold any more (e.g. OAK-3794, OAK-3793) since OAK-1828.
 {{cleanup}} traverses the segments backwards maintaining a list of referenced ids. When a segment is not in that list, it is removed. However, this approach does not work with forward references as those are only seen later when the segment has been removed already. 

cc [~alex.parvulescu], [~frm]",regression,['segmentmk'],OAK,Bug,Blocker,2016-01-12 11:05:42,15
12929232,Move integration tests in a different Maven module,"While moving the Segment Store and related packages into its own bundle, I figured out that integration tests contained in {{oak-core}} contribute to a cyclic dependency between the (new) {{oak-segment}} bundle and {{oak-core}}.

The dependency is due to the usage of {{NodeStoreFixture}} to instantiate different implementations of {{NodeStore}} in a semi-transparent way.

Tests depending on {{NodeStoreFixture}} are most likely integration tests. A clean solution to this problem would be to move those integration tests into a new Maven module, referencing the API and implementation modules as needed.",modularization technical_debt,[],OAK,Improvement,Major,2016-01-11 17:08:56,12
12929149,Review slow running tests,"Some of the tests executed during a normal {{mvn clean test}} execution seem to be very slow if compared with the rest of the suite. On my machine, some problematic tests are:

{noformat}
Running org.apache.jackrabbit.oak.spi.blob.FileBlobStoreTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 10.982 sec
Running org.apache.jackrabbit.oak.plugins.document.BasicDocumentStoreTest
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.961 sec
Running org.apache.jackrabbit.oak.plugins.document.BulkCreateOrUpdateTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.076 sec
Running org.apache.jackrabbit.oak.plugins.document.ConcurrentDocumentStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.054 sec
Running org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteServiceTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 982.526 sec
Running org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreTest
Tests run: 53, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.132 sec
Running org.apache.jackrabbit.oak.plugins.document.LastRevRecoveryAgentTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.068 sec
Running org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStorePerformanceTest
Tests run: 10, Failures: 0, Errors: 0, Skipped: 10, Time elapsed: 10.006 sec
Running org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.017 sec
Running org.apache.jackrabbit.oak.plugins.document.VersionGCWithSplitTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.128 sec
Running org.apache.jackrabbit.oak.security.authentication.ldap.LdapLoginStandaloneTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.96 sec
{noformat}

These tests should be analyzed for potential errors or moved to the integration test phase.",test,[],OAK,Improvement,Major,2016-01-11 10:50:47,2
12928734,Simplify SegmentGraphTest,{{SegmentGraphTest}} has a somewhat complicated setup phase to build a segment store of a certain structure. This is will probably prove unreliable when underlying implementation details of how segments are written change (e.g. with OAK-3348). I would like to refactor the test such that it becomes independent of such implementation details. ,gc,['segmentmk'],OAK,Improvement,Minor,2016-01-08 21:02:51,15
12928689,oak-run compact should check segment version,Off line compaction should exit with a warning if run on a store with non matching segment version. It should provide a {{--force}} option to override this behaviour such that it can still be used for explicit upgrading. ,gc resilience tooling,['run'],OAK,Technical task,Critical,2016-01-08 17:06:48,15
12928688,TarMK tools should check whether they run against a matching version of the repository,"Running tools that write into a segment store might result in unwanted upgrading if the version of the tool uses a more recent segment version than the store. E.g. off line compaction currently upgrades segment format 10 to 11. 

To protected against inadvertent upgrades, a tool should check whether the segment version of the store matches its expectation (currently 11). If not, the tool should exit with a respective warning / error. For some tools it can make sense to provide a flag (e.g. {{--force}}) to override this. With this e.g. offline compaction can still be used for upgrading a segment store if explicitly told to do so. ",resilience tooling,['run'],OAK,Improvement,Critical,2016-01-08 17:02:40,15
12928653,Improve SegmentGraph resilience ,Currently {{SegmentGraph}} just bails out upon hitting a {{SNFE}}. I would like to improve this and include the error in the generated graph. ,gc resilience tooling,['run'],OAK,Improvement,Major,2016-01-08 14:38:28,15
12928039,Adjust package export declarations ,"We need to adjust the package export declarations such that they become manageable with our branch / release model. 

See http://markmail.org/thread/5g3viq5pwtdryapr for discussion.

I propose to remove package export declarations from all packages that we don't consider public API / SPI beyond Oak itself. This would allow us to evolve Oak internal stuff (e.g. things used across Oak modules) freely without having to worry about merges to branches messing up semantic versioning. OTOH it would force us to keep externally facing public API / SPI reasonably stable also across the branches. Furthermore such an approach would send the right signal to Oak API / SPI consumers regarding the stability assumptions they can make. 

An external API / SPI having a (transitive) dependency on internals might be troublesome. In doubt I would remove the export version here until we can make reasonable guarantees (either through decoupling the code or stabilising the dependencies). 

I would start digging through the export version and prepare an initial proposal for further discussion. 

/cc [~frm], [~chetanm], [~mmarth]",api modularization technical_debt,[],OAK,Task,Blocker,2016-01-06 15:31:32,15
12927995,Clean up the FileStore constructor,"The {{FileStore}} constructor consists of more than 150 LoC and is a mess as it depends on the order of initialisation, calls overrideable methods handles different concerns (read only vs. read / write) etc. 

We should up with a cleaner way of instantiating a file store.",technical_debt,['segmentmk'],OAK,Technical task,Major,2016-01-06 11:08:47,12
12924230,TarMK JCR data -> Solr == Exception,"One node was not in Solr after searching for it...

After looking into logs have found following:

ERROR - 2015-12-21 17:05:29.598; org.apache.solr.common.SolrException; null:org.apache.solr.common.SolrException: Exception writing document id /content/dam/my/example.pdf/jcr:content/metadata/wn_previews:previews/wn_previews:spreads/1 to the index; possible analysis error.
    at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:168)
    at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:69)
    at org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:51)
    at org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProcessor.java:870)
    at org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1024)
    at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:693)
    at org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:100)
    at org.apache.solr.handler.loader.JavabinLoader$1.update(JavabinLoader.java:96)
    at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:166)
    at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:136)
    at org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:225)
    at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:121)
    at org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:190)
    at org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:116)
    at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:173)
    at org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs(JavabinLoader.java:106)
    at org.apache.solr.handler.loader.JavabinLoader.load(JavabinLoader.java:58)
    at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)
    at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)
    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)
    at org.apache.solr.core.SolrCore.execute(SolrCore.java:1962)
    at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:777)
    at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:418)
    at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:207)
    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)
    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:455)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
    at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1075)
    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:384)
    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1009)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
    at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
    at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
    at org.eclipse.jetty.server.Server.handle(Server.java:368)
    at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:489)
    at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)
    at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:953)
    at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1014)
    at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:953)
    at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
    at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)
    at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)
    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Document contains at least one immense term in field=""wn_previews:base64_data"" (whose UTF8 encoding is longer than the max length 32766), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '[121, 113, 55, 102, 120, 121, 113, 55, 102, 120, 121, 113, 55, 102, 120, 121, 113, 55, 102, 120, 121, 113, 55, 102, 120, 121, 113, 55, 102, 120]...', original message: bytes can be at most 32766 in length; got 64627
    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:671)
    at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:342)
    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:301)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:241)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:451)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1539)
    at org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:240)
    at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:164)
    ... 48 more
Caused by: org.apache.lucene.util.BytesRefHash$MaxBytesLengthExceededException: bytes can be at most 32766 in length; got 64627
    at org.apache.lucene.util.BytesRefHash.add(BytesRefHash.java:284)
    at org.apache.lucene.index.TermsHashPerField.add(TermsHashPerField.java:151)
    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:645)
    ... 55 more

This can be seen in the default solr.log file

As a result of this exception data transfer to solr is blocked and I have to manually fix the affected data by removing that node",resilience,['solr'],OAK,Bug,Major,2015-12-24 22:25:16,10
12923060,Disable compaction gain estimation if compaction is paused,"I think we should disable compaction estimation when compaction is paused. Estimation interferes with the caches, wastes CPU and IO cycles and is not essential for Oak's operation when compaction is disabled. The only reason it was unconditionally enabled initially is to gather the respective information in production. I think this has turned out to be not too useful so it is safe to disable. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-12-18 16:28:37,15
12923058,Provide option to pass external data store to oak-run check,The {{check}} run mode currently has no option to be run with an external data store. We should probably add such an option. Or/and ensure the check works probably for a segment store with external binaries even if no data store is present.,tooling,"['run', 'segmentmk']",OAK,Improvement,Major,2015-12-18 16:23:57,15
12922985,Test failure: FacetTest,"{{org.apache.jackrabbit.oak.jcr.query.FacetTest}} keeps failing on Jenkins:

{noformat}
testFacetRetrievalMV(org.apache.jackrabbit.oak.jcr.query.FacetTest)  Time elapsed: 5.927 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<tags:[[repository (2), software (2), aem (1), apache (1), cosmetics (1), furniture (1)], tags:[repository (2), software (2), aem (1), apache (1), cosmetics (1), furniture (1)], tags:[repository (2), software (2), aem (1), apache (1), cosmetics (1), furniture (1)], tags:[repository (2), software (2), aem (1), apache (1), cosmetics (1), furniture (1)]]> but was:<tags:[[], tags:[], tags:[], tags:[]]>
	at junit.framework.Assert.assertEquals(Assert.java:100)
	at junit.framework.Assert.assertEquals(Assert.java:107)
	at junit.framework.TestCase.assertEquals(TestCase.java:269)
	at org.apache.jackrabbit.oak.jcr.query.FacetTest.testFacetRetrievalMV(FacetTest.java:80)
{noformat}

Failure seen at builds: 628, 629, 630, 633, 634, 636, 642, 643, 644, 645, 648, 651, 656, 659, 660, 663, 666

See e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/634/#showFailuresLink",ci jenkins test test-failure,['solr'],OAK,Bug,Major,2015-12-18 09:44:55,10
12922638,Clean up the fixtures code in core and jcr modules,"oak-core and oak-jcr modules uses the fixture mechanism to provide NodeStore implementations to the unit/integration tests. There is a few problems with the fixture implementation:

* the {{NodeStoreFixture}} class is duplicated between two modules and supports different set of options (eg. the oak-core version doesn't support the RDB node store at all, while the oak-jcr doesn't support MemoryNodeStore)
* it isn't possible to set the MongoDB URL manually from the Maven command line (it can be done for the RDB, though), which makes running the tests on a Mongo replica hard,
* the Mongo fixture doesn't remove the test database after the test is done.

There should be just one NodeStoreFixture implementation (the oak-jcr can reuse the oak-core version), supporting all values of the {{Fixture}} enum. The Mongo fixture should be more customisable and also should clean-up the database.",tech-debt,"['core', 'jcr']",OAK,Task,Major,2015-12-17 09:12:51,15
12922590,SessionMBean not getting registered due to MalformedObjectNameException,"Due to changes done in OAK-3477 SessionMBean is not getting registered as it contains ',' in the ObjectName. Unfortunately the exception thrown gets lost and this did not got detected so far

{noformat}
javax.management.MalformedObjectNameException: Invalid character in value: `,'
	at javax.management.ObjectName.checkValue(ObjectName.java:1009)
	at javax.management.ObjectName.construct(ObjectName.java:725)
	at javax.management.ObjectName.<init>(ObjectName.java:1425)
	at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardUtils.registerMBean(WhiteboardUtils.java:79)
	at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardUtils.registerMBean(WhiteboardUtils.java:68)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$RegistrationTask.run(RepositoryImpl.java:523)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
{noformat}

The name passed for ObjectName is 
{code}
{name=admin@session-11@Dec 17, 2015 9:57:11 AM, type=SessionStatistics}
{code}

",regresion,['core'],OAK,Bug,Minor,2015-12-17 04:29:15,19
12922345,Drop module oak-js,"The Oak checkout contains a module {{oak-js}}, which is mostly empty apart from a TODO statement. As we didn't work on this and AFAIK do not intend to work on this in the near future, I propose to drop the module for now. ",technical_debt,[],OAK,Task,Minor,2015-12-16 11:19:24,15
12922309,SegmentTracker#collectBlobReferences should retain fewer SegmentId instances,"{{SegmentTracker#collectBlobReferences}} currently keeps a queue of yet unprocessed {{SegmentId}} instances internally. This potentially impacts the system as those instances are also tracked in the segment tracker's segment id tables. I think we should improve the implementation to not retain so many {{SegmentId}} instances and rely on arrays of {{msb}}, {{lsb}} instead. ",datastore gc,['segmentmk'],OAK,Improvement,Major,2015-12-16 09:32:04,14
12921095,Tool for detecting references to pre compacted segments,"While OAK-3560 allows us to detect reference to pre compacted segments through manual inspection, we also need tooling to help detect such cases on site, during longevity tests and for UT/IT.  ",compaction gc tooling,['segmentmk'],OAK,Technical task,Major,2015-12-11 16:29:57,15
12920482,Compaction progress logger: reported number of nodes and binaries is too high,"Once a compaction cycle is through the compaction progress logger prints a message like:

{noforma}
Finished compaction: 26 nodes, 7 properties, 0 binaries.
{noformat}

However the number for nodes and properties includes those items deduplicated through the compaction map, effectively counting some items multiple times even though those where compacted only once and reused later. ",compaction gc,['segmentmk'],OAK,Bug,Major,2015-12-09 16:42:02,15
12920359,Test failure: HeavyWriteIT,"{{org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT}} failed on Jenkins:

{noformat}
heavyWrite[usePersistedMap: false](org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT)  Time elapsed: 106.519 sec  <<< ERROR!
java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.<init>(Segment.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.<init>(Segment.java:198)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:1177)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:224)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	at org.apache.jackrabbit.oak.plugins.segment.RecordId.getSegment(RecordId.java:88)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:506)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:79)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:381)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$UnconnectedHead.update(MemoryNodeBuilder.java:651)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$ConnectedHead.update(MemoryNodeBuilder.java:729)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.head(MemoryNodeBuilder.java:171)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.access$300(MemoryNodeBuilder.java:88)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$UnconnectedHead.update(MemoryNodeBuilder.java:650)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$ConnectedHead.update(MemoryNodeBuilder.java:729)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.head(MemoryNodeBuilder.java:171)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.exists(MemoryNodeBuilder.java:273)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:506)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:515)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createProperties(HeavyWriteIT.java:156)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:148)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:149)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:129)
{noformat}

Seen at build 597",ci jenkins,['segmentmk'],OAK,Bug,Critical,2015-12-09 08:34:48,15
12920357,Test failure: ExternalSharedStoreIT,"{{org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT}} fails on Jenkins: 

{noformat}
testSync(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT)  Time elapsed: 54.498 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.testSync(DataStoreTestBase.java:104)
{noformat}

Seen at builds 163, 164, 598, 601
",ci jenkins,['tarmk-standby'],OAK,Bug,Major,2015-12-09 08:29:48,12
12920094,Implement tooling for tracing a node through the revision history,To diagnose certain issues with gc / checkpoints / indexing we need a tool to trace the evolution of a given node through the revision history. ,tooling,"['run', 'segmentmk']",OAK,Task,Major,2015-12-08 14:33:39,15
12919998,Move the Segment Store into its own bundle,"This epic tracks the work done to move the Segment Store into an independent bundle, detached from oak-core.",modularization technical_debt,['segmentmk'],OAK,Epic,Major,2015-12-08 08:23:33,12
12919995,Build time out after 90 mins on Jenkins,"Every 2nd build or so is aborted after a time out (90 mins):

{noformat}
Build timed out (after 90 minutes). Marking the build as aborted.
Build was aborted
[FINDBUGS] Skipping publisher since build result is ABORTED
Recording test results
Finished: ABORTED
{noformat}

See  e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/585/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/console

Interestingly when successful, the build takes about 30mins, so way below the 90 min. timeout. 

Also seen at builds 587, 590, 591, 593, 595, 597, 598, 604, 608, 609, 610 ",CI Jenkins,['rdbmk'],OAK,Bug,Major,2015-12-08 08:13:09,4
12919659,Compactor should log revisions acting upon,"For post mortem analysis it would be helpful to have the revisions that where involved in a compaction run. I.e. the revision that was compacted, the revisions of the cycles (if any) and the revision that is ultimately applied?",compaction gc,['segmentmk'],OAK,Technical task,Blocker,2015-12-07 11:30:25,15
12919023,Offline compaction doesn't clean up unreferenced tar files,This is a regression introduced with OAK-3329 where cleaning up unreferenced tar files was taken out of {{FileStore#cleanup}}. ,cleanup gc,['segmentmk'],OAK,Bug,Major,2015-12-04 17:00:15,15
12917203,Change default of compaction.forceAfterFail to false,"Having {{compaction.forceAfterFail}} set to {{true}} will block repository writes for an extended period of time (minutes, probably hours) if all previous compaction cycles couldn't catch up with the latest changes. I think this is not acceptable and we should change the default to {{false}}: if compaction is not able to catch up the recommendation should be to move it to a quieter time. ",compaction gc,['segmentmk'],OAK,Task,Major,2015-12-01 09:58:17,15
12917183,Improve handling of IOException,"{{SegmentStore.writeSegment}} doesn't specify its behaviour in the face of IO errors. Moreover {{FileStore.writeSegment}} just catches any {{IOException}} and throws a {{RuntimeException}} with the former as its cause. 

I think we need to clarify this as an immediate cause of the current state is that some of the {{SegmentWriter}} write methods *do* throw an {{IOException}} and some *don't*. ",resilience technical_debt,['segmentmk'],OAK,Improvement,Major,2015-12-01 08:41:23,15
12917177,More resilient BackgroundThread implementation,"Currently {{BackgroundThread}} dies silently when hit by an uncaught exception. We should log a warning. 

Also calling {{Thread#start}} from within the constructor is an anti-pattern as it exposes {{this}} before fully initialised. This is potentially causing OAK-3303. ",resilience,['segmentmk'],OAK,Improvement,Major,2015-12-01 08:28:50,15
12916997,RDBDocumentStore shutdown: improve logging,"We current INFO-level-log the startup, but not the shutdown (dispose()). This makes it harder to see in system logs whether the DocumentStore has been shutdown properly.
",resilience,['rdbmk'],OAK,Technical task,Major,2015-11-30 16:12:09,3
12916934,Improve SegmentMK resilience,Epic for collection SegmentMK resilience improvements,resilience,['segment-tar'],OAK,Epic,Major,2015-11-30 10:59:05,15
12916638,Decouple SegmentBufferWriter from SegmentStore,"Currently {{SegmentBufferWriter.flush()}} directly calls {{SegmentStore.writeSegment()}} once the current segment does not have enough space for the next record. We should try to cut this dependency as {{SegmentBufferWriter}} should only be concerned with providing buffers for segments. Actually writing these to the store should be handled by a higher level component. 

A number of deadlock (e.g. (OAK-2560, OAK-3179, OAK-3264) we have seen is one manifestation of this troublesome dependency. ",technical_debt,['segment-tar'],OAK,Technical task,Major,2015-11-27 13:34:17,15
12916181,Partial re-index from last known good state,"ATM indexes break (by whatever circumstances) users need to perform a full re-index. Depending on the size off the repository this can take a long time.
If the user knows that the indexes were in a good state at a certain revision in the past then it would be very useful, if the user could trigger a ""partial"" re-index where only the content added after a certain revision was updated in the index.",resilience,"['indexing', 'lucene']",OAK,New Feature,Major,2015-11-25 15:32:13,19
12915355,Potential test failure: CompactionAndCleanupIT#testMixedSegments,{{CompactionAndCleanupIT#testMixedSegments}} might fail under some circumstances. It can be certainly be made to fail by increasing concurrency. I suspect this to be caused by OAK-3348. ,compaction gc,['segmentmk'],OAK,Bug,Major,2015-11-23 14:59:44,15
12914080,Remove HierarchicalCacheInvalidator,As discussed in OAK-2187 and due to changes done in OAK-3002 HierrachialCacheInvalidator is now redundant and should be removed. ,technical_debt,['mongomk'],OAK,Task,Minor,2015-11-18 11:03:37,19
12913682,Inconsistent read of hierarchy ,"This is similar to OAK-3388, but about hierarchy information like which child nodes exist at a given revision of the parent node. This issue only occurs in a cluster.",resilience,"['core', 'documentmk']",OAK,Bug,Major,2015-11-17 12:35:04,2
12911832,Backport TarMK revision gc related issues,"Some of the issues related to TarMK revision gc should be back ported to the branches. This issue is for keeping track of which issues and which svn revisions we consider for back porting. The task consists of the following steps:

# Identify issue to back port
# Merge the respective commits into a private forks of the 1.0 and 1.2 branches
# Run tests on builds from the private forks
# On success merge the private forks to the 1.0 and 1.2 branches and update the fix versions of the respective issues. 
    * Update the svn merge info with the respective merged svn revisions. 
    * Update the fix versions of the affected issues.

[~dhasler]: FYI
[~alex.parvulescu], [~frm]: please refrain from merging potential conflicting changes into the branches in the meanwhile. 

",compaction gc,['segmentmk'],OAK,Task,Major,2015-11-10 11:12:36,15
12911545,Evaluate skipping cleanup of a subset of tar files,"Given the fact that tar readers are immutable (we only create new generations of them once they reach a certain threshold of garbage) we can consider coming up with a heuristic for skipping cleanup entirely for consequent cleanup calls based on the same referenced id set (provided we can make this set more stable, aka. OAK-2849).

Ex: for a specific input set a cleanup call on a tar reader might decide that there's no enough garbage (some IO involved in reading through all existing entries). if the following cleanup cycle would have the exact same input, it doesn't make sense to recheck the tar file, we already know cleanup can be skipped, moreover we can skip the older tar files too, as their input would also not change. the gains increase the larger the number of tar files.


",cleanup gc,['segment-tar'],OAK,Improvement,Major,2015-11-09 14:42:07,14
12911540,Remove the SegmentWriter segmentId from the cleanup set,"It looks like the current head's segment id (coming from the SegmentWriter) is always passed to the cleanup's referenced segment ids set, even though it cannot be cleaned (similar to the TarWriter situation) so I propose removing it early from the mentioned set.
benefits include making the cleanup set more stable (head changes quite often so the cleanup set is more volatile) which will help in figuring out if we still need to clean a specific tar file or not based on previous cleanup runs.",cleanup gc,['segmentmk'],OAK,Improvement,Minor,2015-11-09 14:33:50,14
12908516,Improve DefaultSyncContext,"cloning from OAK-3523 in order to not mix the {{ClassCastException}} issue with additional improvements i suggested in the patch, which include e.g. minor refactoring to remove code duplication and improve overall readability (IMO).",technical_debt,['auth-external'],OAK,Improvement,Major,2015-10-28 11:03:36,0
12908277,Tooling for writing segment graphs to a file,[Gephi|https://gephi.org/] turned out to be very valuable for examining segment graphs. I would like to add some tooling so we could dump the segment graph of a {{FileStore}} to a file. ,tooling,['run'],OAK,New Feature,Major,2015-10-27 16:36:57,15
12908135,Use write concern of w:majority when connected to a replica set,"Currently while connecting to Mongo MongoDocumentStore relies on default write concern provided as part of mongouri. 

Recently some issues were seen where Mongo based Oak was connecting to 3 member replica set and there were frequent replica state changes due to use of VM for Mongo. This caused data loss and corruption of data in Oak.

To avoid such situation Oak should default to write concern of majority by default. If some write concern is specified as part of mongouri then that should take precedence. This would allow system admin to take the call of tweaking write concern if required and at same time allows Oak to use the safe write concern.",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-10-27 06:41:11,2
12907331,o.a.j.o.api should not depend on Guava,The o.a.j.o.api has multiple dependencies on classes exported by the Google Guava bundle. The the o.a.j.o.api package should be made independent from Google Guava.,technical_debt,[],OAK,Improvement,Major,2015-10-23 08:52:41,12
12907100,VersionableState.copy doesn't respect OPV flag in the subtree,"while testing my work in OAK-1268 and OAK-2008, i found that items with OPV IGNORE are being copied into the frozen node of a versionable node upon checkin and only the first level child nodes are being tested for the OPV flag.

IMHO the OPV flag should be respected for all items in the subtree and act accordingly. The current bug might prevent versionable child nodes from being properly versioned and will copy items that are expected to be ignored (e.g. access control content) into the version store.

if i am not mistaken the properties are actually tested for the their OPV flag... if that is true, we might even have a bigger issue as the content in the version store is no longer complete and valid (e.g. mandatory/protected/autocreated properties being ignored but the node still being copied over and thus being invalid)",versioning,"['core', 'jcr']",OAK,Bug,Critical,2015-10-22 14:40:23,0
12905902,Unchecked assignements in calls to performVoid(),"Most if not all calls to {{SessionDelegate#performVoid}} pass a raw type to that method instead of parametrizing it with the {{Void}} type, which leads to an ""unchecked assignment"" warning. ",technical_debt,['jcr'],OAK,Improvement,Major,2015-10-19 10:33:05,15
12905230,DefaultSyncContext catches ClassCastException,"the group and group-member sync code in the {{DefaultSyncContext}} twice catches {{ClassCastException}} and swallows exception situations, where a user is found, when actually a {{Group}} was expected.

i would suggest to 
- explicitly test the assumption wrt {{ExternalIdentity}} being a group (instead of waiting for the exception)
- make use of {{UserManager.getAuthorizable(String, Class) to explicitly retrieve a Group with a given ID, when this is actually expected. This method will throws an {{AuthorizableTypeException}} if there exists a {{User}} with that ID as thus properly raise the unexpected behavior instead of swallowing with a log-warning.",technical_debt,['auth-external'],OAK,Bug,Major,2015-10-15 15:41:44,0
12904464,Test failure: CompactionMapTest.removeSome,"Said test fails sporadically:

{noformat}
at org.junit.Assert.assertNull(Assert.java:562)
at org.apache.jackrabbit.oak.plugins.segment.CompactionMapTest.removeSome(CompactionMapTest.java:156)
{noformat}

This is a regression introduced with OAK-3501: the {{recent}} map gets not cleared when {{segmentIdMap}} is empty. This can happen when a recent key is removed again while there are no other changes. ",compaction gc,['segmentmk'],OAK,Bug,Major,2015-10-13 07:26:55,15
12904208,Uniformization of compaction log messages,"The logs generated during different phases of tar garbage collection (compaction) are currently quite heterogenous and difficult to grep/parse.

I propose with the attached patch to uniformize these logs, changing the following:
# all logs start with the prefix {{TarMK GargabeCollection \{\}#:}}
# different phases of garbage collection are easier to identify by the first word after prefix, e.g. estimation, compaction, cleanup
# all values are also printed in a standard unit, with the following format: {{<human_readable_value> (<standard_unit_value>)}}. This makes extraction of information much easier.
# messages corresponding to the same cycle (run) can be grouped by including the runId in the prefix.

Note1: I don't have enough visibility, but the changes might impact any system relying on the old format. Yet, I've seen they have changed before so this might not be a real concern.

Note2: the runId is implemented as a static variable, which is reset every time the class is reloaded (e.g. at restart), so it is unique only during one run.

Below you can find an excerpt of old logs and new logs to compare:

NEW:
{code}
12.10.2015 16:11:56.705 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: started
12.10.2015 16:11:56.707 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: estimation started
12.10.2015 16:11:59.275 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: estimation completed in 2.569 s (2567 ms). Gain is 16% or 1.1 GB/1.3 GB (1062364160/1269737472 bytes), so running compaction
12.10.2015 16:11:59.275 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: compaction started, strategy=CompactionStrategy{paused=false, cloneBinaries=false, cleanupType=CLEAN_OLD, olderThan=36000000, memoryThreshold=5, persistedCompactionMap=true, retryCount=5, forceAfterFail=true, compactionStart=1444659116706, offlineCompaction=false}
12.10.2015 16:12:05.839 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.Compactor Finished compaction: 420022 nodes, 772259 properties, 20544 binaries.
12.10.2015 16:12:07.459 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: compaction completed in 8.184 s (8183 ms), after 0 cycles
12.10.2015 16:12:11.912 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:12:11 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: cleanup started. Current repository size is 1.4 GB (1368899584 bytes)
12.10.2015 16:12:12.368 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:12:11 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: cleanup marking file for deletion: data00008a.tar
12.10.2015 16:12:12.434 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:12:11 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: cleanup completed in 522.8 ms (522 ms). Post cleanup size is 1.2 GB (1217132544 bytes)and space reclaimed 151.8 MB (151767040 bytes). Compaction map weight/depth is 0 B/1 (0 bytes/1).
{code} 

OLD:
{code}
12.10.2015 15:54:55.115 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK compaction started
12.10.2015 15:54:56.082 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore Estimated compaction in 967.6 ms, gain is 7% (1083809280/1170960384) or (1.1 GB/1.2 GB), so running compaction
12.10.2015 15:54:56.083 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK compaction running, strategy=CompactionStrategy{paused=false, cloneBinaries=false, cleanupType=CLEAN_OLD, olderThan=36000000, memoryThreshold=5, persistedCompactionMap=true, retryCount=5, forceAfterFail=true, compactionStart=1444658095115, offlineCompaction=false}
12.10.2015 15:55:01.986 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.Compactor Finished compaction: 419878 nodes, 771824 properties, 20542 binaries.
12.10.2015 15:55:03.273 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK compaction completed after 0 cycles in 7190ms
12.10.2015 15:55:08.032 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:55:08 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK revision cleanup started. Current repository size 1.3 GB
12.10.2015 15:55:08.719 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:55:08 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK revision cleanup completed in 688.0 ms. Post cleanup size is 1.3 GB and space reclaimed 0. Compaction map weight/depth is 0 B/1.
{code}",compaction gc,['core'],OAK,Improvement,Major,2015-10-12 15:11:48,15
12903762,Improve logging during cleanup,We should have better logging during cleanup. E.g. why a file has been skipped / cleaned etc. ,cleanup gc,['segmentmk'],OAK,Improvement,Major,2015-10-09 15:01:22,15
12903667,PersistedCompactionMap could release reference to records early,"Whenever a PersistedCompactionMap becomes empty it will be eventually dropped from the compaction maps chain. this will happen on the next compaction cycle, which happens post-cleanup. so we're potentially keeping a reference to some collectable garbage for up to 2 cycles.
I'd like to propose a patch that allows for eagerly nullifying the reference to the records, making this interval shorter.",compaction gc,['segmentmk'],OAK,Improvement,Minor,2015-10-09 08:42:00,14
12903311,Remove DocumentNodeStore.diff(),"The method is only used by the DocumentMK class, which is now considered a test helper (OAK-2907) and part of the API anymore.

The method should be removed from the DocumentNodeStore and functionality moved to the DocumentMK.find() method.",technical_debt,"['core', 'mongomk']",OAK,Improvement,Minor,2015-10-08 09:32:54,2
12902999,MemoryDiffCache should also check parent paths before falling to Loader (or returning null),"Each entry in {{MemoryDiffCache}} is keyed with {{(path, fromRev, toRev)}} for the list of modified children at {{path}}. A diff calcualted by {{DocumentNodeStore.diffImpl}} at '/' (passively via loader) or {{JournalEntry.applyTo}} (actively) fill each path for which there are modified children (including the hierarchy)

But, if an observer calls {{compareWithBaseState}} on a unmodified sub-tree, the observer will still go down to {{diffImpl}} although cached parent entry can be used to answer the query.",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-10-07 13:55:53,2
12902989,Deadlock when closing a concurrently used FileStore 2.0,"A deadlock was detected while stopping the {{SegmentCompactionIT}} using the exposed MBean.

{noformat}
""main@1"" prio=5 tid=0x1 nid=NA waiting for monitor entry
 waiting for pool-1-thread-10@2111 to release lock on <0xae8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.dropCache(SegmentWriter.java:871)
  at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.close(FileStore.java:1031)
  - locked <0xae7> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT.tearDown(SegmentCompactionIT.java:282)

""pool-1-thread-10@2111"" prio=5 tid=0x1d nid=NA waiting for monitor entry
  java.lang.Thread.State: BLOCKED
 blocks main@1
 waiting for main@1 to release lock on <0xae7> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
  at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.writeSegment(FileStore.java:1155)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:253)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.prepare(SegmentWriter.java:350)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:468)
  - locked <0xae8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:719)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1211)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1156)
  at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1147)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1175)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:451)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:474)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:530)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:208)
{noformat}",resilience,['segmentmk'],OAK,Bug,Major,2015-10-07 13:27:14,15
12902929,LastRevRecovery for self async?,"Currently, when a cluster node starts and discovers that it wasn't properly shutdown, it first runs the complete LastRevRecovery and only continues startup when done.

However, when it fails to acquire the recovery lock, which implies that a different cluster node is already running the recovery on its behalf, it simply skips recovery and continues startup?

So what is it? Is running the recovery before proceeding critical or not? If it is, this code in {{LastRevRecoveryAgent}} needs to change:

{code}
        //TODO What if recovery is being performed for current clusterNode by some other node
        //should we halt the startup
        if(!lockAcquired){
            log.info(""Last revision recovery already being performed by some other node. "" +
                    ""Would not attempt recovery"");
            return 0;
        }
{code}

If it's not critical, we may want to run the recovery always asynchronously. 
cc [~mreutegg]  and [~chetanm]",resilience,['documentmk'],OAK,Bug,Major,2015-10-07 08:58:41,2
12902727,FileStoreIT cleanup after tests,The test doesn't remove its created directories.,cleanup compaction gc,"['core', 'segmentmk']",OAK,Improvement,Minor,2015-10-06 16:07:10,14
12902653,CompactionMapTest does not close file store,Each test in the class retains about 16MB of heap and subsequent tests may fail with an OOME. E.g. see recent build failure on travis: https://travis-ci.org/apache/jackrabbit-oak/builds/83848567,cleanup compaction gc,"['core', 'segmentmk']",OAK,Bug,Minor,2015-10-06 10:04:52,2
12902627,Add generation info to compaction map,"I'd like to add a counter that exposes the current generation of the compaction map, (meaning the number of compaction cycles).
So far I've been using the CompactionMap#getDepth as a heuristic but if you run long enough some old generations will be dropped decreasing the depth, making it unreliable as a generation counter.
I don't want to persist this info, just start at 0 once the system starts up and increase on each CompactionMap#cons call.",cleanup compaction gc,['segmentmk'],OAK,Improvement,Minor,2015-10-06 08:37:15,14
12902425,Confusing SNFE whith oak-run debug,"Running 

{{oak-run debug  /path/to/segmentStore}}

can result in {{SNFE}} s being logged if the file store has been compacted before. This can happen even though the repository is actually consistent according to {{oak-run check}}. 

The reason is {{debug}} traversing all node states of all record ids in all tar files. When a previous cleanup of a tar file did not reach 25% gain it will not clean up that file leaving behind segments possibly containing node states pointing to limbo. Those nodes would result in said {{SNFE}} of {{oak-run}} debug. But as those node states are not reachable from the head node state the repository is still consistent itself. ",cleanup gc tooling,['run'],OAK,Improvement,Major,2015-10-05 14:16:10,15
12902419,NodeDocument.getNodeAtRevision can go into property history traversal when latest rev on current doc isn't committed,"{{NodeDocument.getNodeAtRevision}} tried to look at latest revisions entries for each property in current document. But it just looks at the *last* entry for a given property. In case this last entry isn't committed, the code would go into previous documents to look for a committed value.

(cc [~mreutegg])",performance,"['core', 'mongomk']",OAK,Bug,Major,2015-10-05 13:12:45,2
12902115,Replace BackgroundThread with Scheduler,I think we should replace the background thread with some kind of a scheduler. The goal would be to decouple threading from scheduling. IMO threads should not be managed by the application but by the container. ,technical_debt,['segment-tar'],OAK,Technical task,Major,2015-10-02 13:07:22,12
12900998,Improve the logging capabilities of offline compaction,"The offline compaction instantiates {{FileStore}} using a deprecated constructor. This constructor forces a no-op {{GCMonitor}} that swallows log messages and caught exception.

It would be more appropriate to create the {{FileStore}} using the corresponding {{Builder}}. This has the side effect of configuring a {{LoggingGCMonitor}}, which provides way more information than the current default.",compaction gc,"['core', 'run']",OAK,Improvement,Major,2015-09-28 10:52:10,12
12896376,DocumentNodeStore support for predefined clusterIds should use ClusterNodeInfos,"We currently support explicit assignment of clusterId (builder.setClusterId()). In this case, DocumentNodeStore uses the specified cluster id and skips all code related to creating/maintaining ClusterNodeInfos.

This feature is mainly (only?) used for testing (mainly allowing multiple instances to run from the same machine/instance combination). This works, but causes the logic related to ClusterNodeInfo not to be used at all (for instance, LastRevRecovery).

So we ought to change this config option to use ClusterNodeInfo in a way that is at least similar to real-world use.",test,"['core', 'mongomk', 'rdbmk']",OAK,Improvement,Major,2015-09-25 11:21:42,3
12896027,Track the start time of mark in GC,"Similarly to what is done for a shared datastore in OAK-3360, the start time of the mark should be used as a reference for deletion of blobs in GC for 1.0.x. The problem is that in 1.0.x the mark iterated over the data store which could take a lot of time (many hours) and the buffer due to {{blobGcMaxAgeInSecs}} may get stale due to which data loss could happen.
",resilience,['blob'],OAK,Sub-task,Critical,2015-09-24 08:13:54,16
12895331,Prevent missing checkpoint due to unstable topology from causing complete reindexing,"Async indexing logic relies on embedding application to ensure that async indexing job is run as a singleton in a cluster. For Sling based apps it depends on Sling Discovery support. At times it is being seen that if topology is not stable then different cluster nodes can consider them as leader and execute the async indexing job concurrently.

This can cause problem as both cluster node might not see same repository state (due to write skew and eventual consistency) and might remove the checkpoint which other cluster node is still relying upon. For e.g. consider a 2 node cluster N1 and N2 where both are performing async indexing.

# Base state - CP1 is the checkpoint for ""async"" job
# N2 starts indexing and removes changes CP1 to CP2. For Mongo the checkpoints are saved in {{settings}} collection
# N1 also decides to execute indexing but has yet not seen the latest repository state so still thinks that CP1 is the base checkpoint and tries to read it. However CP1 is already removed from {{settings}} and this makes N1 think that checkpoint is missing and it decides to reindex everything!

To avoid this topology must be stable but at Oak level we should still handle such a case and avoid doing a full reindexing. So we would need to have a {{MissingCheckpointStrategy}} similar to {{MissingIndexEditorStrategy}} as done in OAK-2203 

Possible approaches
# A1 - Fail the indexing run if checkpoint is missing - Checkpoint being missing can have valid reason and invalid reason. Need to see what are valid scenarios where a checkpoint can go missing
# A2 - When a checkpoint is created also store the creation time. When a checkpoint is found to be missing and its a *recent* checkpoint then fail the run. For e.g. we would fail the run till checkpoint found to be missing is less than an hour old (for just started take startup time into account)
",resilience,['query'],OAK,Improvement,Major,2015-09-22 04:49:21,14
12895116,Background update may create journal entry with incorrect id,The conflict check does not consider changes that are made visible between the rebase and the background read.,resilience,"['core', 'mongomk']",OAK,Bug,Critical,2015-09-21 13:12:19,2
12875802,DocumentNodeStoreService fails to restart DocumentNodeStore,"Scenario (test case will follow):

1) service configured for RDBDocumentStore with DataSources

2) gets started

3) DataSource unregisters, DocumentNodeStore gets shut down

4) DataSource is registered again

5) restart of DocumentNodeStore fails with NPE because at least one required component (executor?) now is null

We need to decide whether this is a scenario that should be supported or not. If yes, we need to fix it. If no, we need to make the code more robust and improve diagnostics.",resilience,"['core', 'mongomk', 'rdbmk']",OAK,Bug,Major,2015-09-18 08:16:43,3
12875411,ClusterNodeInfo.createInstance fails to clean up random entries,"{code}
            String mId = """" + doc.get(MACHINE_ID_KEY);
            String iId = """" + doc.get(INSTANCE_ID_KEY);
            if (machineId.startsWith(RANDOM_PREFIX)) {
                // remove expired entries with random keys
                store.remove(Collection.CLUSTER_NODES, key);
                continue;
            }
{code}

The intent seems to be to cleanup entries in the cluster node table that start with RANDOM_PREFIX. However, {{machineId}} is checked instead of {{mId}}. When {{createInstance}} is called with a random id, the whole table might get wiped out. ",resilience,['core'],OAK,Bug,Major,2015-09-17 16:17:18,3
12875392,ClusterNodeInfo uses irrelevant network interface IDs on Windows,"On Windows, all kinds of adapters (tunnel, VPN) return a hardware address of 00-00-00-00-00-00-00-E0 (note 8 bytes, not 6). These addresses are useless for the identification of the machine, however they get used because they are the lowest value.

A potential fix is to change the validity check to:

if (mac != null && mac.length == 6)
",resilience,['core'],OAK,Bug,Major,2015-09-17 15:24:27,3
12864161,Avoid instanceof check in LastRevRecoveryAgent,"Similar to OAK-3390, the instanceof check in LastRevRecoveryAgent does not work when the MongoDocumentStore is wrapped.",technical_debt,"['core', 'mongomk']",OAK,Bug,Major,2015-09-15 07:27:07,2
12863902,Multiplexing NodeStore support in Oak layer,"Supporting multiplexing repository would have impact on various places in Oak design. There are various sub components in Oak which maintain there own storage built on top of NodeStore. For e.g. indexes are stored within NodeStore, permissions are also stored within NodeStore. Adding multiplexing support would impact such stores in following ways

The most basic application of multiplexing support is to support private and shared storage. Under this an Oak application would have a private store and a shared store. Content under certain paths would be stored under private repo while all other content is stored under shared repo

# *Writing* - Any content written via JCR API passes through some {{CommitHooks}}. These hooks are responsible for updating the indexes, permission store etc. Now if any path say /foo/bar gets modified the commits hooks would need to determine under which path in NodeStore should the derived data (index entries, permission etc) should be stored. For simple case of private and shared store where we have 2 sets of paths private and shared these hooks would need to be aware of that and use different path in NodeStore to store the derived content. Key point to note here that any such storage has to differentiate wether the path from which the content is being derived is a private path or shared path

# *Reading* - Reading requirement compliments the writing problem. While performing any JCR operation Oak might need to invoke QueryIndex, PermissionStore etc. These stores in turn would need to perform a read from there storage area within NodeStore. For multiplexing support these components would then need to be aware that there storage can exist in both shared and private stores

h4. Terms Used

# _private repo_ (PR) - Set of paths which are considered private to the application. Tentative example /lib,/apps
# _shared repo_ (SR) - Set of paths which are considered shared and different versions of the application can perform read and write operations on them. Tentative example /content, /etc/workflow/instances
# {{PathToStoreMapper}} - Responsible for mapping a path to store type. For now it can just answer either PR or SR. But the concept can be generalized 

Aim of this story is to prototype changes in Oak layer in a fork to asses the impact on current implementation",multiplexing,['core'],OAK,Epic,Major,2015-09-14 15:17:51,19
12863035,Inconsistent read in cluster with clock differences,This issue is similar to OAK-2929 but related to how the DocumentNodeStore reads a node state when there is a clock difference between multiple cluster nodes. The node state read from a NodeDocument may not be correct when there is a clock difference.,resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-09-10 12:39:42,2
12862768,Make compress-interval configurable,"I'm breaking this part off from OAK-3133 as the backport is harmless and this is a very important option to have.
This only applies to 1.0 and 1.2 branches, as trunk code has evolved quite a bit.",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-09-09 14:29:01,14
12862712,Oak run should use InMemoryCompactionMap by default,"Allow the option to choose the type of CompactionMaps oak-run uses, and set the default to the in-memory version.",compaction gc,"['run', 'segmentmk']",OAK,Improvement,Major,2015-09-09 09:15:49,14
12862677,Test failure: SuggestTest,"Various testcases in SuggestTest test are failing on *1.0 branch* for past few runs 390, 391, 394, 398

{noformat}
testSuggestSql(org.apache.jackrabbit.oak.jcr.query.SuggestTest)  Time elapsed: 7.963 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<[[{term=in 2015 a red fox is still a fox,weight=1}, {term=in 2015 my fox is red, like mike's fox and john's fox,weight=1}]]> but was:<[]>
	at junit.framework.Assert.assertEquals(Assert.java:85)
	at junit.framework.Assert.assertEquals(Assert.java:91)
	at org.apache.jackrabbit.oak.jcr.query.SuggestTest.testSuggestSql(SuggestTest.java:50)
testSuggestXPath(org.apache.jackrabbit.oak.jcr.query.SuggestTest)  Time elapsed: 0.149 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<[[{term=in 2015 a red fox is still a fox,weight=1}, {term=in 2015 my fox is red, like mike's fox and john's fox,weight=1}]]> but was:<[]>
	at junit.framework.Assert.assertEquals(Assert.java:85)
	at junit.framework.Assert.assertEquals(Assert.java:91)
	at org.apache.jackrabbit.oak.jcr.query.SuggestTest.testSuggestXPath(SuggestTest.java:67)
	
{noformat}

",ci jenkins,['solr'],OAK,Bug,Major,2015-09-09 06:59:36,10
12862389,Collapsing external events in BackgroundObserver even before queue is full leads to JournalEntry not getting used,"BackgroundObserver currently merges external events if the last one in queue is also an external event. This leads to diff being done for a revision pair which is different from the ones pushed actively into cache during backgroud read (using JournalEntry) i.e. diff queries for {{diff(""/a/b"", rA, rC)}} while background read had pushed results of {{diff(""/a/b"", rA, rB)}} and {{diff(""/a/b"", rB, rC)}}.

(cc [~mreutegg], [~egli], [~chetanm], [~mduerig])",performance resilience,['core'],OAK,Improvement,Major,2015-09-08 11:49:32,15
12862325,Boosting fields not working as expected,"When the boost support was added the intention was to support a usecase like 

{quote}
For the fulltext search on a node where the fulltext content is derived from multiple field it should be possible to boost specific text contributed by individual field. Meaning that if a title field is boosted more than description, the title (part) in the fulltext field will mean more than the description (part) in the fulltext field.
{quote}

This would enable a user to perform a search like _/jcr:root/content/geometrixx-outdoors/en//element(*, cq:Page)\[jcr:contains(., 'Keyword')\]_ and get a result where pages having 'Keyword' in title come above in search result compared to those where Keyword is found in description.

Current implementation just sets the boost while add the field value to fulltext field with the intention that Lucene would use the boost as explained above. However it does not work like that and boost value gets multiplies with other field and hence boosting does not work as expected",doc-impacting,['lucene'],OAK,Bug,Major,2015-09-08 06:27:07,19
12862207,Estimate compaction based on diff to previous compacted head state,"Food for thought: try to base the compaction estimation on a diff between the latest compacted state and the current state.

Pros
* estimation duration would be proportional to number of changes on the current head state
* using the size on disk as a reference, we could actually stop the estimation early when we go over the gc threshold.
* data collected during this diff could in theory be passed as input to the compactor so it could focus on compacting a specific subtree

Cons
* need to keep a reference to a previous compacted state. post-startup and pre-compaction this might prove difficult (except maybe if we only persist the revision similar to what the async indexer is doing currently)
* coming up with a threshold for running compaction might prove difficult
* diff might be costly, but still cheaper than the current full diff

",compaction gc,['segment-tar'],OAK,New Feature,Minor,2015-09-07 12:20:27,14
12862191,Reduce PerfLogger isDebugEnabled overhead,"If a node is cached, 1/4 of the time which is used to call DocumentNodeStore.getNode is spent in PerfLogger.start and PerfLogger.end just for checking whether or not debug logging is enabled (this is likely much less if no TurboFilters are used).

To reduce the overhead of the PerfLogger, it should not check if debug is enabled in end() if start is below 0 anyway. Moreover, it would help to check only every second if debug is really enabled.",performance,['core'],OAK,Improvement,Minor,2015-09-07 10:06:36,2
12862181,Tracking the start time of mark in GC for a shared datastore,"Currently, for GC in a shared datastore, the last modified timestamp of the earliest references file is used to calculate the max age of blobs to be deleted. There is a possibility that the  process itself could have taken quite a long time which opens up a window that recent blobs could also be deleted. 
The mark process is quite fast and with the default setting of {{blobGcMaxAgeInSecs}} of 24 hours this should not be a problem but if the {{blobGcMaxAgeInSec}} is specified to a lower value then it could create that window described above.",resilience,['blob'],OAK,Sub-task,Minor,2015-09-07 09:12:39,16
12862179,Compactor progress log,"I'd like to introduce some logs to follow the compaction progress. It is very difficult to provide a progress expressed as a percentage because the diff has no knowledge of the total number of nodes, but we can follow the approach taken by the indexers and provide a log each 15k nodes.",cleanup compaction gc,['segmentmk'],OAK,Improvement,Major,2015-09-07 09:05:01,14
12862161,Test failure: SpellcheckTest.testSpellcheckMultipleWords,"{{org.apache.jackrabbit.oak.jcr.query.SpellcheckTest.testSpellcheckMultipleWords}} fails on Jenkins.

Failure seen at builds: 389, 392, 395, 396, 562

https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/396/jdk=jdk-1.6u45,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/console

{noformat}
testSpellcheckMultipleWords(org.apache.jackrabbit.oak.jcr.query.SpellcheckTest)  Time elapsed: 0.907 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<[voting[ in] ontario]> but was:<[voting[, voted,] ontario]>
	at junit.framework.Assert.assertEquals(Assert.java:85)
	at junit.framework.Assert.assertEquals(Assert.java:91)
	at org.apache.jackrabbit.oak.jcr.query.SpellcheckTest.testSpellcheckMultipleWords(SpellcheckTest.java:86)
{noformat}",ci jenkins test test-failure,['solr'],OAK,Bug,Major,2015-09-07 07:37:49,10
12861787,Incremental compaction,"This is OAK-3349 taken to the extreme: given a segment that is almost not referenced any more we could just rewrite the still referenced content. That is, say a segment contains two properties reachable from the current root node state and all its remaining content is not reachable from the root node state. In that case we could rewrite these two properties and create a new root node state referencing the rewritten properties. This would effectively make the segment eligible for being gc-ed. 
Such an approach would start from segments that are sparse and compact these instead of compacting everything as we currently do, which might cause a lot of copying around stuff that already is compact. The challenging part here is probably finding the segments that are sparse as this involves inverting the reference graph. 

Todo: Asses feasibility and impact, implement prototype.",compaction gc scalability,['segment-tar'],OAK,New Feature,Minor,2015-09-04 09:00:14,15
12861783,Partial compaction,"On big repositories compaction can take quite a while to run as it needs to create a full deep copy of the current root node state. For such cases it could be beneficial if we could partially compact the repository thus splitting full compaction over multiple cycles. 
Partial compaction would run compaction on a sub-tree just like we now run it on the full tree. Afterwards it would create a new root node state by referencing the previous root node state replacing said sub-tree with the compacted one. 

Todo: Asses feasibility and impact, implement prototype.",compaction gc scalability,['segment-tar'],OAK,New Feature,Major,2015-09-04 08:45:58,12
12861779,Cross gc sessions might introduce references to pre-compacted segments,"I suspect that certain write operations during compaction can cause references from compacted segments to pre-compacted ones. This would effectively prevent the pre-compacted segments from getting evicted in subsequent cleanup phases. 

The scenario is as follows:
* A session is opened and a lot of content is written to it such that the update limit is exceeded. This causes the changes to be written to disk. 
* Revision gc runs causing a new, compacted root node state to be written to disk.
* The session saves its changes. This causes rebasing of its changes onto the current root (the compacted one). At this point any node that has been added will be added again in the sub-tree rooted at the current root. Such nodes however might have been written to disk *before* revision gc ran and might thus be contained in pre-compacted segments. As I suspect the node-add operation in the rebasing process *not* to create a deep copy of such nodes but to rather create a *reference* to them, a reference to a pre-compacted segment is introduced here. 

Going forward we need to validate above hypothesis, assess its impact if necessary come up with a solution.
",cleanup compaction gc,['segment-tar'],OAK,Improvement,Critical,2015-09-04 08:32:39,15
12861773,Ineffective cleanup after compaction due to references to root,"The cleanup phase after a compaction run is currently not able to remove the pre compacted segments as the previous (pre-compacted) root is still being referenced. Those references are coming from:

* The {{before}} [local variable|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/FileStore.java#L653] in {{FileStore.flush}}.
* The [current head|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentNodeStore.java#L85-L85] of the {{SegmentNodeStore}}.",cleanup compac gc,['segmentmk'],OAK,Improvement,Major,2015-09-04 08:17:52,15
12861600,SessionMBean should provide information about pending refresh,"The session auto refresh feature is implemented by marking sessions pending for refresh. The refresh operation itself however only happens on the next access to the session. 

It would be helpful if {{SessionMBean}} could expose the information whether a session has a pending refresh. Additionally we could expose the current {{RefreshStrategy}} to make the auto refresh behaviour more transparent. ",gc monitoring,['core'],OAK,Sub-task,Major,2015-09-03 15:08:38,3
12861552,Commit may add collision marker for committed revision,"It may happen that a commit adds a collision marker for a revision which is already committed. {{Collision.markCommitRoot()}} does not perform a conditional update when it adds the collision marker. Though, it checks the document after the update if the marked revision is committed.",resilience,"['core', 'mongomk']",OAK,Bug,Minor,2015-09-03 12:20:39,2
12861256,SplitOperations purges _commitRoot entries too eagerly,"OAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.

The purge operation is too eager and removes all such entries, which may result in an undetected hierarchy conflict.",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-09-02 08:55:22,2
12861236,FileStore lock contention with concurrent writers,"Concurrently writing to the file store can lead to a sever lock contention in {{FileStore#readSegment}}. That method searches the current {{TarWriter}} instance for the segment once it could not be found in any of the {{TarReader}} instances. This is the point where synchronizes on the {{FileStore}} instance, which leads to  the contention. 
The effect is only observable once the segment cache becomes full and reads actually need to go to the file store. Thus a possible improvement could be to pin segments from the current tar writer to the cache. Alternatively we could try to ease locking by employing read/write locks where possible. ",compaction,['segmentmk'],OAK,Improvement,Major,2015-09-02 07:26:38,15
12861233,TarMK cleanup blocks writers,"TarMK cleanup exclusively locks the {{FileStore}}, which causes concurrent writers to block until cleanup finished. Initially cleanup was expected to be reasonably fast, however I have seen it taking dozens of minutes under certain circumstances (most likely many tar files with many small segments, aka OAK-2896).",cleanup gc,['segmentmk'],OAK,Improvement,Major,2015-09-02 07:06:17,15
12860662,ConcurrentModificationException when running SegmentOverflowExceptionIT,"{noformat}
Running org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2,426.873 sec <<< FAILURE!
run(org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT)  Time elapsed: 2,426.373 sec  <<< ERROR!
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)
	at java.util.ArrayList$Itr.next(ArrayList.java:831)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.wasCompactedTo(CompactionMap.java:60)
	at org.apache.jackrabbit.oak.plugins.segment.Record.wasCompactedTo(Record.java:64)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.equals(SegmentBlob.java:213)
	at com.google.common.base.Objects.equal(Objects.java:55)
	at org.apache.jackrabbit.oak.plugins.memory.AbstractPropertyState.equal(AbstractPropertyState.java:53)
	at org.apache.jackrabbit.oak.plugins.memory.AbstractPropertyState.equals(AbstractPropertyState.java:90)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1176)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT.run(SegmentOverflowExceptionIT.java:130)
{noformat}

This is caused by concurrently accessing the underlying list of maps in {{CompactionMap#remove}}. ",compaction gc,['segmentmk'],OAK,Bug,Major,2015-08-31 12:15:08,15
12859617,Segment Tar SegmentCache loader stats,"The existing Segment Cache has no loading-related stats, I'd like to see how complicated it would be to add some.",gc,['segment-tar'],OAK,Technical task,Major,2015-08-27 14:28:17,14
12859572,Self recovering instance may not see all changes,"When a DocumentNodeStore instance is killed and restarted, the _lastRev recovery mechanism is triggered on startup. It may happen that the restarted instance does not see all changes that were recovered.",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-08-27 11:08:40,2
12858956,SNFE in SegmentOverflowExceptionIT ,"{{org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT}} can fail with an {{SNFE}}. This is somewhat expected due to the low segment retention time used for this test. That time is apparently needed for this test to reproduce the original issue. So I'd rather not touch it. 

I propose to ignore that exception and retry a couple of times until failing the test. ",test,['segmentmk'],OAK,Improvement,Major,2015-08-26 11:45:50,15
12858640,SegmentOverflowExceptionIT runs forever unless it fails,Currently {{SegmentOverflowExceptionIT}} runs forever or until it fails. We should add a time out after which the test is considered passed.,test,['segmentmk'],OAK,Improvement,Major,2015-08-25 14:36:20,15
12858615,Revision gc blocks repository shutdown,Shutting down the repository while revision gc is running might block for a long time until either compaction estimation/compaction or clean up has finished. We should provide a way to interrupt those operations for a timely shut down. ,cleanup compaction gc,['segmentmk'],OAK,Improvement,Major,2015-08-25 13:14:25,15
12858608,clarify DocumentStore contract with respect to number formats,"The DS API allows setting properties as java.lang.Integer, but implementations vary in whether they can roundtrip Integers; some do, some convert to Long.

The former is observed for MongoMK (which uses BSON internally), the latter is see in RDBMK (which uses JSON).

We should

- clarify that integers can be set, but they will come back as longs, and
- modify existing implementations to always return longs, so bugs surface early
",resilience,"['core', 'mongomk', 'rdbmk']",OAK,Improvement,Major,2015-08-25 12:50:40,3
12858601,Datastore performance improvements,Collector issue for DS performance improvements,performance,['blob'],OAK,Epic,Major,2015-08-25 12:22:44,16
12858540,Test failures on trunk: SolrIndexQueryTestIT.sql2,"{{org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.sql2}} fails regularly on Jenkins: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/350/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=integrationTesting/testReport/junit/org.apache.jackrabbit.oak.plugins.index.solr.query/SolrIndexQueryTestIT/sql2/

{noformat}
java.lang.Exception: Results in target/oajopi.solr.query.SolrIndexQueryTestIT_sql2.txt don't match expected results in file:/x1/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/jdk1.8.0_11/label/Ubuntu/nsfixtures/DOCUMENT_RDB/profile/integrationTesting/oak-core/target/oak-core-1.4-SNAPSHOT-tests.jar!/org/apache/jackrabbit/oak/query/sql2.txt; compare the files for details
	at org.apache.jackrabbit.oak.query.AbstractQueryTest.test(AbstractQueryTest.java:232)
	at org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.sql2(SolrIndexQueryTestIT.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
	at org.junit.rules.RunRules.evaluate(RunRules.java:18)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{noformat}",ci jenkins,['lucene'],OAK,Bug,Major,2015-08-25 07:25:53,4
12858277,Improve DocumentMK resilience,Collection of DocMK resilience improvements,resilience,"['mongomk', 'rdbmk']",OAK,Epic,Critical,2015-08-24 11:57:55,2
12858273,Improve indexing resilience,As discussed bilaterally grouping the improvements for indexer resilience in this issue for easier tracking,resilience,['lucene'],OAK,Epic,Critical,2015-08-24 11:52:32,19
12858272,Improve datastore resilience,As discussed bilaterally grouping the improvements for datastore resilience in this issue for easier tracking,resilience,['blob'],OAK,Epic,Critical,2015-08-24 11:47:26,16
12857823,Deadlock between persisted compaction map and cleanup 2,"Just seen this deadlock while running {{SegmentCompactionIT}}:

{noformat}
""pool-1-thread-47"":
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:910)
	- waiting to lock <0x0000000700110bd0> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:211)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	- locked <0x0000000700328b88> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:121)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:103)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1098)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:85)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:81)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.remove(MemoryNodeBuilder.java:355)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.modify(SegmentCompactionIT.java:448)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:430)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:406)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
""TarMK flush thread [target/SegmentCompactionIT9065337410200765612dir], active since Fri Aug 21 06:53:18 GMT+00:00 2015, previous max duration 40846ms"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:145)
	- waiting to lock <0x0000000700328b88> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.compress(PersistedCompactionMap.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.remove(PersistedCompactionMap.java:155)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.remove(CompactionMap.java:108)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.cleanup(FileStore.java:699)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:628)
	- locked <0x0000000700110bd0> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	- locked <0x000000070017f1c0> (a java.util.concurrent.atomic.AtomicReference)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$1.run(FileStore.java:413)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.jackrabbit.oak.plugins.segment.file.BackgroundThread.run(BackgroundThread.java:70)
{noformat}",cleanup compaction gc,['segmentmk'],OAK,Bug,Major,2015-08-21 11:30:38,15
12857815,Support including and excluding paths for PropertyIndex,As part of OAK-2599 support for excluding and including paths were added to Lucene index. It would be good to have such a support enabled for PropertyIndexe also,performance,['query'],OAK,Improvement,Major,2015-08-21 10:41:54,13
12857550,Optimize NodeDocument.getNewestRevision(),"Most of the time NodeDocument.getNewestRevision() is able to quickly identify the newest revision, but sometimes the code falls to a more expensive calculation, which attempts to read through available {{_revisions}} and {{_commitRoot}} entries. If either of those maps are empty, the method will go through the entire revision history.",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-08-20 14:45:01,2
12857186,Support caching in FileDataStoreService,"FDS on SAN/NAS storage is not efficient as it involves network call. In OAK. indexes are stored SAN/NAS and even idle system does lot of read system generated data. 

Enable caching in FDS so the reads are done locally and async upload to SAN/NAS

See [previous discussions|https://issues.apache.org/jira/browse/OAK-3005?focusedCommentId=14700801&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14700801]",docs-impacting features performance,['blob'],OAK,Improvement,Blocker,2015-08-19 11:53:38,16
12856839,DocumentNodeStore.retrieve() should not throw IllegalArgumentException,"{{DocumentNodeSTore#retrieve(checkpoint)}} may throw an {{IllegalArgumentException}} via {{Revision.fromString(checkpoint)}}.

The javadocs say that it returns a {{NodeState}} or {{null}}. The exception prevents recovery of {{AsyncIndexUpdate}} from a bad recorded checkpoint.",resilience,"['core', 'mongomk', 'rdbmk']",OAK,Improvement,Minor,2015-08-18 07:32:20,2
12856168,Deadlock when closing a concurrently used FileStore,"A deadlock was detected while stopping the {{SegmentCompactionIT}} using the exposed MBean.

{noformat}
Found one Java-level deadlock:
=============================
""pool-1-thread-23"":
  waiting to lock monitor 0x00007fa8cf1f0488 (object 0x00000007a0081e48, a org.apache.jackrabbit.oak.plugins.segment.file.FileStore),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x00007fa8cc015ff8 (object 0x00000007a011f750, a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter),
  which is held by ""pool-1-thread-23""

Java stack information for the threads listed above:
===================================================
""pool-1-thread-23"":
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.writeSegment(FileStore.java:948)
	- waiting to lock <0x00000007a0081e48> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:228)
	- locked <0x00000007a011f750> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.prepare(SegmentWriter.java:329)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:447)
	- locked <0x00000007a011f750> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:698)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1190)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:85)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:81)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setChildNode(MemoryNodeBuilder.java:346)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeAdded(AbstractRebaseDiff.java:211)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:527)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:531)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$2.childNodeChanged(MapRecord.java:404)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:488)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:394)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:488)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compareBranch(MapRecord.java:565)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:470)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:446)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:471)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:527)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:205)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:426)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:399)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
""main"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.dropCache(SegmentWriter.java:850)
	- waiting to lock <0x00000007a011f750> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.close(FileStore.java:830)
	- locked <0x00000007a0081e48> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT.tearDown(SegmentCompactionIT.java:266)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

Found 1 deadlock.
{noformat}",resilience,['segmentmk'],OAK,Bug,Critical,2015-08-14 14:06:12,15
12855498,Lucene IndexPlanner should also account for number of property constraints evaluated while giving cost estimation,"Currently the cost returned by Lucene index is a function of number of indexed documents present in the index. If the number of indexed entries are high then it might reduce chances of this index getting selected if some property index also support of the property constraint.

{noformat}
/jcr:root/content/freestyle-cms/customers//element(*, cq:Page)
[(jcr:content/@title = 'm' or jcr:like(jcr:content/@title, 'm%')) 
and jcr:content/@sling:resourceType = '/components/page/customer’]
{noformat}

Consider above query with following index definition
* A property index on resourceType
* A Lucene index for cq:Page with properties {{jcr:content/title}}, {{jcr:content/sling:resourceType}} indexed and also path restriction evaluation enabled

Now what the two indexes can help in
# Property index
## Path restriction
## Property restriction on  {{sling:resourceType}}
# Lucene index
## NodeType restriction
## Property restriction on  {{sling:resourceType}}
## Property restriction on  {{title}}
## Path restriction

Now cost estimate currently works like this
* Property index - {{f(indexedValueEstimate, estimateOfNodesUnderGivenPath)}}
** indexedValueEstimate - For 'sling:resourceType=foo' its the approximate count for nodes having that as 'foo'
** estimateOfNodesUnderGivenPath - Its derived from an approximate estimation of nodes present under given path
* Lucene Index - {{f(totalIndexedEntries)}}

As cost of Lucene is too simple it does not reflect the reality. Following 2 changes can be done to make it better
* Given that Lucene index can handle multiple constraints compared (4) to property index (2), the cost estimate returned by it should also reflect this state. This can be done by setting costPerEntry to 1/(no of property restriction evaluated)
* Get the count for queried property value - This is similar to what PropertyIndex does and assumes that Lucene can provide that information in O(1) cost. In case of multiple supported property restriction this can be minima of all",performance,['lucene'],OAK,Improvement,Minor,2015-08-12 09:13:41,4
12855447,Solr test often fail with  No such core: oak,"Often it can be seen that all test from oak-solr module fail. And in all such failure following error is reported 

{noformat}
org.apache.solr.common.SolrException: No such core: oak
	at org.apache.solr.client.solrj.embedded.EmbeddedSolrServer.request(EmbeddedSolrServer.java:112)
	at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:118)
	at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:116)
	at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:102)
	at org.apache.jackrabbit.oak.plugins.index.solr.query.SolrQueryIndexTest.testQueryOnIgnoredExistingProperty(SolrQueryIndexTest.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
{noformat}

Most recent failure in https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/325/",CI,['solr'],OAK,Bug,Minor,2015-08-12 05:14:23,10
12852580,Benchmark for adding group members,along with OAK-3170 and in the light of OAK-3165 i would like to create another benchmark test that adds 1-many members to groups.,benchmark,['run'],OAK,Test,Major,2015-08-06 13:46:49,0
12852473,Consistency checker for data/blob store,"Consistency checker for data/blob store to report any missing blobs which can result as a consequence of erroneous gc or extraneous factors. 
* Will report any missing blob ids
* Path of nodes referring to those blobs
",resilience tooling,['blob'],OAK,Sub-task,Major,2015-08-06 05:21:35,16
12852469,[Blob GC] Improvements/tools for blob garbage collection,Container issue for improvements and reporting tools for the blob garbage collection.,resilience tooling,['blob'],OAK,Improvement,Major,2015-08-06 05:11:57,16
12851653,Deadlock between persisted compaction map and cleanup,"Just seen this deadlock while running {{SegmentCompactionIT}}:

{noformat}
""TarMK flush thread [target/SegmentCompactionIT3250704011919039778dir], active since Wed Aug 05 09:25:57 GMT+00:00 2015, previous max duration 2325ms"" daemon prio=10 tid=0x00007f5674872800 nid=0x5dc8 waiting for monitor entry [0x00007f5666a00000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:145)
	- waiting to lock <0x0000000707fc7fe8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.compress(PersistedCompactionMap.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.remove(PersistedCompactionMap.java:155)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.remove(CompactionMap.java:108)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.cleanup(FileStore.java:694)
	- locked <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:628)
	- locked <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	- locked <0x00000007000aed60> (a java.util.concurrent.atomic.AtomicReference)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$1.run(FileStore.java:413)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.jackrabbit.oak.plugins.segment.file.BackgroundThread.run(BackgroundThread.java:70)

""pool-1-thread-34"" prio=10 tid=0x00007f55ec002800 nid=0x5dea waiting for monitor entry [0x00007f56648de000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:904)
	- waiting to lock <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:210)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	- locked <0x0000000707fc7fe8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:215)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:121)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:103)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1098)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:433)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:406)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",cleanup compaction gc,['segmentmk'],OAK,Bug,Critical,2015-08-05 10:39:15,15
12851413,Compaction slow on repository with continuous writes,"OAK-2734 introduced retry cycles and the option to force compaction when all cycles fail. However OAK-2192 introduced a performance regression: each compaction cycle takes in the order of the size of the repository to complete instead of in the order of the number of remaining changes to compact. This is caused by comparing compacted with pre-compacted node states, which is necessary to avoid mixed segments (aka OAK-2192). To fix the performance regression I propose to pass the compactor an additional node state (the 'onto' state). The diff would then be calculated across the pre compacted states, which performs in the order of number of changes. The changes would then be applied to the 'onto' state, which is a compacted state to avoid mixed segments. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-08-04 14:58:15,15
12851063,Unreleased closed sessions can keep a root reference from getting collected,"It looks like even if a component logs out a session, but keeps a reference to it around this will still prevent GC from running, as the session will wrap a _root_ reference pointing to the moment/revision when the session was last accessed.

Extract from jvisualvm:
{code}
this     - value: org.apache.jackrabbit.oak.plugins.segment.SegmentId #505
 <- [106]     - class: org.apache.jackrabbit.oak.plugins.segment.SegmentId[], value: org.apache.jackrabbit.oak.plugins.segment.SegmentId #505
  <- refids     - class: org.apache.jackrabbit.oak.plugins.segment.Segment, value: org.apache.jackrabbit.oak.plugins.segment.SegmentId[] #67 (120 items)
   <- segment     - class: org.apache.jackrabbit.oak.plugins.segment.SegmentId, value: org.apache.jackrabbit.oak.plugins.segment.Segment #81
    <- [124]     - class: org.apache.jackrabbit.oak.plugins.segment.SegmentId[], value: org.apache.jackrabbit.oak.plugins.segment.SegmentId #496
     <- refids     - class: org.apache.jackrabbit.oak.plugins.segment.Segment, value: org.apache.jackrabbit.oak.plugins.segment.SegmentId[] #17 (204 items)
      <- segment     - class: org.apache.jackrabbit.oak.plugins.segment.SegmentId, value: org.apache.jackrabbit.oak.plugins.segment.Segment #17
       <- segmentId     - class: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState, value: org.apache.jackrabbit.oak.plugins.segment.SegmentId #551
        <- base     - class: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder, value: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState #5557
         <- builder     - class: org.apache.jackrabbit.oak.core.MutableRoot, value: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder #5652
          <- root     - class: org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$1, value: org.apache.jackrabbit.oak.core.MutableRoot #151
           <- sd     - class: com.adobe.granite.repository.impl.CRX3SessionImpl, value: org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$1 #117
{code}",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-08-03 10:19:01,14
12849923,Improve binary comparison during repeated upgrades,"OAK-2619 introduced the possibility to run the same upgrade repeatedly and achieve incremental upgrades. This reduces the time taken for {{CommitHook}} processing, because the diff is reduced.

However, for incremental upgrades to be really useful, the content-copy phase needs to be fast. Currently an optimization that was proposed in OAK-2626 was lost due to the implementation of a better solution to some part of the problem. ",performance,['upgrade'],OAK,Improvement,Major,2015-07-29 12:38:43,2
12849891,Extend documentation for SegmentNodeStoreService in http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore,"Currently the documentation at http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore only documents the properties
# repository.home and
# tarmk.size
All the other properties like customBlobStore, tarmk.mode, .... are not documented. Please extend that. Also it would be good, if the table could be extended with what type is supported for the individual properties.",documentation,"['doc', 'segment-tar']",OAK,Technical task,Major,2015-07-29 10:10:48,12
12849878,IAE when specifiying 2G cache for FileStore,"{{FileStore.newFileStore(dir).withCacheSize(2048)}} results in

{noformat}Max memory must not be negative
java.lang.IllegalArgumentException: Max memory must not be negative
	at org.apache.jackrabbit.oak.cache.CacheLIRS.setMaxMemory(CacheLIRS.java:464)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.<init>(CacheLIRS.java:163)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Builder.build(CacheLIRS.java:1537)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Builder.build(CacheLIRS.java:1533)
	at org.apache.jackrabbit.oak.plugins.segment.StringCache.<init>(StringCache.java:52)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.<init>(SegmentTracker.java:126)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:343)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:84)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$Builder.create(FileStore.java:294)
{noformat}

There is an integer overflow cause by using ints instead of longs to specify the cache size.

[~tmueller], could you have a look?",resilience,['segmentmk'],OAK,Bug,Minor,2015-07-29 09:37:11,4
12849650,Make it possible to disable recording of stack trace in SessionStats,"For the rendering of some pages we have to create a lot of sessions. Around 9% of the rendering time is spent inside of RepositoryImpl.login. Half of this time is spent creating the exception in SessionStats. Therefore, it would be useful if the recording of the exception could be disabled to improve the performance.",performance,['core'],OAK,Improvement,Major,2015-07-28 15:32:47,4
12849611,Lucene Version should be based on IndexFormatVersion,"Currently in oak-lucene where ever call is made to Lucene it passes Version.LUCENE_47 as hardcoded version. To enable easier upgrade of Lucene and hence change of defaults for fresh setup this version should be instead based on {{IndexFormatVersion}}.

Say
* For IndexFormatVersion set to V2 (current default) - Lucene version used is LUCENE_47
* For IndexFormatVersion set to V3 (proposed) - Lucene version used would be per Lucene library version

If the index is reindexed then it would automatically be updated to the latest revision",technical_debt,['lucene'],OAK,Sub-task,Major,2015-07-28 13:20:30,19
12849608,Update Lucene to 6.x series,"We should look into updating the Lucene version to 6.x. Java 8 is the minimum Java version required

Note this is to be done for trunk only",technical_debt,['lucene'],OAK,Improvement,Major,2015-07-28 13:13:17,10
12848549,"DataStore / BlobStore: add a method to pass a ""type"" when writing","Currently, the BlobStore interface has a method ""String writeBlob(InputStream in)"". This issue is about adding a new method ""String writeBlob(String type, InputStream in)"", for the following reasons (in no particular order):

* Store some binaries (for example Lucene index files) in a different place, in order to safely and quickly run garbage collection just on those files.

* Store some binaries in a slow, some in a fast storage or location.

* Disable calculating the content hash (de-duplication) for some binaries.

* Store some binaries in a shared storage (for fast cross-repository copying), and some in local storage.",performance,['blob'],OAK,New Feature,Major,2015-07-23 13:02:19,4
12848545,SNFE in persisted comapation map when using CLEAN_OLD,"When using {{CLEAN_OLD}} it might happen that segments of the persisted compaction map get collected. --The reason for this is that only the segment containing the root of the map is pinned ({{SegmentId#pin}}), leaving other segments of the compaction map eligible for collection once old enough.--

{noformat}
org.apache.jackrabbit.oak.plugins.segment.SegmentNotFoundException: Segment 95cbb3e2-3a8c-4976-ae5b-6322ff102731 not found
        at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:919)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:134)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:108)
        at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
        at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
        at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
        at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:118)
        at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:100)
        at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1023)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1033)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
{noformat}",compaction gc,['segmentmk'],OAK,Bug,Critical,2015-07-23 12:50:01,15
12846785,Make compaction map more efficient for offline compaction,"The compaction map might be expensive. See OAK-2862 for the analysis. We should find ways to lower the impact of this on offline compaction. One option would be to make the compress cycle configurable, which would require more heap. Another option would be to leverage the persisted compaction map here also.",compaction gc,['run'],OAK,Improvement,Major,2015-07-22 08:04:20,14
12846450,Skip compaction estimation if threshold is 0,"as noted on OAK-3052, it would be good to have a way to skip the estimation if needed  and this can be easily achieved with the config we already in place for the threshold.",compaction gc,"['core', 'segmentmk']",OAK,Improvement,Minor,2015-07-21 07:41:21,14
12845515,Performance degradation of UnsavedModifications on MapDB,"UnsavedModifications performance degrades when used in combination with the MapDB backed MapFactory. Calls become more and more expensive the longer the instance is in use. The is caused by a limitation of MapDB, which does not remove empty BTree nodes.

A test performed with random paths added to the map and later removed again in a loop shows a increase to roughly 1 second to read keys present in the map when the underlying data file is about 50MB in size.",performance,"['core', 'mongomk']",OAK,Bug,Major,2015-07-16 08:30:51,2
12845482,AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic,"At times the CopyOnWrite reports following exception

{noformat}
15.07.2015 14:20:35.930 *WARN* [pool-58-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed
org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204)
	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219)
	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311)
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105)
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: _2s7.fdt
	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory$COWLocalFileReference.fileLength(IndexCopier.java:837)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory.fileLength(IndexCopier.java:607)
	at org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141)
	at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529)
	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502)
	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508)
	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202)
	... 10 common frames omitted
{noformat}",resilience,['lucene'],OAK,Bug,Major,2015-07-16 04:38:56,19
12844356,Cache recently extracted text to avoid duplicate extraction,"It can happen that text can be extracted from same binary multiple times in a given indexing cycle. This can happen due to 2 reasons

# Multiple Lucene indexes indexing same node - A system might have multiple Lucene indexes e.g. a global Lucene index and an index for specific nodeType. In a given indexing cycle same file would be picked up by both index definition and both would extract same text
# Aggregation - With Index time aggregation same file get picked up multiple times due to aggregation rules

To avoid the wasted effort for duplicate text extraction from same file in a given indexing cycle it would be better to have an expiring cache which can hold on to extracted text content for some time. The cache should have following features
# Limit on total size
# Way to expire the content using [Timed Evicition|https://code.google.com/p/guava-libraries/wiki/CachesExplained#Timed_Eviction] - As chances of same file getting picked up are high only for a given indexing cycle it would be better to expire the cache entries after some time to avoid hogging memory unnecessarily 

Such a cache would provide following benefit
# Avoid duplicate text extraction - Text extraction is costly and has to be minimized on critical path of {{indexEditor}}
# Avoid expensive IO specially if binary content are to be fetched from a remote {{BlobStore}}",performance,['lucene'],OAK,Improvement,Major,2015-07-11 07:54:53,19
12844158,Caching BlobStore implementation ,"Storing binaries in Mongo puts lots of pressure on the MongoDB for reads. To reduce the read load it would be useful to have a filesystem based cache of frequently used binaries. 

This would be similar to CachingFDS (OAK-3005) but would be implemented on top of BlobStore API. 

Requirements
* Specify the max binary size which can be cached on file system
* Limit the size of all binary content present in the cache
",performance resilience,['blob'],OAK,New Feature,Major,2015-07-10 12:09:14,16
12843395,JournalTest.cleanupTest failure on CI,"Following test failure was seen [here|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/248/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=unittesting/testReport/junit/org.apache.jackrabbit.oak.plugins.document/JournalTest/cleanupTest/]

{noformat}
java.lang.AssertionError: expected:<0> but was:<1>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.jackrabbit.oak.plugins.document.JournalTest.cleanupTest(JournalTest.java:183)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}",CI Jenkins,['mongomk'],OAK,Bug,Minor,2015-07-08 04:11:15,21
12843210,LastRevRecoveryAgent can update _lastRev of children but not the root,"As mentioned in [OAK-2131|https://issues.apache.org/jira/browse/OAK-2131?focusedCommentId=14616391&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14616391] there can be a situation wherein the LastRevRecoveryAgent updates some nodes in the tree but not the root. This seems to happen due to OAK-2131's change in the Commit.applyToCache (where paths to update are collected via tracker.track): in that code, paths which are non-root and for which no content has changed (and mind you, a content change includes adding _deleted, which happens by default for nodes with children) are not 'tracked', ie for those the _lastRev is not update by subsequent backgroundUpdate operations - leaving them 'old/out-of-date'. This seems correct as per description/intention of OAK-2131 where the last revision can be determined via the commitRoot of the parent. But it has the effect that the LastRevRecoveryAgent then finds those intermittent nodes to be updated while as the root has already been updated (which is at first glance non-intuitive).

I'll attach a test case to reproduce this.

Perhaps this is a bug, perhaps it's ok. [~mreutegg] wdyt?",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-07-07 15:01:49,2
12842864,Compaction should trace log the current processed path,"A lot of SegmentNotFoundExceptions happen during compaction (offline or online) and these are repeatable exceptions. I would like to add a 'trace' log to make it easy to follow up on the progress and get an indication of where the SNFE is happening, without relying on offline intervention (via oak-run).",gc,"['core', 'segmentmk']",OAK,Improvement,Major,2015-07-06 11:38:58,14
12842854,Compaction Estimation should type check binary properties,"It looks like the compaction estimation mechanism doesn't currently verify the type of the property it tries to load as binary, it will assume that the following call _property.getValue(BINARIES)_  will return nothing if the type is not BINARY. This behavior has changed since this code was written, the way it works now is the method will try to convert to BINARY the property's value.
I'm assuming OAK-3007 is a side-effects of this change.",gc,"['core', 'segmentmk']",OAK,Bug,Major,2015-07-06 10:36:52,14
12842853,FileStore.size doesn't throw IOException but declares it as thrown,I suggest to remove the throws clause and fix the affected clients. ,technical_debt,['segmentmk'],OAK,Improvement,Minor,2015-07-06 10:34:13,15
12842397,Add a compound index for _modified + _id,"As explained in OAK-1966 diff logic makes a call like

bq. db.nodes.find({ _id: { $gt: ""3:/content/foo/01/"", $lt: ""3:/content/foo010"" }, _modified: { $gte: 1405085300 } }).sort({_id:1})

For better and deterministic query performance we would need to create a compound index like \{_modified:1, _id:1\}. This index would ensure that Mongo does not have to perform object scan while evaluating such a query.

Care must be taken that index is only created by default for fresh setup. For existing setup we should expose a JMX operation which can be invoked by system admin to create the required index as per maintenance window",performance resilience,['mongomk'],OAK,Improvement,Blocker,2015-07-02 18:08:38,2
12842389,Use a lower bound in VersionGC query to avoid checking unmodified once deleted docs,"As part of OAK-3062 [~mreutegg] suggested

{quote}
As a further optimization we could also limit the lower bound of the _modified
range. The revision GC does not need to check documents with a _deletedOnce
again if they were not modified after the last successful GC run. If they
didn't change and were considered existing during the last run, then they
must still exist in the current GC run. To make this work, we'd need to
track the last successful revision GC run. 
{quote}

Lowest last validated _modified can be possibly saved in settings collection and reused for next run",performance,"['mongomk', 'rdbmk']",OAK,Improvement,Major,2015-07-02 17:58:34,2
12842257,Persistent cache for previous documents,Previous (aka split) documents contain old revisions and are immutable documents. Those documents should go into the persistent cache to reduce calls to the underlying DocumentStore.,performance,"['core', 'mongomk']",OAK,Improvement,Blocker,2015-07-02 08:55:08,11
12842186,RemoteServerIT failing due to address already in use,"Some of RemoteServerIT failing with Address already in use. Possibly the test setup needs to be changed to use random available port 

{noformat}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:291)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServer.start(RemoteServer.java:54)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.setUp(RemoteServerIT.java:134)
{noformat}

[1] https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/236/",CI Jenkins,['remoting'],OAK,Bug,Minor,2015-07-02 04:29:16,12
12841717,Improve segment cache in SegmentTracker,"The hand crafted segment cache in {{SegmentTracker}} is prone to lock contentions in concurrent access scenarios. As {{SegmentNodeStore#merge}} might also end up acquiring this lock while holding the commit semaphore the situation can easily lead to many threads being blocked on the commit semaphore. The {{SegmentTracker}} cache doesn't differentiate between read and write access, which means that reader threads can block writer threads. ",doc-impacting resilience scalability,['segmentmk'],OAK,Improvement,Major,2015-06-30 15:16:32,15
12841676,IndexStatsMBean should provide some details if the async indexing is failing,"If the background indexing fails for some reason it logs the exception for the first time then it logs the exception like _The index update failed ..._. After that if indexing continues to fail then no further logging is done so as to avoid creating noise.

This poses a problem on long running system where original exception might not be noticed and index does not show updated result. For such cases we should expose the indexing health as part of {{IndexStatsMBean}}. Also we can provide the last recorded exception. 

Administrator can then check for MBean and enable debug logs for further troubleshooting",resilience tooling,['query'],OAK,Improvement,Major,2015-06-30 12:00:00,14
12841626,Make compaction gain estimate threshold configurable,Currently compaction is skipped if the compaction gain estimator determines that less than 10% of space could be reclaimed. Instead of relying on a hard coded value of 10% we should make this configurable. ,compaction gc,['segmentmk'],OAK,New Feature,Minor,2015-06-30 09:17:12,15
12841623,Improve compaction gain estimation logging for the case where there are no tar readers,"When compaction is started on a new and sufficiently small repository such that there is yet no tar reader the compaction gain estimator logs the somewhat confusing message {{gain is 0% (0/0) or (0 B/0 B)}}. 

We should improve the logging for this case.",compaction gc,['segmentmk'],OAK,Improvement,Minor,2015-06-30 09:13:31,15
12841614,Move oak-it-osgi to top level ,"The {{oak-it}} module only contains the single {{oak-it-osgi}} module, which in turn consists of a single test class. 

I suggest to at least move {{oak-it-osgi}} to the top level reactor to be consistent with the flat module hierarchy we adopted. IMO an even better solution would be to move the single test class to {{oak-run}}. Not sure whether this is viable though due to the custom assembly necessary. ",modularization technical_debt,['it'],OAK,Improvement,Minor,2015-06-30 08:53:06,15
12841567,RemoteServerIT test are failing on the CI server,"Most of the test in {{RemoteServerIT}} at times fail on the CI server [1] with following exception

{noformat}
Error Message

/home/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/latest1.7/label/Ubuntu/nsfixtures/SEGMENT_MK/profile/unittesting/oak-remote/target/test-classes/org/apache/jackrabbit/oak/remote/http/handler/addNodeMultiPathProperty.json (No such file or directory)
Stacktrace

java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/latest1.7/label/Ubuntu/nsfixtures/SEGMENT_MK/profile/unittesting/oak-remote/target/test-classes/org/apache/jackrabbit/oak/remote/http/handler/addNodeMultiPathProperty.json (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:127)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:117)
	at com.google.common.io.ByteSource$AsCharSource.openStream(ByteSource.java:404)
	at com.google.common.io.CharSource.read(CharSource.java:155)
	at com.google.common.io.Files.toString(Files.java:391)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.load(RemoteServerIT.java:119)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.testPatchLastRevisionAddMultiPathProperty(RemoteServerIT.java:1199)
{noformat}

[1] https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/232/testReport/",CI Jenkins,['remoting'],OAK,Bug,Minor,2015-06-30 05:03:47,19
12841279,Suspend commit on conflict,"A DocumentNodeStore cluster currently shows a conflict behavior, which
is not intuitive. A modification may fail with a conflict even though
before and after the conflict, the external change is not visible to
the current session. There are two aspects to this issue.

1) a modification may conflict with a change done on another cluster
node, which is committed but not yet visible on the current cluster node.

2) even after the InvalidItemStateException caused by the conflict, a
refreshed session may still not see the external change.

The first aspect is a fundamental design decision and cannot be changed
easily.

The second part can be addressed by suspending the commit until the external
conflict becomes visible on the current cluster node. This would at least
avoid the awkward situation where the external change is not visible after
the InvalidItemStateException.

The system would also become more deterministic. A commit currently goes
into a number of retries with exponential back off, but there's no guarantee
the external modification becomes visible within those retries. 
",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-06-29 09:18:49,2
12840750,DocumentRootBuilder: revisit update.limit default,"update.limit decides whether a commit is persisted using a branch or not. The default is 10000 (and can be overridden using the system property).

A typical call pattern in JCR is to persist batches of ~1024 nodes. These translate to more than 10000 changes (see PackageImportIT), due to JCR properties, and also indexing commit hooks.

",candidate_oak_1_4 performance,"['mongomk', 'rdbmk']",OAK,Improvement,Blocker,2015-06-26 06:43:59,19
12840562,ReferenceEditor reindex detection broken,"This is a fix for OAK-2927 which has a broken reindex detection mechanism. The issue is closed so I cannot reopen it and I'm creating a new one for the fix.

The main issue is that _root_ will never be empty, the check needs to be on the #enter method against the _before_ node",resilience,['core'],OAK,Bug,Major,2015-06-25 14:52:35,14
12840148,[Blob GC] Mbean for reporting shared repository GC stats,"For GC on a shared repository (OAK-1849) it is beneficial to add a JMX Mbean which can provide visibility on the state of GC. It could possibly show:
* Various repositories registered in the DataStore
* State of the blob reference collection for the registered repositories
* Time of the reference files for each registered repository
* Time interval for the earliest and the latest reference file of the registered repositories. This could be used to possibly automate the sweep phase if the time interval is less than a configured value.
",resilience tooling,['blob'],OAK,Sub-task,Major,2015-06-24 09:47:10,16
12839799,Long running MongoDB query may block other threads,Most queries on MongoDB are usually rather fast and the TreeLock acquired in MongoDocumentStore (to ensure cache consistency) is released rather quickly. However there may be cases when a query is more expensive and a TreeLock is held for a long time. This may block other threads from querying MongoDB and limit concurrency.,concurrency,[],OAK,Bug,Major,2015-06-23 09:38:48,2
12839549,Use batch-update in backgroundWrite,"(From an earlier [post on the list|http://markmail.org/thread/mkrvhkfabit4osli]) The DocumentNodeStore.backgroundWrite goes through the heavy work of updating the lastRev for all pending changes and does so in a hierarchical-depth-first manner. Unfortunately, if the pending changes all come from separate commits (as does not sound so unlikely), the updates are sent in individual update calls to mongo (whenever the lastRev differs). Which, if there are many changes, results in many calls to mongo.

OAK-2066 is about extending the DocumentStore API with a batch-update method. That one, once available, should thus be used in the {{backgroundWrite}} as well.",performance,"['core', 'documentmk']",OAK,Improvement,Major,2015-06-22 13:23:29,2
12838792,"SegmentStore cache does not take ""string"" map into account","The SegmentStore cache size calculation ignores the size of the field Segment.string (a concurrent hash map). It looks like a regular segment in a memory mapped file has the size 1024, no matter how many strings are loaded in memory. This can lead to out of memory. There seems to be no way to limit (configure) the amount of memory used by strings. In one example, 100'000 segments are loaded in memory, and 5 GB are used for Strings in that map.

We need a way to configure the amount of memory used for that. This seems to be basically a cache. OAK-2688 does this, but it would be better to have one cache with a configurable size limit.",doc-impacting resilience scalability,['segmentmk'],OAK,Bug,Major,2015-06-18 13:40:24,15
12838403,Improve login performance with huge group membership,"As visible when running {{LoginWithMembershipTest}} default login performance (which uses the {{o.a.j.oak.security.principal.PrincipalProviderImpl}} to lookup the complete set of principals) suffers when a given user is member of a huge number of groups (see also OAK-2690 for benchmark data).

While using dynamic group membership (and thus a custom {{PrincipalProvider}} would be the preferable way to deal with this, we still want to optimize {{PrincipalProvider.getPrincipals(String userId}} for the default implementation.

With the introduction of a less generic implementation in OAK-2690, we might be able to come up with an optimization that makes use of the very implementation details of the user management while at the same time being able to properly secure it as we won't need to extend the public API.
",performance,['core'],OAK,Improvement,Major,2015-06-17 07:46:38,0
12838394,Optimize docCache and docChildrenCache invalidation by filtering using journal,"This subtask is about spawning out a [comment|https://issues.apache.org/jira/browse/OAK-2829?focusedCommentId=14588114&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14588114] on OAK-2829 re optimizing docCache invalidation using the newly introduced external diff journal:

{quote}
Attached OAK-2829-improved-doc-cache-invaliation.patch which is a suggestion on how to avoid invalidating the entire document cache when doing a {{backgroundRead}} but instead making use of the new journal: ie only invalidate from the document cache what has actually changed.

I'd like to get an opinion ([~mreutegg], [~chetanm]?) on this first, I have a load test pending locally which found invalidation of the document cache to be the slowest part thus wanted to optimize this first.

Open still/next:
 * also invalidate only necessary parts from the docChildrenCache
 * junits for all of these
{quote}",scalability,"['core', 'mongomk']",OAK,Sub-task,Major,2015-06-17 07:00:23,2
12838393,Simplify JournalGarbageCollector using a dedicated timestamp property,"This subtask is about spawning out a [comment|https://issues.apache.org/jira/browse/OAK-2829?focusedCommentId=14585733&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14585733] from [~chetanm] re JournalGC:
{quote}
Further looking at JournalGarbageCollector ... it would be simpler if you record the journal entry timestamp as an attribute in JournalEntry document and then you can delete all the entries which are older than some time by a simple query. This would avoid fetching all the entries to be deleted on the Oak side
{quote}
and a corresponding [reply|https://issues.apache.org/jira/browse/OAK-2829?focusedCommentId=14585870&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14585870] from myself:
{quote}
Re querying by timestamp: that would indeed be simpler. With the current set of DocumentStore API however, I believe this is not possible. But: [DocumentStore.query|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/DocumentStore.java#L127] comes quite close: it would probably just require the opposite of that method too: 
{code}
    public <T extends Document> List<T> query(Collection<T> collection,
                                              String fromKey,
                                              String toKey,
                                              String indexedProperty,
                                              long endValue,
                                              int limit) {
{code}
.. or what about generalizing this method to have both a {{startValue}} and an {{endValue}} - with {{-1}} indicating when one of them is not used?
{quote}",scalability,"['core', 'mongomk']",OAK,Improvement,Critical,2015-06-17 06:56:55,11
12837604,RDB: switch to tomcat datasource implementation ,"See <https://people.apache.org/~fhanik/jdbc-pool/jdbc-pool.html>.

In addition, this is the datasource used in Sling's datasource service, so it's closer to what people will use in practice.",resilience,['rdbmk'],OAK,Sub-task,Major,2015-06-13 16:27:58,3
12837067,Fast result size estimate: OSGi configuration,"The fast result size option in OAK-2926 should be configurable, for example over OSGi.

The resultFetchSize should be configurable as well. Currently it is hardcoded (PREFETCH_MAX = 100).",doc-impacting,['query'],OAK,Improvement,Major,2015-06-11 07:43:18,4
12836776,Broken link on documentation site,"http://jackrabbit.apache.org/oak/docs/command_line.html points to http://jackrabbit.apache.org/oak/docs/oak-mongo-js/oak.html, which doesn't exit. ",documentation,['doc'],OAK,Bug,Minor,2015-06-10 09:18:44,2
12836742,DocumentNodeStore gets initialized multiple time with RDB persistence,"Depending on startup sequence it can happen that {{DocumentNodeStore}} gets initialized multiple times. 

So far with Mongo {{DocumentNodeStoreService}}  was only dependent on one external reference of {{BlobStore}}. However with RDB there are two more external reference for {{DataSource}} one for nodes and other for blobs. 

SCR (Felix Service Component Runtime) would invoke both {{bindDataSource}} and {{bindBlobDataSource}} and currently there is no check to avoid re initialization. ",doc-impacting osgi,['rdbmk'],OAK,Bug,Major,2015-06-10 06:16:51,3
12836065,[Blob GC]: Undeleted blobs also being logged in deleted count,The logged number of blobs being deleted also includes blobs which could not be deleted because their last modification timestamp was recent than the configured max age to be deleted,resilience,['blob'],OAK,Bug,Major,2015-06-08 05:52:42,16
12835667,Async index fails with OakState0001: Unresolved conflicts in /:async,"When running on a mongo cluster, in case of high activity the async index could fail with the following exception.

Reproduced with LucenePropertyIndex.

{noformat}
10:55:13.069 [oak-scheduled-executor-5] WARN  o.a.j.o.p.index.AsyncIndexUpdate - [async] The index update failed
org.apache.jackrabbit.oak.api.CommitFailedException: OakState0001: Unresolved conflicts in /:async
	at org.apache.jackrabbit.oak.plugins.commit.ConflictValidator.failOnMergeConflict(ConflictValidator.java:84) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.commit.ConflictValidator.propertyAdded(ConflictValidator.java:54) ~[classes/:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.propertyAdded(EditorDiff.java:82) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:378) ~[classes/:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(EditorDiff.java:148) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400) ~[classes/:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[classes/:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:55) ~[classes/:na]
	at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:61) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:528) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:219) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:171) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:158) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1466) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.mergeWithConcurrencyCheck(AsyncIndexUpdate.java:467) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.access$4(AsyncIndexUpdate.java:445) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate$AsyncUpdateCallback.<init>(AsyncIndexUpdate.java:201) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:372) ~[classes/:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:320) ~[classes/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_60]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) [na:1.7.0_60]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_60]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.7.0_60]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_60]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
{noformat}",resilience,"['mongomk', 'query']",OAK,Bug,Major,2015-06-05 10:02:46,22
12834989,Add MBean to enforce session refresh on all open sessions,"Long running sessions limit the efficient of revision gc as they keep reference to old node states. 

We should consider to add an MBean through which all sessions can be enforced to refresh. This would provide clients with means to fine tune revision gc by first calling that MBean and then triggering a gc cycle. IMO this is preferable to directly enforcing a refresh from the garbage collector. The latter is too invasive and also not required when there are no long running sessions. Offering this functionality to clients as an additional knob to turn is safer. ",compaction doc-impacting gc,"['core', 'jcr']",OAK,New Feature,Major,2015-06-03 10:06:37,15
12834948,RDBConnectionHandler: log failures on setReadOnly() only once,"It appears that WAS wraps Oracle JDBC connection objects and throws upon setReadOnly():
{noformat}
java.sql.SQLException: DSRA9010E: 'setReadOnly' is not supported on the WebSphere java.sql.Connection implementation.
at com.ibm.ws.rsadapter.spi.InternalOracleDataStoreHelper.setReadOnly(InternalOracleDataStoreHelper.java:369)
at com.ibm.ws.rsadapter.jdbc.WSJdbcConnection.setReadOnly(WSJdbcConnection.java:3626)
at org.apache.jackrabbit.oak.plugins.document.rdb.RDBConnectionHandler.getROConnection(RDBConnectionHandler.java:61)
{noformat}

...which of course is a bug in WAS (setReadOnly() is documented as a hint, the implementation is not supposed to throw an exception here); see also <http://www-01.ibm.com/support/docview.wss?uid=swg1PM58588>",resilience,['rdbmk'],OAK,Sub-task,Major,2015-06-03 06:46:39,3
12834514,Sampling rate feature CompactionGainEstimate is not efficient,"The sampling rate feature introduced with OAK-2595 is not efficient. It only prevents uuids from being stored in the bloom filter while the visited set is not affected and thus keeps growing. 

I will remove the feature again for now. We should look for a better solution once this becomes a problem. Will follow up on OAK-2939 re. this. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-06-02 08:07:48,15
12834513,Sampling rate feature CompactionGainEstimate is not efficient,"The sampling rate feature introduced with OAK-2595 is not efficient. It only prevents uuids from being stored in the bloom filter while the visited set is not affected and thus keeps growing. 

I will remove the feature again for now. We should look for a better solution once this becomes a problem. Will follow up on OAK-2939 re. this. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-06-02 08:06:25,15
12834487,Support merge iterator for union order by queries,"Currently, any order by union queries (including optimized OR XPATH) scan a much larger set when returning the results even when the individual queries are sorted by the index itself. 
We should have a merge iterator which would scan a much smaller set as the individual queries would be sorted.",perfomance,['query'],OAK,Sub-task,Major,2015-06-02 06:16:05,16
12834195,Estimation of required memory for compaction is off,"Currently compaction will be skipped if some rough estimation determines that there is not  enough memory to run. That estimation however assumes that each compaction cycle requires as much space as the compaction map already takes up. This is too conservative. Instead the amount of memory taken up by the last compaction cycle should be a better estimate. 

",compaction gc,['segmentmk'],OAK,Bug,Major,2015-06-01 09:27:08,15
12834183,Remove code related to directmemory for off heap caching,"DocumentNodeStore has some code related to off heap which makes use of Apache Directmemory (OAK-891). This feature was not much used and PersistentCache made this feature obsolete.

Recently it was mentioned on Directmemory that there is not much activity going on [1] in that project and it might be referred to attic. In light of that we should remove this feature from Oak

[1] http://markmail.org/thread/atia2ecaa2mugmjx",technical_debt,['mongomk'],OAK,Task,Major,2015-06-01 08:37:19,19
12833968,Certain searches cause lucene index to hit OutOfMemoryError,"Certain search terms can get split into very small wildcard tokens that will match a huge amount of items from the index, finally resulting in a OOME.

For example
{code}
/jcr:root//*[jcr:contains(., 'U=1*')]
{code}
will translate into the following lucene query
{code}
:fulltext:""u ( [set of all index terms stating with '1'] )""
{code}
this will break down when lucene will try to compute the score for the huge set of tokens:
{code}
java.lang.OutOfMemoryError: Java heap space
        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.<init>(OakDirectory.java:201)
        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.<init>(OakDirectory.java:155)
        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.<init>(OakDirectory.java:340)
        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.clone(OakDirectory.java:345)
        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.clone(OakDirectory.java:329)
        at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader$BlockDocsAndPositionsEnum.<init>(Lucene41PostingsReader.java:613)
        at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader.docsAndPositions(Lucene41PostingsReader.java:252)
        at org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.docsAndPositions(BlockTreeTermsReader.java:2233)
        at org.apache.lucene.search.UnionDocsAndPositionsEnum.<init>(MultiPhraseQuery.java:492)
        at org.apache.lucene.search.MultiPhraseQuery$MultiPhraseWeight.scorer(MultiPhraseQuery.java:205)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndex$1.loadDocs(LuceneIndex.java:352)
        at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndex$1.computeNext(LuceneIndex.java:289)
        at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndex$1.computeNext(LuceneIndex.java:280)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndex$LucenePathCursor$1.hasNext(LuceneIndex.java:1026)
        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:645)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.jackrabbit.oak.spi.query.Cursors$PathCursor.hasNext(Cursors.java:198)
        at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndex$LucenePathCursor.hasNext(LuceneIndex.java:1047)
        at org.apache.jackrabbit.oak.plugins.index.aggregate.AggregationCursor.fetchNext(AggregationCursor.java:88)
        at org.apache.jackrabbit.oak.plugins.index.aggregate.AggregationCursor.hasNext(AggregationCursor.java:75)
        at org.apache.jackrabbit.oak.spi.query.Cursors$ConcatCursor.fetchNext(Cursors.java:474)
        at org.apache.jackrabbit.oak.spi.query.Cursors$ConcatCursor.hasNext(Cursors.java:466)
        at org.apache.jackrabbit.oak.spi.query.Cursors$ConcatCursor.fetchNext(Cursors.java:474)
        at org.apache.jackrabbit.oak.spi.query.Cursors$ConcatCursor.hasNext(Cursors.java:466)
{code}
",resilience,['lucene'],OAK,Bug,Blocker,2015-05-30 09:39:08,14
12833739,Limit the scope of exported packages,"Oak currently exports *a lot* of packages even though those are only used by Oak itself. We should probably leverage OSGi subsystems here and only export the bare minimum to the outside world. This will simplify evolution of Oak internal APIs as with the current approach changes to such APIs always leak to the outside world. 

That is, we should have an Oak OSGi sub-system as an deployment option. Clients would then only need to deploy that into their OSGi container and would only see APIs actually meant to be exported for everyone (like e.g. the JCR API). At the same time Oak could go on leveraging OSGi inside this subsystem.

cc [~bosschaert] as you introduced us to this idea. ",modularization osgi technical_debt,[],OAK,Sub-task,Major,2015-05-29 10:20:53,12
12833694,RDBDocumentStore: mitigate effects of large query result sets,"With the DocumentStore query API, large result sets can happen; and these are returned as List<Document>, potentially causing large amounts of memory to be allocated.

In the current implementation, the result list is generated based on a list of internal row presentations (RDBRow). These are currently freed when the method finishes. They should be freed as early as possible.

Furthermore, when the result set gets big, RDBDocumentStore should log an error containing the call chain, so that the component doing the excessive query can be identified (it should use paging instead).

(For completeness: we could also change the code to lazily populate the list; but that would be a bigger change)
",resilience,['rdbmk'],OAK,Sub-task,Major,2015-05-29 06:58:18,3
12833670,Parent of unseen children must not be removable,"With OAK-2673, it's now possible to have hidden intermediate nodes created concurrently.
So, a scenario like:
{noformat}
start -> /:hidden
N1 creates /:hiddent/parent/node1
N2 creates /:hidden/parent/node2
{noformat}
is allowed.

But, if N2's creation of {{parent}} got persisted later than that on N1, then N2 is currently able to delete {{parent}} even though there's {{node1}}.",concurrency technical_debt,"['core', 'mongomk']",OAK,Bug,Minor,2015-05-29 04:59:11,2
12833421,ReferenceEditor newIds consuming lots of memory during migration,"In a large migration its seen that {{ReferenceEditor}} {{newIds}} can consume lots of memory as it records all the uuid property. This system has 33 million uuid index and the set was consuming ~1.5G of memory

We should look into ways such that it does not have to maintain such a big in memory state",resilience,['core'],OAK,Improvement,Minor,2015-05-28 13:35:03,19
12833348,Fast result size estimate,"When asking for the correct result size of a query, the complete result needs to be read, so that access rights checks are made, and (unless the index is known to be up-to-date, and can process all conditions) so that the existence and all query conditions are checked.

Jackrabbit 2.x supports a fast way to get an estimate of the result size, without doing access rights checks. See also JCR-3858.

Please note that according to the JCR API, NodeIterator.getSize() may return -1 (for ""unknown""), and in Oak this is currently done if counting is slow. This would also need to be disabled if a fast result size estimate is needed.
",performance,['query'],OAK,New Feature,Major,2015-05-28 08:42:27,4
12833338,DocumentNodeStore background update thread handling of persistence exceptions,"Seen in a log file:

{noformat}
27.05.2015 11:34:48.130 *WARN* [DocumentNodeStore background update thread] org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore Background operation failed: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: com.ibm.db2.jcc.am.SqlTransactionRollbackException: DB2 SQL Error: SQLCODE=-911, SQLSTATE=40001, SQLERRMC=68, DRIVER=3.65.77
org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: com.ibm.db2.jcc.am.SqlTransactionRollbackException: DB2 SQL Error: SQLCODE=-911, SQLSTATE=40001, SQLERRMC=68, DRIVER=3.65.77
{noformat}

We need to decide whether these are harmless in that the operation will be repeated anyway. If the answer is yes, we may want to tune the log message. If the answer is no, we need to dig deeper.

[~mreutegg] wdyt?",resilience,"['mongomk', 'rdbmk']",OAK,Improvement,Minor,2015-05-28 07:40:06,2
12833327,Scalability tests for large read/write scenarios,Create scalability tests to test out the performance of the read/writes for RDB DocumentStore on a large repository.,scalability,['run'],OAK,New Feature,Major,2015-05-28 07:17:19,16
12833029,Clear the modified and deleted map in PermissionHook after processing is complete,"{{PermissionHook}} has in memory state in {{modified}} and {{deleted}} maps. In case of repository migration which is implemented as a large commit this can consume quite a bit of memory. In one of the migration it was taking ~1 GB of memory.

In a commit involving multiple commit hooks once {{PermissionHook}} has done the work it can clear that state so that memory is not held up untill all the hooks are applied. Specially as IndexingHook takes long time and also has some memory requirements",resilience,['security'],OAK,Improvement,Minor,2015-05-27 12:38:14,0
12832953,oak-jcr bundle should be usable as a standalone bundle,Currently oak-jcr bundle needs to be embedded within some other bundle if the Oak needs to be properly configured in OSGi env. Need to revisit this aspect and see what needs to be done to enable Oak to be properly configured without requiring the oak-jcr bundle to be embedded in the repo,modularization osgi technical_debt,['jcr'],OAK,Improvement,Major,2015-05-27 04:44:07,19
12832746,Review and improve Oak and Jcr repository setup,"There is the {{Oak}} and {{Jcr}} builder classes for setting up Oak and Jcr repositories. Both builders don't have clear semantics regarding the life cycle of the individual components they register. On top of that the requirements regarding those life cycles differ depending on whether the individual components run within an OSGi container or not. In the former case the container would already manage the life cycle so the builder should not. 

IMO we should specify the builders to only be used for non OSGi deployments and have the manage the life cycles of the components they instantiate. OTOH for OSGi deployments we should leverage OSGi subsystems to properly set things up.",modularization technical_debt,"['core', 'jcr']",OAK,Improvement,Major,2015-05-26 15:26:53,12
12832736,Move DocumentMK to test,"The DocumentMK class is not directly used (when using the JCR API), but it is only really used by tests. So it should be moved to tests.

The DocumentMK.Builder class needs to be moved first (to a top level class for example).",technical_debt,['documentmk'],OAK,Improvement,Major,2015-05-26 15:00:08,2
12832721,test failures for oak-auth-ldap on Windows,"testAuthenticateValidateTrueFalse(org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest)  Time elapsed: 0.01 sec  <<< ERROR!
java.io.IOException: Unable to delete file: target\apacheds\cache\5c3940f5-2ddb-4d47-8254-8b2266c1a684\ou=system.data
        at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2279)
        at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
        at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
        at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
        at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
        at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
        at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
        at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
        at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
        at org.apache.jackrabbit.oak.security.authentication.ldap.AbstractServer.doDelete(AbstractServer.java:264)
        at org.apache.jackrabbit.oak.security.authentication.ldap.AbstractServer.setUp(AbstractServer.java:183)
        at org.apache.jackrabbit.oak.security.authentication.ldap.InternalLdapServer.setUp(InternalLdapServer.java:33)


etc...",test,['auth-ldap'],OAK,Bug,Major,2015-05-26 13:46:37,3
12832683,Code coverage,"We should have automated code coverage results, and then decide upon minimum numbers we want to achieve (for example, initially 100% package or class coverage). Once we reached the goal, we can increase the minimum coverage on a module-by-module basis.",technical_debt,[],OAK,Improvement,Major,2015-05-26 10:05:16,4
12831690,Update to Jackrabbit 2.10.1,"OAK-2748 introduced a snapshot dependency to Jackrabbit 2.10.1-SNAPSHOT. Now that 2.10.1 is released, the snapshot dependency can be removed again.",technical_debt,['parent'],OAK,Improvement,Minor,2015-05-21 10:35:42,2
12831661,DataStoreBlobStore should expose a buffer input stream for getInputStream call,"DataStoreBlobStore directly exposes the InputStream from the wrapped DataStore. In most cases underlying DataStore exposes a LazyFileInputStream [0] which is not buffered.

For performance reason the stream finally exposed at the BlobStore layer should be buffered one. See [1] for the discussion

[1] http://markmail.org/thread/xi4isnzw57vphcsq
[0]
https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-data/src/main/java/org/apache/jackrabbit/core/data/LazyFileInputStream.java#L102 
",performance,['blob'],OAK,Improvement,Minor,2015-05-21 09:02:53,19
12831426,Regression - lookupOnValidate does not work,"regression of OAK-2783....

On my local instance, I have tested the 4 combination of the new attributes in org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider
@adminPool.lookupOnValidate (true)
@userPool.lookupOnValidate (true)
and found that only when both are set to true, I was able to login with credentials from LDAP server.  see table below for time stamps of the four tested combinations.

I have setup a test harness at http://10.36.65.137:4502.  It is configured for LDAP server on my laptop, which provides user001 ... user010. All have same password, '1234'. 
Note: I have not repeated the above tests on the test harness due to time constraints.

|| time || adminPool.lookupOnValidate || userPool.lookupOnValidate || logon user001 ||
| 16.05.2015 11:14:59.066 | false | true  | NG @ 16.05.2015 11:16:37.431 (1) |
| 16.05.2015 11:18:40.627 | false | false | NG @ 16.05.2015 11:19:54.971 (2) |
| 16.05.2015 11:21:31.757 | true  | false | NG @ ??. No error in LDAP.log. But username and pwd not match |
| 16.05.2015 11:24:16.277 | true | true | OK |

Excerpts from ldap.log
{code}
(1) 16.05.2015 11:16:37.435 *ERROR* [qtp2069601494-1250] org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider Error while connecting to the ldap server.
java.util.NoSuchElementException: Could not create a validated object, cause: ValidateObject failed

(2) 16.05.2015 11:19:54.971 *ERROR* [qtp2069601494-1249] org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider Error while connecting to the ldap server.
java.util.NoSuchElementException: Could not create a validated object, cause: ValidateObject failed
	at org.apache.commons.pool.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:1233)
	at org.apache.directory.ldap.client.api.LdapConnectionPool.getConnection(LdapConnectionPool.java:56)

{code}
",docs-impacting regression resilience,['auth-ldap'],OAK,Improvement,Minor,2015-05-20 16:41:51,23
12831424,Putting many elements into a map results in many small segments. ,"There is an issue with how the HAMT implementation ({{SegmentWriter.writeMap()}} interacts with the 256 segment references limit when putting many entries into the map: This limit gets regularly reached once the maps contains about 200k entries. At that points segments get prematurely flushed resulting in more segments, thus more references and thus even smaller segments. It is common for segments to be as small as 7k with a tar file containing up to 35k segments. This is problematic as at this point handling of the segment graph becomes expensive, both memory and CPU wise. I have seen persisted segment graphs as big as 35M where the usual size is a couple of ks. 

As the HAMT map is used for storing children of a node this might have an advert effect on nodes with many child nodes. 

The following code can be used to reproduce the issue: 

{code}
SegmentWriter writer = new SegmentWriter(segmentStore, getTracker(), V_11);
MapRecord baseMap = null;

for (;;) {
    Map<String, RecordId> map = newHashMap();
    for (int k = 0; k < 1000; k++) {
        RecordId stringId = writer.writeString(String.valueOf(rnd.nextLong()));
        map.put(String.valueOf(rnd.nextLong()), stringId);
    }

    Stopwatch w = Stopwatch.createStarted();
    baseMap = writer.writeMap(baseMap, map);
    System.out.println(baseMap.size() + "" "" + w.elapsed());
}
{code}

",performance,['segment-tar'],OAK,Bug,Critical,2015-05-20 16:32:35,15
12831411,Avoid accessing binary content if the mimeType is excluded from indexing,"Currently the recommended way to exclude certain types of files from getting indexed is to add them to {{EmptyParser}} in Tika Config. However looking at how Tika works even if mimetype is provided as part metadata. 

Tika Detector try to determine the mimetype by actually reading some bytes from InputStream [1] before looking up from passed MetaData. This would cause unnecessary IO in case large number of binaries are excluded.

We would need to look for way where any access to binary content which is not being indexed can be avoided. One option can to expose a multi value config property which takes a list of mimetypes to be excluded from indexing. If the mimeType provided as part of JCR data is part of that excluded list then call to Tika should be avoided

[1] https://github.com/apache/tika/blob/trunk/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java#L446",performance,['lucene'],OAK,Improvement,Minor,2015-05-20 15:44:16,19
12831410,RepositoryImpl should not manage the lifecycle of ContentRepository,"{{RepositoryImpl}} uses an instance of {{ContentRepository}} that is passed as an external dependency in its constructor.

{{RepositoryImpl}} is not responsible for the creation of the {{ContentRepository}} instance and, as such, should not manage its lifecycle. In particular, the {{ContentRepository#close}} method should not be called when the {{RepositoryImpl#shutdown}} method is executed.",modularization resilience technical_debt,['jcr'],OAK,Sub-task,Major,2015-05-20 15:39:49,12
12831407,RepositoryUpgrade.copy() should optionally continue on errors.,"Currently RepositoryUpgrade.copy() fails on the first error. In practice this is very inconvenient, because any minor inconsistency in the source repository may cause the upgrade to fail.
An option to make best-effort copies is needed.",resilience,['upgrade'],OAK,Improvement,Major,2015-05-20 15:25:28,13
12831360,Speed up lucene indexing post migration by pre extracting the text content from binaries,"While migrating large repositories say having 3 M docs (250k PDF) Lucene indexing takes long time to complete (at time 4 days!). Currently the text extraction logic is coupled with Lucene indexing and hence is performed in a single threaded mode which slows down the indexing process. Further if the reindexing has to be triggered it has to be done all over again.

To speed up the Lucene indexing we can decouple the text extraction
from actual indexing. It is partly based on discussion on OAK-2787

# Introduce a new ExtractedTextProvider which can provide extracted text for a given Blob instance
# In oak-run introduce a new indexer mode - This would take a path in repository and would then traverse the repository and look for existing binaries and extract text from that

So before or after migration is done one can run this oak-run tool to create this store which has the text already extracted. Then post startup we need to wire up the ExtractedTextProvider instance (which is backed by the BlobStore populated before) and indexing logic can just get content from that. This would avoid performing expensive text extraction in the indexing thread.

See discussion thread http://markmail.org/thread/ndlfpkwfgpey6o66",performance,"['lucene', 'run']",OAK,New Feature,Major,2015-05-20 12:43:05,19
12830964,SegmentBlob does not return blobId for contentIdentity,{{SegmentBlob}} currently returns recordId for {{contentIdentity}} even when an external DataStore is configured. Given that recordId is not stable it would be better to return the blobId as part of  {{contentIdentity}} if external DataStore is configured,resilience,['segmentmk'],OAK,Bug,Minor,2015-05-19 09:45:28,19
12830955,"Ignore ""order by jcr:score desc"" in the query engine (for ""union"" queries)","Currently, ""order by jcr:score desc"" is ignored in the Lucene index, however for ""union"" queries, this sort order is enforced in the query engine. This will cause queries to be slow if one of the sub-queries is slow.",performance,['lucene'],OAK,Improvement,Major,2015-05-19 09:18:54,4
12830954,ArrayIndexOutOfBoundsException in UnsavedModifications.put(),"In rare cases a commit may fail to update the pending changes on {{_lastRev}}    of documents. The stack trace is:

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.mapdb.BTreeMap.replace(BTreeMap.java:1174)
        at org.apache.jackrabbit.oak.plugins.document.UnsavedModifications.put(UnsavedModifications.java:90)
        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$10.track(DocumentNodeStore.java:1990)
        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.applyChanges(DocumentNodeStore.java:1056)
        at org.apache.jackrabbit.oak.plugins.document.Commit.applyToCache(Commit.java:598)
        at org.apache.jackrabbit.oak.plugins.document.CommitQueue.afterTrunkCommit(CommitQueue.java:127)
        at org.apache.jackrabbit.oak.plugins.document.CommitQueue.done(CommitQueue.java:83)
{noformat}",resilience,"['core', 'mongomk']",OAK,Bug,Critical,2015-05-19 09:14:14,2
12830925,Add support for generating mongo export command to oak-mongo,"At time to analyse a issue with {{DocumentNodeStore}} running on Mongo we need a dump of various documents so as to recreate the scenario locally. In most case if issue is being observed for a specific path like /a/b then its sufficient to get Mongo documents for /, /a, /a/b and all the split documents for those paths.

It would be useful to have a function in oak-mongo which generates the required export command. For e.g. for path like /a/b following export command would dump all required info

{noformat}
mongoexport -h <mongo server> --port 27017 --db <db name> --collection nodes --out all-required-nodes.json --query '{$or:[{_id : /^4:p\/a\/b\//},{_id : /^3:p\/a\//},{_id : /^2:p\//},{_id:{$in:[""2:/a/b"",""1:/a"",""0:/""]}}]}'
{noformat}",tooling,['run'],OAK,Improvement,Minor,2015-05-19 06:40:46,19
12830668,Tests for SegmentNodeStoreService,{{SegmentNodeStoreService}} currently has no test coverage whatsoever. We should change that.,technical_debt,['segmentmk'],OAK,Improvement,Major,2015-05-18 12:43:30,12
12830649,Support migration without access to DataStore,"Migration currently involves access to DataStore as its configured as part of repository.xml. However in complete migration actual binary content in DataStore is not accessed and migration logic only makes use of

* Dataidentifier = id of the files
* Length = As it gets encoded as part of blobId (OAK-1667)

It would be faster and beneficial to allow migration without actual access to the DataStore. It would serve two benefits

# Allows one to test out migration on local setup by just copying the TarPM files. For e.g. one can only zip following files to get going with repository startup if we can somehow avoid having direct access to DataStore
{noformat}
>crx-quickstart# tar -zcvf repo-2.tar.gz repository --exclude=repository/repository/datastore --exclude=repository/repository/index --exclude=repository/workspaces/crx.default/index --exclude=repository/tarJournal
{noformat}
# Provides faster (repeatable) migration as access to DataStore can be avoided which in cases like S3 might be slow.  Given we solve how to get length

*Proposal*
Have a DataStore implementation which can be provided a mapping file having entries for blobId and length. This file would be used to answer queries regarding length and existing of blob and thus would avoid actual access to DataStore.

Going further this DataStore can be configured with a delegate which can be used as a fallback in case the required details is not present in pre computed data set (may be due to change in content after that data was computed)

",docs-impacting performance,['upgrade'],OAK,New Feature,Major,2015-05-18 10:46:35,19
12830643,ConsistencyChecker#checkConsistency can't cope with inconsistent journal,"When running the consistency checker against a repository with a corrupt journal, it fails with an {{ISA}} instead of trying to skip over invalid revision identifiers:

{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Bad record identifier: foobar
at org.apache.jackrabbit.oak.plugins.segment.RecordId.fromString(RecordId.java:57)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:227)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:178)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:156)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:166)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$ReadOnlyStore.<init>(FileStore.java:805)
at org.apache.jackrabbit.oak.plugins.segment.file.tooling.ConsistencyChecker.<init>(ConsistencyChecker.java:108)
at org.apache.jackrabbit.oak.plugins.segment.file.tooling.ConsistencyChecker.checkConsistency(ConsistencyChecker.java:70)
at org.apache.jackrabbit.oak.run.Main.check(Main.java:701)
at org.apache.jackrabbit.oak.run.Main.main(Main.java:158)
{noformat}

",resilience tooling,['run'],OAK,Bug,Minor,2015-05-18 10:01:39,15
12830634,NPE in SegmentWriter.writeMap,"Under some rare conditions which are not entirely clear yet {{SegmentWriter.writeMap}} results in a {{NPE}}:

{noformat}
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:366)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapLeaf(SegmentWriter.java:417)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:475)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:511)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:711)
{noformat}

This happens when the {{base}} passed to {{writeMap(MapRecord base, Map<String, RecordId> changes)}} is not null but doesn't contain some of the keys *removed* through the updates provided in the passed {{changes}}. ",resilience,['segmentmk'],OAK,Bug,Major,2015-05-18 09:51:34,15
12830623,Compaction should check for required disk space before running,In the worst case compaction doubles the repository size while running. As this is somewhat unexpected we should check whether there is enough free disk space before running compaction and log a warning otherwise. This is to avoid a common source of running out of disk space and ending up with a corrupted repository. ,compaction doc-impacting gc resilience,['segmentmk'],OAK,Improvement,Major,2015-05-18 09:19:48,12
12830621,Test failure: AutoCreatedItemsTest.autoCreatedItems,"{{org.apache.jackrabbit.oak.jcr.AutoCreatedItemsTest.autoCreatedItems}} fails on Jenkins: No default node type available for /testdata/property

{noformat}
javax.jcr.nodetype.ConstraintViolationException: No default node type available for /testdata/property
	at org.apache.jackrabbit.oak.util.TreeUtil.addChild(TreeUtil.java:186)
	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.addChild(NodeDelegate.java:692)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:296)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:262)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:202)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:112)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:262)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:247)
	at org.apache.jackrabbit.oak.jcr.TestContentLoader.getOrAddNode(TestContentLoader.java:90)
	at org.apache.jackrabbit.oak.jcr.TestContentLoader.loadTestContent(TestContentLoader.java:58)
	at org.apache.jackrabbit.oak.jcr.AutoCreatedItemsTest.autoCreatedItems(AutoCreatedItemsTest.java:42)
{noformat}

Failure seen at builds: 130, 134, 138, 249, 292, 297

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/134/jdk=jdk-1.6u45,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/testReport/junit/org.apache.jackrabbit.oak.jcr/AutoCreatedItemsTest/autoCreatedItems_0_/",CI jenkins,['rdbmk'],OAK,Bug,Major,2015-05-18 09:14:21,3
12830620,Test failure: OrderableNodesTest.setPrimaryType,"{{org.apache.jackrabbit.oak.jcr.OrderableNodesTest.setPrimaryType}} fails on Jenkins: No default node type available for /testdata

{noformat}
javax.jcr.nodetype.ConstraintViolationException: No default node type available for /testdata
	at org.apache.jackrabbit.oak.util.TreeUtil.addChild(TreeUtil.java:186)
	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.addChild(NodeDelegate.java:692)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:296)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:262)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:202)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:112)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:262)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:247)
	at org.apache.jackrabbit.oak.jcr.TestContentLoader.getOrAddNode(TestContentLoader.java:90)
	at org.apache.jackrabbit.oak.jcr.TestContentLoader.loadTestContent(TestContentLoader.java:57)
	at org.apache.jackrabbit.oak.jcr.OrderableNodesTest.setPrimaryType(OrderableNodesTest.java:62)
{noformat}

Failure seen at builds: 132, 152, 176

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/132/jdk=jdk-1.6u45,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/testReport/junit/org.apache.jackrabbit.oak.jcr/OrderableNodesTest/setPrimaryType_0_/",CI Jenkins,['rdbmk'],OAK,Bug,Major,2015-05-18 09:09:43,3
12830615,Test failure: RepositorySidegradeTest.verifyAsync (NPE),"{{org.apache.jackrabbit.oak.upgrade.RepositorySidegradeTest.verifyAsync}} fails on Jenkins with a {{NPE}}:

{noformat}
verifyAsync(org.apache.jackrabbit.oak.upgrade.RepositorySidegradeTest)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.jackrabbit.oak.upgrade.RepositorySidegradeTest.verifyAsync(RepositorySidegradeTest.java:125)
{noformat}

Failure seen at builds: 133, 134

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/134/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=unittesting/console",ci jenkins,['upgrade'],OAK,Bug,Major,2015-05-18 08:51:53,4
12830114,"XPath: Performance problems with many ""or"" conditions","XPath queries with many ""or"" condition (around 3000) of the following form can result in a performance problem:

{noformat}
contains(...) or x=1 or x=2 or x=3 ...
{noformat}

This is somewhat similar to OAK-2738, but not quite the same.",performance,['query'],OAK,Bug,Major,2015-05-15 08:51:11,4
12830089,ExternalLoginModule should clear state when login was not successful,"As discussed in [1], it looks like the ExternalLoginModule ignores cleaning up its internal state when login was not successful.

What I assume happens next is the old session (probably the initial one created on the very first login call) would be reused throughout the module's lifetime, which would in the end result in the SNFEs post compaction.

[1] http://markmail.org/thread/pcmlz74ngxl7sqfy",gc resilience,['auth-external'],OAK,Bug,Major,2015-05-15 06:50:18,14
12829503,Bypass CommitQueue for branch commits,"Currently all commits go through the CommitQueue. This applies to commits that fit into memory, branch commits, merge commits and even reset commits.

The guarantee provided by the CommitQueue is only necessary for commits that affect the head revision of the store: commits that fit into memory and merge commits.

Branch and reset commits should bypass the CommitQueue to avoid unnecessary delays of commits. ",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-05-13 09:55:35,2
12829452,Log stats around time spent in extracting text from binaries,"For issues like OAK-2787 it would helpful if we collect some stats around how much time is spent in extracting text from binaries.

For that purpose I would like add some logging around text extraction",tooling,['lucene'],OAK,Improvement,Minor,2015-05-13 04:49:38,19
12828838,CompactionMap#compress() inefficient for large compaction maps,"I've seen {{CompactionMap#compress()}} take up most of the time spent in compaction. With 40M record ids in the compaction map compressing runs for hours. 

I will back this with numbers as soon as I have a better grip on the issue.  ",compaction doc-impacting gc,['segmentmk'],OAK,Improvement,Major,2015-05-11 14:59:47,15
12828818,RDBBlobStore: seen insert failures due to duplicate keys,"In production, we've seen exceptions like this:

{noformat}
 org.apache.jackrabbit.oak.plugins.document.rdb.RDBBlobStore insert document failed for id bd89b0745aa22429234f17dfc3e2a35b744dc6e86f5e8094a4153b2366c4d822 w
ith length 14691 (check max size of datastore_data.data)
com.ibm.db2.jcc.am.SqlIntegrityConstraintViolationException: DB2 SQL Error: SQLCODE=-803, SQLSTATE=23505, SQLERRMC=1;DB2INST1.DATASTORE_DATA, DRIVER=4.16.53
        at com.ibm.db2.jcc.am.fd.a(fd.java:735)
        at com.ibm.db2.jcc.am.fd.a(fd.java:60)
        at com.ibm.db2.jcc.am.fd.a(fd.java:127)
        at com.ibm.db2.jcc.am.to.b(to.java:2422)
        at com.ibm.db2.jcc.am.to.c(to.java:2405)
        at com.ibm.db2.jcc.t4.ab.l(ab.java:408)
        at com.ibm.db2.jcc.t4.ab.a(ab.java:62)
        at com.ibm.db2.jcc.t4.o.a(o.java:50)
        at com.ibm.db2.jcc.t4.ub.b(ub.java:220)
        at com.ibm.db2.jcc.am.uo.sc(uo.java:3526)
        at com.ibm.db2.jcc.am.uo.b(uo.java:4489)
        at com.ibm.db2.jcc.am.uo.mc(uo.java:2833)
        at com.ibm.db2.jcc.am.uo.execute(uo.java:2808)
        at sun.reflect.GeneratedMethodAccessor941.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:600)
        at org.apache.tomcat.jdbc.pool.interceptor.AbstractQueryReport$StatementProxy.invoke(AbstractQueryReport.java:235)
        at com.sun.proxy.$Proxy259.execute(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor941.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:600)
        at org.apache.tomcat.jdbc.pool.interceptor.StatementDecoratorInterceptor$StatementProxy.invoke(StatementDecoratorInterceptor.java:252)
        at com.sun.proxy.$Proxy259.execute(Unknown Source)
        at org.apache.jackrabbit.oak.plugins.document.rdb.RDBBlobStore.storeBlockInDatabase(RDBBlobStore.java:374)
        at org.apache.jackrabbit.oak.plugins.document.rdb.RDBBlobStore.storeBlock(RDBBlobStore.java:340)
{noformat}

This seems to indicate that they key is present in _data but not in _meta. We need to find out whether that's caused by an earlier problem, or whether storeInBlock is supposed to handle this.

(Note that the actual exception message about ""check max size of datastore_data.data"" is misleading; it's due to an earlier attempt to diagnose DB config problems)",resilience,"['blob', 'rdbmk']",OAK,Technical task,Major,2015-05-11 13:54:51,3
12828772,Test failure: OrderableNodesTest,"{{org.apache.jackrabbit.oak.jcr.OrderableNodesTest}} fails on Jenkins when running the {{DOCUMENT_RDB}} fixture.

{noformat}
Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.572 sec <<< FAILURE!
orderableFolder[0](org.apache.jackrabbit.oak.jcr.OrderableNodesTest)  Time elapsed: 3.858 sec  <<< ERROR!
javax.jcr.nodetype.ConstraintViolationException: No default node type available for /testdata
	at org.apache.jackrabbit.oak.util.TreeUtil.addChild(TreeUtil.java:186)
	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.addChild(NodeDelegate.java:692)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:296)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:262)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:202)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:112)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:262)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:247)
	at org.apache.jackrabbit.oak.jcr.TestContentLoader.getOrAddNode(TestContentLoader.java:90)
	at org.apache.jackrabbit.oak.jcr.TestContentLoader.loadTestContent(TestContentLoader.java:57)
	at org.apache.jackrabbit.oak.jcr.OrderableNodesTest.orderableFolder(OrderableNodesTest.java:47)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

Failure seen at builds: 81, 87, 92, 95, 96, 114, 120, 128, 134, 186, 243, 272, 292

See e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/128/jdk=jdk-1.6u45,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/console",ci jenkins,['rdbmk'],OAK,Bug,Major,2015-05-11 10:43:36,3
12828771,Test failure: ObservationRefreshTest,"{{org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest}} fails on Jenkins when running the {{DOCUMENT_RDB}} fixture.

{noformat}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 16.174 sec <<< FAILURE!
observation[0](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 16.173 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: OakMerge0001: OakMerge0001: Failed to merge changes to the underlying store (retries 5, 5286 ms)
	at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:239)
	at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:664)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:489)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.performVoid(SessionImpl.java:424)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.performVoid(SessionDelegate.java:268)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:421)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:176)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0001: OakMerge0001: Failed to merge changes to the underlying store (retries 5, 5286 ms)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:242)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:179)
	at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:158)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1451)
	at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:341)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:487)
	... 43 more
Caused by: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: The node 3:/oak:index/nodetype/:index was already added in revision
r14d31a418d3-0-1, before
r14d31a44656-0-1
	at org.apache.jackrabbit.oak.plugins.document.Commit.checkConflicts(Commit.java:524)
	at org.apache.jackrabbit.oak.plugins.document.Commit.createOrUpdateNode(Commit.java:438)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:341)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:246)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:215)
	at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:200)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:323)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:293)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$200(DocumentNodeStoreBranch.java:52)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:530)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:219)
{noformat}

Failure seen at builds: 48, 55, 115, 118, 123 , 127, 128, 130, 149, 152, 155, 164

See e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/128/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/console",ci jenkins,['rdbmk'],OAK,Bug,Major,2015-05-11 10:39:08,3
12828761,Run background read and write operation concurrently,OAK-2624 decoupled the background read from the background write but the methods implementing the operations are synchronized. This means they cannot run at the same time and e.g. an expensive background write may unnecessarily block a background read.,technical_debt,"['core', 'mongomk']",OAK,Improvement,Minor,2015-05-11 09:53:12,2
12828389,improve RDB diagnostics,"It's essential that the database is configured with the proper charset and collation. After detecting the DB type, attempt to inspect the settings reported by the database. This will require custom code for each DB type, though.",resilience,"['mongomk', 'rdbmk']",OAK,Sub-task,Minor,2015-05-08 15:16:44,3
12828314,CopyOnReadDirectory mode might delete a valid local file upon close,"{{CopyOnReadDirectory}} currently deletes local files which are not found in remote upon close. The list of remote file is fixed for a given revision however list of local files may vary. 

{{IndexTracker}} opens a new {{IndexNode}} upon update before closing the older one. When CopyOnRead is enabled it can happen that same local directory might be in use by two wrapper directories at the same time. 

This introduces a race condition in {{removeDeletedFiles}} method as by the time it is invoked a newer wrapped directory might have started adding new files so those files would get included in the listing done for local directory and hence cause them to be deleted as they would not be found in remote directory which is pinned to older revision. Leading to following exception

{noformat}
Caused by: java.io.FileNotFoundException: /path/to/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/4/_1r.cfe (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:241)
	at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:193)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnReadDirectory$FileReference.openLocalInput(IndexCopier.java:393)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnReadDirectory.openInput(IndexCopier.java:221)
	at org.apache.lucene.store.Directory.copy(Directory.java:185)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexMBeanImpl.dumpIndexContent(LuceneIndexMBeanImpl.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

As a fix the list of local file should be maintained as progress is made once the CopyOnRead instance gets created to ensure it does not pick up files which are added once the directory is closed",resilience,['lucene'],OAK,Bug,Major,2015-05-08 10:30:33,19
12828025,"Query engine: if counter index is not available, cost of traversing is too low","If the traversing index is not available (by removing or renaming the node /oak:index/counter), the cost of traversing is relatively low. This can cause traversals, even thought using a property index (or another index) would be better.

By the way, disabling the counter index (setting the type to a 'disabled') alone does still use the estimation in the counter index. This may or may not be a good thing.

Example costs:

{noformat}
/jcr:root/content//element(*, cq:Page)[@test='withCounter']
cost for aggregate lucene is Infinity
cost for lucene-property is Infinity
cost for reference is Infinity
cost for ordered is Infinity
cost for nodeType is 138.0
cost for property is Infinity
cost for traverse is 27100.0

/jcr:root/content//element(*, cq:Page)[@test='withoutCounter2']
cost for aggregate lucene is Infinity
cost for lucene-property is Infinity
cost for reference is Infinity
cost for ordered is Infinity
cost for nodeType is 1504.0
cost for property is Infinity
cost for traverse is 2000.0
{noformat}",performance,['query'],OAK,Bug,Major,2015-05-07 14:42:22,4
12827982,Improve revision gc on SegmentMK,"This is a container issue for the ongoing effort to improve revision gc of the SegmentMK. 

I'm exploring 
* ways to make the reference graph as exact as possible and necessary: it should not contain segments that are not referenceable any more and but must contain all segments that are referenceable. 
* ways to segregate the reference graph reducing dependencies between certain set of segments as much as possible. 
* Reducing the number of in memory references and their impact on gc as much as possible.

",compaction gc,['segment-tar'],OAK,Epic,Major,2015-05-07 10:40:07,15
12827969,Dependency cleanup ,Early in the next release cycle we should go through the list of Oak's dependencies and decide whether we have candidates we want to upgrade and remove orphaned dependencies. ,technical_debt,[],OAK,Epic,Major,2015-05-07 09:12:56,11
12827657,Introducing a simple document-based discovery-light service (to circumvent documentMk's eventual consistency delays),"When running discovery.impl on a mongoMk-backed jcr repository, there are risks of hitting problems such as described in ""SLING-3432 pseudo-network-partitioning"": this happens when a jcr-level heartbeat does not reach peers within the configured heartbeat timeout - it then treats that affected instance as dead, removes it from the topology, and continues with the remainings, potentially electing a new leader, running the risk of duplicate leaders. This happens when delays in mongoMk grow larger than the (configured) heartbeat timeout. These problems ultimately are due to the 'eventual consistency' nature of, not only mongoDB, but more so of mongoMk. The only alternative so far is to increase the heartbeat timeout to match the expected or measured delays that mongoMk can produce (under say given load/performance scenarios).

Assuming that mongoMk will always carry a risk of certain delays and a maximum, reasonable (for discovery.impl timeout that is) maximum cannot be guaranteed, a better solution is to provide discovery with more 'real-time' like information and/or privileged access to mongoDb.

Here's a summary of alternatives that have so far been floating around as a solution to circumvent eventual consistency:
 # expose existing (jmx) information about active 'clusterIds' - this has been proposed in SLING-4603. The pros: reuse of existing functionality. The cons: going via jmx, binding of exposed functionality as 'to be maintained API'
 # expose a plain mongo db/collection (via osgi injection) such that a higher (sling) level discovery could directly write heartbeats there. The pros: heartbeat latency would be minimal (assuming the collection is not sharded). The cons: exposes a mongo db/collection potentially also to anyone else, with the risk of opening up to unwanted possibilities
 # introduce a simple 'discovery-light' API to oak which solely provides information about which instances are active in a cluster. The implementation of this is not exposed. The pros: no need to expose a mongoDb/collection, allows any other jmx-functionality to remain unchanged. The cons: a new API that must be maintained

This ticket is about the 3rd option, about a new mongo-based discovery-light service that is introduced to oak. The functionality in short:
 * it defines a 'local instance id' that is non-persisted, ie can change at each bundle activation.
 * it defines a 'view id' that uniquely identifies a particular incarnation of a 'cluster view/state' (which is: a list of active instance ids)
 * and it defines a list of active instance ids
 * the above attributes are passed to interested components via a listener that can be registered. that listener is called whenever the discovery-light notices the cluster view has changed.

While the actual implementation could in fact be based on the existing {{getActiveClusterNodes()}} {{getClusterId()}} of the {{DocumentNodeStoreMBean}}, the suggestion is to not fiddle with that part, as that has dependencies to other logic. But instead, the suggestion is to create a dedicated, other, collection ('discovery') where heartbeats as well as the currentView are stored.

Will attach a suggestion for an initial version of this for review.",resilience,['mongomk'],OAK,New Feature,Major,2015-05-06 13:14:26,21
12827624,Log NodePropBundle id for which no bundle is found,"At times in migration following exception is seen

{noformat}
Caused by: java.lang.NullPointerException
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.createProperties(JackrabbitNodeState.java:311)
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.<init>(JackrabbitNodeState.java:149)
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.getChildNodeEntries(JackrabbitNodeState.java:255)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1014)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1015)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1015)
{noformat}

This would happen if the NodePropBundle is null for a given id. It would be good to add a NPE check in loader itself",resilience,['upgrade'],OAK,Improvement,Minor,2015-05-06 11:26:46,19
12827602,Test failure: OSGiIT,"The OSGiIT tests are failing silently, so not failing the build when the tests don't pass.

{code}
Running org.apache.jackrabbit.oak.osgi.OSGiIT
[main] INFO org.ops4j.pax.exam.spi.DefaultExamSystem - Pax Exam System (Version: 3.4.0) created.
[main] INFO org.ops4j.pax.exam.junit.impl.ProbeRunner - creating PaxExam runner for class org.apache.jackrabbit.oak.osgi.OSGiIT
[main] INFO org.ops4j.pax.exam.junit.impl.ProbeRunner - running test class org.apache.jackrabbit.oak.osgi.OSGiIT
ERROR: org.apache.jackrabbit.oak-lucene (23): [org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService(1)] The activate method has thrown an exception
java.lang.NullPointerException: Index directory cannot be determined as neither index directory path [localIndexDir] nor repository home [repository.home] defined
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:236)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService.createTracker(LuceneIndexProviderService.java:197)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService.activate(LuceneIndexProviderService.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}",CI Jenkins,['it'],OAK,Bug,Critical,2015-05-06 09:42:44,19
12827592,Persistent cache: avoid repeated log message after closing,"If the persistent cache is severely broken (after ouf memory for example), in some cases the store is closed, but there is still a risk to get many log message (mainly null pointer exceptions) saying ""Could not retrieve the map size"". This needs to be avoided. Possibly the cache files should be removed in such cases, and the cache re-created.",resilience,['cache'],OAK,Improvement,Major,2015-05-06 09:10:31,4
12827572,TARMK Cold Standby inefficient cleanup,"Following OAK-2817, it turns out that patching the data corruption issue revealed an inefficiency of the cleanup method. similar to the online compaction situation, the standby has issues clearing some of the in-memory references to old revisions.",compaction gc production resilience,"['segmentmk', 'tarmk-standby']",OAK,Bug,Critical,2015-05-06 07:18:33,14
12827559,LIRS cache: allow to disable it when using the persistent cache,"Currently, the LIRS cache is always enabled when using the persistent cache. It should be possible to explicitly disable it in this case.",considerFor1.2,[],OAK,Improvement,Major,2015-05-06 06:47:57,4
12827338,Refactor TarMK,"Container issue for refactoring the TarMK to make it more testable, maintainable, extensible and less entangled. 

For example the segment format should be readable, writeable through standalone means so tests, tools and production code can share this code. Currently there is a lot of code duplication involved here. ",technical_debt,['segment-tar'],OAK,Task,Major,2015-05-05 15:59:52,15
12827300,Test failure: DefaultAnalyzersConfigurationTest,"{{org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest.org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest}} fails on Jenkins.

See e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/123/jdk=latest1.7,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=unittesting/console

Seen on {{DOCUMENT_RDB}} and {{SEGMENT_MK}} with Java 1.7. and 1.8. 

{noformat}
Tests run: 13, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 23.572 sec <<< FAILURE!
org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest  Time elapsed: 23.255 sec  <<< ERROR!
com.carrotsearch.randomizedtesting.ThreadLeakError: 21 threads leaked from SUITE scope at org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest: 
   1) Thread[id=32, name=oak-scheduled-executor-13, state=TIMED_WAITING, group=main]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:1125)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:807)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
   2) Thread[id=25, name=oak-scheduled-executor-6, state=TIMED_WAITING, group=main]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:1125)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:807)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
...
{noformat}",CI jenkins,['solr'],OAK,Bug,Major,2015-05-05 14:49:27,2
12827231,Comparing node states for external changes is too slow,"Comparing node states for local changes has been improved already with OAK-2669. But in a clustered setup generating events for external changes cannot make use of the introduced cache and is therefore slower. This can result in a growing observation queue, eventually reaching the configured limit. See also OAK-2683.",docs-impacting scalability,"['core', 'mongomk']",OAK,Improvement,Blocker,2015-05-05 09:33:07,2
12827202,Jcr builder class does not allow overriding most of its dependencies,"The {{Jcr}} class is the entry point for configuring a JCR repository using an Oak backend. However, it always use a hardcoded set of dependencies ( IndexEditorProvider, SecurityProvider, etc )  which cannot be reset, as they are defined in the constructor and the builder {{with}} methods eagerly configure the backing {{Oak}} instance with those dependencies.

As an example

{code:java|title=Jcr.java}
    @Nonnull
    public final Jcr with(@Nonnull SecurityProvider securityProvider) {
        oak.with(checkNotNull(securityProvider));
        this.securityProvider = securityProvider;
        return this;
    }
{code}

injects the security provider which in turn starts configuring the Oak repository provider

{code:java|title=Oak.java}
    @Nonnull
    public Oak with(@Nonnull SecurityProvider securityProvider) {
        this.securityProvider = checkNotNull(securityProvider);
        if (securityProvider instanceof WhiteboardAware) {
            ((WhiteboardAware) securityProvider).setWhiteboard(whiteboard);
        }
        for (SecurityConfiguration sc : securityProvider.getConfigurations()) {
            RepositoryInitializer ri = sc.getRepositoryInitializer();
            if (ri != RepositoryInitializer.DEFAULT) {
                initializers.add(ri);
            }
        }
        return this;
    }
{code}

Instead, the {{Jcr}} class should store the configured dependencies and only configure the {{Oak}} instance when {{createRepository}} is invoked.",modularization technical_debt,['jcr'],OAK,Sub-task,Major,2015-05-05 09:14:19,14
12827175,[oak-blob-cloud] Test Failures: Add joda-time dependency explicitly with definite version range,"AWS sdk jar - com.amazonaws:aws-java-sdk-core has an open range dependency on joda-time [2.2,) which causes the build to fail.

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.4:process (default) on project oak-blob-cloud: Failed to resolve dependencies for one or more projects in the reactor. Reason: No versions are present in the repository for the artifact with a range [2.2,)
[ERROR] joda-time:joda-time:jar:null
[ERROR]
[ERROR] from the specified remote repositories:
[ERROR] Nexus (http://repository.apache.org/snapshots, releases=false, snapshots=true),
[ERROR] central (http://repo.maven.apache.org/maven2, releases=true, snapshots=false)
[ERROR] Path to dependency:
[ERROR] 1) org.apache.jackrabbit:oak-blob-cloud:bundle:1.4-SNAPSHOT
[ERROR] 2) com.amazonaws:aws-java-sdk:jar:1.9.11
[ERROR] 3) com.amazonaws:aws-java-sdk-support:jar:1.9.11
[ERROR] 4) com.amazonaws:aws-java-sdk-core:jar:1.9.11
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :oak-blob-cloud
Build step 'Invoke top-level Maven targets' marked build as failure
[FINDBUGS] Skipping publisher since build result is FAILURE
Recording test results
{noformat}",CI Jenkins,['blob'],OAK,Bug,Major,2015-05-05 08:09:03,16
12825863,Change default for oak.maxLockTryTimeMultiplier,"The default multipler is currently 3, which translates into a lock try timeout of 6 seconds. This is rather low and may result in merge failures even when a commit acquired the merge lock exclusively. I would like to increase it to 30.",doc-impacting resilience,"['core', 'mongomk']",OAK,Improvement,Minor,2015-04-29 08:56:22,2
12825846,Release merge lock in retry loop,"The DocumentNodeStoreBranch retries merges in two phases. First it retries merges while holding the merge lock non-exclusive and performing sleeps between attempts. If those retries fail the next phase will acquire the merge lock exclusively and perform retries. In the first phase the merge lock is released when the commit goes to sleep, while in the second it is not and may block other commits while sleeping.

DocumentNodeStoreBranch should be changed to release the exclusive lock when the commit goes to sleep.",concurrency,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-29 07:54:45,2
12825462,TARMK Cold Standby cleanup removes too many binary segments,"It looks like the standby cleanup process could remove a lot more binary segments than permitted because of the order in which such segments are persisted. the sync will first persist the data segment referencing a binary, then the binary segment itself, this introduces problems when it spreads over multiple tar files and the #cleanup method expects them to be persisted the other way around (this is an implementation detail of the cleanup, but it is implemented this way for efficiency).",docs-impacting production resilience,['segmentmk'],OAK,Bug,Major,2015-04-28 09:02:22,14
12824211,Refactor the optimize logic regarding path include and exclude to avoid duplication,"{{ObservationManagerImpl}} has a optimize method which process the list of includes and excludes and removes redundant clauses. That logic is now also being used in index filtering (OAK-2599) and is getting duplicated.

Going forward we need to refactor this logic so that both places can use it without copying. Possibly making it part of PathUtils

[~mduerig] Also suggested to further optimize
bq. Also PathFilter#optimise could be further optimised by removing entries that subsume each other (e.g. including /a/b, /a is the same as including (/a. ",technical_debt,['core'],OAK,Improvement,Minor,2015-04-27 09:46:38,19
12823893,Cannot copy a node if parent is not accessible,"If we try to copy a node, in which we have full access, but with no access on the parent node, the copy operation will throw a PathNotFoundException when evaluating checkProtectedNode(getParentPath(""sourceNodePath"")) on the copy() method from org.apache.jackrabbit.oak.jcr.session.WorkspaceImpl

",easytest patch-available,['jcr'],OAK,Bug,Major,2015-04-24 21:20:52,0
12823779,Save Lucene directory listing as array property,"OakDirectory has to at times perform directory listing specially at the time of opening of index. With DocumentNodeStore such listing of child nodes ""might"" be slow if there are lots more deleted nodes and GC has not cleared them so far (due to OAK-1557). 

As seen in OAK-2808 Lucene might be creating and deleting lot more files. To speed up such lookup one OakDirectory can save the listing of child nodes as an array property once the writer is closed. ",performance,['lucene'],OAK,Improvement,Major,2015-04-24 15:30:59,19
12823773,Active deletion of 'deleted' Lucene index files from DataStore without relying on full scale Blob GC,"With storing of Lucene index files within DataStore our usage pattern
of DataStore has changed between JR2 and Oak.

With JR2 the writes were mostly application based i.e. if application
stores a pdf/image file then that would be stored in DataStore. JR2 by
default would not write stuff to DataStore. Further in deployment
where large number of binary content is present then systems tend to
share the DataStore to avoid duplication of storage. In such cases
running Blob GC is a non trivial task as it involves a manual step and
coordination across multiple deployments. Due to this systems tend to
delay frequency of GC

Now with Oak apart from application the Oak system itself *actively*
uses the DataStore to store the index files for Lucene and there the
churn might be much higher i.e. frequency of creation and deletion of
index file is lot higher. This would accelerate the rate of garbage
generation and thus put lot more pressure on the DataStore storage
requirements.

Discussion thread http://markmail.org/thread/iybd3eq2bh372zrl",datastore doc-impacting performance,['lucene'],OAK,Improvement,Major,2015-04-24 15:17:40,11
12823366,oak-run: register JMX beans ,"When starting up oak with oak-run the JMX beans are not registered, but it would be convenient for the registration to happen.",tooling,['run'],OAK,Improvement,Minor,2015-04-23 10:09:41,15
12823041,Conditional remove on DocumentStore,"The DocumentStore API allows for conditional inserts (only add document if not present yet) and updates (using findAndModify() with a condition), but it doesn't allow you to remove a document given some conditions are met.

This feature is required to make sure the VersionGarbageCollector does not remove document that are modified concurrently. See OAK-2778.",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-22 14:25:41,2
12822999,avoid NodeTypeDefDiff code duplication,"org.apache.jackrabbit.oak.plugins.nodetype.NodeTypeDefDiff copies a huge amount of code from org.apache.jackrabbit.spi.commons.nodetype.NodeTypeDefDiff (the latter working on QNodeTypeDefinitions, not NodeTypeDefinitions)

Figure out how to avoid the code duplication.",technical_debt,['core'],OAK,Task,Minor,2015-04-22 11:53:50,3
12822994,Clear excess references before cleanup,"{{FileStore#cleanup}} would be more efficient when getting rid of as much references as possibly beforehand. Excess references are contributed by the current {{TarWriter}} instance and segment cache in {{SegmentTracker}}. 

Those excess references turn out to be especially harmful with many concurrent writers continuously writing to the repository. Starting with a certain write load clean up will become completely blocked. ",compaction gc resilience,['segmentmk'],OAK,Improvement,Major,2015-04-22 11:24:40,15
12822976,Make contributions to reference graph from TarWriter less conservative ,"{{TarWriter#cleanup}} currently adds all its references to the initial elements of the reference graph. It would be sufficient though to just add the references in the tar writer's own graph. 

At the same time I'd like to rename that method from {{cleanup}} to {{collectReferences}}, which better reflects its semantics. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-04-22 10:26:56,15
12822956,OakIndexInput cloned instances are not closed,"Related to the inspections I was doing for OAK-2798 I also noticed that we don't fully comply with the {{IndexInput}} javadoc [1] as the cloned instances should throw the given exception if original is closed, but I also think that the original instance should close the cloned instances, see also [ByteBufferIndexInput#close|https://github.com/apache/lucene-solr/blob/lucene_solr_4_7_1/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java#L271].

[1] : {code}
/** Abstract base class for input from a file in a {@link Directory}.  A
 * random-access input stream.  Used for all Lucene index input operations.
 *
 * <p>{@code IndexInput} may only be used from one thread, because it is not
 * thread safe (it keeps internal state like file position). To allow
 * multithreaded use, every {@code IndexInput} instance must be cloned before
 * used in another thread. Subclasses must therefore implement {@link #clone()},
 * returning a new {@code IndexInput} which operates on the same underlying
 * resource, but positioned independently. Lucene never closes cloned
 * {@code IndexInput}s, it will only do this on the original one.
 * The original instance must take care that cloned instances throw
 * {@link AlreadyClosedException} when the original one is closed.
{code}",resilience,['lucene'],OAK,Bug,Major,2015-04-22 09:28:53,10
12822650,Time limit for HierarchicalInvalidator,"This issue is related to OAK-2646. Every now and then I see reports of background reads with a cache invalidation that takes a rather long time. Sometimes minutes. It would be good to give the HierarchicalInvalidator an upper limit for the time it may take to perform the invalidation. When the time is up, the implementation should simply invalidate the remaining documents.",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-21 11:42:18,2
12822289,Improve system resilience in case of index corruption,"I have had issues in cases when the async Lucene index had gotten corrupted. With this index unusable the only option was to re-index.  The problem however was that there were ongoing queries relying on this index even during re-indexing. Because the original index was corrupted these queries led to further load on the system (traversals afair).

I wonder if we could improve the system resilience in such situations.
One thing I could think of: could we maybe fallback to the last known non-corrupted index state while the re-index is running? This would at least take off the load due to new incoming queries.",resilience,"['indexing', 'lucene', 'query']",OAK,Improvement,Major,2015-04-20 11:35:22,19
12821826,Add QueryEngine.executeQuery without limit and offset,"looking at queries executed in oak it seems that the vast majority doesn't set neither limit nor offset. Since there are not public constants available for NO_LIMIT and NO_OFFSET the corresponding values (i.e. Long.MAX_VALUE and 0 respectively) are repeatedly used. 

since i found that i always have to look up how to indicated the NO_LIMIT, i would like to suggest that we either introduce constants for the 2 parameters or provide another {{executeQuery}} method that doesn't require to specify limit and offset (-> the constants might be kept private in this case).

discussing with [~tmueller] it seemed that he would rather prefer the second approach.

in any case it would allow us to make the various usages of {{QueryEngine.executeQuery}} more readable",technical_debt,"['core', 'query']",OAK,Improvement,Major,2015-04-17 10:06:03,4
12821795,Remove Nullable annotation in Predicates of BackgroundObserver,"{code}
@Override
            public int getLocalEventCount() {
                return size(filter(queue, new Predicate<ContentChange>() {
                    @Override
                    public boolean apply(@Nullable ContentChange input) {
                        return input.info != null;
                    }
                }));
            }

            @Override
            public int getExternalEventCount() {
                return size(filter(queue, new Predicate<ContentChange>() {
                    @Override
                    public boolean apply(@Nullable ContentChange input) {
                        return input.info == null;
                    }
                }));
            }
{code}

both methods should probably check for {{input}} being null before accessing {{input.info}}",technical_debt,['core'],OAK,Bug,Major,2015-04-17 07:50:32,19
12821620,Make LDAP connection pool 'testOnBorrow' configurable,"Depending of the LDAP server configuration, it fails to connect as the server doesn't allow the connection validation query.

It fails on 
{quote}
Caused by: java.util.NoSuchElementException: Could not create a validated object, cause: ValidateObject failed
at org.apache.commons.pool.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:1233)
at org.apache.directory.ldap.client.api.LdapConnectionPool.getConnection(LdapConnectionPool.java:56)
at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.connect(LdapIdentityProvider.java:532)
... 92 common frames omitted
{quote}

Based on customer analyze of Oak code this is the reason it fails:

{quote}
 	I think I have found a solution for the problem. While the system is initializing the connection it tries to validate the connection. This is the reason for the strange search request:

SearchRequest
baseDn : ''
filter : '(objectClass=*)'
scope : base object

Because such kind of requests are not allowed in the client's ldap system the connection is being rejected (as invalid). It is configurable if the connection should be validated. The class org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider contains this code

if (config.getAdminPoolConfig().getMaxActive() != 0) {
adminPool = new LdapConnectionPool(adminConnectionFactory);
adminPool.setTestOnBorrow(true);
adminPool.setMaxActive(config.getAdminPoolConfig().getMaxActive());
adminPool.setWhenExhaustedAction(GenericObjectPool.WHEN_EXHAUSTED_BLOCK);
}

A solution for our Problem would most probably be to change the connectionPool configuration adminPool.setTestOnBorrow(false);
This Parameter comes sadly not from the identity provider configuration.

Is there a way to change this this parameter without creating an own implementation of the identity provider?
{quote}

",docs-impacting resilience,['auth-ldap'],OAK,Improvement,Minor,2015-04-16 18:28:46,23
12821214,Minimize the cost calculation for queries using reference restrictions.,"According to the javadocs (QueryIndex) minimum cost for index is 1. Currently ReferenceIndex returns this minimum value, when it can be used for the query.

But even then cost for remaining indexes is still calculated. We could skip cost calculation of remaining indexes if we achieved the minimum cost already.
It will speed up all queries which can leverage the reference Index.

Example query:

SELECT * FROM [nt:base] WHERE PROPERTY([rep:members], 'WeakReference') = '345bef9b-ffa1-3e09-85df-1e03cfa0fb37'",performance,"['core', 'query']",OAK,Improvement,Major,2015-04-15 15:55:37,4
12821153,Oak builder changes its state during repository creation,"The {{Oak#createContentRepository()}} method changes the state of the builder at every invocation. In particular, it always adds a new {{CommitHook}}.

The observable behavior is that all the {{IndexEditor}} instances are executed twice when the {{Oak}} and {{Jcr}} builders are used together - i.e. when both an instance of {{Repository}} and {{ContentRepository}} are needed.",technical_debt,['core'],OAK,Sub-task,Major,2015-04-15 13:26:25,12
12820818,Fair mode for backgroundOperationLock,The backgroundOperationLock in DocumentNodeStore uses the default non-fair acquisition order. According to JavaDoc of ReentrantReadWriteLock it is possible that a background operation task gets delayed for a long time when the system is under load. We should probably consider using the fair mode for the backgroundOperationLock to make sure background operation tasks do not get delayed excessively.,concurrency,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-14 13:42:39,2
12820814,Enable a Jenkins matrix for the 1.2 branch,Create a matrix for the 1.2 branch as the one we have for trunk,CI jenkins,['it'],OAK,Task,Major,2015-04-14 13:28:08,22
12820786,Configurable maxLockTryTimeMS,OAK-2127 introduced a maxLockTryTimeMS to allow a writer to proceed with a merge even if the merge lock is currently acquired by another thread. The value is currently hardcoded.,doc-impacting,"['core', 'mongomk']",OAK,Improvement,Minor,2015-04-14 11:04:36,2
12820732,Persistent cache: add data in a different thread,"The persistent cache usually stores data in a background thread, but sometimes (if a lot of data is added quickly) the foreground thread is blocked.

Even worse, switching the cache file can happen in a foreground thread, with the following stack trace.

{noformat}
""127.0.0.1 [1428931262206] POST /bin/replicate.json HTTP/1.1"" prio=5 tid=0x00007fe5df819800 nid=0x9907 runnable [0x0000000113fc4000]
   java.lang.Thread.State: RUNNABLE
        ...
	at org.h2.mvstore.MVStoreTool.compact(MVStoreTool.java:404)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.PersistentCache$1.closeStore(PersistentCache.java:213)
	- locked <0x0000000782483050> (a org.apache.jackrabbit.oak.plugins.document.persistentCache.PersistentCache$1)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.PersistentCache.switchGenerationIfNeeded(PersistentCache.java:350)
	- locked <0x0000000782455710> (a org.apache.jackrabbit.oak.plugins.document.persistentCache.PersistentCache)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.NodeCache.write(NodeCache.java:85)
	at org.apache.jackrabbit.oak.plugins.document.persistentCache.NodeCache.put(NodeCache.java:130)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.applyChanges(DocumentNodeStore.java:1060)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToCache(Commit.java:599)
	at org.apache.jackrabbit.oak.plugins.document.CommitQueue.afterTrunkCommit(CommitQueue.java:127)
	- locked <0x0000000781890788> (a org.apache.jackrabbit.oak.plugins.document.CommitQueue)
	at org.apache.jackrabbit.oak.plugins.document.CommitQueue.done(CommitQueue.java:83)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.done(DocumentNodeStore.java:637)
{noformat}

To avoid blocking the foreground thread, one solution is to store all data in a separate thread. If there is too much data added, then some of the data is not stored. If possible, the data that was not referenced a lot, and / or old revisions of documents (if new revisions are available).",resilience,"['cache', 'core', 'mongomk']",OAK,Improvement,Major,2015-04-14 06:59:21,4
12820540,Failed to read from tar file ,"Under some rare circumstances there is a warning in the logs:

{noformat}
11:57:47.375 WARN  [pool-1-thread-24] FileStore.java:865    Failed to read from tar file target/SegmentCompactionIT1331315031754226278dir/data01460a.tar
java.io.IOException: Stream Closed
        at java.io.RandomAccessFile.seek(Native Method) ~[na:1.7.0_75]
        at org.apache.jackrabbit.oak.plugins.segment.file.FileAccess$Random.read(FileAccess.java:105) ~[classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.file.TarReader.readEntry(TarReader.java:502) ~[classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:860) ~[classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:128) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:108) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:348) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readPropsV11(Segment.java:476) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.loadTemplate(Segment.java:449) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:402) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:396) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:79) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNodeCount(SegmentNodeState.java:357) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomReader.readRandomTree(SegmentCompactionIT.java:410) [test-classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomPropertyReader.call(SegmentCompactionIT.java:446) [test-classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomPropertyReader.call(SegmentCompactionIT.java:439) [test-classes/:na]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_75]
{noformat}

This happens due to a race between {{FileStore#readSegment}} reading from tar files and already removed by {{FileStore#flush}}. This isn't a problem as the tar file in question is still present at a newer generation and the {{FileStore}} will eventually read from that one. However the warning looks rather scaring and somewhat implies a defect. 

We should either lower the log level or remove the race. ",gc resilience,['segmentmk'],OAK,Improvement,Minor,2015-04-13 14:02:11,15
12820397,Consolidated JMX view of all EventListener related statistics,"Oak Observation support exposes a {{EventListenerMBean}} [1] which provide quite a bit of details around registered observation listeners. However in a typical application there would be multiple listeners registered. To simplify monitoring it would be helpful to have a _consolidated_ view of all listeners related statistics.

Further the stats can also include some more details which are Oak specific
* Subtree paths to which the listener listens to - By default JCR Api allows single path however Oak allows a listener to register to multiple paths
* If listener is enabled to listen to cluster local and cluster external changes
* Size of queue in BackgroundObserver
* Distribution of change types present in the queue - Local, External etc

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-api/src/main/java/org/apache/jackrabbit/api/jmx/EventListenerMBean.java",monitoring observation,['core'],OAK,Improvement,Major,2015-04-12 14:45:45,2
12820275,Use non unique PathCursor in LucenePropertyIndex,"{{LucenePropertyIndex}} currently uses unique PathCursor [1] due to which the cursor would maintain an in memory set of visited path. This might grow big if result size is big and cursor is traversed completely.

As with current impl the path would not be duplicated we can avoid using unique cursor

[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java#L1153-1154",resilience,['lucene'],OAK,Improvement,Minor,2015-04-11 06:33:43,19
12820040,"Provide a ""different lane"" for slow indexers in async indexing","In case of big repositories, asynchronous index like Lucene Property,
could lag behind as slow indexes, for example Full Text, are taken
care in the same thread pool.

Provide a separate thread pool in which such indexes could be
registered.

",docs-impacting,['core'],OAK,Improvement,Major,2015-04-10 12:03:08,14
12819969,Ignore lesser used old cache entries while invalidating cache entries in background read,"Currently the Cache invalidation logic used in MongoDocumentStore check for cache consistency for all the entries present in the cache. With use of persistent cache its possible that pressure on backend cache would be reduced and some of the cache entries are not being accessed for long time.

Cache invalidation logic should take into account such access statistics and not perform consistency check for cached instance which are not accessed for some long time (10 mins?). Such cache entries should be directly discarded.

PS: Looking at [1] it appears that Guava cache does not enforces a global LRU eviction policy. The policy is maintained per segment table

[1] http://stackoverflow.com/questions/10236057/guava-cache-eviction-policy",performance,"['cache', 'mongomk']",OAK,Improvement,Major,2015-04-10 07:43:21,19
12819966,PersistentCache should rely on eviction callback to add entry to the persistent cache,"Currently when PersistentCache is enabled then any put results in addition of the entry to in memory cache and also to the backing persistent cache. While adding the entry to the persistent cache there is slight overhead of serialization of the entry to be paid.

To avoid such overheads at time of read/write to in memory cache it would be better to move the logic to separate thread. PersistentCache can make use of Guava cache eviction callback and then add the entry to the backend persistent store",performance,['mongomk'],OAK,Improvement,Major,2015-04-10 07:23:14,4
12819945,Change default cache distribution ratio if persistent cache is enabled,"By default the cache memory in DocumentNodeStore is distributed in following ratio

* nodeCache - 25%
* childrenCache - 10%
* docChildrenCache - 3%
* diffCache - 5%
* documentCache - Is given the rest i.e. 57%

However off late we have found that with persistent cache enabled we can lower the cache allocated to Document cache. That would reduce the time spent in invalidating cache entries in periodic reads. So far we are using following ration in few setup and that is turning out well

* nodeCachePercentage=35
* childrenCachePercentage=20
* diffCachePercentage=30
* docChildrenCachePercentage=10
* documentCache - Is given the rest i.e. 5%

We should use the above distribution by default if the persistent cache is found to be enabled
",performance,"['mongomk', 'rdbmk']",OAK,Improvement,Major,2015-04-10 05:17:25,19
12819682,take appropriate action when lease cannot be renewed (in time),"Currently, in an oak-cluster when (e.g.) one oak-client stops renewing its lease (ClusterNodeInfo.renewLease()), this will be eventually noticed by the others in the same oak-cluster. Those then mark this client as {{inactive}} and start recoverying and subsequently removing that node from any further merge etc operation.

Now, whatever the reason was why that client stopped renewing the lease (could be an exception, deadlock, whatever) - that client itself still considers itself as {{active}} and continues to take part in the cluster action.

This will result in a unbalanced situation where that one client 'sees' everybody as {{active}} while the others see this one as {{inactive}}.

If this ClusterNodeInfo state should be something that can be built upon, and to avoid any inconsistency due to unbalanced handling, the inactive node should probably retire gracefully - or any other appropriate action should be taken, other than just continuing as today.

This ticket is to keep track of ideas and actions taken wrt this.",resilience,['core'],OAK,Task,Major,2015-04-09 13:56:43,21
12819638,Oak instance does not close the executors created upon ContentRepository creation,"Oak.createContentRepository does not closes the executors it creates upon close. It should close the executor if that is created by itself and not passed by outside

Also see recent [thread|http://markmail.org/thread/rryydj7vpua5qbub].",CI Jenkins,['core'],OAK,Bug,Minor,2015-04-09 11:23:41,19
12819623,MongoDiffCacheTest.sizeLimit() uses too much memory,The diff created by the test uses a lot of memory. Either test test should be changed or the implementation should ignore further changes once a threshold is reached.,CI,['core'],OAK,Test,Minor,2015-04-09 10:06:23,2
12819614,Compaction does not finish on repository with continuous writes ,"A repository with continuous writes can keep the compactor from completing causing the repository size to grow indefinitely. 

This effect is caused by the compactor trying to catch up with changes that occurred while compacting. I.e. compacting them on top of the already compacted head. When there is a steady stream of incoming changes it can happen that the compactor never actually catches up. ",compaction doc-impacting gc,['segmentmk'],OAK,Bug,Major,2015-04-09 09:21:38,15
12819613,"Option to convert ""like"" queries to range queries","Queries with ""like"" conditions of the form ""x like 'abc%'"" are currently always converted to range queries. With Apache Lucene, using ""like"" in some cases is a bit faster (but not much, according to our tests).

Converting ""like"" to range queries should be disabled by default.

Potential patch:
{noformat}
--- src/main/java/org/apache/jackrabbit/oak/query/ast/ComparisonImpl.java	(revision 1672070)
+++ src/main/java/org/apache/jackrabbit/oak/query/ast/ComparisonImpl.java	(working copy)
@@ -31,11 +31,21 @@
 import org.apache.jackrabbit.oak.query.fulltext.LikePattern;
 import org.apache.jackrabbit.oak.query.index.FilterImpl;
 import org.apache.jackrabbit.oak.spi.query.PropertyValues;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * A comparison operation (including ""like"").
  */
 public class ComparisonImpl extends ConstraintImpl {
+    
+    static final Logger LOG = LoggerFactory.getLogger(ComparisonImpl.class);
+    
+    private final static boolean CONVERT_LIKE_TO_RANGE = Boolean.getBoolean(""oak.convertLikeToRange"");
+    
+    static {
+        LOG.info(""Converting like to range queries is "" + (CONVERT_LIKE_TO_RANGE ? ""enabled"" : ""disabled""));
+    }
 
     private final DynamicOperandImpl operand1;
     private final Operator operator;
@@ -193,7 +203,7 @@
                     if (lowerBound.equals(upperBound)) {
                         // no wildcards
                         operand1.restrict(f, Operator.EQUAL, v);
-                    } else if (operand1.supportsRangeConditions()) {
+                    } else if (operand1.supportsRangeConditions() && CONVERT_LIKE_TO_RANGE) {
                         if (lowerBound != null) {
                             PropertyValue pv = PropertyValues.newString(lowerBound);
                             operand1.restrict(f, Operator.GREATER_OR_EQUAL, pv);
@@ -203,7 +213,7 @@
                             operand1.restrict(f, Operator.LESS_OR_EQUAL, pv);
                         }
                     } else {
-                        // path conditions
+                        // path conditions, or conversion is disabled
                         operand1.restrict(f, operator, v);
                     }
                 } else {
{noformat}",performance,['query'],OAK,Improvement,Major,2015-04-09 09:16:52,4
12819364,NPE when calling Event.getInfo(),"On a very busy site, we're observing an NPE in the code that should gather information about a JCR event for our custom event handler. ",observation,['core'],OAK,Bug,Major,2015-04-08 15:56:16,12
12818944,NodeStateSolrServersObserver should be filtering path selectively,"As discussed in OAK-2718 it'd be good to be able to selectively find Solr indexes by path, as done in Lucene index, see also OAK-2570.
This would avoid having to do full diffs.",performance,['solr'],OAK,Improvement,Major,2015-04-07 14:24:14,10
12818886,IndexCopier fails to delete older index directory upon reindex,"{{IndexCopier}} tries to remove the older index directory incase of reindex. This might fails on platform like Windows if the files are still memory mapped or are locked.

For deleting directories we would need to take similar approach like being done with deleting old index files i.e. do retries later.

Due to this following test fails on Windows (Per [~julian.reschke@gmx.de] )

{noformat}
Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec <<< FAILURE!
deleteOldPostReindex(org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest)  Time elapsed: 0.02 sec  <<< FAILURE!
java.lang.AssertionError: Old index directory should have been removed
        at org.junit.Assert.fail(Assert.java:93)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertFalse(Assert.java:68)
        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest.deleteOldPostReindex(IndexCopierTest.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
{noformat}",resilience,['lucene'],OAK,Bug,Minor,2015-04-07 10:32:11,19
12818550,Misleading warn message about local copy size different than remote copy in oak-lucene with copyOnRead enabled,"At times following warning is seen in logs

{noformat}
31.03.2015 14:04:57.610 *WARN* [pool-6-thread-7] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _0.cfs in NIOFSDirectory@/path/to/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/path/to/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 1040384 differs from remote 1958385. Content would be read from remote file only
{noformat}

The file length check provides a weak check around index file consistency. In some cases this warning is misleading. For e.g. 

# Index version Rev1 - Task submitted to copy index file F1 
# Index updated to Rev2 - Directory bound to Rev1 is closed
# Read is performed with Rev2 for F1 - Here as the file would be locally created the size would be different as the copying is in progress

In such a case the logic should ensure that once copy is done the local file gets used",resilience,['lucene'],OAK,Improvement,Minor,2015-04-06 07:05:18,19
12787285,Test failures on Jenkins,"This issue is for tracking test failures seen at our Jenkins instance that might yet be transient. Once a failure happens too often we should remove it here and create a dedicated issue for it. 

h2. Current issues

|| Test                                                                                       || Builds || Fixture      || JVM || Branch ||
| org.apache.jackrabbit.oak.jcr.ConcurrentAddIT.addNodesSameParent | 427, 428, 758 | DOCUMENT_NS, SEGMENT_MK, DOCUMENT_RDB | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.jcr.ConcurrentAddIT.addNodes\[DocumentNodeStore\[RDB\] on jdbc:h2:file:./target/oaktest\] | 829, 846 | | | 1.4 |
| org.apache.jackrabbit.oak.jcr.ConcurrentAddIT| 720, 818 | DOCUMENT_RDB | 1.7 | 1.5 |
| org.apache.jackrabbit.oak.jcr.ConcurrentAddReferenceTest.addReferences[DocumentNodeStore\[RDB] on jdbc:h2:file:./target/oaktest] |  778, 850 | RDB, NS | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.testReorder\[RDBDocumentStore on jdbc:h2:file:./target/oaktest] |  851 | | | 1.2 |
| org.apache.jackrabbit.oak.osgi.OSGiIT.listServices | 851 | | | 1.2 |
| org.apache.jackrabbit.oak.plugins.document.BulkCreateOrUpdateClusterTest.testConcurrentWithConflict\[RDBFixture:RDB-Derby(embedded)] | 797, 823, 841, 842, 843, 847, 848, 850, 853 | | | 1.4, 1.5 |
| org.apache.jackrabbit.oak.plugins.document.BulkCreateOrUpdateTest | 731, 732, 767 | DOCUMENT_RDB, DOCUMENT_NS | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteServiceIT.testLargeStartStopFiesta | 803, 823, 849, 853 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteServiceTest | 361, 608 | DOCUMENT_NS, SEGMENT_MK | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreTest.recoverBranchCommit | 805 | | | 1.0 |
| org.apache.jackrabbit.oak.plugins.document.blob.RDBBlobStoreTest | 673, 674, 786, 787 | SEGMENT_MK, DOCUMENT_NS, DOCUMENT_RDB | 1.7, 1.8 |  |
| org.apache.jackrabbit.oak.plugins.document.blob.RDBBlobStoreTest.testUpdateAndDelete[MyFixture: RDB-Derby(embedded)] | 780, 785, 786, 787 | RDB, NS, | 1.7, 1.8 | 1.5, 1.4 |
| org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest | 648, 679 | SEGMENT_MK, DOCUMENT_NS | 1.8 |  |
| org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest.reuseLocalDir                 | 81      | DOCUMENT_RDB | 1.7   | |
| org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinitionTest | 770 | DOCUMENT_RDB, DOCUMENT_NS, SEGMENT_MK | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexEditorTest | 490, 623, 624, 656, 679 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexHookIT.testPropertyAddition | 775, 782, 783, 789, 821, 832 | Segment, rdb | 1.7, 1.8 | 1.5, 1.2, 1.0 |
| org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.testNativeMLTQuery | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.testNativeMLTQueryWithStream | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.plugins.index.solr.query.SolrQueryIndexTest | 148, 151, 490, 656, 679 | SEGMENT_MK, DOCUMENT_NS, DOCUMENT_RDB | 1.5, 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.index.solr.server.EmbeddedSolrServerProviderTest.testEmbeddedSolrServerInitialization | 490, 656, 679 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.plugins.index.solr.util.NodeTypeIndexingUtilsTest | 663 | SEGMENT_MK | 1.7 | |
| org.apache.jackrabbit.oak.plugins.index.solr.util.NodeTypeIndexingUtilsTest.testSynonymsFileCreation | 627 | DOCUMENT_RDB |1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.ExternalBlobIT.testDataStoreBlob | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.ExternalBlobIT.testNullBlobId | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.ExternalBlobIT.testSize | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite\[usePersistedMap: false] | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite\[usePersistedMap: true] | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest | 731, 766, 767, 773, 777, 815 | SEGMENT_MK | 1.7, 1.8 | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxyFlippedEndByte | 804 | | | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxyFlippedIntermediateByteSSL | 777 | Segment | 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxyFlippedStartByteSSL | 773, 843 | Segment | 1.8 | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxySSLSkippedBytes | 788,806 | Segment | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT | 722, 731, 733, 755, 759, 776, 812, 815, 816, 817 | SEGMENT_MK | 1.7, 1.8 | 1.4, 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedIntermediateByte | 837, 848 | | | |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedIntermediateByte2 | 837 | | | | 
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedIntermediateByteChange2 | 797, 837 | | | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedStartByte | 794, 837, 848 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxySkippedBytes | 788, 837, 848 | Segment | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxySkippedBytesIntermediateChange | 779, 776, 773 | Segment | 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testSync | 837 | | | |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT.testProxySkippedBytes | 788 | Segment | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.FailoverSslTestIT | 759 | SEGMENT_MK | 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.FailoverSslTestIT.testFailoverSecure | 794, 846 | | | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.StandbyTest.testSync | 839 | | | |
| org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT | 643 | DOCUMNET_NS | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.run.osgi.DocumentNodeStoreConfigTest.testRDBDocumentStoreRestart | 621 | DOCUMENT_NS | 1.8 | |
| org.apache.jackrabbit.oak.run.osgi.DocumentNodeStoreConfigTest.testRDBDocumentStore_CustomBlobStore | 52, 181, 399 |  SEGMENT_MK, DOCUMENT_NS | 1.7 | |
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapDefaultLoginModuleTest | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest | 689 | SEGMENT_MK | 1.8 | |
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticate | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticateCaseInsensitive | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticateFail | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticateValidateTrueTrue | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetGroups | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetMembers | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetUserByForeignRef | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetUserByRef | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetUserByUserId | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testListUsersWithMissingUid | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testSplitDNIntermediatePath | 833 | | | | 
| org.apache.jackrabbit.oak.spi.security.authorization.cug.impl.* | 648 | SEGMENT_MK, DOCUMENT_NS | 1.8 |  |
| org.apache.jackrabbit.oak.standalone.RepositoryBootIT | 755 | DOCUMENT_NS | 1.7 | |
| org.apache.jackrabbit.oak.standalone.RepositoryBootIT.repositoryLogin | 778, 781, 793 | RDB, NS | 1.7, 1.8 | 1.5, 1.4 |

h2. fixed or not happening for a while

|| Test                                                                                       || Builds || Fixture      || JVM || Branch ||
| Build crashes: malloc(): memory corruption | 477 | DOCUMENT_NS | 1.5 | |
| org.apache.jackrabbit.j2ee.TomcatIT | 589 | SEGMENT_MK | 1.8 | |
| org.apache.jackrabbit.j2ee.TomcatIT.testTomcat | 489, 493, 597, 648, 801 | DOCUMENT_NS, SEGMENT_MK | 1.7 |  |
| org.apache.jackrabbit.oak.jcr.ConcurrentFileOperationsTest.concurrent | 110, 382 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.MoveRemoveTest.removeExistingNode | 115 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.jcr.OrderedIndexIT.oak2035                                         | 76, 128 | SEGMENT_MK , DOCUMENT_RDB  | 1.5   | |
| org.apache.jackrabbit.oak.jcr.RepositoryTest.addEmptyMultiValue | 115 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.jcr.cluster.NonLocalObservationIT | 731 | DOCUMENT_RDB | 1.8 | |
| org.apache.jackrabbit.oak.jcr.nodetype.NodeTypeTest.updateNodeType | 243, 400 | DOCUMENT_RDB | 1.5, 1.8 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.disjunctPaths | 121, 157, 396 | DOCUMENT_RDB | 1.5, 1.7, 1.8 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.removeSubtreeFilter | 94 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.testReorder | 155 | DOCUMENT_RDB | 1.8 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTesttestMove | 308 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.query.QueryPlanTest.nodeType | 272 | DOCUMENT_RDB | 1.8 | |
| org.apache.jackrabbit.oak.jcr.query.QueryPlanTest.propertyIndexVersusNodeTypeIndex | 90 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest | 171 | SEGMENT_MK | 1.8 | |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest.testNoSuggestions | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest.testSuggestSql | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest.testSuggestXPath | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.jcr.version.VersionablePathNodeStoreTest.testVersionablePaths | 361 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.osgi.OSGiIT | 767, 770 | SEGMENT_MK, DOCUMENT_RDB, DOCUMENT_NS | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.osgi.OSGiIT.bundleStates | 163, 656 | SEGMENT_MK, DOCUMENT_RDB, DOCUMENT_NS | 1.5, 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.StandbyTestIT.testSyncLoop                 | 64      | ?            | ?     | |
| org.apache.jackrabbit.oak.run.osgi.JsonConfigRepFactoryTest.testRepositoryTar                | 41      | ?            | ?     | |
| org.apache.jackrabbit.oak.run.osgi.PropertyIndexReindexingTest.propertyIndexState | 492 | DOCUMENT_NS | 1.5 | |
| org.apache.jackrabbit.oak.stats.ClockTest.testClockDriftFast | 115, 142 | SEGMENT_MK, DOCUMENT_NS | 1.6, 1.8 | |
| org.apache.jackrabbit.oak.upgrade.cli.SegmentToJdbcTest.validateMigration | 486 | DOCUMENT_NS | 1.7|  |
| org.apache.jackrabbit.test.api.observation.PropertyAddedTest.testMultiPropertyAdded          | 29      | ?            | ?     | |
",CI Jenkins,['it'],OAK,Epic,Major,2015-04-01 07:40:32,15
12787040,High memory usage of CompactionMap,"In environments with a lot of volatile content the {{CompactionMap}} can end up eating a lot of memory. From {{CompactionStrategyMBean#getCompactionMapStats}}:

{noformat}
[Estimated Weight: 317,5 MB, Records: 39500094, Segments: 36698], 
[Estimated Weight: 316,4 MB, Records: 39374593, Segments: 36660], 
[Estimated Weight: 315,4 MB, Records: 39253205, Segments: 36620], 
[Estimated Weight: 315,1 MB, Records: 39221882, Segments: 36614], 
[Estimated Weight: 314,9 MB, Records: 39195490, Segments: 36604], 
[Estimated Weight: 315,0 MB, Records: 39182753, Segments: 36602], 
[Estimated Weight: 360 B, Records: 0, Segments: 0],
{noformat}


This causes compaction to be skipped:

{noformat}
2015-03-30:30.03.2015 02:00:00.038 *INFO* [] [TarMK compaction thread [/foo/bar/crx-quickstart/repository/segmentstore], active since Mon Mar 30 02:00:00 CEST 2015, previous max duration 3854982ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore Not enough available memory 5,5 GB, needed 6,3 GB, last merge delta 1,3 GB, so skipping compaction for now
{noformat}",compaction gc resilience,['segmentmk'],OAK,Improvement,Major,2015-03-31 14:26:33,15
12787030,Possible null-dereference when calling ItemImpl#perform,FindBugs complains about usages of ItemImpl#perform that is annotated with {{@CheckForNull}} but callers expecting a non-null return value.... most prominently in NodeImpl,technical_debt,['jcr'],OAK,Bug,Major,2015-03-31 13:58:29,0
12787023,Troublesome AbstractTree.toString,"the default {{toString}} for all tree implementations calculates a string containing the path, the toString of all properties as well as the names of all child tree... this is prone to cause troubles in case for trees that have plenty of properties and children.

i would strongly recommend to review this and make the toString of trees both meaningful and cheap.",technical_debt,['core'],OAK,Improvement,Major,2015-03-31 13:29:15,19
12786731,ConcurrentAddIT occasionally fail with OakMerge0001,"When running Integration testing often the test fail with the following

{noformat}
addNodes[2](org.apache.jackrabbit.oak.jcr.ConcurrentAddIT)  Time elapsed: 9.88 sec  <<< ERROR!
java.lang.RuntimeException: org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0001: OakMerge0001: Failed to merge changes to the underlying store (retries 5, 5240 ms)
	at org.apache.jackrabbit.oak.spi.lifecycle.OakInitializer.initialize(OakInitializer.java:64)
	at org.apache.jackrabbit.oak.Oak.createContentRepository(Oak.java:561)
	at org.apache.jackrabbit.oak.jcr.Jcr.createRepository(Jcr.java:202)
	at org.apache.jackrabbit.oak.jcr.AbstractRepositoryTest.createRepository(AbstractRepositoryTest.java:125)
	at org.apache.jackrabbit.oak.jcr.AbstractRepositoryTest.getRepository(AbstractRepositoryTest.java:115)
	at org.apache.jackrabbit.oak.jcr.AbstractRepositoryTest.createAdminSession(AbstractRepositoryTest.java:149)
	at org.apache.jackrabbit.oak.jcr.AbstractRepositoryTest.getAdminSession(AbstractRepositoryTest.java:136)
	at org.apache.jackrabbit.oak.jcr.ConcurrentAddIT.addNodes(ConcurrentAddIT.java:54)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0001: OakMerge0001: Failed to merge changes to the underlying store (retries 5, 5240 ms)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:257)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:191)
	at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1434)
	at org.apache.jackrabbit.oak.spi.lifecycle.OakInitializer.initialize(OakInitializer.java:62)
	... 34 more
Caused by: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: update of 1:/jcr:system failed, race condition?
	at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.internalCreateOrUpdate(RDBDocumentStore.java:914)
	at org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStore.createOrUpdate(RDBDocumentStore.java:256)
	at org.apache.jackrabbit.oak.plugins.document.Commit.createOrUpdateNode(Commit.java:437)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:341)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:246)
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:215)
	at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:200)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:311)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:281)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$200(DocumentNodeStoreBranch.java:52)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:520)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:234)
	... 38 more
{noformat}

",CI Jenkins,"['it', 'jcr', 'rdbmk']",OAK,Bug,Major,2015-03-30 14:00:56,3
12785952,Test failure: QueryResultTest.testGetSize,"{{org.apache.jackrabbit.core.query.QueryResultTest.testGetSize}} fails every couple of builds:

{noformat}
junit.framework.AssertionFailedError: Wrong size of NodeIterator in result expected:<48> but was:<-1>
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.Assert.failNotEquals(Assert.java:287)
	at junit.framework.Assert.assertEquals(Assert.java:67)
	at junit.framework.Assert.assertEquals(Assert.java:134)
	at org.apache.jackrabbit.core.query.QueryResultTest.testGetSize(QueryResultTest.java:47)
{noformat}

Failure seen at builds: 29, 39, 59, 61, 114, 117, 118, 120, 139, 142

See e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/59/jdk=jdk-1.6u45,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=unittesting/testReport/junit/org.apache.jackrabbit.core.query/QueryResultTest/testGetSize/
",CI Jenkins,['core'],OAK,Bug,Major,2015-03-26 17:33:32,4
12785898,Segment.readString optimization,"The method Segment.readString is called a lot, and even a small optimization would improve performance for some use cases. Currently it uses a concurrent hash map to cache strings. It might be possible to speed up this cache.",performance,['core'],OAK,Improvement,Major,2015-03-26 14:57:06,4
12785845,"Persistent cache: log activity and timing data, and possible optimizations","The persistent cache most likely reduce performance in some uses cases, but currently it's hard to find out if that's the case or not.

Activity should be captured (and logged with debug level) if possible, for example writing, reading, writing in the foreground / background, opening and closing, switching the generation, moving entries from old to new generation.

Adding entries to the cache could be completely decoupled from the foreground thread, if they are added to the persistent cache in a separate thread.

It might be better to only write entries if they were accessed often. To do this, entries could be put in the persistent cache once they are evicted from the in-memory cache, instead of when they are added to the cache. If that's done, we would maintain some data (for example access count) on which we can filter.",tooling,['core'],OAK,Improvement,Major,2015-03-26 11:04:10,4
12785837,Track root state revision when reading the tree,"Currently the DocumentNodeState has two revisions:

- {{getRevision()}} returns the read revision of this node state. This revision was used to read the node state from the underlying {{NodeDocument}}.
- {{getLastRevision()}} returns the revision when this node state was last modified. This revision also reflects changes done further below the tree when the node state was not directly affected by a change.

The lastRevision of a state is then used as the read revision of the child node states. This avoids reading the entire tree again with a different revision after the head revision changed because of a commit.

This approach has at least two problems related to comparing node states:

- It does not work well with the current DiffCache implementation and affects the hit rate of this cache. The DiffCache is pro-actively populated after a commit. The key for a diff is a combination of previous and current commit revision and the path. The value then tells what child nodes were added/removed/changed. As the comparison of node states proceeds and traverses the tree, the revision of a state may go back in time because the lastRevision is used as the read revision of the child nodes. This will cause misses in the diff cache, because the revisions do not match the previous and current commit revisions as used to create the cache entries. OAK-2562 tried to address this by keeping the read revision for child nodes at the read revision of the parent in calls of compareAgainstBaseState() when there is a diff cache hit. However, it turns out node state comparison does not always start at the root state. The {{EventQueue}} implementation in oak-jcr will start at the paths as indicated by the filter of the listener. This means, OAK-2562 is not effective in this case and the diff needs to be calculated again based on a set of revisions, which is different from the original commit.

- When a diff is calculated for a parent with many child nodes, the {{DocumentNodeStore}} will perform a query on the underlying {{DocumentStore}} to get child nodes modified after a given timestamp. This timestamp is derived from the lower revision of the two lastRevisions of the parent node states to compare. The query gets problematic for the {{DocumentStore}} if the timestamp is too far in the past. This will happen when the parent node (and sub-tree) was not modified for some time. E.g. the {{MongoDocumentStore}} has an index on the _id and the _modified field. But if there are many child nodes the _id index will not be that helpful and if the timestamp is too far in the past, the _modified index is not selective either. This problem was already reported in OAK-1970 and linked issues.

Both of the above problems could be addressed by keeping track of the read revision of the root node state in each of the node states as the tree is traversed. The revision of the root state would then be used e.g. to derive the timestamp for the _modified constraint in the query. Because the revision of the root state is rather recent, the _modified constraint is very selective and the index on it would be the preferred choice.",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-03-26 10:28:08,2
12785591,Introduce time difference detection for DocumentNodeStore,"Currently the lease mechanism in DocumentNodeStore/mongoMk is based on the assumption that the clocks are in perfect sync between all nodes of the cluster. The lease is valid for 60sec with a timeout of 30sec. If clocks are off by too much, and background operations happen to take couple seconds, you run the risk of timing out a lease. So introducing a check which WARNs if the clocks in a cluster are off by too much (1st threshold, eg 5sec?) would help increase awareness. Further drastic measure could be to prevent a startup of Oak at all if the difference is for example higher than a 2nd threshold (optional I guess, but could be 20sec?).",resilience,"['core', 'mongomk', 'rdbmk']",OAK,Improvement,Major,2015-03-25 16:56:16,21
12785538,Update lease without holding lock,"A lease update of the DocumentNodeStore on MongoDB will acquire a lock in MongoDocumentStore to perform the changes. The locking is only necessary for changes in the 'nodes' collection, because only those documents are cached and the locking makes sure the cache is consistent. The MongoDocumentStore must be changed to only acquire a lock when changes are done in the 'nodes' collection.",concurrency technical_debt,"['core', 'mongomk']",OAK,Improvement,Major,2015-03-25 14:50:54,2
12785512,Query engine: faster cost calculation,"If there are many indexes, preparing a query can take a long time, in relation to executing the query.

The query execution plans can be cached. The cache should be invalidated if there are new indexes, or indexes are changed; a simple solution might be to use a timeout, and / or a manual cache clean via JMX or so.",performance,"['core', 'query']",OAK,Improvement,Major,2015-03-25 13:16:01,4
12785502,Include change type information in perf logs for diff logic,"Currently the diff perf logs in {{NodeObserver}} does not indicate what type of change was processed i.e. was the change an internal one or an external one. 

Having this information would allow us to determine how the cache is being used. For e.g. if we see slower number even for local changes then that would indicate that there is some issue with the diff cache and its not be being utilized effectively ",observation performance resilience tooling,['core'],OAK,Improvement,Minor,2015-03-25 12:00:53,11
12784761,"RDB: Tool/scripts for repair, recovery and sub-tree deletion","Scripts and/or support should be added in oak-run console to support repair and recovery options as supported for Mongo.
Also, needed are options that are supported in the oak-mongo.js file specially sub-tree deletion which has been found useful to delete corrupted indexes.",production tools,['rdbmk'],OAK,Improvement,Major,2015-03-23 08:25:35,16
12783861,Unique property index can trigger OOM during upgrade of large repository,"{{PropertyIndexEditor}} when configured for unique index maintains an in memory state of indexed property in {{keysToCheckForUniqueness}}. This set would accumulate all the unique values being indexed.

In case of upgrade where the complete upgrade is performed in single commit this state can become very large. Further later while exiting the editor validates that all such values are actually unique by iterating over all such values.

We should look into other possible ways to enforce uniqueness constraint",performance resilience,['upgrade'],OAK,Bug,Major,2015-03-21 12:32:52,4
12783856,SegmentOverflowException in HeavyWriteIT on Jenkins,"{noformat}
heavyWrite(org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT)  Time elapsed: 96.384 sec  <<< ERROR!
org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowException: Segment cannot have more than 255 references 47a9dc3c-c6f9-4b5f-a61a-6711da8b68c2
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.getSegmentRef(SegmentWriter.java:353)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:382)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapLeaf(SegmentWriter.java:426)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:484)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:511)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:720)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1108)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1091)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:396)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1082)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1110)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:97)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:83)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:79)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:501)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:507)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createProperties(HeavyWriteIT.java:137)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:129)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:130)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:110)
{noformat}

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/35/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/

cc [~alex.parvulescu]",CI jenkins,['segmentmk'],OAK,Bug,Critical,2015-03-21 12:03:42,15
12783748,Glob restriction test failures on Jenkins,"The following tests fail often on Jenkins:

{noformat}
testGlobRestriction2(org.apache.jackrabbit.oak.jcr.security.authorization.ReadTest): Authorizable with ID group2 already exists
testGlobRestriction3(org.apache.jackrabbit.oak.jcr.security.authorization.ReadTest): Authorizable with ID group2 already exists
{noformat}

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/33/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=unittesting/",CI jenkins,['core'],OAK,Bug,Major,2015-03-20 21:12:48,0
12783618,Failed expectations in TarMK standby tests,"I see many similar test failures for {{FailoverMultipleClientsTestIT}} and {{RecoverTestIT}} on Jenkins. For example:

{noformat}
testSyncLoop(org.apache.jackrabbit.oak.plugins.segment.standby.StandbyTestIT): expected:<{ checkpoints = { ... }, root = { ... } }> but was:<{ root : { } }>
  testLocalChanges(org.apache.jackrabbit.oak.plugins.segment.standby.RecoverTestIT): expected: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState<{ root = { ... } }> but was: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState<{ root = { ... } }>
{noformat}

See
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=integrationTesting/console
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/console
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=integrationTesting/console",CI Jenkins,['tarmk-standby'],OAK,Bug,Major,2015-03-20 14:20:42,15
12783612,Test failures in TarMK standby: Address already in use,"The following tests fail probably all for the same reason:

{noformat}
testProxySkippedBytes(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxySkippedBytesIntermediateChange(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedStartByte(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByte(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByte2(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByteChange(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByteChange2(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
{noformat}

Stacktraces always look something like:
{noformat}
java.lang.Exception: proxy not started
	at org.apache.jackrabbit.oak.plugins.segment.NetworkErrorProxy.reset(NetworkErrorProxy.java:87)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.useProxy(DataStoreTestBase.java:176)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.testProxySkippedBytes(DataStoreTestBase.java:118)
{noformat}

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=integrationTesting/console",CI jenkins,['tarmk-standby'],OAK,Bug,Major,2015-03-20 14:07:16,15
12783611,Repository Upgrade could shut down the source repository early,"I noticed that during the upgrade we can distinguish 2 phases: first copying the data from the source, then applying all the Editors (indexes and co.).
After phase 1 is done the repository upgrader could shut down the old repo to allow clearing some memory resources which might be used for the second phase.",resilience,['upgrade'],OAK,Improvement,Major,2015-03-20 14:07:07,19
12783608,Test failures in LDAP authentication: Failed to bind an LDAP service,"The following tests all fail with the same error message ""Failed to bind an LDAP service (1024) to the service registry."". 

{noformat} 
testAuthenticateFail(org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest): Failed to bind an LDAP service (1024) to the service registry.
testGetGroups2(org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest): Failed to bind an LDAP service (1024) to the service registry.
org.apache.jackrabbit.oak.security.authentication.ldap.LdapDefaultLoginModuleTest: Failed to bind an LDAP service (1024) to the service registry.
testGetUserByUserId(org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest): Failed to bind an LDAP service (1024) to the service registry.
{noformat} 

The stacktrace is always similar:

{noformat}
java.net.BindException: Address already in use]
	at org.apache.directory.server.ldap.LdapServer.startNetwork(LdapServer.java:528)
	at org.apache.directory.server.ldap.LdapServer.start(LdapServer.java:394)
	at org.apache.directory.server.unit.AbstractServerTest.setUp(AbstractServerTest.java:273)
	at org.apache.jackrabbit.oak.security.authentication.ldap.InternalLdapServer.setUp(InternalLdapServer.java:37)
	at org.apache.jackrabbit.oak.security.authentication.ldap.LdapLoginTestBase.beforeClass(LdapLoginTestBase.java:86)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.mina.transport.socket.nio.NioSocketAcceptor.open(NioSocketAcceptor.java:198)
	at org.apache.mina.transport.socket.nio.NioSocketAcceptor.open(NioSocketAcceptor.java:51)
	at org.apache.mina.core.polling.AbstractPollingIoAcceptor.registerHandles(AbstractPollingIoAcceptor.java:547)
	at org.apache.mina.core.polling.AbstractPollingIoAcceptor.access$400(AbstractPollingIoAcceptor.java:68)
	at org.apache.mina.core.polling.AbstractPollingIoAcceptor$Acceptor.run(AbstractPollingIoAcceptor.java:422)
	at org.apache.mina.util.NamePreservingRunnable.run(NamePreservingRunnable.java:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Failure seen at builds: 31, 40, 52, 62, 63, 64, 72, 95, 103, 114, 126, 136, 145, 183

See 
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=unittesting/
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=unittesting/


",CI Jenkins technical_debt,['auth-ldap'],OAK,Bug,Minor,2015-03-20 13:58:13,23
12783588,Test failure: OrderableNodesTest.testAddNode,"{{org.apache.jackrabbit.oak.jcr.OrderableNodesTest.testAddNode}} fails on Jenkins when running the {{DOCUMENT_RDB}} fixture. 

Failure seen at builds: 30, 44, 57, 58, 69, 78, 115, 121, 130, 132, 142, 152

https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/30/jdk=jdk-1.6u45,label=Ubuntu,nsfixtures=DOCUMENT_RDB,profile=unittesting/testReport/junit/org.apache.jackrabbit.oak.jcr/OrderableNodesTest/testAddNode_0_/",CI Jenkins,['rdbmk'],OAK,Bug,Major,2015-03-20 12:37:35,3
12782524,FilterImpl violates nullability contract ,"{{FilterImpl#getSupertypes}}, {{FilterImpl#getPrimaryTypes}} and {{FilterImpl#getMixinTypes}} might all return {{null}} although {{Filter}}'s contract mandates \@Nonull.",technical_debt,['core'],OAK,Bug,Major,2015-03-17 10:28:56,0
12782216,TimeSeriesMax's frequent 'drops to 0',"The current implementation of TimeSeriesMax - which is what is backing eg the very important 'ObservationQueueMaxLength' statistics - has a very infamous behavior: it does very frequent, intermittent 'jumps back to 0'. This even though the queue-lengths are still at the previous highs, as can often be seen with subsequent measurements (which eg are still showing there are 1000 events in the observation queue).

The reason seems to be that
* the value is increased via {{TimeSeriesMax.recordValue()}} during a 1 second interval
* reset to 0 via {{TimeSeriesMax<init>.run()}} every second

So basically, every second the counter is reset, then during 1 second if any call to {{recordValue()}} happens, it is increased.

This in my view is rather unfortunate - as it can result in mentioned 'jumpy-0' behavior, but it can also jump to values in between if the largest queue does not reports its length during 1 second.

It sounds a bit like this was done this way intentionally? (perhaps to make it as inexpensive as possible) or could this be fixed?",observation tooling,['core'],OAK,Bug,Major,2015-03-16 14:53:21,15
12782058,Use buffered variants for IndexInput and IndexOutput,"Lucene provides a buffered variants for {{IndexInput}} and {{IndexOutput}}. Currently Oak extends these classes directly. For better performance itshould extend the buffered variants.

As discussed [here|https://issues.apache.org/jira/browse/OAK-2222?focusedCommentId=14178265#comment-14178265]",performance,['lucene'],OAK,Improvement,Major,2015-03-15 09:45:56,19
12781804,Cleanup Oak jobs on buildbot,"Since we're moving towards Jenkins, let's remove the buildbot jobs for Oak.
The buildbot configuration is here: https://svn.apache.org/repos/infra/infrastructure/buildbot/aegis/buildmaster/master1/projects",CI,[],OAK,Sub-task,Major,2015-03-13 13:28:04,15
12781803,Cleanup Oak Travis jobs,"Since we're moving toward Jenkins, let's remove the Travis jobs for Oak. ",CI,['it'],OAK,Sub-task,Major,2015-03-13 13:27:19,2
12781712,Add test for GC of previous docs of a deleted document,As [noted|https://issues.apache.org/jira/browse/OAK-2557?focusedCommentId=14358704#comment-14358704] by [~tmueller] we do not have a test coverage for case when a deleted document has previous documents and Version GC should also remove those previous documents,CI technical_debt,['mongomk'],OAK,Task,Minor,2015-03-13 04:03:00,11
12781604,Too many reads for child nodes,"The DocumentNodeStore issues a lot of reads when sibling nodes are deleted, which are also index with a property index.

The following calls will become a hotspot:

{noformat}
	at org.apache.jackrabbit.oak.plugins.document.mongo.MongoDocumentStore.query(MongoDocumentStore.java:406)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildDocs(DocumentNodeStore.java:846)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildren(DocumentNodeStore.java:788)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getChildren(DocumentNodeStore.java:753)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeState.getChildNodeCount(DocumentNodeState.java:194)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.getChildNodeCount(ModifiedNodeState.java:198)
	at org.apache.jackrabbit.oak.plugins.memory.MutableNodeState.getChildNodeCount(MutableNodeState.java:265)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNodeCount(MemoryNodeBuilder.java:293)
	at org.apache.jackrabbit.oak.plugins.index.property.strategy.ContentMirrorStoreStrategy.prune(ContentMirrorStoreStrategy.java:456)
{noformat}

I think the code triggering this issue is in {{ModifiedNodeState.getChildNodeCount()}}. It keeps track of already deleted children and requests {{max += deleted}}. The actual {{max}} is always 1 as requested from {{ContentMirrorStoreStrategy.prune()}}, but as more nodes get deleted, the higher {{max}} gets passed to {{DocumentNodeState.getChildNodeCount()}}. The DocumentNodeStore then checks if it has the children in the cache, only to find out the cache entry has too few entries and it needs to fetch one more.

It would be best to have a minimum number of child nodes to fetch from MongoDB in this case. E.g. when NodeState.getChildNodeEntries() is called, the DocumentNodeState fetches 100 children.",performance,['mongomk'],OAK,Improvement,Major,2015-03-12 20:17:42,2
12781593,Release merge lock before branch is reset,MongoMK still holds the merge lock when it resets a persisted branch. Concurrency can be improved if the merge lock is released before the branch is reset.,concurrency technical_debt,"['core', 'mongomk']",OAK,Improvement,Major,2015-03-12 19:40:10,2
12781550,Repeated upgrades,"When upgrading from Jackrabbit 2 to Oak there are several scenarios that could benefit from the ability to upgrade repeatedly into one target repository.

E.g. a migration process might look as follows:

# upgrade a backup of a large repository a week before go-live
# run the upgrade again every night (commit-hooks only handle delta)
# run the upgrade one final time before go-live (commit-hooks only handle delta)

In this scenario each upgrade would require a full traversal of the source repository. However, if done right, only the delta needs to be written and the commit-hooks also only need to process the delta.
",doc-impacting,['upgrade'],OAK,New Feature,Minor,2015-03-12 16:47:54,13
12781511,Improve performance of queries with ORDER BY and multiple OR filters,"When multiple OR constraints are specified in the XPATH query, itis broken up into union of multiple clauses. If query includes an order by clause, the sorting in this case is done by traversing the result set in memory leading to slow query performance.

Possible improvements could include:
* For indexes which can support multiple filters (like lucene, solr) such queries should be efficient and the query engine can pass-thru the query as is.
** Possibly also needed for other cases also. So, we can have some sort of capability advertiser for indexes which can hint the query engine 

and/or
* Batched merging of the sorted iterators returned for the multiple union queries (possible externally).",performance,['query'],OAK,Improvement,Major,2015-03-12 14:21:24,16
12781432,Annotate intermediate docs with property names,"Reading through a ValueMap can be very inefficient if the changes of a given
property are distributed sparsely across the previous documents. The current
implementation has to scan through the entire set of previous documents to
collect the changes.

Intermediate documents should have additional information about what properties
are present on referenced previous documents. 
",performance,['mongomk'],OAK,Improvement,Major,2015-03-12 08:17:33,2
12781228,Thread.interrupt seems to stop repository,"We have a sporadic problem with Sling's JCR installer 3.3.8 and Oak (tar mk). It seems to timing related: the JCR installer does a Thread#interrupt at one point and sometimes this brings the hole instance to stop. Nothing else is going on any more. 
While of course, a workaround is to remove the Thread.interrupt call in the JCR installer (which we did, see SLING-4477), I have the fear that this can happen with any code that is using the repository and gets interrupted.
This error is hard to reproduce, however with three people testing we could see this several times happening",doc-impacting resilience,['doc'],OAK,Improvement,Critical,2015-03-11 15:59:29,15
12780726,Allow excluding certain paths from getting indexed for particular index,"Currently an {{IndexEditor}} gets to index all nodes under the tree where it is defined (post OAK-1980).  Due to this IndexEditor would traverse the whole repo (or subtree if configured in non root path) to perform reindex. Depending on the repo size this process can take quite a bit of time. It would be faster if an IndexEditor can exclude certain paths from traversal

Consider an application like Adobe AEM and an index which only index dam:Asset or the default full text index. For a fulltext index it might make sense to avoid indexing the versionStore. So if the index editor skips such path then lots of redundant traversal can be avoided. 

Also see http://markmail.org/thread/4cuuicakagi6av4v",performance,"['core', 'query']",OAK,New Feature,Major,2015-03-10 06:17:29,19
12780520,more (jmx) instrumentation for observation queue,"While debugging issues with the observation queue it would be handy to have more detailed information available. At the moment you can only see one value wrt length of the queue: that is the maximum of all queues. It is unclear if the queue is that long for only one or many listeners. And it is unclear from that if the listener is slow or the engine that produces the events for the listener.

So I'd suggest to add the following details - possible exposed via JMX? :
# add queue length details to each of the observation listeners
# have a history of the last, eg 1000 events per listener showing a) how long the event took to be created/generated and b) how long the listener took to process. Sometimes averages are not detailed enough so such a in-depth information might become useful. (Not sure about the feasibility of '1000' here - maybe that could be configurable though - just putting the idea out here).
# have some information about whether a listener is currently 'reading events from the cache' or whether it has to go to eg mongo 
# maybe have a 'top 10' listeners that have the largest queue at the moment to easily allow navigation instead of having to go through all (eg 200) listeners manually each time.
",monitoring observation,['core'],OAK,Improvement,Blocker,2015-03-09 15:51:07,15
12780517,High memory consumption of CompactionGainEstimate,"{{CompactionGainEstimate}} keeps a set for the visited record ids. Each entry in that set is represented by an instance of {{ThinRecordId}}. For big repositories the instance overhead lead to {{OOME}} while running the compaction estimator. 
",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-03-09 15:43:30,15
12780441,Commit does not ensure w:majority,"The MongoDocumentStore uses {{findAndModify()}} to commit a transaction. This operation does not allow an application specified write concern and always uses the MongoDB default write concern {{Acknowledged}}. This means a commit may not make it to a majority of a replica set when the primary fails. From a MongoDocumentStore perspective it may appear as if a write was successful and later reverted. See also the test in OAK-1641.

To fix this, we'd probably have to change the MongoDocumentStore to avoid {{findAndModify()}} and use {{update()}} instead.",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-03-09 09:38:12,2
12780152,observation processing too eager/unfair under load,"The current implementation of oak's observation event processing is too eager and thus unfair under load scenarios. 

Consider having many (eg 200) Eventlisteners but only a relatively small threadpool (eg 5 as is the default in sling) backing them. When processing changes for a particular BackgroundObserver, that one (in BackgroundObserver.completionHandler.call) currently processes *all changes irrespective of how many there are* - ie it is *eager*. Only once that BackgroundObserver processed all changes will it let go and 'pass the thread' to the next BackgroundObserver. Now if for some reason changes (ie commits) are coming in while a BackgroundObserver is busy processing an earlier change, this will lengthen that while loop. As a result the remaining (eg 195) *EventListeners will have to wait for a potentially long time* until it's their turn - thus *unfair*.

Now combine the above pattern with a scenario where mongo is used as the underlying store. In that case in order to remain highly performant it is important that the diffs (for compareAgainstBaseState) are served from the MongoDiffCache for as many cases as possible to avoid doing a round-trip to mongoD. The unfairness in the BackgroundObservers can now result in a large delay between the 'first' observers getting the event and the 'last' one (of those 200). When this delay increases due to a burst in the load, there is a risk of the diffs to no longer be in the cache - those last observers are basically kicked out of the (diff) cache. Once this happens, *the situation gets even worse*, since now you have yet new commits coming in and old changes still having to be processed - all of which are being processed through in 'stripes of 5 listeners' before the next one gets a chance. This at some point results in a totally inefficient cache behavior, or in other words, at some point all diffs have to be read from mongoD.

To avoid this there are probably a number of options - a few one that come to mind:
* increase thread-pool to match or be closer to the number of listeners (but this has other disadvantages, eg cost of thread-switching)
* make BackgroundObservers fairer by limiting the number of changes they process before they give others a chance to be served by the pool.",observation,['core'],OAK,Improvement,Critical,2015-03-06 19:34:26,15
12780035,Set pauseCompaction default to false,As we start seeing good results with the current approach to compaction I'd like to have it running per default. This allows us to gather more information while we are running up towards the 1.2 release. ,compaction gc,['segmentmk'],OAK,Improvement,Major,2015-03-06 10:33:30,15
12779689,RepositoryManager must not register WhiteboardExecutor with Oak,"{{org.apache.jackrabbit.oak.jcr.osgi.RepositoryManager}} currently registers the {{WhiteboardExecutor}} with Oak which internally again register with OSGi ServiceRegistry. This causes recursion as leading to stackoverflow.

As Oak creates an {{Executor}} in absence on explicitly provided one RepositoryManager should not set the {{Executor}}",osgi,['jcr'],OAK,Improvement,Minor,2015-03-05 05:08:53,19
12779007,SessionMBean should provide information about pending refresh,"The session auto refresh feature is implemented by marking sessions pending for refresh. The refresh operation itself however only happens on the next access to the session. 

It would be helpful if {{SessionMBean}} could expose the information whether a session has a pending refresh. Additionally we could expose the current {{RefreshStrategy}} to make the auto refresh behaviour more transparent. ",gc monitoring,['core'],OAK,Sub-task,Major,2015-03-03 10:25:15,15
12778740,SegmentStore deadlock on shutdown,"We've ran into a deadlock in the Oak SegmentNodeStore between
_org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.deactivate(SegmentNodeStoreService.java:300)_ and _org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:120)_
This was seen on Oak 1.0.11.

{code}
Java stack information for the threads listed above 
 
""pool-9-thread-5"":
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:656)
	- waiting to lock <0x0000000680024468> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:120)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:101)
	- locked <0x00000006b0b7ae28> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.size(MapRecord.java:128)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:462)
	- locked <0x0000000680c634a8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:460)
	- locked <0x0000000680c634a8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:660)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1020)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1022)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1003)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:396)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:994)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1003)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:396)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:994)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:91)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:77)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:73)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setChildNode(MemoryNodeBuilder.java:331)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setChildNode(MemoryNodeBuilder.java:323)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.child(MemoryNodeBuilder.java:311)
	at org.apache.jackrabbit.oak.plugins.index.property.strategy.ContentMirrorStoreStrategy.insert(ContentMirrorStoreStrategy.java:112)
	at org.apache.jackrabbit.oak.plugins.index.property.strategy.ContentMirrorStoreStrategy.update(ContentMirrorStoreStrategy.java:81)
	at org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexEditor.leave(PropertyIndexEditor.java:261)
	at org.apache.jackrabbit.oak.spi.commit.CompositeEditor.leave(CompositeEditor.java:74)
	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeAdded(EditorDiff.java:130)
	at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:391)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeAdded(EditorDiff.java:125)
	at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:391)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeAdded(EditorDiff.java:125)
	at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:391)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeAdded(EditorDiff.java:125)
	at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:391)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeAdded(EditorDiff.java:125)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$2.childNodeAdded(MapRecord.java:383)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$3.childNodeAdded(MapRecord.java:425)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:483)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:422)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:380)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:540)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(EditorDiff.java:148)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$2.childNodeChanged(MapRecord.java:389)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$3.childNodeChanged(MapRecord.java:430)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:473)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:422)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:380)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:540)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(EditorDiff.java:148)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$2.childNodeChanged(MapRecord.java:389)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$3.childNodeChanged(MapRecord.java:430)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:473)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:422)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:380)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:540)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(EditorDiff.java:148)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$2.childNodeChanged(MapRecord.java:389)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:473)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:380)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:540)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:354)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:300)
	- locked <0x000000068afa00e8> (a org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate)
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105)
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
 ""FelixStartLevel"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:167)
	- waiting to lock <0x0000000680c634a8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:431)
	- locked <0x000000068461ecb8> (a java.util.concurrent.atomic.AtomicReference)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.close(FileStore.java:582)
	- locked <0x0000000680024468> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.deactivate(SegmentNodeStoreService.java:300)
	- locked <0x00000006800243c8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.dispose(AbstractComponentManager.java:580)
	at org.apache.felix.scr.impl.config.ConfigurableComponentHolder.disposeComponents(ConfigurableComponentHolder.java:406)
	at org.apache.felix.scr.impl.BundleComponentActivator.dispose(BundleComponentActivator.java:335)
	at org.apache.felix.scr.impl.Activator.disposeComponents(Activator.java:313)
	at org.apache.felix.scr.impl.Activator.access$300(Activator.java:45)
	at org.apache.felix.scr.impl.Activator$ScrExtension.destroy(Activator.java:198)
	at org.apache.felix.utils.extender.AbstractExtender$2.run(AbstractExtender.java:290)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at org.apache.felix.utils.extender.AbstractExtender.destroyExtension(AbstractExtender.java:312)
	at org.apache.felix.utils.extender.AbstractExtender.bundleChanged(AbstractExtender.java:186)
	at org.apache.felix.framework.util.EventDispatcher.invokeBundleListenerCallback(EventDispatcher.java:869)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:790)
	at org.apache.felix.framework.util.EventDispatcher.fireBundleEvent(EventDispatcher.java:515)
	at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4409)
	at org.apache.felix.framework.Felix.stopBundle(Felix.java:2526)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1315)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)
	at java.lang.Thread.run(Thread.java:744)
{code}",resilience,['segmentmk'],OAK,Bug,Major,2015-03-02 11:33:57,14
12778255,Intermediate commit during async indexing,"A recent issue found at a customer unveils a potential issue with the async indexer. Reading the AsyncIndexUpdate.updateIndex it looks like it is doing the entire update of the async indexer *in one go*, ie in one commit.

When there is - for some reason - however, a huge diff that the async indexer has to process, the 'one big commit' can become gigantic. There is no limit to the size of the commit in fact.

So the suggestion is to do intermediate commits while the async indexer is going on. The reason this is acceptable is the fact that by doing async indexing, that index is anyway not 100% up-to-date - so it would not make much of a difference if it would commit after every 100 or 1000 changes either.",resilience,['lucene'],OAK,Improvement,Major,2015-02-27 17:34:43,4
12777494,Implement MBean monitoring garbage collection,"Provide monitoring for the garbage collection process:
* time series of repository size
* time series of space reclaimed
* time stamp of last clean up
* time stamp of last compaction
* last error
* time stamp when next gc run is scheduled
* ...",compaction gc monitoring,['segmentmk'],OAK,Sub-task,Major,2015-02-25 11:23:29,15
12776807,SegmentMk IT tests are too intensive,"Seen on the 1.0 branch only so far when running ITs on my local machine, but travis reports the same:

https://travis-ci.org/apache/jackrabbit-oak/builds/51589769

It doesn't necessarily mean the problem is with SegmentReferenceLimitTestIT even though the heap dump shows most of the memory consumed by Segments and SegmentWriter. A recent build on trunk was successful for me where we have the same test.",CI travis,"['core', 'segmentmk']",OAK,Test,Major,2015-02-23 11:00:23,14
12776197,Support index time aggregation in Solr index,"Solr index is only able to do query time aggregation while that ""would not perform well for multi term searches as each term involves a separate call and with intersection cursor being used the operation might result in reading up all match terms even when user accesses only first page"", therefore it'd be good to implement index time aggregation like in Lucene index. (/cc [~chetanm])",performance,['solr'],OAK,Improvement,Major,2015-02-19 16:00:50,10
12774577,Truncate journal.log after off line compaction,After off line compaction the repository contains a single revision. However the journal.log file will still contain the trail of all revisions that have been removed during the compaction process. I suggest we truncate the journal.log to only contain the latest revision created during compaction.,compaction gc,['segmentmk'],OAK,New Feature,Minor,2015-02-12 14:20:06,15
12774256,Provide initial implementation of the Remote Operations specification,"To provide something that can be played with, and to verify the feasibility of the specification draft, an initial implementation of the HTTP API should be provided.

The API should follow the general behavior described [here|https://wiki.apache.org/jackrabbit/frm/RemoteOperations] and the HTTP semantics defined [here|https://wiki.apache.org/jackrabbit/frm/HttpOperations]. ",api remoting,['remoting'],OAK,Sub-task,Minor,2015-02-11 16:34:37,12
12773875,Root record references provide too little context for parsing a segment,"According to the [documentation | http://jackrabbit.apache.org/oak/docs/nodestore/segmentmk.html] the root record references in a segment header provide enough context for parsing all records within this segment without any external information. 

Turns out this is not true: if a root record reference turns e.g. to a list record. The items in that list are record ids of unknown type. So even though those records might live in the same segment, we can't parse them as we don't know their type. ",tools,['segment-tar'],OAK,Bug,Major,2015-02-10 15:50:46,12
12773538,Flag Document having many children,"Current DocumentMK logic while performing a diff for child nodes works as below

# Get children for _before_ revision upto MANY_CHILDREN_THRESHOLD (which defaults to 50). Further note that current logic of fetching children nodes also add children {{NodeDocument}} to {{Document}} cache and also reads the complete Document for those children
# Get children for _after_ revision with limits as above
# If the child list is complete then it does a direct diff on the fetched children
# if the list is not complete i.e. number of children are more than the threshold then it for a query based diff (also see OAK-1970)

So in those cases where number of children are large then all work done in #1 above is wasted and should be avoided. To do that we can mark those parent nodes which have many children via special flag like {{_manyChildren}}. One such nodes are marked the diff logic can check for the flag and skip the work done in #1

This is kind of similar to way we mark nodes which have at least one child (OAK-1117)

",performance,['mongomk'],OAK,Improvement,Major,2015-02-09 18:58:38,19
12772648,Move spellcheck config to own configuration node,Currently spellcheck configuration is controlled via properties defined on main config / props node but it'd be good if we would have its own place to configure the whole spellcheck feature to not mix up configuration of other features / parameters.,docs-impacting technical_debt,['lucene'],OAK,Improvement,Major,2015-02-05 10:06:57,11
12772647,Move suggester specific config to own configuration node,Currently suggester configuration is controlled via properties defined on main config / props node but it'd be good if we would have its own place to configure the whole suggest feature to not mix up configuration of other features / parameters.,docs-impacting technical_debt,['lucene'],OAK,Improvement,Major,2015-02-05 10:06:13,11
12772397,Move our CI to Jenkins,"We should strive for stabilization of our CI setup, as of now we had Buildbot and Travis.
It seems ASF Jenkins can perform jobs on different environments (*nix, Windows and others) so we can evaluate that and check if it better address our needs.",CI build infrastructure,[],OAK,Task,Critical,2015-02-04 15:35:23,15
12771991,Add support for atomic counters on cluster solutions,"As of OAK-2220 we added support for atomic counters in a non-clustered situation. 

This ticket is about covering the clustered ones.",scalability,['core'],OAK,Improvement,Blocker,2015-02-03 10:02:47,22
12771304,DataStoreBlobStore: chunk ids should not contain the size,"The blob store garbage collection (data store garbage collection) uses the chunk ids to identify binaries to be deleted. The blob ids contain the size now (<contentHash>#<size>), and the blob id is currently equal to the chunk id.

It would be more efficient to _not_ use the size, and instead just use the content hash, for the chunk ids. That way, enumerating the entries that are in the store is potentially faster. Also, it allows us to change the blob id in the future, for example add more information to it (for example the creation time, or the first few bytes of the content) if we ever want to.",datastore performance,['blob'],OAK,Improvement,Major,2015-01-30 10:28:29,4
12770899,Resolve the base directory path of persistent cache against repository home,"Currently PersistentCache uses the directory path directly. Various other parts in Oak which need access to the filesystem currently make use of {{repository.home}} framework property in OSGi env [1]

Same should also be used in PersistentCache

[1] http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore ",technical_debt,"['core', 'documentmk']",OAK,Improvement,Minor,2015-01-29 03:30:18,2
12768421,Support continuable sessions ,"Implement support for continuable sessions to keeps state across multiple client/server interactions. Continuable sessions do not require any additional state on the server (i.e. Oak) apart form the apparent repository state. 

To continue a session a client would obtain a continuation token from the current session. This token can be used on the next call to {{Repository.login}} to obtain a new {{Session}} instance that is based on the same repository revision that the session the token was obtained from. Additionally the token could contain information re. authentication so subsequent request can go through a simplified authentication procedure. ([~asanso]'s work on OAuth might be of help here.)

Transient changes are not supported in continuable sessions. Obtaining a continuation token from a session with transient changes results in an error. 

Continuable sessions are typically short lived (i.e. the time of a single HTTP request). Specifically continuable session do not retain the underlying repository revision from being garbage collected. Clients need to be able to cope with respective exceptions. 



",api,['core'],OAK,Sub-task,Major,2015-01-19 11:23:51,12
12768418,Oak API remoting,"Container issues for collecting tasks related to remoting the Oak API. Such a remoting should be:

* stateless on the Oak side apart from the apparent persisted state in the content repository, 

* independent from {{oak-jcr}}, but reusing JCR related plugins from {{oak-core}} as required (e.g. for name space and node type handling),

* agnostic of any protocol bindings,

* ...",api remoting,['remoting'],OAK,Task,Major,2015-01-19 11:10:06,12
12768365,[sonar]Some statements not being closed in RDBDocumentStore,"The Sonar analysis for Oak [1] report shows some warnings for RDBDocumentStore [2]. The critical ones are related to not closing of statement and resultset instances. 

[1] https://analysis.apache.org/dashboard/index/org.apache.jackrabbit:jackrabbit-oak?did=2
[2] https://analysis.apache.org/resource/index/179726?display_title=true&tab=violations",technical_debt,['rdbmk'],OAK,Sub-task,Minor,2015-01-19 06:08:56,3
12767764,Investigate ways to make revision gc more precise ,"Current approaches to revision garbage collection tend to be too conservative (too little space reclaimed, e.g. OAK-2045) or too aggressive (removing segments still being used, e.g. OAK-2384). 

This issue is to explore ways to make revision gc on TarMk more precise. ",gc,['segmentmk'],OAK,Task,Major,2015-01-15 15:58:05,15
12767762,Auto-refresh sessions on revision gc,"The approach to revision garbage collection taken in OAK-2192 assumes that long running background sessions call refresh once they become active again. Incidentally this is true as such background sessions usually are admin sessions and those are always auto-refreshed on access (see OAK-88, OAK-803, and OAK-960). However as soon as we move away from admin sessions this might not be true any more and we might start seeing {{SegmentNotFoundException}} s unless the user explicitly refreshes the session. 

To prevent this we should make all sessions auto refresh once revision gc runs. ",gc,['segmentmk'],OAK,Improvement,Major,2015-01-15 15:51:07,15
12767745,Monitoring to track old NodeStates,"We should add some monitoring that allows us to track ""old"" node states, which potentially block revision gc. 

Possible approaches:

* Add monitoring too old revisions (root node states) along with the stack traces from where they have been acquired.

* Include RecordId of root node state in the {{SessionMBean}}.

* Add additional tooling on top of the {{SessionMBean}} to make it easier to make sense of the wealth of information provided. 
",gc monitoring tooling,['segment-tar'],OAK,Sub-task,Major,2015-01-15 15:08:42,15
12767742,Provide more information in SegmentNotFoundException,"There is currently no way to distinguish between a {{SegmentNotFoundException}} occurring because of a removed segment by gc or because of another corruption. Optimally we would tell in the exception why the segment is gone, how old it was when gc removed it and who/what was still referring to it at that time. In order to do that, we probably need some kind of log for the following data: When a segment was removed (because a new generation of the .tar file was made, or because the .tar file was removed), we should log the segment, the file name, and the date+time of the removal. If the segment was then not found because it was too old, then another type of exception should be thrown instead, for example ""ReadTimeoutException"", with a message that contains as much data as possible: the data+time of the segment, date+time of the removal of the segment, about when compaction was run, date+time of the session login and last refresh, the stack trace of where the session was acquired.",gc monitoring,['segmentmk'],OAK,Sub-task,Major,2015-01-15 15:05:49,15
12767740,Improve monitoring capabilities for TarMk revision gc,"Container devoted to improving monitoring of the TarMk revision garbage collection process. The overall goal is to make it more transparent what revision gc does, how it performs, why it failed etc. ",gc monitoring tooling,['segment-tar'],OAK,Task,Major,2015-01-15 14:59:39,15
12767690,SegmentNodeStoreService prone to deadlocks,"The SegmentNodeStoreService is prone to deadlocks because of the way in which is synchronizes access to the _SegmentNodeStore_ delegate.

The issue can now be seen on #deactivate, when the deregistration is being synchronously broadcast and if a referring service calls #getNodeStore the deadlock happens.

{code}
Found one Java-level deadlock:
=============================
""qtp844483043-936"":
  waiting to lock monitor 0x000001d1aacc7208 (object 0x000001d231f52698, a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService),
  which is held by ""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)""
""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)"":
  waiting to lock monitor 0x000001d4d0907c88 (object 0x000001d2334be930, a java.lang.Object),
  which is held by ""pool-5-thread-4""
""pool-5-thread-4"":
  waiting to lock monitor 0x000001d1aacc7208 (object 0x000001d231f52698, a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService),
  which is held by ""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)""

Java stack information for the threads listed above:
===================================================
""qtp844483043-936"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:144)
	- waiting to lock <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:73)
	at org.apache.jackrabbit.oak.spi.state.ProxyNodeStore.getRoot(ProxyNodeStore.java:35)
	at org.apache.jackrabbit.oak.core.MutableRoot.<init>(MutableRoot.java:160)
	at org.apache.jackrabbit.oak.core.ContentSessionImpl.getLatestRoot(ContentSessionImpl.java:110)
	at org.apache.jackrabbit.oak.spi.security.authentication.AbstractLoginModule.getRoot(AbstractLoginModule.java:403)
	at org.apache.jackrabbit.oak.security.authentication.token.TokenLoginModule.getTokenProvider(TokenLoginModule.java:215)
	at org.apache.jackrabbit.oak.security.authentication.token.TokenLoginModule.login(TokenLoginModule.java:128)
	at org.apache.felix.jaas.boot.ProxyLoginModule.login(ProxyLoginModule.java:52)
	at sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:161)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:256)
	at com.adobe.granite.repository.impl.CRX3RepositoryImpl.login(CRX3RepositoryImpl.java:92)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:197)
	at org.apache.sling.jcr.base.AbstractSlingRepository2.login(AbstractSlingRepository2.java:297)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProviderInternal(JcrResourceProviderFactory.java:289)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProvider(JcrResourceProviderFactory.java:201)
	at org.apache.sling.resourceresolver.impl.tree.ResourceProviderFactoryHandler.login(ResourceProviderFactoryHandler.java:164)
	at org.apache.sling.resourceresolver.impl.tree.RootResourceProviderEntry.loginToRequiredFactories(RootResourceProviderEntry.java:95)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolverInternal(CommonResourceResolverFactoryImpl.java:109)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolver(CommonResourceResolverFactoryImpl.java:90)
	at org.apache.sling.resourceresolver.impl.ResourceResolverFactoryImpl.getResourceResolver(ResourceResolverFactoryImpl.java:93)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.getAnonymousResolver(SlingAuthenticator.java:839)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.doHandleSecurity(SlingAuthenticator.java:478)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.handleSecurity(SlingAuthenticator.java:438)
	at org.apache.sling.engine.impl.SlingHttpContext.handleSecurity(SlingHttpContext.java:121)
	at org.apache.felix.http.base.internal.context.ServletContextImpl.handleSecurity(ServletContextImpl.java:335)
	at org.apache.felix.http.base.internal.handler.ServletHandler.doHandle(ServletHandler.java:337)
	at org.apache.felix.http.base.internal.handler.ServletHandler.handle(ServletHandler.java:300)
	at org.apache.felix.http.base.internal.dispatch.ServletPipeline.handle(ServletPipeline.java:93)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:50)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.i18n.impl.I18NFilter.doFilter(I18NFilter.java:128)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.sslfilter.internal.SslFilter.doFilter(SslFilter.java:55)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.sslfilter.internal.SslFilter.doFilter(SslFilter.java:89)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at com.adobe.granite.license.impl.LicenseCheckFilter.doFilter(LicenseCheckFilter.java:298)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.security.impl.ReferrerFilter.doFilter(ReferrerFilter.java:290)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.featureflags.impl.FeatureManager.doFilter(FeatureManager.java:115)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.engine.impl.log.RequestLoggerFilter.doFilter(RequestLoggerFilter.java:75)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.base.internal.dispatch.FilterPipeline.dispatch(FilterPipeline.java:76)
	at org.apache.felix.http.base.internal.dispatch.Dispatcher.dispatch(Dispatcher.java:49)
	at org.apache.felix.http.base.internal.DispatcherServlet.service(DispatcherServlet.java:67)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:722)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:229)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)"":
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.unbindTopologyEventListener(DiscoveryServiceImpl.java:242)
	- waiting to lock <0x000001d2334be930> (a java.lang.Object)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.BindMethod.invoke(BindMethod.java:37)
	at org.apache.felix.scr.impl.manager.DependencyManager.invokeUnbindMethod(DependencyManager.java:1717)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.invokeUnbindMethod(SingleComponentManager.java:404)
	at org.apache.felix.scr.impl.manager.DependencyManager$MultipleDynamicCustomizer.removedService(DependencyManager.java:376)
	at org.apache.felix.scr.impl.manager.DependencyManager$MultipleDynamicCustomizer.removedService(DependencyManager.java:304)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.unregister(AbstractComponentManager.java:1011)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.unregister(AbstractComponentManager.java:992)
	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:141)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.unregisterService(AbstractComponentManager.java:1054)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:900)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:974)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:895)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.unregisterService(AbstractSlingRepositoryManager.java:258)
	at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.stop(AbstractSlingRepositoryManager.java:345)
	at com.adobe.granite.repository.impl.SlingRepositoryManager.deactivate(SlingRepositoryManager.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:974)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:895)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.unregisterNodeStore(SegmentNodeStoreService.java:320)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.deactivate(SegmentNodeStoreService.java:295)
	- locked <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.reconfigure(SingleComponentManager.java:638)
	at org.apache.felix.scr.impl.config.ConfigurableComponentHolder.configurationUpdated(ConfigurableComponentHolder.java:328)
	at org.apache.felix.scr.impl.config.ConfigurationSupport.configurationEvent(ConfigurationSupport.java:290)
	at org.apache.felix.cm.impl.ConfigurationManager$FireConfigurationEvent.sendEvent(ConfigurationManager.java:2032)
	at org.apache.felix.cm.impl.ConfigurationManager$FireConfigurationEvent.run(ConfigurationManager.java:2002)
	at org.apache.felix.cm.impl.UpdateThread.run(UpdateThread.java:103)
	at java.lang.Thread.run(Thread.java:745)
""pool-5-thread-4"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:144)
	- waiting to lock <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:73)
	at org.apache.jackrabbit.oak.spi.state.ProxyNodeStore.getRoot(ProxyNodeStore.java:35)
	at org.apache.jackrabbit.oak.core.MutableRoot.<init>(MutableRoot.java:160)
	at org.apache.jackrabbit.oak.core.ContentSessionImpl.getLatestRoot(ContentSessionImpl.java:110)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.<init>(SessionDelegate.java:160)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$1.<init>(RepositoryImpl.java:273)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.createSessionDelegate(RepositoryImpl.java:271)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:257)
	at com.adobe.granite.repository.impl.CRX3RepositoryImpl.login(CRX3RepositoryImpl.java:92)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl$2.run(SlingRepositoryImpl.java:108)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl$2.run(SlingRepositoryImpl.java:100)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAsPrivileged(Subject.java:536)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl.createAdministrativeSession(SlingRepositoryImpl.java:100)
	at org.apache.sling.jcr.base.AbstractSlingRepository2.loginAdministrative(AbstractSlingRepository2.java:362)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProviderInternal(JcrResourceProviderFactory.java:246)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getAdministrativeResourceProvider(JcrResourceProviderFactory.java:209)
	at org.apache.sling.resourceresolver.impl.tree.ResourceProviderFactoryHandler.login(ResourceProviderFactoryHandler.java:162)
	at org.apache.sling.resourceresolver.impl.tree.RootResourceProviderEntry.loginToRequiredFactories(RootResourceProviderEntry.java:95)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolverInternal(CommonResourceResolverFactoryImpl.java:109)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getAdministrativeResourceResolver(CommonResourceResolverFactoryImpl.java:76)
	at org.apache.sling.resourceresolver.impl.ResourceResolverFactoryImpl.getAdministrativeResourceResolver(ResourceResolverFactoryImpl.java:98)
	at org.apache.sling.discovery.impl.cluster.ClusterViewServiceImpl.getClusterView(ClusterViewServiceImpl.java:132)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.getTopology(DiscoveryServiceImpl.java:418)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.handlePotentialTopologyChange(DiscoveryServiceImpl.java:466)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.handleTopologyChanged(DiscoveryServiceImpl.java:650)
	- locked <0x000001d2334be930> (a java.lang.Object)
	at org.apache.sling.discovery.impl.topology.TopologyChangeHandler.handleTopologyChanged(TopologyChangeHandler.java:134)
	at org.apache.sling.discovery.impl.topology.TopologyChangeHandler.handleEvent(TopologyChangeHandler.java:124)
	at org.apache.felix.eventadmin.impl.handler.EventHandlerProxy.sendEvent(EventHandlerProxy.java:412)
	at org.apache.felix.eventadmin.impl.tasks.SyncDeliverTasks.execute(SyncDeliverTasks.java:118)
	at org.apache.felix.eventadmin.impl.handler.EventAdminImpl.sendEvent(EventAdminImpl.java:114)
	at org.apache.felix.eventadmin.impl.security.EventAdminSecurityDecorator.sendEvent(EventAdminSecurityDecorator.java:96)
	at org.apache.sling.jcr.resource.internal.OakResourceListener.sendOsgiEvent(OakResourceListener.java:243)
	at org.apache.sling.jcr.resource.internal.OakResourceListener.changed(OakResourceListener.java:133)
	at org.apache.jackrabbit.oak.plugins.observation.NodeObserver$NodeEventHandler.leave(NodeObserver.java:208)
	at org.apache.jackrabbit.oak.plugins.observation.FilteredHandler.leave(FilteredHandler.java:51)
	at org.apache.jackrabbit.oak.plugins.observation.EventGenerator$Continuation.run(EventGenerator.java:175)
	at org.apache.jackrabbit.oak.plugins.observation.EventGenerator.generate(EventGenerator.java:118)
	at org.apache.jackrabbit.oak.plugins.observation.NodeObserver.contentChanged(NodeObserver.java:156)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:117)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:111)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Found 1 deadlock.
{code}",resilience,['segmentmk'],OAK,Bug,Blocker,2015-01-15 09:29:50,19
12765997,SegmentNotFoundException when keeping JCR Value references,"With OAK-2192 revision gc started to remove segments older than a certain threshold. The underlying assumption was that old sessions would call refresh (i.e. auto refresh) anyway once they become active again. However, it turns out that refreshing a sessions does not affect JCR values as those are directly tied to the underlying record. Accessing those values after its segment has been gc'ed results in a {{SegmentNotFoundException}}. 

Keeping reference to JCR values is an important use case for Sling's {{JcrPropertyMap}}, which is widely used.",gc,['core'],OAK,Bug,Critical,2015-01-09 11:21:56,15
12765321,Regular CI failures for DOCUMENT_RDB on buildbot ,"There are many tests failing on http://ci.apache.org/builders/oak-trunk for the {{DOCUMENT_RDB}} fixture:

{noformat}
  addNodes[3](org.apache.jackrabbit.oak.jcr.ConcurrentAddIT): expected:<100> but was:<99>
  testMVNameProperty[3](org.apache.jackrabbit.oak.jcr.NameAndPathPropertyTest): org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0001: OakMerge0001: Failed to merge changes to the underlying store (retries 5, 5432 ms)
  testMVNameProperty[3](org.apache.jackrabbit.oak.jcr.NameAndPathPropertyTest): org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: java.sql.SQLException: Data source is closed
  testMVPathProperty[3](org.apache.jackrabbit.oak.jcr.NameAndPathPropertyTest): Branch with failed reset
  testMVPathProperty[3](org.apache.jackrabbit.oak.jcr.NameAndPathPropertyTest): java.sql.SQLException: Data source is closed
  testInvalidPathProperty[3](org.apache.jackrabbit.oak.jcr.NameAndPathPropertyTest): initializing RDB blob store
  orderableFolder[3](org.apache.jackrabbit.oak.jcr.OrderableNodesTest): java.sql.SQLException: Data source is closed
  orderableFolder[3](org.apache.jackrabbit.oak.jcr.OrderableNodesTest): java.sql.SQLException: Data source is closed
{noformat}

And many more. See e.g. http://ci.apache.org/builders/oak-trunk/builds/890/steps/compile/logs/stdio


",CI buildbot,['jcr'],OAK,Bug,Major,2015-01-07 08:20:57,3
12762836,Sporadic test failure of OSGiIT.listBundles on Buildbot,"See http://markmail.org/message/idx2y2dwpkaxchsp for previous mention.

I suggest to use the mechanism from OAK-2371 to exclude the tests on that CI environment for now. ",CI buildbot test,['it'],OAK,Bug,Minor,2014-12-19 12:37:16,19
12762290,SegmentMK thin consistency check on startup,"Following OAK-2323, I'd like to add a thin consistency check on startup to verify if the current rev is broken by some pending transaction or not.

The current startup check behavior is too lazy and it can miss some broken repos where the super root is only partly persisted.

{code}
java.lang.IllegalStateException: String is too long: 4531747125156176000
at org.apache.jackrabbit.oak.plugins.segment.Segment.loadString(Segment.java:344)
at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:311)
at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:305)
at org.apache.jackrabbit.oak.plugins.segment.Segment.loadTemplate(Segment.java:388)
at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:359)
at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:353)
at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:74)
{code}",production resilience,['segmentmk'],OAK,Bug,Major,2014-12-17 14:01:16,14
12760809,Wrong handling of InterruptedException in BackgroundThread,{{BackgroundThread}} catches {{InterruptedException}} but doesn't set the thread's interrupted status. ,resilience,['core'],OAK,Bug,Major,2014-12-10 17:31:47,15
12760088,SegmentMK consistency check,We need a tool to check a SegmentMK repository for consistency. Such a tool should start at the most recent version in the journal and traverse back until it finds the latest good revision. ,offline production resilience tools,['run'],OAK,New Feature,Major,2014-12-08 08:54:05,15
12759635,Compaction estimation includes all data segments,"The original design of the compaction estimator was to only focus on binary segments when computing the amount of garbage that can be collected.
This has a flaw when it runs on systems with external data stores configured as there will be almost no binary segments and the estimation will always be _0_, no need for compaction.

The change to include all segments is not big, the _uuids_ bloomfilter is already initialized with the total number of segments, and the segment collection already loads everything in memory.
",compaction gc,"['core', 'segmentmk']",OAK,Bug,Major,2014-12-05 09:09:58,14
12758755,Better handling for external binaries in the segment explorer,The Segment Explorer should be able to (more) gracefully print the info for external binaries when there's no BlobStore hooked in.,tools,"['run', 'segmentmk']",OAK,Improvement,Major,2014-12-01 21:29:04,14
12757748,Corrupt repository after concurrent version operations,"Performing version operations (checkin / checkout / addVersionLabel) concurrently can corrupt the repository. 

Executing the following code in parallel from multiple threads demonstrates this:
{code}
Version version = versionManager.checkin(vPath);
versionManager.checkout(vPath);
String label = version.getName() + "" "" + Thread.currentThread().getName();
version.getContainingHistory()
    .addVersionLabel(version.getName(), label, true);
{code}

In my tests this eventually lead to all sorts of exceptions:

{noformat}
java.lang.IllegalStateException: RefId '85' doesn't exist in data segment 0c5c0814-902c-429c-ad41-cd82aea276a2
	at org.apache.jackrabbit.oak.plugins.segment.Segment.getRefId(Segment.java:196)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.internalReadRecordId(Segment.java:307)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readRecordId(Segment.java:303)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getBucketList(MapRecord.java:134)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntries(MapRecord.java:347)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntries(MapRecord.java:325)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:474)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:394)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:544)
...
{noformat}

{noformat}
java.lang.IllegalStateException: String is too long: 2159501163930351661
	at org.apache.jackrabbit.oak.plugins.segment.Segment.loadString(Segment.java:352)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:319)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:313)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.loadTemplate(Segment.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:367)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:361)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:78)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:408)
...
{noformat}

{noformat}
java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
	at org.apache.jackrabbit.oak.plugins.segment.file.TarWriter.writeEntry(TarWriter.java:206)
	at org.apache.jackrabbit.oak.plugins.segment.file.TarWriter.writeEntry(TarWriter.java:200)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.writeSegment(FileStore.java:682)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:228)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.prepare(SegmentWriter.java:329)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeTemplate(SegmentWriter.java:969)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1039)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1062)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:395)
...
{noformat}

{noformat}
Caused by: java.lang.IllegalArgumentException: Invalid type tag: 81
	at org.apache.jackrabbit.oak.api.Type.fromTag(Type.java:202)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.loadTemplate(Segment.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:367)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:361)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:78)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getProperty(SegmentNodeState.java:122)
...
{noformat}

{noformat}
Caused by: java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.pos(Segment.java:178)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.loadString(Segment.java:326)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:319)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:313)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:174)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:147)
	at org.apache.jackrabbit.oak.plugins.memory.AbstractPropertyState.equal(AbstractPropertyState.java:53)
...
{noformat}

Will attach a patch with a test case shortly.",corruption,['segmentmk'],OAK,Bug,Major,2014-11-25 17:08:02,14
12757647,Switch default IndexFormatVersion to V2 ,"OAK-2276 added support for {{IndexFormatVersion}} where {{V1}} is compatible with existing {{LuceneIndex}} while {{V2}} is compatible with newer index implemention being worked on OAK-2278.

Once implementation in OAK-2278 is stable enough we should switch the default version to be used for fresh index (unless overrided with {{compatMode}} ) from V1 to V2",performance,['lucene'],OAK,Task,Major,2014-11-25 08:55:34,19
12755383,Compaction estimation time should not depend on number of checkpoints,"Checkpoints can be seen as unix symlinks. The compaction estimation will blindly follow those links when computing the set of bulk segments that are in-use. 
For example, when there are 1.9k checkpoints, the estimator will have to traverse the same repo 1.9k times to determine this set, even though it falls onto already traversed paths. 
This is also misleading for debugging, it all looks like it's loading segments while estimating.",compaction gc,"['core', 'segmentmk']",OAK,Bug,Major,2014-11-14 15:06:39,14
12754445,Add metadata about the changed value to a PROPERTY_CHANGED event on a multivalued property,"When getting _PROPERTY_CHANGED_ events on non-multivalued properties only one value can have actually changed so that handlers of such events do not need any further information to process it and eventually work on the changed value; on the other hand _PROPERTY_CHANGED_ events on multivalued properties (e.g. String[]) may relate to any of the values and that brings a source of uncertainty on event handlers processing such changes because there's no mean to understand which property value had been changed and therefore to them to react accordingly.
A workaround for that is to create Oak specific _Observers_ which can deal with the diff between before and after state and create a specific event containing the ""diff"", however this would add a non trivial load to the repository because of the _Observer_ itself and because of the additional events being generated while it'd be great if the 'default' events would have metadata e.g. of the changed value index or similar information that can help understanding which value has been changed (added, deleted, updated). ",observation,"['core', 'jcr']",OAK,Improvement,Major,2014-11-11 11:27:33,15
12753708,SegmentBlob equals check should verify compaction in both directions,"It looks like the SegmentNodeState checks compaction in one direction [0], while the SegmentBlob checks in the other direction. This will not break anything as the fallback is to use byte comparison, but it will be terribly slow.
I'm proposing to verify both directions in the #equals check, to make sure we don't miss out.


[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentNodeState.java#L401
[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentBlob.java#L198
",compaction gc,['segmentmk'],OAK,Bug,Major,2014-11-07 14:45:39,14
12753132,Observation events accessibility check should respect session refresh settings,This is related to OAK-2000. I think the accessibility check needs to respect the session refresh settings when acquiring the root object.,observation,['jcr'],OAK,Bug,Major,2014-11-05 19:04:16,15
12751650,IndexOutOfBoundsException in o.a.j.o.scalability.ScalabilityNodeRelationshipSuite,"When running the scalability suite even if not run the org.apache.jackrabbit.oak.scalability.ScalabilityNodeRelationshipSuite fails with 

{noformat}
Apache Jackrabbit Oak
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.jackrabbit.oak.scalability.ScalabilityRunner.main(ScalabilityRunner.java:143)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:162)
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at java.util.Collections$UnmodifiableList.get(Collections.java:1152)
	at org.apache.jackrabbit.oak.scalability.ScalabilityNodeRelationshipSuite.<clinit>(ScalabilityNodeRelationshipSuite.java:89)
	... 2 more
{noformat}",scalability,[],OAK,Bug,Major,2014-10-30 10:22:53,22
12751610,CopyOnWriteDirectory implementation for Lucene for use in indexing,"Currently a Lucene index when is written directly to OakDirectory. For reindex case it might happen that Lucene merge policy read the written index files again and then perform a sgement merge. This might have lower performance when OakDirectroy is writing to remote storage.

Instead of that we can implement a CopyOnWriteDirectory on similar lines to  OAK-1724 where CopyOnReadDirectory support copies the  index locally for faster access. 

At high level flow would be

# While writing index the index file is first written to local directory
# Any write is done locally and once a file is written its written asynchronously to OakDirectory
# When IndexWriter is closed it would wait untill all the write is completed

This needs to be benchmarked with existing reindex timings to see it its actually beneficial",docs-impacting performance,['lucene'],OAK,New Feature,Major,2014-10-30 05:05:49,19
12751356,"provide a way to update the ""created"" timestamp of a NodeDocument","Both the MongoDocumentStore and the RDBDocumentStore maintain a ""_modCount"" property, which uniquely identifies a version of a document in the persistence.

Sometimes, we read data from the persistence although we already might have the document cached. This happens:

a) when the cached document is older than what the caller asked for

b) when running a query (for instance when looking up children of a node)

In both cases, we currently replace the cache entry with a newly built NodeDocument.

It would make sense to re-use the existing document instead. (This would probably require modifying the ""created"" timestamp, but would avoid the trouble of having to update the cache at all) ",performance,['mongomk'],OAK,Improvement,Major,2014-10-29 11:18:25,3
12748072,Repository upgrade does not correctly update jcr:all aggregate privileges and bits,When custom privileges are present {{jcr:all}} does not correctly reflect those via {{#getAggregatePrivileges}} and via its bit map. ,production upgrade,['upgrade'],OAK,Bug,Critical,2014-10-14 16:33:07,0
12748070,Print tar file graph in segment explorer,I think it would be useful if the segment explorer could print the graph of a tar file along with its references. ,tools,['core'],OAK,Improvement,Major,2014-10-14 16:18:35,15
12748057,No garbage collection after reaching generation z,"Tar garbage collection generates new generation of tar files once it determines a given file contains garbage. New generations will have the next lower case letter from the alphabet appended to its file name. When the letter 'z' is reached, no further garbage collection is done. 

I think we need to fix this as otherwise garbage collection just stops working arbitrarily. ",gc,['core'],OAK,Improvement,Minor,2014-10-14 15:26:19,15
12748055,Concurrent commit during compaction results in mixed segments,"Changes that are committed during a segment store compaction run will be compacted on top of the already compacted changes. However the compactor uses the wrong before state in this case. Instead of compacting against the compacted before state it uses the un-compacted before state. The resulting state will thus contain references to un-compacted state, making those not eligible for later clean up. ",compaction gc,"['core', 'segmentmk']",OAK,Bug,Major,2014-10-14 15:10:26,15
12747934,Fix intermittent failure in JaasConfigSpiTest,"Intermittent failures on windows are observed in JaasConfigSpiTest with following exception

{noformat}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.841 sec <<< FAILURE!
defaultConfigSpiAuth(org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest)  Time elapsed: 3.835 sec  <<< ERROR!
java.lang.reflect.UndeclaredThrowableException
	at $Proxy7.login(Unknown Source)
	at javax.jcr.Repository$login.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest.defaultConfigSpiAuth(JaasConfigSpiTest.groovy:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.jackrabbit.oak.run.osgi.OakOSGiRepositoryFactory$RepositoryProxy.invoke(OakOSGiRepositoryFactory.java:325)
	... 37 more
Caused by: javax.jcr.LoginException: No LoginModules configured for jackrabbit.oak
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:264)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:222)
	... 42 more
Caused by: javax.security.auth.login.LoginException: No LoginModules configured for jackrabbit.oak
	at javax.security.auth.login.LoginContext.init(LoginContext.java:256)
	at javax.security.auth.login.LoginContext.<init>(LoginContext.java:499)
	at org.apache.jackrabbit.oak.spi.security.authentication.JaasLoginContext.<init>(JaasLoginContext.java:49)
	at org.apache.jackrabbit.oak.security.authentication.LoginContextProviderImpl.getLoginContext(LoginContextProviderImpl.java:85)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:161)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:256)
	... 43 more

Running org.apache.jackrabbit.oak.run.osgi.JsonConfigRepFactoryTest
{noformat}",CI buildbot test,['pojosr'],OAK,Task,Minor,2014-10-14 06:08:06,19
12747685,Release 1.1.1,Issues for 1.1.1 https://issues.apache.org/jira/issues/?jql=project%20%3D%20OAK%20AND%20fixVersion%20%3D%201.1.1,release,[],OAK,Task,Major,2014-10-13 09:48:22,22
12746165,Observation tests sporadically failing,"{{JackrabbitNodeTest#testRenameEventHandling}} fails sporadically on the Apache buildbot with missing events (e.g. http://ci.apache.org/builders/oak-trunk-win7/builds/642). 

Same holds for other tests in the {{ObservationIT}} suite. 
",CI buildbot observation test,['jcr'],OAK,Bug,Major,2014-10-06 15:17:11,15
12744052,Segment Compactor will not compact binaries > 16k,"The compaction bit rely on the SegmentBlob#clone method in the case a binary is being processed but it looks like the #clone contract is not fully enforced for streams that are qualified as 'long values' (>16k if I read the code correctly). 
What happens is the stream is initially persisted as chunks in a ListRecord. When compaction calls #clone it will get back the original list of record ids, which will get referenced from the compacted node state [0], making compaction on large binaries ineffective as the bulk segments will never move from the original location where they were created, unless the reference node gets deleted.

I think the original design was setup to prevent large binaries from being copied over but looking at the size problem we have now it might be a good time to reconsider this approach.


[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentBlob.java#L75
",compaction gc,"['core', 'segmentmk']",OAK,Bug,Major,2014-09-25 15:21:13,14
12744000,Release Oak 1.0.7,Issues for 1.0.7: https://issues.apache.org/jira/issues/?jql=project%20%3D%20OAK%20AND%20fixVersion%20%3D%201.0.7,Release,[],OAK,Task,Major,2014-09-25 11:26:20,2
12742399,Killing a node may stop async index update to to 30 minutes (Tar storage),"When killing a node that is running the sync index update, then this async index update will not run for up to 15 minutes, because the lease time is set to 15 minutes.

I think the lease time should be much smaller, for example 1 minute, or maybe even 10 seconds.

Also, we might need to better document this issue (in addition to the warning in the log file). For non cluster case we can do away with lease time out and this for such cases indexing would not get paused upon restart post abrupt shutdown",resilience,['query'],OAK,Improvement,Major,2014-09-18 09:27:09,19
12742379,Optimize reads from secondaries,"OAK-1645 introduced support for reads from secondaries under certain
conditions. The current implementation checks the _lastRev on a potentially
cached parent document and reads from a secondary if it has not been
modified in the last 6 hours. This timespan is somewhat arbitrary but
reflects the assumption that the replication lag of a secondary shouldn't
be more than 6 hours.

This logic should be optimized to take the actual replication lag into
account. MongoDB provides information about the replication lag with
the command rs.status().
",performance scalability,"['core', 'mongomk']",OAK,Improvement,Major,2014-09-18 07:17:44,18
12738281,"DocumentStore API: batch create, but no batch update","The DocumentStore API currently has a call for creating many nodes at once.

However, this will sometimes fail for large save operations in JCR, because in the DS persistence, JCR-deleted nodes are still present (with a deleted flag). This causes two subsequent sequences of

1) create test container
2) create many child nodes
3) remove test container

to behave very differently, depending on whether the test container is created for the first time or not.

(see CreateManyChildNodesTest)

",performance,"['core', 'documentmk', 'mongomk', 'rdbmk']",OAK,Improvement,Blocker,2014-09-01 15:04:34,2
12738253,JMX stats for operations being performed in DocumentNodeStore,"Currently DocumentStore performs various background operations like

# Cache consistency check
# Pushing the lastRev updates
# Synchrnizing the root node version

We should capture some stats like time taken in various task and expose them over JMX to determine if those background operations are performing well or not. For example its important that all tasks performed in background task should be completed under 1 sec (default polling interval). If the time taken increases then it would be cause of concern

See http://markmail.org/thread/57fax4nyabbubbef",tooling,['documentmk'],OAK,Improvement,Major,2014-09-01 12:05:40,19
12738234,Release Oak 1.0.6,Issues for 1.0.6: https://issues.apache.org/jira/issues/?jql=project%20%3D%20OAK%20AND%20fixVersion%20%3D%201.0.6,Release,[],OAK,Task,Major,2014-09-01 10:18:11,16
12737509,MBean to provide consolidated cache stats ,Currently DocumentNodeStore has 5 different types of caches and each register there own MBean. To get a better understanding of the overall cache usage it would be good to have a {{ConsolidatedCacheStatsMBean}} which depicts all the stats in tabular form,monitoring,['core'],OAK,Improvement,Minor,2014-08-28 17:32:24,19
12737059,Optimize orderings by date fields,"Sorting by date fields is very slow in oak, especially if result set size is large.

I'm running the following JCR-SQL2 query

{code}
SELECT * FROM [cq:PageContent] AS [c] WHERE ISDESCENDANTNODE('/content')
{code}

which returns 3270 results on my oak repo.

{noformat}
Query execution times are as below
---------------------------------------
No order clause 		|  0,147 sec
ORDER BY [jcr:title]	        |  1,203 sec
ORDER BY [jcr:createdBy]	|  1,018 sec
ORDER BY [jcr:created]		| 25,229 sec
{noformat}

Ordering by date field adds extra 24 seconds overhead.

",performance,['query'],OAK,Bug,Major,2014-08-27 09:34:55,22
12735772,Long running JCR session prevent live cleanup in Segment FileStore,"Cleanup operation in SegmentNodeStore detects the un referenced garbage and clean it up. To determine the reference validity it starts with an initial set of SegmentId which have a live java reference. 

This works fine for simple setup but when Oak repository is used in an application (like Sling) where application code can create long running session (for observation) then such session are bound to old NodeState at time of startup. Such references prevent the cleanup logic to remove older revisions while system is running. Such revisions can only be removed via an offline compaction-> cleanup.

Need to find out a way where we can _migrate_ such old NodeState references to newer revisions",gc,['segmentmk'],OAK,Bug,Major,2014-08-21 12:32:27,15
12734528,Define standards for plan output,"Currently, the syntax for the plan output is chaotic as it varies significantly from index to index. Whereas some of this is expected - each index type will have different data to output, Oak should provide some standards about how a plan will appear.",tooling,['query'],OAK,Improvement,Minor,2014-08-15 22:04:46,4
12733229,"Optimal index usage for XPath queries with ""order by"" combined with ""or""","XPath queries with ""or"" are converted to union, even if there is an ""order by"" clause. In such cases, sorting is done in memory. See also OAK-2022.

For some queries, it might be better to not use union, but use an ordered index instead. This is tricky to decide up-front, but it would be possible to estimate the cost of both variants and pick the one that seems better.",performance,['query'],OAK,Improvement,Major,2014-08-11 13:05:14,4
12732220,Make blob gc max age configurable in SegmentNodeStoreService,The blob gc max age setting is not configurable when using {{SegmentNodeStoreService}}. This can be made configurable and will be useful for testing.,datastore,['core'],OAK,Improvement,Minor,2014-08-06 04:23:18,16
12730767,Verify the maven baseline output and fix the warnings,"Currently the maven baseline plugin only logs the package version mismatches, it doesn't fail the build. It would be beneficial to start looking at the output and possibly fix some of the warnings (increase the OSGi package versions).",build modularization osgi technical_debt,['core'],OAK,Improvement,Major,2014-07-30 11:56:22,15
12730473,Observation events accessibility not checked correctly,Before delivering an observation event it is checked whether the respective item is actually accessible through the associated session. However the check is currently done against the state of the session from the time the event listener was registered instead of from the time the event is being sent. ,observation,"['core', 'jcr']",OAK,Bug,Major,2014-07-29 12:56:48,15
12729663,Limit no of children listed with ls command in Oak Console,Oak Console 'ls' command currently lists down all the child node which cause issue for node have large no of children. As a fix ls command should dump max say 50 child node and allow user to change the limit as part of arguments,console,['run'],OAK,Improvement,Minor,2014-07-25 06:13:55,19
12729658,Add command to dump Lucene index in Oak Console,"Add a command in Oak Run Console to dump lucene index and also provide stats related to Lucene index

",console,['run'],OAK,New Feature,Minor,2014-07-25 05:13:09,19
12729095,Implement full scale Revision GC for DocumentNodeStore,"So far we have implemented garbage collection in some form with OAK-1341. Those approaches help us remove quite a bit of garbage (mostly due to deleted nodes) but till some part is left

However full GC is still not performed due to which some of the old revision related data cannot be GCed like
* Revision info present in revision maps of various commit roots
* Revision related to unmerged branches (OAK-1926)
* Revision data created to property being modified by different cluster nodes

So having a tool which can perform above GC would be helpful. For start we can have an implementation which takes a brute force approach and scans whole repo (would take quite a bit of time) and later we can evolve it. Or allow system admins to determine to what level GC has to be done",resilience scalability,['mongomk'],OAK,New Feature,Major,2014-07-23 06:54:24,2
12728484,Add path exclusion to JackrabbitEventFilter,Implement the new Jackrabbit API introduced with JCR-3797,observation,"['core', 'jcr']",OAK,New Feature,Major,2014-07-21 07:20:33,15
12727885,Wrong values reported for OBSERVATION_EVENT_DURATION,The value reported for the {{RepositoryStatistics.Type#OBSERVATION_EVENT_DURATION}} statistic is wrong. Instead of the total time spent *processing* observation events it reports the total time *producing* observation events. ,monitoring observation,['jcr'],OAK,Bug,Major,2014-07-17 09:27:23,15
12727863,Fail fast on branch conflict,"The current MongoMK implementation performs retries when it runs into merge
conflicts caused by collisions. It may be possible to resolve a conflict by resetting
the branch back to the state as it was before the merge and re-run the commit hooks again.
This helps if the conflict was introduced by a commit hook. At the moment the retries
also happen when the conflict was introduced before the merge. In this case, a retry
is useless and the commit should fail fast.
",performance,"['core', 'mongomk']",OAK,Improvement,Minor,2014-07-17 07:09:16,2
12727342,generalize MongoDB-specific tests,"It appears that there are tests that have been rewritten exclusively for the Mongo persistence, but which really are specific to the DocumentMK (formerly MongoMK).

These should be rewritten to be useable for all implementations of the DocumentStore API.",CI technical_debt,['mongomk'],OAK,Improvement,Major,2014-07-15 14:17:00,11
12727333,Optimize the diff logic for large number of children case ,"DocumentNodeStore currently makes use of query to determine child nodes which have changed after certain time. Query used is something like

{noformat}
db.nodes.find({ _id: { $gt: ""3:/content/foo/01/"", $lt: ""3:/content/foo010"" }, _modified: { $gte: <start time> } }).sort({_id:1})
{noformat}

OAK-1966 tries to optimize the majority case where start times is recent and in that case it makes use of _modified index. However if the start time is quite old and a node has large number of children say 100k then it would involve scan of all those 100k nodes as _modified index would not be of much help. 

Instead of querying like this we can have a special handling for cases where large number of children are involved. It would involve following steps

After analyzing the runtime queries in most case it is seen that even with old modified time the number of change nodes is < 50

# Mark parent nodes which have large number of children say > 50
# On such nodes we would keep an array of \{modifiedtime, childName\} ## Array would be bounded say keep last 50 updates. This can be done via splice and push operators [1]
## Each entry in array would record modifiedtime and name of child node which was modified. 
## Array would be sorted on modifiedtime
# Each updated to any child belonging to such parent would also involve update to above array
# When we query for modified we check if the parent has such an array (if parent is in cache) and if that array has time entries from the required start time we directly make use of that and avoid the query

This should reduce needs for such queries in majority of cases

[1] http://docs.mongodb.org/manual/reference/operator/update-array/
 ",performance resilience,['mongomk'],OAK,Improvement,Major,2014-07-15 13:13:52,2
12726576,Performance degradation due to SessionDelegate.WarningLock,"In OAK-1703, we have added a new class WarningLock that internally uses an Exception to remember the stack trace. This seems to be used for every SessionDelegate object. With Java 6 and older, this is very problematic because it will cause ""java.lang.Throwable.fillInStackTrace(Native Method)"" to be called for almost every call to any of the Oak JCR methods, and ""fillInStackTrace(Native Method)"" is known to be be very slow. Java 7, I believe, will at some point give up and not fill in the stack trace any more. But with Java 6 and older, this is a big problem.",Performance,['jcr'],OAK,Bug,Major,2014-07-10 12:46:38,15
12726532,Expose URL for Blob source ,"In certain scenarios for performance reasons its desirable to have direct access to the Blob source. 

For e.g. if using a FileDataStore having a direct access to the native file system path of the blob (if not stored in chunks) is more useful than repository path e.g. native tools don't understand repository path, instead file system path can be passed directly to native tools for processing binary.

Another usecase being ability exposed signed S3 url which would allow access to binary content directly",datastore,['core'],OAK,Improvement,Major,2014-07-10 09:23:04,19
12726185,Session.logout performance poor,"Problem:
Session.logout was observed to take 14% of time in a performance test of a reasonably real-world load.

Method:
Use the attached sling junit test case to run 8 concurrent instances of the test. profile with YourKit or  similar and see >50% time taken by logout.

Expected:
Logout should be practically free.

Solution:
The attached patch avoids a bug in guava-15 (still present in guava-17 the latest) where the former use of addCallback triggered many CancellationExceptions when sessions were quickly created and logged out.
",Performance,['jcr'],OAK,Bug,Major,2014-07-08 20:50:32,15
12725972,Set correct OSGi package export version,"This issue serves as a reminder to set the correct OSGi package export versions before we release 1.2.

OAK-1536 added support for the BND baseline feature: the baseline.xml files in the target directories should help us figuring out the correct versions. ",modularization osgi technical_debt,[],OAK,Task,Critical,2014-07-08 08:52:12,15
12724756,MAX_QUEUED_CONTINUATIONS feature not working in EventGenerator class,"Since OAK-1422 the  {{Continuation}} created in {{fullQueue()}} is put to the front of the List. This causes it to be taken right off the list again on the next call to {{generate()}} instead of first continuing with the rest of the list allowing it to shrink. As a result the list may grow up to 2 x {{MAX_QUEUED_CONTINUATIONS}} instead of 1 + {{MAX_QUEUED_CONTINUATIONS}} as anticipated. 

",Observation,['core'],OAK,Bug,Major,2014-07-01 15:50:06,15
12717708,Optimize SegmentWriter.prepare(),"A significant part of the time in writing new SegmentMK records is spent in the {{SegmentWriter.prepare()}} method, especially in the part where the exact set of segment references is computed. In most cases that computation could be short-circuited to improve write performance.",performance,"['core', 'segmentmk']",OAK,Improvement,Major,2014-05-31 02:50:01,24
12717693,SegmentMK: Inefficient flat node comparisons,"The SegmentMK has an optimization for the common case where only a single child node among many has been updated. For the most part this code works very well, but there's one code path where this optimization is currently not applied and as a result a node comparison ends up traversing the full list of child nodes.

The troublesome code path gets triggered when a single child node is updated in one commit and then another commit does some more complex changes (adds or removes a node and/or modifies more than a single node). 

Usually this isn't too big an issue since traversing even thousands of child node entries is very fast with the SegmentMK, but things slow down a lot when there are millions of children. Unfortunately that is exactly what happens with the UUID index in a large repository with millions of referenceable nodes...",performance,"['core', 'segmentmk']",OAK,Bug,Major,2014-05-31 00:04:01,24
12717071,Generic operation tasks should be able to return specific results,"The generic interface for operation management tasks added with OAK-1160 does so far not provide a way for specific tasks to return a value apart from a genetic status. With the consistency checking we are starting to add (OAK-1448) such a needs start to arise. 

To address this I propose to change type of the tasks that can be passed to the constructor of {{ManagementOperation}}. The result type of the task (i.e. {{Callable}}) should change from {{Long}} to some generic container, which would carry the result of the task. ",monitoring,['core'],OAK,Improvement,Major,2014-05-28 16:01:06,15
12716831,unit tests for concurrent DocumentStore access,Add tests that exercise multiple DS instances accessing the same persistence.,CI technical_debt,"['mongomk', 'rdbmk']",OAK,Sub-task,Major,2014-05-27 15:18:17,3
12716811,Segment Explorer,"I'm thinking about working on a desktop tool that would allow browsing the repository and would provide tarmk specific information: segment ids, tar files, sizes, checkpoints and so on.",tools,['segmentmk'],OAK,Improvement,Major,2014-05-27 13:45:28,14
12715768,Use SegmentMK for testing where possible,There are still a few places left where {{MicroKernelImpl}} is used for running tests. As {{SegementMK}} is the default for going forward I suggest we change those tests accordingly. ,test,"['it', 'solr', 'upgrade']",OAK,Improvement,Major,2014-05-21 14:43:48,15
12715514,"ISE: ""Unexpected value record type: f2"" is thrown when FileBlobStore is used","The stacktrace of the call shows something like
{code}
20.05.2014 11:13:07.428 *ERROR* [OsgiInstallerImpl] com.adobe.granite.installer.factory.packages.impl.PackageTransformer Error while processing install task.
java.lang.IllegalStateException: Unexpected value record type: f2
at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.length(SegmentBlob.java:101)
at org.apache.jackrabbit.oak.plugins.value.BinaryImpl.getSize(BinaryImpl.java:74)
at org.apache.jackrabbit.oak.jcr.session.PropertyImpl.getLength(PropertyImpl.java:435)
at org.apache.jackrabbit.oak.jcr.session.PropertyImpl.getLength(PropertyImpl.java:376)
at org.apache.jackrabbit.vault.packaging.impl.JcrPackageImpl.getPackage(JcrPackageImpl.java:324)
{code}

The blob store was configured correctly and according to the log also correctly initialized
{code}
20.05.2014 11:11:07.029 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService Initializing SegmentNodeStore with BlobStore [org.apache.jackrabbit.oak.spi.blob.FileBlobStore@7e3dec43]
20.05.2014 11:11:07.029 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService Component still not activated. Ignoring the initialization call
20.05.2014 11:11:07.077 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK opened: crx-quickstart/repository/segmentstore (mmap=true)
{code}

Under which circumstances can the length within the SegmentBlob be invalid?
This only happens if a File Blob Store is configured (http://jackrabbit.apache.org/oak/docs/osgi_config.html). If a file datastore is used, there is no such exception.",resilience,['blob'],OAK,Bug,Major,2014-05-20 15:43:31,12
12715494,Document Oak Clustering,"the 'clustering' page in our oak documentation is currently an empty placeholder and it would be great if there would be some initial pointers.

[~chetanm], [~mreutegg], what do you think?",documentation,['doc'],OAK,Task,Major,2014-05-20 14:34:08,19
12714523,Improved SegmentWriter,"At about 1kLOC and dozens of methods, the SegmentWriter class currently a bit too complex for one of the key components of the TarMK. It also uses a somewhat non-obvious mix of synchronized and unsynchronized code to coordinate multiple concurrent threads that may be writing content at the same time. The synchronization blocks are also broader than what really would be needed, which in some cases causes unnecessary lock contention in concurrent write loads.

To improve the readability and maintainability of the code, and to increase performance of concurrent writes, it would be useful to split part of the SegmentWriter functionality to a separate RecordWriter class that would be responsible for writing individual records into a segment. The SegmentWriter.prepare() method would return a new RecordWriter instance, and the higher-level SegmentWriter methods would use the returned instance for all the work that's currently guarded in synchronization blocks.",technical_debt,['segmentmk'],OAK,Sub-task,Minor,2014-05-15 16:35:24,14
12714248,oak-solr-core test failures on Java 8 and later,"The following {{oak-solr-core}} test failures occur when building Oak with Java 8:

{noformat}
Failed tests:
  testNativeMLTQuery(org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTest): expected:</test/[a]> but was:</test/[b]>
  testNativeMLTQueryWithStream(org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTest): expected:</test/[a]> but was:</test/[b]>
{noformat}

The cause of this might well be something as simple as the test case incorrectly expecting a specific ordering of search results.",java8 java9 jenkins test,['solr'],OAK,Bug,Minor,2014-05-14 19:15:46,10
12713027,ConstraintViolationException seen with multiple Oak/Mongo with ConcurrentCreateNodesTest,"While running ConcurrentCreateNodesTest with 5 instances writing to same Mongo instance following exception is seen

{noformat}
Exception in thread ""Background job org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer@3f56e5ed"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:111)
    at org.apache.jackrabbit.oak.benchmark.AbstractTest$1.run(AbstractTest.java:481)
Caused by: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:225)
    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:679)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:553)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:417)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:414)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:308)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:127)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:414)
    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:100)
    ... 1 more
Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.constraintViolation(TypeEditor.java:150)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.getEffectiveType(TypeEditor.java:286)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.<init>(TypeEditor.java:101)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditorProvider.getRootEditor(TypeEditorProvider.java:85)
    at org.apache.jackrabbit.oak.spi.commit.CompositeEditorProvider.getRootEditor(CompositeEditorProvider.java:80)
    at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:53)
    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)
    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)
    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch$InMemory.merge(AbstractNodeStoreBranch.java:498)
    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch.merge(AbstractNodeStoreBranch.java:300)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:129)
    at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1275)
    at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:405)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:551)
    ... 7 more
{noformat}

This has been reported by [~rogoz]",concurrency,['mongomk'],OAK,Bug,Minor,2014-05-08 07:07:54,2
12713023,Benchmark for blob upload and search longevity,Have a longevity test which incrementally increases the load by adding blobs and then running full text search to measure the execution times and the performance degradation for increased loads.,benchmark test,['run'],OAK,Sub-task,Major,2014-05-08 06:12:16,16
12712931,Debugging console,It would be nice to for {{oak-run}} to come with a debugging console like the {{cli}} mode in {{jackrabbit-standalone}}.,production tools,['run'],OAK,New Feature,Major,2014-05-07 20:21:02,19
12712928,TarMK compaction,"The TarMK would benefit from periodic ""compact"" operations that would traverse and recreate (parts of) the content tree in order to optimize the storage layout. More specifically, such compaction would:

* Optimize performance by increasing locality and reducing duplication, both of which improve the effectiveness of caching.
* Allow the garbage collector to release more unused disk space by removing references to segments where only a subset of content is reachable.",production tools,['segmentmk'],OAK,New Feature,Major,2014-05-07 20:12:46,14
12712793,Create a benchmark for Full text search with Solr,As a follow up to OAK-1702 it'd be good to have a similar benchmark for the Solr indexer.,benchmark test,"['benchmarks', 'solr']",OAK,Bug,Major,2014-05-07 09:20:24,10
12712602,Missing documentation around Ordered Index,"oak-doc is missing documentation about the usage of the OrderedIndex.
",doc,['doc'],OAK,Improvement,Major,2014-05-06 15:31:41,15
12711671,ConcurrentConflictTest fails occasionally,"Occurs every now and then on buildbot. E.g.:
http://ci.apache.org/builders/oak-trunk-win7/builds/16",concurrency,"['core', 'mongomk']",OAK,Bug,Minor,2014-05-01 07:28:50,2
12710305,Better cooperation for conflicting updates across cluster nodes,"Every now and then we see commit failures in a cluster when many sessions try to update the same property or perform some other conflicting update.

The current implementation will retry the merge after a delay, but chances are some session on another cluster node again changed the property in the meantime. This will lead to yet another retry until the limit is reached and the commit fails. The conflict logic is quite unfair, because it favors the winning session.

The implementation should be improved to show a more fair behavior across cluster nodes when there are conflicts caused by competing session.",concurrency scalability,"['core', 'documentmk']",OAK,Improvement,Major,2014-04-24 12:59:37,2
12710287,Inaccurate values reported by RepositoryStatsMBean,"This is a follow up from OAK-1757.

The accuracy of the values reported by {{RepositoryStatsMBean}} for {{SESSION_WRITE_DURATION}} and {{SESSION_READ_DURATION}} depend on the value of {{Clock#FAST_CLOCK_INTERVAL}}. 

The 1s reset interval of the duration counters might be to small for the inaccuracies of the clock resolution to average out. ",monitoring,['core'],OAK,Bug,Major,2014-04-24 10:51:34,15
12709986,Sporadic IllegalStateException in AbstractServiceTracker.getServices,"Seeing the below IlegalStateException about tracker being null several times on a 4-node oak-mongo cluster. There were no log.warn 'Timed out waiting for change processor to stop' near those errors (but there was once hour(s) before in one case).

{code}16.04.2014 05:34:50.908 *ERROR* [oak-executor-1619] org.apache.sling.extensions.threaddump.internal.Activator Uncaught exception in Thread Thread[oak-executor-1619,1,Configuration
 Admin Service]
java.lang.IllegalStateException: null
        at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
        at org.apache.jackrabbit.oak.spi.whiteboard.AbstractServiceTracker.getServices(AbstractServiceTracker.java:60)
        at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardExecutor.execute(WhiteboardExecutor.java:40)
        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1.run(BackgroundObserver.java:130)
        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$ListenableFutureTask.run(BackgroundObserver.java:283)
        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$ListenableFutureTask.done(BackgroundObserver.java:278)
        at java.util.concurrent.FutureTask$Sync.innerSet(FutureTask.java:281)
        at java.util.concurrent.FutureTask.set(FutureTask.java:141)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:339)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{code}",concurrency,['core'],OAK,Bug,Major,2014-04-23 09:30:36,15
12709744,add docu how to connect to Mongo w/ credentials,"we should document how to use MongoMK when MongoDB requires credentials to connect to. According to [~chetanm] this would work as
{quote}
in OSGi config we can specify uri [1] to mongodb://admin:admin@localhost:27017

[1] http://api.mongodb.org/java/current/com/mongodb/MongoURI.html 
{quote}",documentation,"['doc', 'mongomk']",OAK,Improvement,Trivial,2014-04-22 11:21:11,19
12709720,Node name queries should use an index,"The node name queries don't use any index currently, making them really slow and triggering a lot of traversal warnings.

Simply adding node names to a property index would be too much content indexed, but as Lucene already indexes the node names, using this index would be one viable option.

{code}
/jcr:root//*[fn:name() = 'jcr:content']
/jcr:root//*[jcr:like(fn:name(), 'jcr:con%')] 
{code}",performance,['query'],OAK,Improvement,Major,2014-04-22 09:15:40,19
12709036,Cleanup documentation of _modCount,"The documentation of

  Document.MOD_COUNT

""The modification count on the document. This is an long value incremented on every modification.""

gives the impression that this is a mechanism that is part of the DocumentStore API contract (which IMHO it is not)",documentation,['mongomk'],OAK,Task,Minor,2014-04-17 12:26:45,2
12707672,Concurrent updates of ordered index in cluster may fail,"In a clustered deployment with DocumentNodeStore on MongoDB it may happen that concurrent updates on the new ordered index fail because of conflicts.

A common use case is maintaining an ordered index on a last modified date. When nodes with such a date are added concurrently on multiple cluster nodes, then all of them will try to update the ordered index at one end of the key list. The DocumentNodeStore will perform a couple of retries but there is no guarantee that the cluster nodes will sync within that time frame or some other session conflicts yet another time.

A possible workaround is to declare the index as asynchronous.",concurrency,['core'],OAK,Bug,Major,2014-04-10 09:33:17,22
12707639,Enable passing of a execution context to runTest in multi threaded runs,"Benchmark runner has support for specifying concurrency levels to execute the test with varying level of concurrency. In most cases the test case would be operating on a JCR session. With multi threaded runs we need a way to have jcr session bound to that thread of execution.

To support that {{AbstractTest}} should provide a way for client to provide a executionContext object which sub classes can provide. That context would be managed per thread and passed to the runTest method if not null",concurrency test,['benchmarks'],OAK,Improvement,Minor,2014-04-10 05:21:44,19
12707374,Improve warning logged on concurrent Session access,"OAK-1601 introduced warnings that are logged when a session is accessed concurrently from different threads. The modalities however differ from those of Jackrabbit 2. The message 

{code}
Attempt to perform ""sessionOperation"" while another thread is concurrently writing to ""session"". Blocking until the other thread is finished using this session. Please review your code to avoid concurrent use of a session.
{code}

is logged for the current thread

* if the current threads attempts a write operation while another thread already executes a write operation in Jackrabbit 2,
* if the current thread attempts a write operation while another thread already executes any operation. 

We should make these warnings identical to those of Jackrabbit 2.

",concurrency,['jcr'],OAK,Bug,Major,2014-04-09 07:26:44,15
12707136,Improve LargeOperationIT accuracy for document nodes store fixture,As [noted | https://issues.apache.org/jira/browse/OAK-1414?focusedCommentId=13942016&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13942016] on OAK-1414 {{LargeOperationIT}} is somewhat inaccurate for the document node store fixture where the collected data tends to be noisy. We should look into ways to make  the tests results more accurate for this case.,test,['jcr'],OAK,Improvement,Major,2014-04-08 08:48:51,15
12707129,Document Solr index,Provide documentation about the Oak Solr index. That should contain information about the design and how to configure it.,documentation,"['doc', 'solr']",OAK,Task,Major,2014-04-08 08:04:42,10
12706946,"document atomicity of DS.update(collection, keys, update)","Please document (I'll assume it's similar to ""remove"", in that it is ""best effort"")?",concurrency,['mongomk'],OAK,Bug,Major,2014-04-07 13:21:51,2
12706594,"document atomicity of DS.remove(collection, keys)","[~mreutegg]
I believe it's best effort (looking at the MongoDB impl), but it would be good to clarify.

In particular, should the operation abort then one removal failed, or keep going? What's the expectation when a document doesn't exist?",documentation,"['core', 'mongomk']",OAK,Task,Major,2014-04-04 09:14:11,2
12706187,JCR Event Info should contain NodeType for all Events ,"OAK-1661 added node type information for {{NODE_ADDED}} and {{NODE_REMOVED}} events. We should consider adding this for all event types however.  Even property events would contain node type of the node the property is associated with (parent).

An implication of this is however that we also need to adapt the TCK as this will cause {{org.apache.jackrabbit.test.api.observation.GetInfoTest}} to fail, which expects the info map to be generally empty. ",observation,['jcr'],OAK,Improvement,Minor,2014-04-02 15:03:21,15
12706142,FileDataStore inUse map causes contention in concurrent env,"JR2 FileDataStore#inUseMap [1] is currently a synchronized map and that at times causes contention concurrent env. This map is used for supporting the Blob GC logic for JR2. 

With Oak this map content is not used. As a fix we can either

# Set inUseMap to a Guava Cache Map which has weak keys and value
# Set inUseMap to a no op map where all put calls are ignored
# Modify FDS to disable use of inUseMap or make {{usesIdentifier}} protected

#3 would be a proper fix and #2 can be used as temp workaround untill FDS gets fixed

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-data/src/main/java/org/apache/jackrabbit/core/data/FileDataStore.java#L118",concurrency,['core'],OAK,Improvement,Minor,2014-04-02 11:17:55,19
12705591,Creating multiple checkpoint on same head revision overwrites previous entries,"Currently when a checkpoint is created in DocumentNodeStore then it is saved in form of currentHeadRev=>expiryTime. Now if multiple checkpoints are created where head revision has not changed then only the last one would be saved and previous entries would be overridden as revision is used as key

One fix would be to change the expiry time only if the new expiry time is greater than previous entry. However doing that safely in a cluster (check then save) is currently not possible with DocumentStore API as the modCount check if only supported for Nodes.

",resilience,['mongomk'],OAK,Bug,Minor,2014-03-31 07:13:16,2
12703715,Implement noInternal from JackrabbitEventFilter,Implement the {{noInternal}} flag that will be added with JCR-3759. ,observation,['core'],OAK,Improvement,Minor,2014-03-26 13:15:19,15
12703438,Omit warnings about accessing commit related info when external events are excluded,"Currently we log a warning when calling {{Event.getUserID()}}, {{Event.getUserData()}} or {{Event.getDate()}} without first checking whether the event is not external. However we should inhibit such warnings for the case where the filter already excludes external events. ",observation,['core'],OAK,Improvement,Minor,2014-03-25 12:08:32,15
12702892,org.apache.jackrabbit.oak.plugins.document.mongo.CacheInvalidationIT fails,"Fails frequently on my W7 desktop:

testCacheInvalidationHierarchicalNotExist(org.apache.jackrabbit.oak.plugins.document.mongo.CacheInvalidationIT)  Time elapsed: 0.04 sec  <<< FAILURE!
java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:92)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertTrue(Assert.java:54)
        at org.apache.jackrabbit.oak.plugins.document.mongo.CacheInvalidationIT.testCacheInvalidationHierarchicalNotExist(CacheInvalidationIT.java:171)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
",CI,['mongomk'],OAK,Task,Minor,2014-03-21 16:03:50,19
12702870,DocumentStore-specific test framework,Add tests that test DS implementations directly.,technical_debt test,['mongomk'],OAK,Task,Minor,2014-03-21 13:43:01,3
12702434,DocumentNS: Implement refined conflict resolution for addExistingNode conflicts,Implement refined conflict resolution for addExistingNode conflicts as defined in the parent issue for the document NS.,resilience,['mongomk'],OAK,Sub-task,Major,2014-03-19 17:42:06,2
12702353,OSGi Configuration for Query Limits,"In OAK-1395 we added limits for long running queries. The limits can be changed with system properties, now we should make the settings changeable using OSGi",configuration,['query'],OAK,Improvement,Minor,2014-03-19 10:24:41,4
12702110,Expose RevisionGCMBean for supported NodeStores ,{{NodeStore}} implementations should expose the {{RevisionGCMBean}} in order to be interoperable with {{RepositoryManagementMBean}}. See OAK-1160.,monitoring,"['mongomk', 'segmentmk']",OAK,Improvement,Major,2014-03-18 10:19:10,15
12702108,Expose FileStoreBackupRestoreMBean for supported NodeStores,{{NodeStore}} implementations should expose the {{FileStoreBackupRestoreMBean}} in order to be interoperable with {{RepositoryManagementMBean}}. See OAK-1160.,monitoring,"['mongomk', 'segment-tar']",OAK,Improvement,Major,2014-03-18 10:16:05,1
12702101,Document and test additional BlobStore contracts,"The original BlobStore implementations support an additional contract, which is tested, but so far the applications can't rely on. The contract is that concatenating multiple blobIds is a valid blobId, and means the binaries are concatenated. The use cases are to support partial and concurrent uploads / transfers. Depending on the backend, this can speed up transfers quite a bit. Also, it allows new use cases, for example ""resume upload"" without having to re-upload or stream the existing binary. 

The DataStore implementations don't support those use cases. Now, with the DataStoreBlobStore compatibility wrapper, this contract can't be supported by all BlobStore implementations. That's fine. However, the tests against the other BlobStores should still test this contract.

I will add a new marker interface ""ChunkingBlobStore"" so the unit tests can verify the contract.",api documentation test,['core'],OAK,Improvement,Minor,2014-03-18 09:42:17,4
12701867,Incorrect handling of addExistingNode conflict in NodeStore,"{{MicroKernel.rebase}} says: ""addExistingNode: node has been added that is different from a node of them same name that has been added to the trunk.""

However, the {{NodeStore}} implementation
# throws a {{CommitFailedException}} itself instead of annotating the conflict,
# also treats the equal childs with the same name as a conflict. ",concurrency observation technical_debt,['mongomk'],OAK,Bug,Major,2014-03-17 10:53:23,2
12700973,ShareableNodesTest.testAddShareableMixin failures,"I'm seeing this test fail consistently on our internal CI builds.

_org.apache.jackrabbit.core.observation.ShareableNodesTest.testAddShareableMixin_:
bq. Change processor already stopped

Stacktrace
{code}
java.lang.IllegalStateException: Change processor already stopped
	at com.google.common.base.Preconditions.checkState(Preconditions.java:150)
	at org.apache.jackrabbit.oak.jcr.observation.ChangeProcessor$RunningGuard.stop(ChangeProcessor.java:259)
	at org.apache.jackrabbit.oak.jcr.observation.ChangeProcessor.stop(ChangeProcessor.java:192)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.stop(ObservationManagerImpl.java:267)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.dispose(ObservationManagerImpl.java:117)
	at org.apache.jackrabbit.oak.jcr.session.SessionContext.dispose(SessionContext.java:387)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl$10.perform(SessionImpl.java:465)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl$10.perform(SessionImpl.java:462)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:263)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.safePerform(SessionDelegate.java:306)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.safePerform(SessionImpl.java:129)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.logout(SessionImpl.java:462)
	at org.apache.jackrabbit.test.AbstractJCRTest.cleanUp(AbstractJCRTest.java:439)
	at org.apache.jackrabbit.test.AbstractJCRTest.tearDown(AbstractJCRTest.java:448)
	at org.apache.jackrabbit.test.api.observation.AbstractObservationTest.tearDown(AbstractObservationTest.java:67)
	at junit.framework.TestCase.runBare(TestCase.java:140)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:464)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at org.apache.jackrabbit.test.ConcurrentTestSuite.access$001(ConcurrentTestSuite.java:29)
	at org.apache.jackrabbit.test.ConcurrentTestSuite$2.run(ConcurrentTestSuite.java:67)
	at EDU.oswego.cs.dl.util.concurrent.PooledExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:662)
{code}",observation,['core'],OAK,Bug,Major,2014-03-12 13:53:09,15
12699169,Implement low disk space and low memory monitoring,We should implement these monitoring for those MKs where it makes sense. ,monitoring,['core'],OAK,Sub-task,Minor,2014-03-06 15:03:55,15
12699101,Concurrent FlatTreeWithAceForSamePrincipalTest fails on Oak-Mongo,The benchmark test fails when run concurrently in a cluster. Setting up the test content fails with a conflict. I assume this happens because nodes in the permission store are populated concurrently and may conflict.,concurrency,"['mongomk', 'run']",OAK,Bug,Minor,2014-03-06 09:29:55,2
12698876,"Property index on ""jcr:primaryType"" returns the wrong cost","For queries of type, the property index on jcr:primaryType is used, even if only a subset of all node types are indexed:

{noformat}
/jcr:root//element(*,rep:User)[xyz/@jcr:primaryType]
{noformat}

The problem is that this index returns the wrong cost. It should return ""infinity"", because the index doesn't have enough data if not all node types and mixins are indexed.",performance resilience,"['core', 'query']",OAK,Bug,Minor,2014-03-05 14:25:05,4
12698873,Verify restore to revision on MongoNS,"Following the discussion on OAK-1339, the backup will be controlled from Mongo directly, but we still need to verify 2 things:
 - ongoing transactions will be discarded on restore, the head has to point to the latest stable revision
 - as an added benefit, the restore could happen to an older revision (see the sharded setup where a node can get ahead of the others between the moment the backup starts and when it will finish across the board)",production,['mongomk'],OAK,Improvement,Critical,2014-03-05 14:15:03,16
12698610,ObservationTest failure on Windows,"{{ObservationTest}} fails often on some Windows machines:

{noformat}
pathFilter[3](org.apache.jackrabbit.oak.jcr.observation.ObservationTest)  Time elapsed: 0.864 sec  <<< FAILURE!
java.lang.AssertionError: Missing events: [path = /events/only/here/below/this, type = 1]
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationTest.pathFilter(ObservationTest.java:315)
{noformat}

and

{noformat}
filterPropertyOfParent[2](org.apache.jackrabbit.oak.jcr.observation.ObservationTest)  Time elapsed: 0.88 sec  <<< FAILURE!
java.lang.AssertionError: Missing events: [path = /test_node/a/jcr:primaryType, type = 4, path = /test_node/a/foo, type = 4, path = /test_node/a/b, type = 1]
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationTest.filterPropertyOfParent(ObservationTest.java:614)
{noformat}

I have the suspicion this is an issue with the test similar to OAK-1486",observation,"['core', 'jcr']",OAK,Bug,Major,2014-03-04 11:13:04,15
12698426,BackgroundObserverTest occasionally failing,"{{BackgroundObserverTest.concurrentObservers}} occasionally fails for the Windows 7 CI build:

{noformat}
concurrentObservers(org.apache.jackrabbit.oak.spi.commit.BackgroundObs)  Time elapsed: 5.058 sec  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserverTest.concurrentObservers(BackgroundObserverTest.java:62)
{noformat}",observation,['core'],OAK,Bug,Major,2014-03-03 13:57:15,15
12697726,Failing test for MergeSortedIterators,"While running Oak in a two node clutser following exception is seen. It basically comes because the AsynchUpdate tries to update async-status concurrently

{noformat}
27.11.2013 17:56:35.507 *ERROR* [pool-5-thread-1] org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@fcf98c2 : com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diff(MongoMK.java:165) ~[na:na]
	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:481) ~[na:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:103) ~[na:na]
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler:2.4.2]
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler:2.4.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.getNode(MongoNodeStore.java:507) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffFewChildren(MongoMK.java:313) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffImpl(MongoMK.java:229) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:168) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:165) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 11 common frames omitted
Caused by: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:103) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.next(MergeSortedIterators.java:85) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getLatestValue(NodeDocument.java:1041) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getNodeAtRevision(NodeDocument.java:456) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.readNode(MongoNodeStore.java:653) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.access$000(MongoNodeStore.java:80) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:510) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:507) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 23 common frames omitted
{noformat}",cluster,['mongomk'],OAK,Bug,Major,2014-02-27 15:25:45,2
12696542,Many extra events are dispatched from a move event,"When moving a node many extra events are dispatched in OAK in compared to other implementations

On Oak a node added and node remove events are dispatched for each node in the hierarchy being moved.  As well there is a property add and property remove event dispatched for each property in the node hierarchy.  

This compares to previous implementations where only a Node Moved, node added and node removed event is dispatched for the parentnode being moved.

See [0] for an example.

For me this is problematic for a couple of reasons:

1) We are dispatching more events than we did previously.  In cases where nodes are frequently moved this will add extra load on the system. 
2) It is becoming increasingly difficult to ignore events related to a move without spending extra cycles to make that determination. 
3) Many pre-existing event listeners will be executing on events that they previously would not have.

I know the JCR spec indicates that an implementation may choose to dispatch these events or not, but I suggest we change OAK to not throw these extra events.  If we do not many observation listeners will act on events they previously did not will likely cause problems.

Also, if we could add a simple marker in any event’s info map which is related to a node move (ie: the node removed, node added etc) it would be very helpful when trying to ignore events caused by a move.  (which I believe to be the case in many situations).

[0] 
Move “c” in the hierarchy below from /a/b to /a/z:

/a/b/c/d/e
to:
/a/z/c/d/e

Results in:

CRX2:
/a/b, type: {node removed}
/a/z/b, type: {node added}
/a/z/b, type: {node moved}

OAK:
/a/b/c, type: {node removed}
/a/z/c, type: {node moved}
/a/z/c, type: {node added}
/a/b/c/jcr:primaryType, type: {property removed}
/a/b/c/jcr:createdBy, type: {property removed}
/a/b/c/jcr:created, type: {property removed}
/a/b/c/d, type: {node removed}
/a/z/c/jcr:primaryType, type: {property added}
/a/z/c/jcr:createdBy, type: {property added}
/a/z/c/jcr:created, type: {property added}
/a/z/c/d, type: {node added}
/a/b/c/d/jcr:primaryType, type: {property removed}
/a/b/c/d/jcr:createdBy, type: {property removed}
/a/b/c/d/jcr:created, type: {property removed}
/a/b/c/d/e, type: {node removed}
/a/z/c/d/jcr:primaryType, type: {property added}
/a/z/c/d/jcr:createdBy, type: {property added}
/a/z/c/d/jcr:created, type: {property added}
/a/z/c/d/e, type: {node added}
/a/b/c/d/e/jcr:primaryType, type: {property removed}
/a/b/c/d/e/jcr:createdBy, type: {property removed}
/a/b/c/d/e/jcr:created, type: {property removed}
/a/z/c/d/e/jcr:primaryType, type: {property added}
/a/z/c/d/e/jcr:createdBy, type: {property added}
/a/z/c/d/e/jcr:created, type: {property added}
",observation,['jcr'],OAK,Improvement,Major,2014-02-21 14:10:42,15
12696536,JCR version purge,Expose capability to purge JCR versions so that higher level apps can start a clean up.,production resilience,['jcr'],OAK,Improvement,Major,2014-02-21 13:53:37,2
12696534,Non-blocking reindexing,"For huge Oak repos it will be essential to re-index some or all indexes in case they go out of sync in a non-blocking way (i.e. the repo is still operation while the re-indexing takes place).

For an asynchronous index this should not be much of a problem. One could drop it and recreate (as an added benefit it might be nice if the user could simply add a property ""reindex"" to the index definition node to trigger this).

For synchronous indexes, I suggest the mechanism creates an asynchronous index behind the scenes first and once it has caught up
* blocks writes (?)
* removes the existing synchronous index
* moves asynchronous index in its place and makes it synchronous",production resilience,['query'],OAK,Improvement,Blocker,2014-02-21 13:49:13,14
12696532,document oak:unstructured performance advantages,"For long child node lists it is much better (in terms of performance) to use a non-ordered node type. Unfortunately, nt:unstructured is ordered.
We should have a ""performance hint"" on this in the docs.",documentation,"['doc', 'jcr']",OAK,Task,Trivial,2014-02-21 13:42:25,15
12696529,Warn on huge multi-valued properties,It is an explicit design non-goal of Oak to support huge amounts of values in multi-valued properties. If a user still tries to create these we should at least throw a WARN in the logs to indicate that usage of MVPs is wrong.,production resilience,['jcr'],OAK,Improvement,Trivial,2014-02-21 13:40:13,15
12696515,Expose query plans,"For debugging slow queries we need a way to analyze: which indexes where used and why (what were the cost responses of the non-used indexes).
This is related to OAK-1217",production resilience,['query'],OAK,Improvement,Minor,2014-02-21 12:27:40,4
12696504,Tool to detect and possibly fix permission store inconsistencies,"I think we should prepare for cases where the permission store (managed as a tree mirrored to the content tree) goes out of sync with the content tree for whatever reason.

Ideally, that would be an online tool (maybe exposed via JMX) that goes back the MVCC revisions to find the offending commit (so that have a chance to reduce the number of such occurences) and fixes the inconsistency on head.",production resilience tools,['core'],OAK,Sub-task,Minor,2014-02-21 12:08:02,0
12696503,Offline tool to repair MongoMK documents,"For cases where the document semantics in Mongo that are created by Oak get corrupted to a point that Oak does not come up anymore (but MongoDB is still available), we should have a mechanism to fix those inconsistencies.

Of course, one could use Mongo tools like cmdline or MongoHub to manually go in, but an automated approach would be preferable in the medium term.",production resilience tools,['mongomk'],OAK,Improvement,Minor,2014-02-21 12:03:16,19
12696502,Offline tool to repair TarMK,"We should have a tool to inspect and repair TarMK files in case the repository does not come up due to a corruption in the tar files.

Ideally, the tool could be pointed to an existing backup and use the backup to fix broken binaries (that might have been erroneously been deleted by the DS GC).

Once we have the tool, we could automatically run it after backups and on the failover instance (OAK-1161)",production resilience tools,['segmentmk'],OAK,Improvement,Major,2014-02-21 11:55:53,24
12693122,Occasional ConcurrentFileOperationsTest failure,"Most recent test failure on buildbot http://ci.apache.org/builders/oak-trunk/builds/4290/steps/compile/logs/stdio says:

{noformat}
concurrent[2](org.apache.jackrabbit.oak.jcr.ConcurrentFileOperationsTest)  Time elapsed: 1.69 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: OakState0001: Unresolved conflicts in /test-node/session-6
{noformat}",concurrency,"['core', 'mongomk']",OAK,Bug,Minor,2014-02-04 15:24:49,2
12692131,Only one Observer per session,"As mentioned in OAK-1332, a case where a single session registers multiple observation listeners can be troublesome if events are delivered concurrently to all of those listeners, since in such a case the {{NamePathMapper}} and other session internals will likely suffer from lock contention.

A good way to avoid this would be to have all the listeners registered within a single session be tied to a single {{Observer}} and thus processed sequentially.

Doing so would also improve performance as the listeners could leverage the same content diff. As the listeners come from a single session and thus presumably from a single client, there's no need to worry about one client blocking the work of another.",observation,['jcr'],OAK,Improvement,Major,2014-01-29 20:32:24,15
12691621,DocumentNodeState#compareAgainstBaseState too slow,"{{DocumentNodeState#compareAgainstBaseState}} usually falls back to the default implementation in {{AbstractNodeState#compareAgainstBaseState(NodeState, NodeStateDiff)}}, which is slow. See also the TODO in the code. This negatively affects performance when generation observation events. ",observation,['mongomk'],OAK,Improvement,Major,2014-01-28 11:51:42,2
12691096,Expose the preferred transient space size as repository descriptor ,"The problem is that the different stores have different transient space characteristics. for example the MongoMK is very slow when handling large saves.

suggest to expose a repository descriptor that can be used to estimate the preferred transient space, for example when importing content.

so either a boolean like: 
  {{option.infinite.transientspace}}

or a number like:
  {{option.transientspace.preferred.size}}

the later would denote the average number of modified node states that should be put in the transient space before the persistence starts to degrade.",api,"['core', 'jcr']",OAK,Improvement,Major,2014-01-24 19:08:34,19
12689805,Implement RepositoryStatistics from Jackrabbit API,Implement repository statistics (TimeSeries) for those values it makes sense on Oak.,monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2014-01-20 12:35:07,15
12689255,MicroKernel API: clarify semantics of `read` method,"the javadoc of {{MicroKernel#read}} currently states that 

{quote}
An attempt is made to read as many as {{length}} bytes, but a smaller number may be read.
{quote}

under what conditions a smaller amount might be read is not specified. 

with the current specification an api consumer would either have to know the length of the blob in advance (i.e. by calling  {{MicroKernel#getLength}}) or  would need to call the {{MicroKernel#read}} method twice to make sure that the blob content is fully read. 

i suggest to clarify the contract as follows:

Reads up to {{length}} bytes of data from the specified blob into the given array of bytes where the actual number of bytes read is {{min(length, max(0, blobLength - pos))}}.
",documentation,"['mk', 'mongomk', 'segmentmk']",OAK,Improvement,Major,2014-01-16 14:56:26,25
12688733,Reduce calls to MongoDB,As discussed with Chetan offline we'd like to reduce the number of calls to MongoDB when content is added to the repository with a filevault package import.,performance,"['core', 'mongomk']",OAK,Improvement,Major,2014-01-14 13:21:44,2
12688013,Bundle nodes into a document,"For very fine grained content with many nodes and only few properties per node it would be more efficient to bundle multiple nodes into a single MongoDB document. Mostly reading would benefit because there are less roundtrips to the backend. At the same time storage footprint would be lower because metadata overhead is per document.

Feature branch - https://github.com/chetanmeh/jackrabbit-oak/compare/trunk...chetanmeh:OAK-1312

*Feature Docs* - http://jackrabbit.apache.org/oak/docs/nodestore/document/node-bundling.html",performance,"['core', 'documentmk']",OAK,New Feature,Major,2014-01-09 07:55:27,19
12684964,RandomizedReadTest fails with OutOfMemoryError: PermGen space,"This happened while running the maven build with {{-PintegrationTesting}}:

{code}
Running org.apache.jackrabbit.oak.jcr.random.RandomizedReadTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.418 sec
org.apache.maven.surefire.util.SurefireReflectionException: java.lang.reflect.InvocationTargetException; nested exception is java.lang.reflect.InvocationTargetException: null
java.lang.reflect.InvocationTargetException
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space

Results :

Tests run: 722, Failures: 0, Errors: 0, Skipped: 48
{code}

The crucial point being Surefire silently ignoring the following tests such that the build happily succeeds making following failures. Note, that test suite consists of  2003 tests in contrast to the 722 reported by Surefire. ",test,['jcr'],OAK,Bug,Major,2013-12-16 14:30:35,15
12684921,ObservationManager#removeEventListener prone to deadlocks ,"The contract for {{ObservationManager#removeEventListener}} mandates: ""A listener may be deregistered while it is being executed. The deregistration method will block until the listener has completed executing.""

However a strict implementation of this contract is prone to deadlocks: clients unregistering event listeners need to take care not to hold a lock that is also acquired from the event listener being unregistered as this will lead to a deadlock
",observation,['jcr'],OAK,Improvement,Major,2013-12-16 11:03:06,15
12684481,"Root.commit(String, CommitHook) : CommitHook is not part of oak-api","[~fmeschbe] had a look at the oak api and spotted the following problem:

Root#commit(String, CommitHook)

But the CommitHook interface is not part of the OAK API. we quickly searched for usages and found that this is only used for the Item#save case in oak-jcr to assert that the set of modifications is contained with the subtree defined by the specified target item.

IMO we should get rid of the flavour of Root#commit again and solve the Item-save issue differently. For example we could change it to Root#commit(String, String absPath) where the absPath would be the path of the target item...",api,['core'],OAK,Bug,Major,2013-12-13 16:10:59,15
12684290,TCK tests slow on SegmentMK+Mongo,"The tests take a very long time to complete on my machine. Most likely this is also the case on travis and the reason why recent builds time out.

I'll attach a failsafe report.",performance test,['core'],OAK,Bug,Major,2013-12-12 15:33:40,24
12684282,Clean up RepositoryStub classes,"There are various overlapping RepositoryStub classes that need some clean up.

A while ago we decided to switch to Oak+TarMK as default TCK setup. The TCK configuration still points to OakRepositoryStub, which is derived from OakRepositoryStubBase. In OAK-1207 we changed OakRepositoryStubBase to use the TarMK. This duplicates code in OakTarMKRepositoryStub.",test,['jcr'],OAK,Task,Minor,2013-12-12 15:21:31,2
12682951,Failure in ObservationRefreshTest ,"Failed tests:   observation[2](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest): added nodes expected:<1000> but was:<442>

Tests run: 4, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 106.957 sec <<< FAILURE!
observation[3](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 53.047 sec  <<< FAILURE!
java.lang.AssertionError: added nodes expected:<1000> but was:<906>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
observation[2](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 58.379 sec  <<< FAILURE!
java.lang.AssertionError: added nodes expected:<1000> but was:<396>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
",observation,['jcr'],OAK,Bug,Major,2013-12-05 17:29:06,15
12682124,Make AsynchIndexUpdate task to run only on a single node in a cluster,"Currently the {{AsynchIndexUpdate }} job which performs the indexing in background is run on every node in a cluster. This at times causes commit failures when running Oak in a cluster using Mongo MK. As merging of indexed content say Lucene is tricky to implement it would be better to restrict this job to run as a singleton in a cluster

See http://markmail.org/thread/qff2fj7nqtbuhr4i for more discussion",cluster,['core'],OAK,Improvement,Major,2013-12-02 12:30:23,19
12681562,IllegalStateException in MergeSortedIterators,"While running Oak in a two node clutser following exception is seen. It basically comes because the AsynchUpdate tries to update async-status concurrently

{noformat}
27.11.2013 17:56:35.507 *ERROR* [pool-5-thread-1] org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@fcf98c2 : com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diff(MongoMK.java:165) ~[na:na]
	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:481) ~[na:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:103) ~[na:na]
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler:2.4.2]
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler:2.4.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.getNode(MongoNodeStore.java:507) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffFewChildren(MongoMK.java:313) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffImpl(MongoMK.java:229) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:168) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:165) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 11 common frames omitted
Caused by: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:103) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.next(MergeSortedIterators.java:85) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getLatestValue(NodeDocument.java:1041) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getNodeAtRevision(NodeDocument.java:456) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.readNode(MongoNodeStore.java:653) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.access$000(MongoNodeStore.java:80) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:510) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:507) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 23 common frames omitted
{noformat}",cluster,['mongomk'],OAK,Bug,Major,2013-11-27 12:45:23,2
12681420,"Write access control of ""touched"" content","Following up from OAK-928 and OAK-948 to clarify the interesting case of updates that set a property (or a broader subtree of content) to the exact same value that it used to have.

Since such ""touching"" of content results in an empty content diff, the {{PermissionValidator}} doesn't get invoked and thus write access controls are not checked. Additionally (as reported in OAK-948) no observation events get sent for such updates. This seems like a reasonable thing to do, as if nothing changes there should be no need to check for write access or to inform observation listeners.

However, OAK-928 makes this case trickier, as it opens a possibility to use brute force to circumvent read access controls for certain kinds of content. For example, if an attacker knows (or can guess) the existence of a certain read/write-protected property (e.g. some sensitive configuration setting), it's possible to repeatedly try to update that property with different likely values. Normally the update would fail with an exception because of the write protection, but when the attempted  update matches the stored value there would be no exception because no change gets detected. At that point the attacker would know what the stored value is!

The above scenario is somewhat artificial as it only works for highly specific cases, so I'm not sure how important it is for us to address this case at the repository level.

If we don't address this then a simple workaround for security-sensitive content would be to deny read access to the whole containing node and add a property containing a random value along the sensitive information. That would make it impossible for the attacker to use this mechanism to guess the sensitive bits, as they'd also need to guess what the random value is.",security,['core'],OAK,Improvement,Major,2013-11-26 21:39:31,0
12680757,support for QueryStat MBean,"The JMX bindings for {{QueryStat}} are available in Jackrabbit, but not in Oak. Having support for this in Oak would make it easy to identify long running queries.",production resilience,['query'],OAK,New Feature,Major,2013-11-22 14:16:31,4
12679183,DocumentNodeStore: annotate conflicts,Conflict handling is mostly implemented in MongoMK but it does not yet annotate conflicts on rebase.,technical_debt,['documentmk'],OAK,Bug,Minor,2013-11-14 09:15:19,2
12678986,Security Concerns wrt Index Definitions,"see http://markmail.org/message/5ivmvhi7jbuo3jp6 for the initial request for discussion.

biggest pain points from a security perspective:

- missing protection or concept for protection in a default setup
- location of the index definitions
- node types of the nodes associated with index definitions and index content",security,['core'],OAK,Bug,Critical,2013-11-13 10:34:12,0
12678216,Generic interfaces for operation tasks,"Could we add generic (i.e. MK independent) interfaces that can be used by higher levels to trigger certain ops tasks? The the application could decide when would be a good time to run them.
I am thinking especially about backup/restore (OAK-1158), MVCC revision cleanup (OAK-1158) and DSGC (OAK-377)",monitoring,"['core', 'mk']",OAK,New Feature,Major,2013-11-08 12:45:14,15
12676732,Observation listener PLUS,"Oak should provide an *extended and efficient JCR observation listener* mechanism to support common use cases not handled well by the restricted options of the JCR observation (only base path, node types and raw events). Those cases require listeners to register much more broadly and then filter out their specific cases themselves, thus putting too many events into the observation system and creating a huge overhead due to asynchronous access to the modified JCR data to do the filtering. This easily is a big performance bottleneck with many writes and thus many events.

Previous discussions [on the list|http://markmail.org/message/oyq7fnfrveceemoh] and in OAK-1120, and [latest discussion on the list|http://markmail.org/message/x2l6tv4m7bxjzqqq].

The goals should be:
* performance: handle filtering as early as possible, during the commit, where access to the modified data is already present
* provide robust implementation for typical filtering cases
* provide an asynchronous listener mechanism as in JCR
* minimize effect on the lower levels on Oak (a visible addition in oak-commons or oak-jcr should be enough)
* for delete events, allow filtering on the to-be-deleted data (currently not possible in jcr listeners that run after the fact)
* ignore external cluster events by default; have an extra option if you really want to register for external events
* if possible: design as an extension of the jcr observation to simplify migration for existing code
* if possible: provide an intelligent listener that can work with pure JCR (aka Jackrabbit 2) as well, by falling back to in-listener-filtering
* maybe: synchronous option using the same simple interface (instead of raw Oak plugins itself); however, not sure if there is a benefit if they can only read data and not change or block the session commit

Typical filtering cases:
- paths with globbing support (for example /content/foo/*/something)
- check for property values (equal, not equal, contains etc.), most importantly
sling:resourceType in Sling apps
- allow to check properties on child nodes as well, typically jcr:content
- check for any parent/ancestor as well (e.g. change deep inside a node type = foo structure should be triggered, even if the node with the type wasn't modified; very important to support efficiently)
- node types (already in jcr observation)
- created/modified/deleted events, separate from move/copy
- and more... a custom filter should be possible to pass through (with similar access as the {{Observer}})",observation performance,"['commons', 'jcr']",OAK,New Feature,Major,2013-10-30 22:30:24,15
12675263,Immediate delivery of events from local commits ,"As discussed in http://markmail.org/message/otpckosnwfzjvqoj, it should be possible to deliver observation events from local commits immediately instead of waiting for the default one-second polling delay.",observation,"['core', 'jcr']",OAK,Improvement,Major,2013-10-23 14:44:50,24
12675262,Support user data in local events,"With the new CommitInfo mechanism it should be possible to pass user data along with local commits.

The current user data implementation in ObservationManagerImpl is incorrect, as it just attaches the given user data to any events delivered to that session instead of attaching it to the commits done by that session.",observation,"['core', 'jcr']",OAK,New Feature,Major,2013-10-23 14:42:53,24
12672808,Query with descendent node and access control fails to return result,"The scenario is bit complex. Running a query with following condition does not give any result

*  Node path is like {{/home/users/geometrixx-outdoors/emily.andrews@mailinator.com/social/relationships/following/aaron.mcdonald@mailinator.com}}
* It has a Glob jcr:read for everyone at {{\*/social/relationships/following/\*}}
* The query is like 
bq. /jcr:root/home//social/relationships/following//*[id='aaron.mcdonald@mailinator.com']
* The query is executed with anonymous session

On JR2 it returns expected result while on Oak it does not give any result",compatibility,"['core', 'security']",OAK,Bug,Minor,2013-10-08 12:57:15,4
12671609,Periodically poll for external events,"Currently external events are only reported along with local changes. That is, when local changes are persisted external changes are detected and reported along with the local changes. This might cause external events to be delayed indefinitely on cluster nodes without writes. 

We might want to implement a solution that regularly polls for external events. 
See OAK-1055 for why a previous implementation didn't work. 

",observation,['core'],OAK,Improvement,Minor,2013-10-01 16:19:55,15
12671374,Occasional test failure in ObservationTest.observation(),"The test occasionally fails with
{code}
Failed tests:
observation[1](org.apache.jackrabbit.oak.jcr.observation.ObservationTest):
Unexpected events: [EventImpl{type=8, jcrPath='/test_node/property',
userID='oak:unknown', identifier='/test_node', info={}, date=0,
userData=null, external=true}, EventImpl{type=16,
jcrPath='/test_node/n1/p1', userID='oak:unknown',
identifier='/test_node/n1', info={}, date=0, userData=null,
external=true}, EventImpl{type=4, jcrPath='/test_node/n1/p2',
userID='oak:unknown', identifier='/test_node/n1', info={}, date=0,
userData=null, external=true}, EventImpl{type=2, jcrPath='/test_node/n3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=8,
jcrPath='/test_node/n3/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n3', info={}, date=0, userData=null,
external=true}, EventImpl{type=8, jcrPath='/test_node/n3/p3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=2, jcrPath='/test_node/{4}',
userID='oak:unknown', identifier='/test_node/{4}', info={}, date=0,
userData=null, external=true}, EventImpl{type=8,
jcrPath='/test_node/{4}/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/{4}', info={}, date=0, userData=null,
external=true}, EventImpl{type=1, jcrPath='/test_node/n2',
userID='oak:unknown', identifier='/test_node/n2', info={}, date=0,
userData=null, external=true}, EventImpl{type=4,
jcrPath='/test_node/n2/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n2', info={}, date=0, userData=null,
external=true}, EventImpl{type=4, jcrPath='/test_node/property',
userID='oak:unknown', identifier='/test_node', info={}, date=0,
userData=null, external=true}, EventImpl{type=16,
jcrPath='/test_node/n1/p1', userID='oak:unknown',
identifier='/test_node/n1', info={}, date=0, userData=null,
external=true}, EventImpl{type=8, jcrPath='/test_node/n1/p2',
userID='oak:unknown', identifier='/test_node/n1', info={}, date=0,
userData=null, external=true}, EventImpl{type=2, jcrPath='/test_node/n2',
userID='oak:unknown', identifier='/test_node/n2', info={}, date=0,
userData=null, external=true}, EventImpl{type=8,
jcrPath='/test_node/n2/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n2', info={}, date=0, userData=null,
external=true}, EventImpl{type=1, jcrPath='/test_node/n3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=4,
jcrPath='/test_node/n3/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n3', info={}, date=0, userData=null,
external=true}, EventImpl{type=4, jcrPath='/test_node/n3/p3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=1, jcrPath='/test_node/{4}',
userID='oak:unknown', identifier='/test_node/{4}', info={}, date=0,
userData=null, external=true}, EventImpl{type=4,
jcrPath='/test_node/{4}/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/{4}', info={}, date=0, userData=null,
external=true}]
{code}

As [noted before | http://markmail.org/message/lk3vrrcn5edib73d]  having {{external=true}} and also the event types indicate that the events are being seen ""in reverse"" (i.e. reverse diffing of the node states involved). ",observation,['core'],OAK,Bug,Major,2013-09-30 14:49:11,15
12669503,Avoid turning multivalued properties to String arrays,"Many places especially in the security code use String arrays as an intermediate representation for multivalued name and string properties. Unfortunately this practice leads to quite a bit of extra memory allocation and extra work in performance-critical places like AC evaluation.

For example, a significant percentage of the time in the SetProperty benchmark goes to PrivilegeUtil.readDefinitions(), just because of the array conversion that requires a number of extra object allocations and at least two extra iterations over the relevant value strings.

That extra work could be avoided in readDefinitions() and other similar places if the Iterable<String> return type of PropertyState.getValues() was used directly instead of a String array.",performance,['core'],OAK,Improvement,Major,2013-09-19 18:14:28,24
12668260,Optimise path parsing,"As Jukka [mentioned | https://issues.apache.org/jira/browse/OAK-978?focusedCommentId=13751242&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13751242] on OAK-978, is often on the critical path and the changes done there had a bad impact on performance:

{code}
Apache Jackrabbit Oak
# ReadPropertyTest               min     10%     50%     90%     max       N
Jackrabbit                         4       5       5       6      14   11287
Oak-Tar                           14      15      16      16      27    3855
{code}

Until we are able to come up with a better solution that separates parsing from name mapping, I suggest to use the following heuristic to shortcut path parsing: shortcut iff the JCR path does not start with a dot, does not contain any of {}[]/ and if it contains a colon the session does not have local re-mappings.
",performance,['core'],OAK,Sub-task,Major,2013-09-12 14:20:44,15
12663170,Query: Filter doesn't contain fulltext constraints from joins ,"Example query:
{code}
SELECT a.* 
FROM [nt:unstructured] AS a 
INNER JOIN [nt:unstructured] AS b 
ON b.[jcr:uuid] = a.testref 
WHERE a.type = 'child' 
AND (CONTAINS(a.*, 'testJoinWithOR4') OR b.type = 'parent' AND CONTAINS(b.*, 'testJoinWithOR4'))
{code}

I'm not sure why this happens, but I noticed stepping through the code that the filter generated on the query doesn't contain any fulltext constraints. It does however contain the 'type' info which will trick the query engine into picking a property index, failing the test because is returns more results than it should.

See failing tests on the lucene module:
 - org.apache.jackrabbit.core.query.JoinTest#testJoinWithOR4
 - org.apache.jackrabbit.core.query.JoinTest#testJoinWithOR5",performance resilience,"['core', 'query']",OAK,Bug,Minor,2013-08-12 08:49:06,4
12662067,MBean to track sessions,"Create JMX MBean to track Session and session related information:

* stack trace from where the session has been acquired,
* age of the session,
* last (read/write) access to the session,
* last refresh of the session,
* conflict information (e.g. unresolved conflicts),
* session attributes,
* ...

",monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2013-08-06 08:31:18,15
12660929,Query engine index selection tweaks: shortcut and hint,"This issue covers 2 different changes related to the way the QueryEngine selects a query index:

 Firstly there could be a way to end the index selection process early via a known constant value: if an index returns a known value token (like -1000) then the query engine would effectively stop iterating through the existing index impls and use that index directly.

 Secondly it would be nice to be able to specify a desired index (if one is known to perform better) thus skipping the existing selection mechanism (cost calculation and comparison). This could be done via certain query hints [0].


[0] http://en.wikipedia.org/wiki/Hint_(SQL)

",candidate_oak_1_6 performance,['query'],OAK,Improvement,Critical,2013-07-31 16:11:20,4
12659287,Concurrent commits may cause duplicate observation events,"Chetan discovered that in some cases spurious observation events would be created when to sessions save concurrently. In a nutshell the problem occurs since the current implementation of observation expects a linear sequence of revisions (per cluster node). However on Root.commit there is a small race between rebasing and merging a branch: when another session saves inside this time frame, its branch will have the same base revision like that of the former session. In this case the sequence of revisions is effectively non linear.

Full discussion: http://markmail.org/message/cbzrztagurplxo4r",observation,['core'],OAK,Bug,Major,2013-07-23 08:55:00,15
12659063,Optimize namespace lookups,"The current namespace handling code does a lot of repetitive work, which shows up in hotspots like XML imports and Sling's namespace mapping code.",performance,"['core', 'jcr']",OAK,Sub-task,Major,2013-07-22 11:13:31,24
12657954,Run Jackrabbit Observation tests,"Similar to OAK-237, I'd like to import the existing Observation tests from Jackrabbit and run them against an Oak repository",observation,"['core', 'jcr']",OAK,Improvement,Major,2013-07-16 09:06:17,14
12653704,Generating observation events takes too long when intermediate save calls are involved,Creating observation events is much more expensive when a transaction is broken down through intermediate save calls compared to only having a single save call. ,observation,['core'],OAK,Improvement,Major,2013-06-19 13:45:39,15
12653663,SegmentMK: File backend restart problem due to missing padding,The TarMK fails to properly reopen a repository if its size is more than 256MB and the last entry in a data tar file isn't aligned at the end of the file. This can happen for example if a large transaction or binary is being persisted across tar file boundaries.,TarMK,['core'],OAK,Sub-task,Major,2013-06-19 08:07:42,24
12652344,Mangement info and statistics for observation listeners,"Observation listeners (OAK-144) might create backward compatibility issues. To ease the transition we should provide useful information about registered listeners e.g.:
* Number of listeners
* Session (user) a listener belongs to
* Filter set for the listener
* Number of events fired
* Last couple of events fired
* Size of pending queue
* ...",monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2013-06-12 08:57:40,15
12652343,Expose repository management data and statistics,We should come up with a way to expose repository management information and statistics. See JCR-2936 plus subtasks and JCR-3243 for how this is done in Jackrabbit 2. See [this discussion | http://apache-sling.73963.n3.nabble.com/Monitoring-and-Statistics-td4021905.html] for an alternative proposal.,monitoring,"['core', 'jcr']",OAK,New Feature,Major,2013-06-12 08:50:15,15
12652187,Enable stats for various caches used in Oak by default,"To get a better picture around the usage of cache it would be helpful to enable the [statistics|http://code.google.com/p/guava-libraries/wiki/CachesExplained#Statistics] for various caches used in Oak

{code:java}
nodeCache = CacheBuilder.newBuilder()
                        .weigher(...)
                        .maximumWeight(...)
                        .recordStats()
                        .build();
{code}

Once enabled it allows to get stats like below
{noformat}
CacheStats{hitCount=763322, missCount=51333, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=3496}
{noformat}

As stats collection adds a very minor overhead we can look into making this setting configurable. 

Untill we expose the stats via JMX one can extract the value in Sling env via approach mentioned in [this gist|https://gist.github.com/chetanmeh/5748650]",monitoring,[],OAK,Sub-task,Minor,2013-06-11 09:37:58,19
12645620,MBean to track observation listeners,"It would be useful to have a JMX MBean (or something similar) that kept track of and exposed information about all registered JCR observation listeners. The tracked information should include things like where the listener was registered (stack trace), what filter settings are used, and whether the listener is using methods like isExternal() or getUserID().",monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2013-05-01 09:54:53,24
12643898,File backend for the SegmentMK,"The SegmentMK now has two backends: one in-memory and the other based on MongoDB. Since MongoDB isn't readily available everywhere and the in-memory backend obviously isn't suited for production use, it would be useful to have an additional SegmentMK backend based on just the local file system.

Inspired by, though not directly based on, the proprietary TarPM in CRX, I propose to implement such a backend using the tar file format as the basis.

Unlike the MongoDB backend or the original TarPM, such a ""TarMK"" backend would be designed primarily for optimum performance on a *single-node* deployment, i.e. without support for clustering. A typical deployment could be replicated set of independent copies of a repository, designed for massively parallel read-only or mostly-read workloads.",TarMK,['core'],OAK,Sub-task,Major,2013-04-22 09:20:26,24
12642260,Implement backward compatible observation,"As [discussed | http://markmail.org/message/6bqycmx6vbq7m25c] we might want look into implementing an alternative approach to observation, which trades some scalability for improved backward compatibility. ",observation,"['core', 'jcr']",OAK,New Feature,Major,2013-04-12 15:44:22,15
12639538,Fix sql2 test errors,This patch fixes the errors occurred while executing the test,oak solr test unit-test,['solr'],OAK,Sub-task,Minor,2013-03-28 09:56:09,10
12637752,Optimize access to node type information,Many transient modifications require access to relevant node type information. These accesses should be optimized as they currently add significant overhead.,performance,"['core', 'jcr']",OAK,Sub-task,Major,2013-03-19 12:50:17,24
12637725,Optimize Session.getNamespacePrefixes(),"Apache Sling uses Session.getNamespacePrefixes() quite often. E.g. when reading content through a JcrPropertyMap:

{code}
   java.lang.Thread.State: RUNNABLE
        at java.util.HashMap.put(HashMap.java:372)
        at org.apache.jackrabbit.oak.plugins.name.Namespaces.getNamespaceMap(Namespaces.java:64)
        at org.apache.jackrabbit.oak.plugins.name.ReadOnlyNamespaceRegistry.getPrefix(ReadOnlyNamespaceRegistry.java:116)
        at org.apache.jackrabbit.oak.jcr.SessionImpl.getNamespacePrefix(SessionImpl.java:529)
        - locked <0x00000007daf6c590> (a java.util.HashMap)
        at org.apache.jackrabbit.oak.jcr.SessionImpl.getNamespacePrefixes(SessionImpl.java:495)
        at org.apache.sling.jcr.resource.JcrPropertyMap.escapeKeyName(JcrPropertyMap.java:381)
        at org.apache.sling.jcr.resource.JcrPropertyMap.read(JcrPropertyMap.java:344)
        at org.apache.sling.jcr.resource.JcrPropertyMap.get(JcrPropertyMap.java:126)
        at org.apache.sling.jcr.resource.JcrPropertyMap.get(JcrPropertyMap.java:147)
{code}

I'd like to optimize the case when there are not session re-mapped namespaces. In this case the prefixes can be returned from the namespace registry as is. This avoids the loop and reduces calls on the Oak API. Initial testing shows a performance improvement by a factor of 3.",performance,['jcr'],OAK,Sub-task,Minor,2013-03-19 09:00:29,2
12635546,Observation generates NPE in an existing EventListener,"Because there is no user id passed on to the events generated by the _ChangeProcessor_, the sling EventListener throws a bunch of NPEs when it receives the events.

{code}
06.03.2013 11:33:13.866 *ERROR* [pool-4-thread-1] org.apache.jackrabbit.oak.plugins.observation.ChangeProcessor Unable to generate or send events java.lang.NullPointerException
at java.util.Hashtable.put(Hashtable.java:394)
at org.apache.sling.jcr.resource.internal.JcrResourceListener.sendOsgiEvent(JcrResourceListener.java:298)
at org.apache.sling.jcr.resource.internal.JcrResourceListener.onEvent(JcrResourceListener.java:218)
at org.apache.jackrabbit.oak.plugins.observation.ChangeProcessor$EventGeneratingNodeStateDiff.sendEvents(ChangeProcessor.java:154)
at org.apache.jackrabbit.oak.plugins.observation.ChangeProcessor.run(ChangeProcessor.java:117)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
{code}",observation,['core'],OAK,Improvement,Major,2013-03-06 10:46:34,15
12635349,Optimise TreeImpl.getBaseState() ,"Currently {{TreeImpl.getBaseState()}} calculates the base state of the tree on the fly on each call. As it turns out this method ends up being called by nearly every JCR method call. As recalculation is somewhat expensive since it recursively needs to calculate the base states of all parent trees, an optimisation would be to pre calculate the base state on instance creation.",performance,[],OAK,Sub-task,Minor,2013-03-05 15:00:56,15
12624464,Inconsistent journal with concurrent updates,"As discussed on the list [1] the journal might be inconsistent when merges are involved. 

[1] http://markmail.org/message/4xwfwbax3kpoysbp",concurrency,['mk'],OAK,Bug,Major,2012-12-18 13:25:36,25
12616917,Remove exported packages from Mongo MK Bundle,The oak-mongomk bundle currently exports couple of packages which are not required to be exported. These exports should be removed,osgi,['mongomk'],OAK,Task,Minor,2012-11-20 11:27:02,19
12613758,OSGi related dependencies should be set to provided scoped and not marked as optional,"Currently the OSGi related dependencies org.osgi.core and org.osgi.compendium are marked as optional. Due to this packages under org.osgi.* are marked as optional which is not correct. 

As such dependencies are already marked as provided scope they would not be included as part of transient dependencies. For more details refer to [1]

Fix: The optional flag should be removed

[1] http://markmail.org/thread/njukyten6fdipts3",osgi,['core'],OAK,Bug,Minor,2012-10-27 13:36:16,19
12612238,JMX service to configure auto-cancel or long running queries,"Queries can potentially run for a very long time. I suggest we provide a way to list the running queries, and to cancel them using JMX.

See also http://java.net/jira/browse/JSR_333-49",monitoring,['core'],OAK,Sub-task,Minor,2012-10-17 15:23:31,4
12610547,MongoDB microkernal integration with OSGi,Need to convert the oak-mongomk module to an OSGi bundle and expose the MongoMicroKernel as OSGi service,osgi,['mongomk'],OAK,Sub-task,Major,2012-10-05 11:55:43,19
12608413,Deref (jcr:deref) support,"Test class DerefTest.

For now there are just parse exceptions:
{noformat}
javax.jcr.query.InvalidQueryException: java.text.ParseException: Query:
testroot/people/jcr:deref((*)@worksfor, '*'); expected: <end>
{noformat}
",CI,"['jcr', 'query']",OAK,Sub-task,Major,2012-09-20 09:44:35,4
12607710,Updated Oak site layout,"As discussed on oak-dev@ (http://markmail.org/message/e2pzcdtrxv7aqd6f), a designer at Adobe has contributed an updated site layout for us.",Website documentation,['doc'],OAK,Sub-task,Minor,2012-09-14 20:40:16,24
12603988,JAAS Authentication failing in OSGi env due to classloading issue,"At times Repository login fails because the LoginModule cannot be instantiated 

{noformat}
Caused by: javax.jcr.LoginException: unable to find LoginModule class: org.apache.jackrabbit.oak.security.authentication.LoginModuleImpl
	at org.apache.jackrabbit.oak.jcr.RepositoryImpl.login(RepositoryImpl.java:139)
	at org.apache.jackrabbit.oak.jcr.SessionImpl.impersonate(SessionImpl.java:114)
	at org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:101)
	at $Proxy9.impersonate(Unknown Source)
	at org.apache.sling.jcr.davex.impl.servlets.SlingDavExServlet$1.getLongLivedSession(SlingDavExServlet.java:206)

{noformat}

As a fix LoginContextProviderImpl should switch the Thread's context classloader (TCCL) to oak-jcr bundle's classloader so that required loginmodule class can be instantiated",osgi sling,['jcr'],OAK,Sub-task,Major,2012-08-19 17:33:25,24
12603584,MicroKernelService should set metatype to true to easier configuration,MicroKernelService currently uses @Component annotation without enabling metatype. If metatype is enabled it would simply the configuration of home directory. ,osgi,['mk'],OAK,Improvement,Minor,2012-08-15 13:42:24,19
12603581,Add import for org.h2 in oak-mk bundle,"The oak-mk bundle depends on H2 database. It internally uses Class.forName('org.h2.Driver"") to load the H2 driver. Due to usage of Class.forName Bnd is not able to add org.h2 package to Import-Package list. So it should have an explicit entry in the maven-bundle-plugin config as shown below

{code:xml}
<Import-Package>
  org.h2;resolution:=optional,
  *
</Import-Package>
{code}

Without this MicroKernalService loading would fail with a CNFE",osgi,['mk'],OAK,Bug,Major,2012-08-15 13:34:53,15
12601231,Sling I18N queries not supported by Oak,"The Sling I18N component issues XPath queries like the following:

{code:none}
//element(*,mix:language)[fn:lower-case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message)
{code}

Such queries currently fail with the following exception:

{code:none}
javax.jcr.query.InvalidQueryException: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (
        at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:115)
        at org.apache.jackrabbit.oak.jcr.query.QueryImpl.execute(QueryImpl.java:85)
        at org.apache.sling.jcr.resource.JcrResourceUtil.query(JcrResourceUtil.java:52)
        at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.queryResources(JcrResourceProvider.java:262)
        ... 54 more
Caused by: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.getSyntaxError(XPathToSQL2Converter.java:704)
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.read(XPathToSQL2Converter.java:410)
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseExpression(XPathToSQL2Converter.java:336)
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseCondition(XPathToSQL2Converter.java:279)
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseAnd(XPathToSQL2Converter.java:252)
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseConstraint(XPathToSQL2Converter.java:244)
        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.convert(XPathToSQL2Converter.java:153)
        at org.apache.jackrabbit.oak.query.QueryEngineImpl.parseQuery(QueryEngineImpl.java:86)
        at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:99)
        at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:39)
        at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:110)
{code}",sling xpath,"['core', 'query']",OAK,Bug,Minor,2012-08-02 15:22:25,4
12598044,MicroKernel filter syntax is not proper JSON,"Since we're leveraging JSON quite a bit around the MicroKernel, it would be good if the syntax of the filter argument to getNodes was that of a proper JSON object. Currently the MicroKernel expects names in the filter to be raw tokens instead of quoted strings, as they should be in a normal JSON object.",json,['mk'],OAK,Bug,Minor,2012-07-09 15:21:18,24
12597232,Move the JCR TCK back to the integrationTesting profile,"In revision 1325811 the JCR TCK tests were moved from the integrationTesting profile to a normal build. However, since then the execution time of the TCK has grown to 3+ minutes, which is more than the rest of the Oak build combined. To keep the build time down to at most a couple of minutes, I'd like to move the TCK run back to the integrationTesting profile where it will get executed only when explicitly requested (typically manually before a commit or after one in a CI build).",tck,['jcr'],OAK,Improvement,Minor,2012-07-03 13:00:17,24
12595933,Full text search index,"Oak should support full text search against content in the repository.

As a default implementation of such a feature I'd like to add a simple Lucene-based full text index, somewhat similar to the search index we have in Jackrabbit 2.x.",lucene,"['core', 'query']",OAK,New Feature,Major,2012-06-26 16:14:18,24
12595785,Basic JCR LockManager support,"Even though we currently don't (and perhaps never will) support full JCR locking functionality in Oak, it would still be good to have a basic LockManager implementation for clients that assume it's present and use it simply to access lock tokens and to ask whether a particular node is locked or not.",locking,['jcr'],OAK,Improvement,Minor,2012-06-25 12:44:58,24
12558561,Oak performance benchmark,We need a performance test suite for benchmarking Oak against previous versions (to detect regressions) and other comparable repositories (to know where we stand). ,performance,['benchmarks'],OAK,New Feature,Major,2012-05-30 09:16:03,24
12557435,MicroKernel API: specify retention policy for old revisions,the MicroKernel API javadoc should specify the minimal guaranteed retention period for old revisions. ,api,['mk'],OAK,Improvement,Major,2012-05-24 13:16:12,25
12549684,Create smoke-test build profile,"For quick turn around cycles during development we should have a way to run the most important tests only during a build and exclude longer running tests. 

I propose to create a Maven profile ""smoke-test"" which excludes long running tests. This ensures all tests are run by default but smoke testing can be used during development. 
",test,[],OAK,Improvement,Major,2012-04-05 12:01:30,15
12549631,Efficient diffing of large child node lists,As noted on oak-dev@ revision 1298366 broke the earlier ability to efficiently compare node states with large lists of child nodes. We should restore that feature to avoid introducing a major performance bottleneck for such cases.,performance,['mk'],OAK,Improvement,Major,2012-04-05 06:54:58,24
12546803,Query implementation,"A query engine needs to be implemented. 

A query parser in oak-core should be able to handle xpath, sql2 and optionally other query languages. The jcr component must generate a valid query in one of those languages from JQOM queries and pass that statement along with value bindings, limit, offset, and name space mappings to the oak-core. 

We need to:
* Define the oak-core API for handling queries. How are do we handle name space mappings, limit and offset
* Implement a query builder in the jcr component which takes care of translating JQOM queries to statements in string form 
* Implement a query parser in oak-core and decide on a versatile AST representation which works with all query languages and which is extensible to future query languages.
* Implement the actual query execution engine which interprets the query AST


",query,"['core', 'jcr']",OAK,New Feature,Major,2012-03-16 15:24:40,4
12546795,MVCC causes write skew,Trans-session isolation differs from Jackrabbit 2. Snapshot isolation can result in write skew. See http://wiki.apache.org/jackrabbit/Transactional%20model%20of%20the%20Microkernel%20based%20Jackrabbit%20prototype,documentation test,[],OAK,Test,Major,2012-03-16 14:59:39,15
12546374,Implement a test suite for the MicroKernel,We should have a test suite which thourougly covers the contract of the MicroKernel API,test,['mk'],OAK,New Feature,Major,2012-03-14 10:29:59,25
12546219,Document and tighten contract of Microkernel API,"We should do a review of the Microkernel API with the goal to clarify, disambiguate and document its contract.",api documentation,['mk'],OAK,Improvement,Major,2012-03-13 11:40:53,3
12545672,JCR bindings for Oak,"One of the proposed goals for the 0.1 release is at least a basic JCR binding for Oak. Most of that already exists in /jackrabbit/sandbox, we just need to decide where and how to place it in Oak. I think we should either put it all under o.a.j.oak.jcr in oak-core, or create a separate oak-jcr component for the JCR binding.

As for functionality, it would be nice if the JCR binding was able to do at least the following:

{code}
Repository repository = JcrUtils.getRepository(...);

Session session = repository.login(...);
try {
    // Create
    session.getRootNode().addNode(""hello"")
        .setProperty(""world"",  ""hello world"");
    session.save();

    // Read
    assertEquals(
        ""hello world"",
        session.getProperty(""/hello/world"").getString());

    // Update
    session.getNode(""/hello"").setProperty(""world"", ""Hello, World!"");
    session.save();
    assertEquals(
        ""Hello, World!"",
        session.getProperty(""/hello/world"").getString());

    // Delete
    session.getNode(""/hello"").delete();
    session.save();
    assertTrue(!session.propertyExists(""/hello/world""));
} finally {
    create.logout();
}
{code}
",jcr,['jcr'],OAK,New Feature,Major,2012-03-08 16:32:39,15
12545565,Runnable jar packaging,"We should have a simple runnable jar version of Oak that can be started by double-clicking on the jar or by executing:

{code}
$ java -jar oak-run-0.1.jar
{code}

The jar should start up an Oak repository, and keep running until killed. No specific shutdown sequence or command should be assumed or required; the repository *must* be in a stable state regardless of how the process terminates.",packaging,['run'],OAK,New Feature,Major,2012-03-07 21:41:11,24
12545335,Setup basic build structure,"As suggested on dev@:

{quote}
To get us started, I'll set up a basic multimodule build in oak/trunk
along the lines of what we have in jackrabbit/trunk. As initial
components I propose the following:

   * oak-core, for the main codebase (incl. unit tests)
   * oak-run, for a runnable jar packaging
   * oak-it, for integration tests
   * oak-bench, for performance tests
{quote}
",maven,[],OAK,Task,Major,2012-03-06 12:48:50,24
